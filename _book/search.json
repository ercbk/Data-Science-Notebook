[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science",
    "section": "",
    "text": "Preface\nThese notes were transferred from Evernote to Quarto, so some of notes may be difficult to read as the editing of the note format is still an ongoing process.\nNotes without numbers in sidebar should be a good indicator of a note that Iâ€™ve finished formatting.\nInitially, I had no intent of publicly releasing these notes, so I havenâ€™t attributed all the authors of the content inside this notebook. If you see your work and want credit, please let me know and Iâ€™ll do so.\nAlso, if you see any mistakes or have any questions, please open an issue at the github repository."
  },
  {
    "objectID": "qmd/algorithms-learn-to-rank.html#sec-alg-ltr-misc",
    "href": "qmd/algorithms-learn-to-rank.html#sec-alg-ltr-misc",
    "title": "Learn-to-Rank",
    "section": "Misc",
    "text": "Misc\n\nDefines a ranking function to score each document based on a given query. The documents are then ranked in descending order of these scores, representing the relative relevance of the documents to the query.\n\nThe target variable is some kind of relevance score.\nFor recommendation systems, users are the queries and the products they are interested in are the documents.\n\nTL;DR: LambdaLoss is the most state-of-the-art method available in current py, r packages (See Listwise Ranking &gt;&gt; LambdaLoss), but maybe not overall\nNotes from\n\nLearning to Rank: A Complete Guide to Ranking using Machine Learning\nWhat Is Learning to Rank: A Beginnerâ€™s Guide to Learning to Rank Methods\nHow to evaluate Learning to Rank Models\n\nShows some manual calculations of the metrics to get a feel for how they work.\n\n\nResources\n\nList of approaches and their papers\nApproaches used by popular search engines\n\nUse Cases:\n\nInformation retrieval problems, such as document retrieval, collaborative filtering, sentiment analysis, and online advertising\nTravel Agencies â€” Given a user profile and filters (check-in/check-out dates, number and age of travelers, â€¦), sort available rooms by relevance.\n\nVector Space Models (basic)\n\nCompute a vector embedding (e.g.Â using Tf-Idf or BERT) for each query and document, and then compute the relevance score f(x) = f(q, d) as the cosine similarity between the vectors embeddings of q and d.\nAlso see: wiki\nExample:Â  Learning to rank is good for your ML career - Part 1: background and word embeddings - Embracing the Random\n\nexplainer, uses a python Keras model to create embeddings, compares embeddings with cosine similarity."
  },
  {
    "objectID": "qmd/algorithms-learn-to-rank.html#sec-alg-ltr-preproc",
    "href": "qmd/algorithms-learn-to-rank.html#sec-alg-ltr-preproc",
    "title": "Learn-to-Rank",
    "section": "Preprocessing",
    "text": "Preprocessing\n\nFeature Engineering\n\nCount the number of times a word in the query has occurred in the document\n\ne.g â€œSushiâ€ appears once in d1"
  },
  {
    "objectID": "qmd/algorithms-learn-to-rank.html#sec-alg-ltr-diag",
    "href": "qmd/algorithms-learn-to-rank.html#sec-alg-ltr-diag",
    "title": "Learn-to-Rank",
    "section": "Diagnostics",
    "text": "Diagnostics\n\nMisc\n\nAlso see Algorithms, Recommendation &gt;&gt; Metrics\nUse binary relevance metrics if the goal is to assign a binary relevance score to each document.\nUse graded relevance metrics if the goal is to set a continuous relevance score for each document.\n\nMean Average Precision (MAP)\n\n\nMean average precision within the top k highest-ranked documents\nQ is the total number of queries and r is the relevance scroe\nIssues\n\nIt does not consider the ranking of the retrieved items, only the presence or absence of relevant documents.\nIt may not be appropriate for datasets where the relevance of items is not binary, as it does not consider an itemâ€™s degree of relevance.\n\n\nMean Reciprocal Rank (MRR)\n\nWhere Q is the total number of queries and r is the relevance score\nIssue: considers only the first relevant document for the given query\n\nNormalized Discounted Cumulative Gain (NDCG)\n\nAccounts for both the relevance and the position of the results\nIn the DCG equation, the numerator increases when the documentâ€™s relevance is high; the denominator increases when the position of the document increases. Altogether, DCG value will go high when highly relevant items are ranked higher.\nBound between 0 and 1, where 1 is the best"
  },
  {
    "objectID": "qmd/algorithms-learn-to-rank.html#sec-alg-ltr-ptrank",
    "href": "qmd/algorithms-learn-to-rank.html#sec-alg-ltr-ptrank",
    "title": "Learn-to-Rank",
    "section": "Pointwise Ranking",
    "text": "Pointwise Ranking\n\nThe total loss is computed as the sum of loss terms defined on each document dáµ¢ (hence pointwise) as the distance between the predicted score sáµ¢ and the ground truth yáµ¢, for i=1â€¦n.\n\nBy doing this, we transform our task into a regression problem, where we train a model to predict y.\n\nData is made up of queries (q) and documents (d) associated with those queries\nIssues\n\nTrue relevance (aka absolute relevance) scores are needed to train the model\n\nTo get the ground truth relevance score per each document in the query, we can use human annotators or the number of clicks received for a particular document."
  },
  {
    "objectID": "qmd/algorithms-learn-to-rank.html#sec-alg-ltr-prwse",
    "href": "qmd/algorithms-learn-to-rank.html#sec-alg-ltr-prwse",
    "title": "Learn-to-Rank",
    "section": "Pairwise Ranking",
    "text": "Pairwise Ranking\n\nThe total loss is computed as the sum of loss terms defined on each pair of documents dáµ¢, dâ±¼ (hence pairwise) , for i, j=1â€¦n.\nThe objective function on which the model is trained is to predict whether yáµ¢ &gt; yâ±¼ or not, i.e.Â which of two documents is more relevant.\n\nBy doing this, we transform our task into a binary classification problem, (1 if yáµ¢ &gt; yâ±¼, 0 otherwise).\n\nIn many scenarios training data is available only with partial information, e.g.Â we only know which document in a list of documents was chosen by a user (and therefore is more relevant), but we donâ€™t know exactly how relevant is any of these documents\n\nThis method only requires a partial ordering (i.e.Â relevance) of the documents for each query in contrast to the absolute relevance required for pointwise ranking. (see LambdaRank)\n\nIssues\n\nbiased towards queries with large document pairs\n\nFirst used by RankNet, which used a Binary Cross Entropy (BCE)"
  },
  {
    "objectID": "qmd/algorithms-learn-to-rank.html#sec-alg-ltr-lstwse",
    "href": "qmd/algorithms-learn-to-rank.html#sec-alg-ltr-lstwse",
    "title": "Learn-to-Rank",
    "section": "Listwise Ranking",
    "text": "Listwise Ranking\n\n\nThe loss is directly computed on the whole list of documents (hence listwise) with corresponding predicted ranks. It sums over all pairs of items within a query.\nMaximizes the evaluation metric in contrast to pointwise and pairwise ranking methods\nSoftRank (2008 paper)\n\nInstead of predicting a deterministic score s = f(x) like lambdaRank, a smoothened probabilistic score s~ ğ’©(f(x), ÏƒÂ²) is predicted. \n\nThe ranks k are non-continuous functions of predicted scores s, but thanks to the smoothening, probability distributions can be calculated for the ranks of each document.\nFinally, SoftNDCG, the expected NDCG over this rank distribution, is optimized, which is a smooth function.\n\n\nListNet (2007 paper)\n\nEach ranked list corresponds to a permutation, and loss is defined over the permutation space\nGiven a list of scores, s, the probability of any permutation is defined using the Plackett-Luce model.\nLoss is computed as the Binary Cross-Entropy distance between true and predicted probability distributions over the space of permutations.\nExample: Learning to rank is good for your ML career - Part 2: letâ€™s implement ListNet! - Embracing the Random\n\nexplainer, uses a keras model to implement listnet\n\n\nLambdaLoss (2018 paper)\n\n\nIntroduced a generalized framework that uses as a mixture model, where the ranked list, Ï€ , is treated as a hidden variable. Then, the loss is defined as the negative log likelihood of such model.\nResults\n\nAll other listwise methods (RankNet, LambdaRank, SoftRank, ListNet, â€¦) are special configurations of this general framework. Indeed, their losses are obtained by accurately choosing the likelihood p(y | s, Ï€) and the ranked list distribution p(Ï€ | s).\nThis framework allows us to define metric-driven loss functions directly connected to the ranking metrics that we want to optimize.\n\nPackages\n\n{{allRank}}: PyTorch-based framework for training neural Learning-to-Rank (LTR) models, featuring implementations of:\n\ncommon pointwise, pairwise and listwise loss functions\nfully connected and Transformer-like scoring functions\ncommonly used evaluation metrics like Normalized Discounted Cumulative Gain (NDCG) and Mean Reciprocal Rank (MRR)\nclick-models for experiments on simulated click-through data\n\n{{pytorchltr}}: support the infrastructure necessary for performing LTR experiments in PyTorch\n\nUtilities to automatically download and prepare several public LTR datasets\nSeveral pairloss and LambdaLoss functions associated with a few different metrics\n\n{{tensorflow_ranking}}: TensorFlow LTR library\n\nCommonly used loss functions including pointwise, pairwise, and listwise losses.\nCommonly used ranking metrics like Mean Reciprocal Rank (MRR) and Normalized Discounted Cumulative Gain (NDCG).\nMulti-item (also known as groupwise) scoring functions.\nLambdaLoss implementation for direct ranking metric optimization.\nUnbiased Learning-to-Rank from biased feedback data."
  },
  {
    "objectID": "qmd/algorithms-learn-to-rank.html#sec-alg-ltr-lmdrnk",
    "href": "qmd/algorithms-learn-to-rank.html#sec-alg-ltr-lmdrnk",
    "title": "Learn-to-Rank",
    "section": "LambdaRank",
    "text": "LambdaRank\n\nLambdaRank is a gradient, despite being closely related to the gradient of the classic pairwise loss function, and Itâ€™s a pointwise scoring function, meaning that the LightGBM ranker â€œtakes a single document at a time as its input, and produces a score for every document separately.â€\n\nSo, a bit of a hybrid between pointwise and pairwise ranking\n\nMisc\n\nXGBoost can also perform lambdarank (docs)\n\nDemo\nobjective\n\nrank:pairwise: Use LambdaMART to perform pairwise ranking where the pairwise loss is minimized\nrank:ndcg: Use LambdaMART to perform list-wise ranking where Normalized Discounted Cumulative Gain (NDCG) is maximized\nrank:map: Use LambdaMART to perform list-wise ranking where Mean Average Precision (MAP) is maximized\n\neval_metric (docs)\n\ndefault: mean average precision\nauc: When used with LTR task, the AUC is computed by comparing pairs of documents to count correctly sorted pairs. This corresponds to pairwise learning to rank. The implementation has some issues with average AUC around groups and distributed workers not being well-defined.\naucpr: Area under the PR curve. Available for classification and learning-to-rank tasks.\n\nFor ranking task, only binary relevance label is supported. Different from map (mean average precision), aucpr calculates the interpolated area under precision recall curve using continuous interpolation.\n\n\ngroup input format (docs)\n\nFor a ranking task, XGBoost requires an file that indicates the group information.\n\nquery id columns (docs)\n\nFor a ranking task, you may embed query group ID for each instance in the LIBSVM file by adding a token of form qid:xx in each row\n\n\n\nLightGBM:\n\nlambdarank objective: LGBMRanker(objective=\"lambdarank\")\nExpects the target (relevance score) to be an integer\n\nIf your score is a decimal, then you can apply a transformation to get an integer\n\nmultiply by 10, 100, etc. to get an integer (e.g.Â 0.10 -&gt; 0.14 * 100 = 14) (See Example: anime recommendation)\nBin by quantile, then apply a ranking function (See Example: stock portfolio)\n\nUseful if the variable has negative values\n\n\n\nDatasets should be sorted by user id or query id (might be required for group parameter, see below)\n\nDidnâ€™t see this in every example, so I donâ€™t know if itâ€™s required or not. None of the authors gave me complete confidence that they knew what they were doing.\n\n\nExample: py, LightGBM, anime recommendation (article)\n\ntest_size = int(1e5)\nX,y = train_processed[features],train_processed[target].apply(lambda x:int(x * 10))\ntest_idx_start = len(X)-test_size\nxtrain,xtest,ytrain,ytest = X.iloc[0:test_idx_start],X.iloc[test_idx_start:],y.iloc[0:test_idx_start],y.iloc[test_idx_start:]\n\nget_group_size = lambda df: df.reset_index().groupby(\"user_id\")['user_id'].count()\ntrain_groups = get_group_size(xtrain)\ntest_groups = get_group_size(xtest)\nprint(sum(train_groups) , sum(test_groups))\n#(4764372, 100000)\n\nmodel = LGBMRanker(objective=\"lambdarank\")\nmodel.fit(xtrain,ytrain,group=train_groups,eval_set=[(xtest,ytest)],eval_group=[test_groups],eval_metric=['ndcg'])\n\nIn this example, â€œrelevance_scoreâ€ is the relevance score (i.e.Â target), users (â€œuser_idâ€) are the queries and the anime (â€œanime_idâ€) they are interested in are the documents.\n\nOther predictors included\n\nThe â€œgroupâ€ parameter enables the model to learn the relative importance of different features within each group, which can improve the modelâ€™s overall performance\n\nA query (e.g.Â user) can have many documents (e.g.Â products) associated with it. So in this example, a user and the products associate with him is a â€œgroupâ€.\nThe total groups should be the number of samples (i.e.Â total number of users or queries)\ne.g.Â group = [3, 2, 1] says the first user has 3 products associated with her, the seconder user has 2 products, and the third user has 1 product.\nâ€œSo itâ€™s vital to sort the dataset by user_id(query_id) before creating the group parameter.â€\n\nPredicting relevance scores in production for the same users in the training set\n\nYou need to have a selection of products that the user hasnâ€™t used before.\n\ne.g.Â movies: if youâ€™re recommending movies to a user, you shouldnâ€™t recommend ones theyâ€™ve already seen. (i.e.Â movies mapped to their user id in the training data)\n\nIn this example\n\nSelect the userâ€™s favorite N number of genres.\nFor each genre in the above-selected genres, pick the highest-rated m animes. Now you have M* N animes to rank for that user.\nJust create the user base and anime-based features. And finally, call the .predict() method with the created feature vector.\n\n\nExample: py, LightGBM, score interpretation (article)\n\nimport lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=1)\n\nquery_train = [X_train.shape[0]]\nquery_val = [X_val.shape[0]]\nquery_test = [X_test.shape[0]]\n\ngbm = lgb.LGBMRanker()\ngbm.fit(X_train, y_train, group=query_train,\nÂ  Â  Â  Â  eval_set=[(X_val, y_val)], eval_group=[query_val],\nÂ  Â  Â  Â  eval_at=[5, 10, 20], early_stopping_rounds=50)\n\nNo clue what the data isâ€¦ could be movies for a recommendation\nâ€œearly_stopping_rounds=50â€ says training continues until thereâ€™s no improvement for 50 rds.\nâ€œeval_atâ€Â  are the k values used to evaluate nDCG@k over the validation set\nPredictions\n\n\ntest_pred = gbm.predict(X_test)\nX_test[\"predicted_ranking\"] = test_pred\nX_test.sort_values(\"predicted_ranking\", ascending=False)\n\nâ€œThis numbers can be interpreted as probabilities of a item being relevant (or being at the top).â€\n\nMost are negative, though! (See below, Example: ranking stocks for a portfolio &gt;&gt; Predictions on the test set)\n\nThe scores are ordered into a ranking and can be evaluated using a ranking metric (Precision@k, MAP@K, nDCG@K)\nExample: py, LightGBM, ranking stocks for a portfolio (article)\n\nTarget is â€œTarget.â€ It might be daily returns that being used as the relevance score. Queries is â€œDateâ€ and the documents are stocks (â€œSecuritiesCodeâ€)\nThe target variable is a real-valued float and has negative values, so it was ranked per Date, then binned into quantiles.\n\n\ndf[\"Target\"] = df.groupby(\"Date\")[\"Target\"].rank(\"dense\", ascending=False).astype(int)\ndf[\"Target\"] = pd.qcut(df.Target, 30).cat.codes\n\ncat.codes will give what are essentially ranks after the binning\nTime Series Splits\n\n# Just some arbitrary dates\ntime_config = {'train_split_date': '2021-12-06',\nÂ  Â  Â  Â  Â  Â  Â  'val_split_date'Â  : '2022-02-10',\nÂ  Â  Â  Â  Â  Â  Â  'test_split_date' : '2022-02-20'}\ntrain = df[(df.Date &gt;= time_config['train_split_date']) & (df.Date &lt; time_config['val_split_date'])]\nval = df[(df.Date &gt;= time_config['val_split_date']) & (df.Date &lt; time_config['test_split_date'])]\ntest = df[(df.Date &gt;= time_config['test_split_date'])]\n\nGroup parameter\n\nquery_train = [train.shape[0] /2000] * 2000Â  # Because we have 2000 stocks in each time group\nquery_val = [val.shape[0] / 2000] * 2000\nquery_test = [test.shape[0] / 2000] *2000\n\nModel\n\nfrom lightgbm import LGBMRanker\ncol_use = [c for c in df.columns if c not in [\"RowId\",\"Date\", \"Target\"]] # predictors\nmodel_return = LGBMRanker(n_estimators=15000,\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  random_state=42,\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  num_leaves=41,\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  learning_rate=0.002,\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  #max_bin =20,\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  #subsample_for_bin=20000,\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  colsample_bytree=0.7,\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  n_jobs=2)\nmodel_return.fit(train[col_use], train['Target'],\nÂ  Â  Â  Â  Â  Â  group = query_train,\nÂ  Â  Â  Â  Â  Â  verbose=100,\nÂ  Â  Â  Â  Â  Â  early_stopping_rounds=200,\nÂ  Â  Â  Â  Â  Â  eval_set=[(val[col_use], val['Target'])],\nÂ  Â  Â  Â  Â  Â  eval_group=[query_val],\nÂ  Â  Â  Â  Â  Â  eval_at=[1] #Make evaluation for target=1 ranking, I choosed arbitrarily\nÂ  Â  Â  Â  Â  Â  Â  Â  )\n\nâ€œearly_stopping_rounds=200â€ says training continues until thereâ€™s no improvement for 200 rds.\nâ€œeval_atâ€Â  are the k values used to evaluate nDCG@k over the validation set\nPredictions on the test set\n\nfor (prices, options, financials, trades, secondary_prices, sample_prediction) in iter_test:\nÂ  Â  try:\nÂ  Â  Â  Â  sample_prediction['Rank'] = model_return.predict(prices[col_use]) * -1\nÂ  Â  Â  Â  # Get the ranks from prediction first and for the duplicated ones, just rank again\nÂ  Â  Â  Â  sample_prediction['Rank'] = sample_prediction.groupby(\"Date\")[\"Rank\"].rank(\"dense\",Â \nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ascending=False).astype(int)\nÂ  Â  Â  Â  sample_prediction['Rank'] = sample_prediction.groupby(\"Date\")[\"Rank\"].rank(\"first\").astype(int) - 1\nÂ  Â  except:\nÂ  Â  Â  Â  sample_prediction['Rank'] = 0\nÂ  Â  sample_prediction = sample_prediction.replace([-np.inf, np.inf], np.nan).fillna(0.0)\nÂ  Â  # register your predictions\nÂ  Â  env.predict(sample_prediction)\nÂ  Â  display(sample_prediction)\n\nâ€œiter_testâ€ is some kind of list or maybe json object imported from the kaggle (this example is a kaggle notebook) that has the unseen data to be used to predict with your final model (â€œ# get iterator to fetch data day by dayâ€)\nPreds are multiplied by -1 because most are probably negative for some reason (see prev. example)\nTheyâ€™re ranked per query (â€œDateâ€)\nThe stock ranked first for each query (â€œDateâ€) is pulled"
  },
  {
    "objectID": "qmd/algorithms-marketing.html#sec-alg-mark-misc",
    "href": "qmd/algorithms-marketing.html#sec-alg-mark-misc",
    "title": "Marketing",
    "section": "Misc",
    "text": "Misc\n\nAlso see Business Plots\nPropensity Model and Uplift Score Model are basically predicting the same thing with the Uplift Score Model notes going a bit further in detail with the marketing experiment\nPopulation targeting based on your average conversion rate (Thread)\n\nLow avg conversion rate: Treat those with higher probabilities of buying, because the extra nudge helps them get over the finish line.\nHigh avg conversion rate (&gt;80%): Treat those that are less likely to convert\nMiddling avg conversion rate: Treat those with predictions closer to 0.5\n\nLimitations of a simple rules based approach (typical baseline model)\n\nIt is likely not exploiting all the data you have at your disposal whether it be more precise information on the customer journey or your website or other data sources you may have at your disposal like CRM data.\nWhile it seems obvious that customers classified as â€œHotâ€ are more likely to purchase than â€œWarmâ€ which are more likely to purchase than â€œColdâ€, this approach does not give us any specific figures on how likely they are to purchase. Do â€œwarmâ€ customers have 3% chance to purchase ? 5%? 10% ?\nUsing simple rules, the number of classes you can obtain is limited, which limits how customized your targeted response can be."
  },
  {
    "objectID": "qmd/algorithms-marketing.html#sec-alg-mark-prop",
    "href": "qmd/algorithms-marketing.html#sec-alg-mark-prop",
    "title": "Marketing",
    "section": "Propensity Model",
    "text": "Propensity Model\n\nUses GA data for your website to model probabilities of a customer purchasing\nHelps marketers to decrease cost per acquisition (CPA) and increase ROI\n\nYou might want to have a different marketing approach with a customer that is very close to buying than with one who might not even have heard of your product.\nAlso if you have a limited media budget , you can focus it on customers that have a high likelihood to buy and not spend too much on the ones that are long shots\n\nExample: Using Google Analytics data\n\nNotes from Scoring Customer Propensity using Machine Learning Models on Google Analytics Data\nData\n\nUsed GA360 so the raw data is nested at the session-level\n\nSee Google, Analytics &gt;&gt; Misc &gt;&gt; Google Analytics data in BigQuery for more details on this type of data\n\nAfter processing you want 1 row per customer\nGA keeps data for 3 months by default\n\nCreate features\n\nGeneral Features - metrics that give general information about a session\nFavorite Features - Categorical data\n\nA user can have multiple categories so youâ€™ll have to count instances for each category and choose a favorite and do something about ties\nSome variables may have high cardinality\n\nAfter choosing a â€œfavoriteâ€ for each user, group categories with low counts into an Other category.\nDepending on how many categories are left, youâ€™ll need to choose a encoding method\n\n\nProduct Features - numerics that help answer if a customer is likely to buy a specific product\nSimilar Product Features - numerics for substitute products which are products similar to the product of interest\n\nKnowing that the customer interacted with other products with similar function and price range can definitely be useful.\nSimilar products for a given product are defined using business inputs\n\nNotes\n\nNot familiar with marketing campaigns but product, similar product features might be optional depending if the campaign is for a specific product or a group of products or general sale (e.g.Â black friday) or maybe the campaign is taylored to each group of customers.\n\n\nProcessing\n\nEach row is a customer\nCompute the features by aggregating values over a 3 month time window for each customer\nCompute the target (purchase/no purchase) using the sessions in a 3 weeks time window subsequent to the feature time window to detect whether the customer purchased or didnâ€™t purchase over this window.\n\nIf there is at least one purchase of the product in the time window, Target it equal to 1 (defined as Class 1), else Target is equal to 0 (defined as Class 0)\nLikely a strong class imbalance. Potential solutions:\n\nupsample/downsample as appropriate\nSwitch the target variable from â€œmaking a purchaseâ€ to making an â€œadd to cartâ€\n\nmodel looses a bit in terms of business signification but increasing the volume of Class 1 more than compensates\n\n\n\n\nFit a binary classification model (add_to_cart/didnâ€™t add_to_cart)\n\nAlso see Cross-Validation &gt;&gt; Sliding Window CV\nInstead of each fold using a different block of observations for the validation/test set, each successive fold slides the interval of the target variable interval (e.g.Â 3 weeks) from the previous fold\nSplit the data before creating the folds to avoid leakage.\nFor each algorithm average loss (e.g.Â PR-AUC score) across test folds.\nChose your algorithm and fit the final model on the whole dataset\n\nCalculate Model Uplift\n\nSort customers by their probability score by dividing the customers into ventiles (i.e.Â 20 bins).\nUplift is defined as the Class 1 Rate in the top 5%Â (1st bin) / the Class 1 Rate across all the dataset (all bins).\n\nClass 1 Rate of top 5% = number of customers in the 1st bin that added_to_cart / total number of customers in the 1st bin\nClass 1 Rate of whole dataset = number of customers that added_to_cart in the dataset / total number of customers in the dataset\n\nExample:\n\nIf we have 21% Add to Cart in the top Top 5 % of the dataset vs 3% Add to cart Rate in whole dataset\nuplift = 7, which means our model is 7 times more effective than a random model.\n\nIâ€™m not sure if Iâ€™d say a â€œrandomâ€ model since this implies that customers are chosen at randomly. If the whole dataset was used, then doesnâ€™t this imply that no â€œchoiceâ€ made? So, Iâ€™d say this is the uplift over no model being used\n\n\n\nGo live with Ad campaign and calculate ROAS (Return on Ad Spend)\n\nArticle didnâ€™t really touch on the graphic but these are my assumptions\nSplit into groups\n\nLooks like only group is chosen to take part in the experiment and the rest are control or rule-based (see Misc section).\n\nSplit into Segments\n\nThis looks like the binning of probabilities that occurred during the uplift calculation earlier\nOnly 8 bins are shown but that is probably just because of the size of the graphic. Should probably use 20 bins like before\n\nActivation\n\nThe first 3 bins get ad money. This would map to the top 5%, 10%, and 15% bins with the 5% bin getting twice the ad spend as the others.\n\nEvaluation\n\nIt says itâ€™s based on conversions but Iâ€™m guessing they get to Revenue somehow since ROAS should be in dollars\nIf you canâ€™t get the Revenue number directly see Uplift Model &gt;&gt; 3 types of Uplift calculations for each type of offer\n\nShows how to get from Conversion Uplift to and estimate of Revenue Uplift\n\n\n\nCompare the modelâ€™s ROAS with Control or Rules-based groups\n\nGoogle also has Session Quality Score which comes which is available to compare the ROAS too."
  },
  {
    "objectID": "qmd/algorithms-marketing.html#sec-alg-mark-uplift",
    "href": "qmd/algorithms-marketing.html#sec-alg-mark-uplift",
    "title": "Marketing",
    "section": "Uplift Score model",
    "text": "Uplift Score model\n\nAlso see\n\nMarketing &gt;&gt; Customer Segmentation, Customer Journey\n\nSimilar goals to Propensity Model\nNotes from https://towardsdatascience.com/uplift-modeling-e38f96b1ef60\nGoal:\n\nidentify which customers are most likely to purchase when given an offer (e.g.Â buy one get one free).\nHelps to control marketing costs by efficiently targeting the customers where marketing spending will be most effective.\n\nSteps:\n\nRun an experiment where youâ€™ve taken a group of customers and randomly assigned them into treatment and control groups and recorded whether or not they purchased after receiving the offer and a period of time. Here the treatment is the custormer receiving a discount offer or a buy-one-get-one free offer.\nCalculate the three uplift metrics for each offer type.\nFit a binary class model that provides conversion probabilities for each customer.\n\nAlso see Propensity Model\n\nFit a multiclass model whose probabilities will be used to calculate an uplift score for each customer.\n\nThe conversion probabilities will be used to investigate which features are driving the conversion uplift.\nThe upper quantile uplift scores can be used to target customers that are more likely to respond to a particular offer.\n\n\nMetric Calculations\n\nUplift\n\nConversion Uplift (%) = Conversion rate of treatment group - conversion rate of control group.\n\nâ€œconversionâ€ means a purchase was made.\n\nLarger is better.\n\nPurchase rate increase/decrease resulting from the offer campaign\n\nOrder Uplift = Conversion uplift * # converted customers in treatment group.\n\nThe number of purchases as a result of the offer campaign\n\nRevenue Uplift ($) = Order Uplift * Average order $ value.\n\nExpected revenue as a result of the offer campaign\n\n\nbase conversion rate:\n\n\nÂ  Â  Â  Â  df %&gt;%\nÂ  Â  Â  Â  Â  Â  filter(offer = \"no offer\") %&gt;%\nÂ  Â  Â  Â  Â  Â  summarize(base_conv = mean(conversion))\n\noffer has 3 categories: no offer, discount, buy_one_get_one (bogo)\n\ndiscount and bogo conversion rates similar. Calc by filtering the different offers\n\ndiscount conversion uplift = discount conversion rate - base conversion rate\n\nbogo conversion uplift similar\n\n# of converted customers in discount group:\n\nÂ  Â  Â  Â  df %&gt;%\nÂ  Â  Â  Â  Â  Â  filter(offer = \"discount\", conversion == 1) %&gt;%\nÂ  Â  Â  Â  Â  Â  count( )\n\ndiscount order uplift = discount conversion uplift * # of converted customers in discount groupÂ \nFor Revenue Uplift equation, â€œAverage order $ valueâ€ was a constant which I guess is just a descriptive statistic you could easily calc from your sales data.\nViz: group_by predictor var â€“&gt; mutate calc conversion mean â€“&gt; bar plot with x = pred var, y = conversion mean\n\nbars with the higher conversion means are more important in driving the conversion uplift metric\n\nModels\n\nModel conversion probabilities: xgboost with conversion as the target var\n\npredict on the test set\nModel GOF: on the test set, compare the predicted and real order upticks for discount\n\nreal_order_upticks = length(test_dat) * (conversion mean for offer == discount - conversion mean for offer == no offer)\npredicted_order_upticks = length(test_dat) * (predicted conversion probabilities mean for offer == discount - predicted conversion probabilities mean for offer == no offer)\nerror = abs(real - predicted)/real\nreal revenue = real_order_upticks * avg order $ value (similar for predicted revenue)\ndo the same for bogoÂ \n\n\n Â add probabilities from a multi-class model where the classes of your target variable are:\n\nTreatment Responders (TR): Customers were given offer and did purchase\nTreatment Non-Responders (TN): Customer were given offer and didnâ€™t purchase\nControl Responders (CR): Customers werenâ€™t given offer and purchased\nControl Non-Responders (CN): Customers werenâ€™t given offer and didnâ€™t purchase\n\nCreate a target variable with the classes above.\n\nTreatment will include customer who received either a discount or bogo offer and control is made up of the â€œno offerâ€ customers.\n\nMay need to make the target variable numeric for xgboost.\n\n\nIn the example, there was a history variable which was the total amount($) that was purchased by the customer in the past. This variable was k-means clustered into 5 clusters, and the clusters were used as a variable. e.g.Â customer id# 12 belongs to cluster 4. I donâ€™t know if this is a better way of binning a continuous variable or what. Need to research if maybe itâ€™s some sort of Kaggle thing or something shown to work well with xgboost. 5 clusters were used with no explanation of how that number was reached.\nPredictor variables used:\n\nrecency: months since last purchase\nclustered_history: clustered history variable which was the $value of the historical purchases\nused_discount: indicator on whether the customer had used a â€œdiscountâ€ offer before this experiment\nused_bogo: indicator on whether a customer had used a â€œbuy one get oneâ€ offer before this experiment\nzip_code: class of the zip code as Suburban/Urban/Rural\nis_new: indicates if the customer is new\nchannel: methods of contact that the customer is using, Phone/Web/Multichannel\n\nXGBoost model trained. Predict( ) using training data to get class probabilities.\nTake target var predicted probabilities and calculate the uplift score for each customer using the above formula.\nModel results analysis: compare uplift metrics of customers with an upper quantile (&gt; 0.75) uplift score to those with a lower quantile (&lt; 0.25). Use test set.\n\nTake the Revenue Uplift from the first section and calc the revenue uplift per targeted customer. This will be used as a baseline to compare the two quantiles against.\n\nÂ base_Discount Revenue Uplift per targeted = Discount Revenue Uplift / # of customers with offer == discount\n\nfilter test data where uplift_score &gt; quantile(uplift_score, 0.75) â€“&gt; calc the Discount Uplift metrics â€“&gt; calc the test_Discount Revenue Uplift per targeted customer.\nEstimated percentage change in the Discount Revenue Uplift per targeted customer = (test_DRU_per_cust - base_DRU_per_cust) / base_DRU_per_cust\nSays that the model estimates that targeting the upper quantile uplift scores with a discount offer will lead to a  increase in revenue uplift per customer."
  },
  {
    "objectID": "qmd/algorithms-marketing.html#sec-alg-mark-clv",
    "href": "qmd/algorithms-marketing.html#sec-alg-mark-clv",
    "title": "Marketing",
    "section": "Customer Lifetime Value (CLV)",
    "text": "Customer Lifetime Value (CLV)\n\nThe present value of all future cash flows of a current customer (inflation adjusted and usually cost of capital adjusted)\n\nThink the cost of capital adjustment is taken care of by the discount rate used in the DERL calculation below.\nAlso see\n\nAlgorithms, Product &gt;&gt; Cost Benefit Analysis (CBA) &gt;&gt; Internal Rate of Return\nProject, Planning &gt;&gt; Decision Models &gt;&gt; Terms &gt;&gt; Cost of Capital, Internal Rate of Return\nMorgan Stanleyâ€™s Guide to Cost of Capital\n\nThread about the guide\n\n\n\nAllows marketers to ensure that the most valuable customers have the least churn probability in the long term\nThe more valuable the customer is, the more likely you should be to build products theyâ€™d buy, to buy ad space in the magazines they read, to provide a dedicated sales person to them, and to proactively address their needs.\nMisc\n\nNotes from:\n\nhttps://towardsdatascience.com/calculating-customer-lifetime-values-using-a-shifted-beta-geometric-model-86bf538444f4\nhttp://www.brucehardie.com/notes/018/DERL_in_Excel.pdf\n\nAlso see\n\nMarketing &gt;&gt; Conversion Lift Model &gt;&gt; Cost per Incremental Conversion\nOverview of paper, A Deep Probabilistic Model for Customer Lifetime Value Prediction\nVideo: Bizsci\n\nlab-58-customer-lifetime-value-rfm-calc\npython customer lifetime value, rfm + xgboost (youtube)\n\n\nPackages: {CLVTools}, {BTYD}, {BTYDplus}, {{btyd}}\nAccountants discount the result to todayâ€™s dollar value so that you know how much each customer is worth now (that is, taking future inflation into account).\n\nWhat is it used for?\n\nmarketing campaigns â€” how much should we spend to acquire or retain customers?\n\nacquire a customer:\n\nSelect a marketing campaign whose acquisition cost is lower than the CLV for an average active customer\n\nacquisition cost = total_campaign_spend / # of new customers acquiredÂ \n\n\nretain: spend on each customer according to their current years (retention length) as an active customer\n\nGuessing marketing decides what percentage of the clv is appropriate to spend\nUse retention rate to judge new features to apps or webpages\nDoes this marketing campaign attract users with high retention rates?\n\n\ncustomer segmentation â€” who are our most valuable customers and what are their demographic and behavioral traits?\n\nSee Marketing &gt;&gt; Customer Segmentation\n\noverall health of the business â€” how is our CLV changing over time? (growth metric)\ntotal company value â€” how can we estimate the value of our existing and future customers?\nused as a constant in loss functions for ML models\n\nIf your model outputs a false positive and your resultant actions cause you to lose a good customer, then the CLV of that customer is a cost related to the false positive.\n\n\nPrimary ways to increase CLV\n\nDecrease churn\nIncrease prices\nSell more to each customer\n\nBusiness model elements must be taken into account when calculating CLV\n\ncontractual vs non-contractual\n\ncontractual: subscription, credit cards, software-as-a-service\nnon-contractual: grocery store, car sales\n\ncontinuous vs discrete (transaction frequency)\n\ncontinuous: credit card payments to a retail business, i.e.Â unpredictable\ndiscrete: customers pay on a fixed cycle, e.g.Â monthly, yearly payments\n\n\nnon-contractual and continuous\n\nDaily activity rate for an app used as a retention rate\n\nUsers are grouped by start date which is their cohort\nFor each cohort, the number of users that are active on the app is counted each day after their start date\nRentention rate_day = user_count_day/cohort_size\n\nSo, thereâ€™s a â€œretention rateâ€ for each day after the start date\n\nThis data can be used to perform an A/B test to monitor how each cohort reacts in terms of app activity after a push notification or marketing campaign of some sort.\n\nBG/NBD Gamma-Gamma model\n\nBG/NBD model predicts the future number of transactions per customer, which is then fed into the Gamma-Gamma model to predict their monetary value\nMisc\n\nNotes from: Buy Till You Die: Understanding Customer Lifetime Value\n\nExample using {{btyd}}\n\nAlso see:\n\nBeta Geometric Negative Binomial Distribution (BG-NBD) model: explainer, py example\n\npredicts the future number of purchases made by a customer as well as their probability of being alive, i.e.Â not churned yet\n\n\nPackages\n\n{BTYDplus} - various types of Bayesian NBD models\n{{btyd}}\n\n\nBG/NBD\n\nData needed for each customer: time, volume, and value of each of their purchases\nCalculate these features:\n\nRecency, length of time since a customerâ€™s last purchase or last visit to the website or the mobile app\n\nCan be in units of hours, days, weeks, etc (Assume Time and Recency should have the same units)\n(customers[\"max\"] - customers[\"min\"]) / np.timedelta64(1, freq) / freq_multiplier\n\nFrequency, or the count of time periods the customer made a purchase in\n\nRemove all the rows in which frequency is zero, that is all customers who have made only one purchase\nImportant note: some resources around the web claim that frequency is the number of repeat purchases the customer has made which makes it one less than the total number of purchases. This is incorrect, since frequency should ignore multiple purchases in the same time period\n\nTime, or the customerâ€™s age (this is the time difference between the customerâ€™s first purchase and the end date of our analysis, often the last date available in the data)\n\nCan be in units of hours, days, weeks, etc (Assume Time and Recency should have the same units)\n\nMonetary value, or the average revenue or income from the customerâ€™s repeat purchases\n\nAssumptions\n\nTransactions\n\nNumber of transactions has a Poisson dgp\n\nÎ» = 2 means that a customer makes two purchases per time period, on average\n\nThe rate Î» is different for each customer, and its distribution over all customers is assumed to be the Gamma distribution\nWhen the number of transactions is 0, the customer has churned\n\nChurn\n\nAfter each purchase, a customer might churn with some probability p.Â This probability is different for each customer, and its distribution over all customers is assumed to be the Beta distribution.\n\n\nUse model to estimate distribution parameters:\n\nModel takes Recency, Frequency, and (I think) Time variables\nGamma (customer transaction rates): r and Î±\nBeta (churn probability): a and b\n\nPlot distributions with parameter estimates to see if they look reasonable\nFrequency-Recency Heatmap\n\n\nColored by the expected number of future purchases in a predefined time span (e.g.Â 7 days) for a given Recency and Frequency value of a customer\nNote that 0 on the y-axis is at the top\nInterpretation\n\nBottom-Right: we can expect the most purchases from customers who have historically featured high frequency and high recency\n\nTop-Right: The usually high-freq customers that havenâ€™t bought in a while\n\nBottom-Left: Customers that buy infrequently, but we have seen them recently, so they might or might not buy again â€” hence the tail of decreasing predictions stretching towards the bottom-left corner\n\n\nChurn Heatmap\n\n\nColored by churn probability for a given Recency and Frequency value of a customer\nNote that 0 on the y-axis is at the top\nInterpretation\n\nBottom-Right: high-freq-recently-seen customers are the most likely to have not churned (â€œstill aliveâ€)\n\n\n\n\n\ncontractual and discrete\n\ncustomers are cohorted based on how many consecutive years theyâ€™ve been active customers. An active customer is a customer with a current contract/subscription.\n\nCohort 1 has customers who have their initial contract + 3 contract renewals.\n\n\nGreen represents current active customers\n\n\nThe retention rate, r, for each cohort, k, and number of active years , t,Â  is the ratio of active customers, c, for that subscription year and the previous subscription year.\n\n\n\nexample for cohort 1, 2nd interval:Â  \n\n\nThe churn rates which are just 1-r.\nThe annual survival rates for each cohort\n\n Â where k is the cohort and t is the final contract year for which r is being calculated.\n\nFor cohort 0:\n\n\n\nForecasting survival rates\n\nChurn rates can be modeled with Beta Regression which can be used to calculate retention rates and therefore survival rates\n\nA basic forecast model would be to use the average retention rate of all the cohorts\n\nExample: IfÂ   , thenÂ  \nUsing this method neglects the fact that as customer loyalty (# of consecutive subscriptions) increases the likelihood a customer subscribes the next year (i.e.Â survives) increases\n\nTechnically, the literature says that a â€œshifted-beta-geometric (sBG) distributionâ€ is used to calculate the Î± and Î² parameters of a Beta Distribution. Unless this is a different calculation from a standard Beta distribution, the parameters can be calculated using the sample mean and sample standard deviation. Compare â€œDERL PAPERâ€ bkmkâ€™d in the CLV folder with calculation from bkmk in Beta regression folder.\nCurves and interpretations wrt to various Beta Regression parameter values, Î± and Î²\n\n\n\n\nA survival rate can be calculated using the retention rates from formula using the model estimates for Î± and Î².\n\n\n\n\n\nCompute Discounted Expected Residual Lifetime (DERL)\n\nA CLV measure for contractual businesses\n\n\nFormula assumes a constant net cashflow per period (e.g.Â contract = $1000 per customer per year)\nâ€œactive in nâ€ refers to number of years as an active customer\n\nsee first sentence of the â€œcontractual and discreteâ€ section above for an active customer definition\nsee 3rd bullet below for an example\n\nd: discount rate (aka hurdle rate)\n\nMinimum expected return on investment. 10% is common and 5% is usually the minimum, i.e.Â a replacement-level ROI. If the discount rate is 10% percent, then that says that the current investment strategies are yielding at least 10%, so this one should be at least that good. The thinking is, â€œwhy spend money on this investment if it has a lower discount rate instead of putting it to work in one of the other strategies thatâ€™s yielding at least 10%?â€\n\nThe summation goes from n to infinity but in practice, is stopped when term values approach zero.\n\nExample goes out to t = 200 and the last term is equal to 5 x 10^-12. which seems like extreme overkill to me.\n\nExample for cohort 2: 3 active years with 2 renewals\n\nCalculate the Survival Rates out to the specified number of terms\n\nS(0) = 1, S(1) = [(beta + (t=1) -1) / (alpha + beta + (t=1) - 1)] * S(0), S(2) = same equation with t=2 * S(1) â€¦ compute until S(200)\n\nConceptually, S(0) makes sense because t = 0 is the initial contract, so everyone â€œsurvived.â€ Therefore the survival rate = 1. But it doesnâ€™t really follow if you set t = 0 in the retention rate equation\n\n\nCalculate the Survival Rate Ratios,  , first half of the DERL equation\n\nWe calculating for cohort 2, where custormers have n = 3 active years, so the denominator with always be n-1 =2. The number just follows the index, t, in the summation.\nS(3)/S(2), S(4)/s(2), â€¦, S(200)/S(2)\n\nCalculate the Discount Rate half of the equation using a 10% discount rate,  :\n\n(1/(1+0.10))^(3-3), same^(4-3), â€¦, same^(200-3).\nSo this ratio acts a scaling factor than makes the terms smaller and smaller.\n\nderl_2 = sum(element-wise product of last two steps)Â \n\nIn R, this would some kind of recursive loop? Maybe a map2 function with a vector with t-values and a constant n vector (so as not to hard-code) as arguments. Or a pmap and include a constant, d vector\nfor a customer who has made two contract renewals (evaluated just before the point in time at which the third contract renewal decision is to be made)\n\n\n\n\nValue of Consumer Base\n\n\nValue = [(# number of consumers in cohort 0) * contract_value * derl_0] + [(number of consumers in cohort 1) * contract_value * derl_1] + â€¦"
  },
  {
    "objectID": "qmd/algorithms-marketing.html#sec-alg-mark-churn",
    "href": "qmd/algorithms-marketing.html#sec-alg-mark-churn",
    "title": "Marketing",
    "section": "Churn",
    "text": "Churn\n\nMisc\n\nAlso see\n\nMarketing &gt;&gt; Workflow &gt;&gt; Churn examples\nDiagnostics, Classification &gt;&gt; Scoring &gt;&gt; Comparison of similar scoring models & Custom Cost Functions\nGoogle, Analytics &gt;&gt; Explore &gt;&gt; User Lifetime & Segment overlap\nProduct Development &gt;&gt;\n\nWhy do leave and stay?\nMetrics &gt;&gt; Growth Metrics\n\nAlgorithms, Product &gt;&gt; Retention Analysis\n\n\nIt usually costs more to acquire a customer than it does to retain a customer. Focusing on customer retention enables companies to maximize customer revenue over their lifetime.\nPotential targets\n\nCancellation of last product, no transactions in the last three months\nAlso contractual (e.g., bank) and non-contractual (e.g., e-shop) client relationships.\n\nPotential features\n\nBeware of look-ahead bias when selecting features\n\nIf youâ€™re trying to predict an event and your labels are determined with knowledge of future events, youâ€™re also introducing look-ahead bias.\nExample: if you label a customerâ€™s eventual outcome as â€œchurnedâ€ during a time period when they hadnâ€™t actually churned yet, then your model has access to future information in the training data.\n\nSocio-Demographic\n\nProducts Owned,\nHistorical Transactions\nClient-Company Interaction (i.e.Â active user?),\ne-Commerce Behaviour\n\n\nAlso important to be careful about how far in advance we want to estimate the propensity to leave. In other words, how long is the time between the day we look at clients through the available features and the day we can tell if they have left? If that time is too short, we wonâ€™t have much time to make any kind of response. If, on the other hand, it is too long, the model will be less accurate and up to date.\nCHAID (chi-squared automated interaction detection)\n\nold school, interpretable method\nBins continuous variables, integers coerced to factors, and seems to be a decision tree model where the splits are chosen by a chi-square test between each predictor and the churn outcome variable.\nNeed to distinguish between predictors that are nominal or ordinal\nThis article has other tutorial links and good visuals,Â https://ibecav.netlify.com/post/analyzing-churn-with-chaid/\n\n2 stage model approach\n\nDonâ€™t model who was most likely to leave, model who could best be persuaded to stayâ€”in other words, which customers considering jumping ship would be most likely to respond to a promotion.Â More efficient marketing spend\n\ni.e.Â swing customers - like politicians looking for swing voters because they are persuadable.\n\nIf you model the wrong objective, you can squander money on swaths of customers who were going to defect anyway and underinvest in customers they should have doubled down on.\n\nThink this has to be 2 stages\n\nFilter data before promotions over some window â€“&gt; model traditional churn â€“&gt; filter data after promotion â€“&gt; label which likely churns left and which ones didnâ€™t â€“&gt; model churn with new churn labels using data after promotion because you want probability of churn given promotion\n\nSo youâ€™d have 2 models: 1 to identify churners and 1 to identify swing customers from churners\n\nCan we determine which types of incentive would work best with each type of customer?\n\n\n\nSurvival Model\n\nPredict probability of leaving (i.e.Â churn) at several time points over the next months\nAllows you to anticipate and prioritize your marketing actions more effectively in time and, ultimately, reduce the churn rate.\nExample: Given a customer has contacted support, predict the probability of unsubscribing over time. (article)\n\nData\n\nthe interaction: the date, the reason for the call (sign-up/support), and the channel (email/phone).\nthe customer: age and gender.\nsubscription: product, price, billing cycle (monthly/annual), sign-up\n\nFeature engineering: the number of times the customer has contacted the company in the past, the duration since the customer subscribed, and cyclical date-related features.\nK-M survival curve\n\nCustomers with a monthly subscription are more volatile, and they tend to churn more often and faster during the first years after their subscription\n\nCox proportional hazards\n\nProportional Hazards Assumption (for churn):\n\nIf a customer has a risk of churn at an initial observation that is twice as low as another customer, then for all subsequent time observations, the risk of churn remains twice as low.\n\nSurvival Function for 5 randomly selected customers\n\nCustomer 2 is most likely to churn in the first few days, while customers 1, 3, and 4 are at much less risk\n\nCumulative Hazards Function for 5 randomly selected customers\n\nAgrees with results of the survival function\n\nCoefficients\n\nML methods\n\nAlso see Regression, Survival &gt;&gt; ML\nUsing Dynamic AUC as the metric makes it possible to evaluate each model only on the time points that are most important in the context\n\nSee Regression, Survival &gt;&gt; Diagnostics\n\nResults on the validation set\n\nDynamic AUC scores over the first 2 years (see other charts)\nNot really much difference between the RF, GBM, and Cox PH models after about 200 days, but the GBM had the best average AUC over the 2-year period of around 0.80\n\n\n\n\nDiagnostics\n\nsensitivity/recall (how many of the clients who actually leave were detected by the model)\n\nin churn prediction we usually have higher costs on false negatives\n\nprecision (how many of the clients identified by the model actually left)"
  },
  {
    "objectID": "qmd/algorithms-ml.html#sec-alg-ml-discrim",
    "href": "qmd/algorithms-ml.html#sec-alg-ml-discrim",
    "title": "1Â  ML",
    "section": "Discriminant Analysis",
    "text": "Discriminant Analysis\n\nMisc\n\nThe features to train Quadratic Discriminant Analysis (QDA) should be strictly normally distributed, making it easy for QDA to calculate and fit an ellipsoid shape around the distribution\nVery fast even on a 1M row dataset\n\nLinear Discriminant Analysis (LDA)\n\nNotes from StatQuest: Linear Discriminant Analysis (LDA) clearly explained video\nGoal is find an axis (2 dim) for a binary outcome or plane (3 dim) for a 3-category outcome, etc. that separates the predictor data which is grouped by the outcome categories.\nHow well the groups are separated is determined by projecting the points on this lower dim object (e.g.Â axis, plane, etc.) and looking at these criteria:\n\nDistance (d) between the means of covariates (by outcome group) should be maximized\nScatter (s2) ,i.e.Â variation, of data points per covariate (by outcome group) should be minimized\n\nMaximizing the ratio of Distance to Scatter determines the GoF of the separator\n\n\nFigure shows a example of a 2 dim predictor dataset that been projected onto a 1 dim axis. Dots are colored according to a binary outcome (green/red)\nIn the binary case, the difference between the means is the distance, d.\n\nFor multinomial outcomes, there are a couple differences:\n\nA centroid between all the predictor data is chosen, and centroids within each category of the grouped predictor data are chosen. For each category, d is the distance between the group centroid and the overall centroid\n\n\nThe chosen group predictor centroids are determined by maximizing the distance-scatter ratio\n\nUsing the coordinates of the chosen group predictor centroids, a plane is determined.\n\n\nFor a 3 category outcome, 2 axes (i.e.Â a plane) are determined which will optimally separate the outcome categories\n\n\nBy looking at which predictors are most correlated with the separator(s), you can determine which predictors are most important in the discrimination between the outcome categories.\nThe separation can be visualized by using charting the data according to the separators\n\n\nExample shows the data is less overlap between black and blue dots and therefore grouped better using LDA than PCA."
  },
  {
    "objectID": "qmd/algorithms-ml.html#sec-alg-ml-svm",
    "href": "qmd/algorithms-ml.html#sec-alg-ml-svm",
    "title": "1Â  ML",
    "section": "Support Vector Machines",
    "text": "Support Vector Machines\n\n\nMisc\n\nPackages: {e1071}, {kernlab}, {LiblineaR}, {{sklearn}}\nAlso see\n\nModel Building, tidymodels &gt;&gt; Model Specification &gt;&gt; Support Vector Machines\nModel Building, sklearn &gt;&gt; Misc &gt;&gt; Tuning\n\n\nProcess\n\nWorks on the principle that you can linearly separate a set of points from another set of point simply by transforming the dataset from dimension n to dimension n + 1.\n\nThe transformation is made by a feature transformation function, Ï†(x). For two dimensions, a particular Ï†(x) might transform the vector, x = {xâ‚, xâ‚‚}, which is in 2 dimensions, to {xÂ²â‚, âˆš(2xâ‚xâ‚‚), xÂ²â‚‚}, which is 3 dimensions\n\nTransforming a set of vectors into a higher dimension, performing a mathematical operation (e.g.Â dot product), and transforming the vectors back to lower dimension is involves many steps and therefore is computationally expensive.\n\nThe problem can become computationally intractable fairly quickly. Kernels are able perform these operations in much fewer steps.\n\nCreates a hyperplane at a threshold that is equidistant between classes of the target variable\nEdge observations are called Support Vectors and the distance between them and the threshold is called the Maximum Margin\n\nKernels\n\nGaussian Kernel\n\\[\nK(x,y) = e^{-\\gamma \\lVert x - y \\rVert^2}\n\\]\n\nK(x,y) performs the dot product in the higher dimensional space without having to first transform the vectors\n\n\nHyperparameters\n\ngamma â€“ All the kernels except the linear one require the gamma parameter. ({e1071} default: 1/(data dimension)\ncoef0 â€“ Parameter needed for kernels of type polynomial and sigmoid ({e1071} default: 0).\ncost â€“ The cost of constraints violation ({e1071} default: 1)â€”it is the â€˜Câ€™-constant of the regularization term in the Lagrange formulation.\n\nC = 1/Î» (R) or 1/Î± (sklearn)\nWhen C is small, the regularization is strong, so the slope will be small\n\ndegree - Degree of the polynomial kernel function ({e1071} default: 3)\nepsilon - Needed for insensitive loss function (see Regression below) ({e1071} default: 0.1)\n\nWhen the value of epsilon is small, the model is robust to the outliers.\nWhen the value of epsilon is large, it will take outliers into account.\n\nnu - For {e1071}, needed for types: nu-classification, nu-regression, and one-classification\n\nRegression\n\n\nStochastic Gradient Descent is used in order minimize MAE loss\n\nAlso see\n\nModel building, sklearn &gt;&gt; Stochaistic Gradient Descent (SGD)\nLoss Functions &gt;&gt; Misc &gt;&gt; Mean Absolute Error (MAE)\n\n\nEpsilon Insensitive Loss - The idea is to use an â€œinsensitive tubeâ€ where errors less than epsilon are ignored. For errors &gt; epsilon, the function is linear.\n\nEpsilon defines the â€œwidthâ€ of the tube.\nSee Loss Functions &gt;&gt; Huber loss for something similar\nSquared Epsilon Insensitive loss is the same but becomes squared loss past a tolerance of epsilon\n\nL2 typically used the penalty\nIn SVM for classification, â€œmargin maximizationâ€ is the focus which is equivalent to the coefficient minimization with a L2 norm. For SVR, usually the focus is on â€œepsilon insensitive.â€\n\nVisualization\n\nDecision Boundary\n\nExample\n\n# Create a grid of points for prediction\nx1_grid &lt;- seq(min(data$x1), max(data$x1), length.out = 100)\nx2_grid &lt;- seq(min(data$x2), max(data$x2), length.out = 100)\ngrid &lt;- expand.grid(x1 = x1_grid, x2 = x2_grid)\n\npredicted_labels &lt;- predict(svm_model, newdata = grid)\n\nplot(data$x1, data$x2, col = factor(data$label), pch = 19, main = \"SVM Decision Boundary\")\npoints(grid$x1, grid$x2, col = factor(predicted_labels), pch = \".\", cex = 1.5)\nlegend(\"topright\", legend = levels(data$label), col = c(\"red\", \"blue\"), pch = 19)\n\n(Legend colors in the wrong order)\nx1_grid and x2_grid provide equally spaced points within the range of sample data.\ngrid is a df of all combinations of these points\nThe predicted labels from [grid] are colored and visualize the decision boundary.\n{e1071} provides a plot function that also does this: plot(svm_model, data = data, color.palette = heat.colors)"
  },
  {
    "objectID": "qmd/algorithms-ml.html#sec-alg-ml-trees",
    "href": "qmd/algorithms-ml.html#sec-alg-ml-trees",
    "title": "1Â  ML",
    "section": "1.1 Trees",
    "text": "1.1 Trees\n\nAlgorithmic models that recursively split the data into smaller and more homogeneous subgroups. Predictions are the same for every member of the subgroup (aka piece-wise constant). Forests smooth out the piecewise predictions by averaging over groups of trees.\nHow Tree models get probabilities\n\nMethod 1: Each tree predicts the class of x according to the leaf node x falls within. The leaf node output is the majority class of the training points it contains. The predictions of all trees are considered as votes, and the class with the most votes is taken as the output of the forest. This is the original formulation of random forests proposed by Breiman (2001).\nMethod 2: Each tree outputs a vector [p1,â€¦,pk], where k is the number of classes, representing the predicted probability of each class given x. This may be estimated as the relative class frequencies of training points in the leaf node x falls within. The forest output is the average of these vectors across trees, representing a conditional distribution over classes given x.\nExample: Data point falls into a treeâ€™s leaf where yellow is the predicted class (2nd child split) \n\nFor this tree in the ensemble, there are 3 yellow and 1 green in the terminal leaf (2nd child split). Therefore the probability of Yellow is 75%.\nFor each class, the trees that predict that class have their probabilites averaged to produced the predicted probability for that class.\n\n\n\n\n1.1.1 Decision Trees\n\nClassification\n\nShowing entropy but misclassification error or the gini index can be used.\nCalculate Shannon Entropy of dependent variable (Y):\n where P(Y) is the marginal probability\n\nFor a binary variable, this would be\n\nIn general the Shannon Entropy equation is\n where p is a probability and c is the number of classes for the variable and S = subset of data or the node.\nProbabilities are between (0,1) and taking a log of numbers in this interval produces a negative value. Hence, the negative at the beginning of the expression.\nIf the natural log, ln, is used then itâ€™s called deviance\n\nCalculate the entropy of the target, Y, with respect toÂ each independent variable, x. For variable,\n\n\n, with number of classes, c :\n\nI do NOT like the way the equation is written above. In videos, this type of entropy isnâ€™t given a name, but I think it matches conditional entropy in its description and calculation.\n\nConditional Entropy (for a particular x),\n\n\nThis definition uses\n\n\nWhere H is used to as the symbol for entropy.\n\n\n\n\n\nThe marginal probability for that class of that variable, i.e.Â ratio of instances of that class in the entire dataset.\n\nExample: \n\nNot explicitly shown above, but for the entropy calculations, it uses the sum of the rows as the denominator in probability calculations. This fits with a â€œconditionalâ€ type of entropy.\n\n\nCalculate information gain for variable  \n\nRepeat for all independent variables\n\nSelect the independent variable with the largest gain for first split (\n\nFirst split, i.e.Â root node, is the most influential variable\n\nIf categorical variable chosen, leaves are all levels of that variable\n\nSubset dataset by var == level (for each branch\nRepeat entropy and information gain calculations on the subsetted data set\n\nBranches with entropy &gt; 1 are split unless some other stopping criteria is reached\n\nChoose variable with largest information gain and split by that variable\nKeeping repeating until maxdepth reached or minimum node size (number of rows in subset) reached\n\nNumerical vars are binned and treated like categorical vars\nPredicted class is the mode of the classes in the appropriate terminal node\n\nRegression\n\nFor each predictor var, choose a separator value, s\n\ne.g var1 &gt; 5 and var1 &lt;= 5 where s = 5\n\nCalculate the mean y value for both regions then calculate the MSE ((obs - mean)^2) of both regions. Sum of both MSEs. The optimal separator produces the lowest sum MSE.\nWhichever predictor has lowest sum MSE is chosen as the split variable.\nRecursively repeat. For example, repeat on region where var1 &gt;5 and repeat on region where var1 &lt;= 5.\nContinue until max.depth, max splits reached or data points in created region is less than a minimum or MSEs being calculated are all greater than a chosen amount, orâ€¦ etc. (Hyperparameters)\nPrediction is the mean in the appropriate terminal node\n\n\n\n\n1.1.2 Random Forest\n\n\nSeveral independent decision trees are fitted. Each tree just gets a part of the variable and then splits the outcome space according to the features in X\nWhen we â€˜â€™drop downâ€™â€™ a new point x, it will end up in a leaf for each tree. A leaf is a set with observations i and taking the average over all yi in that leaf gives the prediction for one tree. These predictions are then averaged to give the final result. Thus, for a given x if you want to predict the conditional mean of Y given that x, you:\n\nâ€œDrop downâ€ the x each tree (this is indicated in red in the above figure). Since the splitting rules were made on X, your new point x will safely land somewhere in a leaf node.\nFor each tree you average the responses yi in that leaf to get an estimate of the conditional mean of each tree.\nYou average each conditional mean over the trees to get the final prediction.\n\nAveraging the prediction of all trees leads to a marked reduction in variance.\nMissing Predictor Data\n\nSee StatQuest: Random Forests Part 2: Missing data and clustering video for more details\nProcess: Classification model\n\nMissingness is in the training data\n\nChoose intial values for the missing data\n\nLooks at that predictorâ€™s values that have the same outcome value as the observation with the missing data\n\nCategorical: For example, if the row has an observed outcome of 1 (i.e.Â event), then it will look at that predictorâ€™s values with outcome = 1 and choose the most popular category for the missing value\nNumeric: same as categorical, except the median value of predictor is chosen for the missing value\n\n\nCreate a â€œProximity Matrixâ€ to determine which observation is most similar to observation with the missing data\n\nThe matrix values are counts of how many times each row ends up in the node as the missing data row across all the trees in the forest\nThe counts are then divided by the number of trees in the forest\n\nCategorical: Weights for each category are calculated (see video). These weights are multiplied times the observed frequency of the category in the training data. The category with the highest weighted frequency becomes the new value for the missing data.\nNumerical: The weights are used to calculate a weighted average. So, weight * median is the new value for the missing data\nProcess is repeated until the values donâ€™t change within a tolerance\n\nMissingness in the out-of-sample data\n\nA copy of the observation with the missingness is made for each outcome category.\nThe proximity matrix procedure is done for each copy\nThen a prediction for each copy with itâ€™s new value is made in each tree of the forest. (of course the label for each copy has now been stripped)\nWhichever copy had itâ€™s (stripped) outcome label predicted correctly by the most trees wins and that label is prediction for that observation\n\n\n\n\n\n\n1.1.3 Isolation Forests\n\nUsed for anomaly detection. Algorithm related to binary search.\nNotes from paper: https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/icdm08b.pdf\nAlso see Anomaly Detection &gt;&gt; Isolation Forests\nThe tree algorithm chooses a predictor at random for the root node. Then randomly chooses either the minimum or the maximum of that variable as the splitting value. The algorithm recursively subsamples like normal trees (choosing variables and split points in the same manner) until each terminal node has one data point or replicates of the same data point or preset maximum tree height is reached. Across the trees of a forest, anomalies with have a shorter average path length from root to terminal node.\n\nThe algorithm is basically looking for observations with combinations of variables that have extreme values. The process of continually splitting subsamples of data will run out data points and be reduced to a single observation more quickly for an anomalous observation than a common observation.\nMakes sense. Picturing a tree structure, there shouldnâ€™t be too many observations with more that a few minimums/maximums of variable values. The algorithm weeds out these observations as it moves down the tree structure.\n\nAny or all of these wouldnâ€™t necessarily be global minimum/maximums since weâ€™re dealing with subsamples of variable values as we move down the tree.\n\nPaper has some nice text boxes with pseudocode that goes through the steps of the algorithm.\n\nAnomaly scores range from 0 to 1. Observations with a shorter average path length will have a larger score.\n\nAnomaly score,\n\n\nWhere E(h(xi)) is the average path length across the isolation forest for that observation\n\n\n\nWhere H(i) is the Harmonic number,\n\n\nGuidelines\n\nThe closer an observationâ€™s score is to 1 the more likely that it is an anomaly\nThe closer to zero, the more likely the observation isnâ€™t an anomaly.\nObservations with scores around 0.5 means that the algorithm canâ€™t find a distinction.\n\n\n\n\n\n1.1.4 Distributional Trees/Forests\n\n\nBlends the distributional modeling of gamlss (additive, nonlinear,Â  location, scale, shape) and the abrupt-change detection, additive + multiplicative effects capability, inclusion of interaction effects of decision trees / random forests. For regression trees, estimating all the distributional parameters instead of just the mean makes calculating the uncertainty easier.\n\nCART trees donâ€™t have aÂ concept of statistical signiï¬cance, and so cannot distinguish between a signiï¬cant and an insigniï¬cant improvement in the information measure.\nCART tree predictions areÂ piecewise-constant (every observation in the node has the same prediction), it will not be accurate unless the tree is large. But a large tree is harder to interpret than a small one.\nLinear trends are difficult for trees with piecewise-constant predictions\n\nAlgorithm splits based on changes in the mean and higher moments. So able to capture things like changes in variance.\nConditional distributions allow for the computation of prediction intervals.\nThis framework embeds recursive partitioning into statistical model estimation and variable selection\nThe statistical formulation of the algorithm ensures the validity of interpretations drawn from the resulting model\n{partykit}\n\nNotes from: https://arxiv.org/pdf/1804.02921.pdf\ntl;dr procedure\n\nFor each distributional parameter (e.g.Â mean, sd), a â€œscoreâ€ matrix is computed using the target values and the distributionâ€™s likelihood function\nThe score matrix is used to create a test statistic for each predictor\nThe predictor with the lowest p-value associated with its test statistic is the splitting variable\nThe split point is determined by the point that produces the lowest p-value in one of the split regions\nProcess continues for each leaf until no variables produce a p-value below a certain threshold (e.g.Â Î± = 0.05)\nThe distributional parameters associated with the leaf that a new observation falls into is used as the prediction for a tree.\n\nTree procedure\n\nFor each distributional parameter (e.g.Â mean, std.dev), calculate the value of the maximum likelihood estimator (MLE)\nTake the derivative of the log-likelihood function. Plug in the value(s) of the MLE parameter(s) and a yi value to get a â€œscoreâ€ for every value of the response variable. Repeat for each distributional parameter. (The score should fluctuate around zero.)\n\n\nand\n\n\nWhere k is the number of distributional parameters and n is the number of training observations\n\nGaussian example with\n :\n\nSolve for the MLE of the mean and calculate the value:\n\nSolve for the MLE of the variance and calculate the value:\n\nWeâ€™re calculating a score for each value of the outcome variable so we can remove the summation symbol from the derivative of the log-likelihood function w.r.t. the mean. This leaves us with the mean score function:\n\nSame thing but with the derivative of the log-likelihood function w.r.t. the variance:\n\n\n\nNull Hypothesis test each predictor variable vs the parameter score matrix where H0 = independence â€” Two methods: CTree and MOB\n\nCTree is permutation test based\n\nEach test statistic vector, T, for 1,â€¦,l predictors and n observations is calculated by:\n\n\nwhere\n\n\nis the 1xk row of the score matrix and v is a transformation function that depends on whether the predictor variable, Z, is a numeric or character type.\n\n\n\nIf the predictor variable, Z, is a numeric:\n\nv is an identity function, so Z remains unchanged.\nCorresponding to the first observation, the first row of the score matrix is multiplied by the first value of the predictor variable resulting in a 1xk row vector.\nn 1xk row vectors are added together\nThe summed 1xk vector is transposed by the vec function into the k-vector, T.\n\nIf the predictor variable, Z, is a character variable with H categories:\n\nv creates an indicator variable where the hth value is 1 indicates that Ziâ€™s value is the hth category.\nCorresponding to the first observation, we multiply this Hx1 vector times the 1xk, first row of the score matrix which results in a sparse Hxk matrix.\nn Hxk matrices are added together\nvec then stacks each column of the summed Hxk matrix to create a column vector, T, with H*k rows.\n\n\nT is standardized by maximum or quadratic method.\n\nt just represents a statistic thatâ€™s calculated from a permutation of the scores. T is handled in the same way.\npartykit::ctree.pdf shows the calculations for Î¼ and Î£\n\n\nwhere Î£+ is the pseudo-inverse of the covariance matrix\n\nCalculating the pseudo-inverse makes this method more computationally intensive\n\nUsing this quadratic method, c is Chi-Square test statistic.\n\n\n\nFor the maximum method, c is Normal test statistic (partykit::ctree.pdf)\nno idea why the numerator has one â€œkâ€ and the bottom has â€œkk.â€ Maybe itâ€™s a typo.\n\n\nFind the p-value associated with each predictorâ€™s c statistic\n\n\n\nThis says the p-value, P, is the probability,\n\n\n, (associated with the null hypothesis for this particular variable)Â that the standardized T stat is as or more extreme than the group of standardized t stats of the permuted scores.\n\n\n\nis the symmetric group of permutations and weights. Weights being either 0 or 1 depending on whether the observation is present in that nodeâ€™s data subset.\n\n\n\n\nMOB stands for model based method\n\nUses a M-Fluctuation test to test for an â€œinstabilityâ€ by calculating a supLM test statistic. An instability is whatâ€™s interpreted from a p-value &lt; 0.05\n\n\n\n\nis a minimum amount of scores that you choose, then\n\nNo guidelines for\n\n\nIn addition to choosing a minimum, the paper does mention also trimming node data points at each end by 10%.\n\n\n\nis called â€œits variance function.â€\n\n\n\n\n\n\nmeans floor of nt, which means round down to the integer.\n\nThe subscript\n\n\nsays the scores are ordered from highest to lowest (aka anti-rank) according to the predictor variable, Zj, values\n\n\ncovariance matrix\n\n\nThe distribution from which the p-value is calculated has something to do with a Bessel process and stuff converging to a Brownian Bridge, so I decided to shut it down here.\n\nSee https://eeecon.uibk.ac.at/~zeileis/papers/Zeileis%2BHothorn%2BHornik-2008.pdf for details\n\n\n\nBonferonni adjust the p-value according the number of variables, m\n\n\n\nSelect predictor variables with adjusted p-values lower than the threshold\nFrom that selection, the predictor with the lowest p-value is chosen as the splitting variable.\n\nIf no p-values are lower than the threshold, splitting is halted and the terminal node is reached for that branch (aka pre-pruning).\nPre-pruning not usually done in distributional forests (mincriterion = 0).\n\nChoose the optimal split point for the chosen variable where the lowest p-value is produced in one of the two created sub-regions (i.e the maximum test statistic).\nProcedure is repeated (like traditional trees) in the created leaves and continues until stopping criteria reached (e.g.Â no p-values lower than threshold, number of observations in node is below minimum, etc)\nIn practice, predicting, using just a tree, involves finding the node with the criterion that fits the new observation and using the estimated distributional parameters of the subsample belonging to that node as the model prediction.\n\nThe paper says this can be thought of as a weighted maximum likelihood estimation. The mathematical notation is similar to what I show below for forests.\n\n\nForest procedure\n\nThe idea of random forests is to train an ensemble of trees, each on diï¬€erent training data obtained through resampling or subsampling. In each node only a random subset of the covariates is considered for splitting to reduce the correlation among the trees and to stabilize the variance of the model\nNotes from: https://arxiv.org/pdf/1701.02110.pdf\nPretty good explainer of the weight system used below to calculate predicted means across all leaves in the forest (from {drf} explainer)\n\nInstead of directly calculating the mean in a leaf node, one calculates the weights that are implicitly used when doing the mean calculation. The weight wi(x) is a function of (1) the test point x and (2) an observation i. That is, if we drop x down a tree, we observe in which leaf it ends up. All observations that are in that leaf get a 1, all others 0.\n\nSo if we end up in a leaf with observations (1,3,10), then the weight of observations 1,3,10 for that tree is 1, while all other observations get 0.\n\nWe then further divide that weight by the number of elements in the leaf node.\n\nIn the example before, we had 3 observations in the leaf node, so the weights for observations 1,3, 10 are 1/3 each, while all other observations still get a weight of 0 in this tree. Averaging these weights over all trees gives the final weight wi(x).\n\nCalculating the mean as we do in a traditional Random Forest, is the same as summing up wi(x)*yi\n\nFor predictions:\n\nFor a training observation and a tree, determine whether a new observation zÂ belongs in the same node as the training observation, zi.\nCalculate a weight according to whether the new observation and the training observation are in the same node.\n\nIf they are in the same node\n\n where i denotes the training observation and n is the number of observations in that node of that particular tree, t.\n\nIf zi, the training observation, and the new observation, z, arenâ€™t in the same node\n\nwit = 0\n\n\nCalculate wit for each tree the training observation belongs to.\n\nResampling or subsampling may exclude a training observation from some of the trees\n\nSum all tree weights, wit, for that training observation and divide by the number of trees to get the forest-weight for that training observation.\n\n\n\nWhere Ti is the total trees that use that training observation in its learning sample.\nSo the forest weight is the average weight per tree for that training observation\n\nIn the paper, this process is described by a more compact notation:\n\n\nThe numerator in the end part of this equation indicates whether the bth terminal node, Bb, contains the training observation, zi, and the new observation, z, for tree, t.\n is the number of observations in that terminal node\n\n\nThe paper describes predictions being calculated by a weighted MLE as it did for a single tree, but for forests it didnâ€™t explicitly give an â€œin practiceâ€ description of process.\n\n\n\nEach parameter that has been calculated for each terminal node of each tree has a subset of the learning data associated with it. The forest weight-likelihood products of each observation are summed over this subset. The parameter with the largest sum is chosen as the prediction.\n\nRepeat for each distributional parameter.\n\n\n\n\n\n\n{drf}\n\n\nNotes from DRF: A Random Forest for (almost) everything\nUltimately when a RF finishes splitting data, each leaf should ideallly contain a homogeneous set of points in terms of approximating a the conditional distribution, P(Y|X=xi), but this only applies to the conditional mean of that leaf. As seen in the pic, the mean doesnâ€™t fully describe that leafâ€™s conditional distribution.\n\nEvery distribution except x2 has a similar means, but sets (x1,x4,x6) and (x3, x5, x7) have different variances.\n\ndrf is able to fit a RF with these more homogeneous leaves by transforming the leafâ€™s yi subsamples into a Reproducing Kernel Hilbert Space with a kernel.\n\nIn this infinite-dimensional space, conditional means are able to fully represent conditional distributions and the Maximum Mean Discrepancy (MMD) is (efficiently) calculated.\nThe MMD measures the similarity between distributions\nThus if the conditional distribution of Y given xi and xj are similar, they will be grouped in the same leaf.\n\ndrf uses the same weighting system for its forest as {partykit} in order to produce predictions."
  },
  {
    "objectID": "qmd/algorithms-ml.html#sec-alg-ml-boost",
    "href": "qmd/algorithms-ml.html#sec-alg-ml-boost",
    "title": "1Â  ML",
    "section": "1.2 Boosting",
    "text": "1.2 Boosting\n\nfrom https://www.economist.com/graphic-detail/2021/03/11/how-we-built-our-covid-19-risk-estimator\n\nIn order to capture such complexity, we needed to allow for the possibility that comorbidities do not have constant effects that can simply be added together, but instead interact with each other, producing overall risk levels that are either higher or lower than the sum of their parts.\n\nSays main effects werenâ€™t good enough and needed to use interactions\n\nGradient-boosted trees make predictions by constructing a series of â€œdecision treesâ€, one after the other. The first tree might begin by checking if a patient has hypertension, and then if they are older or younger than 65. It might find that people over 65 with hypertension often have a fatal outcome. If so, then whenever those conditions are met, it will move predictions in this direction. The next tree then seeks to improve on the prediction produced by the previous one. Relationships between variables are discovered or refined with each tree.\n\nDifference between XGBoost and LightGBM (Raschka)\n\nXGBoostâ€™s trees are based on breadth-first search, comparing different features at each node.\nLightGBM performs depth-first search, focusing on a single feature & growing the tree from there.\n\n\n\n1.2.1 Gradient boosted machines (GBM)\n\nChoose a differentiable loss function, Ï, such as \n\nIn gradient boosting, 1/n is exchanged for 1/2, to make it differentiable. The mean of the loss function, SSE in this case, calculated over all observations for a model is called the â€œempirical riskâ€ which is what boosting is trying to minimize.\n\nCalculate the negative gradient, aka first derivative. For regression trees, this turns out to be\n\n\nwhich is just the residuals. Pg 360 of The Elements of Statistical Learning has other loss functions and their negative gradients. Classification uses a multinomial deviance loss function.\n\nInitialize using the optimal constant model, which is just a single terminal node tree. Think this means the initial predictions are just mean of the target,\n\nCalculate the negative gradient vector (residuals), r, by plugging in the predicted values.\nFit a regression tree with the residuals, r, as the target variable. The mean of the residuals for that region (terminal node) is the prediction,\n .\nAdd the predicted residuals vector to the initial predictions vector,\n\n\nto get the next set of predictions to feed into the negative gradient equation.\n\nRepeat steps 4 - 6 until some stopping criteria is met.\n\n\n\n1.2.2 LightGBM\n\nFind optimal split points using a histogram based algorithm\n\nGOSS (Gradient Based One Side Sampling)\n\nretains instances with large gradients while performing random sampling on instances with small gradients.\n\nExample: Gaussian Regression - observations with small residuals are downsampled by random selection while those observations with large residuals remain\n\n\n\nEFB (Exclusive Feature Bundling)\n\nReduce feature space by bundling features together that are â€œmutually exclusiveâ€ (i.e.Â varA doesnâ€™t take a value of 0 in the same observation as varB).\n\ni.e.Â Bundles sparse features together\n\nCreate bundles and assign features\n\nConstruct a graph with weighted (measure of conflict between features) edges. Conflict is measure of the fraction of exclusive features which have overlapping non zero values.\nSort the features by count of non zero instances in descending order.\nLoop over the ordered list of features and assign the feature to an existing bundle (if conflict &lt; threshold) or create a new bundle (if conflict &gt; threshold).\n\nMerging\n\nArticle wasnâ€™t coherent on this precedure\n\n\n\n\n\n1.2.3 XGBoost\n\nFYI has various gradient function families: binomial, poisson, tweedie, softmax (multi-category classification)\nUtilizes histogram-based algorithm for finding optimal split points.\n\nBuckets continuous features into discrete bins to construct feature histograms during training. It costs O(#data * #feature) for histogram building and O(#bin * #feature) for split point finding.\n\nRegularization: It penalizes more complex models through both LASSO (L1) and Ridge (L2) regularization to prevent overfitting.\n\nRegularization function\n\n\nThis function gets minimized during training\nT is the total number of trees\nw is a leaf weight\nTuning parameters\n\nÎ± controls how much we want to penalize the sum of the absolute value of leaf weights (L1 regularization)\nÎ» controls how much we want to penalize the sum of squared leaf weightsÂ  (L2 regularization)\nÎ³ is used to control how the number of trees in the model is penalized\n\n\n\nSparsity Awareness: XGBoost naturally admits sparse features for inputs by automatically â€˜learningâ€™ best missing value depending on training loss and handles different types of sparsity patterns in the data more efficiently.\nWeighted Quantile Sketch: XGBoost employs the distributed weighted Quantile Sketch algorithm to effectively find the optimal split points among weighted datasets.\nMultinomial: all the trees are constructed at the same time, using a vector objective function instead of a scalar one, i.e.Â there is an objective for each class. (i.e.Â Classifying n classes generate trees n times more complex)\n\nThe objective name is multi:softprob when using the integrated objective in XGBoost. Although, the aim is not really the softprob , but the log loss of the softmax. But softmax is not the gradient of softmax , but the gradient of its log loss\n\\[\n\\begin {align}\n\\mbox{soft}_\\mbox{max}(x_i) &= \\frac{e^{x_i}}{\\sum_j e^{x_j}}\\\\\n\\mbox{log}_\\mbox{loss}(x_i) &= -\\ln(\\mbox{soft}_\\mbox{max}(x_i)) = -ln(e^{x_i}) + \\ln\\left(\\sum_j e^{x_j}\\right) = -x_i + \\ln\\left(\\sum_j e^{x_j}\\right) \\\\\n\\frac{\\partial\\mbox{log}_\\mbox{loss}(x_i)}{\\partial x_i} &= \\frac{\\partial (-x_i + \\ln\\left(\\sum_j e^{x_j}\\right)}{\\partial x_i} = -1 + \\frac{e^{x_i}}{\\sum_j e^{x_j}} = \\mbox{soft}_\\mbox{max}(x_i)\n\\end {align}\n\\]\n\ni.e.Â the objective optimized is not softmax or softprob, but their log loss.\n\n\nCross-validation: The algorithm comes with built-in cross-validation method at each iteration, taking away the need to explicitly program this search and to specify the exact number of boosting iterations required in a single run.\nComponent-wise Boosting (From mboost_PKG tutorial docs)\n\ntl;dr - Same as GBM except instead of fitting a tree model to the residuals, youâ€™re fitting many types of small models (few predictors). Best small model updates the predictions after each iteration.\nThe goal is to minimize empirical risk\n\n\nwhere\n\n\nis the loss function and\n\n\nthe predictor function\n\nThe loss function is usually the negative log-likelihood function for the distribution of the outcome variable. For a Gaussian distribution, this is equivalent to the least squares objective function\n\nSteps\n\nCompute the negative gradient of the loss function which is the negative first derivative with respect to the predictor function,\n .\n\n\ncan be thought of as the vector of predictions at the mthÂ iteration of the algorithm. So to begin, we create an initial vector,\n\n\nwith â€œoffset values.â€\n\nFor glmboost, the offset is the mean of the outcome variable. Iâ€™d guess itâ€™s probably the same for gamboost.\n\nCompute the negative gradient vector:\n\n\nFor the first iteration, m = 1 and\n\n\nFit each baselearner to the negative gradient vector.\n\nA baselearner is like a subset statistical model of the overall statistical model. They can be linear regressors, penalized linear regressors, shallow trees, penalized splines, etc. Each one using one or more predictors with or without interaction terms.\n\nFor each baselearner, calculate the residual sum of squares,\n\n\nfrom itâ€™s predictions.\n\nWhichever baselearner has the smallest RSS, scale itâ€™s predictions with a learning rate factor and combine them with the previous iterationâ€™s predictions\n\n\n\nwhere\n\n\nis the learning rate.\nOptimization of Î½ isnâ€™t critical. Only required to be low enough as to not overshoot the minimum empirical risk, e.g.Â Î½ = 0.1.\n\n\nAfter the predictions are updated, steps 3-6 are repeated until the number of iterations, set by the value of the mstop, is reached.\n\nmstop is a hyperparameter that you optimize to prevent overfitting. The value can be chosen by cv or AIC\n\n\nEach baselearnerâ€™s contribution to the final prediction vector is\n\n\nover all iterations where that baselearner was selected\nI think this is the value of the\n\n\non the y-axis of the partial dependence plots (pdp).\nIf the variables have been centered (maybe need to be completely standardized), then the magnitude of the y-axis (and the variable range within it) can be used as a signifier of variable importance and variables can be compared that way.\nIf a variable has multiple baselearners selected, you can combine all the contributions and plot the combined effect pdp by predict(which = â€œvariableâ€), row summing the values, and plotting. See the end of mboost.pdf for details.\n\n\nComponent-wise boosting performs variable selection unlike some other boosting algorithms"
  },
  {
    "objectID": "qmd/algorithms-product.html#sec-alg-prod-misc",
    "href": "qmd/algorithms-product.html#sec-alg-prod-misc",
    "title": "2Â  Algorithms, Product",
    "section": "2.1 Misc",
    "text": "2.1 Misc\nFundamentally, in the context of for-profit businesses, almost any analytics problem can be cast as an optimization problem where some aspect of the business needs to be maximized or minimized. In most cases, it is some kind of **revenue maximization** or **cost** **minimization**.\n\nAlso see [Business Plots](Business Plots)"
  },
  {
    "objectID": "qmd/algorithms-product.html#sec-alg-prod-prcopt",
    "href": "qmd/algorithms-product.html#sec-alg-prod-prcopt",
    "title": "2Â  Algorithms, Product",
    "section": "2.2 Price Optimization",
    "text": "2.2 Price Optimization\n\nNotes from:\n\nhttps://www.statworx.com/ch/blog/food-for-regression-using-sales-data-to-identify-price-elasticity/\n\nresults in tables are jacked up\n\nhttps://www.statworx.com/de/blog/food-for-regression-mixing-in-cross-elasticities-and-promotional-effects/\nhttps://www.statworx.com/de/blog/combining-price-elasticities-and-sales-forecastings-for-sales-improvement/\n\nAllows pricing to be used as a tool to target KPIs\n\nForecasted sales are lower than expected. According to the price elasticity, which price will allow us to reach our sales target?\nForecasted sales are higher than expected. According to the price elasticity, which price will increase profit while staying inline with our sales target.\n\nQuestions:\n\nhow many unitss can we expect to sell at a given price?\nHow does a 10% price change affect demand?\n\nData\n\nVariables are time series.\nBasic variables:\n\nPrice\nSales or orders\n\nPotential variables\n\nCompetitor prices\n\nLogging this variable (as with your productâ€™s Price) usually produces the best results.\n\nIn-store alternatives\n\nPrice\nSales\n\nNew promotional activities\n\nDummy variable\n\nSeasonal effects\n\n\nPrice Elasticity of Demand (PED) - measures the responsiveness of the demand to changes in price\n\nAlmost always negative â€“ except for luxury goods (Veblen) and Giffen goods(?)\n\nA positive elasticity can indicate an omitted variable.\n\n\n Â means relatively inelastic where demand hardly changes in response to a change in price\n Â means relatively elastic where demand is responsive to price changes\n Â means unitary elastic where revenue is maximized and price is optimized\n\nCoefficients arenâ€™t decimal percents (e.g.Â the regression coefficient of 1% elasticity will be 1 not 0.01)\nCoefficient interpreted as a 1% increase in Price results in a &lt;Î²1&gt; percent change in Demand.\n\nCommon Models\n\n\n\nWhere Y = demand (e.g.Â sales, orders), and X is price\nLog-Log fits more closely with economic theory and is usually the optimal choice.\n\n\n\n\n\nThese costs can be per day, per month, etc.\n\n\n\nOptimal price doesnâ€™t depend on fixed costs\n\n\nCross Price ElasticityÂ -Â The responsiveness of demand for your product to changes in a competitorâ€™s price\n\nPositive values are common. Higher prices of a competitor (substitutional good) should result in increase sales for your product. Thus, a positive elasticity.\nNegative values can occur if the competitor raises prices on a complementary good, which is good that is often purchased along with your product.\n\nA rise in the price of oil filters many depress sales of engine oil.\n\n\n\nAdvanced Log-Log model:\n\nAdditionally includes Competitor Price and dummy variables for a Promotional Campaign and Summer months\n\n\nUse price elasticity of demand (PED) to find the price required to meet a sales target\n\nCobb-Douglas function\n\n\n\nQ(p) is quantity sold as a function of price, p\nA is forecasted sales\nÎµ is the PED\nQ0 is our sales target\nQ0 - Q(p) = 0 since we want our quantity sold to equal our sales target\n\n\nWe can use ARIMA, ETS, etc to forecast sales for the period that Q0 covers,Â model the price elasticity, and then calculate the price.\n\nUsed CIs for the PED to give a range of prices\n\nIâ€™d also incorporate the forecast CIs"
  },
  {
    "objectID": "qmd/algorithms-product.html#sec-alg-prod-cba",
    "href": "qmd/algorithms-product.html#sec-alg-prod-cba",
    "title": "2Â  Algorithms, Product",
    "section": "2.3 Cost Benefit Analysis (CBA)",
    "text": "2.3 Cost Benefit Analysis (CBA)\n\nNotes from:Â http://freerangestats.info/blog/2019/11/24/cost-benefit-analysis\nAlso see\n\nProject Management &gt;&gt; Decision Model\nVideo Bizsci learning lab 68\n\n{tidyquant} has excel functions for Net Present Value (NPV), Future Value of Cashflow (FV), and Present Value of Future Cashflow (PV)\nNet Present Value - the real value of benefits in todayâ€™s dollars minus the real value of costs (where the â€œrealâ€ value means future values are discounted by some chosen discount rate - the choice of which makes a big difference to how much the analysis values either the short term or the long term)\n\n\n\nm is the number of years (or whatever time unit) for the investment\nbenefit is the benefit value (e.g.Â revenue) for each year of the investment (vector)\ncost is the cost value for each year of the investment (vector)\nCost and benefit must have the same unit of measurement (e.g.Â dollars) and be vectors of equal length.\ndiscount is the discount rate (decimal)\n\nMinimum expected return on investment. 10% is common and 5% is usually the minimum, i.e.Â a replacement-level ROI. If the discount rate is 10% percent, then that says that the current investment strategies are yielding at least 10%, so this one should be at least that good. The thinking is, â€œwhy spend money on this investment if it has a lower discount rate instead of putting it to work in one of the other strategies thatâ€™s yielding at least 10%?â€\nExample references New Zealand and Australia governmental guidelines for various departments that range from 5-7%.\n\nNotice i starts at 0 and goes to m-1 (investment window remains unchanged). This is because the initial year is â€œdiscountedâ€ (whatever that means). The discount quantity in the equation goes to 1 when i = 0, so itâ€™s not a factor in the calculation for the initial year.\nBasic Simulation: Can use the Gamma Distribution to sample different random values for costs and benefits. Then plug into a npv function with a specified discount rate to get a simulated distribution of potential values. Visualize with a density graph.\n\nThis models the potential uncertainty with costs and benefits.\n\n\n\nInternal Rate of Return - the discount rate at which benefits break even with costs\n\nIf the internal rate of return is higher than your cost of capital (aka hurdle rate) and higher than the rate of return of other uses of capital, then it is worth investing in.\n\ncost of capital typically calculated as Weighted Average Cost of Capital (WACC)\n\nProject, Planning &gt;&gt; Decision Models &gt;&gt; Terms &gt;&gt; Cost of Capital, Internal Rate of Return\nMorgan Stanleyâ€™s Guide to Cost of Capital (Also in R &gt;&gt; Documents &gt;&gt; Business)\n\nThread about the guide\n\nWACC = w1*cost_of_equity + w2*cost_of_debt\n\nw1, w2 depend on the companyâ€™s capital structure (e.g.Â 70% equity, 30% debt)\nCAPM = Rf + Î² (Rm - Rf)\n\ncost_of_equity approximated using capital asset pricing model (CAPM)\nRf = risk-free rate of return\nRm = market rate of return\nÎ² = risk estimate or a companyâ€™s stock beta or a similar public companyâ€™s stock beta\n\ncost_of_debt = (interest_expense / total_debt) * (1 - T)\n\ninterest_expense = interest paid on companyâ€™s current debt\n\nSince interest expense is tax-deductible, the debt is calculated on an after-tax basis\n\nT = companyâ€™s marginal tax rate\n\n\nAs of January 2019, transportation in railroads has the highest cost of capital at 11.17%. The lowest cost of capital can be claimed by non-bank and insurance financial services companies at 2.79%. The cost of capital is also high among both biotech and pharmaceutical drug companies, steel manufacturers, Internet (software) companies, and integrated oil and gas companies.Those industries tend to require significant capital investment in research, development, equipment, and factories.\n\n\nCompute with an optimization function\n\nCan calculate for specific cost, benefit, and discount rate or use the simulated values (above) to get a density of potential values.\n\n\nMore Complicated Case - benefit and cost, can be broken down into parts with uncertainty included in each estimate\n\ncost = c(initial_cost, remaining_costs)\n\ninitial_cost = rnorm(n=1, mean = initial_cost_est, sd = initial_cost*uncertainty)\n\nuncertainty = 0.10 in the example\nâ€œ_estâ€ variables or constants are just the initial estimates (given to us from marketing or whoever the stakeholders are)\n\nremaining_costs = fixed_costs + ad_hoc_costs\n\nongoing_costs = rnorm(n=m-1, mean = fixed_costs,, sd = fixed_costs*uncertainty)\n\nm = investment window\n\nad_hoc_costs = rbinom(n=m-1, size = 1, p = ad_hoc_prob)\n\ncosts that occur that werenâ€™t planned for\n\n\n\nbenefits = customers * customer_spend\n\ncustomers =Â customer_level_shift * (1 + customer_growth) ^ (0 : m-1) # m-1 since starting at 0\n\ncustomer_level_shift = rnorm(1, customer_level_shif_est, customer_level_shift * uncertainty)\n\nlevel shift is described as the number of customers attained in year 0.\n\ncustomer_growth = rnorm(1, customer_growth_est, customer_growth_est * uncertainty)\n\ngrowth of the customer base after year 0\n\n\ncustomer_spend =Â rnorm(1, customer_spend_est, customer_spend_est * uncertainty)"
  },
  {
    "objectID": "qmd/algorithms-product.html#sec-alg-prod-cj",
    "href": "qmd/algorithms-product.html#sec-alg-prod-cj",
    "title": "2Â  Algorithms, Product",
    "section": "2.4 Customer Journey",
    "text": "2.4 Customer Journey\n\nMisc\n\nI think these notes are from a biz sci video\nAlso see\n\nMarketing &gt;&gt; Customer Journey\nProduct Development &gt;&gt; Conversion Funnel\n\nEDA on channel groups that lead to transactions for 1st purchasers, 2nd purchasers, etc. Use binary classification model to develop a strategy around predictor variables that increase or decrease the probability that a customer will perform a transaction\n\nProcess\n\nDefine the different scenarios to cover\nMake groups of experts so they can cover each scenario\nOver the user journey in our application, map each user behaviour to the events or data\nPrioritize per step and select the minimum events needed to explain the user behaviour\nCombine all the events for the data scientists to use as features for the segmentation or modelling\n\nGoogle store data - visitor_id, session_id (each row is a session), channel_group, date, total_transactions (# of transactions per session, na = no transaction)\nChannels:\n\norganic - google searching\ndirect - email\nreferral - hyperlink on website\ndisplay - ads on websites\n\npath splitting\n\ncust_idÂ \n\ngroup by cust, transaction, channel\n\n\nCleaning steps\n\ntotal trans -&gt; only 1 or 0 (trans or no trans(NAs))\ngroupby visitor\narrange by date\ncumsum trans to create var so 1st purch = 1, 2nd purch = 2, â€¦, etc\nlag(cumsumvar) to create a path\nmerge vistor and path to create a visitor_path_id\ncreate an indicator to show whether a path_id ended in trans or not\nfilter only paths that ended in transaction\ncount number of visitor_path_ids for each transaction number (1st purchase, â€¦) and remove transaction numbers with few visitor_path_ids\n\nViz\n\nball and stick: (plotly with buttons that select 1st purchase, 2nd purchase, â€¦ etc.)\n\nWhich channel types have the greatest proportion of transactions for 1st purchasers, 2nd purchasers, etc.?\ny axis: channel type\nx axis: percent ending in transaction\n\nheatmap: customer_transaction_path_id vs channel\n\nThink this answers the same question as the ball and stick\nyaxis: 1st purchase, 2nd purchase, â€¦ , last purchase,\nx-axis: channel type\ncells = percent that ended in transaction, color by percent\n\n\nModels (include visitor_path_ids that did NOT end in a transaction, make channels into indicator vars)\n\nbinary model (transaction = 1, no transaction = 0)\n\nuse model agnostic viz to examine high probability predictions that didnâ€™t convert.\nDevelop strategy around predictor variables that lead to that prediction (increase or decrease value) to nudge customer into transaction."
  },
  {
    "objectID": "qmd/algorithms-product.html#sec-alg-prod-convfun",
    "href": "qmd/algorithms-product.html#sec-alg-prod-convfun",
    "title": "2Â  Algorithms, Product",
    "section": "2.5 Conversion Funnel",
    "text": "2.5 Conversion Funnel\n\nNotes from a BizSci learning lab (like lab 24 or something)\n\nUses BigQuery and Google Analytics\n\nAlso see\n\nProduct Development &gt;&gt; Conversion Funnels\nMarketing &gt;&gt; Customer Journey\n\ngoogle analytics 360\n\nmore granularity than free version\nCan schedule to export daily (nested) table of data to BiqQuery\n\nJSON data (amazon redshift, azure use postgres)\n\n\nWhich steps in conversion path are most important\n\nweb page hits involved in a purchase per page\n\nbasket.html\n\n\nWhich step in the path are custumers getting hung up on?\nstores data from session which is the entry to exit of website\nvisitid is the session id\n\neach visitid has a hits col with a nested tibble which has all the pages the customer visited\n\na hit is a visit to a webpage (pagePath)\nBigquery allows free 1 Tb of data\nConnection to bigq\n\nconnect to bigquery::bq_auth\ndbconnect with billing arg = project name (from bigquery)\ndbplyr::tbl, select(visitid, hits), collect\npluck(â€œhitsâ€, 1, â€œpageâ€), tojson prettify fromjson unnest(cols = pagePath:pagPathLevelId)\n\nHow many unique visitors per day\n\ndbplyr::tbl (table_*), distinct, count, collect\nâ€œ*â€ is a wildcard. Processes code on all tables in project in bigq without bringing the data into R before performing the calc.\n\nTotal transactions over time period\n\ncanâ€™t use dbplyr for the lower levels of nested tables. Have to use sql\nSELECT date, SUM(totals.transactions) AS total_transactions\n\ntotals.transactions is in the form column.subcolumn for json structured tables\n\nFROM â€œbiquery-public-blahblahâ€\nGROUP BY data\nORDER BY total_transactions\n\nTransactions by referral\n\nSELECT trafficsource.medium then basically same query as above\n\nWhat paths are customers taking by session that lead to conversion\n\nJoin a couple tables by visitid and collect into R\n\ncols: fulluserid visitid hitnumber pagepath\ncols: visitid total_transaction_revenue (which is scaled to millions for some reason)\n\nTake joined table and\n\nfilter transaction_rev &gt;0 to get only transactions that lead to â€œconversionsâ€ (purchases)\ncount number of rows per pagePath\nsummarize median_hit_number, median_visit_number\nggplot(pagePath, n, fill = n), geom_bar, coord_flip\n\nadd incentives to customers to get to the top pages for those values"
  },
  {
    "objectID": "qmd/algorithms-product.html#sec-alg-prod-markbask",
    "href": "qmd/algorithms-product.html#sec-alg-prod-markbask",
    "title": "2Â  Algorithms, Product",
    "section": "2.6 Market Basket Analysis",
    "text": "2.6 Market Basket Analysis\n\nAssociation Rules\n\nKey metrics\n\nSupport = num_sku1_sold / num_transactions (What percent of transactions involved sku1 on average)\nConfidence = support(sku1, sku2) / support(sku1) (Of the transactions involving sku1, what percentage also involved sku2?)\n\ni.e.Â equivalent to the probability that someone will buy sku2 given they also bought sku1 (see Basket Complementary)\nsupport(sku1, sku2) = num_sku1_sku2_sold / num_transactions\n\nLift = support(sku1, sku2) / [support(sku1) * support(sku2)] (How likely were you to buy sku2 if you have already bought sku1? or vice versa)\n\nGuidelines\n\nLift &gt; 1 positive association between sku1 and sku2 (more likely to buy together)\nLift &lt; 1 negeative association between sku2 and sku2 (less likely to buy together)\n\n\nBasket Complementary (article)\n\nUsing confidences of pairs of products and the confidence of their reciprocal to rank cross-selling recommendations\nTransactions for a user listed on the left\nConf(Cornflakes â†’ Milk) is the confidence which is the probability that someone wil buy milk given theyâ€™ve also bought cornflakes\n\n= num_corn_milk_sold/num_corn_sold = 2 / 5 = 0.40 (equivalent to formula above in key metrics)\n\nConf(Milk â†’ Cornflakes) = num_corn_milk_sold/num_milk_sold = 2 / 6 = 0.33\nThe more similar the two probabilities are for each pair, the closer the point comes to the line of equality (the red dashed line that runs diagonally through the origin), and the more complementary the items become.\nItâ€™s rare that a dot will land exactly on the line of equality, so the green and orange lines (tolerances) parallel to the red line mark how far off a dot is from this ideal setting, using different levels of tolerance.\nLarge asymmetries indicate one-sided complementarity. Such imbalances will be quite common when, for instance, items of hugely different prices are involved.\n\nExample: When someone buys a house, for example, they may want to buy a bookcase, but buying a bookcase doesnâ€™t mean someone wants to buy a house: this would be an instance of one-sided complementarity.\nMe: So choosing recommendations should take into consideration the price of item in the cart and select one-sided asymmetries with high confidences that are similar to that price or less.\n\nTolerances categorize and rank pairs of products\n\nComplementary at such-and-such a tolerance level â€“ 0%, 1%, 5%, etc. â€“ generating a score of sorts.\nIn cases where a dot representing the two-way dependencies between two items falls within a narrow band â€“ corresponding to a smaller tolerance â€“ the more inseparable the items are, and the more sensible a cross-selling recommendation may become."
  },
  {
    "objectID": "qmd/algorithms-product.html#sec-alg-prod-cohort",
    "href": "qmd/algorithms-product.html#sec-alg-prod-cohort",
    "title": "2Â  Algorithms, Product",
    "section": "2.7 Cohort Analysis",
    "text": "2.7 Cohort Analysis\n\nVisualizing user retention is a useful way for e.g.Â retail and subscription businesses to keep track of how long customers and users tend to stay with the them and spot differences in how cohort sizes change over time.\nMisc\n\nCohort - group of new users who start an interaction (e.g.Â subscription, logging on to a website) on the same date\n{cohort} has some nice cleaning functions\nA grouped line chart would be another visualization option\n\nCompare cohorts by month\n\nTable showing cohort interactions over time\nExample\n\n\nonline_cohorts %&gt;%Â \nÂ  cohort_table_month(id_var = CustomerID, date = InvoiceDate)\n\nTakes tibble with a customer id column and invoice date column and creates a monthly cohort table\nCohort 1 all had their first purchase on Dec 2010 and only 622 customers from this group also made a purchase in Jan 2011\nWith this function, customers donâ€™t have make a purchase in Jan 2011 in order to be counted in Feb 2011\nAs a percent\n\nÂ  Â  Â  Â  Â  Â  online_cohorts %&gt;%Â \nÂ  Â  Â  Â  Â  Â  Â  cohort_table_month(id_var = CustomerID, date = InvoiceDate) %&gt;%\nÂ  Â  Â  Â  Â  Â  Â  cohort_table_pct()\n\n36.7% of the Dec 2010 cohort made a purchase in Feb 2011\nCompare cohorts by period\n\nonline_cohorts %&gt;%Â \nÂ  cohort_table_month(id_var = CustomerID,\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  dateÂ  = InvoiceDate) %&gt;%\nÂ  shift_left_pct()\n\nCohort1â€™s t0 is Dec 2010 and cohort2â€™s t0 is Jan 2011\nHeatmap\n\nConvert to long format\n\n\nonline_cohorts %&gt;%Â \nÂ  Â  Â  Â  Â  Â  Â  cohort_table_month(id_var = CustomerID, date = InvoiceDate) %&gt;%\nÂ  Â  Â  Â  Â  Â  Â  cohort_table_pct() %&gt;%\nÂ  Â  Â  Â  Â  Â  Â  pivot_longer(-cohort) %&gt;%\nÂ  Â  Â  Â  Â  Â  Â  filter(name != \"t0\") # these are all 100 so not interesting\n\nColumns: cohort, name (i.e.Â period), value (percentage)\nHeatmap table\n\nonline_cohorts %&gt;%\nÂ  mutate(time = as.numeric(str_remove(name, \"t\"))) %&gt;%\nÂ  ggplot(aes(time, reorder(cohort, desc(cohort)))) +\nÂ  geom_raster(aes(fill = log(value))) +\nÂ  coord_equal(ratio = 1) +\nÂ  geom_text(aes(label = glue::glue(\"{round(value,0){style='color: #990000'}[}]{style='color: #990000'}%\")),Â \nÂ  Â  Â  Â  Â  Â  Â  Â  sizeÂ  = 3,Â \nÂ  Â  Â  Â  Â  Â  Â  Â  colour = \"snow\") +\nÂ  scale_fill_gradient(guide = \"none\") +\nÂ  theme_minimal(base_size = 16) +\nÂ  theme(panel.gridÂ  = element_blank(),\nÂ  Â  Â  Â  panel.border = element_blank()) +\nÂ  labs(y = \"cohort\")"
  },
  {
    "objectID": "qmd/algorithms-product.html#sec-alg-prod-retent",
    "href": "qmd/algorithms-product.html#sec-alg-prod-retent",
    "title": "2Â  Algorithms, Product",
    "section": "2.8 Retention Analysis",
    "text": "2.8 Retention Analysis\n\nRetention Rate - measures the proportion of users that return to your product over some specified time\n\n4 choices to make before calculation the retention rate:\n\nRetention rate type\nUser type\nAction type\nCohorts\n\n\nTypes\n\nN-day (or week or month) Retention\n\nmeasures among users who first used the product at day 0, what proportion of them are still active at day N.\nN indicates the end point of the interval\n\nRolling Retention\n\nEnd point is the day you view the data and the choice of starting point determines the length of the interval\n\nBracket Retention\n\nA proportion of users after day 0, that had activity during some interval. This interval occurs sometime after day 0 and the data viewing date.\nexample: Pinterest\n\nMeasures â€œthe percentage of new signups that are still doing key actions during a one-week time window of 28â€“35 days after signupâ€.\n\n\n\nUser\n\ntypes\n\nNew user\nChurned user: inactive after day x\nInactive user: inactive from day 0 to day x\nReactive user: active after churned/inactive\nActive user: active users who are not new and not reactive\n\nNew user retention measures the proportion of new users who stay active.\nActive user retention measures the proportion of active users who stay active\n\nAction\n\nChoosing an action\n\nMore interested in monetization values?\nAre you in a growth stage where you looking to retain your users through certain actions?\n\ntypes\n\nvisiting the product page\nstaying for a certain amount of time\nconducting certain actions (e.g.. scrolling)\npurchasing a product\n\n\nCohorts\n\nDemographics: gender, age, etc.\nAcquisition: acquisition time, acquisition source, etc.\nBehavior\n\nAnalysis\n\nCalculate Retention Rates\n\nRecommendations\n\nStart with the N-day/week/month retention\nStart with new users (especially for new products)\n\nDetermine the time interval\n\nplot how frequently users use the product\nplot % users who use the product vs times with various intervals\nplot retention with various intervals\nsee which one makes sense for your product.\nItâ€™s okay to include more than one time interval in your analysis.\n\nRetention Rate vs Time\n\nSmiling curve when users come back more and more over time.\nDeclining curve signifies danger\nFlattening curve signifies a healthy product.\nThe goal for a product is to shift the curve up and flatten or uptail the curve\n\n\nDescriptive statistics and correlations\nSurvival model\nDashboard (all available cohorts/groups)\n\nretention curve\ntriangle retention table (see Cohort Analysis section)\n\nCohort (date), New Users (count), Day 1, Day 2, â€¦\n\nEvery cohort has 100% for Day 0 so not necessary to include\n\n\n\nWhat is the user behavior that contributes to the moment a new user becomes a long-term customer?\n\ni.e.Â Find the user events and onboarding behaviors that are related to retention in future weeks/months.\ne.g.Â Facebook focuses on â€œgetting 7 friends in 10 days.â€\n\nHow do the most successful long-term users form a habit to use the product?\n\nDescriptive analysis on long-term users\nQuestions\n\nWhat features do they keep using?\nWhat attributes do they have?\nWhat is the user journey for them?"
  },
  {
    "objectID": "qmd/algorithms-product.html#sec-alg-prod-nps",
    "href": "qmd/algorithms-product.html#sec-alg-prod-nps",
    "title": "2Â  Algorithms, Product",
    "section": "2.9 Net Promoter Score (NPS)",
    "text": "2.9 Net Promoter Score (NPS)\n\nUse sentiment analysis on customer reviews and social media posts to generate a proxy for NPS based on sentiment scores.\nUse Cases\n\nIdentify your brandâ€™s promoters, detractors, and neutrals\nCalculate the % of promoters vs detractors to give you insights into which direction your brand is progressing. A higher % of promoters indicates potential revenue growth and vice versa indicates that your brand is going down.\nSpecific text strings from extremely negative reviews will give you insights into areas of improvement. A simple word cloud for positive and negative reviews will give you insights into what works best for your brand and does not.\nThe same exercise repeated every quarter will give you insights into the trend of customer experience â€” is there an increasing trend of promoters or an increasing trend of detractors?\n\nAlso see Survey, Design &gt;&gt; Response Scales &gt;&gt; Net Promoter Score\nProcess\n\nCalculate the sentiment score for each customer review.\n\nmin-max normalize scores to a range of 0 to 10\n\nFor every customer, calculate the average sentiment score.\n\nOptional: weight score by number of reviews posted\n\nStems from the idea that we would like to focus on customers posting frequent reviews rather than customers who buy/post very rarely\n\n\nCategorize customers are promoters, neutrals, or detractors based on the average sentiment score.\n\nFor ideas on bins, see Survey, Design &gt;&gt; Response Scales &gt;&gt; Net Promoter Score\n\n\nExample (only split into promoters/detractors)"
  },
  {
    "objectID": "qmd/algorithms-recommendation.html#sec-alg-recom-misc",
    "href": "qmd/algorithms-recommendation.html#sec-alg-recom-misc",
    "title": "Recommendation",
    "section": "Misc",
    "text": "Misc\n\nResources\n\nSee this series of posts, https://towardsdatascience.com/recsys-series-part-8-the-14-properties-to-take-into-account-when-evaluating-real-world-recsys-f71cc6e1f195\n\nPackages\n\n{{cornac}} - Focuses on making it convenient to work with models leveraging auxiliary data (e.g., item descriptive text and image, social network, etc). Cornac enables fast experiments and straightforward implementations of new models. It is highly compatible with existing machine learning libraries (e.g., TensorFlow, PyTorch).\n{{lightfm}} - Python implementation of a number of popular recommendation algorithms for both implicit and explicit feedback.\n{{merlin}} - Open source library providing end-to-end GPU-accelerated recommender systems, from feature engineering and preprocessing to training deep learning models and running inference in production.\n\nModel Evaluation\n\nA temporal split is used, so you have the same users in train as in test. (See article in LightFM)\nSort by rating in descending order for each user. Otherwise, the metrics will be bad.\nFrom the model predictions, items already â€œseenâ€ by the user should be removed from the predictions\nIf you have low metrics, for example PrecisionK below 0.1, and you donâ€™t know what the reason is â€” the data or the model, or maybe the metric calculation\n\nCheck the sparsity index of your user/item matrix (see Metrics &gt;&gt; Misc)\nUse the MovieLens (.zip) dataset and train your model on it.\n\nIf its metrics are low on MovieLens too, then the cause is in the model\nif the metrics are good, then the likely cause lies in the preprocessing and postprocessing stages.\n\n\nIf Random model and Most popular model (baseline models) metrics are close to ML models, it is worth checking the data â€” maybe the number of unique items is too low.\n\nThis can also happen if we have very little data, or maybe there is a bug in the training code.\n\nIf values higher than 0.5 for Precision@k look too high and it is worth checking if there is a bug in the script or if we are lowering the sparsity index too much.\nAlways compare how many users and items you have left after lowering the sparsity index. Sometimes in the pursuit of quality you can lose almost all users, so you should look for a compromise.\nIf you already have a model in production, select the most promising candidates for online AB testing on KPIs. For testing recommender models, also see Experiments, A/B Testing &gt;&gt; Interleaving Experiments\n\nBaseline models\n\nMost Popular\ndf[item_col].value_counts()[:top_n]\n\nThis model may produce suprisingly good results but it will have low coverage.\n\nIf your company decides to expand the number of items it offers in the future, coverage for this model will be even lower.\n\n\nRandom Model\n\nOutputs random item ids\n\n\nâ€œRich get Richerâ€ problem\n\nThe most popular items get more and more recommendations which creates a feedback loop. This prevents new items or new vendors from getting exposure.\nSolution: Initially make the probability of a recommended item uniform (with some regard to category, etc.) and collect ratings. After enough ratings have been collected include rating as a feature in the ML model\n\nKrogerâ€™s â€œForget Somethingâ€\n\nFeature on checkout page that shows items the customer has previously frequently bought\n\nShould items be ordered by recency or most frequent?\n\nFilters\n\nSeasonal - frequently bought items theyâ€™ve bought during major holiday time intervals\n\nThis probably should be put into a separate tab or part of a filter list\nKroger has this but itâ€™s more of a recommendation list that has seasonal foods. Maybe this should be part of a separate recommendation feature\n\nItems not on their current grocery list\n\nNot automatic on Krogerâ€™s but it should be part of preprocessing before the list is presented to the customer\n\n\n\nEthics\n\nYglesias post on Facebookâ€™s (and all social media) only recommending to drive engagement. Makes me think there could be a score based on hate-speech or radicalism using sentiment analysis that could be included into the ranking algorithm that would make more ethical recommendations\n\nComplimentary item recommendation\n\nRecommender thatâ€™s similar to market basket analysis in that it recommends items that are often bought together with item being purchased.\nResources\n\nHow to provide complementary item recommendations in e-commerce marketplaces\n\n\nFiltering Features Based on Contextual Dimensions\n\ne.g.Â When the movie is watched or if the user is watching the movie with family or friends (which room itâ€™s watched in might hint at this?)\nNotes from Engineering Features for Contextual Recommendation Engines\nTraditional embedding techniques grows exponentially with the number of contextual dimensions (i.e.Â see multiverse as an example), alternative approaches like factorization machines can be employed to preserve tractability.\nUsing multi-layer perceptrons vs.Â matrix factorization, there is more flexibility in how embeddings are derived and used to find the most similar items to users. For example, embeddings can be learned whereby different dimensions have different weights in determining the users location in space.\nAn outcome could be explicit feedback like a rating, implicit feedback like watch time, or a non-feedback quantity like price.\nScenario: Outcome = F(I,C)\n\nItems (I) and context (C) play a dominant role in the recommender, whereas the user takes a backseat.\n\ne.g.Â New users with limited user-item history\n\nCategoricals are 1-hot encoded and aggregation calculations of numerics per category. Then, the category and the new numeric are combined.\n\nExample\n\n\ne.g.\n\navg_rating_work_photography = photography (1-hot) x AVG Rating (for photography work) = 2.92 for this prospective employee\n\n\nTherefore, each unit/subject has only 1 row.\n\nTo avoid unnecessary complexity and large dimension spaces, features are filtered based on the userâ€™s context\n\nExample: Filling a medium scope, medium budget photography job using a jobs website (see above table)\n\nIf prospective employee has other skills besides those pertaining to photography, then those are filtered out\nSo, in the above table, variables with â€œwork_photographyâ€, â€œadobe_photoshopâ€, â€œscope_mediumâ€, â€œbudget_mediumâ€, and all the experience variables would be included in the dimension space and the rest would be discarded\n\nInteractions like AVG Ratings when both the Budget == medium and the experience level == expert are not considered.\nNon-linearities can still be derived by the training algorithm, or additional contextual dimensions can be created as the the product of existing ones.\n\nA tree model (e.g.Â XGBoost) is trained using this dataset with the outcome being some kind of feedback column (e.g.Â from employers that hired the person for a job).\n\nThis modelâ€™s predictions are used to produce recommendations with this particular context but Iâ€™m not sure how thatâ€™s supposed to work.\n\n\nScenario: Outcome = F(U, C)\n\nThe recommendation isnâ€™t for a discrete item, but rather a continuous value.\n\ne.g.Â a platform recommending the price a property rental company should charge for a property on a weekend.\n\n\nQuestions\n\nâ€œWe can rebuild this representation now considering each past job as the current job, stacking each past job as a set of additional rows. For past jobs, we can create an additional column to capture explicit feedback received by the freelancer such as being hired, or implicit feedback like being messaged.â€\nNo idea what â€œtalentâ€ is"
  },
  {
    "objectID": "qmd/algorithms-recommendation.html#sec-alg-recom-metrics",
    "href": "qmd/algorithms-recommendation.html#sec-alg-recom-metrics",
    "title": "Recommendation",
    "section": "Metrics",
    "text": "Metrics\n\nMisc\n\nAlso see\n\nAlgorithms, Learn-to-Rank &gt;&gt; Diagnostics\nSimilarity Scoring bkmk folder\n\nJaccard Index, Map@k\n\n\nCompare your recommender click-through rates of your recommender to those of the baseline.\nIf your model is producing extremely low metrics, then you could have a sparsity issue\n\nSparsity Index\n\\[\n\\mbox{index} = 1 - \\frac{\\mbox{interactions} \\times \\mbox{items}}{\\mbox{users}}\n\\]\n\nGuessing â€œinteractionsâ€ are the number of 1s in your user/item matrix\n\nâ€œUsersâ€ and â€œItemsâ€ are the sizes of those two dims in your user-item matrix, so users*items would be the number of entries in your matrix\nTherefore interactions / (users*items) would be the proportion of 1s in your matrix and subtracting 1 makes it the proportion of 0s\n\nHigher values = sparser user/item matrix\nTypically, a sparsity-index of about 98% is sufficient for training\n\nReducing overly-sparse user/item matrices can have major effects on metric scores\n\nCheck user/item distribution\n\n\n\nCoverage\n\nThe metric allows you to see percentage of items used by the recommendation system.\nUsually very important for businesses to make sure that the content (e.g.Â movies) they have on their site is used to its full potential.\nCoverage = number of unique items in recommendations / all unique items\n\nDiversity\n\nHigh diversity values mean that users have an opportunity to discover new genres, diversify their experience, and spend more time on the site. As a rule, it increases the retention rate and has a positive impact on revenue.\nCan be measured in many different ways\n\nAverage similarity for top_n.\nMedian value of the number of unique genres (e.g.Â movies, books) or another category in hierarchy of product categories.\n\n\nMean Reciprocal Rank\n\\[\n\\mbox{score}_i = \\frac{1}{\\mbox{rank}_i}\n\\]\n\n(Wiki)\nScores based the rank the recommender model gave the â€œcorrectâ€ recommendation when it supplied itâ€™s list of recommendations. (i.e.Â The second recommendation is the recomendation the model felt was 2nd most likely to be correct)\nThe average score is calculated across all recommendations to represent the score of the model.\nExample: 3 queries\n\n\nNormalized Discounted Cumulative Gain (NDCG) (Mhaskar, 2015)\n\\[\n\\begin{align}\n&\\mbox{NDCG}_{\\mbox{pos}} = \\frac{\\mbox{DCG}_{\\mbox{pos}}}{\\mbox{iDCG}} \\\\\n&\\mbox{where}\\;\\; \\mbox{DCG} = \\sum_{\\mbox{pos}=1}^n \\frac{\\mbox{relevance}_{\\mbox{pos}}}{\\ln(\\mbox{pos}+1)}\n\\end{align}\n\\]\n\nRelevant documents appearing lower in the recommendation list are penalized as the graded relevance value is reduced logarithmically proportional to the position of the result\nNotes\n\nVery relevant results are more useful than somewhat relevant results which are more useful than irrelevant results (cumulative gain)\nRelevant results are more useful when they appear earlier in the set of results (discounting).\nThe result of the ranking should be irrelevant to the query performed (normalization).\n\n\nMAP@k - Mean average precision within the top k highest-ranked impressions\n\nExample: If a user watched movie recommendations ranked 1,2,3 and 5 but not 4th ranked recommendation, then the average score for that user would be (1/1 + 2/2 + 3/3 + 3/4 + 4/5)/5 = 0.91\n\nPrecision/Recall@k (article)\n\nPrecision@k\n\nThe proportion of recommended items in the top-k set that are relevant\nPrecision@k = (# out of k recommended items that match the observed relevant scores) / k\nk is a user definable integer that is set by the user to match the top-n recommendations objective (i.e.Â k = n)\nExample: 80% precision means on average that 80% of the top-n recommendations predicted by the model are relevant to the user.\nIssue: If there are no items recommended. i.e.Â number of recommended items at k is zero, we cannot compute precision at k since we cannot divide by zero.\n\nIn that case we set precision@k to 1. This makes sense because in that case we do not have any recommended item that is not relevant.\n\n\nRecall@k\n\nThe proportion of relevant items found in the top-k recommendations\nRecall@k = (# of k recommended items that match the observed relevant scores) / (total # of true relevant items)\nk is a user definable integer that is set by the user to match the top-n recommendations objective (i.e.Â k = n)\nExample: 40% recall means on average 40% of the total number of the (observed) relevant items appear in the top-n results.\nIssue: If there are no items recommended. i.e.Â number of recommended items at k is zero, we cannot compute precision at k since we cannot divide by zero.\n\nIn that case we set recall@k to be 1. This makes sense because we do not have any relevant items that are not identified in our top-k results.\n\n\nRelevance - A relevant item for a specific user-item pair means that this item is a good recommendation for the user in question.\n\nExample: User/Movie\n\n\nAssume that any true (i.e.Â observed) rating above 3.0 corresponds to a relevant item and any true rating below 3.0 is irrelevant.\n\nThe threshold is subjective. There are multiple ways to set this threshold value such as taking into consideration the history of ratings given by the user.\n\nUnder the above definition, the movies in the first 2 rows are â€œrelevantâ€ to those users, and the last three movies would be â€œirrelevantâ€ to those users\n\n\nRecommended - Items are generated by recommendation algorithm with a predicted rating greater than the relevance threshold\nIn calculations, ignore all the ratings where the actual value is not known\n\nRecList (article, article) - Behavior tests; black-box-ish py library that takes a sort of sentiment analysis approach to grading recommenders\n\n{checklist} for recommender systems (see Diagnostics, NLP &gt;&gt; Behavioral Tests &gt;&gt; Misc &gt;&gt; Packages)\nPrinciples\n\nComplementary and similar items satisfy different logical relations\n\nWhile similar items are interchangeable, complementary ones may have a natural ordering\ne.g.Â Recommend hdmi cable if a tv is bought, but not a tv if a hdmi cable is bought.\n\nNot all mistakes are equally bad\n\nExample: Truth: When Harry Met Sally\n\nRecommending Terminator is a worse miss than Youâ€™ve Got Mail\n\n\nSome groups are more important than others\n\nTolerate a small decrease in overall accuracy if a subset of users we care about is happier\nExample: Promoting Nike products so if the recommender results in substantial increased Nike Sales, for Italian users on iphones it becomes terrible, then thatâ€™s an acceptable trade-off\n\n\n\nRGRecSys (Salesforce)\n\nEvaluation toolkit that encompasses multiple dimensions - robustness with respect to sub-populations, transformations, distributional disparity, attack, and data sparsity\nGithub\nPaper"
  },
  {
    "objectID": "qmd/algorithms-recommendation.html#sec-alg-recom-colfil",
    "href": "qmd/algorithms-recommendation.html#sec-alg-recom-colfil",
    "title": "Recommendation",
    "section": "Collaborative Filtering",
    "text": "Collaborative Filtering\n\nFocuses on the relationship of users and items in question (ideal when the data contains ratings for the various items offered).\nExample person A and B listen to Song X\n\nIf person B listens often to song Y, then A is very likely to like song Y as well.\n\nDifferences in embeddings can be used to find analagous product relationships\n\nI think for this to work, youâ€™d have some sort of product description (e.g.Â ingredients, materials, comments, consumers describing taste, feel, look, etc.) embedding and not just product name embeddings.\nExample: Find the closest diet soft drink to a brand that doesnâ€™ t offer a diet version of their soft drink\n\nCalculate the difference between the diet coke embedding and coke embedding\nFind a diet soft drink embedding that has a similar delta with the soft drink embedding that doesnâ€™t offer a diet version.\n\n\nAlso read about applying regularization after SVD to control for overfitting(?)\nInstead of K-NN, a correlation matrix can be used to find similar items after an item has been inputted.\nClustering embeddings\n\nkNN is has O(N*K) complexity, where N is the number of items and K is the size of each embedding.\n\nApproximate nearest neighbor (ANN) algorithms typically drop the complexity of a lookup to O(log(n))\n\nSee Clustering, General\n\n\n\n\n\nNon-Negative Matrix Factorization (NMF)\n\nMisc\n\nPackages:\n\n{NMF}\n\nMethods\n\nAlternating Least Squares (ALS)\n\nSpark has recommendation model that uses this algorithm\n\nHierarchical Alternating Least Squares (HALS)\n\n\nDecomposes the sparse user-item matrix (N_users â¨¯ N_items) to 2 lower dimensional matrices (W & H)\n\nThe original matrix must have all non-negative entries (I think) The product of W and H is an approximation of the original user-item matrix. Being an approximation is how itâ€™s different from SVD This is because the 2 resulting matrices are constrained to be non-negative (every elt is non-negative) instead of orthogonal\n\n\n\n\n\n__\nSKU1\nSKU2\nSKU3\n\n\nCUST1\n0\n1\n1\n\n\nCUST2\n0\n0\n1\n\n\nCUST3\n1\n0\n0\n\n\n\n\nI donâ€™t think these are indicator columns. I think theyâ€™re quantities, so each cell can have any positive integer value (or zero).\nCustomer Segment Matrix, W: N_users â¨¯ N_latent_factors\n\nEach column is a â€œsegmentâ€ or â€œbasisâ€\nEach cell is the customerâ€™s score for that segment\n\nProduct Segment Matrix, H: N_latent_factors â¨¯ N_items\n\nEach row is a â€œsegmentâ€ or â€œbasisâ€\nEach cell is the productâ€™s score for that segment\n\nThe dimension of â€œN_latent_factorsâ€ is a tuning parameter\n\nIf a customer and product have high scores for the same segments, then our factorization is implying that this cell in the customer-by-product matrix has a high value.\n\nA customerâ€™s predicted rating for an item is calculated by multiplying the customer vector by the item vector\nIn general, by multiplying both W and H matrices, we end up with an estimate of the original matrix, but more densely filled with all of our predicted ratings\n\nThe resulting latent dimensions will capture the most relevant information or characteristics about each customer and item and therefore improve the performance of the downstream clustering task\nResults can be visualized with a heatmap\n\nExample had 2 heatmaps (customer segment matrix, product segment matrix)\nA dark cell for customer i and segment j says that the customer prefers products with high scores in segment j (dark cells in the product heatmap)\n\nI think the scores are normalized, so closer to 1 means a stronger association\n\n\nIssues\n\nCold Start Problem - When you have new users or items that you want to make predictions for since the model had no possibility to learn anything for them, leaving you with basically random recommendations for them\n\nSee LightFM for potential solution\n\nUser-Item matrices can be too sparse\n\nSee Metrics &gt;&gt; Misc &gt;&gt; Sparsity Index\nSolutions:\n\nRemove some users or items that have low interaction counts (i.e.Â 1s in user/item matrix)\nDevelop a hierarchy of information about the productâ€”from brand, to style, to size (or SKU), and choose the appropriate level of the hierarchy to use\n\n\nFactorization parameters\n\nDetermine the starting state of the matrix in the estimation process (i.e.Â which product information to use? see 1st issue)\nloss function\n\nFrobenius norm and the Kullback-Leibler divergence commonly used (at least in NLP) to minimize the distance between the user-item matrix and its approximation (W x H)\n\n\\(k\\) - Number of segments\n\nToo many segments, and the information is hard to digest; too few, and you are not explaining the data well\n\n\\(\\alpha\\) - A regularization parameter for learning W and H\n\nRegularization is an optional technique that can be used to have more control over how outliers can influence the algorithm\n\n\\(\\mbox{tolerance}\\) - Stopping condition for the NMF algorithm. i.e.Â how close is the approximation X â‰ˆ W x H.\n\nLower tolerance means more training iterations\n\n\n\nUse Cases\n\nDeveloping product-based customer segments to build need-based personas\nDeciding which products should be offered together as a bundle\nBuilding a product recommendation engine that uses a customerâ€™s segment to determine which products should be merchandised\n\nSimilar items beget similar ratings from similar users\n\nTopic Analysis\n\nAlso see NLP, Topic &gt;&gt; NMF\nPick a couple columns of a df (e.g.Â movie titles, plot descriptions)\ntf-idf the plot descriptions and remove stopwords, names, etc.\nApply NMF\n\nFor each segment (rows) in H, find the k most important words (columns) (i.e.Â words with top k scores)\n\nEach segment is a genre. Use the words to label the genres for the movies.\n\nAdjusting the number of segments might produce more coherent results if necessary.\n\n\n\nExample (word2vec, GMM, HDBSCAN)\n\nNotes from https://towardsdatascience.com/using-gaussian-mixture-models-to-transform-user-item-embedding-and-generate-better-user-clusters-a46e1062d621\nMusic playlist data: Users hash ids (users), Artist names (items)\nSteps\n\nItems are characters so apply word2vec to get embeddings\n\nEach unique item will have a numeric vector (item_vec) associated with it.\n\nFor each userâ€™s playlist, compute average vector of all the item vectors associated with it.\n\n(item1_vec + item2_vec + â€¦)/n\nEach user is represented by a (averaged) vector\n\nWe have item_df which has each item and its word2vec numeric representation, and we have a user_df which has each user and their playlists represented by a mean embedding numeric vector\nFit a gaussian mixture model, gmm_mod, with the item_df\nFor each user, get vector of probabilities of that user belonging to each cluster by user_probs &lt;â€“ predict(gmm_mod, data = user_df)\n\nBy increasing the number user_df features by creating gmm cluster probabilities, it helps other clustering algorithms separate users more easily.\n\nCluster user_probs using HDBSCAN\n\nWhere user_probs should look like col1 = users, col2 = gmmclust1_probs, col3 = gmmcluster2_probs, etc.\n\n\nOptional processing that can improve results\n\nLogging and standardizing before clustering\nRemoving outliers (e.g.Â very popular artists) at the start\n\n\n\n\n\nLightFM\n\nCreates embeddings for additional features of the user and the product. Then, adds all the user embeddings together and adds all the product embeddings together and proceeds as normal\nIn the cold start case, the new user gets a default embedding for â€œuser_idâ€ and â€œproduct_idâ€, but by using the additional feature embeddings, you get an informative rating.\nMisc\n\nNotes from A Performant Recommender System Without Cold Start Problem\nPaper (2015)(code)\n\nResults (only tested on 2 datasets w/binary labels)\n\nâ€œIn both cold-start and low-density scenarios, LightFM performs at least as well as pure content-based models, substantially outperforming them when either (1) collaborative information is available in the training set or (2) user features are included in the model.â€\nâ€œWhen collaborative data is abundant (warm-start, dense user-item matrix), LightFM performs at least as well as the MF model.â€\nâ€œEmbeddings produced by LightFM encode important semantic information about features and can be used for related recommendation tasks such as tag recommendations.â€\n\n\n\nMethod Comparison\n\nTypical collaborative filtering (e.g.Â user ids, movie ids)\n\nLightFM"
  },
  {
    "objectID": "qmd/algorithms-recommendation.html#sec-alg-recom-contb",
    "href": "qmd/algorithms-recommendation.html#sec-alg-recom-contb",
    "title": "Recommendation",
    "section": "Content-based",
    "text": "Content-based\n\nMeasure similarities in product characteristics and matching based on the strength of the measures.\nVarious similarity scoring algorithms\n\nDensity Adjusting\nVector Embedding\nCo-Occurrence\n\nCollaborative Topic Modeling/Regression (CTM/CTR) for text-based items with enhanced accuracy and out-of-matrix prediction capabilities. * out-of-matrix capability: able to generalize the recommendations for new, completely unrated items: since no ratings are observed for item j, a model without this capability cannot derive its latent vector of qualities.\n\nBuilt on top of Latent Dirchlet Allocation (LDA) and Probabilistic Matrix Factorization (PMF)"
  },
  {
    "objectID": "qmd/algorithms-recommendation.html#sec-alg-recom-comprec",
    "href": "qmd/algorithms-recommendation.html#sec-alg-recom-comprec",
    "title": "Recommendation",
    "section": "Company Recommenders",
    "text": "Company Recommenders\n\nSpotify - Truncated SVD of user-item matrix then K-NN to find similar groupings\nYoutube\n\n\nPaper\nSimilar to GMM example (above) except:\n\na DL model takes the embeddings and outputs class probabilities instead of a GMM\napprox-kNN is used instead of HDBSCAN to cluster users\n\nCombines two different deep neural networks, a first to select good candidate videos that youâ€™d like if you watched, and a second to pick the best candidates that you are most likely to watch the longest. (Think the candidate NN is the one shown in the figure)\n\nTikTok\n\n\nNotes from:\n\nThe Batch\n\nSummarizes a nytimes article of a leaked internal document\n\nTikTokâ€™s Secret Sauce\n\nFlow chart starts with the primary goal on the left (increase Daily Active Use). From left to right, it breaks down variables into other variables that are probably predictive of the variable preceding it. Therefore maximizing the right-most variables will increase the primary KPI (daily active use)\nFor each user, videos are ranked according to an expected value equation (i.e sum of probabilities and values): Plike x Vlike + Pcomment x Vcomment + Eplaytime x Vplaytime + Pplay x Vplay\n\nPs are likely predicted values that the user will do the action (e.g.Â like, comment, play) for a particular video.\nVs are values that the tiktok has determined for that particular action\n\nMaybe determined by a regression standardized coefficient (daily_active_use ~ comment + â€¦) or variable importance value\n\nE may be â€œestimated.â€ Might be a descriptive statistic for a user. e.g.Â avg playtime given var1, var2, etc.\n\nmaybe the vars are the â€œOther featuresâ€ or Creative quality variables, etc.\n\n\nThe Batch article discusses ways to penalize videos\nUI design: Swiping instead of scrolling\nTreats each video more or less independently to assess its viral potential, caring relatively little about how many followers the creator has\nMore exploration, less exploitation\n\nThere is a tradeoff between safe but somewhat boring recommendations that are similar to recommendations that worked well in the past (â€œexploitationâ€), and risky recommendations that are unlikely to be good but have a high payoff if they do turn out to be good (â€œexplorationâ€).\nPlaces a relatively high emphasis on exploration compared to other platforms\nTikTok is able to take risks because all it takes is a swipe (see UI design and article for details)\n\n\nNetflix\n\nUses a Embarrassingly Shallow Autoencoders (EASE) for embeddings\nUses Propensity Correction (Paper) to prevent feedback loops which can bias the recommender\n\nInstagram\n\nHow Instagram suggests new content"
  },
  {
    "objectID": "qmd/algorithms-recommendation.html#sec-alg-recom-otnot",
    "href": "qmd/algorithms-recommendation.html#sec-alg-recom-otnot",
    "title": "Recommendation",
    "section": "Other Notes",
    "text": "Other Notes\n\nRecSys 2020 â€” Takeaways and Notable Papers\n\nInverse propensity scoring was a popular approach taken to debias recommendations\nIncreased shift towards sequence models (with SASRec (2018) and BERT4Rec (2019) being common benchmarks) and bandit and reinforcement learning for recommender systems\nThree sources of recommendation complexity, namely:\n\nPlacement: Where is the recommendation located on the user interface?\nPerson: Who is seeing the recommendation? What are her past experiences with the recommendation placement?\nContext: What is going on at that moment? What are the userâ€™s needs?\n\nThere are many contexts where similarity is not required or can worsen recommendations. For example, users might want a change of pace or mood from that horror movie they just watched. Also, does the user stick to a specific genre (e.g., Korean dramas) or hop around diverse genres? A better understanding will help improve the user experience.\n\n\nDifferent data splitting strategies (for train and validation) can affect the relative performance of recommendation systems in offline evaluation\n\nSplitting strategies\n\nLeave-one-last: Leave one last item, leave one last basket/session\nTemporal: Temporal split within each user, temporal split (on same date) globally\nRandom: For each user, split interactions into train and test data\nUser: Split some users into train, the rest into test\n\nRelative performance of recommenders changed often across splitting strategies (indicated by the rank swaps)\n\n\nModel Evaluation\n\nSR-GNN: Best hit rate, mean reciprocal rank, and nDCG\nV-STAN: Best precision, recall, and mean average precision\nV-SKNN, GRU4Rec: Best coverage and popularity\nSTAMP: Satisfactory in all metrics\nUsing human experts to evaluate recommendations\n\nIn contrast to the offline evaluation metrics, human experts found GRU4Rec to have very relevant recommendations. However, because its recommendations did not match the IDs of products added to cart, GRU4Rec did not perform as well on offline evaluation metrics.\nSTAMP and GRU4Rec performed best in the second step and STAMP was put through an A/B test. This led to a 15.6% increase in CTR and an 18.5% increase in revenue per session.\n\n\nSimilarity\n\nMatrix factorization faster w/sufficient regularization outperformed and was faster than DL approaches\n\nAdding unexpectedness to recommendations to add freshness\n\nTwo kinds\n\nPersonalized: Some users are variety seekers and thus more open to new videos\nSession-based: If a user finishes the first episode of a series, itâ€™s better to recommend the next episode. If the user binged on multiple episodes, itâ€™s better to recommend something different.\n\n\nBad idea to use default parameters for Word2vec-based recommendations"
  },
  {
    "objectID": "qmd/anomaly-detection.html#sec-anomdet-misc",
    "href": "qmd/anomaly-detection.html#sec-anomdet-misc",
    "title": "Anomaly Detection",
    "section": "Misc",
    "text": "Misc\n\nAlso see\n\nOutliers\nbkmks\n\nTime Series &gt;&gt; Cleaning/Processing &gt;&gt; Outliers\nBusiness Applications &gt;&gt; Fraud/Anomaly Detection\n\n\nFrom Sci Kit Learn (link)\n\nCommon Questions\n\nâ€œI need to know why your model detected an anomaly. I need sound root cause analysis before I adjust my manufacturing process.â€ (see variable importance tracking)\nâ€œAnomaly detection is not enough: when a model detects an anomaly, itâ€™s already too late. I need prediction to justify investing time and effort into such an approach.â€ (see event rates)\nâ€œI need prescription: tell me what I should do to prevent a failure from happening.â€ (see variable importance tracking)\n\nCommon Criticisms\n\nâ€œThere are some false positives, I donâ€™t have time to investigate each event!â€ (see event rates)\nâ€œYour model only detects anomalies when they already happened, itâ€™s useless!â€œ (see event rates)\nâ€œI have hundreds of sensors: when an anomaly is detected by your model, I still have to investigate my whole operations, Iâ€™m not saving any time here!â€ (see variable importance tracking)\n\nAmazon Lookout for Equipment - managed service from AWS dedicated to anomaly detection\nAny abnormal event visible in your time series will either be a:\n\nPrecursor Event\nDetectable Anomaly (forewarning about a future event)\nA Failure\nMaintenance Activity\nHealing Period (while your industrial process recovers after an issue)\n\nTypes\n\nShocks - abrupt changes, spikes\nLevel Shifts - can happen when a given time series shifts between range of values based on underlying conditions or operating modes.\n\nLevel â€“ The average value for a specific time period\nIf you want to consider all operating modes when detecting anomalies, you need to take care to include all of them in your training data\n\nTrending: a set of signals can change over time (not necessarily in the same direction).\n\nWhen you want to assess the condition of a process or of a piece of equipment, these trending anomalies will be great precursors events to search. They will help you build forewarning signals before actual failures may happen.\n\n\nUse average event rates to filter out false positives and predict an upcoming event\n\n\nTake action if the event rate (i.e.Â rate of predicted events) starts to grow too large (allowing you to move from detecting to predicting)\n\ne.g.Â You can decide to only notify an operator after the daily event rate reaches at least 200 per day. This would allow you to only react to 3 events out of the 41 detected (aka predicted) during this period\n\nUse historical event data to calculate an event rate threshold\nBy only reacting after a threshold predicted event rate has been reached, you filter out false positives (when scarce events are detected)\n\nTrack variable importance over time to narrow the field of potential causes of an event\n\n\n2 stacked column charts represent two anomalous events.\nEach color is a predictor variable (e.g.Â sensor) that was important to the prediction of the event.\n\nOnly need to focus on a few (e.g.Â top five predictor variables)\n\nUse to examine false positives and actual events\n\n\nFor the false positive (left)\n\nThe percentage of importance attributed the top 5 is much less than that for an actual event\nRed is less important and Yellow is more important than when thereâ€™s an actual event\n\n\n\nRolling Spectral Entropy\n\n\n\nSpectral Entropy is the normalized (power) spectral density )(PSD)\nMisc\n\nNotes from Anomaly Detection in Univariate Stochastic Time Series with Spectral Entropy\n\nGuidelines\n\nA series which has strong trend and seasonality (and so is easy to forecast) will have entropy close to 0.\n\nIn the case of noisy time series, this indicates an anomaly.\n\nA series that is very noisy (and so is difficult to forecast) will have entropy close to 1.\n\nExample\n\ndef spectral_entropy(x, freq, nfft=None):Â  Â \nÂ  Â  _, psd = periodogram(x, freq, nfft = nfft)Â  Â \nÂ  Â  # calculate shannon entropy of normalized psd\nÂ  Â  psd_norm = psd / np.sum(psd)\nÂ  Â  entropy = np.nansum(psd_norm * np.log2(psd_norm))\nÂ  Â  return -(entropy / np.log2(psd_norm.size))\n\nwindow = 200\nnfft = None\ndf = pd.DataFrame(data=x, columns=['x'])\ndf['x_roll_se'] = df['x'].rolling(window).apply(lambda x: spectral_entropy(x,freq=100,nfft=nfft))\n\nIf the FFT size is not specified, we will use the window size"
  },
  {
    "objectID": "qmd/anomaly-detection.html#sec-anomdet-charts",
    "href": "qmd/anomaly-detection.html#sec-anomdet-charts",
    "title": "Anomaly Detection",
    "section": "Charts",
    "text": "Charts\n\nThe goal to breakdown an anomaly (e.g.Â manufacturing process outage) into constituent parts (sensor readings).\n\n\nBy analyzing the sensor readings, you can potentially find the area causing the anomaly\nThere also might be leading indicators that are predictive of an anomaly.\n\nOften youâ€™re looking at many time series (e.g.Â a manufacturing process) when performing EDA for anomaly detection, and conventional time series charts are insufficient\nHorizon Chart\n\n\nHue (e.g.Â blue or red) represents values above or below a certain value\nDarkness/lightness of the color represents the extremeness of the value\n\ni.e.Â the darker the color the larger the magnitude of the y-axis value\n\nVertical layers in the normal chart become horizontal layers of the horizon chart\n\nlayer feature may provide more detail than the strip chart\n\n\nStrip Chart\n\nSimilar to horizon in that y-axis values get binned and represented by colors\n\nExample\n\n\nColors\n\nBlue - low, Gold - medium, Red - high\n\nColumns of red indicate shocks (e.g.Â around 2022-11-15)\n\nExample\n\n\nColors\n\nBlue - low, Gold - medium, Red - high\n\nMajor color changes in color indicate trend/level shifts\n\ne.g.Â the change from a lot of red to a lot of blue after December 2017"
  },
  {
    "objectID": "qmd/anomaly-detection.html#sec-anomdet-isofor",
    "href": "qmd/anomaly-detection.html#sec-anomdet-isofor",
    "title": "Anomaly Detection",
    "section": "Isolation Forests",
    "text": "Isolation Forests\n\nAlso see Algorithms, ML &gt;&gt; Trees &gt;&gt; Isolation Forests\nUsed for anomaly detection. Algorithm related to binary search.\nNotes from paper: https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/icdm08b.pdf\nThe tree algorithm chooses a predictor at random for the root node. Then randomly chooses either the minimum or the maximum of that variable as the splitting value. The algorithm recursively subsamples like normal trees (choosing variables and split points in the same manner) until each terminal node has one data point or replicates of the same data point or preset maximum tree height is reached. Across the trees of a forest, anomalies with have a shorter average path length from root to terminal node.\n\nThe algorithm is basically looking for observations with combinations of variables that have extreme values. The process of continually splitting subsamples of data will run out data points and be reduced to a single observation more quickly for an anomalous observation than a common observation.\nMakes sense. Picturing a tree structure, there shouldnâ€™t be too many observations with more that a few minimums/maximums of variable values. The algorithm weeds out these observations as it moves down the tree structure.\n\nAny or all of these wouldnâ€™t necessarily be global minimum/maximums since weâ€™re dealing with subsamples of variable values as we move down the tree.\n\nPaper has some nice text boxes with pseudocode that goes through the steps of the algorithm.\n\nAnomaly scores range from 0 to 1. Observations with a shorter average path length will have a larger score.\n\nAnomaly score\n\\[\n\\begin{aligned}\n&s(x_i, n) = 2^{-\\frac{\\mathbb{E}(h(x_i))}{c(n)}}\\\\\n&\\begin{aligned}\n\\text{where}\\quad c(n) &= 2H(n-1) - \\frac{2(n-1)}{n} \\\\\nH(i) &= \\ln(i) + \\gamma\n\\end{aligned}\n\\end{aligned}\n\\]\n\n\\(\\mathbb{E}(h(x_i))\\) is the average path length across the isolation forest for that observation\n\\(H(i)\\) is the harmonic number and \\(\\gamma\\) is Eulerâ€™s constant\n\nGuidelines\n\nThe closer an observationâ€™s score is to 1 the more likely that it is an anomaly\nThe closer to zero, the more likely the observation isnâ€™t an anomaly.\nObservations with scores around 0.5 means that the algorithm canâ€™t find a distinction."
  },
  {
    "objectID": "qmd/anomaly-detection.html#sec-anomdet-autoenc",
    "href": "qmd/anomaly-detection.html#sec-anomdet-autoenc",
    "title": "Anomaly Detection",
    "section": "Autoencoders",
    "text": "Autoencoders\n\nAn outlier is something that the autoencoder has not seen often during training, so it might have trouble finding a good encoding for them.\n\nAn autoencoder tries to learn good encodings for a given dataset. Since most data points in the dataset are not outliers, the autoencoder will be influenced most by the normal data points and should perform well on them.\n\nMisc\n\nAlso see Feature Reduction &gt;&gt; Autoencoders"
  },
  {
    "objectID": "qmd/apache-arrow.html#sec-apache-arrow-misc",
    "href": "qmd/apache-arrow.html#sec-apache-arrow-misc",
    "title": "Arrow",
    "section": "Misc",
    "text": "Misc\n\nResources\n\nDocs\nApache Arrow R Cookbook\nCheatsheet\n\nFeather format built for speed, not compression. so slightly larger files than parquet\n\nFeature for short term storage and parquet for longer term storage\n\nEven with S3 support enabled, network speed will be a bottleneck unless your machine is located in the same AWS region as the data.\nTo create a multi-source dataset, provide a list of datasets to open_dataset() instead of a file path, or simply concatenate them like big_dataset &lt;- c(ds1, ds2)\nMore verbose installation + get compression libraries and AWS S3 support\nSys.setenv(\nÂ  ARROW_R_DEV = TRUE,\nÂ  LIBARROW_MINIMAL = FALSE\n)\ninstall.packages(\"arrow\")\n\nInstallation takes some time, so this lets you monitor progress to make sure it isnâ€™t locked.\n\nInfo about your Arrow installation - arrow_info()\n\nversion, compression libs, c++ lib, runtime, etc.\n\nCreating an Arrow dataset\n\nHas a script that downloads monthly released csv files; creates a Hive directory structure; and converts them to parquet files with partitioning based on that structure.\n\nStatically hosted parquet files provide one of the easiest to use and most performant APIs for accessing bulkÂ¹ data, and are far simpler and cheaper to provide than custom APIs. (article)\n\nPros and cons\nList of cases when are static files inappropriate"
  },
  {
    "objectID": "qmd/apache-arrow.html#sec-apache-arrow-apis",
    "href": "qmd/apache-arrow.html#sec-apache-arrow-apis",
    "title": "Arrow",
    "section": "APIs",
    "text": "APIs\n\nSingle File\n\nContains functions for each supported file format (CSV, JSON, Parquet, Feather/Arrow, ORC).\n\nStart with read_Â  or write_Â  followed by the name of the file format.\ne.g.Â read_csv_arrow() , read_parquet() , and read_feather()\n\nWorks on one file at a time, and the data is loaded into memory.\n\nDepending on the size of your file and the amount of memory you have available on your system, it might not be possible to load the dataset this way.\nExample\n\n111MB RAM used - Start of R session\n135MB - Arrow package loaded\n478MB - After using read_csv_arrow(\"path/file.csv\", as_data_frame = FALSE) to load a 108 MB file\n\n525MB with â€œas_data_frame = TRUEâ€ (data loaded as a dataframe rather than an Arrow table)\n\n\n\n\nDataset\n\nCan read multiple file formats\nCan point to a folder with multiple files and create a dataset from them\nCan read datasets from multiple sources (even combining remote and local sources)\nCan be used to read single files that are too large to fit in memory.\n\nData does NOT get loaded into memory\nQueries will be slower if the data is not in parquet format\n\ne.g.Â dat &lt;- open_dataset(\"~/dataset/path_to_file.csv\")"
  },
  {
    "objectID": "qmd/apache-arrow.html#sec-apache-arrow-datobjs",
    "href": "qmd/apache-arrow.html#sec-apache-arrow-datobjs",
    "title": "Arrow",
    "section": "Data Objects",
    "text": "Data Objects\n\nScalar - R doesnâ€™t have a scalar class (only vectors)\nScalar$create(value, type)\nArray and ChunkedArray\nChunkedArray$create(..., type)\nArray$create(vector, type)\n\nOnly difference is that one can be chunked\n\nRecordBatch and Table\nRecordBatch or Table$create(...)\n\nSimilar except Table can be chunked\n\nDataset - list of Tables with same schema\nDataset$create(sources, schema)\nData Types (?decimal) (Table$var$cast(decimal(3,2))\n\nint8(), 16, 32, 64\nuint8(), â€¦\nfloat(), 16, 32, 64\nhalffloat()\nbool(), boolean()\nutf8(), large_utf8\nbinary(), large_binary, fixed_size_binary(byte_width)\nstring()\ndate32(), 64\ntime32(unit = c(\"ms\", \"s\")), 64\ntimestamp(unit, timezone)\ndecimal()\nstruct()\nlist_of(), large_list_of(), fixed_size_list_of()"
  },
  {
    "objectID": "qmd/apache-arrow.html#sec-apache-arrow-ops",
    "href": "qmd/apache-arrow.html#sec-apache-arrow-ops",
    "title": "Arrow",
    "section": "Operations",
    "text": "Operations\n\nread_csv_arrow(&lt;csv_file&gt;, as_data_frame = FALSE)\n\nReads csv into memory as an Arrow table\nas_data_frame - if TRUE (default), reads into memory as a dataframe\n\nwrite_parquet\n\ncompression\n\ndefault â€œsnappyâ€ - popular\nâ€œuncompressedâ€\nâ€œzstdâ€ (z-standard)\n\nhigh performance from Google\ncompressed to smaller size than snappy\n\n\nuse_dictionary\n\ndefault TRUE - encode column types e.g.Â factor variables\nFALSE - increases file size dramatically (e.g.Â 9 kb to 86 kb)\n\nchunk_size\n\nIf performing batch tasks, you want the largest file sizes possible\nif accessing randomly (?), you might want smaller chunck sizes\nexplodes file size (e.g.Â 9 kb to 396 kb)\n\n\nConvert large csv to parquet\nmy_data &lt;- read_csv_arrow(\nÂ  \"~/dataset/path_to_file.csv\",\nÂ  as_data_frame = FALSE\n)\nwrite_parquet(data, \"~/dataset/my-data.parquet\")\ndat &lt;- read_parquet(\"~/dataset/data.parquet\", as_data_frame = FALSE) # loaded into memory as an Arrow table\n\nReduces size of data stored substantially (e.g.Â 15 GB csv to 9.5 GB parquet)"
  },
  {
    "objectID": "qmd/apache-arrow.html#sec-apache-arrow-part",
    "href": "qmd/apache-arrow.html#sec-apache-arrow-part",
    "title": "Arrow",
    "section": "Partitioning",
    "text": "Partitioning\n\nPartitioning increases the number of files and it creates a directory structure around the files.\nPros\n\nAllows Arrow to construct a more efficient query\nCan be read and written with parallelism\n\nCons\n\nEach additional file adds a little overhead in processing for filesystem interaction\nCan increase the overall dataset size since each file has some shared metadata\n\nBest Practices\n\nAvoid having individual Parquet files smaller than 20MB and larger than 2GB.\n\nHaving files beyond this range will cancel out the benefit of your query grouping by a partition column. (see article for benchmarks)\n\nAvoid partitioning layouts with more than 10,000 distinct partitions.\nOptimal Size is 512MB â€” 1GB (docs)\n\nView metadata of a partitioned dataset\nair_data &lt;- open_dataset(\"airquality_partitioned_deeper\")\n\n# View data\nair_data\n\n## FileSystemDataset with 153 Parquet files\n## Ozone: int32\n## Solar.R: int32\n## Wind: double\n## Temp: int32\n## Month: int32\n## Day: int32\n##\n## See $metadata for additional Schema metadata\n\nThis is a â€œdatasetâ€ type so data wonâ€™t be read into memory\nAssume $metadata will indicate which columns the dataset is partitioned by\n\nPartition a large file and write to arrow format\n1lrg_file &lt;- open_dataset(&lt;file_path&gt;, format = \"csv\")\nlrg_file %&gt;%\n2Â  Â  group_by(var) %&gt;%\n3Â  Â  write_dataset(&lt;output_dir&gt;, format = \"feather\")\n\n1\n\nPass the file path to open_dataset()\n\n2\n\nUse group_by() to partition the Dataset into manageable chunks\n\n3\n\nUse write_dataset() to write each chunk to a separate Parquet fileâ€”all without needing to read the full CSV file into R\n\n\n\nopen_dataset is fast because it only reads the metadata of the file system to determine how it can construct queries\n\nPartition Columns\n\nPreferrably chosen based on how you expect to use the data (e.g.Â important group variables)\nExample: partition on county because your analysis or transformations will largely be done by county even though since some counties may be much larger than others and will cause the partitions to be substantially imbalanced.\nIf there is no obvious column, partitioning can be dictated by a maximum number of rows per partition\nwrite_dataset(\nÂ  data,\nÂ  format = \"parquet\",\nÂ  path = \"~/datasets/my-data/\",\nÂ  max_rows_per_file = 1e7\n)\ndat &lt;- open_dataset(\"~/datasets/my-data\")\n\nFiles can get very large without a row count cap, leading to out-of-memory errors in downstream readers.\nRelationship between row count and file size depends on the dataset schema and how well compressed (if at all) the data is\nOther ways to control file size.\n\nâ€œmax_rows_per_groupâ€ - splits up large incoming batches into multiple row groups.\n\nIf this value is set then â€œmin_rows_per_groupâ€ should also be set or else you may end up with very small row groups (e.g.Â if the incoming row group size is just barely larger than this value)."
  },
  {
    "objectID": "qmd/apache-arrow.html#sec-apache-arrow-fpdn",
    "href": "qmd/apache-arrow.html#sec-apache-arrow-fpdn",
    "title": "Arrow",
    "section": "Fixed Precision Decimal Numbers",
    "text": "Fixed Precision Decimal Numbers\n\nComputers donâ€™t store exact representations of numbers, so there are floating point errors in calculations. Doesnâ€™t usually matter in analysis, but it can matter in transaction-based operations.\ntxns &lt;- tibble(amount = c(0.1, 0.1, 0.1, -0.3)) %&gt;%\nÂ  Â  summarize(balance = sum(amount, na.rm = TRUE\n# Should be 0\ntxns\n# 5.55e-17\nThe accumulation of these errors can be costly.\nArrow can fix this with fixed precision decimals\n# arrow table (c++ library)\n# collect() changes it to a df\ntxns &lt;- Table$create(amount = c(0.1, 0.1, 0.1, -0.3))\ntxns$amount &lt;- txns$amount$cast(decimal(3,2))\ntxns\n# blah, blah, decimal128, blah\n\nwrite_parquet(txns, \"data/txns_decimal.parquet\")\ntxns &lt;- spark_read_parquet(\"data/txns_decimal.parquet\")\ntxns %&gt;%\nÂ  Â  summarize(balance = sum(ammount, na.rm = T))\n# balance\n#Â  Â  0"
  },
  {
    "objectID": "qmd/apache-arrow.html#sec-apache-arrow-quer",
    "href": "qmd/apache-arrow.html#sec-apache-arrow-quer",
    "title": "Arrow",
    "section": "Queries",
    "text": "Queries\n\nExample: Filter partitioned files\nlibrary(dbplyr)\n# iris dataset was written and partitioned to a directory path stored in dir_out\nds &lt;- arrow::open_dataset(dir_out, partitioning = \"species\")Â \n# query the dataset\nds %&gt;%Â \nÂ  filter(species == \"species=setosa\") %&gt;%\nÂ  count(sepal_length) %&gt;%Â \nÂ  collect()\n\nformat â€œ&lt;partition_variable&gt;=&lt;partition_value&gt;â€\ncompute stores the result in Arrow\ncollect brings the result into R\n\nExample: libarrow functions\narrowmagicks %&gt;%Â \nÂ  mutate(days = arrow_days_between(start_date, air_date)) %&gt;%Â \nÂ  collect()\n\nâ€œdays_betweenâ€ is a function in libarrow but not in {arrow}. In order to use it, you only have to put the â€œarrow_â€ prefix in front of it.\nUse list_compute_functions to get a list of the available functions\n\nList of potential functions available (libarrow function reference)\n\n\nWhen the query is also larger than memory\nlibrary(arrow)\nlibrary(dplyr)\nnyc_taxi &lt;- open_dataset(\"nyc-taxi/\")\nnyc_taxi |&gt;\nÂ  filter(payment_type == \"Credit card\") |&gt;\nÂ  group_by(year, month) |&gt;\nÂ  write_dataset(\"nyc-taxi-credit\")\n\nIn the example, the input is 1.7 billion rows (70GB), output is 500 million (15GB). Takes 3-4 mins.\n\nUser-defined functions\n\n\nregister_scalar_function - accepts base R functions inside your function"
  },
  {
    "objectID": "qmd/apache-arrow.html#sec-apache-arrow-cloud",
    "href": "qmd/apache-arrow.html#sec-apache-arrow-cloud",
    "title": "Arrow",
    "section": "Cloud",
    "text": "Cloud\n\nAccess files in Amazon S3 (works for all file types)\ntaxi_s3 &lt;- read_parquet(\"s3://ursa-labs-taxi-data/2013/12/data.parquet)\n# multiple files\nds_s3 &lt;- open_dataset(s3://ursa-labs-taxi-data/\", partitioning = c(\"year\", \"month\"))\n\nAs of 2021, only works for Amazon uri\nread_parquet can take a minute to load\nYou can see the folder structure in the read_parquet S3 uri\nExample Query\n# over 125 files and 30GB\nds_s3 %&gt;%\nÂ  Â  filter(total_amount &gt; 100, year == 2015) %&gt;%\nÂ  Â  select(tip_amount, total_amount, passenger_count) %&gt;%\nÂ  Â  mutate(tip_pct = 100 * tip_amount / total_amount) %&gt;%\nÂ  Â  group_by(passenger_count) %&gt;%\nÂ  Â  summarize(median_tip_pct = median(tip_pct),\nÂ  Â  Â  Â  Â  Â  Â  n = n()) %&gt;%\nÂ  Â  print() # is this necessary?\n\nPartitioning allowed Arrow to bypass all files that werenâ€™t in year 2015 directory and only perform calculation on those files therein.\n\n\nAccess Google Cloud Storage (GCS)\n\nDocs"
  },
  {
    "objectID": "qmd/apache-spark.html#sec-apache-spark-misc",
    "href": "qmd/apache-spark.html#sec-apache-spark-misc",
    "title": "Spark",
    "section": "Misc",
    "text": "Misc\n\nPackages\n\n{sparklyr}\n{sparklyr.nested} - Extension for nested data\n{sparkxgb} - XGBoost in Spark\n{sparklyr.flint} - Time Series Computation\n{multiplyr}\n\nAlternative option for data with &gt; 10M rows and you only have access to one machine\nSpreads data over local cores\n\n\nResources\n\n2022 sparklyr cheatsheet\nMastering Apache Spark with R\n\nPotential issues with transferring R code to Spark\n\nCanâ€™t subset reference dataframe columns\n# doesn't work\nresults &lt;- your_function(products_ref$order_id)\n\n# Have to collect the column variable and bring the data into the R environ\norder_id &lt;- product_ref %&gt;%\nÂ  Â  Â  Â  Â  Â  Â  Â  select(order_id) %&gt;%\nÂ  Â  Â  Â  Â  Â  Â  Â  collect()\nresults &lt;- your_function(order_id)\n\nAlso becomes an issue within nested functions. Might need to add collect( ) to the end of the inner function code.\nCluster Manager - Responsible to allocate the resources across the Spark Application. This architecture has several advantages. Each application run is isolated from other application run, because each gets its own executor process. Driver schedules its own tasks and executes it in different application run on different JVM. Downside is, that data can not be shared across different Spark applications, without being written (RDD) to a storage system, that is outside of this particular application\nDrill - A lightweight version of Spark (no ML stuff, just query). It was orphaned after Dec 19 after some company bought some other company. Can be used through {sergeant} PKG.\nHive - Apache Hive is a data warehouse open-source project which allows querying of large amounts of data. Like SQL it uses an easy-to-understand language called Hive QL\nHive partitions - Used to split the larger table into several smaller parts based on one or multiple columns (partition key, for example, date, state e.t.c). The hive partition is similar to table partitioning available in SQL server or any other RDBMS database tables.\n\nEmbeds field names and values in path segments, such as â€œ/year=2019/month=2/ or part_.parquetâ€. HDFS - Hadoop Distributed File System is a data storage system used by Hadoop. It provides flexibility to manage structured or unstructured data. Storing large amounts of financial transactional data in an HDFS to query using Hive QL.\n\nKafka - More complex to work with than NiFi as it doesnâ€™t have a user interface (UI), mainly used for real-time streaming data. It is a messaging system first created by LinkedIn engineers. Streaming real-time weather events using Kafka\nSpark integration services for the Cloud: AWS EMR, Azure HDInsight, GCP Dataproc.\nIf Spark is used with Databricks, another particularly interesting format is the delta format which offers automatic optimisation tools."
  },
  {
    "objectID": "qmd/apache-spark.html#sec-apache-spark-procov",
    "href": "qmd/apache-spark.html#sec-apache-spark-procov",
    "title": "Spark",
    "section": "Process Overview",
    "text": "Process Overview\n\nDistributed computation is performed by\n\nConfiguring - Requests the cluster manager for resources: total machines, memory, and so on.\nPartitioning - Splits the data available for computation across each compute instance.\nExecuting - Means running an arbitrary transformation over each partition.\nShuffling - Redistributes data to the correct machine.\nCaching - Preserves data in memory across different computation cycles.\nSerializing - Transforms data to be sent over the network to other workers or back to the driver node.\n\n\nSpark driver (aka master node) - Orchestrates the execution of the processing and its distribution among the Spark executors (Also called slave nodes).\n\nNot necessarily hosted by the computing cluster, it can be an external client.\n\nCluster Manager - Manages the available resources of the cluster in real time.\n\nWith a better overview than the Spark application, it allocates the requested resources to the Spark driver if they are available\ne.g.Â Standalone cluster manager (Sparkâ€™s own manager that is deployed on private cluster), Apache Mesos, Hadoop YARN, or Kubernetes\n\n\nExample (process)\ndata &lt;- copy_to(sc,Â \n  data.frame(id = c(4, 9, 1, 8, 2, 3, 5, 7, 6)),\n             repartition = 3)\n\ndata %&gt;% arrange(id) %&gt;% collect()\n\nThe numbers 1 through 9 are partitioned across three storage instances.\nEach worker node loads this implicit partition; for instance, 4, 9, and 1 are loaded in the first worker node.\nA task is distributed to each worker to apply a transformation to each data partition in each worker node.\nThe task executes a sorting operation within a partition.\n\nTasks are denoted as f(x) in the Spark webui DAG\n\nThe result is then shuffled to the correct machine to finish the sorting operation across the entire dataset, which completes a stage.\n\nThe sorted results can be optionally cached in memory to avoid rerunning this computation multiple times.\n\nFinally, a small subset of the results is serialized, through the network connecting the cluster machines, back to the driver node to print a preview of this sorting example.\n\n\nSpark Job\n\n\nStages are a set of operations that Spark can execute without shuffling data between machines\n\nOften delimited by a data transfer in the network between the executing nodes\ne.g.Â A join operation between two tables.\n\nTask: A unit of execution in Spark that is assigned to a partition of data.\n\nSpark Transformation types:\n\n\nShuffle: A redistribution of the data partitions in the network between the executing nodes\n\nIsnâ€™t usually 1:1 copying as hash keys are typically used to determine how data is grouped by and where to copy. This process usually means data is copied through numerous executors and machines.\nComputationally expensive\n\nSpeed Factors: Data size and latency within cluster\n\nWide transformations require shuffling\n\n\nLazy Evaluation - Triggers processing only when a Spark action is run and not a Spark transformation\n\nAllows Spark to prepare a logical and physical execution plan to perform the action efficiently."
  },
  {
    "objectID": "qmd/apache-spark.html#sec-apache-spark-setup",
    "href": "qmd/apache-spark.html#sec-apache-spark-setup",
    "title": "Spark",
    "section": "Set-Up",
    "text": "Set-Up\n\nJava\n\nInstall Java or see which version you have installed\nSee which versions of Spark are compatible with your version of Java\n\nVersions that available to install - spark_available_versions()\nVersions that have been installed - spark_installed_versions()\nInstall\n\nspark_install has a version argument if you want a specific version\nspark_install(version = \"3.1\")\n\nVersion 3.1 is compatible with Java 11\nUninstall a version - spark_uninstall(\"2.2.0\")\nConnection\n\nConnect\nsc &lt;- spark_connect(\n  master = \"local\",\n  version = \"3.1.1\",\n  config = conf)\n\nsc is the connection object\nlocal means that the cluster is set-up on your local machine\nconf is the list of configuration parameter:value pairs (see below, Optimization &gt;&gt; Configuration)\n\n\nDisconnect from cluster - spark_disconnect_all()"
  },
  {
    "objectID": "qmd/apache-spark.html#sec-apache-spark-errors",
    "href": "qmd/apache-spark.html#sec-apache-spark-errors",
    "title": "Spark",
    "section": "Errors",
    "text": "Errors\n\njava.lang.OutOfMemoryError: Java heap space\n\nout-of-memory (OOM) error\nSolutions:\n\nadd more executor memory\nrebalance executor memory through parameters\n\n\nService 'sparkDriver' failed after 16 retries\n\nWorker node networking error\nSee article for details on implementing these solutions\nSolutions\n\nExporting the corresponding SPARK_LOCAL_IP environment variable that is loaded when the JVM is initialized on worker nodes\nSetting the corresponding spark.driver.bindAddress configuration in the SparkSession (note that this approach will override SPARK_LOCAL_IP environment variable)\nUpdating the hostname on your local machine\nCheck whether you have enabled a Virtual Private Network (VPN) â€” or any other tool that may be affecting the networking on your local machine â€” as this may sometimes affect binding addresses\n\n\n\n\nData Skew\n\nThe OOM error can occur on joins of very large tables or very large table + medium table with skewed data because of unevenly distributed keys\n\nWhen a key (also see shuffle in Process Overview &gt;&gt; Spark Transformation types above) has considerably more volume than the others, this â€œHOT KEYâ€ causes a data skew\nWhen they say â€œskewedâ€, they mean something closer to imbalanced. Partitions and join/group_by variables are usually discrete and this seems like a imbalanced categorical variable issue\n\nExamples:\n\nA sales analysis that requires a breakdown by city. The cities with more populations like New York, Chicago, San Fransico have a higher chance to get data skew problems.\nA table is partitioned by month and it has many more records in a few months than all the rest\nToo many null values in a join or group-by key\n\nOther symptoms\n\nFrozen stages and tasks\nLow utilization of CPU\ne.g.Â most tasks finish within a reasonable amount of time, only to have one task take forever\n\n\nIn the webui, the bottom bar is a task that takes substantially more time to complete than the other tasks\n\n\nSolution: Adaptive Query Execution (AQE)\n\nImplemented in Spark 3.0 and default in 3.2.\n\nMay not provide the most optimized settings for edge cases\n\nSee Deep Dive into Handling Apache Spark Data Skew\n\nDetails on diagnosing, and manually tuning settings (includes salting code and links)\n\n\n\nVideo\nSee Five Tips to Fasten Skewed Joins in Apache Spark for explainer on the process\nSet spark.sql.adaptive.enabled = TRUE in the config\nConfiguration parameters\n\nspark.sql.adaptive.skewJoin.enabled : This boolean parameter controls whether skewed join optimization is turned on or off. Default value is true.\nspark.sql.adaptive.skewJoin.skewedPartitionFactor: This integer parameter controls the interpretation of a skewed partition. Default value is 5.\nspark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes: This parameter in MBs also controls the interpretation of a skewed partition. Default value is 256 MB.\nA partition is considered skewed when both (partition size &gt; skewedPartitionFactor * median partition size) and (partition size &gt; skewedPartitionThresholdInBytes) are true.\n\nIssues\n\nCannot handle â€˜Full Outer Joinâ€™\nCannot handle skewedness on both input datasets\n\nCan handle skew only in the left dataset in the Left Joins category (Outer, Semi and Anti)\nCan handle skew in the right dataset in the Right Joins category\n\n\n\nSolution: Use broadcast joins (See Optimization &gt;&gt; Shuffling &gt;&gt; Broadcast Joins)\n\nFor inner joins, See Optimization &gt;&gt; Shuffling &gt;&gt; Iterative Broadcast Joins\n\nSolution: â€œSaltâ€ the join key(s)\n\nSee video (havenâ€™t found any good step-by-step resources)\nFor the largest table, you concantenate the join variable with â€œ_â€ + random number or letter\n\nMay be better to create a new column with the salted join key so it can be deleted later\nI think the range of the random numbers might need to be the number of threads youâ€™re using but it may not matter as long as its larger than the cardinality of your join variable\n\nFor the other table(s) involved in the join, each join variable value should have the same range of numbers as itâ€™s counterpart in the largest table. So rows will essentially be replicated.\n\nThis increases the size of the table.\n\nIt might increase it to the size of the largest table as all other column values get replicated.\n\n\nJoin tables then perform calculations (e.g.Â group_by(join_id) %&gt;% count)\nDelete the join column or remove the concantenated part\n\nSolution: Salted Sort Merge Join\n\nNotes from\n\nFive Tips to Fasten Skewed Joins in Apache Spark\n\nAjay Gupta (says contact him for code snippets)\n\n\nAlso see Deep Dive into Handling Apache Spark Data Skew\n\nDetails on diagnosing, and manually tuning settings (includes salting code and links)\n\nUseful when joining a large skewed dataset with a smaller non-skewed dataset but there are constraints on the executorâ€™s memory\nCan be used to perform Left Join of smaller non-skewed dataset with the larger skewed dataset which is not possible with Broadcast Hash Join even when the smaller dataset can be broadcasted to executors\nIssues\n\nCannot handle Full Outer Join\nCannot handle skewness on both input datasets\n\nCan handle skew only in the left dataset in the Left Joins category (Outer, Semi and Anti)\nCan handle skew only in the right dataset in the Right Joins category\n\n\nHave to turn off the â€˜Broadcast Hash Joinâ€™ approach. This can be done by setting â€˜spark.sql.autoBroadcastJoinThresholdâ€™ to -1\nSimilar to â€˜Iterative Broadcast Hashâ€™ Join (Optimization &gt;&gt; Shuffling &gt;&gt; Iterative Broadcast Joins)\nProcess Option 1\n\nAn additional column â€˜salt keyâ€™ is introduced in one of the skewed input datasets.\nA number is randomly assigned from a selected range of salt key values for the â€˜salt keyâ€™ column to every record\nA for-loop is initiated on salt key values in the selected range. For every salt key value:\n\nFIlter the salted input dataset for the iterated salt key value\nJoin the salted filtered input dataset with the other unsalted input dataset to produce a partial joined output.\n\nTo produce the final joined output, all the partial joined outputs are combined together using the Union operator.\n\nProcess Option 2\n\n\nAn additional column â€˜salt keyâ€™ is introduced in one of the skewed input datasets.\nA number is randomly assigned from a selected range of salt key values for the â€˜salt keyâ€™ column to every record\nA for-loop is initiated on salt key values in the selected range. For every salt key value:\n\nThe second non skewed input dataset is enriched with the current iterated salt key value by repeating the the same value in the new â€˜saltâ€™ column to produce a partial salt enriched dataset.\n\nAll these partial enriched datasets are combined using the Union operator to produce a combined salt enriched dataset version of the second non-skewed dataset.\nThe first skewed salted dataset is Joined with the second salt enriched dataset to produce the final joined output\n\nProcess Option 3\n\nSolution: Broadcast MapPartitions Join\n\nOnly method to handle a skewed â€˜Full Outer Joinâ€™ between a large skewed dataset and a smaller non-skewed dataset.\n\nsupports all type of Joins and can handle skew in either or both of the dataset\n\nIssues\n\nRequires considerable memory on executors.\n\nLarger executor memory is required to broadcast the smaller input dataset and to support intermediate in-memory collection for manual Join provision.\n\n\nThe smaller of the two input dataset is broadcasted to executors while the Join logic is manually provisioned in the â€˜MapPartitionsâ€™ transformation which is invoked on the larger non-broadcasted dataset."
  },
  {
    "objectID": "qmd/apache-spark.html#sec-apache-spark-opt",
    "href": "qmd/apache-spark.html#sec-apache-spark-opt",
    "title": "Spark",
    "section": "Optimization",
    "text": "Optimization\n\n\nUse event timeline in the UI to find bottlenecks\n\n\nConfiguration\n\nValues of parameters can be viewed in the UI under the Environment tab\nConfiguration object used as input to the config arg in spark_connect\nMisc\n\nRecommended to request significantly more memory for the driver than the memory available over each worker node.\nIn most cases, you will want to request one core per worker.\n\nParameters\n\ncores.local: (local mode) number of cores you want used by the cluster\nshell.driver-memory: amount RAM you want used by the driver node (or cluster if local)\nspark.memory.fraction: percentage of that RAM you want dedicated to jobs that will be run in the cluster and storing RDDs\n\nDefault: 0.60\nProbably want to hold some back for monitoring, administering the cluster, etc.\n\nspark.memory.storageFraction: percentage of RAM you want specifically for storing RDDs\n\nIf youâ€™re mostly doing things like quickly filtering and retrieving subsets, then you can set this equal to your spark.memory.fraction\nSpark will borrow execution memory from storage and vice versa if needed and if possible; therefore, in practice, there should be little need to tune the memory settings\n\nspark.executor.memory: percentage of RAM you want specifically for executing actions\n\nExample (method 1)\n conf &lt;- list()\n conf$`sparklyr.cores.local` &lt;- 6\n conf$`sparklyr.shell.driver-memory` &lt;- \"6G\"\n conf$`spark.memory.fraction` &lt;- 0.9\nExample (method 2)\n# Initialize configuration with defaults\nconfig &lt;- spark_config()\n\n# Memory\nconfig[\"sparklyr.shell.driver-memory\"] &lt;- \"2g\"\n\n# Cores\nconfig[\"sparklyr.connect.cores.local\"] &lt;- 2\nExample (method 3)\ndefault:\n  sparklyr.shell.driver-memory: 2G\n\nAllows you to have cleaner code\nCreate a config.yml file in the working dir or a parent dir. Then thereâ€™s no need to specify the config parameter when creating a connection\n\nYou can also specify an alternate configuration filename or location by setting the file parameter in spark_config()\nCan also change the default configuration by changing the value of the R_CONFIG_ACTIVE environment variable. See the GitHub rstudio/config repo for additional information\n\n\n\n\n\nPartitioning\n\nIdeally, Spark organises one thread per task and per CPU core\nIncreasing partitions can help performance but donâ€™t create too many partitions\n\nThere is a deterioration of I/O performance due to the operations performed by the file system (e.g.Â opening, closing, listing files), which is often amplified with a distributed file system like HDFS.\nScheduling problems can also be observed if the number of partitions is too large.\n\nTypes\n\nImplicit: type of partitioning already present in the storage system\n\ne.g.Â directory tree of Arrow files that have been partitioned; table schema in a relational db (?)\n(default) itâ€™s more effective to run computations where the data is already located\n\nExplicit: Manaully setting the number of partitions (i.e.Â repartitioning)\n\nUseful when when you have many more or far fewer compute instances than data partitions. In both cases, it can help to repartition data to match your cluster resources\nVarious data functions, like spark_read_csv() or copy_to(), already support a repartition parameter to request that Spark repartition data appropriately\n\nAlso,spark_df %&gt;% sdf_repartition(4)(e.g.Â repartitions a Spark DataFrame to have 4 partitions)\n\nShould be shown as the spark.sql.shuffle.partitions parameter in the webui or config\n\nsdf_coalesce(x, partitions) can reduce the number of partitions without shuffling\n\n\n\nCheck number of partitions for a data object\nsdf_len(sc, 10) %&gt;% sdf_num_partitions()\n&gt;&gt; 2\n\nsdf_len creates a spark df with a sequence length (e.g.Â 10 in this toy example)\n\nI think this a numeric vector in R but they just call everything a df in Spark\n\nI think the default is set the partitions to the number of cores/instances that are being used in the cluster\n\n\n\n\nCaching\n\nFunctions like spark_read_parquet() or copy_to() load/cache data objects into memory (aka resilient distributed dataset (RDD))\n\nSave data from being lost when a worker fails\ntbl_uncache(sc, \"iris\") can be used to free up memory by removing a RDD from cache\n\nInspect RDDs through the UI (Storage Tab &gt;&gt; )\n\nCheckpoints\n\nsdf_checkpoint caches a spark dataframe or an object thats coerceable to a spark dataframe\n\nUseful during analysis after computationally intensive calculations so that you donâ€™t have to repeat them\n\nThere is a cost to writing these results to disk, so you might need to test whether the calculation is intensive enough for checkpointing to be more efficient.\n\nspark_set_checkpoint_dir() spark_get_checkpoint_dir() are for setting and getting the location of the cache directory\nCache is deleted after the end of the spark session\n\n\n\n\n\nShuffling\n\n\nSpark stores the intermediate results of a shuffle operation on the local disks of the executor machines, so the quality of the disks, especially the I/O quality, is really important.\n\nThe use of SSD disks will significantly improve performance for this type of transformation\n\nSort Merge (Default) requires full shuffling of both data sets via network which is heavy task for Spark\n\nOther Joins: Docs\n\n\n\nBroadcast Joins\n\n** Think this only needs to be explicitly specified for Spark 2.x. Spark 3.0 has Adaptive Query Execution (AQE) which uses the optimal type of join automatically. **\n\nVideo\nMay need Databricks (runtime &gt;7.0) to get AQE and set spark.sql.adaptive.enabled = TRUE in the config\n\nUseful when one df is orders of magnitude smaller than the one youâ€™re joining it too\n\nIt pushes one of the smaller DataFrames to each of the worker nodes to reduce shuffling the bigger DataFrame\nBy duplicating the smallest table, the join no longer requires any significant data exchange in the cluster apart from the broadcast of this table beforehand\nIf you are joining two data sets and both are very large, then broadcasting any table would kill your spark cluster and fails your job.\n\nCan be set according to difference in size in bytes using spark.sql.autoBroadcastHashJoin (not sure if this is available in {sparklyr}\n\nDefault = 10 MB difference in sizes\nSetting to -1 means spark always uses a broadcast join (maybe useful with sufficient memory available)\n\nAdjust (increase?) spark.sql.autoBroadcastJoinThreshold to get smaller tables broadcasted\n\nShould be done to ensure sufficient driver and executor memory\nSetting to -1 disables it\n\nIssues\n\nNot Applicable for Full Outer Join.\nFor Inner Join, executor memory should accommodate at least smaller of the two input dataset. (See Iterative Broadcast Joins below as a potential solution)\nFor Left , Left Anti and Left Semi Joins, executor memory should accommodate the right input dataset as the right one needs to be broadcasted.\nFor Right , Right Anti and Right Semi Joins, executor memory should accommodate the left input dataset as the left one needs to be broadcasted.\nThere is also a considerable demand of execution memory on executors based on the size of broadcasted dataset.\n\nExample:\nsdf_len(sc, 10000) %&gt;%\n  sdf_broadcast() %&gt;%\n  left_join(sdf_len(sc, 100))\n\n\n\nIterative Broadcast Joins\n\n** Limited to Inner Joins only**\nAn adaption of â€˜Broadcast Hashâ€™ join in order to handle larger skewed datasets. It is useful in situations where either of the input dataset cannot be broadcasted to executors. This may happen due to the constraints on the executor memory limits.\nBreaks downs one of the input data set (preferably the smaller one) into one or more smaller chunks thereby ensuring that each of the resulting chunk can be easily broadcasted.\n\nOutputs from these multiple joins is finally combined together using the â€˜Unionâ€™ operator to produce the final output.\n\nProcess\n\n\nAssign a random number out of the desired number of chunks to each record of the Dataset in a newly added column, â€˜chunkIdâ€™.\nA for-loop is initiated to iterate on chunk numbers. For each iteration:\n\nRecords are filtered on the â€˜chunkIdâ€™ column corresponding to current iteration chunk number.\nThe filtered dataset, in each iteration, is then joined with the unbroken other input dataset using the standard â€˜Broadcast Hashâ€™ Join to get the partial joined output.\nThe partial joined output is then combined with the previous partial joined output.\n\nAfter the loop is exited, one would get the overall output of the join operation of the two original datasets\n\n\n\n\n\nSerialization\n\nKryo Serializer that can provide performance improvements over the default Java Serializer\nconfig &lt;- spark_config()\nconfig$spark.serializer &lt;- \"org.apache.spark.serializer.KryoSerializer\"\n\n\n\nUnion Operator\n\ne.g.Â df = df1.union(df2).union(df3) (article)\nIf users donâ€™t use union on entirely different data sources, union operators will face a potential performance bottleneck â€” Catalyst isnâ€™t â€œsmartâ€ to identify the shared data frames to reuse.\nEnsure rows follow the same structure:\n\nThe number of columns must be identical.\nColumn data types should match\nThe column names should follow the same sequence for each data frame. Nevertheless, thatâ€™s not mandatory.\n\nThe first data frame will be chosen as the default for the column name. So mixing order can potentially cause an undesired result.\nSpark unionByName is intended to resolve this issue.\n\n\nunionAll is an alias to union that doesnâ€™t remove duplication. Weâ€™d need to add distinct after performing union to perform SQL-like union operations without duplication.\nTypical Use Case\n\n\nâ€œFact 1â€ and â€œFact 2â€ are 2 large tables that have been joined and split (e.g.Â filtering) into subsets. Each subset uses different transformations, and eventually, we combine those 4 data frames into the final one.\nSince Catalyst doesnâ€™t recognize these are the same dfs, it performs the two big table join four times!\n\nSolutions\n\nDouble the number of executors to run more concurrent tasks\nHint to Catalyst and let it reuse the joined data frame from memory through caching\n\nExample: Caching\n## Perform inner join on df1 and df2\ndf = df1.join(df2, how=\"inner\", on=\"value\")\n\n## add cache here\ndf.cache()\n\n## Split the joined result into two data frames: one only contains the odd numbers, another one for the even numbers\ndf_odd = df.filter(df.value % 2 == 1)\ndf_even = df.filter(df.value % 2 == 0)\n\n## Add a transformation with a field called magic_value which is generated by two dummy transformations.\ndf_odd = df_odd.withColumn(\"magic_value\", df.value+1)\ndf_even = df_even.withColumn(\"magic_value\", df.value/2)\n\n## Union the odd and even number data frames\ndf_odd.union(df_even).count()\n\nReduces the plan from 50 to 32 stages"
  },
  {
    "objectID": "qmd/apache-spark.html#sec-apache-spark-dating",
    "href": "qmd/apache-spark.html#sec-apache-spark-dating",
    "title": "Spark",
    "section": "Data Ingestion",
    "text": "Data Ingestion\n\nSend data from R to the Spark Cluster\niris_ref &lt;- dplyr::copy_to(sc, iris_df, \"iris_tbl\")\n\niris_ref\n\nList object in R environment that is a reference to iris_tbl in spark\nWork with this object in R like normal\n\niris_df\n\nData object in the R environment that you want to load into spark\n\niris_tbl\n\nName that you want the spark table to have in the spark cluster\n\n\nManipulate ref objs and create new object in spark and R\nfull_products &lt;- \n  dplyr::copy_to(sc, products_ref %&gt;%\n      inner_join(departments_ref) %&gt;%\n      inner_join(aisles_ref),\n      name = \"product_full\",\n      overwrite = TRUE)\n# or use the spark object name to create an ref object\ntbl(sc, \"product_full\") %&gt;% select(var_name)\n\ncopy_to also can be used to create new objects in spark from reference objects in R\n\nproducts_ref, departments_ref, and aisles_ref are reference objects for tables inside spark\nproducts_full is the name we want for the new table inside spark\nfull_products will be the reference object for products_full\n\n\nRead csv file into spark cluster\nproducts_ref &lt;- spark_read_csv(sc, \"products_tbl\", \"folder/folder/products.csv\")\n\nproducts_ref is the reference object\nsc is the connection object\nproducts_tbl is the name we want for the table inside spark\nLast part is the path to the file\n\nView tables in cluster - src_tbl(sc)\nBring Data from the Cluster into R\npred_iris &lt;- \n  ml_predict(model_iris, test_iris) %&gt;%\n    dplyr::collect()\n\ncollect brings the predictions data into the R environment"
  },
  {
    "objectID": "qmd/apache-spark.html#sec-apache-spark-func",
    "href": "qmd/apache-spark.html#sec-apache-spark-func",
    "title": "Spark",
    "section": "Functions",
    "text": "Functions\n\nnrow \\(\\rightarrow\\) sdf_nrow(ref_obj)\n\nUsing nrow on a reference object with output a NA\n\nquantile \\(\\rightarrow\\) percentile\nExample:\ncars %&gt;%\n  summarize(mpg_percentile = percentile(mpg, array(0, 0.25, 0.5, 0.75, 1))) %&gt;%\n  mutate(mpg_percentile = explode(mpg_percentile))\n\ncars is a reference object\npercentile returns a list so explode (unnesting/flattening) coerces it into a vector\n\nI found an example of â€œexplodeâ€ in {sparklyr} docs but no function documentation. Function also available in {sparkr} and {sparklyr.nested}\n\n\ncor \\(\\rightarrow\\) ml_corr(ref_obj)\n\nPearson correlation on a reference object"
  },
  {
    "objectID": "qmd/apache-spark.html#sec-apache-spark-spsql",
    "href": "qmd/apache-spark.html#sec-apache-spark-spsql",
    "title": "Spark",
    "section": "Spark SQL",
    "text": "Spark SQL\n\nGet sql query from a dplyr operations\ncount(ref_obj) %&gt;%\n  show_query()\n\nCan use mutate, group_by, summarize, across, etc.\n\nUse SQL to query spark object\nDBI::dbGetQuery(\n  conn = sc,\n  statement = \"SELECT COUNT(*) AS 'n' FROM `full_product`\"\n)\n\nWhere full_product is the name of the table in the cluster"
  },
  {
    "objectID": "qmd/apache-spark.html#sec-apache-spark-mod",
    "href": "qmd/apache-spark.html#sec-apache-spark-mod",
    "title": "Spark",
    "section": "Modeling",
    "text": "Modeling\n\nPartition, Register, Create Reference Object\n\nsdf_partition creates partition reference object for train, test sets\nsdf_register takes the partition reference object and â€œregistersâ€ the splits in the cluster\n\nNot sure if partition_iris is in the cluster, but I think when the â€œregisteringâ€ occurs is when the splitting takes place inside the cluster and separate objects are created.\n\ntbl creates a separate reference object for train data in R\npartition_iris &lt;- sdf_partition(iris_ref, training = 0.8, testing = 0.2)\nsdf_register(partition_iris, c(\"spark_training_iris\", \"spark_testing_iris\"))\ntidy_iris &lt;- \n  tbl(sc, \"spark_iris_training\") %&gt;%\n      select(Species, Petal_Length, Petal_Width)\n\nTrain Basic Spark ML model\nmodel_iris &lt;- \n  tidy_iris %&gt;%\n    ml_decision_tree(response = \"Species\", \n                     features = c(\"Petal_Width\", \"Petal_Length\"))\ntest_iris &lt;- tbl(sc, \"spark_testing_iris\")\n\ntest_iris and tidy_iris are reference objects\nI donâ€™t think model_iris is a reference object. Itâ€™s a spark class objecti but I believe itâ€™s in your environment and not on the cluster\n\nPredict\npred_iris &lt;- \n  ml_predict(model_iris, test_iris) %&gt;%\n    dplyr::collect()\nSummary - summary(model)\nModels\n\nOLS - ml_linear_regression(cars, mpg ~ hp)\n\nWhere cars is a reference object"
  },
  {
    "objectID": "qmd/apache-spark.html#sec-apache-spark-stream",
    "href": "qmd/apache-spark.html#sec-apache-spark-stream",
    "title": "Spark",
    "section": "Streaming",
    "text": "Streaming\n\nIntro to Spark Streaming with sparklyr\nSpark server will continue to run and detect changes/updates\n\nTo files in a folder\nTo Apache Kafka stream\n\nStart a stream - stream_read_csv(sc, \"&lt;input folder path&gt;/\")\n\nMake sure path to folder (e.g.Â â€œstream_input/â€) ends with a backslash\n\nWrite output of processed stream - stream_write_csv(&lt;processed_stream_obj&gt;, \"&lt;output folder path&gt;/\")\nStop stream\n\nMight have to stop connection to spark server with spark_disconnect_all()\n\nExample:\nstream &lt;- stream_read_csv(sc, \"stream_input/\")\nstream %&gt;%\nÂ  Â  select(mpg, cyl, disp) %&gt;%\nÂ  Â  stream_write_csv(\"stream_output/\""
  },
  {
    "objectID": "qmd/apache-spark.html#sec-apache-spark-mon",
    "href": "qmd/apache-spark.html#sec-apache-spark-mon",
    "title": "Spark",
    "section": "Monitoring",
    "text": "Monitoring\n\nSparklint\n\n\nGithub\nProvides advance metrics and better visualization about your spark applicationâ€™s resource utilization\n\nVital application life time stats like idle time, average vcore usage, distribution of locality\nView task allocation by executor\nVCore usage graphs by FAIR scheduler group\nVCore usage graphs by task locality\nCurrent works-in-progress (WIP)\n\nFind rdds that can benefit from persisting\nAutomated report on application bottle neck\nOpportunity to give the running application real-time hints on magic numbers like partitions size, job submission parallelism, whether to persist rdd or not\n\n\n\nwebui\n\nspark_web(sc)\n\nWhere sc is the spark connection object\n\nResources\n\nDocs: https://spark.apache.org/docs/latest/web-ui.html\nR in Spark (sections on DAGs, event timeline, and probably more)\n\nDefault URL: http:driver_host:4040\n\nSpark metrics also available through a REST API\n\nAPI Metrics\n\nSparkListener API docs: https://spark.apache.org/docs/2.4.8/api/java/org/apache/spark/scheduler/SparkListener.html\nExamples: start/end, job start/end, stage start/end, execution time, records read/written, bytes read/written, etc.\n\nMight require knowing Java to take advantage of the api\n\nExample of a â€œlistenerâ€ was written in Java but itâ€™s an API, so not why you canâ€™t use any language\n\n\nPrometheus\n\nExperimental support\n\nREST API: /metrics/executors/Prometheus\n\nConditional to â€œspark.ui.prometheus.enabled=trueâ€\n\n\n\nGraphana\n\nVisualizes metrics streaming to it \n\nExample: This sends metrics to influxdb using a Graphite â€œprotocolâ€ which can be visualized in Graphana\n\n\n\nMonitor tasks and memory usage\n\nMight be able do determine more efficient memory allocations\n\n\n\nPlugins\n\nAllow you to monitor\n\nCloud Storage\nOS metrics (important for kubernetes)\nImproved HDFS monitoring"
  },
  {
    "objectID": "qmd/apache-spark.html#sec-apache-spark-cloud",
    "href": "qmd/apache-spark.html#sec-apache-spark-cloud",
    "title": "Spark",
    "section": "Cloud",
    "text": "Cloud\n\nDatabricks\n\nMisc\n\nFree community edition available\n{sparkR} and {sparklyr} are already installed on a databricks instance\nUsing RStudio and databricks\n\nVia driver node\n\nR runtime including required packages installed on every node in the cluster while RStudio Server only runs on the driver node where also the web UI is provided\nNo network latency occurs by transferring data between R and Databricks as the JVM processes run on the same machine\nAvoids having to set up another infrastructure\nIf your Databricks cluster is constantly running and you havenâ€™t lot going on besides the interactive workloads, then this option might even be more convenient\n\nSeparate environment\n\nAvoids resource contention\nAllows you to connect to any other remote storage or compute resources if required\nFor details on this connection see RStudio and Databricks Better Together\n\n\n\nConnect\n\nsparklyr::spark_connect(method = \"databricks\")\n\nMay require some extra information on the location of the databricks spark location\n\nExample:\ndatabricks_connect_spark_home &lt;- system(\"databricks-connect get-spark-home\", intern = TRUE)\nsc &lt;- spark_connect(\nÂ  Â  method = \"databricks\",\nÂ  Â  spark_home = databricks_connect_spark_home,\nÂ  Â  config = conf\n)\n\nMonitoring\n\nDatabricks workspace provides through its UI a fairly easy and intuitive way of visualizing the run history of individual jobs\nDatabricks REST API can be used to extract you jobs data\n\ndocs\nMonitoring Databricks jobs through calls to the REST API\n\n\n\n\n\nAWS\n\nAWS Glue allows you to spin up in a couple of seconds a spark cluster up to 100 executors where each executor is 4 CPUs and 16GB RAM.\nAWS EMR is a managed service if you want a more configurable cluster\nEMR\n\nCons\n\nOver-Provisioning or under-provisioning leads to wasted resources or queued jobs\n\ne.g.Â Ad-Hoc exploration of the researchers with bursts of high usage.\n\nCanâ€™t run multiple versions of Spark simultaneously.\n\nExample: The consequence of this was a burden for migration and testing. Routinely creating new EMR clusters to test out against new dependencies. The added infrastructure burden delayed upgrades and increased the accumulation of technical debt.\n\nScattered visibility\n\nNo single place to visualize Spark job metrics, Executor/Driver Logs, and the Application UI\nDebugging executor failures due to out-of-memory was often tricky\n\nSince Spark is a distributed computational framework - failure stack traces wouldnâ€™t always be informative about the root cause since it may be a red herring.\n\n\nLack of configuration agility\n\nNo ability to change the underlying execution environment to remediate the failure reason(s) automatically\n\n\n\n\n\n\nKubernetes\n\nOptions\n\nSparkâ€™s integration with Kubernetes (non-AWS I think)\nEMR on EKS\n\nGood binding for IAM, EMRFS, and S3, and AWS-provided integrations and container images\n\n\nEMR on EKS\n\nStitchfix configuration\n\nMisc\n\nSaw a 45-55% reduction in infrastructure cost for all our Spark-based compute by switching from EMR to EMR on EKS\nTheyâ€™re running thousands of daily spark jobs, so this may not be a solution for everyone plus it would require some substantial programming expertise to implement some of this stuff\n\nCluster Autoscaler - As Spark pods are scheduled, EKS cluster nodes are added or removed based on load\n\nSet to cull nodes after an unused interval of 5 minutes and a scanning interval of 1 minute\n\nContainer images use official EMR on EKS images as base images and then add other dependencies\n\nSame cluster can run multiple versions of Spark\n\nTesting a new version only requires producing a new container image base and running an ad hoc testing job\n\nEach cluster node uses the image pre-puller pattern to cache the Spark images to improve job startup time\n\nMonitoring\n\nEKS pod events + EMR on EKS CloudWatch events\n\nEKS pod events: provide granular visibility into the setup, execution, and tear down of a job.\nEMR cloudwatch: provide high-level overview of the job state (i.e., queued, started, or finished).\n\nStream logs from S3 buckets to Flotilla for monitoring and debugging\n\nEach of the container logs comes from the driver, executor, and controller pods.\n\nMetrics\n\nCluster instrumented with Victoria Metrics to collect detailed container metrics and visualized via Grafana\n\nCan diagnose difficult shuffles, disk spills, needed or unneeded caching, mis-sized executors, poor concurrency, CPU throttling, memory usage, network throttling, and many other job execution details\n\n\nActive and Completed jobs\n\nSomething technical (see article for details)\n\n\nAdaptive Behaviors\n\nExecutor and Driver Memory\n\nIf a job fails due to Executor or Driver running out of memory (exit code 137), automatically schedule the next run with more memory. (link)\n\nOther stuff (see article for details)\n\n\n\n\n\n\nGoogle Cloud Platform (GCP)\n\nRead data from cloud storage to local spark cluster\nbucket_name &lt;- \"ndir-metis-bucket\"\npath &lt;- glue::glue(\"gs://{bucket_name}/asteroid/Asteroid_Updated.csv\")\ndf &lt;- spark_read_csv(path, sep=',', inferSchema=True, header=True)\n\nI adapted this from a pyspark example but should be similar for R"
  },
  {
    "objectID": "qmd/apis.html#sec-apis-misc",
    "href": "qmd/apis.html#sec-apis-misc",
    "title": "APIs",
    "section": "Misc",
    "text": "Misc\n\nDefinition\n\nREST API\n\nDesign questions\n\nShould the API receive the entire datapoint (e.g sensitve customer info) or just an ID for you to query in a database itself?\nWhere should the model be loaded from? Disk? Cloud? (see Production, Deployment &gt;&gt; Model Deployment Strategies)\nWhat diagnostic output should be returned along with result?\n\nUse CI/CD to unit test, rebuild, and deploy the API every time thereâ€™s a push a commit to the production branch of your repo.\nBest Practices Thread\n\nVersioning\nIDs vs UUIDs\nNested resources\nJSON API\nLet the client decide what it wants\n\nImportant to create unit tests to use before code goes into production\n\nTest all endpoints\nCheck data types\n{testthat}\n\nExample\nlibrary(testthat)\nsource(\"rest_controller.R\")\ntestthat(\"output is a probability\", {\nÂ  Â  input &lt;- list(id = 123, name = \"Ralph\")\nÂ  Â  result &lt;- make_prediction(input)\nÂ  Â  expect_gte(result, 0)\nÂ  Â  expect_lte(result, 1)\n})\n\n\nThe only difference here between GET and POST is that you canâ€™t put parameters and their values in the URL for POST. The parameters and values are passed in the request body as JSON.\n\nGET is a request for data from a server POST sends data to a server and also can receive data.\n\nAn IO-bound task spends most of its time waiting for IO responses, which can be responses from webpages, databases, or disks. For web development where a request needs to fetch data from APIs or databases, itâ€™s an IO-bound task and concurrency can be achieved with either threading or async/await to minimize the waiting time from external resources."
  },
  {
    "objectID": "qmd/apis.html#sec-apis-terms",
    "href": "qmd/apis.html#sec-apis-terms",
    "title": "APIs",
    "section": "Terms",
    "text": "Terms\n\nAsync/Await â€”Â  Unlike threading where the OS has control, with this method, we can decide which part of the code can be awaited and thus control can be switched to run other parts of the code. The tasks need to cooperate and announce when the control will be switched out. And all this is done in a single thread with the await command. (article)\nThreading â€” Uses multiple threads and takes turns to run the code. It achieves concurrency with pre-emptive multitasking which means we cannot determine when to run which code in which thread. Itâ€™s the operating system that determines which code should be run in which thread. The control can be switched at any point between threads by the operating system. This is why we often see random results with threading (article)\nBody â€” information that is sent to the server. (Canâ€™t use with GET requests.)\nEndpoint â€” a part of the URL you visit. For example, the endpoint of the URL https://example.com/predict is /predict\nHeaders â€” used for providing information (think authentication credentials, for example). They are provided as key-value pairs\nMethod â€” a type of request youâ€™re sending, can be either GET, POST, PUT, PATCH, and DELETE. They are used to perform one of these actions: Create, Read, Update, Delete (CRUD)"
  },
  {
    "objectID": "qmd/apis.html#sec-apis-meth",
    "href": "qmd/apis.html#sec-apis-meth",
    "title": "APIs",
    "section": "Methods",
    "text": "Methods\n\n\nMisc\n\nIf youâ€™re writing a function or script, you should check whether the status code is in the 200s before additional code runs.\n\nGET\n# example 1\nargs &lt;- list(key = \"&lt;key&gt;\", id = \"&lt;id&gt;\", format = \"json\", output = \"full\", count = \"2\")\napi_json &lt;- GET(url = URL, query = args)\n\n# example 2 (with headers)\nres = GET(\"https://api.helium.io/v1/dc_burns/sum\",\nÂ  Â  Â  Â  Â  query = list(min_time = \"2020-07-27T00:00:00Z\"\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  , max_time = \"2021-07-27T00:00:00Z\"),\nÂ  Â  Â  Â  Â  add_headers(`Accept`='application/json'\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  , `Connection`='keep-live'))\n\n# example 3\nget_book &lt;- function(this_title, this_author = NA){\nÂ  httr::GET(\nÂ  Â  url = url,\nÂ  Â  query = list(\nÂ  Â  Â  apikey = Sys.getenv(\"ACCUWEATHER_KEY\"),\nÂ  Â  Â  q = ifelse(\nÂ  Â  Â  Â  is.na(this_author),\nÂ  Â  Â  Â  glue::glue('intitle:{this_title}'),\nÂ  Â  Â  Â  glue::glue('intitle:{this_title}+inauthor:{this_author}')\nÂ  Â  Â  Â  )))\n}\nPOST\n# base_url from get_url above\nbase_url &lt;- \"https://tableau.bi.iu.edu/\"\nvizql &lt;- dashsite_json$vizql_root\nsession_id &lt;- dashsite_json$sessionid\nsheet_id &lt;- dashsite_json$sheetId\n\npost_url &lt;- glue(\"{base_url}{vizql}/bootstrapSession/sessions/{session_id}\")\n\ndash_api_output &lt;- POST(post_url,\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  body = list(sheet_id = sheet_id),\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  encode = \"form\",\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  timeout(300))\nExample: Pull parsed json from raw format\nmy_url &lt;- paste0(\"http://dataservice.accuweather.com/forecasts/\",\n                  \"v1/daily/1day/571_pc?apikey=\", \n                 Sys.getenv(\"ACCUWEATHER_KEY\"))\nmy_raw_result &lt;- httr::GET(my_url)\n\nmy_content &lt;- httr::content(my_raw_result, as = 'text')\n\ndplyr::glimpse(my_content) #get a sense of the structure\ndat &lt;- jsonlite::fromJSON(my_content)\n\ncontent has 3 option for extracting and converting the content of the GET output.\n\nâ€œrawâ€ output asis\nâ€œtextâ€ can be easiest to work with for nested json\nâ€œparsedâ€ is a list\n\n\nExample: json body\n\n\n\nFrom thread\nâ€œuse auto_unbox = TRUE; otherwise there are some defaults that mess with your API formatâ€\nâ€œurlâ€ is the api endpoint (obtain from api docs)\nheaders"
  },
  {
    "objectID": "qmd/apis.html#httr2",
    "href": "qmd/apis.html#httr2",
    "title": "APIs",
    "section": "{httr2}",
    "text": "{httr2}\n\nPOST\n\n\nContacts Home Assistant API and turns off a light."
  },
  {
    "objectID": "qmd/apis.html#sec-apis-plumb",
    "href": "qmd/apis.html#sec-apis-plumb",
    "title": "APIs",
    "section": "{plumber}",
    "text": "{plumber}\n\nServes R objects as an API\n3 Main Components: Function Definition, Request Type, API Endpoint\nMisc\n\nAdding `host = â€œ0.0.0.0â€ to run_pr() opens the API to external traffic\n{valve} - Auto-scales plumber APIs concurrently using Rust libraries Axum, Tokio, and Deadpool â€” similar to how gunicorn auto-scales fastapi and Flask apps\n\nCloud options for serving Plumber APIs\n\nInstall everything on an Amazon EC2 instance\nUsing a Docker image\n\nSaturn Cloud Deployments\n\nGoogle Cloud Run\nDocker/Kubernetes\n\nManaged Solutions\n\nRStudio Connect\nDigital Ocean\n\n\nLoad Testing\n\n{loadtest}\n\nTest how your API performs under various load scenarios\nOutputs tibble of various measurements\nExample:\nlibrary(loadtest)\nresults &lt;- loadtest(url = &lt;api_url&gt;, method = \"GET\", threads = 200, loops = 1000)\n\nSays simulate 200 users hitting the API 1000 times\n\n\n\nDocumentation\n\nPlumber creates an OpenAPI (aka Swagger) YAML file that documents parameters, tags, description, etc. automatically for users to know how to use your API\nAccess\n\nView webui, e.g .(http://127.0.0.1:9251/__docs__/)\n\nEdit the yaml\n\ne.g.Â (http://127.0.0.1:9251/openapi.json)\n\n\nScaling\n\nNatively can only handle 1 request at a time\n{valve} - Parallelize your plumber APIs. Redirects your plumbing for you.\n{future} - can be used to spawn more R processes to handle multiple requests\n\nResource: Rstudio Global 2021\nExample\n# rest_controller.R\nfuture::plan(\"multisession\")\n\n@* @post /make-prediction\nmake_prediction &lt;- function (req) {\nÂ  Â  future::future({Â  Â  Â  Â \nÂ  Â  Â  Â  user_info &lt;- req$body\nÂ  Â  Â  Â  df_user &lt;- clean_data(user_info) # sourced helper function\nÂ  Â  Â  Â  result &lt;- predict(model, data = df_user)\nÂ  Â  Â  Â  result\nÂ  Â  })\n}\n\n\nLogging\n\nUseful for debugging, monitoring performance, monitoring usage\nProvides data for ML monitoring to alert in case of data/model drift\n{logger}\n\nExample:\n#* @post /make-prediction\nmake_predicition &lt;- function(req) {\nÂ  Â  user_info &lt;- req$body\nÂ  Â  df_user &lt;- clean_data(user_info) # sourced helper function\nÂ  Â  result &lt;- predict(model, data = df_user)\nÂ  Â  logger::log_info(glue(\"predicted_{user_info$id}_[{result}]{style='color: #990000'}\"))\nÂ  Â  aws.s3::s3save(data.frame(id = user_info$id, result = result), ...)\nÂ  Â  result\n}\n\n\nExample: Basic Get request\n\nrest_controller.R\n#* @get /sum\nfunction(a, b) {\nÂ  Â  as.numeric(a) + as.numeric(b)\n}\n\nâ€œ/sumâ€ is an endpoint\n\nRun Plumber on rest_controller.R\nplumber::pr(\"rest_controller.R\") %&gt;%\nÂ  Â  plumber::pr_run(port = 80)\n\n80 is a standard browser port\n\nGet the sumÂ  of 1 + 2 by sending a Get request\n\nType â€œ127.0.0.1/sum?a=1&b=2â€ into your browser\nhttr::GET(\"127.0.0.1/sum?a=1&b=2\")\n\n\nExample: Basic Model Serving\n\nrest_controller.R\nsource(\"helper_functions.R\")\nlibrary(tidyverse)\n\nmodel &lt;- read_rds(\"trained_model.rds\")\n\n#* @post /make-prediction\nmake_predicition &lt;- function(req) {\nÂ  Â  user_info &lt;- req$body\nÂ  Â  df_user &lt;- clean_data(user_info) # sourced helper function\nÂ  Â  result &lt;- predict(model, data = df_user)\nÂ  Â  result\n}"
  },
  {
    "objectID": "qmd/apis.html#sec-apis-reqlib",
    "href": "qmd/apis.html#sec-apis-reqlib",
    "title": "APIs",
    "section": "{{requests}}",
    "text": "{{requests}}\n\nUse Session to make a pooled request to the same host (Video, Docs)\n\nExample\nimport pathlib\nimport requests\n\nlinks_file = pathilib.Path.cwd() / \"links.txt\"\nlinks = links_file.read_text().splitlines()[:10]\nheaders = {\"User-Agent\": \"Mozilla/5.0 (X!!; Linux x86_64; rv:89.0) Gecko/20100101 Firefox/89.0}\n\n# W/o Session (takes about 16sec)\nfor link in links:\n  response = requests.get(link, headers=headers)\n  print(f\"{link} - {response.status_code}\")\n\n# W/Session (takes about 6sec)\nwith requests.Session() as session:\n  for link in links:\n    response = session.get(link, headers=headers)\n    print(f\"{link} - {response.status_code}\")\n\nThe first way syncronously makes a get request to each URL\n\nMakes several requests to the same host\n\nThe second way reuses the underlying TCP connection, which can result in a significant performance increase.\n\n\nRetrieve Paged Results One at a Time\n\nGenerator\nfrom typing import Iterator, Dict, Any\nfrom urllib.parse import urlencode\nimport requests\n\n\ndef iter_beers_from_api(page_size: int = 5) -&gt; Iterator[Dict[str, Any]]:\n    session = requests.Session()\n    page = 1\n    while True:\n        response = session.get('https://api.punkapi.com/v2/beers?' + urlencode({\n            'page': page,\n            'per_page': page_size\n        }))\n        response.raise_for_status()\n\n        data = response.json()\n        if not data:\n            break\n\n        yield from data\n\n        page += 1\nIterate through each page of results\n&gt;&gt;&gt; beers = iter_beers_from_api()\n&gt;&gt;&gt; next(beers)\n{'id': 1,\n 'name': 'Buzz',\n 'tagline': 'A Real Bitter Experience.',\n 'first_brewed': '09/2007',\n 'description': 'A light, crisp and bitter IPA brewed...',\n 'image_url': 'https://images.punkapi.com/v2/keg.png',\n 'abv': 4.5,\n 'ibu': 60,\n 'target_fg': 1010,\n...\n}\n&gt;&gt;&gt; next(beers)\n{'id': 2,\n 'name': 'Trashy Blonde',\n 'tagline': \"You Know You Shouldn't\",\n 'first_brewed': '04/2008',\n 'description': 'A titillating, ...',\n 'image_url': 'https://images.punkapi.com/v2/2.png',\n 'abv': 4.1,\n 'ibu': 41.5,"
  },
  {
    "objectID": "qmd/association-copulas.html#sec-assoc-cop-misc",
    "href": "qmd/association-copulas.html#sec-assoc-cop-misc",
    "title": "3Â  Copulas",
    "section": "3.1 Misc",
    "text": "3.1 Misc\n\nNotes from: https://hudsonthames.org/copula-for-pairs-trading-introduction/\nAlso see\n\nForecasting, Nonlinear &gt;&gt; Misc &gt;&gt; packages, copulas\nFinance &gt;&gt; Mean Reversion Strategy or Pairs Trading\n\nPackages\n\n{{latentcor}}: semi-parametric latent Gaussian copula models\n\nUsed in finance for non-linear and tail risk qualities but currently doesnâ€™t take autocorrelation into account\nUsed to create joint distributions that can be used to describe associated â€œentitiesâ€ that may not be from the same distribution\n\nIf each entity has a different behavior, we cannot assume they follow the same distribution.\nAnd most importantly, each entity is likely to influence the others â€” we cannot assume they are independent. Take product cannibalization, for example: In retail, a successful product pulls demand away from similar items in its category.\nHence, each entity may have a different distribution. Plus, we should find a way to model their correlation, since independence is seldom feasible in most practical scenarios.\n\nNotes\n\nA copula is a multivariate distribution that can be formed from a variety of underlying distributions (e.g.Â gamma, normal, beta) with a specified correlation structure (depending on the type of copula you choose). You create one of these copulas from your data, and sample from it. These samples are used to run simulations on.\n\nExample:\n\nTake return prices from a few correlated stocks and create a copula.\nTrain a model with economic predictors and your sampled copula data as the response.\nFeed values for you economic predictors that indicate an economic state (e.g.Â recession) to your model and forecast the response to see how that group of stocks reacts.\n\n\nThe specified correlation stucture is called the â€œdependence structure.â€\n\ne.g.Â asymetrical correlation or tail correlation\n\nThe Gaussian copula is typically described as \\(\\Phi_R (\\Phi^{-1}(u_1), \\ldots, \\Phi^{-1}(u_d))\\), but each \\(u\\) is NOT a variable in your data. Each \\(u\\) is the result of feeding a variable of your data through its ECDF. That result is always a uniform random variable with 0,1 parameters, \\(\\mathcal{U}(0,1)\\), hence the â€œu.â€ So that copula definition is equivalent to \\(\\Phi_R (\\Phi^{-1}(F_1(X_i), \\ldots, \\Phi^{-1}(F_d(X_d))\\) where \\(X_i\\) is one of your data variables and \\(F_i\\) is its ECDF.\nEach ECDF of your data is a marginal distribution is often simply referred to as a â€œmarginal.â€\n{vinecopula} and {copula} have a function called pobs which feeds your data through an ecdf and scales it by (n+1). Didnâ€™t know it had to be scaled, so maybe an ecdf doesnâ€™t always output values between 0 and 1 like a cdf does."
  },
  {
    "objectID": "qmd/association-copulas.html#sec-assoc-cop-its",
    "href": "qmd/association-copulas.html#sec-assoc-cop-its",
    "title": "3Â  Copulas",
    "section": "3.2 Inverse Transform Sampling",
    "text": "3.2 Inverse Transform Sampling\n\nNotes from video\nUnderstanding IVS will help with understanding the copula mathematics\nRelationship between PDF and CDF (e.g.Â exponential distribution)\n\n\nProbability x â‰¤ 2 is\n\nthe shaded area of the PDF\nIs the output of CDF(x) where x = 2\n\nEquivalence between the PDF and CDF shown in the top integral\n\nMore general form shown in the bottom integral\n\n\nThe inverse of the CDF give you the value of x for any probability\n\n\ne.g.Â CDF-1(0.7) = 2 and CDF-1(0.5) = 0.7\nWhichever distributionâ€™s CDF-1 is used, the output of that function will be from that distribution (e.g.Â 2, 0.7)\n\nMathematically:\n\\[\nu_i \\sim \\mathcal{U}(0, 1)\\\\\nx_i = \\mbox{CDF}^-1(u_i)\n\\]\n\n\nExpression says to take a sample from a Uniform distribution, plug that into the inverse CDF, and get a sample from the that CDFâ€™s distribution\n\n\n\nExample: Gamma Distribution\ngamma1 &lt;- rgamma(1e6, shape=1)\n\nhist(gamma1, main='gamma distribution', cex.main=1.3, cex.lab=1.3, cex.axis=1.3, prob='true')\n\n# pgamma is the cdf of gamma\nu &lt;- pgamma(gamma1, shape=1)\nhist(u, main='Histogram of uniform samples from gamma CDF', cex.main=1.3, cex.lab=1.3, cex.axis=1.3, prob='true')\n\n# qgamma is the inverted cdf of gamma\ngamma_transformed &lt;- qgamma(u, shape=1)\nhist(gamma_transformed, main='Histogram of transformed gamma', cex.main=1.3, cex.lab=1.3, cex.axis=1.3,prob='true')"
  },
  {
    "objectID": "qmd/association-copulas.html#sec-assoc-cop-sklar",
    "href": "qmd/association-copulas.html#sec-assoc-cop-sklar",
    "title": "3Â  Copulas",
    "section": "3.3 Sklarâ€™s Theorem",
    "text": "3.3 Sklarâ€™s Theorem\n\nGuarantees the existence and uniqueness of a copula for two continuous random variables\nFor two random variables \\(S_1\\), \\(S_2\\) in \\([-\\infty, \\infty]\\). \\(S_1\\) and \\(S_2\\) have their own fixed, continuous CDFs, \\(F_1\\), \\(F_2\\).\n\nConsider their (cumulative) joint distribution\n\\[\nH(s_1, s_2) := P(S_1 \\leq s_1, S_2 \\leq s_2)\n\\]\nNow take the uniformly distributed quantile random variable, \\(U_1(S_1)\\), \\(U_2(S_2)\\). For every pair, \\((u_1, u_2)\\), drawn from the pairâ€™s quantile, we define the *bivariate copula, \\(C: [0,1] \\times [0,1] \\rightarrow [0,1]\\) as:\n\\[\n\\begin {align}\nC(u_1, u_2) &= P(U_1 \\leq u_1, U_2 \\leq u_2) \\\\\n            &= P(S_1 \\leq F_1^{-1}(u_1), S_1 = F_2^{-1}(u2)) \\\\\n            &= H(F_1^{-1}(u_1), F_2^{-1}(u_2))\n\\end {align}\n\\]\n\nWhere \\(F_1^{-1}\\) and \\(F_2^{-1}\\) are inverses (i.e.Â solved for S) of the marginal CDFs, \\(F_1\\) and \\(F_2\\).\nA copula is just the joint cumulative density for quantiles of a pair of random variables.\n\\(H\\) is â€œsomeâ€ function. It varies with the type of copula (see types section).\nSee Probability notebook, â€œSimulation of a random variable values of a distribution using the distributionâ€™s cdfâ€ section and bookmarks similarly named for some intuition behind whatâ€™s happening in Sklarâ€™s Theorem\n\nIf the 1st quantile is from 0 to 0.25 then randomly select some numbers in that range using U(0,1)\n\nThis is unclear to me, he might be calling every number drawn from U(0,1) a â€œquantileâ€\nBut I think because youâ€™re using quantiles itâ€™s non-linear which kind a makes sense (thinking about why quantile regression is used sometimes), so maybe he is talking about deciles, quartiles, etc.\nMaybe (see third line of this section) each quantile is treated as a separate dateset where numbers are drawn from U(0,1) and copula calculated.\n\nFind the inverse CDF of the distribution\nPlug those numbers from the 1st quantile into the inverse CDF to get the simulated values\nRepeat for other random variable\n\n\n\n\n\nThe scatter plot crosshair says of the value of the inverse CDF of variable U using input 0.1 corresponds to the value of the inverse CDF of variable V using input 0.3. The closer the points are to the y = x line the greater the association between them (like a Q-Q plot)\nMathematically the cumulative conditional probabilities shown in the scatter plot are given by taking partial derivatives of the joint inverse CDF:\n\n\nAside: taking the derivative of a (not inverse) marginal (not joint) CDF is the pdf\n\nThe copula density is defined as:\n\n\nWhich is a probability density. Larger the copula density, the denser the clump of points in the scatter plot\n\n\nCoefficients of Lower and Upper Tail Dependence: quantifies the strength of association during joint tail events for each random variableâ€™s distribution\n\nNot discernible from plots, so needs to be calculated\nUpper tail dependencies refers to the how closely two variables increase together during an extreme â€œpositiveâ€ event\n\ne.g.Â How strongly 2 stocks move together during an huge gain\nLower tail dependencies are similar except the event is in extreme â€œnegativeâ€ direction\n\nFor stocks at least, lower tail dependencies tend to be much stronger than upper tail tendencies\n\nTypes\n\nNot including the actual bivariate copula formulas because Iâ€™m not sure how the â€œHâ€ (see bivariate copula def above) works in practice (and I donâ€™t want to frustrate future me). I am including descriptions and important characteristics which should have practical applicability. See article for copula formulas.\nArchimedean\n\nParametric and uniquely determined by generator functions, Ï†, that use a parameter, Î¸\n\nÎ¸ measures how â€œcloselyâ€ the two random variables are â€œrelatedâ€, and its exact range and interpretation are different across different Archimedean copulas\nGenerators seem to act like the inverse CDFs in the bivariate copula formula\nGenerators:\n\n\nSymmetric and scalable to multiple variables, although a closed-form solution may not be available in higher dimensions\n\nElliptical\n\nSymmetric and easily extended to multiple variables\nAssumes symmetry on both upward co-moves and downward moves (i.e.Â lacks flexibility)\nGaussian - uses Gaussian inverse CDF and a correlation matrix\nStudent-t - similar as Gaussian but with degrees of freedom\n\nMixed\n\nWeighted ensemble of the copulas above\nHelps with overfitting and more finely calibrating upper and lower tail dependencies\nWeights should sum to 1\n\nOther Notes\n\nThe wording below is a bit confusing\n\nâ€œDonâ€™t haveâ€ I think means doesnâ€™t have the capability to detect or isnâ€™t sensitive to\nâ€œStronger center dependenceâ€ might mean a greater ability to detect or maybe a center dependence bias\n\nIâ€™m not even sure what a â€œcenterâ€ dependency means\n\n\nFrank and Gaussian copulas donâ€™t have tail dependencies\n\nGaussian contributed to 2008 financial crisis\n\nFrank copula has a stronger center dependence than a Gaussian (?)\nCopulas with upper tail dependence: Gumbel, Joe, N13, N14, Student-t.\nCopulas with lower tail dependence: Clayton, N14 (weaker than upper tail), Student-t.\nStudent t copula emphasizes extreme results: it is usually good for modelling phenomena where there is high correlation in the extreme values (the tails of the distribution).\n\nNote also that the correlation is symmetrical, so the strength of correlation is the same for both tails. This might be an issue for some applications.\n\n\n\nNotes\n\nDefinition\n\n\nCopulas are joint cumulative distribution functions (c.d.f.) for unit-uniform random variables\n\nProbability integral transform\n\n\nStates that we can transform any continuous random variable to a uniform one by plugging it into its own c.d.f.\nTransform a uniform random variable to any continuous random variable\n\n\nSo plugging a Uniform random variable into the quantile function which is the inverse of the cdf and outputs a continuous random variable\n\n\nGaussian Copula\n\n\nExample\n\nDefining a (generic) Copula (aka joint cdf) for two random variables\n\n\nWhere FX(x), FY(y) are cdfs of Gamma, Beta distributions respectively\n\nDefining a Gaussian Copula for these 2 random variables\n\n\nFormat: Copula = joint cdf(quantile(cdf(gamma_random_variable), cdf(gamma_random_variable))\n\nConstruct a Copula\n\nTransform the Gamma and Beta marginals into Uniform marginals via the respective c.d.f.s\nTransform the Uniform marginals into standard Normal marginals via the quantile functions\nDefine the joint distribution via the multivariate Gaussian c.d.f. with zero mean, unit variance and non-zero covariance (covariance matrix R)\n\nSample from a Copula\n\nThe bi-variate random variable has the above properties. (standard Gamma/Beta marginals with Gaussian Copula dependencies)\nSteps (reverse of the copula process)\n\nDraw a sample from a bi-variate Gaussian with mean zero, unit variance and non-zero covariance (covariance matrix R).\n\nYou now have two correlated standard Gaussian variables.\n\nTransform both variables with the standard Gaussian c.d.f. (i.e.Â plugging each into a gaussian cdf)\n\nYou now have two correlated Uniform variables. (via probability integral transform)\n\nTransform one variable with the standard Beta quantile function and the other variable with the Gamma quantile function\n\nCode (Julia)\nusing Measures\nRandom.seed!(123)\n\n# Step 1: Sample bi-variate Gaussian data with zero mean and unit variance\nmu = zeros(2)\nR = [1 0.5; 0.5 1]\nsample = rand(MvNormal(mu,R),10000)\n\n# Step 2: Transform the data via the standard Gaussian c.d.f.\nsample_uniform = cdf.(Normal(), sample)\n\n# Step 3: Transform the uniform marginals via the standard Gamma/Beta quantile functions\nsample_transformed = sample_uniform\nsample_transformed[1,:] = quantile.(Gamma(),sample_transformed[1,:])\nsample_transformed[2,:] = quantile.(Beta(),sample_transformed[2,:])\n\n\nVisuals\n\nNote: We could drop the zero-mean, unit-variance assumption on the multivariate Gaussian.\n\nIn that case we would have to adjust the Gaussian c.d.f. to the corresponding marginals in order to keep the integral probability transform valid.\nSince we are only interested in the dependency structure (i.e.Â covariances), standard Gaussian marginals are sufficient and easier to deal with\n\nExample: Gaussian Copula derived from beta and gamma vectors (i.e.Â â€œmarginalsâ€) (article)\n\nIn this example, marginal 1 and marginal 2 are sampled from the beta and gamma distributions, respectively\n# draw our data samples from 2 distributions, a beta and a gamma -Â \nbeta1 = stats.distributions.beta(a=10, b=3).rvs(1000)\ngamma1 = stats.distributions.gamma(a=1, loc=0).rvs(1000)\n\n# - we use the emprical cdf instead of beta's or gamma's cdf\n# - we do this to show that copulas can be computed regardless of the\n#Â  underlying distributions\necdf1 = ECDF(beta1)Â  Â  Â  # F(beta1) = u1\necdf2 = ECDF(gamma1)Â  Â  Â  # F(gamma1) = u2\n# small correction to remove infinities\necdf1.y[0]=0.0001\necdf2.y[0]=0.0001\n\nx1=stats.norm.ppf(ecdf1.y) # Î¦^-1(u1)\nx2=stats.norm.ppf(ecdf2.y) # Î¦^-1(u1)\n\n# Parameters of Î¦2\nmu_x = 0\nvariance_x = 1\nmu_y = 0\nvariance_y = 1\ncov=0.8\n\n# I think this is just some preprocessing to get the vectors into the correct shape for the mvn function\nX, Y = np.meshgrid(x1,x2)\npos = np.empty(X.shape + (2,))\npos[:, :, 0] = X; pos[:, :, 1] = Y\n\n#remember phi2 is just a multivariate normal CDF\nrv = stats.multivariate_normal([mu_x, mu_y], [[variance_x, cov], [cov, variance_y]])\nphi2=rv.cdf(pos)\n\nNote how Empirical CDFs are used which what youâ€™d use if you didnâ€™t know the underlying distribution of your two vectors\n\nSee Distributions &gt;&gt; Terms &gt;&gt; Empirical CDFs\n\nSteps\n\nCompute ECDFs of vectors with â€œunknownâ€ distributions\nApply Gaussian Copula formula\n\nCompute gaussian inverse CDFs for each vector\nDecide on parameter values of multivariate gaussian distribution\n\nMean, Variance, and Covariance\n\nCreate multivariate gaussian distribution\nApply multivariate gaussian CDF to the inverse CDFs of the two vectors"
  },
  {
    "objectID": "qmd/association-general.html#sec-assoc-gen-misc",
    "href": "qmd/association-general.html#sec-assoc-gen-misc",
    "title": "General",
    "section": "Misc",
    "text": "Misc\n\nAlso see\n\nEDA &gt;&gt; Correlation\nNotebook &gt;&gt; Statistical Inference &gt;&gt; Correlation\n\nE(Ï…|x)=0 is equivalent to Cov(x,Ï…)=0 or Cor(x,Ï…)=0\nA negative correlation between variables is also called anticorrelation or inverse correlation\nIndependence - Two random variables are independent if the product of their individual probability density functions equals the joint probability density function\nPartial Correlation\n\\[\n\\frac{\\mbox{Cov}(X, Y) - \\mbox{Cov}(X, Z) \\cdot \\mbox{Cov}(Y, Z)}{\\sqrt{\\mbox{Var}(X) - \\mbox{Cov}(X, Z)^2}\\cdot \\sqrt{\\mbox{Var}(Y) - \\mbox{Cov}(Y, Z)^2}}\n\\]\n\nMeasures the association (or correlation) between two variables when the effects of one or more other variables are removed from such a relationship.\n\nIn the above equation, I think itâ€™s the partial correlation between x and y given z.\n\npsych::partial.r(y ~ x - z, data)\nExample: {correlation}\nhead(correlation::correlation(mtcars, partial = TRUE))\n\n#&gt; # Correlation Matrix (pearson-method)\n\n#&gt; Parameter1 | Parameter2 |     r |         95% CI | t(30) |      p\n#&gt; -----------------------------------------------------------------\n#&gt; mpg        |        cyl | -0.02 | [-0.37,  0.33] | -0.13 | &gt; .999\n#&gt; mpg        |       disp |  0.16 | [-0.20,  0.48] |  0.89 | &gt; .999\n#&gt; mpg        |         hp | -0.21 | [-0.52,  0.15] | -1.18 | &gt; .999\n#&gt; mpg        |       drat |  0.10 | [-0.25,  0.44] |  0.58 | &gt; .999\n#&gt; mpg        |         wt | -0.39 | [-0.65, -0.05] | -2.34 | &gt; .999\n#&gt; mpg        |       qsec |  0.24 | [-0.12,  0.54] |  1.34 | &gt; .999\n#&gt; \n#&gt; p-value adjustment method: Holm (1979)\n#&gt; Observations: 32\n\nVisualization\n\npacman::p_load(see, ggraph)\ncorrelation::correlation(mtcars, partial = TRUE) |&gt; \n  plot()\n\nResources\n\nDealing with correlation in designed field experiments: part I\n\nExcellent tutorial on partial, joint correlations in block design\n\nppcor pkg: An R Package for a Fast Calculation to Semi-partial Correlation Coefficients\n\nExplainer for semi-partial, partial correlation\n\n\nAlso see notebook for a method using regression models"
  },
  {
    "objectID": "qmd/association-general.html#sec-assoc-gen-cont",
    "href": "qmd/association-general.html#sec-assoc-gen-cont",
    "title": "General",
    "section": "Continuous",
    "text": "Continuous\n\nSpearmanâ€™s Rank\n\\[\n\\rho = 1 - \\frac{6\\sum_i d_i^2}{n(n^2-1)}\n\\]\n\n\\(d_i\\): The difference in ranks for the ith observation\nMeasures how well the relationship between the two variables can be described by a monotonic function\nRank correlation measures the similarity of the order of two sets of data, relative to each other (recall that PCC did not directly measure the relative rank).\n\nValues range from -1 to 1 where 0 is no association and 1 is perfect association\nNegative values donâ€™t mean anything in ranked correlation, so just remove the negative\n\nLinear relationship is a specific type of monotonic relationship where the rate of increase remains constant â€” in other words, unlike a linear relationship, the amount of change (increase or decrease) in a monotonic relationship can vary.\nSee bkmks for CIs\nPackages\n\n{stats::cor.test(method = â€œspearmanâ€)}\n{DescTools::SpearmanRho}\n{wCorr} - Pearson, Spearman, polyserial, and polychoric correlations, in weighted or unweighted form\n\n\nKendallâ€™s Tau\n\nNon-parametric rank correlation\n\nNon-parametric because it only measures the rank correlation based on the relative ordering of the data (and not the specific values of the data).\n\nShould be pretty close to Sspearmanâ€™s Rank but a potentially faster calculation\nFlavors: a, b (makes adjustment for ties), c (for different sample sizes for each variable)\n\nUse Tau-b if the underlying scale of both variables has the same number of possible values (before ranking) and Tau-c if they differ.\ne.g.Â One variable might be scored on a 5-point scale (very good, good, average, bad, very bad), whereas the other might be based on a finer 10-point scale. In this case, Tau-c would be recommended.\n\nPackages\n\n{stats::cor.test(method = â€œkendallâ€)} - Doesnâ€™t state specifically but I think it calculates a and b depending on whether ties are present or not\n{DescTools} - has all 3 flavors\n\n\nBayesian\n\nSteps: {brms}\n\nList the variables youâ€™d like correlations for within mvbind().\nPlace the mvbind() function within the left side of the model formula.\nOn the right side of the model formula, indicate you only want intercepts (i.e., ~ 1).\nWrap that whole formula within bf().\nThen use the + operator to append set_rescor(TRUE), which will ensure brms fits a model with residual correlations.\nUse non-default priors and the resp argument to specify which prior is associated with which criterion variable\n\nGaussian\n\nExample: multiple variables\nf9 &lt;-Â \n Â  brm(data = d,\n Â  Â family = gaussian,\n Â  Â bf(mvbind(x_s, y_s, z_s) ~ 0,\n Â    Â  sigma ~ 0) +\n    set_rescor(TRUE),\nÂ  Â  prior(lkj(2), class = rescor),\nÂ  Â  chains = 4, cores = 4,\nÂ  Â  seed = 1)\n\n## Residual Correlations:Â \n##Â  Â  Â  Â  Â  Â  Â  Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## rescor(xs,ys)Â  Â  0.90Â  Â  Â  0.02Â  Â  0.87Â  Â  0.93 1.00Â  Â  3719Â  Â  3031\n## rescor(xs,zs)Â  Â  0.57Â  Â  Â  0.07Â  Â  0.42Â  Â  0.69 1.00Â  Â  3047Â  Â  2773\n## rescor(ys,zs)Â  Â  0.29Â  Â  Â  0.09Â  Â  0.11Â  Â  0.46 1.00Â  Â  2839Â  Â  2615\nStandardized data is used here but isnâ€™t required\n\nWill need to set priors though (see article for further details)\n\nSince the data is standardized, the sd can be fixed at 1\n\nbrms models log of sd by default, hence sigma ~ 0 since log 1 = 0\n\nCorrelations are the estimates for rescor(xs,ys), rescor(xs,zs) rescor(ys,zs)\n\nStudent t-distribution\n\nIf the data has any outliers, pearsonâ€™s coefficient is substantially biased.\nExample: correlation between x and y\n\\\nf2 &lt;-Â \nÂ  Â  brm(data = x.noisy,Â \nÂ  Â  family = student,\nÂ  Â  bf(mvbind(x, y) ~ 1) + set_rescor(TRUE),\nÂ  Â  prior = c(prior(gamma(2, .1), class = nu),\n    Â  Â  Â  Â  Â  prior(normal(0, 100), class = Intercept, resp = x),\nÂ  Â  Â  Â  Â  Â  Â  prior(normal(0, 100), class = Intercept, resp = y),\nÂ  Â  Â  Â  Â  Â  Â  prior(normal(0, 100), class = sigma, resp = x),\nÂ  Â  Â  Â  Â  Â  Â  prior(normal(0, 100), class = sigma, resp = y),\nÂ  Â  Â  Â  Â  Â  Â  prior(lkj(1), class = rescor)),\nÂ  Â  iter = 2000, warmup = 500, chains = 4, cores = 4,Â \nÂ  Â  seed = 210191)\n\n## Population-Level Effects:Â \n##Â  Â  Â  Â  Â  Â  Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## x_InterceptÂ  Â  -2.07Â  Â  Â  3.59Â  Â  -9.49Â  Â  4.72 1.00Â  Â  2412Â  Â  2651\n## y_InterceptÂ  Â  1.93Â  Â  Â  7.20Â  -11.31Â  Â  16.81 1.00Â  Â  2454Â  Â  2815\n##Â \n## Family Specific Parameters:Â \n##Â  Â  Â  Â  Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## sigma_xÂ  Â  18.35Â  Â  Â  2.99Â  Â  13.12Â  Â  24.76 1.00Â  Â  2313Â  Â  2816\n## sigma_yÂ  Â  36.52Â  Â  Â  5.90Â  Â  26.13Â  Â  49.49 1.00Â  Â  2216Â  Â  3225\n## nuÂ  Â  Â  Â  Â  2.65Â  Â  Â  0.99Â  Â  1.36Â  Â  4.99 1.00Â  Â  3500Â  Â  2710\n## nu_xÂ  Â  Â  Â  1.00Â  Â  Â  0.00Â  Â  1.00Â  Â  1.00 1.00Â  Â  6000Â  Â  6000\n## nu_yÂ  Â  Â  Â  1.00Â  Â  Â  0.00Â  Â  1.00Â  Â  1.00 1.00Â  Â  6000Â  Â  6000\n##Â \n## Residual Correlations:Â \n##Â  Â  Â  Â  Â  Â  Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## rescor(x,y)Â  Â  -0.93Â  Â  Â  0.03Â  Â  -0.97Â  Â  -0.85 1.00Â  Â  2974Â  Â  3366\n\nN = 40 simulated from a multivariate normal with 3 outliers\nCorrelation is the rescor(x,y) estimate -0.93; true value is -0.96\n\nUsing a pearson coefficient, cor = -0.6365649\nUsing brms::brm with family = gaussian, rescor(x,y) estimate -0.61"
  },
  {
    "objectID": "qmd/association-general.html#sec-assoc-gen-disc",
    "href": "qmd/association-general.html#sec-assoc-gen-disc",
    "title": "General",
    "section": "Discrete",
    "text": "Discrete\n\nMisc\n\nAlso see\n\nMultiple Correspondence Analysis (MCA) (see bkmks &gt;&gt; Features &gt;&gt; Reduction)\nDiscrete Analysis Notebook\n\nPackages\n\n{PAsso} - Assesses the Partial Association Between Ordinal Variables\n\nAllows users to perform a wide spectrum of assessments, including quantification, visualization, and hypothesis testing.\nVignette\n\n\nBinary vs Binary Similarity measures (paper)\n\nNote that a pearson correlation between binaries can be useful (see EDA &gt;&gt; Misc &gt;&gt; {correlationfunnel})\nTypes:\n\nJaccard-Needham\nDice\nYule\nRussell-Rao\nSokal-Michener\nRogers-Tanimoto\nKulzinsky\n\nPackages\n\n{{scipy}} - Also has other similarity measures\n\n\n\nPhi Coefficient - Used for binary variables when the categories are truly binary and not crudely measuring some underlying continuous variable (i.e.Â dichotomization of a continuous variable)\n\nâ€œA Pearson correlation coefficient estimated for two binary variables will return the phi coefficientâ€ (Phi coefficient wiki)\n(Contingency Table) Two binary variables are considered positively associated if most of the data falls along the diagonal cells. In contrast, two binary variables are considered negatively associated if most of the data falls off the diagonal\nAlso see StackExchange discussion on the difference between Phi Coefficient and Tetrachoric correlation\n{DescTools::Phi}\n\nCramerâ€™s V - Association between two nominal variables\n\nSee Discrete Analysis notebook\n{DescTools::CramerV}\n\nPolychoric - Suppose each of the ordinal variables was obtained by categorizing a normally distributed underlying variable, and those two unobserved variables follow a bivariate normal distribution. Then the (maximum likelihood) estimate of that correlation is the polychoric correlation.\n\n{polycor}\n{psych::polychoric}\n\nFor correct=FALSE, the results agree perfectly with {polycor}\nFor very small data sets, the correction for continuity for the polychoric correlations can lead to difficulties, particularly if using the global=FALSE option, or if doing just one correlation at a time. Setting a smaller correction value (i.e., correct =.1) seems to help.\n\n{DescTools::CorPolychor}\n{wCorr} - Pearson, Spearman, polyserial, and polychoric correlations, in weighted or unweighted form\n\nTetrachoric - Used for binary variables when those variables are a sort of crude measure of an underlying continuous variable\n\nAlso see StackExchange discussion on the difference between Phi Coefficient and Tetrachoric correlation\nExample of appropriate use case: Suppose there are two judges who judge cakes, say, on some continuous scale, then based on a fixed, perhaps unknown, cutoff, pronounce the cakes as â€œbadâ€ or â€œgoodâ€. Suppose the latent continuous metric of the two judges has correlation coefficient Ï.\nâ€œthe contingency tables are â€˜balancedâ€™ row-wise and col-wise, you get good correlation between the two metrics, but the tetrachoric tends to be a bit larger than the phi coefficient. When the cutoffs are somewhat imbalanced, you get slightly worse correlation between the metrics, and the phi appears to â€˜shinkâ€™ towards zero.â€\nThe estimation procedure is two stage ML.\n\nCell frequencies for each pair of items are found. Cells with zero counts are replaced with .5 as a correction for continuity (correct=TRUE).\nThe marginal frequencies are converted to normal theory thresholds and the resulting table for each item pair is converted to the (inferred) latent Pearson correlation that would produce the observed cell frequencies with the observed marginals\n\n{psych::tetrachoric}\n\nThe correlation matrix gets printed, but the correlations can also be extracted with $rho\nCan be sped up considerably by using multiple cores and using the parallel package. The number of cores to use when doing polychoric or tetrachoric may be specified using the options command. (e.g options(\"mc.cores\"=4);)\nsmooth = TRUE - For sets of data with missing data, the matrix will sometimes not be positive definite. Uses a procedure to transform the negative eigenvalues.\nFor relatively small samples with dichotomous data if some cells are empty, or if the resampled matrices are not positive semi-definite, warnings are issued. this leads to serious problems if using multi.cores. The solution seems to be to not use multi.cores (e.g., options(mc.cores =1)\n\nGoodman and Kruskalâ€™s Gamma\n\nA measure of rank correlation, i.e., the similarity of the orderings of the data when ranked by each of the quantities. It measures the strength of association of the cross tabulated data when both variables are measured at the ordinal level.\nFor 2-way contingincy tables (i.e.Â 2x2 tables)\nIt makes no adjustment for either table size or ties.\nValues range from âˆ’1 (100% negative association, or perfect inversion) to +1 (100% positive association, or perfect agreement). A value of zero indicates the absence of association.\n{DescTools::GoodmanKruskalGamma}"
  },
  {
    "objectID": "qmd/association-general.html#sec-assoc-gen-mix",
    "href": "qmd/association-general.html#sec-assoc-gen-mix",
    "title": "General",
    "section": "Mixed",
    "text": "Mixed\n\nMisc\n\n{psych::mixedCor} - finds Pearson correlations for the continous variables, polychorics for the polytomous items, tetrachorics for the dichotomous items, and the polyserial or biserial correlations for the various mixed variables (no polydi?)\n\nBiserial - correlation between a continuous variable and binary variable, which is assumed to have resulted from a dichotomized normal variable\n\n{psych::biserial}\n\nPolydi - correlation between multinomial variable and binary variable\n\n{psych::polydi}\n\nPolyserial - polychoric correlation between a continuous variable and ordinal variable\n\nBased on the assumption that the joint distribution of the quantitative variable and a latent continuous variable underlying the ordinal variable is bivariate normal\n{polycor}\n{psych::polyserial}\n{wCorr} - Pearson, Spearman, polyserial, and polychoric correlations, in weighted or unweighted form\n\nX2Y\n\nHandles types: continuous-continuous, continuous-categorical, categorical-continuous and categorical-categorical\nCalculates the % difference in prediction error after fitting a decision tree between two variables of interest and the mean (numeric) or most frequent (categorical)\nFunction is available through a script (Code &gt;&gt; statistical-testing &gt;&gt; correlation)\n\narticle with documentation and usage, https://rviews.rstudio.com/2021/04/15/an-alternative-to-the-correlation-coefficient-that-works-for-numeric-and-categorical-variables/\n\nAll x2y values where the y variable is continuous will be measuring a % reduction in MAE. All x2y values where the y variable is categorical will be measuring a % reduction in Misclassification Error. Is a 30% reduction in MAE equal to a 30% reduction in Misclassification Error? It is problem dependent, thereâ€™s no universal right answer.\n\nOn the other hand, since (1) all x2y values are on the same 0-100% scale (2) are conceptually measuring the same thing, i.e., reduction in prediction error and (3) our objective is to quickly scan and identify strongly-related pairs (rather than conduct an in-depth investigation), the x2y approach may be adequate.\n\nNot symmetric, but can average both scores to get a pseudo-symmetric value\nBootstrap CIs available\n\nCopulas\n\nlatentcor PKG: semi-parametric latent Gaussian copula models"
  },
  {
    "objectID": "qmd/association-general.html#sec-assoc-gen-nonlin",
    "href": "qmd/association-general.html#sec-assoc-gen-nonlin",
    "title": "General",
    "section": "Non-linear",
    "text": "Non-linear\n\nMisc\n\nAlso see General Additive Models &gt;&gt; Diagnostics for a method of determining a nonlinear relationship for either continuous or categorical outcomes.\n\nÎ¾ (xi) coefficient\n\nPaper: A New Coefficient of Correlation\nArticle: Exploring the XI Correlation Coefficient\nExcels at oscillatory and highly non-monotonic dependencies\nXICOR::xicor - calculates Î¾ and performs a significance test (H0: independent)\n\nXICOR::calculateXI just calculates the Î¾ coefficient\n\nProperties (value ranges; interpretation)\n\nIf y is a function of x, then Î¾ goes to 1 asymptotically as n (the number of data points, or the length of the vectors x and y) goes to Infinity.\nIf y and x are independent, then Î¾ goes to 0 asymptotically as n goes to Infinity.\n\nValues can be negative, but this negativity does not have any innate significance other than being close to zero\nn &gt; 20 necessary\n\nn larger than about 250 probably sufficient to get a good estimate\n\nFairly efficient (O(nlogn), compared to some more powerful methods, which are O(n2))\nIt measures dependency in one direction only (is y dependent on x not vice versa)\nDoesnâ€™t tell you if the relationship is direct or inverse"
  },
  {
    "objectID": "qmd/association-time-series.html#misc",
    "href": "qmd/association-time-series.html#misc",
    "title": "4Â  Time Series",
    "section": "4.1 Misc",
    "text": "4.1 Misc"
  },
  {
    "objectID": "qmd/association-time-series.html#cross-correlation-function-ccf",
    "href": "qmd/association-time-series.html#cross-correlation-function-ccf",
    "title": "4Â  Time Series",
    "section": "4.2 Cross-Correlation Function (CCF)",
    "text": "4.2 Cross-Correlation Function (CCF)\n\nThe correlation between two stationary series. The cross-correlation function (CCF) helps you determine which lags of time series X predicts the value of time series Y.\nThe set of sample correlations between xt+h and yt for h = 0, Â±1, Â±2, Â±3, and so on. A negative value for h is a correlation between the x-variable at a time before t and the y-variable at time t. For instance, consider h = âˆ’2. The CCF value would give the correlation between xtâˆ’2 and yt.\nWhen calculating correlations between lags of a variable and the variable itself (ACF) or another variable (CCF) you canâ€™t, for example in a CCF, simply take corr(xt-k, yt) for the correlation of x at lag k and y and repeat for all the lags of x. Since the corr formula/function requires the mean of the series, you would be using a different mean for xk for each calculation of the correlation of each pair. The assumption is that the series are (second-order?) stationary and therefore have a constant mean (i.e.Â each series has 1 mean (and variance)), so the mean(s) of (each) original series should be used in the calculations of the correlations.\n\nSource: Modern Applied Statistics with S, Venables and Ripley\nCompared one against the other in COVID-19 CFR project and they actually produce similar patterns but different values of the CCF. The corr method produce inflated CCF values.\n\nWe expect about 1.75 false alarms out of the 35 sample cross-correlations even after prewhitening\nTerms\n\nIn a cross-correlation in which the direction of influence between two time-series is hypothesized or known,\n\nThe influential time-series is called the â€œinputâ€ time-series\nThe affected time-series is called the â€œoutputâ€ time-series.\n\nWhen one or more xt+h , with h negative, are predictors of yt, it is sometimes said that xÂ leadsÂ y.\nWhen one or more xt+h, with h positive, are predictors of yt, it is sometimes said that x lags y.\n\nAvoid spurious correlations\n\nMake sure thereâ€™s a theoretical reason for the two series to be related\n**Autocorrelation of at least one series should be removed (transformed to white noise)\n\nWith autocorrelation present:\n\nThe variance of cross-correlation coefficient is high and therefore spurious correlations are likely\nSignificance calculations are no longer valid since the CCF distribution will not be normal and the variance is no longer 1/n\n\n\nA problem even with stationary series (even more so with non-stationary series)\n\nSteps\n\nTest for stationarity\nFind number of differences to make the series stationary\n\nNot sure if each series should be differenced the same number of times\n\nWhy would you test a seasonal series and non-seasonal series for an association.\nThe requirement is stationarity, so maybe try using the highest difference needed between both series\n\nIn dynamic regression, Hyndman says difference all variables if one needs differencing, so proabably applicable here.\n\n\nSome examples also used log transformation , but when I did, it produced nonsensical CCF values. (covid cfr project). So, beware.\n\nSeasonal Difference â€“&gt; Difference\n(Optional) Lag scatter plots if the differenced series and look for patterns to get an idea of the strength of the linear correlation\n\nIf thereâ€™s a nonlinear pattern, might be difficult or might not to use a nonlinear or nonparametric correlation function. See 3rd bullet under CCF header above for discussion of stationarity assumption. May not be necessary with another correlation type.\n\nPrewhiten both series (cryer, chan method)\nApply correlation function\n\nPrewhitening\n\nIf either series contain autocorrelation, or the two series share common trends, it is difficult for the CCF to identify meaningful relationships between the two time series. Pre-whitening solves this problem by removing the autocorrelation and trends.\nExample from COVID-19 CFR project\n# numbers of differences\nind_cases_diff &lt;- forecast::ndiffs(ind_cases_ts)\nind_deaths_diff &lt;- forecast::ndiffs(ind_deaths_ts)\n# seasonal\nind_cases_sdiff &lt;- forecast::nsdiffs(ind_cases_ts)\nind_deaths_sdiff &lt;- forecast::nsdiffs(ind_deaths_ts)\n# no seasonal diffs needed\nind_cases_proc &lt;- diff(ind_cases_ts, ind_cases_diff)\nind_deaths_proc &lt;- diff(ind_deaths_ts, ind_deaths_diff)\n\n# fit AR model with processed input series\nind_cases_ar &lt;- ind_cases_proc %&gt;%\nÂ  ts_tsibble() %&gt;%\nÂ  model(AR(value ~ order(p = 1:30), ic = \"aicc\"))\n# pull AR coefs\nind_ar &lt;- coef(ind_cases_ar) %&gt;%Â \nÂ  filter(stringr::str_detect(term, \"ar\")) %&gt;%Â \nÂ  pull(estimate)\n# linearly filter both input and output series using coefs\nind_cases_fil &lt;- stats::filter(ind_cases_proc, filter = c(1, -ind_ar),\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  method = 'convolution', sides = 1)\nind_deaths_fil &lt;- stats::filter(ind_deaths_proc, filter = c(1, -ind_ar),\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  method = 'convolution', sides = 1)\n# spike at -20 with corr = 0.26; nonsensical lags at -4 and -15, -68\nggCcf(ind_cases_fil, ind_deaths_fil)"
  },
  {
    "objectID": "qmd/association-time-series.html#distances",
    "href": "qmd/association-time-series.html#distances",
    "title": "4Â  Time Series",
    "section": "4.3 Distances",
    "text": "4.3 Distances\n\nDynamic Time Warping distance (DTW)\n\ndtw_basic( ) , dtw2( )\nCompares 2 series by â€œwarpingâ€ the time axis to bring them as close as possible to each other and measuring the sum of the distances between the points\nSymmetric (i.e.Â dist from A to B equals the distance from B to A) only if:\n\nEither symmetric1 or symmetric2 step patterns are used\nSeries are equal length after any constraints are used\n\nalgorithm compares two series by calculating a local cost matrix (LCM) and traversing it to find the optimal warping path (minimal cost)\nList of components:\n\nStep pattern that determines how the alg traverses the rows of the LCM to find the optimal path\nWindow range that limits the number lcm calculations for each point\n\n\n\n\n\nFigure shows alignment of two series, x and y.\n\nThe initial and final points of the series must match, but other points along the axis may be â€œwarpedâ€ in order to minimize the distance/cost.\nThe dashed blue lines are the warp curves and show how some points are mapped to each other.\n\nX is the query (or test) series\nYis the reference series\nSteps\n\nCalc LCM matrix for series X and Y\nSimultaneously move along each row of the LCM using a chosen step patternÂ  (see window constraint to get part of a visual of this process)\n\nThe minimum lcm for each pointÂ  along x-axis is found. The sequence of minimum lcms or minimum alignment is Ï†.\nCalc the cost, DTWp, using the lcms in the minimum alignment\n\n\n\nmÏ† is a per-step weighting coefficient (edge weight in patterns fig)\nMÏ† is the normalization constant\nk is a pairs of points (or position along the x-axis) in the minimum alignment\ndtw_basic( ) sets p = 2 (the dtw in the dtw pkg doesnâ€™t use p in this equation)\n\n\n\nChoose the alignment with the lowest cost, DTWp (i.e.Â sum of lcm distances for that alignment)\n\nComponents\n\nLocal Cost Matrix (LCM)\n\nComputed for each pair of series that are compared\nThe Lp norm (distance) between the query series and reference series\n\n\n\nxi and yj are elements of the test and reference time series\nv stands for â€œvariableâ€ which is for comparing multivariate series\n\ni.e.Â the Lp norm for each pair of points is summed over all variables in the multivariate series\n\np is the order of the norm used\n\ne.g.Â 1 is Manhattan distance; 2 is Euclidean\n** Choice of p only matters if multivariate series are being used **\n\n\n\nEach lcm(i , j) value fills a spot in the n x m matrix, LCM (where 1 &lt; i &lt; n and 1 &lt; j &lt; m)\n\nStep Patterns\n\nstep.pattern arg\nDetermines how algorithm moves across the rows of the LCM to create alignments (time axis warps)\nEach pattern is a set of rules and weights\n\nThe rules are used to create different alignments of the LCM (i.e warping of the time axis)\nThe edge weights, mÏ†, are used the DTW calculation\n\n\n\nPatterns in fig\n\nsymmetric1Â  symmetric2 asymmetricÂ  rabinerJuangStepPattern(4, â€œcâ€, TRUE) (i.e., Rabiner-Juangâ€™s type IV with slope weighting)\n\nOnly some of the patterns are normalizable (i.e.Â MÏ† is used in the DTW equation below) (normalize arg)\n\nNormalization may be important when\n\nComparing alignments between time series of different lengths, to decide the best match (e.g., for classification)\nWhen performing partial matches (?)\n\nFor dtw_basic( ), doc says only supported with symmetric2\nrabinerJuangStepPattern() with slope weighting types c and d are normalizable\nsymmetricP* (where * is a number) are all normalizable (not shown in fig)\n\ndtwclust pkg author says symmetric1 most commonly used. dtw pkgÂ  and dtw_basic( ) use symmetric2 by default.\n\n\nWindow Constraints\n\nLimits the region that the lcm calculation takes place.\n\nReduces computation time but makes sense that you donâ€™t want to compare points that are separated by to large a time interval\n\nSakoe-Chiba window creates a calculation region along the diagonal of the LCM\n\n\n1 set of lcm calculations occurs within the horizontal, rectangular block of the query series and the vertical, rectangular block of the reference series.\nSakoe-Chiba requires equal length series but a â€œslanted bandâ€ is equivalent and works for unequal length series.\n\nâ€œSlanted bandâ€ is whatâ€™s used by dtwclust when the window constraint is used.\n\n\nOptimal window size needs to be tuned\n\nCan marginally speed up the DTW calculation, but they are mainly used to avoid pathological warping\nw, the window size, is ~half the size of the actual region covered\n\n[(i, j - w), (i, j + w)] which has 2w + 1 elements\n\nA common w is 10% of the sample size, smaller sizes sometimes produce better results\n\n\nLower Bounds (LB)\n\ndtw_lb( )\nUses a lower bound the dtw distance to speed up computation\n\nA considerably large dataset would be needed before the overhead of DTW becomes much larger than that of dtw_lbâ€™s iterations\nMay only be useful if one is only interested in nearest neighbors, which is usually the case in partitional clustering\n\nSteps\n\nCalculates an initial estimate of a distance matrix between two sets of time series using lb_improved( )\n\nInvolves the â€œlower boundâ€ calculation; didnâ€™t get into it\n\nUses the estimate to calculate the corresponding true DTW distance between only the nearest neighbors (row-wise minima of dist.matrix) of each series in x found in y\nUpdates distance matrix with DTW values\nContinues iteratively until no changes in the nearest neighbors occur\n\nOnly if dataset is very large will this method will be faster than dtw_basic( ) in the calculation of DTW\nNot symmetric, no multivariate series\nRequires\n\nBoth series to be equal length\nWindow constraint defined\nNorm defined\n\nValue of LB (tightness of envelope around series) affected by step pattern which is set in dtw_basic( ) and included via â€¦ in dtw_lb\n\nSize of envelopes in general: LB_KeoghpÂ  &lt;Â  LB_Improvedp &lt;Â  DTWp\n\n\n\nSoft DTW\n\nsdtw( )\nâ€œregularizes DTW by smoothing itâ€ Â¯\\_(ãƒ„)_/Â¯\n\nâ€œsmoothnessâ€ controlled by gamma arg\n\ndefault: 0.01\nWith lower values resulting in less smoothing\n\n\nUses a gradient to efficiently calculate cluster prototypes\nNot recommended for stand-alone distance calculations\n\nNegative values can happen\n\nSymmetric and handles series of different lengths and multivariate series\n\nShape-based distance (SBD)\n\nSBD( )\nUsed in k-Shape Clustering\n\nBased on the cross-correlation with coefficient normalization (NCCc) sequence between two series\n\nFast (uses FFT to calc), competitive with other distance algorithms, and supports series with different lengths\nSymmetric, no multivariate series\nIn preprocessing arg, set to z-normalization\n\n\nTriangular global alignment kernel distance\n\nGAK( )\nâ€œRegularizes DTW by smoothing itâ€Â  Â¯\\_(ãƒ„)_/Â¯\nSymmetric when normalized (dist a to b = dist b to a)\nSupports multivariate series and series of different length (as long as one series isnâ€™t half the length of the other)\nSlightly more computationally expensive than DTW\n\n\n\nÏƒ ,â€œsigmaâ€, can defined by the user but if left as NULL, the function estimates it\nT is the triangular constraint and is similar to the window constraint in DTW but there no arg for it, so I guess itâ€™s taken care of\nNo idea what i and j refer to\n\nWould have to look it up in the original paper or there is a separate website and package for it\n\n\nIf normalize = TRUE, then a distance is returned, can be compared with the other distance measure, and used in clustering\n\nIf FALSE, a similarity is returned"
  },
  {
    "objectID": "qmd/aws.html#sec-aws-misc",
    "href": "qmd/aws.html#sec-aws-misc",
    "title": "AWS",
    "section": "Misc",
    "text": "Misc\n\nNotes from Skillshare: Absolute Beginners Introduction to Amazon Web Services\nHow to choose the right GPU for Deep Learning\nOptimizations\n\nIncreased the number of threads that the AWS CLI uses to some large number (the default is 10) withÂ aws configure set default.s3.max_concurrent_requests 50\nIf downloading data is more of a bottleneck than cpu power, use a network speed optimized ec2 instance (â€œnâ€ in the name) such as c5n.4xl."
  },
  {
    "objectID": "qmd/aws.html#sec-aws-creman",
    "href": "qmd/aws.html#sec-aws-creman",
    "title": "AWS",
    "section": "Create and Manage Account",
    "text": "Create and Manage Account\n\naws.amazon.comÂ  â€“ create a free account (top right)\n\ne: &lt;email&gt;\np: &lt;password&gt;\nacct name:\nall info needs entered:\n\ntick personal,\nfull name,phone number, country, address, city, state, postal code,\ntick youâ€™ve read user agreement\n\n\nPayment info\n\ncredit card, exp date, name\n\nVerify phone\n\ncountry, phone number (again), the â€œi am not a robotâ€ thing, click call me now\n\n4 digit code pops up on screen, computer calls you, and you enter code\nclick continue\n\n\nSelecting the Basic/free acct option (1-year),\nyou can skip the Personalize your experience stuff. Hit â€œsign into the consoleâ€ (mid-right)\n\nenter email and password\n\nBasic account page set-up\n\nUpper right corner â€“&gt; name of the account â€“&gt; dropdown menu â€“&gt; My Account\n\nMain Body â€“&gt; Alternate Contact info â€“&gt; right side, click edit\nMain Body â€“&gt; Security Challenge Questions â€“&gt; click edit\nMain Body â€“&gt; IAM User and Role Access â€“&gt;Â  click edit â€“&gt; check Activate box â€“&gt; click update\n\nthink this is just for generating roles that have access to billing\n\nMain Body â€“&gt; Manage Communication preferences â€“&gt; edit\n\nnewsletters, services info, etc.\n\nMain Body â€“&gt; Manage AWS support plan\n\nalready selected free earlier\nwhere you can upgrade from free tier to a paid plan\n\n(nothing to fill out) left panel â€“&gt;Â Dashboard, Bills, Cost Explorer, Reports, allocation tags, credits, misc accounting stuff\n\nstats on your spending\ninvoice summary of this monthâ€™s usage\nVisual breakdown of the cost/services youâ€™re incurring\nreports\ncreate tags to designate which departments/projects are using which services\nwhere to input promo codes\n\nCreate Spending Alerts\n\nleft panel â€“&gt; Budgets â€“&gt; Create Budget\nselect type: Cost, usage, reservation, utilization\n\nuse cost\n\n$20\n\n\nGive budget a name\nPeriod: monthly, etc.\n\nuse monthly, itâ€™s the smallest period available\n\nselect start date and end date\nAmount\nCan set limits per service\nNotifications\n\nactual, greater than, 50% of budget amount\n\nother option besides actual is forecasted\n\nemail address\noption for SNS service but didnâ€™t discuss what that is\ncreate new notification\n\nadd additional email alert for 75%\n\n\nclick create budgetÂ (bottom right)\n\nleft panel â€“&gt; Preferences\n\nset notifications for when you exceed the free tier services\nget emailed billing invoice\nBilling alerts, reports\n\n\n\nIdentity Access Management\n\nhome console â€“ top search box under AWS Services, type â€œIAMâ€\n\nselect Identity and access management\n\nChange User Sign-in URL to something more memorable\n\nexample: root account name\n\nercbk\n\nclick customize (mid-right) and type name\nURL will be changed to .signin.aws.amazon.com/console\n\nSecurity Status section (main body\n\nActivate multi-factor authentication (MFA)\n\nThis is for root user, see change password section below if youâ€™re a user thatâ€™s part of a group\nphysical MFA (e.g.Â usb drive) or virtual MFA\n\nchoose virtual\n\nclick next twice to the bar code\nuse authenticator app to scan bar code\n\nclick + in app, then click bar code, and scan bar code on screen\n\nAsks for 2 consecutive pins that display on the app\nClick finish\n(may need to refresh page on main screen in order to see green tick mark)\n\n\nCreate individual IAM users\n\nAllows you to distribute various permissions to people with different roles in your org\nNeed to set up an admin user to get access keys so you can use AWS programmatically\nleft panel â€“&gt; Users â€“&gt; add users â€“&gt; enter user name â€“ select access types\n\nchoose access type\n\ntick programmatic and management console\n\nenter console password\nrequire password reset: nope (untick box)\nclick next (bottom right) to set permissions\n\nSet permissions\n\nif youâ€™re creating user groups (left panel), you can skip this and set the permissions group-wide\nSelect Attach existing policies directly\n\ntick box â€œAdministratorAccessâ€ â€“&gt; click next (bottom right)\n\ndude did this through groups and policies, so may have to go that route if AdministratorAccess isnâ€™t available, then add the user to the group. Also thereâ€™s a button to attach additional â€œpoliciesâ€ later on if needed\n\n\n\nReview and click create User\na lot more to thisâ€¦ groups, policiesÂ  (see vid for details)Â \n\nSelect password policy\n\ntick allow users to set password and untick everything else â€“&gt; apply policy\nactivate/deactivate security token regions - he didnâ€™t go into this but all US and EU regions were activated by default, so probably doesnâ€™t need to be messed with.\n\nCopy URL, log out as root user, Now you can starting going to URL and sign in under the administrator acct you made\n\nTo change password\n\nleft panel â€“&gt; Users â€“&gt; select yourself â€“&gt; security credentials tab â€“&gt; Console password\nCan also create MFA in same tab\n\n\n\nCloud Trail\n\nlog of actions taken on account\ntype cloud trail in search box\nCreate trail -Â  lets you store the logs in a S3 bucket"
  },
  {
    "objectID": "qmd/aws.html#sec-aws-basserv",
    "href": "qmd/aws.html#sec-aws-basserv",
    "title": "AWS",
    "section": "Overview of Basic Services",
    "text": "Overview of Basic Services\n\nCompute\n\nEC2 - elastic cloud compute - computer clusters\n\nclosely related to on-premise set-ups\n\nElastic Container Service (ECS) - spin up containers on top of EC2\nLambda -Â serverless architecture\n\ncomputing resources can scale and descend automatically based on real-time demands.\nhandles security patches and OS updates automatically\nissue with â€œtimeoutsâ€ - not optimal for long running applications\ndependency limit at 50 MB, can add 512 MB more to a tmp file after function has executed\nspins down when not in use, so you donâ€™t pay for downtime, but takes 5 or more secs to spin back up\n\ncan ping server from time to time to keep it â€œwarmâ€\n\nless tweakable than EC2, if problems occur, less flexible in terms of your team handling it\nThe code you run on AWS Lambda is uploaded as a â€œLambda functionâ€. Each function has associated configuration information, such as its name, description, entry point, and resource requirements. The code must be written in a â€œstatelessâ€ style i.e.Â it should assume there is no affinity to the underlying compute infrastructure. Local file system access, child processes, and similar artifacts may not extend beyond the lifetime of the request, and any persistent state should be stored in Amazon S3, Amazon DynamoDB, or another Internet-available storage service. Lambda functions can include libraries, even native ones.\n\n\nStorage\n\nS3\n\nDatabase -automanaged by AWS\n\nrelational db service (RDS)\ndynamoDB - noSQL\nElasticCache - redis\nRedshift - cadillac data warehouse service\n\nManagement Tools\n\nCloudWatch\n\nmonitor cpu usage\nlatency\nset alarms around metrics"
  },
  {
    "objectID": "qmd/aws.html#sec-aws-cli",
    "href": "qmd/aws.html#sec-aws-cli",
    "title": "AWS",
    "section": "CLI",
    "text": "CLI\n\ngoogle â€œdownload aws cliâ€\nGet keys\n\nlog into aws\nsearch IAM â€“&gt; goto iam\nleft panel â€“&gt; users â€“&gt; your usesrname â€“&gt; security credentials tab â€“&gt; create access key â€“&gt; 2 keys are created â€“&gt; click download csv file\n\naccess key id (kind of like a username)\nsecret access key (kind of like a password)\n\n\nSet-up\n\nConfigure profile\n\naws configure --profile &lt;name&gt;\n\nchoose a name, can be anything\n\nused ercbk\n\n\nYouâ€™ll be asked for your\n\nâ€œaccess key idâ€ and â€œsecret access keyâ€, enter those\n\nused eb_admin values\n\noptional: default region: us-east-2 or hit enter to skip\n\nused us-east-2\n\noptional: default output format: just hit enter (he didnâ€™t go into this)\n\n\n\ncommands\n\nHelp commands\n\ngives description, flags, inputs, etc.\naws help\naws &lt;service&gt; help\n\neg aws iam help\n\naws &lt;service&gt; &lt;command&gt; help\n\neg aws iam list-users help\n\nCan also go to AWS documentation at website"
  },
  {
    "objectID": "qmd/aws.html#sec-aws-ec2",
    "href": "qmd/aws.html#sec-aws-ec2",
    "title": "AWS",
    "section": "EC2",
    "text": "EC2\n\nOn home screen, in search window, type â€œec2â€, click link\n\n\nEC2 Page\n\nleft panel â€“ Dashboard\n\nMain Body â€“ Under Resources\n\nInstances - which cluster instances you have running\nVolumes - which storage services you have\nkey pairs - you get a key pair for each running instance\nOther stuffâ€¦\n\n\nleft panel â€“ Instances\n\ninstances\n\nwhere you launch on-demand instances\nno bidding, you pay full price\n\nlaunch templates\n\nstored instance configurations\nalternate method to launch instances\n\nspot instances (requests)\n\nbid for available instances for a cheaper price\nIf the Spot price increases above your bid price, capacity is no longer available, or the spot request has constraints that canâ€™t be met, then the Spot Instance can be â€œinterrupted.â€\n\nhttps://aws.amazon.com/premiumsupport/knowledge-center/spot-instance-terminate/\nEBS volumes can be attached, snapshots taken, or results sent to s3 buckets to prepare for a potential terminations (see above link)\n\n\nreserved instances\n\nyou can make a reservation for an instance to guarantee itâ€™s availability at the time you specify. Can be cheaper than on-demand.\n\ndedicated host\n\nWhen you buy instances you share hosts (servers) with other people. Here you can guarantee only you are on the host with your instances.\n\n\nleft panel â€“ images\n\nAMI\n\nscenario: you launch an ec2 and load more packages onto it and want to save that AMI to reuse at a later time. Here you can take a snapshot of that image.\nyou can share these images with other users, sell the image in the marketplace\n\n\nleft panel â€“ Elastic Block Store\n\nvolumesÂ \n\nhard drives - hdd (cold or optimized, ssd, iops ssd )\n\nsnapshots\n\nback-up of the volume\ncan be used to launch another instance if the current one is terminated\n\n\nleft_panel â€“ Network and Security\n\nSecurity GroupsÂ (also see docker notebook â€“ aws â€“ running a task)\n\nfirewall you place in front of an EC2 instance\nspecify allowed ports and allowed ip connections\ncanâ€™t restrict to inbound or outbound traffic; open is open\n\nElastic IPs\n\nallows you to fix static ips (up to 5) to your instances\n\none less thing youâ€™d have to configure if youâ€™re starting and stopping ec2 instances a lot.\n\n\nPlacement Groups\n\ndecreases latency by placing all of your instances in the same or closely placed hosts\n\nKey Pairs\n\na form of tags that you can use to access your instance more easily,\n\nuse values as descriptors of use case for instance to organize and monitor\nCan use to SSH into your instance\npairs are key:value. Example: key= sales, valueÂ  = sales_forecast\n\n\nNetwork Interfaces\n\ngives a network card to your instance so it can connect to the internet\n\n\nleft panel â€“ Load Balancing\n\nscaling managed by aws\nroutes traffic to different instances\nLoad Balancers (also see Docker notebook, aws create load balancer section)\n\ntypes: application, network\n\nTarget Groups\n\nleft panel â€“ auto scaling\n\nadds instances if triggers set in load balancers (latency, cpu usage, memory)\nshuts down instances once load decreases\nkeeps cost lower\n\nleft panel â€“ systems manager\n\nallows you to run a command across multiple instances by specifying tags, ids, etc\ncan use scripts that update packages\n\n\n\n\nLaunch\n\nSearch ec2 in the search window\nSteps\n\ntop-right header â€“ choose a region\n\nwhere are the users located (latency)\n\nleft panel â€“ Instances â€“ instances (or Spot Instances)\n\nclick launch instances\nchoose an AMI, click select on right-hand side\nchoose compute option by ticking box on the left and click next on bottom right\n\neg t2 - micro (free tier option)\n\nconfigure instance details\n\nselect how many instances you want to launch\nnetwork, subnet, auto-assign public ip\n\ndefaults should be fine\n\nIAM role - mentions something about databases\nshutdown behavior: â€œstopâ€ terminates instance when you stop it\nenable termination protection: protects against accidental termination\n\nthink this might have to do with spot instances being terminated\n\nmonitoring - enable CloudWatch ($)(seeÂ Overview of basic services section)\n\nfree version updates metrics every 5 min\ndetailed version updates metrics every 1 min\n\nT2 unlimited\n\nenabling says if your t2 instance cpu goes over 20% usage, then start using my credits, and once my credits are used, then bill me for the rest\nguessing if this is unticked then your t2 is throttled below 21% to remain totally free\n\n\nAdd storage\n\nsee Elastic Block Store above for details on available drives\ncomes with a â€œrootâ€ drive by default\n\nChoose HD type, size of storage, and whether to delete on termination (volume gets deleted)\nGeneral Purpose SSD and up to 30 GB available for free tier.\nAlso saw a snapshot ID I think but he didnâ€™t discuss\n\nclick add volume to add multiple HDs\n\nAdd tags\n\nadd tag\nenter key and value (different from key pair below)\ntick which resources you want to use the tag for: instance and/or volume\n\ncan create unique tags for different resources\n\n\nConfigure security group\n\nAssign a new security group or select an existing group\n\ntick select existing â€“&gt; tick default\n\n\nClick review and launch\n\nshows all the options you selected above\nHit launch\n\nChoose existing or create new key pair\n\nfile thatâ€™s necessary to ssh (linux) or log into (windows)\nCreate new key pair â€“&gt; enter name â€“&gt; download key pair file\n\nor you can select previous key pair and use the same file\n\nLaunch instance\n\nLaunch Status\n\nClick View Instances\n\nWatch status column, usually takes a couple minutes to launch\nscreen is split in half, use divider to increase size of lower screen if you want to view the details of the instance you started\nI think youâ€™re inÂ left panel â€“ instances â€“ instances\n\n\n\n\n\n\n\nConnect to/ Terminate Instance\n\nAlso see Docker, AWS &gt;&gt; Create Cluster: EC2 with SSH Access\nleft panel â€“ instances â€“ instances\n\nhighlight instance you want to connect to\nIn bottom of split EC2 screen, copy IPv4 Public IP and save it in notebook++ or somewhere\n\nOpen an inbound port to instance\n\nleft panel â€“Â Network and Security â€“ Security Groups\n\nclick on security group you selected during launch of instance\nlower half of screen â€“ inbound tab\n\nâ€œall trafficâ€ means full communication allowed between all instances within this security group\nclick edit â€“ add rule\n\nType â€“ click dropdown â€“ choose SSH\n\nafter choosing SSH, it also inputs port 22\n\nSource â€“ ditto â€“ choose â€œMy IPâ€\n\nautomatically finds your ipÂ  and inputs it\n\nDescription -Â  â€œSSH for  ipâ€ or whatever you want\n\nfor Windows Server AMI instance\n\nType â€“ click dropdown â€“ choose RDP\n\nchooses port 3389\n\nSource â€“ ditto â€“ choose â€œMy IPâ€\n\nautomatically finds your ipÂ  and inputs it\n\nDescription - â€œRDP for  ipâ€ or whatever you want\n\n\n\n\nIf on a linux machine (locally), click instance, choose connect, follow instructions\nIf on windows (locally):\n\nOpen Puttygen (key generator)\n\nClick load â€“ find and select the key pair file (.pem) you downloaded before launching the instance\nClick Save private key\n\nItâ€™ll ask if youâ€™re sure you donâ€™t want to attach a passphrase â€“ click yes\nenter name of .ppk file (no need to add extension) â€“ Click Save\n\n\nOpen Putty\n\nConfiguration window opens\n\nIn Host Name (IP address) box â€“ paste IPv4 Public IP (that you copied from earlier)\nleft panel â€“ Connections â€“ SSH â€“ Auth\n\nOptions-for-controlling-SSH-authentication window opens\n\nprivate-key-for-authentication box â€“ click browse\n\nselect the .ppk file you made\n\n\n\nClick Open (bottom right) to open connection\n\nÂ Windows pop-up â€“ click yes\n\n\n\nPutty CLI opens\n\nâ€œlogin asâ€Â \n\ntype username for the AMI you used\n\nexample was a basic linux AMI with username â€œEC2-userâ€\nFor RStudio AMI, should be â€œubuntuâ€\n\n\nHit enter and should be connected\n\n\nTerminate\n\nleft panel â€“ instances â€“ instances\n\nselect instance you want to terminate â€“ right click instance_id (or anywhere on row) â€” instance state â€“ terminate\n\n\nConnect to a Windows Server AMI instance\n\nWindows Search â€œRemote Desktop Connectionâ€\n\nopen it\n\nFor â€œComputer:â€, type in the IPv4 public ip â€“ click connect\nFor base ami\n\nusername: administrator\npassword\n\nleft panel â€“ instance â€“ instance\n\nright click instance row â€“ Get Windows Password\n\nclick Choose FIle â€“ select key pair file .pem file (not .ppk)\nclick Decrypt Password (bottom right)\nCopy password, paste into Remote Desktop Connection\n\n\nClick OK\n\n\na window with opens up with Windows OS on it (takes a minute or two to completely load)\n\n\n\n\nConfigure Load Balancer and Application Ports\n\nAlso see Docker, AWS &gt;&gt; Create Application Load Balancer (ALB)\nleft panel â€“ Network and Security â€“ Security GroupsÂ (also see docker notebook â€“ aws â€“ running a task)\nExample: We want the Application Instances to only talk to the load balancer (inbound) and the load balancer talks to the public (inbound) and application instances (outbound) (Reminder all inbound ports are also outbound ports)\n\nCreate security group for EC2 Instance\n\nCreate Security Group (blue button, upper left)\n\nEnter name and description\nclick create\n\nInbound tab (lower half)\n\nclick edit\n\nType: HTTP (auto-inputs protocol TCP and port 80)\nSource: type name of Load Balancer security group (long box)\n\nautocomplete will help\nafter clicking name, a group id is entered into the field\n\nWeird, but id resembles the group id listed in the top half of screen but doesnâ€™t exactly match. Should be correct though.\n\ndrop down should be â€œcustomâ€\n\nclick add rule\nType: HTTPS\n\nsame thing but auto-inputs port 443\n\nClick Save\n\n\noutbound tab\n\nBy default all outbound traffic goes out on the inbound ports, but you can specify additional ports\nclick edit\n\nType: all traffic (auto-inputs protocol all, port range 0-65535)\nSource: same as for inbound\n\n\n\nCreate security group for Load Balancer\n\nCreate Security Group\n\nSame procedure as above, different name\n\nInbound tab\n\nclick edit\n\nType: HTTP\n\nkeep Source as is\n\n0.0.0.0 means accept traffic from everywhere\n\n\nclick add rule\nType: HTTPS\n\nsame\n\nclick Save\n\n\nOutbound tab\n\nclick edit\n\nType: HTTPÂ (auto-inputs protocol TCP and port 80)\nSource: type the application security group name (long box)\n\nsee load balancer section for other details\n\nclick add rule\nType: HTTP\n\nsame\n\nclick Save"
  },
  {
    "objectID": "qmd/aws.html#sec-aws-s3",
    "href": "qmd/aws.html#sec-aws-s3",
    "title": "AWS",
    "section": "S3",
    "text": "S3\n\nSearch S3\nautoscales,, replicates your data to prevent total loss, ability to version data, max file size 5TB\ncharged for what you use (GB/mo)\n3 different classes\n\nstandard\n\nhighest availability (99.999%), durability (replicated across hosts multiple times)\nmost expensive of the classes\n\ninfrequently accessed\n\nless durability (replicates), should be data that doesnâ€™t end your world if lost\n\nglacial\n\nfor archiving purposes\n\n\nSecurity\n\nIAM\n\ngive certain persons or departments permissions\n\nS3 policies\n\nmake certain buckets public, private, etc.\n\n\nBuckets\n\nmust have unique name across all AWS\n\nassume this is taken care of by some auto-generated id\nCanâ€™t contain periods (dns-compliant)\n\nregion-specific\ncross-region replication\n\ncopy objects from one bucket to another for a fee\n\n\nObjects\n\nfiles, artifacts, etc\nkey:value look-up\n\nthe keys are essentially just file paths\n\nbucket_name/folder/file.txt\nbucket_name/folder/*\n\ngets everything\n\n\nthe values are the objects\n\nYou can directly download files from the console but not folders\n\nselect file, click on â€œmoreâ€ button (top left), select â€œdownload asâ€, follow directions\n\nTo make publicly available through a web link you have to specify that policy (see below)\n\nÂ Create a basic bucket\n\nClick Create Bucket (top left)\n\nOpens Wizard\n\nName and Region\n\nEnter unique DNS compliant name (no periods)\nEnter Region\nClick next\n\nSet Properties\n\nkeep defaults\nclick next\n\nSet Permissions\n\nkeep defaults\nclick next\n\nReview\n\nclick create bucket\n\n\n\nUpload to bucket\n\nUpload via aws console\n\nClick bucket name\nclick upload button (top left)\nOpens Wizard\n\nSelect files\n\nclick add files or drag and drop folders into the window\nclick next\n\nSet Permissions\n\nkeep defaults\nclick next\n\nSet Properties\n\nStorage class\n\nstandard, standard-ia (infrequently accessed), reduced redundancy\n\nencryption\nkeep defaults\nclick next\n\nReview\n\nclick upload\n\n\n\nSee below for uploading via CLI\n\n\nCreate Bucket Policy to allow for public download (from a weblink)\n\nclick bucket name\nclick Permissions tab (top mid)\nBucket policy requires a json expression\n\nclick policy generator (bottom left)\n\nopens up new browser tab\n\nSelect type of policy\n\nS3 Bucket Policy\n\neffect (allow or deny)\n\nselect allow\n\nPrincipal\n\ntype â€œ*â€ (asterisk) to give everyone access\n\nActions\n\ncan select more than one action if you want\nselect â€œGetObjectâ€\n\nAmazon Resource Name\n\nThe format of the required expression is given below the box\nyour substituting your bucket_name/key_name into the expression\n\nfor key name he used * to signify â€œeverythingâ€, but I think this would be your path to the resources (not including bucket_name)\n\ne.g.Â folder1/folder2/file.csv or folder1/folder2/*\n\n\nClick add statement\nClick generate policy\ncopy json expression\n\n\nGo back to previous browser tab with the permissions tab and paste the expression into the window\nClick Save (mid right)\n\nThereâ€™s also a DELETE button if you want to remove a policy from the bucket\n\nShould see a â€œpublicâ€ tag on the permissions tab confirming the policy has been set\n\nDownload from bucket\n\nGet download link for a file that has a public permission set\n\ntick box of selected file\n\nwindow with info about the file should open on the right side\n\ncopy download link in overview section, paste in browser or use programatically\n\nDownload via console\n\nclick file name â€“ overview tab\n\nÂ various downloading options\n\n\nProgramatically: see section Upload to bucket via CLI below (uses copy command)\n\nGive bucket viewing (and downloading) access via IAM (need to have administrator permissions)\n\nsearch IAM\nleft panel â€“ Policies\nclick create policy (top left)\nclick choose service (mid)\n\ntype or select â€œS3â€\n\nclick select actions\n\nUnder Access Level\n\nList\n\nselect all (4)\n\nRead\n\nView policy\n\nGetBucketAcl, GetBucketCORS, GetBucketLocation, GetBucketLogging, GetBucketPolicy, GetBucketTagging, GetObjectAcl, ListBucketByTags, ListBucketVersions\n\nDownload policy (should be a separate policy from View)\n\nGetObject\n\n\nResources\n\nyou have to select resources (e.g.Â buckets, objects (i.e.Â folders, files) that the actions above affect\n\nView policy\n\nchoose all resources (which is buckets and objects)\n\nDownload policy\n\nchoose specific\nclick ARN\n\nEnter the bucket name you want to give access to (or tick â€œanyâ€ box to give access to all buckets)\nenter object name (or tick â€œanyâ€ box for access to all objects)\n\nClick add\n\n\n\nClick Review Policy (bottom right)\n\nEnter a Name\n\neg ListAllBucketsObjsS3\n\nClick Create Policy (bottom right)\nSelect or click newly created policy (should be back to left panel â€“ policies console\n\nnew policy name should be in a clickable banner at top of screen\nOr choose it from list of policies thatâ€™s displayed\n\nGo to Attached entities tab\n\nclick attach\nselect users or groups you want the policy to apply to\nclick attach (bottom right)\n\n\n\nView buckets you have permissions with: aws s3 ls or aws s3api list-buckets --output text\nUpload to bucket via CLI\n\nAlso see above for uploading via aws console\naws s3 help, aws s3  help\naws s3 cp     can copy files via:\n\nbucket to bucket\nlocal to bucket\nbucket to local\n\nlocal to bucket\n\naws s3 cp &lt;C:\\\\Users\\\\path\\\\to\\\\file.csv&gt; &lt;s3://&lt;bucket/path/to/folder/name.csv&gt; --region &lt;bucket? region&gt; --profile &lt;profile name&gt;\nsee cli section above on how to create profile\nrefresh console if you already have it open to see the file\n\nbucket to localÂ \n\naws s3 cp &lt;s3://&lt;bucket/path/to/folder/name.csv&gt; &lt;C:\\\\Users\\\\path\\\\to\\\\file.csv&gt;Â --region &lt;bucket? region&gt; --profile &lt;profile name&gt;\nsame thing as before, just reversing the  and  URIs\n\n\nVersioning\n\nIf you enable versioning and then disable it, then the versioned objects will remain but new objects wonâ€™t be versioned\n\nÂ Would have to manually delete the versioned objects manually\n\nbucket name â€“ properties tab\n\nclick on â€œVersioningâ€ card\ntick Enable versioningÂ \nclick save\n\nTo see the different file versions\n\nbucket name â€“ folder\nclick versions: show button (upper left)\n\nOnce versions are displayed you can download and delete files/versions in various menus\n\nclick file name â€“ â€œlatest versionâ€ drop down (next to file name, top left)\n\nshows all versions with options to download or delete\n\nclick file name â€“ overview tab\n\nvarious download options\n\ntick file name/version box â€“ click more button (upper left) â€“ select delete â€“ click delete button"
  },
  {
    "objectID": "qmd/base-r.html#sec-baser-misc",
    "href": "qmd/base-r.html#sec-baser-misc",
    "title": "Base R",
    "section": "Misc",
    "text": "Misc\n\nMagrittr + base\nmtcars %&gt;% {plot(.$hp, .$mpg)}\nmtcars %$% plot(hp, mpg)\n\nBy wrapping the RHS in curly braces, we can override the rule where the LHS is passed to the first argument ## Options {#sec-baser-opts .unnumbered}\n\nRemove scientific notation\noptions(scipen = 999)\nWide and long printing tibbles\n# in .Rprofile\nmakeActiveBinding(\".wide\", function() { print(.Last.value, width = Inf) }, .GlobalEnv)\n\nAfter printing a tibble, if you want to see it in wide, then just type .wide + ENTER.\nCan have similar bindings for `.long` and `.full`.\n\nHeredocs - Powerful feature in various programming languages that allow you to define a block of text within the code, preserving line breaks, indentation, and other whitespace.\ntext &lt;- r\"(\nThis is a\nmultiline string\nin R)\"\n\ncat(text)"
  },
  {
    "objectID": "qmd/base-r.html#sec-baser-usrfuns",
    "href": "qmd/base-r.html#sec-baser-usrfuns",
    "title": "Base R",
    "section": "User Defined Functions",
    "text": "User Defined Functions\n\nAnonymous (aka lambda) functions: \\(x) {} (&gt; R 4.1)\nfunction(x) {\nÂ  x[which.max(x$mpg), ]\n}\n# equivalent to the above\n\\(x) {\nÂ  x[which.max(x$mpg), ]\n}\nDots (â€¦)\n\nMisc\n\n{ellipsis}: Functions for testing functions with dots so they fail loudly\n{rlang} dynamic dots: article\n\nSplice arguments saved in a list with the splice operator, !!! .\nInject names with glue syntax on the left-hand side of := .\n\n\nUser Defined Functions\nmoose &lt;- function(...) {\nÂ  Â  dots &lt;- list(...)\nÂ  Â  dots_names &lt;- names(dots)\nÂ  Â  if (is.null(dots_names) || \"\" %in% dots_names {\nÂ  Â  Â  Â  stop(\"All arguments must be named\")\nÂ  Â  }\n}\nNested Functions\nf02 &lt;- function(...){\n  vv &lt;- list(...)\n  print(vv)\n}\nf01 &lt;- function(...){\n  f02(b = 2,...)\n}\n\nf01(a=1,c=3)\n#&gt; $b\n#&gt; [1] 2\n#&gt; \n#&gt; $a\n#&gt; [1] 1\n#&gt; \n#&gt; $c\n#&gt; [1] 3\nSubset dots values\nadd2 &lt;- function(...) {\nÂ  Â  ..1 + ..2\n}\nadd2(3, 0.14)\n# 3.14\nSubset dots dynamically: ...elt(n)\n\nSet a value to n and get back the value of that argument\n\nNumber of arguments in â€¦ : ...length()"
  },
  {
    "objectID": "qmd/base-r.html#sec-baser-funs",
    "href": "qmd/base-r.html#sec-baser-funs",
    "title": "Base R",
    "section": "Functions",
    "text": "Functions\n\ndo.call - allows you to call other functions by constructing the function call as a list\n\nArgs\n\nwhat â€“ Either a function or a non-empty character string naming the function to be called\nargs â€“ A list of arguments to the function call. The names attribute of args gives the argument names\nquote â€“ A logical value indicating whether to quote the arguments\nenvir â€“ An environment within which to evaluate the call. This will be most useful if what is a character string and the arguments are symbols or quoted expressions\n\nExample: Apply function to list of vectors\nvectors &lt;- list(c(1, 2, 3), c(4, 5, 6), c(7, 8, 9))\ncombined_matrix &lt;- do.call(rbind, vectors)\n\ncombined_matrix\n##      [,1] [,2] [,3]\n## [1,]    1    2    3\n## [2,]    4    5    6\n## [3,]    7    8    9\nExample: Apply multiple functions\ndata_frames &lt;- list(\n  data.frame(a = 1:3), \n  data.frame(a = 4:6), \n  data.frame(a = 7:9)\n  )\nmean_results &lt;- do.call(\n  rbind, \n  lapply(data_frames, function(df) mean(df$a))\n  )\n\nmean_results\n##      [,1]\n## [1,]    2\n## [2,]    5\n## [3,]    8\n\nFirst the mean is calculated for column a of each df using lapply\n\nlapply is supplying the data for do.call in the required format, which is a list or character vector.\n\nSecond the results are combined into a matrix with rbind\n\n\nsink - used to divert R output to an external connection.\n\nUse Cases: exporting data to a file, logging R output, or debugging R code.\nArgs\n\nfile: The name of the file to which R output will be diverted. If file is NULL, then R output will be diverted to the console.\nappend: A logical value indicating whether R output should be appended to the file (TRUE) or overwritten (FALSE). The default value is FALSE.\ntype: A character string. Either the output stream or the messages stream. The name will be partially match so can be abbreviated.\nsplit: logical: if TRUE, output will be sent to the new sink and the current output stream, like the Unix program tee.\n\nExample: Logging output of code to file\nsink(\"r_output.log\")      # Redirect output to this file\n# Your R code goes here\nsink()                    # Turn off redirection\n\noutput file could also have an extension like â€œ.txtâ€\n\nExample: Debugging\nsink(\"my_function.log\")   # Redirect output to this file\nmy_function()\nsink()                    # Turn off redirection\nExample: Appending output to a file\nsink(\"output.txt\", append = TRUE)  # Append output to the existing file\ncat(\"Additional text\\n\")  # Append custom text\nplain text\nsink()  # Turn off redirection\n\npmin and pmax\n\nFind the element-wise maximum and minimum values across vectors in R\nExample\nvec1 &lt;- c(3, 9, 2, 6)\nvec2 &lt;- c(7, 1, 8, 4)\npmax(vec1, vec2)\n#&gt; [1] 7 9 8 6\npmin(vec1, vec2)\n#&gt; [1] 3 1 2 4\nExample: With NAs\ndata1 &lt;- c(7, 3, NA, 12)\ndata2 &lt;- c(9, NA, 5, 8)\npmax(data1, data2, na.rm = TRUE)\n#&gt; [1] 9 3 5 12\n\nswitch\n\nExample:\nswitch(parallel,\n         windows = \"snow\" -&gt; para_proc,\n         other = \"multicore\" -&gt; para_proc,\n         no = \"no\" -&gt; para_proc,\n         stop(sprintf(\"%s is not one of the 3 possible parallel argument values. See documentation.\", parallel)))\n\nparallel is the function argument. If it doesnâ€™t match one of the 3 values, then an error is thrown.\nIf the argument value is matched, then the quoted value is stored in para_proc\n\n\ndynGet\n\nLooks for objects in the environment of a function.\nWhen an object from the outer function is an input for a function nested around 3 layers deep or more, it may not be found by that most inner function. dynGet allows that function to find the object in the outer frame\nArguments\n\nminframe: Integer specifying the minimal frame number to look into (i.e.Â how far back to look for the object)\ninherits: Should the enclosing frames of the environment be searched?\n\nExample:\n1function(args) {\n  if (method == \"kj\") {\n      ncv_list &lt;- purrr::map2(grid$dat, \n                              grid$repeats, \n                              function(dat, reps) {\n         rsample::nested_cv(dat,\n                            outside = vfold_cv(v = 10, \n                                               repeats = dynGet(\"reps\")),\n                            inside = bootstraps(times = 25))\n      })\n  }\n}\n\n2function(data) {\n    if (chk::vld_used(...)) {\n        dots &lt;- list(...)\n        init_boot_args &lt;-\n          list(data = dynGet(\"data\"),\n               stat_fun = cles_boot, # internal function\n               group_variables = group_variables,\n               paired = paired)\n        get_boot_args &lt;-\n          append(init_boot_args,\n                 dots)\n    }\n    cles_booted &lt;-\n      do.call(\n        get_boot_ci,\n        get_boot_args\n      )\n}\n\n1\n\nExample from Nested Cross-Validation Comparison\n\n2\n\nExample from {ebtools::cles}\n\n\n\nmatch.arg\n\nPartially matches a functionâ€™s argument values to list of choices. If the value doesnâ€™t match the choices, then an error is thrown\nExample:\nkeep_input &lt;- \"input_le\"\nkeep_input_val &lt;- \n  match.arg(keep_input,\n            choices = c(\"input_lags\",\n                        \"input_leads\",\n                        \"both\"),\n            several.ok = FALSE)\nkeep_input_val\n#&gt; [1] \"input_leads\"\n\nseveral.ok = FALSE says only 1 match is allowed otherwise an error is thrown.\nThe error message is pretty informative btw.\n\n\nmatch.fun\n\nExample\nf &lt;- function(a,b) {\n  a + b\n}\ng &lt;- function(a,b,c) {\n  (a + b) * c\n}\nh &lt;- function(d,e) {\n  d - e\n}\nyolo &lt;- function(FUN, ...) {\n  FUN &lt;- match.fun(FUN)\n  params &lt;- list(...)\n  FUN_formals &lt;- formals(FUN)\n  idx &lt;- names(params) %in% names(FUN)\n  do.call(FUN, params[idx])\n}\nyolo(h, d = 2, e = 3)\n#&gt; -1"
  },
  {
    "objectID": "qmd/base-r.html#sec-baser-pipe",
    "href": "qmd/base-r.html#sec-baser-pipe",
    "title": "Base R",
    "section": "Pipe",
    "text": "Pipe\n\n\nBenefits of base pipe\n\nMagrittr pipe is bloated with special features which may make it slower than the base pipe\nIf not using tidyverse, itâ€™s one less dependency (maybe one day it will be deprecated in tidyverse)\n\nBase pipe with base and anonymous functions\n# verbosely\nmtcars |&gt; (function(.) plot(.$hp, .$mpg))()\n# using the anonymous function shortcut, emulating the dot syntax\nmtcars |&gt; (\\(.) plot(.$hp, .$mpg))()\n# or if you prefer x to .\nmtcars |&gt; (\\(x) plot(x$hp, x$mpg))()\n# or if you prefer to be explicit with argument names\nmtcars |&gt; (\\(data) plot(data$hp, data$mpg))()\nUsing â€œ_â€ placeholder:\n\nmtcars |&gt; lm(mpg ~ disp, data = _)\nmtcars |&gt; lm(mpg ~ disp, data = _) |&gt; _$coef\n\nBase pipe .[ ]Â  hack\nwiki |&gt;\nÂ  read_html() |&gt;\nÂ  html_nodes(\"table\") |&gt;\nÂ  (\\(.) .[[2]])() |&gt;\nÂ  html_table(fill = TRUE) |&gt;\nÂ  clean_names()\n# instead of\ndjia &lt;- wiki %&gt;%\nÂ  read_html() %&gt;%\nÂ  html_nodes(\"table\") %&gt;%\nÂ  .[[2]] %&gt;%\nÂ  html_table(fill = TRUE) %&gt;%\nÂ  clean_names()\nMagrittr, base pipe differences\n\nmagrittr: %&gt;% allows you change the placement with a . placeholder.\n\nbase: R 4.2.0 added a _ placeholder to the base pipe, with one additional restriction: the argument has to be named\n\nmagrittr: With %&gt;% you can use . on the left-hand side of operators like â€œ\\(\", \\[\\[, \\[ and use in multiple arguments (e.g. df %&gt;% {split(.\\)x, .$y))\n\nbase: can hack this by using anonymous function\n\nsee Base pipe with base and anonymous functions above\nsee Base pipe .[ ]Â  hack above\n\n\nmagrittr: %&gt;% allows you to drop the parentheses when calling a function with no other arguments (e.g.Â dat %&gt;% distinct)\n\nbase: |&gt; always requires the parentheses. (e.g.Â dat |&gt; distinct())\n\nmagrittr: %&gt;% allows you to start a pipe with . to create a function rather than immediately executing the pipe\n\nPurrr with base pipe\ndata_list |&gt;\nÂ  map(\\(x) clean_names(x))\n# instead of\ndata_list %&gt;%\nÂ  map( ~.x %&gt;% clean_names)\n# with split\nstar |&gt;\nÂ  split(~variable) |&gt;\nÂ  map_df(\\(.) hedg_g(., reading ~ value), .id = \"variable\")\n# instead of\nstar %&gt;%\nÂ  split(.$variable) %&gt;%\nÂ  map_df(. %&gt;% hedg_g(., reading ~ value), .id = \"variable\")"
  },
  {
    "objectID": "qmd/base-r.html#sec-baser-str",
    "href": "qmd/base-r.html#sec-baser-str",
    "title": "Base R",
    "section": "Strings",
    "text": "Strings\n\nsprintf\nx &lt;- 123.456               # Create example data\n\nsprintf(\"%f\", x)           # sprintf with default specification\n#&gt; [1] \"123.456000\"\n\nsprintf(\"%.10f\", x)        # sprintf with ten decimal places\n#&gt; [1] \"123.4560000000\"\n\nsprintf(\"%.2f\", x)         # sprintf with two rounded decimal places\n#&gt; [1] \"123.46\"\n\nsprintf(\"%1.0f\", x)        # sprintf without decimal places\n#&gt; [1] \"123\"\n\nsprintf(\"%10.0f\", x)       # sprintf with space before number\n#&gt; [1] \"       123\"\n\nsprintf(\"%10.1f\", x)       # Space before number & decimal places\n#&gt; [1] \"     123.5\"\n\nsprintf(\"%-15f\", x)        # Space on right side\n#&gt; [1] \"123.456000     \"\n\nsprintf(\"%+f\", x)          # Print plus sign before number\n#&gt; [1] \"+123.456000\"\n\nsprintf(\"%e\", x)           # Exponential notation\n#&gt; [1] \"1.234560e+02\"\n\nsprintf(\"%E\", x)           # Exponential with upper case E\n#&gt; [1] \"1.234560E+02\"\n\nsprintf(\"%g\", x)           # sprintf without decimal zeros\n#&gt; [1] \"123.456\"\n\nsprintf(\"%g\", 1e10 * x)    # Scientific notation\n#&gt; [1] \"1.23456e+12\"\n\nsprintf(\"%.13g\", 1e10 * x) # Fixed decimal zeros\n#&gt; [1] \"1234560000000\"\n\npaste0(sprintf(\"%f\", x),   # Print %-sign at the end of number\n       \"%\")\n#&gt; [1] \"123.456000%\"\n\nsprintf(\"Let's create %1.0f more complex example %1.0f you.\", 1, 4)\n#&gt; [1] \"Let's create 1 more complex example 4 you.\"\nstr2lang - Allows you to turn plain text into code.\ngrowth_rate &lt;- \"circumference / age\"\nclass(str2lang(growth_rate))\n#&gt; [1] \"call\"\n\nExample: Basic\neval(str2lang(\"2 + 2\"))\n#&gt; [1] 4\n\neval(str2lang(\"x &lt;- 3\"))\nx\n#&gt; [1] 3\nExample: Run formula against a df\ngrowth_rate &lt;- \"circumference / age\"\nwith(Orange, eval(str2lang(growth_rate)))\n\n#&gt;   [1] 0.25423729 0.11983471 0.13102410 0.11454183 0.09748172 0.10349854\n#&gt;   [7] 0.09165613 0.27966102 0.14256198 0.16716867 0.15537849 0.13972380\n#&gt;  [13] 0.14795918 0.12831858 0.25423729 0.10537190 0.11295181 0.10756972\n#&gt;  [19] 0.09341998 0.10131195 0.08849558 0.27118644 0.12809917 0.16867470\n#&gt;  [25] 0.16633466 0.14541024 0.15233236 0.13527181 0.25423729 0.10123967\n#&gt;  [31] 0.12198795 0.12450199 0.11535337 0.12682216 0.11188369"
  },
  {
    "objectID": "qmd/base-r.html#sec-baser-cond",
    "href": "qmd/base-r.html#sec-baser-cond",
    "title": "Base R",
    "section": "Conditionals",
    "text": "Conditionals\n\n&& and || are intended for use solely with scalars, they return a single logical value.\n\nSince they always return a scalar logical, you should use && and || in your if/while conditional expressions (when needed). If an & or | is used, you may end up with a non-scalar vector inside if (â€¦) {} and R will throw an error.\n\n& and | work with multivalued vectors, they return a vector whose length matches their input arguments.\nstopifnot\npred_fn &lt;- function(steps_forward, newdata) {\nÂ  stopifnot(steps_forward &gt;= 1)\nÂ  stopifnot(nrow(newdata) == 1)\nÂ  model_f = model_map[[steps_forward]]\nÂ  # apply the model to the last \"before the test period\" row to get\nÂ  # the k-steps_forward prediction\nÂ  as.numeric(predict(model_f, newdata = newdata))\n}"
  },
  {
    "objectID": "qmd/base-r.html#sec-baser-sort",
    "href": "qmd/base-r.html#sec-baser-sort",
    "title": "Base R",
    "section": "Ordering Columns and Sorting Rows",
    "text": "Ordering Columns and Sorting Rows\n\nAscending: df[with(df, order(value)), ]\n\nâ€œvalueâ€ is the column used to sort the df by\n\nDescending: df[with(df, order(-value)), ]\nBy Multiple Columns\n\nDescending then Ascending: df[with(df, order(-value, id)), ]\n\nChange position of columns\n# Reorder column by index manually\ndf2 &lt;- df[, c(5, 1, 4, 2, 3)]\ndf3 &lt;- df[, c(1, 5, 2:4)]\n# Reorder column by name manually\nnew_order = c(\"emp_id\",\"name\",\"superior_emp_id\",\"dept_id\",\"dept_branch_id\")\ndf2 &lt;- df[, new_order]"
  },
  {
    "objectID": "qmd/base-r.html#set-operations",
    "href": "qmd/base-r.html#set-operations",
    "title": "Base R",
    "section": "Set Operations",
    "text": "Set Operations\n\nUnique values in A that are not in B\na &lt;- c(\"thing\", \"object\")\nb &lt;- c(\"thing\", \"gift\")\n\nunique(a[!(a %in% b)])\n#&gt; [1] \"object\"\n\nsetdiff(a, b)\n\nsetdiff is slower\n\nUnique values of the two vectors combined\nunique(c(a, b))\n#&gt; [1] \"thing\"  \"object\" \"gift\"\n\nunion(a, b)\n\nunion is just a wrapper for unique"
  },
  {
    "objectID": "qmd/base-r.html#sec-baser-subset",
    "href": "qmd/base-r.html#sec-baser-subset",
    "title": "Base R",
    "section": "Subsetting",
    "text": "Subsetting\n\nLists and Vectors\n\nRemoving Rows\n# Remove specific value from vector\nx[!x == 'A']\n\n# Remove multiple values by list\nx[!x %in% c('A', 'D', 'E')]\n\n# Using setdiff\nsetdiff(x, c('A','D','E'))\n\n# Remove elements by index\nx[-c(1,2,5)]\n\n# Using which\nx[-which(x %in% c('D','E') )]\n\n# Remove elements by name\nx &lt;- c(C1='A',C2='B',C3='C',C4='E',C5='G')\nx[!names(x) %in% c('C1','C2')]\n\nDataframes\n\nRemove specific Rows\ndf &lt;- df[-c(25, 3, 62), ]\nRemove column by name\ndf &lt;- df[, which(names(df) == \"col_name\")]\ndf &lt;- subset(df, select = -c(col_name))\ndf &lt;- df[, !names(df) %in% c(\"col1\", \"col2\"), drop = FALSE]\nFilter and Select\ndf &lt;- subset(df, subset = col1 &gt; 56, select = c(col2, col3))"
  },
  {
    "objectID": "qmd/base-r.html#sec-baser-err",
    "href": "qmd/base-r.html#sec-baser-err",
    "title": "Base R",
    "section": "Error Handling",
    "text": "Error Handling\n\nstop\n\nExample:\nswitch(parallel,\n       windows = \"snow\" -&gt; para_proc,\n       other = \"multicore\" -&gt; para_proc,\n       no = \"no\" -&gt; para_proc,\n       stop(sprintf(\"%s is not one of the 3 possible parallel argument values. See documentation.\", parallel)))\n\nparallel is the function argument. If it doesnâ€™t match one of the 3 values, then an error is thrown.\n\n\ntry\n\nIf something errors, then do something else\nExample\n\ncurrent &lt;- try(remDr$findElement(using = \"xpath\",\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  '//*[contains(concat( \" \", @class, \" \" ),\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  concat( \" \", \"product-price-value\", \" \" ))]'),\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  silent = T)\n#If error : current price is NA\nif(class(current) =='try-error'){\nÂ  Â  currentp[i] &lt;- NA\n} else {\nÂ  Â  # do stuff\n}\ntryCatch\n\nRun the main code, but if it â€œcatchesâ€ an error, then the secondary code (the workaround) will run.\n\npct_difference_error_handling &lt;- function(n1, n2) {\n# Try the main code\nÂ  tryCatch(pct_diff &lt;- (n1-n2)/n1,\nÂ  Â  Â  Â  # If you find an error, use this code instead\nÂ  Â  Â  Â  Â  error = return(\nÂ  Â  Â  Â  Â  Â  cat( 'The difference between', as.integer(n1), 'and', as.integer(n2), 'is',\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  (as.integer(n1)-as.integer(n2)), 'which is',\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  100*(as.integer(n1)-as.integer(n2))/as.integer(n1),\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  '% of', n1 )#cat\nÂ  Â  Â  Â  Â  Â  ),\nÂ  Â  Â  Â  Â  # finally = print('Code ended') # optional\nÂ  Â  Â  Â  Â  )#trycatch\nÂ  # If no error happens, return this statement\nÂ  return ( cat('The difference between', n1, 'and', n2, 'is', n1-n2,\nÂ  Â  Â  Â  Â  Â  Â  ', which is', pct_diff*100, '% of', n1) )\n}\n\nAssumes the error will be the user enters a string instead of a numeric. If errors, converts string to numeric and calcs.\nâ€œfinallyâ€ - This argument will always run, regardless if the try block raises an error or not. So it could be a completion message or a summary, for example."
  },
  {
    "objectID": "qmd/base-r.html#sec-baser-mods",
    "href": "qmd/base-r.html#sec-baser-mods",
    "title": "Base R",
    "section": "Models",
    "text": "Models\n\nreformulate - Create formula sytax programmatically\n# Creating a formula using reformulate()\nformula &lt;- reformulate(c(\"hp\", \"cyl\"), response = \"mpg\")\n\n# Fitting a linear regression model\nmodel &lt;- lm(formula, data = mtcars)\n\nformula\n##&gt; mpg ~ hp + cyl\n\nCan also use as.formula\n\nDF2formula - Turns the column names from a data frame into a formula. The first column will become the outcome variable, and the rest will be used as predictors\nDF2formula(Orange)\n#&gt; Tree ~ age + circumference\nformula - Provides a way of extracting formulae which have been included in other objects\nrec_obj |&gt; prep() |&gt; formula()\n\nWhere â€œrec_objâ€ is a tidymodels recipe object\n\nData from Model Object\n\nmodel$model return the data dataframe\ndeparse(model$call$data) gives you that name of your data object as a string.\n\nmodel$call$data gives you the data as an unevaluated symbol;\n\neval(model$call$data) gives you back the original data object, if it is available in the current environment."
  },
  {
    "objectID": "qmd/bayes-priors.html#sec-priors-misc",
    "href": "qmd/bayes-priors.html#sec-priors-misc",
    "title": "Priors",
    "section": "Misc",
    "text": "Misc\n\nResources\n\nStan/brms prior distributions (mc-stan/function-ref)\n\nDistributions are towards the bottom of the guide\n{brms} should have all the distributions available in Stan according to their docs (see Details section)\n\n\nUsing meta-analyis or previous studies to create informed priors\n\nâ€œSystematic use of informed studies leads to more precise, but more biased estimates (due to non-linear information flow in the literature). Critical comparison of informed and skeptical priors can provide more nuanced and solid understanding of our findings.â€ (thread + paper)\n\ni.e.Â try both and compare the result\n\n\nPrior sensitivity analysis\n\n{priorsense}\n\nVideo, Thread\n\n{BayesSenMC}\n\nFor binary exposure and a dichotomous outcome\nGenerates different posterior distributions of adjusted odds ratio under different priors of sensitivity and specificity, and plots the models for comparison. It also provides estimations for the specifications of the models using diagnostics of exposure status with a non-linear mixed effects model.\nVignette\n\n\nStatistical Rethinking\n\nThe â€œflatnessâ€ of a Normal prior is controlled by the size of the s.d. value\n\nnot in logistic regression (see examples below)\n\nFlat priors result in poor frequency properties (i.e.Â consistently give bad inferences) in realistic settings where studies are noisy and effect sizes are small. (Gelman post)\nWeakly informative priors: they allow some implausibly strong relationships but generally bound the lines to possible ranges of the variables. (fig 5.3, pg 131)\nWe want our priors to be skeptical of large differences [in treatment effects], so that we reduce overfitting. Good priors hurt fit to sample but are expected to improve prediction. (pg 337)\nWe donâ€™t formulate priors based on the sample data. We want the prior predictive distribution to live in the plausible outcome space, not fit the sample.\nFor logistic regression and poisson regression, a flat prior in the logit space is not a flat prior in the outcome probability space (pg 336)\nAs long as the priors are vague, minimizing the sum of squared deviations to the regression line is equivalent to finding the posterior mean. pg 200\nâ€œAs always in rescaling variables, the goals are to create focal points that you might have prior information about, prior to seeing the actual data values. That way we can assign priors that are not obviously crazy, and in thinking about those priors, we might realize that the model makes no sense. But this is only possible if we think about the relationship between measurements and parameters, and the exercise of rescaling and assigning sensible priors helps us along that path. Even when there are enough data that choice of priors is not crucial, this thought exercise is useful.â€ pg 258\nComparing the posteriors with the priors"
  },
  {
    "objectID": "qmd/bayes-priors.html#sec-priors-preproc",
    "href": "qmd/bayes-priors.html#sec-priors-preproc",
    "title": "Priors",
    "section": "Preprocessing",
    "text": "Preprocessing\n\nCentering the predictor\n\nMakes the posterior easier to sample\nReduces covariance among the parameter posterior distributions\nMakes it easier to define the prior on average temperature in the center of the time range (instead defining prior for temperature at year 0).\nLinks to Gelman posts about centering your predictors (article)\n\nIf you standardize your predictors, you can use a mean of 0 for the prior on your intercept\n\nWith flat priors, this doesnâ€™t make much of difference"
  },
  {
    "objectID": "qmd/bayes-priors.html#sec-priors-getprecs",
    "href": "qmd/bayes-priors.html#sec-priors-getprecs",
    "title": "Priors",
    "section": "Get Prior Recommendations",
    "text": "Get Prior Recommendations\n\nExample: Fitting a spline\n# get recommended prior specifications\n# s is the basis function brms imports from mgcv pkg\nbrms::get_prior(data = d2,Â \nÂ  Â  Â  Â  Â  Â  Â  Â  family = gaussian,Â \nÂ  Â  Â  Â  Â  Â  Â  Â  doy ~ 1 + s(year))\n##Â  Â  Â  Â  Â  Â  Â  Â  Â  priorÂ  Â  classÂ  Â  coef group resp dpar nlpar boundÂ  Â  Â  sourceÂ \n##Â  Â  Â  Â  Â  Â  Â  Â  Â  (flat)Â  Â  Â  Â  bÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  defaultÂ \n##Â  Â  Â  Â  Â  Â  Â  Â  Â  (flat)Â  Â  Â  Â  b syear_1Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  (vectorized)Â \n##Â  student_t(3, 105, 5.9) InterceptÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  defaultÂ \n##Â  Â  student_t(3, 0, 5.9)Â  Â  Â  sdsÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  defaultÂ \n##Â  Â  student_t(3, 0, 5.9)Â  Â  Â  sds s(year)Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  (vectorized)Â \n##Â  Â  student_t(3, 0, 5.9)Â  Â  sigmaÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  default\n\n# applying the recommendations\n# multi-level method for spline fitting\nb4.11 &lt;- brm(data = d2,Â \nÂ  Â  Â  Â  Â  Â  family = gaussian,Â \nÂ  Â  Â  Â  Â  Â  # k = 19, corresponds to 17 basis functions I guess ::shrugs::Â \nÂ  Â  Â  Â  Â  Â  # The default for s() is to use whatâ€™s called a thin plate regression splineÂ \nÂ  Â  Â  Â  Â  Â  # bs uses a basis splineÂ \nÂ  Â  Â  Â  Â  Â  temp ~ 1 + s(year, bs = \"bs\", k = 19),Â \nÂ  Â  Â  Â  Â  Â  prior = c(prior(normal(100, 10), class = Intercept),Â \nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  prior(normal(0, 10), class = b),Â \nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  prior(student_t(3, 0, 5.9), class = sds),Â \nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  prior(exponential(1), class = sigma)),Â \nÂ  Â  Â  Â  Â  Â  iter = 2000, warmup = 1000, chains = 4, cores = 4,Â \nÂ  Â  Â  Â  Â  Â  seed = 4,Â \nÂ  Â  Â  Â  Â  Â  control = list(adapt_delta = .99))\nExample: Multinomial Logistic Regression\n# Outcome categorical variable has k = 3 levels. We fit k-1 models. Hence the 2 intercept priors\n# intercept model\nget_prior(data = d,Â \nÂ  Â  Â  Â  Â  # refcat sets the reference category to the 3rd level\nÂ  Â  Â  Â  Â  family = categorical(link = logit, refcat = 3),\nÂ  Â  Â  Â  Â  # just an intercept model\nÂ  Â  Â  Â  Â  career ~ 1)\n##Â  Â  Â  Â  Â  Â  Â  Â  priorÂ  Â  class coef group resp dpar nlpar boundÂ  source\n##Â  Â  Â  Â  Â  Â  Â  Â  (flat) InterceptÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  default\n##Â  student_t(3, 3, 2.5) InterceptÂ  Â  Â  Â  Â  Â  Â  Â  Â  mu1Â  Â  Â  Â  Â  Â  default\n##Â  student_t(3, 3, 2.5) InterceptÂ  Â  Â  Â  Â  Â  Â  Â  Â  mu2Â  Â  Â  Â  Â  Â  default\n\nb11.13io &lt;-\nÂ  brm(data = d,Â \nÂ  Â  Â  family = categorical(link = logit, refcat = 3),\nÂ  Â  Â  career ~ 1,\nÂ  Â  Â  prior = c(prior(normal(0, 1), class = Intercept, dpar = mu1),\nÂ  Â  Â  Â  Â  Â  Â  Â  prior(normal(0, 1), class = Intercept, dpar = mu2)),\nÂ  Â  Â  iter = 2000, warmup = 1000, cores = 4, chains = 4,\nÂ  Â  Â  seed = 11,\nÂ  Â  Â  file = \"fits/b11.13io\")\n\nAs of brms 2.12.0, â€œspecifying global priors for regression coefficients in categorical models is deprecated.â€ Meaning â€” if we want to use the same prior for both, we need to use the dpar argument for each"
  },
  {
    "objectID": "qmd/bayes-priors.html#sec-priors-finterp",
    "href": "qmd/bayes-priors.html#sec-priors-finterp",
    "title": "Priors",
    "section": "Formulating an Intercept Prior",
    "text": "Formulating an Intercept Prior\n\nExample SR 6.3.1 pgs 182-83\n\nA thought process on how to set a predictor prior based on its relationship to the outcome and an intercept prior.\n\nExample SR 7.1.1 pg 200\n\nOutcome variable was scaled, outcome/max(outcome)\n\nValues now between 0 and 1\nUseful for when 0 is a meaningful boundary\n\nNow able to center the intercept prior on mean of outcome, Î± âˆ¼ Normal(0.5, 1)\n\nSays that the average species with an average body mass (predictor variable) has a brain volume (outcome variable) with an 89% credible interval (Â± 1.5 sd) from about âˆ’1 to 2.\n\nBody mass was centered, so itâ€™s at its average is when its value is zero.\n\n\n\nExample SR 8.3.2 pg 259\n\nSimilar to 7.1.1 example except thereâ€™s the observation that a sd = 1 for the intercept prior is too large given that the outcome is bdd between 0 and 1 (after scaling)\na &lt;- rnorm( 1e4 , 0.5 , 1 )\nsum( a &lt; 0 | a &gt; 1 ) / length( a )\n[1] 0.6126\n\n61% of the prior is outside the bounds for the outcome which makes no sense\n\nIf itâ€™s 0.5 units from the mean to zero, then a standard deviation of 0.25 should put only 5% of the mass outside the valid range.\na &lt;- rnorm( 1e4 , 0.5 , 0.25 )\nsum( a &lt; 0 | a &gt; 1 ) / length( a )\n[1] 0.0486\n\nNot sure why you want 5% outside the valid range of the outcome variable\n\n\nExample (Ch 11 pg 335-6)\n\nWith logistic regression, flat Normal priors arenâ€™t priors with a high sd.\n\n\nThe Normal prior on the logit scale with the large sd says that the probabilty of an event is either 0 or 1 which usually isnâ€™t reasonable.\n\nlogit(pi) = Î±\n\nÎ± ~ Normal(0, 1.5) â€” the curve for the probability of an event is very flat, looks like a mesa\nÎ± ~ Normal(0, 1.0) â€”the curve for the probability of an event is a fat hill shape. A little more skeptical of extreme probabilities\n\n\nExample\n\nAlso have ggplot code in Documents &gt;&gt; R &gt;&gt; Code &gt;&gt; Simulations &gt;&gt; sim-prior-predictive-distr.R\nPoisson regression (pg 356)\nIn poisson regression, flat normal priors arenâ€™t priors with high s.d.\n\n# prior predictive distribution\ncurve( dlnorm( x , 3 , 0.5 ) , from=0 , to=100 , n=200 )\n\nSince poisson regression uses a log link, the outcome is log-normal. Weâ€™re simulating the effect of a normal prior on a log-normal outcome which is why the simulation code uses dlnorm.\nâ€œnumber of toolsâ€ is the outcome variable\nThe prior with s.d. 10 has almost all the probability density at zero and huge mean\na &lt;- rnorm(1e4,0,10)\nlambda &lt;- exp(a)\nmean(lambda)\n[1] 9.622994e+12\nThis usually doesnâ€™t make sense for a prior\nThe prior with s.d. 0.5 has a mean around 20 and a more spread out probability density which makes much more sense given the literature on the subject."
  },
  {
    "objectID": "qmd/bayes-priors.html#sec-priors-fslopp",
    "href": "qmd/bayes-priors.html#sec-priors-fslopp",
    "title": "Priors",
    "section": "Formulating a Slope Prior",
    "text": "Formulating a Slope Prior\n\nExample SR pg 259\n\nslopes centered on zero, imply no prior information about direction\nHow big could can the slopes be in theory?\n\nAfter centering, range of each predictor is 2â€”from âˆ’1 to 1 is 2 units.\nTo take us from the theoretical minimum of outcome variable = 0 on one end to the observed maximum of 1â€”a range of 1 unitâ€”on the other would require a slope of 0.5 from either predictor variableâ€”0.5 Ã— 2 = 1.\n\nAssign a standard deviation of 0.25, then 95% of the prior slopes are from âˆ’0.5 to 0.5, so either predictor could in principle account for the entire range, but it would be unlikely\n\nExample SR pg 336-7\n\nWith logistic regression, flat Normal priors arenâ€™t priors with a high sd.\n\n\nShows difference between two levels of the treatment effect (i.e.Â 2 different treatments) on the 0/1 outcome\nThe prior with large sd has all the probability density massed at 0 and 1\n\nSays that the 2 treatments are completely alike or completely different\n\nThe prior with the small sd (e.g.Â Normal(0, 0.5) is concentrated from about 0 to about 0.4\n\nAlthough 0 difference in treatments has the highest probability, the mean is at a difference around 0.10\n\nPrior says that large differences between treatments are very unlikely, but if the data contains strong evidence of large differences, they will shine through\n\nPairs nicely with an intercept prior, Î± ~ Normal(0, 1.5)\nAn example of a weakly informative prior that reduces overfitting the sample data\n\n\n\nExample: pg 357\n\nset.seed(11)\n## TOP ROW\n\n# how many lines would you like?\nn &lt;- 100\n# simulate and wrangle\ntibble(i = 1:n,\n       a = rnorm(n, mean = 3, sd = 0.5)) %&gt;%\n  mutate(`beta%~%Normal(0*', '*10)` = rnorm(n, mean = 0 , sd = 10),\n         `beta%~%Normal(0*', '*0.2)` = rnorm(n, mean = 0 , sd = 0.2)) %&gt;%\n  pivot_longer(contains(\"beta\"),\n               values_to = \"b\",\n               names_to = \"prior\") %&gt;%\n  expand(nesting(i, a, b, prior),\n         x = seq(from = -2, to = 2, length.out = 100)) %&gt;%\n\n  # plot\n  ggplot(aes(x = x, y = exp(a + b * x), group = i)) +\n  geom_line(size = 1/4, alpha = 2/3,\n            color = wes_palette(\"Moonrise2\")[4]) +\n  labs(x = \"log population (std)\",\n       y = \"total tools\") +\n  coord_cartesian(ylim = c(0, 100)) +\n  facet_wrap(~ prior, labeller = label_parsed)\n\n## BOTTOM ROW\nprior &lt;-\n  tibble(i = 1:n,\n         a = rnorm(n, mean = 3, sd = 0.5),\n         b = rnorm(n, mean = 0, sd = 0.2)) %&gt;%Â \n  expand(nesting(i, a, b),\n         x = seq(from = log(100), to = log(200000), length.out = 100))\n# left\np1 &lt;-\n  prior %&gt;%\n  ggplot(aes(x = x, y = exp(a + b * x), group = i)) +\n  geom_line(size = 1/4, alpha = 2/3,\n            color = wes_palette(\"Moonrise2\")[4]) +\n  labs(subtitle = expression(beta%~%Normal(0*', '*0.2)),\n       x = \"log population\",\n       y = \"total tools\") +\n  coord_cartesian(xlim = c(log(100), log(200000)),\n                  ylim = c(0, 500))\n# right\np2 &lt;-\n  prior %&gt;%\n  ggplot(aes(x = exp(x), y = exp(a + b * x), group = i)) +\n  geom_line(size = 1/4, alpha = 2/3,\n            color = wes_palette(\"Moonrise2\")[4]) +\n  labs(subtitle = expression(beta%~%Normal(0*', '*0.2)),\n       x = \"population\",\n       y = \"total tools\") +\n  coord_cartesian(xlim = c(100, 200000),\n                  ylim = c(0, 500))\n# combine\np1 | p2\n\nWith poisson regression, flat Normal priors arenâ€™t priors with a high sd.\nOutcome: total_tools, predictor: log_population\nBottom row fig titles have a typo. Should be a ~ dnorm(3, 0.5) since itâ€™s the Intercept prior\nVariables have been standardized; total_tools simulated with intercept + predictor priors. So the y axis is simulating the potential fitted values.\nTop Left (0 is mean of log_population):\n\nlarge sd: mostly results in explosive growth of tools just after mean of log_population or explosive decline just before mean log_population (unlikely)\n\nTop Right (0 is mean of log_population)\n\nsmall sd (flatter): most results are around the mean of the intercept prior results (see above) but still allows for more extreme estimates. (reasonable)\n\nBottom Left\n\n100 trend lines between total tools and un-standardized log population\n\nViewing prior predictive trends with un-standardized variables is more natural to see whatâ€™s happening\n\n100 total tools is probably the most we expect to ever see in these data\n\nLooks like 80-85% of the trend lines are under 100. still keeps some explosive possibilities.\n\n\nBottom Right\n\n100 trend lines between total tools and un-standardized, un-logged population\n\nViewing prior predictive trends with un-standardized, un-transformed variables is even more natural to see whatâ€™s happening\n\nWhen a predictor variable is logged in a regression with a log-link (i.e.Â log-log), this means we are assuming diminishing returns for the raw predictor variable.\n\nEach additional person contributes a smaller increase in the expected number of tools\nDiminishing returns as a predictor value continues to increase makes sense in many situations which is why logging predictors is a popular transformation\n\n\nThoughts\n\nBottom-right seems like the right way to visualize the prior to think about the association between the outcome and predictor\nTop row and bottom-left seem to give a better sense of how many explosive possibilities and their patterns that your allowing for with different transformations"
  },
  {
    "objectID": "qmd/bayes-priors.html#sec-priors-fsigp",
    "href": "qmd/bayes-priors.html#sec-priors-fsigp",
    "title": "Priors",
    "section": "Formulating a Sigma Prior",
    "text": "Formulating a Sigma Prior\n\nCommon to start with exponential(1)\nTightening the spread of the Exponential distribution by using a Gamma distribution (Thread)\n\n\nYou can keep â€œmean = 1â€ (aka exponential(1) and adjust the â€œsdâ€.\n\nSee Distributions &gt;&gt; Gamma for details on the process\n\nAlso allows you to move most of the mass of the prior a littler further away from 0.\nAnother alternative is the Weibull distribution"
  },
  {
    "objectID": "qmd/bayes-priors.html#sec-priors-eapfam",
    "href": "qmd/bayes-priors.html#sec-priors-eapfam",
    "title": "Priors",
    "section": "Extracting a Prior From a Model",
    "text": "Extracting a Prior From a Model\n\nExample: Logistic Regression (SR sect 11.1.1 pg 336)\n\nIntercept\n\n# prior_samples and inv_logit_scaled are brms functions\n# theme is from ggthemes\nprior_samples(b11.1) %&gt;%\n  mutate(p = inv_logit_scaled(Intercept)) %&gt;%\n\n  ggplot(aes(x = p)) +\n  geom_density(fill = wes_palette(\"Moonrise2\")[4],\n               size = 0, adjust = 0.1) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  xlab(\"prior prob pull left\")"
  },
  {
    "objectID": "qmd/bayes-priors.html#sec-priors-simppd",
    "href": "qmd/bayes-priors.html#sec-priors-simppd",
    "title": "Priors",
    "section": "Simulating a Prior Predictive Distribution",
    "text": "Simulating a Prior Predictive Distribution\n\nExample: SR pg 176\n# log-normal prior\nsim_p &lt;- rlnorm( 1e4 , 0 , 0.25 )\n\n# \"this prior expects anything from 40% shrinkage up to 50% growth\"\nrethinking::precis( data.frame(sim_p) )\n# 'data.frame': 10000 obs. of 1 variables:\n#       mean    sd  5.5% 94.5% histogram\n# sim_p 1.03  0.26  0.67  1.48 â–â–ƒâ–‡â–‡â–ƒâ–â–â–â–â–â–\n\n# tidy-way\nsim_p &lt;-\n  tibble(sim_p = rlnorm(1e4, meanlog = 0, sdlog = 0.25))sim_p %&gt;%\n    mutate(`exp(sim_p)` = exp(sim_p)) %&gt;%\n    gather() %&gt;%\n    group_by(key) %&gt;%\n    tidybayes::mean_qi(.width = .89) %&gt;%\n    mutate_if(is.double, round, digits = 2)\n\n## # A tibble: 2 x 7\n##  key         value .lower .upper .width .point .interval\n##  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;chr&gt;     &lt;chr&gt;\n## 1 exp(sim_p)  2.92   1.96   4.49   0.89   mean        qi\n## 2 sim_p       1.03   0.67    1.5   0.89   mean        qi\n\nVisualize with ggplot\n\n# wrangle\nsim_p %&gt;%\n  mutate(`exp(sim_p)` = exp(sim_p)) %&gt;%\n  gather() %&gt;%\n  # plot\n  ggplot(aes(x = value)) +\n  geom_density(fill = \"steelblue\") +\n  scale_x_continuous(breaks = c(0, .5, 1, 1.5, 2, 3, 5)) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  coord_cartesian(xlim = c(0, 6)) +\n  theme(panel.grid.minor.x = element_blank()) +\n  facet_wrap(~key, scale = \"free_y\", ncol = 1)\n\nExample:\n\nPossible Intercept Priors\n\ngrid &lt;- seq(-3, 3, \n             length.out = 1000) # evenly spaced values from -3 to 3\nb0_prior &lt;- \n   map_dfr(.x = c(0.5, 1, 2), # .x represents the three sigmas \n           ~ data.frame(grid = grid,\n                        b0 = dnorm(grid, mean = 0, sd = .x)),\n                        .id = \"sigma_id\")\n# Create Friendlier Labels\nb0_prior &lt;- b0_prior %&gt;%\n  mutate(sigma_id = factor(sigma_id, \n         labels = c(\"normal(0, 0.5)\",\n                    \"normal(0, 1)\",\n                    \"normal(0, 2)\")))\nggplot(b0_prior, aes(x = grid, y = b0)) +\n  geom_area(fill = \"cadetblue4\", color = \"black\", alpha = 0.90) +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0), limits = c(0, 0.85)) +\n  labs(x = NULL,\n       y = \"probability density\",\n       title = latex2exp::TeX(\"Possible $\\\\beta_0$ (intercept) priors\")) +\n       facet_wrap(~sigma_id, nrow = 3)\nSampling of lines from \\(\\beta_0\\) and \\(\\beta_1\\) priors\n\nb0b1 &lt;- \n  map2_df(.x = c(0.5, 1, 2), \n          .y = c(0.25, 0.5, 1), \n          ~ data.frame(\n              b0 = rnorm(100, mean = 0, sd = .x),\n              b1 = rnorm(100, mean = 0, sd = .y)), \n          .id = \"sigma_id\"\n  )\n\n# Create friendlier labels\nb0b1 &lt;- \n  b0b1 %&gt;%\n    mutate(sigma_id = factor(sigma_id, \n                             labels = c(\"b0 ~ normal(0, 0.5); b1 ~ normal(0, 0.25)\",\n                                        \"b0 ~ normal(0, 1); b1 ~ normal(0, 0.50)\",\n                                        \"b0 ~ normal(0, 2); b1 ~ normal(0, 1)\")))\n\nggplot(b0b1) +\n  geom_abline(aes(intercept = b0, slope = b1), color = \"cadetblue4\", alpha = 0.75) +\n  scale_x_continuous(limits = c(-2, 2)) +\n  scale_y_continuous(limits = c(-3, 3)) +\n  labs(x = \"x\",\n       y = \"y\",\n  title = latex2exp::TeX(\"Sampling of lines from $\\\\beta_0$ and $\\\\beta_1$ priors\")) +\n  facet_wrap(~sigma_id, nrow = 3)"
  },
  {
    "objectID": "qmd/bayes-priors.html#sec-priors-conjp",
    "href": "qmd/bayes-priors.html#sec-priors-conjp",
    "title": "Priors",
    "section": "Conjugate Priors",
    "text": "Conjugate Priors\n\nIf the posterior distributions p(Î¸ | x) are in the same probability distribution family as the prior probability distribution p(Î¸), the prior and posterior are then called conjugate distributions, and the prior is called a conjugate prior for the likelihood function p(x | Î¸)\nBenefits\n\nBayesian updates no longer need to compute the product of the likelihood and prior (only addition is needed).\n\nThis product is computationally expensive and sometimes not feasible.\nOtherwise numerical integration may be necessary\n\nMay give intuition, by more transparently showing how a likelihood function updates a prior distribution.\n\nAll members of the exponential family have conjugate priors.\nList\n&lt;Beta posterior&gt;\nBeta prior * Bernoulli likelihood â†’ Beta posterior\nBeta prior * Binomial likelihood â†’ Beta posterior\nBeta prior * Negative Binomial likelihood â†’ Beta posterior\nBeta prior * Geometric likelihood â†’ Beta posterior\n\n&lt;Gamma posterior&gt;\nGamma prior * Poisson likelihood â†’ Gamma posterior\nGamma prior * Exponential likelihood â†’ Gamma posterior\n\n&lt;Normal posterior&gt;Â \nNormal prior * Normal likelihood (mean) â†’ Normal posterior\n\n&lt;Others&gt;\nDirichlet prior * Multinomial likelihood â†’ Dirichlet posterior"
  },
  {
    "objectID": "qmd/bayes-priors.html#sec-priors-pelic",
    "href": "qmd/bayes-priors.html#sec-priors-pelic",
    "title": "Priors",
    "section": "Prior Elicitation",
    "text": "Prior Elicitation\n\nTranslating expert opinion to a probability density than you can use as a prior\nMisc\n\nNotes from:\n\nVideo: On using expert information in Bayesian statistics (R &gt;&gt; Videos)\nPaper: Methods for Eliciting Informative Prior Distributions (R &gt;&gt; Documents &gt;&gt; Bayes)\n\nPackages\n\n{SHELF} - shiny apps for eliciting for various distributions and via various methods\n\nwebpage also has links to papers\n\n\nPrior sensitivity analysis should done especially if youâ€™re using expert-informed priors.\n\nBest Practice\n\nUse domain experts to set constraints for your priors (e.g.Â upper and lower limits) instead of formulating a prior\n\nBest to use data, previous studies, etc. to formulate priors\n\nUse domain experts to inform you on relationships between variables\n\nElicitation process is resource intensive for what is probably minimal gain in comparison to data\n\nIt takes a lot of your time and their time to get this right\nExperts may not have the statistical knowledge to understand what you need or how to convey the information\n\nIn this case, youâ€™ll need to school them on basic statistical concepts\n\n\nWhen to use expert information to formulate a prior\n\nIf your DAG specifies a model that requires more data than you have.\n\nWhile it might be necessary to augment your data, be aware that expert knowledge has been shown to much less useful in complex models rather than simpler models.\n\nIf the your data is really noisy\nIf experts know something that isnâ€™t represented by your data or canâ€™t be captured by the data\nIf domain expertise is required given your research question\n\nInterviews with experts\n\nQuality Control: If your subject matter allows, try to create â€œcalibratingâ€ questions to weed-out the experts that arenâ€™t really experts\n\nShould be questions that you are certain of the answer and are things any expert should know\n\nThis can be difficult for some subject matter.\n\nMaybe consult with an expert that youâ€™re confident is an expert to help come up with some questions.\n\nQuestions should be standardized, so you know that the results from each expert are consistent.\nFace-to-face elicitation produces greater quality results, because the facilitator can clarify the questions from the experts if needed.\nTry to keep experts from biasing the information they give you\n\nDonâ€™t use experts that have seen the results of your model\n\nIf theyâ€™ve seen your raw data thatâ€™s okay. (I dunno about this, even if theyâ€™ve seen eda plots?)\n\nDonâ€™t provide them with any estimates you may have from previous studies or other experts\nDonâ€™t let them fixate on outlier scenarios they may have encountered\n\nRecord conversations with video and/or audio\n\nIf problems surface when evaluating the expertâ€™s information, these can be useful to go back over the information collection process\n\nWas there a misunderstanding between you and the expert on what information you wanted\nWas the information biased? (see above)\nIf using mulitiple experts, maybe subgroups have different viewpoints/experiences which is causing a divergence in opinion (e.g nurses vs psychologists treating PTSD)\n\n\nIf problems surface when evaluating the expertâ€™s information, it can be useful to gather specific experts that differ and have them discuss why they hold their substantially differing opinions. Afterwards, they may adjust their opinions and youâ€™ll have a greater consensus.\nProcess\n\nElicit location parameter (e.g.Â via Trial Roulette) from the expert\n\nTrial Roulette (see paper in Misc for details)\n\nRequires the expert to have sufficient statistical knowledge to be able to place the blocks to form an appropriate distribution\nUser should be aware that distribution output may be inappropriate for sample data\n\nExample: Algorithm may output a Gamma distribution which is inappropriate for percentage data since the upper bound can be greater than one\n\nParameter space is split into subsections (e.g.Â quantiles)\nUser assigns blocks to each subsection\nExample From MATCH website which was an earlier implementation of {SHELF}\n\nTop chart is a histogram where each cell is a â€œblockâ€ (called â€œchipsâ€ at the bottom). The right panel shows the options for setting the axis ranges and number of bins\nBottom chart evidently estimates distribution parameters from the histogram in the top chart which are your priorâ€™s parameters\n\n\nWith experts with less statistical training, it may be better for you to give them scenarios (e.g.Â combinations of quantiles of the predictor variables) and have them predict the outcome variable.\n\nCompute the statistics given their answers. Show them the results. Ask them to give an uncertainty range around that statistic.\nExample: From their predictions, you calculate the mean. Then you present them with this average and ask them about their uncertainty?\n\ni.e.Â What is the range around this value they expect the average to be in?\n\n\nAlso try combinations of methods\nSee paper in Misc for other options\n\nFeedback session\n\nExplain to the expert how youâ€™re interpreting their information. Do they agree with your interpretation? Refine information based on their feedback.\n\nElicit scale and shape parameters (upper and lower bounds)\nFeedback session\nEvaluate distribution\n\n\nEvaluating Expert Distribution(s)\n\nMisc\n\nMight be better to use another measure instead of K-L divergence (see Inspect the distributions visually section below)\n\ne.g Jensen-Shannon Divergence, Population Stability Index (see Production, ML Monitoring for details)\n\n\nCalculate K-L divergence between the expert distribution and the computed posterior using the expert distribution as a prior\n\nSmaller K-L divergence means the 2 distributions are more similar\nLarger K-L divergence means the 2 distributions are more different\n\nCreate a benchmark distribution\n\nShould be a low information distribution as compared to the sample data distribution\ne.g.Â uniform distribution\n\nCalculate K-L divergence between the benchmark distribution and the computed posterior using the benchmark distribution as the prior\nCalculate ratio of K-L divergences (expert K-L/benchmark K-L)\n\nGreater than 1 is bad. Indicates a â€œprior data conflictâ€ and it may be better to drop this expertâ€™s distribution\nLess than 1 is good. Potentially an informative prior\n\nInspect the distributions visually (Expert prior distributions and computed posterior from benchmark prior)\n\nK-L divergence penalyzes more certain distributions (i.e.Â skinny, tall) than less certain distribtutions (fatter, shorter) even if they have the same mean/median and mostly the same information\n\nSo, an expert that is more certain may have a disqualifying ratio of K-L difference while a less certain expert with a very similar distribution has a qualifying ratio.\n\nAfter inspecting the distributions, you may determine that distributions really are too different and the expert is far too certain to keep.\n\n\n\n\nAggregate distributions if youâ€™re eliciting from multiple experts\n\nAverage the distributions (i.e.Â equal weights for all experts)\nRank experts (e.g.Â by K-L ratio), weight them, then calculate a weighted average distribution\nUse aggregated distribution(s) as your prior(s)"
  },
  {
    "objectID": "qmd/bayes-reporting.html#sec-bayes-rep-misc",
    "href": "qmd/bayes-reporting.html#sec-bayes-rep-misc",
    "title": "Reporting",
    "section": "Misc",
    "text": "Misc\n\nAlso see Mathematics, Statistics &gt;&gt; Descriptive &gt;&gt; Understanding CI, sd, and sem Bars\n{posterior} rvars class\n\nobject class thatâ€™s designed to interoperate with vectorized distributions in {distributional}, to be able to be used inside data.frame()s and tibble()s, and to be used with distribution visualizations in the {ggdist}.\nDocs\n\nRemember CIs of parameter estimates including zero are not evidence of the null hypothesis (i.e.Â Î² = 0).\n\nEspecially if CIs are broad and most of the posterior probability distribution is massed away from zero\n\nVisualization for differences (Thread)"
  },
  {
    "objectID": "qmd/bayes-reporting.html#sec-bayes-rep-sdfe",
    "href": "qmd/bayes-reporting.html#sec-bayes-rep-sdfe",
    "title": "Reporting",
    "section": "Significant Digits for Estimates",
    "text": "Significant Digits for Estimates\n\nMisc\n\nNotes from: Bayesian workflow book - Digits\n\nBefore we can answer how many chains and iterations we need to run, we need to know how many significant digits we want to report\nMCMC in general doesnâ€™t produce independent draws and the effect of dependency affects how many draws are needed to estimate different expectations\nGuidelines in general\n\nIf the posterior would be close to a normal(Î¼,1), then\n\nFor 2 significant digit accuracy,\n\n2000 independent draws from the posterior would be sufficient for that 2nd digit to only sometimes vary.\n4 chains with 1000 iterations after warmup is likely to give near two significant digit accuracy for the posterior mean. The accuracy for 5% and 95% quantiles would be between one and two significant digits.\nWith 10,000 draws, the uncertainty is 1% of the posterior scale which would often be sufficient for two significant digit accuracy.\n\nFor 1 significant digit accuracy, 100 independent draws would be often sufficient, but reliable convergence diagnostics may need more iterations than 100.\nFor posterior quantiles, more draws may be needed (need more draws to get values towards the tails of the posterior)\n\nSome quantities of interest may have posterior distribution with infinite variance, and then the ESS and MCSE are not defined for the expectation.\n\nIn such cases, use median instead of mean and mean absolute deviation (MAD) instead of standard deviation.\nVariance of parameter posteriors\nas_draws_rvars(brms_fit) %&gt;%\nÂ  Â  summarise_draws(var = distributional::variance)Â \n#&gt;Â  Â  variableÂ  var\n#&gt;Â  Â  &lt;chr&gt;Â  Â  &lt;dbl&gt;\n#&gt;Â  1 muÂ  Â  Â  Â  11.6\n#&gt;Â  2 tauÂ  Â  Â  12.8\n#&gt;Â  3 theta[1]Â  39.7\n#&gt;Â  4 theta[2]Â  21.5\n\n\nSteps\n\nCheck convergence diagnostics for all parameters\n\ne.g.Â RHat, ESS, autocorrelation plots (see Diagnostics, Bayes)\n\nLook at the posterior for quantities of interest and decide how many significant digits is reasonable taking into account the posterior uncertainty (using SD, MAD, or tail quantiles)\n\nYou want to be able to distinguish you upper or lower CI from the point estimate\n\ne.g.Â Point estimate is 2.1 and you upper CI is 2.1 then you want at least another significant digit.\n\n\nCheck that MCSE is small enough for the desired accuracy of reporting the posterior summaries for the quantities of interest.\n\nCalculate the range of variation due to MC sampling for your paramter (See MCSE example)\n\nMC sampling error is the average amount of variation thatâ€™s expected from changing seeds and re-running the analysis\n\nIf the accuracy is not sufficient (i.e.Â range is too wide), report less digits or run more iterations.\n\n\nMonte Carlo standard error (MCSE) - uncertainty about a parameter estimate due to MCMC sampling error\n\nPackages\n\n{posterior} is the preferred package for brms objects\n{mcmcse} - methods to calculate MCMC standard errors for means and quantiles using sub-sampling methods. (Different calculation than used by Stan)\nbayestestR::mcse uses Kruschke 2015 method of calculation\n\nExample: brms, MCSE quantiles\n# Coefficient and CI estimates for the \"beta100\" variable\nas_draws_rvars(brms_fit) %&gt;%\n  subset_draws(\"beta100\") %&gt;%\n  summarize_draws(mean, ~quantile(.x, probs = c(0.05, 0.95)))\n#&gt; variable  mean      5%   95%\n#&gt; beta100   1.966  0.673 3.242\n\nas_draws_rvars(brms_fit) %&gt;%\n  subset_draws(\"beta100\") %&gt;% # select variable\n  summarize_draws(mcse_mean, ~mcse_quantile(.x, probs = c(0.05, 0.95)))\n#&gt; variable  mcse_mean  mcse_q5 mcse_q95\n#&gt; beta100       0.013    0.036    0.033\n\nSpecification\n\nâ€œmcse_meanâ€ and â€œmeanâ€ are available as preloaded functions that summary_draws can use out of the box\nâ€œmcse_quantileâ€ (also in {posterior}) and â€œquantileâ€ are not preloaded functions so theyâ€™re called as lambda functions\n\nThese are MCSE values for\n\nthe summary estimate (aka point estimate) which is the mean of the posterior in this case\nAnd the CI values of that summary estimate\n\nTail quantiles will have greater amounts of error sampling in the tails of the posterior than in the bulk (i.e.Â less accurate tail estimates)\nFewer points, more uncertainty\n\n\nCalculate the range of variation due to Monte Carlo\n\nMultiply the MCSE values by 2, the likely range of variation due to Monte Carlo is Â±0.02 for mean and Â±0.07 for 5% and 95% quantiles\n\nMultiplying by 2, since I guess theyâ€™re assuming a normal distribution posterior, therefore estimate Â± 1.96 * SE\n\n\nConclusion for â€œbeta100â€ coefficient\n\nIf the mean estimate for beta100 is reported as 2 (rounded up from 1.966), then there is unlikely to be any variation in that estimate due to MCMC sampling. (i.e.Â okay to report the estimate as 2)\n\nThis is because\n\n1.966 + 0.02 = 1.986 which would still be rounded up to 2\n1.966 - 0.02 = 1.946 which would still be rounded up to 2\n\n\n\nDraws and iterations\n\nWith an MCSE in the 100ths (e.g.Â 0.07), 4 times more iterations would halve the MCSEs\nWith an MCSE in the 1000ths (e.g.Â 0.007), 64 times more iterations would halve the MCSEs\nMCSEs depend on the quantity type. Continuous quantities (e.g.Â parameter estimates) have more information than discrete quantities (e.g.Â indicator values used to calculate probabilities).\n\nFor example, above, the estimate for whether the temperature increase is larger than 4 degrees per century has high ESS, but the indicator variable contains less information (than continuous values) and thus much higher ESS would be needed for two significant digit accuracy."
  },
  {
    "objectID": "qmd/bayes-reporting.html#probabilistic-inference-of-estimates",
    "href": "qmd/bayes-reporting.html#probabilistic-inference-of-estimates",
    "title": "Reporting",
    "section": "Probabilistic Inference of Estimates",
    "text": "Probabilistic Inference of Estimates\n\nMisc\n\nNotes from: Bayesian workflow book - Digits\n\nExample: probability that an estimate is positive\nas_draws_rvars(brms_fit) %&gt;%\n  # binary 1/0, posterior samples &gt; 0\n  mutate_variables(beta0p = beta100 &gt; 0) %&gt;% \n  subset_draws(\"beta0p\") %&gt;% # select variable\n  summarize_draws(\"mean\", mcse = mcse_mean)\n\n#&gt; variable  mean  mcse\n#&gt;   beta0p 0.993 0.001\n99.3% probability the estimate is above zero +/- 0.2% (= 2*MCSE)\nMCSE indicates that we have enough MCMC iterations for practically meaningful reporting that the probability that the variable (e.g.Â temperature) is increasing (i.e.Â slope is positive) is larger than 99%\nExample: probability that an estimate &gt; 1,2,3,4\nas_draws_rvars(brms_fit) %&gt;%\nÂ  subset_draws(\"beta100\") %&gt;%\nÂ  # binary 1/0 variable\nÂ  mutate_variables(beta1p = beta100 &gt; 1,\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  beta2p = beta100 &gt; 2,\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  beta3p = beta100 &gt; 3,\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  beta4p = beta100 &gt; 4) %&gt;%\nÂ  subset_draws(\"beta[1-4]p\", regex=TRUE) %&gt;%\nÂ  summarize_draws(\"mean\", mcse = mcse_mean, ESS = ess_mean)\n\n#&gt; variableÂ  mean mcseÂ  ESS\n#&gt;  beta1p 0.896 0.006 3020\n#&gt;  beta2p 0.487 0.008 4311\n#&gt;  beta3p 0.088 0.005 3188\n#&gt;  beta4p 0.006 0.001 3265\nTaking into account MCSEs given the current posterior sample, we can summarise these as\n\np(beta100&gt;1) = 88%â€“91%,\np(beta100&gt;2) = 46%â€“51%,\np(beta100&gt;3) = 7%â€“10%,\np(beta100&gt;4) = 0.2%â€“1%.\n\nTo get these probabilities estimated with 2 digit accuracy would again require more iterations (16-300 times more iterations depending on the quantity), but the added iterations would not change the conclusion radically.perature in the center of the time range (instead defining prior for temperature at year 0)."
  },
  {
    "objectID": "qmd/bayes-troubleshooting-hmc.html#sec-trblhmc-misc",
    "href": "qmd/bayes-troubleshooting-hmc.html#sec-trblhmc-misc",
    "title": "Troubleshooting HMC",
    "section": "Misc",
    "text": "Misc"
  },
  {
    "objectID": "qmd/bayes-troubleshooting-hmc.html#sec-trblhmc-divtrans",
    "href": "qmd/bayes-troubleshooting-hmc.html#sec-trblhmc-divtrans",
    "title": "Troubleshooting HMC",
    "section": "Divergent Transitions",
    "text": "Divergent Transitions\n\nâ€œdivergent transitions after warmupâ€\nSee Taming Divergences in Stan Models and Identifying non-identifiability\nSee Statistical Rethinking &gt;&gt; Ch.9 MCMC &gt;&gt; Issues\nDivergent Transition - A rejected proposed parameter value in the posterior during the sampling process\n\nToo many DTs could indicate a poor exploration of the posterior by the sampling algorithm and possibly biased estimates.\n\nIf the DTs are happening in the same region of the posterior then that region isnâ€™t being sampled by the HMC algorithm\n\nIf there are â€œsteepâ€ areas in the posterior, these areas can break the sampling process resulting in a â€œbadâ€ proposed parameter value.\n\nSolutions\n\nadjust priors from flat to weakly informative\nNeed more data\nIncrease adapt_delta closer to 1 (default: 0.8)\nReparameterize the model"
  },
  {
    "objectID": "qmd/bayes-troubleshooting-hmc.html#sec-trblhmc-chnmix",
    "href": "qmd/bayes-troubleshooting-hmc.html#sec-trblhmc-chnmix",
    "title": "Troubleshooting HMC",
    "section": "Chains Not Mixing",
    "text": "Chains Not Mixing\n\nMisc\n\nNotes from When MCMC fails: The advice weâ€™re giving is wrong. Hereâ€™s what we you should be doing instead. (Hint: itâ€™s all about the folk theorem.)\n\nPotential issues\n\nPriors on some parameters are weak or nonexistent or the data are too weak to identify all the parameters in the model.\n\nSigns: Chains are exploring extreme regions of the parameter space. Check out the y-axis range in trace and see how high or low the values are.\nExamples: elasticity parameters of -20 or people with 8 kg livers\n\nCoding mistake\n\nStan examples:\n\nYou can forget to use a log link or set a prior using variance instead sd\nArray indices and for loops donâ€™t match\n\n\nMinor modes in the tails of the posterior distribution\n\nYour posterior is multimodal and all but one of the modes have near-zero mass\nSigns: different chains will cluster in different places\nSolutions:\n\nUsing starting values near the main mode (brms init arg; see [Statistical Rethinking &gt;&gt; Ch 4]((https://ercbk.github.io/Statistical-Rethinking-Notebook/qmd/chapter-4.html){style=â€œcolor: greenâ€}\n\nCan also be used in general cases where youâ€™re getting bad mixing from you chains\n\ne.g.Â divergent transitions, large numbers of transitions, high R-hat values, and/or very low effective sample size estimates\n\nDiagnostic example\n\nCheck the intercept warm-up (For real-world models, itâ€™s good to look at the trace plots for all major model parameters)\n\ngeom_trace &lt;- function(subtitle = NULL,Â \nÂ  Â  Â  Â  Â  Â  Â  Â  Â  xlab = \"iteration\",Â \nÂ  Â  Â  Â  Â  Â  Â  Â  Â  xbreaks = 0:4 * 500) {\nlist(\nannotate(geom = \"rect\",Â \nÂ  Â  Â  Â  xmin = 0, xmax = 1000, ymin = -Inf, ymax = Inf,\nÂ  Â  Â  Â  fill = fk[16], alpha = 1/2, size = 0),\ngeom_line(size = 1/3),\nscale_color_manual(values = fk[c(3, 8, 27, 31)]),\nscale_x_continuous(xlab, breaks = xbreaks, expand = c(0, 0)),\nlabs(subtitle = subtitle),\ntheme(panel.grid = element_blank())\n)Â  Â \n}\np1 &lt;-\nggmcmc::ggs(fit1, burnin = TRUE) %&gt;%\nfilter(Parameter == \"b_Intercept\") %&gt;%Â \nmutate(chain = factor(Chain),\nÂ  Â  intercept = value) %&gt;%Â  Â \nggplot(aes(x = Iteration, y = intercept, color = chain)) +\ngeom_trace(subtitle = \"fit1 (default settings)\") +\nscale_y_continuous(breaks = c(0, 650, 1300), limits = c(NA, 1430))Â \np1\n\n\n\n\n\nOne of our chains eventually made its way to the posterior, three out of the four stagnated near their starting values (lines near zero).\nSet starting values manually. Same values to all 4 chains.\ninits &lt;- list(\nIntercept = 1300,\nsigmaÂ  Â  = 150,\nbetaÂ  Â  Â  = 520\n)\nlist_of_inits &lt;- list(inits, inits, inits, inits)\nfit2 &lt;- brm(\ndata = dat,\nfamily = exgaussian(),\nformula = rt ~ 1 + (1 | id),\ncores = 4, seed = 1,\ninits = list_of_inits\n)\n\nMuch moâ€™ better, but not evidence of chain convergence since all started at the same value\nNo practical for large models with many parameters\nSet starting values (somewhat) randomly by function\nset_inits &lt;- function(seed = 1) {Â  Â \nset.seed(seed)\nlist(\n# posterior for the intercept often looks gaussian\nIntercept = rnorm(n = 1, mean = 1300, sd = 100),\n# posteriors for sigma and beta need to nonnegative (alt:rgamma)\nsigmaÂ  Â  = runif(n = 1, min = 100, max = 200),\nbetaÂ  Â  Â  = runif(n = 1, min = 450, max = 550)\n)Â  Â \n}\n\nlist_of_inits &lt;- list(\n# different seed values will return different results\nset_inits(seed = 1),\nset_inits(seed = 2),\nset_inits(seed = 3),\nset_inits(seed = 4)\n)\n\n\nChains are mixing and evidence of convergence since we started at different starting values\nNeed to also check sigma and beta\n\nFitting multiple models and averaging predictions (stacking). Donâ€™t fully understand but this is from a section 5.6 of Gelmanâ€™s Bayesian Workflow paper\n\nâ€œdivide the model into pieces by introducing a strong mixture prior and then fitting the model separately given each of the components of the prior. Other times the problem can be addressed using strong priors that have the effect of ruling out some of the possible modesâ€\n\nAlso â€œSoon we should have Pathfinder implemented and this will get rid of lots of these minor modes automaticallyâ€\nThe model can be reparameterized\n\nThink this had to do with Divergent Transitions\nsigns: you have coefficients like 0.000002\nsolutions:\n\nUse variables that have log transformed or scaled (e.g.Â per million). For some reason, ittâ€™s difficult for the sampler when parameters values are on vastly different scales.\nreparameterize to â€œunit scaleâ€\n\nI think scale inÂ  â€œunit scaleâ€ refers to scale as a distribution parameter like sd is the scale parameter in a Normal distribution, and â€œunit scaleâ€ is scale = 1 (e.g.Â sd = 1 in standardization). But thereâ€™s more to this and I havenâ€™t S.R. ch 13 yet\nparameters with distributions such as cauchy, student-t, normal, or any distribution in the location-scale family are good reparameterization candidates\nsee Stan User Guide\nSee Ch 13.4 in Statistical Rethinking\n\n\n\nCommon (misguided?) solutions\n\nIncrease iterations\nTweak adapt_delta and max_treedepth parameters to make it explore the space more carefully\n\nOther\n\nSequential Monte Carlo (SMC) is a potential solution multimodal posterior problem (Stanâ€™s NUTS sampler may already do this to some extent. See Ch. 9 Statistical Rethinking)\n\nBayesTools PKG implements a SMC sampler\nAlgorithm details - https://docs.pymc.io/notebooks/SMC2_gaussians.html?highlight=smc"
  },
  {
    "objectID": "qmd/bayes-workflow.html#sec-bayes-wkflw-misc",
    "href": "qmd/bayes-workflow.html#sec-bayes-wkflw-misc",
    "title": "Workflow",
    "section": "Misc",
    "text": "Misc\n\nAlso see Model Building, Concepts &gt;&gt; Misc &gt;&gt; Regression Workflow\nNotes from\n\nBayesian Workflow (Gelman, Vehtari)\n\nCurrent Checklist\n\nCheck convergence diagnostics\nDo posterior predictive checking\nCheck residual plots\nModel comparison (if prediction)\n\nAnalysis Checklist (Thread)\n\na suitably flexible Bayesian regression adjustment model,\nchosen by cross-validation/LOO,\nincluding Gaussian processes for the unit-level effects over time (and space/network if relevant),\nimputation of missing data, and\ninformative priors for biases in the data collection process."
  },
  {
    "objectID": "qmd/big-data.html#sec-bgdat-misc",
    "href": "qmd/big-data.html#sec-bgdat-misc",
    "title": "Big Data",
    "section": "Misc",
    "text": "Misc\n\nRcppArmadillo::fastLmPure Not sure what this does but itâ€™s rcpp so maybe faster than lm for big data.\n.lm.fit is a base R lm function that is 30%-40% faster than lm."
  },
  {
    "objectID": "qmd/big-data.html#sec-bgdat-hghperf",
    "href": "qmd/big-data.html#sec-bgdat-hghperf",
    "title": "Big Data",
    "section": "High Performance",
    "text": "High Performance\n\n{rpolars}: arrow product; uses SIMD which is a low-level vectorization that can be used to speed up simple operations like addition, subtraction, division, and multiplication\n\nResources\n\nCookbook Polars for R\n\nAlso see collapse &gt;&gt; vs arrow/polars\n\n{collapse}: Fast grouped & weighted statistical computations, time series and panel data transformations, list-processing, data manipulation functions, summary statistics and various utilities such as support for variable labels. Class-agnostic framework designed to work with vectors, matrices, data frames, lists and related classes i.e.Â xts, data.table, tibble, pdata.frame, sf.\n\noptions(collapse_mask = \"all\")\nlibrary(collapse)\n\nCode chunk above can optimize any script. No other changes necessary. Quick demo.\nvs arrow/polars (benchmark)\n\nDepends on the data/groups ratio\n\nIf you have â€œmany groups and little data in each groupâ€ then use collapse\n\nIf your calculations involve â€œmore complex statistics algorithms like the median (involving selection) or mode or distinct value count (involving hashing)(cannot, to my knowledge, benefit from SIMD)â€ then use collapse.\n\n\n{r2c}: Fast grouped statistical computation; currently limited to a few functions, sometimes faster than {collapse}\n{data.table}: Enhanced data frame class with concise data manipulation framework offering powerful aggregation, extremely flexible split-apply-combine computing, reshaping, joins, rolling statistics, set operations on tables, fast csv read/write, and various utilities such as transposition of data.\n{matrixStats}: Efficient row-and column-wise (weighted) statistics on matrices and vectors, including computations on subsets of rows and columns.\n{kit}: Fast vectorized and nested switches, some parallel (row-wise) statistics, and some utilities such as efficient partial sorting and unique values.\n{fst}: A compressed data file format that is very fast to read and write. Full random access in both rows and columns allows reading subsets from a â€˜.fstâ€™ file."
  },
  {
    "objectID": "qmd/big-data.html#sec-bgdat-lgmem",
    "href": "qmd/big-data.html#sec-bgdat-lgmem",
    "title": "Big Data",
    "section": "Larger than Memory",
    "text": "Larger than Memory\n\nOnly work with a sample of the data\n\nRandom sample in CLI\n\nSee binder for code\n\nOnly read the first n lines\n\nset n_max arg in readr::read_*\n\n\ndatasette.io - App for exploring and publishing data. It helps people take data of any shape, analyze and explore it, and publish it as an interactive website and accompanying API.\n\nWell documented, many plugins\n\nRill - A tool for effortlessly transforming data sets into powerful, opinionated dashboards using SQL.\n\nDocs, Example Projects\nPowered by Sveltekit & DuckDB = conversation-fast, not wait-ten-seconds-for-result-set fast\nWorks with your local and remote datasets â€“ imports and exports Parquet and CSV (s3, gcs, https, local)\nNo more data analysis â€œside-questsâ€ â€“ helps you build intuition about your dataset through automatic profiling\nNo â€œrun queryâ€ button required â€“ responds to each keystroke by re-profiling the resulting dataset\nRadically simple interactive dashboards â€“ thoughtful, opinionated, interactive dashboard defaults to help you quickly derive insights from your data\nDashboards as code â€“ each step from data to dashboard has versioning, Git sharing, and easy project rehydration\n\nOnline duckdb shell for parquet files (gist, https://shell.duckdb.org/)\nselect max(wind)Â \nfrom 'https://raw.githubusercontent.com/RobinL/iris_parquet/main/gridwatch/gridwatch_2023-01-08.parquet';\n-- Takes 6 seconds on the first query, 200ms on subsequent similar queries\n\nselect *Â \nfrom 'https://raw.githubusercontent.com/RobinL/iris_parquet/main/NSPL/NSPL.parquet'Â \nwhere pcd = 'SW1A1AA';\n-- Takes 13 seconds on the first query, 100ms on subsequent similar queries\nCSV Editors\n\nFor editing or reformatting cells\nPopular spreadsheet programs like googlesheets (100MB) and excel (25MB online) have file size limits and theyâ€™re slow to upload to. The following programs are free(-ish) local alternatives only limited by your RAM.\nSuggest for files over a few hundred MBs that you open as Read-Only\n\nOpening the files as â€œEditableâ€ will probably balloon the memory cost to at least 5 times the file size. (e.g.Â 350MB csv \\(\\rightarrow\\) 2GB RAM)\n\nModern CSV - Nice modern interface, read-only mode that can open large csvs (100s of MBs) without making much of a dent in your RAM, fully featured (moreso if you pay a small-ish one time fee)\n\nDocs, Feature free/upgrade list\nStill has some functionality in read-only mode (e.g.Â search, sort)\n\nOpenRefine - Has read-only, Several add-ons, Completely open source.\n\nDocs, List of Extensions\nNo functionality when read-only (must create a project to do anything) â€” just reading\nStarts with a 1024 MB RAM usage limit which is proably fine for editing around a 100MB csv. Need to set the limit higher in a config file in order to edit larger files.\nOnce you create a project, I think it has some editing features that youâ€™d have to pay for with Modern CV.\nOpens other file formats besides csv (e.g.Â xlsx, xml, json, etc)\n\n\ncsvkit - suite of command-line tools for converting to and working with CSV\n\nInstallation docs\n\nOne of the articles your terminal has to be a bash terminal but I dunno\n\nIf so, they recommend cmder or enabling the Linux subsystem with WSL2.\n\n\nNotes from\n\nArticle with additional examples and options\n\nFeatures\n\nPrint CSV files out nicely formatted\nCut out specific columns\nGet statistical information about columns\n\nConvert excel files to CSV files:\nin2csv excel_file.xlsx &gt; new_file.csv\n# +remove .xlsx file\nin2csv excel_file.xlsx &gt; new_file.csv && rm excel_file\nSearch within columns with regular expressions:\ncsvgrep -c county -m \"HOLT\" new_file.csv\n# subset of columns (might be faster) with pretty formatting\ncsvcut -c county,total_cost new_file.csv | csvgrep -c county -m \"HOLT\" | csvlook\n\nSearches for â€œHOLTâ€ in the â€œcountyâ€ column\n\nQuery with SQL\n\nsyntax csvsql --query \"ENTER YOUR SQL QUERY HERE\" FILE_NAME.csv\nExample\n\n\nView top lines: head new_file.csv\nView columns names: csvcut -n new_file.csv\nSelect specific columns: csvcut -c county,total_cost,ship_date new_file.csv\n\nWith pretty output: csvcut -c county,total_cost,ship_date new_file.csv | csvlook\nCan also use column indexes instead of names\n\nJoin 2 files: csvjoin -c cf data1.csv data2.csv &gt; joined.csv\n\nâ€œcfâ€ is the common column between the 2 files\n\nEDA-type stats:\ncsvstat new_file.csv\n# subset of columns\ncsvcut -c total_cost,ship_date new_file.csv | csvstat\n\nJSONata - a lightweight, open-source query and transformation language for JSON data, inspired by the â€˜location pathâ€™ semantics of XPath 3.1.\n\nMisc\n\nNotes from: Hrbrmstrâ€™s article\nJSONata also doesnâ€™t throw errors for non-existing data in the input document. If during the navigation of the location path, a field is not found, then the expression returns nothing.\n\nThis can be beneficial in certain scenarios where the structure of the input JSON can vary and doesnâ€™t always contain the same fields.\n\nTreats single values and arrays containing a single value as equivalent\nBoth JSONata andÂ jqÂ can work in the browser (JSONata embedding code, demo), butÂ jqÂ has a slight speed edge thanks to WASM. However, said edge comes at the cost of a slow-first-start\n\nFeatures\n\nDeclarative syntax that is pretty easy to read and write, which allows us to focus on the desired output rather than the procedural steps required to achieve it\nBuilt-in operators and functions for manipulating and combining data, making it easier to perform complex transformations without writing custom code in a traditional programming language like python or javascript\nUser-defined functions that let us extend JSONataâ€™s capabilities and tailor it to our specific needs\nFlexible output structure that lets us format query results into pretty much any output type\n\n\njq + jsonlite - json files\njsoncrack.com - online editor/tool to visualize nested json (or regular json)\njj - cli tool for nested json. Full support for ndjson as well as setting/updating/deleting values. Plus it lets you perform similar pretty/ugly printing that jq does.\nsqlite3 - CLI utility allows the user to manually enter and execute SQL statements against an SQLite database or against a ZIP archive.\n\nalso directly against csv files (post)\n\ntextql - Execute SQL against structured text like CSV or TSV\n\nRequire Go language installed\nOnly for Macs or running a docker image\n\ncolumnq-cli - sql query json, csv, parquet, arrow, and more\nfread + CLI tools\n\nFor large csvs and fixing large csv with jacked-up formating see article, RBlogger version\n\n{arrow}\n\nconvert file into parquet files\n\npass the file path to open_dataset, use group_by to partition the Dataset into manageable chunks\nuse write_datasetto write each chunk to a separate Parquet fileâ€”all without needing to read the full CSV file into R\n\ndplyr support\n\nmultiplyr\n\nOption for data &gt; 10M rows and you only have access to one machine\nSpreads data over local cores\n\n{sparklyr}\n\nspin up a spark cluster\ndplyr support\nSet-up a cloud bucket and load data into it. Then, read into a local spark cluster. Process data.\n\n{h2o}\n\nh2o.import_file(path=path) holds data in the h2o cluster and not in memory\n\n{disk.frame}\n\nsupports many dplyr verbs\nsupportsÂ  future package to take advantage of multi-core CPUs but single machine focused\nstate-of-the-art data storage techniques such as fast data compression, and random access to rows and columns provided by the fst package to provide superior data manipulation speeds\n\nMatrix ops\n\nsee bkmks: mathematics &gt;&gt; packages\n\n{ff}\n\nsee bkmks: data &gt;&gt; loading/saving/memory\nThink it converts files to a ff file type, then you load them and use ffapply to perform row and column operations with base R functions and expressions\nmay not handle character and factor types but may work with {bit} pkg to solve this"
  },
  {
    "objectID": "qmd/big-data.html#sec-bgdat-viz",
    "href": "qmd/big-data.html#sec-bgdat-viz",
    "title": "Big Data",
    "section": "Viz",
    "text": "Viz\n\nScatterÂ plots\n\n{scattermore}, {ggpointdensity}\n{ggrastr}\n\nRasterize only specific layers of a ggplot2 plot (for instance, large scatter plots with many points) while keeping all labels and text in vector format. This allows users to keep plots within a reasonable size limit without losing the vector properties of scale-sensitive information.\ngithub; tweet\n\n\nH2O\n\nh2o.aggregator Reduces data size to a representive sample, then you can visualize a clustering-based method for reducing a numerical/categorical dataset into a dataset with fewer rows A count column is added to show how many rows is represented by the exemplar row (I think)\n\nAggregator maintains outliers as outliers but lumps together dense clusters into exemplars with an attached count column showing the member points.\nFor cat vars:\n\nAccumulate the category frequencies.\nFor the top 1,000 or fewer categories (by frequency), generate dummy variables (called one-hot encoding by ML people, called dummy coding by statisticians).\nCalculate the first eigenvector of the covariance matrix of these dummy variables.\nReplace the row values on the categorical column with the value from the eigenvector corresponding to the dummy values.\n\ndocs; article\n\n\n{dbplot}\n\nplots data that are in databases\n\nAlso able to plot data within a spark cluster\n\ndocs\n\nObservableHQ\n\n{{{deepscatter}}}\n\nThread (using Arrow, duckdb)"
  },
  {
    "objectID": "qmd/business-plots.html",
    "href": "qmd/business-plots.html",
    "title": "5Â  Business Plots",
    "section": "",
    "text": "TOC\n\nMarketing Plots\n\nCumulative Gains\nCumulative Lift\nResponse Plot\nCumulative Response\n\nFinancial Plots\n\nProfit\nCosts and Revenue\nROI\n\n\nMarketing Plots\n\n{modelplotr} PKG has some nice implementations. Notes for marketing and financial graphs taken from articles and vignettes introducing that package.\ntl;dr - Most useful/popular are the Cumulative Gains and Cumulative Response graphs.Â \nNote: the example objective is to select the customers of a bank that are most likely to respond to an offer to purchase a â€œterm depositâ€. The outcome is binary: â€œterm depositâ€ or â€œnoâ€\nInformation from models used in these plots\n\nPredicted probability for the target class\n(x-axis) Equally sized groups (ntiles - mash-up of â€œnâ€ and â€œpercentileâ€) based on this predicted probability (e.g.Â splitting observations into deciles. Top 10% in predicted probability for target class would be in the first decile.)\nActual number of observed target class observations in these groups\n\nThe test dataset is used for the plots to get a realistic idea of what a marketing campaign in the wild would produce.\n\nResponse Plot has some GOF capability so I could maybe see using the validation set with that plot to compare models with.\n\nCumulative GainsÂ (aka gains plot)\n\nAnswers the question: â€œWhen we apply the model and select the best X ntiles, what % of the actual target class observations can we expect to target?â€\n\ny-axis = % of positive events (1s in binary classification) out of the entire dataset\n\nHow to apply:\n\nChoose a probability threshold (i.e.Â the corresponding ntile on the x-axis). The graph shows the percentage of observations on the y-axis that are within that threshold\nChoose the percentage of customers that you can afford to target with your campaign. The corresponding ntile on the x-axis shows the ntile and therefore the associated probability of positive result.\n\nâ€œWhen we select 20% with the highest probability according to gradient boosted trees, this selection holds 87% of all term deposit cases in test data.â€\n\nsays using the top 20% will include 87% of all the 1s (in binary classification) in the entire dataset.\n\n\nIf the gains is 87%, then there are potentially 13% of the total 1s that wonâ€™t be included in the campaign if we only target the top 20% percent.\n\n\nwizard model (perfect model) line - line takes steepest route to 100% on y-axis as possible, depending on the percentage of your outcome variable is the target level.\n\nFor the graph above, it looks like around 12% of the outcome variable values are the positive event case since the line reaches the 100% on the y-axis a little past the 1st decile. So the perfect model predicts all those values as being the positive class.\n\n\nCumulative LiftÂ (aka index or lift plot)\n\nEspecially useful for companies with little to no experience with data models\nAnswers the question: â€œWhen we apply the model and select the best X ntiles, how many times better is that than using no model at all?â€\nâ€œno model at allâ€ (i.e.Â coin flip) is a random model (also seen in the gains plot) is represented by a horizontal line at y = 1 or 100% depending on how the y-axis is specified. It is the ratio of theÂ % of actual target category observations in each ntile toÂ the overall % of actual target category observations after randomization of the rows of the data set.\nThe amount of lift canâ€™t be generalized to all models and all data sets. So there arenâ€™t guidelines as to what is a â€œgoodâ€ lift score and what isnâ€™t.Â If 50% of your data belongs to the target (positive) class of interest, a perfect model would â€˜onlyâ€™ do twice as good (lift: 2) as a random selection. If 10% of the data belong to the positive class, then lift = 10 or 1000% is the best possible lift score.\nHow to apply:\n\nChoose a ntile (x-axis) and the corresponding y value can be used to explain to stakeholders how many times or what percent better this model is at selecting the top prospects than random selection.\n\nâ€œA term deposit campaign targeted at a selection of 20% of all customers based on our gradient boosted trees model can be expected to have a 4 times higher response (434%) compared to a random sample of customers.â€\n\n\nÂ  \n\nResponse Plot\n\nplots the percentage of *target class* observations per ntile\n\nnote: the cumulative gains y-axis is total observations where this plotâ€™s y-axis is just positive class (1s in a binary classification model)\n\nAnswers the question: â€œWhen we apply the model and select ntile X, what is the expected % of target class observations in that ntile?â€ but also gives information about the model fit.\nHow to apply:\n\nThis plot is more important in what it tells about the model fit than what it says about how many observations are in a particular ntile\n\nA good fitting model will have a sharp sloping line with the highest response % in the lower ntiles. This says that the model is giving high probability scores to the vast majority of the positive class observations\nFor model comparison: the earlier the line crosses the horizontal (random model) line should indicate a steeper slope and therefore a better fit.\n\nâ€œWhen we select decile 1 (10th percentile) according to model gradient boosted trees in dataset test data the % of term deposit cases in the selection is 51%.â€\nThe horizontal line represents a random model (i.e.Â the % of target class cases in the total set)\n\nFrom the ntile where the line intersects the horizontal dashed-line and onwards, the % of target class cases is lower than a random selection of cases would hold.\n\n\n\n\nCumulative Response\n\nAnswers the question: â€œWhen we apply the model and select up until ntile X, what is the expected % of target class observations in the selection?\n\nOften used to decide - together with business colleagues - up until what decile to select for a marketing campaign\n\nHow to apply:\n\nâ€œWhen we select ntiles 1 until 30 according to model gradient boosted trees in dataset test data, the % of term deposit cases in the selection is 36%.â€\n\nIn other words, targeting these customers should produce a response rate (percent of customers purchasing a term deposit) of 35% on average as compared to randomly selecting the same number of customers which is 12% (term deposits/total obs for the test set).\nThe y-axis is the percentage of 1s (in binary classification) in that subset (ntiles from 1 to 30). Different from cumulative gains where the y-axis is the percentage of 1s in the entire dataset.\n\nIs that response big enough to have a successfull campaign, given costs and other expectations? Will the absolute number of sold term deposits meet the targets? Or do we lose too much of all potential term deposit buyers by only selecting the top 30%? To answer that question, we can go back to the cumulative gains plot.\nThe dashed horizontal is the same as in the Response Plot\n\n\n\n\nFinancial Plots\n\n{modelplotr} PKG has some nice implementations. Notes for marketing and financial graphs taken from articles and vignettes introducing that package.\nNote: the example objective is to select the customers of a bank that are most likely to respond to an offer to purchase a â€œterm depositâ€. The outcome is binary: â€œterm depositâ€ or â€œnoâ€\n\nfixed costs = $75,000 (a tv commercial and some glossy print material)\nvariable costs per unit = $50 (customers are given an incentive to buy)\nprofit per unit = $250\n\nInformation from models used in these plots\n\n(also in marketing plots) Predicted probability for the target class\n(also in marketing plots) (x-axis) Equally sized groups (ntiles - mash-up of â€œnâ€ and â€œpercentileâ€) based on this predicted probability (e.g.Â splitting observations into deciles. Top 10% in predicted probability for target class would be in the first decile.)\n(also in marketing plots) Actual number of observed target class observations in these groups\nfixed costs (e.g.Â sales force expenses, advertising campaigns, sales promotion, and distribution costs)\nvariable costs per unit (e.g.Â sales commission, bonuses, and performance allowances)\nprofit per saleÂ \n\nThe test dataset is used for the plots to get a realistic idea of what a marketing campaign in the wild would cost and return. A validation set could be used on the Revenue and Costs Plot and models could be compared based risk of nonprofitability.\nProfit Plot\n\nAnswers the question: â€œWhen we apply the model and select up until ntile X, what is the expected profit of the campaign?â€\nHow to apply:\n\nPretty self-explanatory. The most profitable ntile is the one directly under the apex of the curve.\nThe most profitable ntile is highlighted by default, but this can be specified if so desired\nannotation means?\n\n\n\nCosts and Revenues Plot\n\nAnswers the question: â€œWhen we apply the model and select up until decile X, what are the expected revenues and investments of the campaign?â€\nThe costs are the cumulative costs of selecting up until a given decile and consist of both fixed costs and variable costs.\nThe revenues take into account the expected response % - as plotted in the cumulative response plot - as well as the expected revenue per response.\nSolid curve is the revenue and the dashed diagonal line is the total costs\nHow to apply:\n\nThe campaign is profitable in the plot area where revenues exceed costs.\nGives an idea of the range of spending that can be considered while the campaign remains profitable. Ranges could be associated with risk. The smaller the range, the greater the risk given the uncertainty of the models. Various campaign ranges could be compared based on this risk.\nSee profits plot for optimal ntile.\n\n\n\nROI Plot\n\nAnswers the question: â€œWhen we apply the model and select up until decile X, what is the expected % return on investment of the campaign?â€\nThe ntile at which the campaign profit is maximized is not necessarily the same as the ntile where the campaign ROI is maximized\n\nIt can be the case that a bigger selection (higher decile) results in a higher profit, however this selection needs a larger investment (cost), impacting the ROI negatively.\nSo maximum ROI can be considered the most effficient use of resources, but it takes money to make (the most) money.\n\nBasic formula for ROI = Net Profit / Total Investment * 100\nHow to apply:\n\nThe ntile directly underneath the apex of the curve is where the ROI is maximized."
  },
  {
    "objectID": "qmd/causal-inference.html#sec-causinf-misc",
    "href": "qmd/causal-inference.html#sec-causinf-misc",
    "title": "Causal Inference",
    "section": "Misc",
    "text": "Misc\n\nNotes from\n\nhttps://fabiandablander.com/r/Causal-Inference.html\n\nStatistical models measure associations (e.g.Â linear, non-linear) which is mutual information among the variables\n\ne.g.Â wind and leaves moving in a tree (doesnâ€™t answer whether the leaves moving creates the wind or the wind creates leaving moving)\n\nCausal inference predicts the conseqences after an intervention (i.e.Â action)\n\nYou must know the direction of causation in order to predict the conseqences of an intervention (unlike measuring associations)\nAnswers the question, â€œWhat happens if I do this?â€\n\nCausal inference is able to reconstruct unobserved counterfactual outcomes.\n\nAnswers the question, â€œWhat happens if I had done something else?â€\n\nCausal assumptions are necessary in order to make causal inferences\n\nmultiple regression does not distinguish causes from confounds\np-values are not causal statements\n\nDesigned to control type I error rate\n\nAIC, etc are purely predictive\n\nCausal Experiment Assumptions\n\nsee tlverse workshop notes and ebook for listing of assumptions and definitions,Â  https://tlverse.org/acic2019-workshop/intro.html#identifiability\n\nThe tlverse Project seeks to use ML models to calculate causal effects. Uses Super Learner ensembling and Targeted Maximum Likelihood Estimation (TMLE) which they call Targeted Learning.\n\nIgnorability - By randomly assigning treatment, researchers can ensure that the potential outcomes are independent of treatment assignment, so that the average difference in outcomes between the two groups can only be attributable to treatment\n\nEngineering outcome variables using potential adjustment variables does not automatically adjust for those variables in your model\n\nNotes from There Are No Magic Outcome Variables\nExample\n\n\nP is population density\nX is the variable of interest\nGDP and P have been used to create GDP/P\nP influences X and provides a backdoor path to GDP/P, so P must be adjusted for\nEven if P doesnâ€™t influence X, the point is that constructing GDP/P using P doensâ€™t automatically adjust for P\n\n\nRandomized experiments remove all paths from the treatment variable, X\n\n\nAdjusting for Z, B, and C can add precision to measurement of the treatment effect since they are causal to Y, but they arenâ€™t necessary to get an unbiased estimate of the treatment effect.\n\nTable 2 fallacy (Notes from McElreath video, 2022 SR Lecture 6)\n\n\nThe 2nd table presented in a paper is usually a summary of all the effects of a regression. The fallacy is that the coefficient of each variable is treated as causal.\nExample: The effect of HIV on Stroke\n\nThe model is lm(Stroke ~ HIV + Smoke + Age)\n\nOnly the coefficient of the HIV variable should be treated as causal and none of the other adjustment variables (Smoke, Age)\n\nThe effects for Smoke and Age are only partial.\nThere are likely unobserved confounding variables, U, on the effect of Smoking on Stroke (e.g.Â other lifestyle variables).\n\nSmoke is confounded so itâ€™s causal estimate is biased\nAge is also confounded since Smoke is now a collider and has been conditioned upon. This opens the non-causal path, Age-Smoke-U-Stroke.\n\nAge-Smoke is frontdoor, but the backdoor path, Smoke-U, also becomes a backdoor path for Age once Smoke is conditioned upon. (aka sub-backdoor path)\nSo any open path that contains a backdoor path must also be closed\n\n\n\nSolutions\n\nDonâ€™t include effect estimates of adjustment variables\nExplicitly interpret each effect estimate according to the causal model\n\nSee 2022 SR at the end of Lecture 6 where McElreath breaks down the interpretation of each adjustment variable estimated effect.\n\n\n\nPartial Identification (Handling Unobserved Confounds)\n\nSometimes the confounding paths of a DAG model can be not be resolved.\n\nFor confounders that influence the treatment and outcome, see:\n\nStructural Causal Models &gt;&gt; Bayesian examples\nIf thereâ€™s a mediator, see Other Articles &gt;&gt; Frontdoor Adjustment\n\nMeasure proxies for the unobserved confound if itâ€™s not practical/ethical to measure\n\ni.e.Â If the confound is ability, then test scores, letters of recommendation, etc. could be proxies.\n\nExample: 2022 SR Lecture 10 video, code\n\n\nA: Admitted to Grad School, G: Gender, D: Dept, u: Ability, T1,2,3: 3 Test Scores\n\nAbility is latent variable/unobserved confounder\nTest Scores are proxies for Ability\n\nBoth models are fit simultaneously\nCouldnâ€™t find a way to use {brms} to code this and Kurz didnâ€™t included it in his brms SR book.\n\n\n\nA biased estimate is better than no estimate. It can provide an upper bound\nFind a natural experiment or design one\nSensitivity Analysis\n\nAfter the analyis, you should be able to make the statement, â€œIn order for the confound to be responsible for the entire causal effect, it was have to be .â€\n\n\nPackages\n\n{tipr} - tools for tipping point sensitivity analyses\n\nSteps for using sensitivity analysis\n\nPerform a sensitivity analysis to determine plausibly how much of the causal effect is due to confounding paths\n\nAssume the confound exists, model itâ€™s consequences for different strengths/kinds of influence\nExample: 2022 SR Lecture 10 video, code \n\nA: Admitted to Grad School, G: Gender, D: Dept, u: Unobserved Confounder\nBoth models are fit simultaneously\nValues for Î² and Î³ are specified and u is estimated as a parameter\nI think Gender (G) is an interaction in both models which I didnâ€™t think was possible given there are no arrows of influence from gender to u.\n\nSince gender is a moderator it wouldnâ€™t necessarily have to be an influence arrow, it would only need to be an arrow from G to the effect of u on D (see Moderator Analysis), so maybe this is kosher\nCould also be that Iâ€™m misunderstanding McElreathâ€™s code he uses to specify his models with {Rethinking}.\n\nCouldnâ€™t find a way to use {brms} to code this and Kurz didnâ€™t included it in his brms SR book.\n\n\nUse previous studies that have effect strengths of those potential confounding variables\nCompare the strengths from the previous studies to the strength determined from the sensitivity analysis. The difference is a good guess for the strength of the causal effect of your treatment variable."
  },
  {
    "objectID": "qmd/causal-inference.html#sec-causinf-causdes",
    "href": "qmd/causal-inference.html#sec-causinf-causdes",
    "title": "Causal Inference",
    "section": "Causal Design",
    "text": "Causal Design\n\nNotes from McElreath video\nMisc\n\nWhen trying to determine the relationship (e.g.Â linear, nonlinear) between variables and remove inconsequential variables, the Double Debiased ML procedure might be useful.\n\nDouble Debiased Machine Learning - basic concepts, links to papers, videos\nEconML (Microsoft) and causalml (Uber) has included the method in their libraries\n\n\nWhen trying to infer causal relationships, we should not blindly enter all variables into a regression in order to â€œcontrolâ€ for them, but think carefully about what the underlying causal DAG could look like. Otherwise, we might induce spurious associations (e.g.Â confounding such as collider bias).\nOverview\n\nMake a causal model (i.e.Â DAG)\n\nNeed background information in order to make the causal assumptions represented in the DAG\nDAGs only show whether or not a variable influences another, not how the influence occurs (e.g.Â DAGs canâ€™t show interactions between variables or whether the association is non-linear)\n\nUse it to design data collection and statistical procedures\n\nSteps:\n\nDetermine two variables of interest (exposure, outcome) that you want to determine if a causal relationship exists and what effect the exposure has.\nUse domain knowledge or prior scholarship to determine the relevant variable and the likely associations between all variables in data\nCreate the DAG\n\nIdentify the direct causal path between exposure and outcome\nIdentify other explanatory variables and label their directions of influence with each other, the exposure, and the outcome variable\nConsider which variables (especially the exposure and the outcome) have unobserved variables influencing them.\n\nAnalyze the DAG\n\nIdentify colliders and use d-separation to determine conditional independencies\nIdentify additional paths (backdoor paths, sub-backdoor paths) between exposure and outcome\nUse the backdoor criterion to determine the set of variables that need to be adjusted for in order to block all backdoor paths with only the direct causal path remaining open.\nAdd additional adjustment variables that are causal to the outcome variable (but donâ€™t confound the treatment effect) in order to add precision to the estimate of the treatment effect\n\nCreate simulated data that fits the DAG (i.e.Â a generative model)\nPerform statistical analysis (i.e.Â SCMs) on the simulated dataÂ  to make sure you can measure the causal effect.\nDesign experiment and collect the data\nRun the statistical analysis on the collected data and calculate the average causal effect (ACE) under the assumptions that your DAG and model specifications are correct.\nBased on your results, revise the DAG and SCM as necessary and repeat as necessary\n\nBad Adjustment Variables (Code and more details included in 2022 SR, Lecture 6)\n\nFor all examples, Z is the adjustment variable thatâ€™s being considered; X is the treatment and Y is the outcome\n\nIn each scenario, including Z produces a biased estimate of X, so the correct model is Y ~ X.\n\nM-bias\n\n\nZ doesnâ€™t have a direct causal influence on the either X or Y, but when itâ€™s conditioned upon it becomes a collider due to unobserved confounds that have a direct causal influence on X and Y.\nCommon issue in Political Science and network analysis\nExample\n\nY: Health of Person 2\nX: Health of Person 1\nZ: Friendship status\n\nPre-treatment variable (tend to be open to collider paths) since they could be friends before the exposure\n\nU: Hobbies of Person 1\nV: Hobbies of Person 2\n\n\nPost-Treatment Bias\n\n\nZ is a mediator and conditioning upon Z blocks the path from X to Y, but opens the backdoor path through the unobserved confound, U.\nCommon in medical studiesÂ Â \nExample\n\nY: Lifespan\nX: Win Lottery\nZ: Happiness\nU: Contextual Confounds\n\n\nSelection Bias\n\n\nSame as collider bias\n\nThis version adds an unobserved confounder\n\nExample\n\nY: Income\nX: Education\nZ: Values\nU: Family\n\n\nCase-Control Bias\n\n\nZ is a descendent. Since Z has information about Y, conditioning on it will narrow the variation of Y and distort the measured effect of X.\nAlso see Association &gt;&gt; Single Path DAGs &gt;&gt; Descendent\nExample\n\nY: Occupation\nX: Education\nZ: Income\n\n\nPrecision Parasite\n\n\n2 versions: with and without U\n\nWithout U, conditioning on Z removes variation from X and lessens (but doesnâ€™t bias) the precision of the estimated effect of X on Y (i.e.Â inflated std.error)\nWith U, the effect of X is biased and that bias is amplified when Z is included.\n\n\nPeer Bias\n\n\nClassic DAG of the Berkley Admission-Race-Department study\nAlso see Structural Causal Models &gt;&gt; Example (Bayesian Peer Bias)\nX is race, E is department, Q is unobserved (e.g.Â student quality), Y is Admission\nDepartment cannot be conditioned upon because itâ€™s a collider with Q and would bias the estimate of X through a sub-backdoor path, X-E-Q-Y\nOnly the total effect of X on Y can be estimated (Y ~ X) since E cannot be conditioned upon but thatâ€™s not interesting and maybe not precise"
  },
  {
    "objectID": "qmd/causal-inference.html#sec-causinf-terms",
    "href": "qmd/causal-inference.html#sec-causinf-terms",
    "title": "Causal Inference",
    "section": "Terms",
    "text": "Terms\n\nAverage Causal Efffect (ACE) - average population effect thatâ€™s calculated from an intervention (see Counterfactual definition for info on Individual Causal Effects)\n\nIf X is binary, then  Â is the average causal effectÂ (see Simpsonâ€™s Paradox example)\n\nCalculated from a contingency table\n\nAlso, \n\nThis looks like the interpretation of the slope in a regression model.\n\n\nBackdoor Criterion - A valid causal estimate is available if it is possible to condition on variables such that all backdoor paths are closed\n\nGiven two nodes, X and Y, an adjustment set,Â L, fulfills the backdoor criterion ifÂ \n\nno member inÂ LÂ is a descendant of X and\nmembers inÂ LÂ block all backdoor paths (â€œshutting the backdoorâ€) between X and Y.\n\nAdjusting forÂ LÂ thus yields the causal effect of Xâ†’Y.\nAfter executing an intervention, the conditional distribution in the observational DAG (seeing) will correspond to the interventional distribution (doing) when blocking the spurious path. (see Simpsonâ€™s Paradox example)\n\nBackdoor Path - A non-causal path that enters a causal variable in a DAG rather than exits it.\n\ne.g.Â the path that connects a collider to a causal variable points from the collider to the causal variable\nSub-backdoor Path - this path begins with a frontdoor path but through conditioning on a variable, it opens a connecting backdoor path which biases the treatment effect\n\nsee Misc &gt;&gt; Table 1 Fallacy and Causal Design &gt;&gt; Bad Adjustment Variables &gt;&gt; Peer Bias\n\n\nThe causal effect is the distribution of Y when we change x, averaged over the distributions of the adjustment variables (Z)\nCausal Hierarchy (lowest to highest)\n\nAssociation\n\nassociated action: Seeing - observational; observing the value of Y when X = x\n\n , observational distribution; What values Y would likely take on if X happened to equal x.\n\n\nIntervention\n\nassociated action (do-Calculus): Doing -Â  experimental; observing the value of Y after setting X = x\n\n , interventional distribution; What values Y would likely take on if X would be set to x.\nUsing the do operator allows us to make inferences about the population but not individuals.\ndo(X) means to cut all of the backdoor paths into X, as if we did a manipulative experiment. The do-operator changes the graph, closing the backdoors.\nThe do-operator defines a causal relationship, because Pr(Y|do(X)) tells us the expected result of manipulating X on Y, given a causal graph.\n\nWe might say that some variable X is a cause of Y when Pr(Y|do(X)) &gt; Pr(Y|do(not-X)).\n\n(makes more sense to me with a binary outcome, Pr(Y = 1|do(X), but maybe Y as a continuous variable can be defined a subset. â€¦I dunno)\n\n\nThe ordinary conditional probability comparison, Pr(Y|X) &gt; Pr(Y|not-X), is not the same. It does not close the backdoor.\nNote that what the do-operator gives you is not just the direct causal effect. It is the total causal effect through all forward paths.\n\nTo get a direct causal effect, you might have to close more backdoors.\n\nThe do-operator can also be used to derive causal inference strategies even when some backdoors cannot be closed.\n\n\nCounterfactual\n\nassociated action: ImaginingÂ - what would be the outcome if the alternative wouldâ€™ve happened.\nIndividual Causal Effects can be calculated but it requires stronger assumptions and deeper understanding of the causal mechanisms\n\nNeed to research this part further.\nIf the underlying SCM is linear then the ICE = ACE.\n\n\n\nA collider along a path blocks that path. However, conditioning on a collider (or any of its descendants) unblocks that path\n\nWhen a collider is conditioned upon, the change in the association between the two nodes it separates is called collider bias.\n\ne.g.Â if Z is a collider between X and Y, conditioning upon Z will induce an association between X and Y.\n\n\nA conditioning set, \\(L\\), is the set of nodes we condition on (it can be empty).\nConfounding is the situation where a (possibly unobserved) common cause obscures the causal relationship between two or more variables.\n\nThere is more than one causal path between two nodes.\nA causal effect of X on Y is confounded ifÂ  \nCollider bias is a type of confounding. When a collider is controlled for, a second (or more) path opens, and the effect is confounded\n\nX and Y are d-separated byÂ [LÂ if conditioning on all members inÂ [LÂ blocks all paths between the nodes, X and Y.\n\nTool for checking the conditional independencies which are visualized in DAGs.\n\nA descendant is a node connected to a parent node by that parent nodeâ€™s outgoing arrow.\nFrontdoor Adjustment - In a causal chain with three nodes Xâ†’Zâ†’Y, we can estimate the effect of X on Y indirectly by combining two distinct quantities: (Useful for when unobserved confounders prevent direct causal estimation)\n\nThe estimate of the effect of X on Z, P(Z|do(X))\nThe estimate of the effect of Z on Y, P(Y|do(Z), X)\n\nFrontdoor Path - a path that exits a causal variable in a DAG rather than enters it.\n\ne.g.Â the path that connects a causal variable, X, to an outcome variable, Y, has an arrow that points from X to Y.\n\nMarkov Equivalence - A set of DAGs, each with the same conditional independencies\nMediation Analysis -Â seeks to identify and explain the mechanism or process that underlies an observed relationship between an independent variable and a dependent variable via the inclusion of a third hypothetical variable, known as a mediator variable (z-variable in the DAGs of â€œpipesâ€ below)\n\nIncluding a mediator and the independent variable in a regression will result in the independent variable not being signficant and the mediator being significant.\n\nModeration Analysis - Like mediation analysis, it allows you to test for the influence of a third variable, Z, on the relationship between variables X and Y, but rather than testing a causal link between these other variables, moderation tests for when or under what conditions an effect occurs.\nA node is a parent of another node if it has an outgoing arrow to that node\nA path from X to Y is a sequence of nodes and edges such that the start and end nodes are X and Y, respectively.\nResidual Confounding occurs when a confounding variable is measured imperfectly or with some error and the adjustment using this imperfect measure does not completely remove the effect of the confounding variable.\n\nExample: Women who smoke during pregnancy have a decreased risk of having a Down syndrome birth.\n\nThis is puzzling, as smoking is not often thought of as a good thing to do. Should we ask women to start smoking during pregnancy?\nIt turns out that there is a relationship between age and smoking during pregnancy, with younger women being more likely to indulge in this bad habit. Younger women are also less likely to give birth to a child with Down syndrome. When you adjust the model relating smoking and Down syndrome for the important covariate of age, then the effect of smoking disappears. But when you make the adjustment using a binary variable (age&lt;35 years, age &gt;=35 years), the protective effect of smoking appears to remain.\n\n\nStructural Causal Models (SCMs) - relate causal and probabilistic statements; each equation is a causal statement\n\n\n\nâ€œ:=â€ is the assignment operator\nX is a direct cause of Y which it influences through the function f( )\n\nwhere f is a statistical model\n\nThe noise variables, ÏµX and ÏµY, are assumed to be independent.\n\nThere are Stochastic and Deterministic SCMs. Deterministic SCMs presented in article."
  },
  {
    "objectID": "qmd/causal-inference.html#sec-causinf-assoc",
    "href": "qmd/causal-inference.html#sec-causinf-assoc",
    "title": "Causal Inference",
    "section": "Association",
    "text": "Association\n\n\n\nFar left: lm(Y ~ X); X and Y show a linear correlation when Z is NOT conditioned upon\nLeft: lm(Y ~ X + Z); X and Y show NO linear correlation when Z is conditioned upon\nRight: lm(Y ~ X); X and Y show NO linear correlation when Z is NOT conditioned upon\nFar Right:Â  lm(Y ~ X + Z); X and Y show a linear correlation when Z is conditioned upon"
  },
  {
    "objectID": "qmd/causal-inference.html#sec-causinf-singpath",
    "href": "qmd/causal-inference.html#sec-causinf-singpath",
    "title": "Causal Inference",
    "section": "Single path DAGs",
    "text": "Single path DAGs\n\n\nFor each of these DAGs, Z would be the only member of the conditioning set.\nThe first 3 DAGs represent the scatter plots above\n\nZ only blocks the path between X and Y when itâ€™s conditioned upon.\n\nX and Y are associated (e.g.Â linear correlation, mutual information, etc.) when Z is ignored\nConditioning on Z results in X and Y no longer being associated (i.e.Â conditional independence)\n\nThe first and second DAGs are elemental confounds or relations called â€œPipes.â€\n\nThe left one\n\nIn general, DO NOT add these variables to your model\n\nThese paths are causal so they shouldnâ€™t be blocked\nIf your goal isnâ€™t causal inference, then adding these variables might provide predictive information\ne.g.Â If there was a causal arrow from X to Y, the far left DAG would NOT have a backdoor path and therefore Z would notÂ  be conditioned upon to block the path, X-Z-Y\n\nThe path from X to Z is a frontdoor path since the arrow exits X.\n\n\nSometimes you DO condition on these variables\n\nDuring mediation analysis, you condition on these variables as part of the process to determine how much of the effect goes through Z.\nThe mediation path can have an important interpretation depending on your research question\n\ne.g.Â indirect descrimination\n\nSee Statistical Rethinking &gt;&gt; Chapter 11 &gt;&gt; Conclusion of Berkeley Admissions example\n\nalso Lecture 9 2022 video\n\n\n\n\n\nThe right one is a backdoor path and should be conditioned on.\nEverything you can learn about Y from X (or vice versa) happens through Z, therefore learning about X separately provides no additional information\nZ is traditionally labelled a mediator\n\nThe third DAG is an elemental confoundÂ  or relation called a â€œFork.â€\n\nIn general, add these variables to your model\nThese are backdoor paths and are NOT causal\nX and Y have a common cause in Z and some of the mutual information about Z they each contain, overlaps, and creates an association (when Z isnâ€™t conditioned upon).\n\n\nThe fourth DAG is an elemental confound or relation called a â€œCollider.â€\n\n\nIn general, do NOT add these variables to your model\nZ blocks the path between X and Y unless conditioned upon.\nAn association between X and Y is inducedÂ  by conditioning on Z, lm(Y ~ X + Z)\n\nX and Y are independent causes of Z. Z contains information about both X and Y, but X doesnâ€™t contain any information about Y and vice versa.\nA small X and a sufficiently large Y (and vice versa) can produce a Z = 1. So X and Y have compensatory relationship in causing Z.\n\ni.e.Â For a given value of Z, learning something about X tells us what Y might have been.\n\n\n\nThe last elemental confound or relation is called a â€œDescendent.â€\n\n\nConditioning on a descendent variable, D, is like conditioning on the variable, Z itself, but weaker. A descendent is a variable influenced by another variable.\nControlling for D will also control, to a lesser extent, for Z. The reason is that D has some information about Z. This will (partially) open the path from X to Y, because Z is a collider. The same holds for non-colliders. If you condition on a descendent of Z in the pipe, itâ€™ll still be like (weakly) closing the pipe."
  },
  {
    "objectID": "qmd/causal-inference.html#sec-causinf-dualpath",
    "href": "qmd/causal-inference.html#sec-causinf-dualpath",
    "title": "Causal Inference",
    "section": "Dual path DAGs",
    "text": "Dual path DAGs\n\n\nCausal paths do not flow against arrows but associations can.\nTwo examples of DAGs representing confounding\n\nThese are the 2 middle DAGs above with an additional path from X to Y\nIf Z is NOT conditioned on (i.e.Â top path is not blocked), then the causal effect of X on Y would be confounded.\n\n\n\n\nThe paths from X to Y:\n\nThe path through Z matches the first DAG.\n\nTherefore X and Y are conditionally independent given Z.\n\nThe path through W matches the fourth DAG\n\nTherefore X and Y are conditionally dependent given W.\n\n\nThe path through W (collider) is blocked unless W is conditioned upon\nThe path through Z is open unless Z is conditioned upon\nIf Z and W are conditioned upon, then the path between X and Y is open through W and an association is present."
  },
  {
    "objectID": "qmd/causal-inference.html#sec-causinf-interv",
    "href": "qmd/causal-inference.html#sec-causinf-interv",
    "title": "Causal Inference",
    "section": "Intervention",
    "text": "Intervention\n\n\nSince actual interventions are usually unfeasible, we want to be able to determine causality with observational data. This requires two assumptions:\n\nThe intervention occurs locally. Which means that only the variable we target is the one that receives the intervention.\nThe mechanism by which variables interact do not change through interventions; that is, the mechanism by which a cause brings about its effects does not change whether this occurs naturally or by intervention\n\nThe Doing row of DAGs (aka manipulated DAGs) represents setting X = x\n\nFor DAGs 1 and 4, Y is still affected\n\nMoving from seeing to doing didnâ€™t change anything\n\n\nFor DAGs 2 and 3, Y is now UNaffected\n\nUsing the assumptions and some mathematical manipulation (See article for details):\n\n\n\nThus, the interventional distribution we care about is equal to the (observational) conditional distribution of Y given X when we adjust for Z\n\n\n\n\nThe rule: After an intervention, incoming arrows are cut from the node where the intervention took place."
  },
  {
    "objectID": "qmd/causal-inference.html#sec-causinf-confound",
    "href": "qmd/causal-inference.html#sec-causinf-confound",
    "title": "Causal Inference",
    "section": "Confounding",
    "text": "Confounding\n\n\nThe backdoor criterion tells us which variable we need to adjust for in order to for our model to yield a causal relationship between two variables (i.e.Â graphically, nodes)\n\nBlocks all spurious, that is, non-causal paths between X and Y.\nLeaves all directed paths from X to Y unblocked\nCreates no spurious paths\n\nExample\n\nCausal effect of Z on U is confounded by XÂ because in addition to the legitimate causal path Zâ†’Yâ†’Wâ†’U, there is also an unblocked path Zâ†Xâ†’Wâ†’U which confounds the causal effect\n\nSince Xâ€™s arrow enters the causal variable of interest, Z, itâ€™s arrow is a backdoor path and needs to be blocked/closed\nThere are some descendant nodes that make the confounding a little difficult to parse out, but this graph is essentially\n\n\nwhich is the same as the second example DAG for confounding in the Association section\n\n\nThe backdoor criterion would have us condition on X, which blocks the spurious path and renders the causal effect of Z on U unconfounded.\n\nThe reduced, confounding DAG above is the same as the third DAG (without the path from Z to U) in the Association section. Conditioning on Z in that example blocked the path between X and Y, so it makes sense that conditioning on X in the reduced DAG would block the Z to X to U path. And therefore, theÂ Zâ†Xâ†’Wâ†’U would also be blocked in the complete DAG.\n\nNote that conditioning on W would also block this spurious path; however, it would also block the causalÂ path, Zâ†’Yâ†’Wâ†’U.\n\n\nIf we breakdown the complete DAG into the modular components involving W, we can see these are the same as the first example DAG in the Association section.\nW is also collider for X and Y, but I donâ€™t think that has any bearing when discussing the causal effect of Z on U."
  },
  {
    "objectID": "qmd/causal-inference.html#sec-causinf-appsimp",
    "href": "qmd/causal-inference.html#sec-causinf-appsimp",
    "title": "Causal Inference",
    "section": "Application: Simpsonâ€™s Paradox Example",
    "text": "Application: Simpsonâ€™s Paradox Example\n\nSex as the adjustment variableÂ Â  Â Â Â  Â Â Â  \n\nPatients CHOOSE whether or not to take a drug to cure some disease.\nMen choosing to take the drug recover at a higher percentage that those that didnâ€™t\nWomen choosing to take the drug recover at a higher percentage that those that didnâ€™t\nBut overall, those that chose to take the drug recovered at a lower percentage than those that didnâ€™t.\nSo should a doctor prescribe the drug or not?\nSuppose we know that women are more likely to take the drug, that being a woman has an effect on recovery more generally, and that the drug has an effect on recovery.Â \nCreate DAGs\n\n\nS=1 as being female,\nD=1 as having chosen to take the drug\nR=1 as having recovered\nThe right DAG indicates either forcing everyone to either take the drug or not take the drug\nNotice that  Â therefore our calculated effect will be confounded.\n\nBackdoor criterion says the manipulated DAG (right) will correspond to the observational DAG (left) if we condition on Sex.\n\n\nUse intervention formula from Intervention section\n\n\nAverage Causal Effect = 0.832 - 0.782 = 0.050. So the drug has a positive effect on average.\n\n\nBlood Pressure as the adjustment variable \n\nBlood Pressure instead of sex is used as the adjustment. Blood Pressure is a post-treatment variable.\nRelatively same observations as before. High or Low Blood Pressure with the drug produces better results than those that chose not to take the drug. Yet overall, those that chose the drug recovered at a lower percentage.\n\nSince Blood Pressure (B) is post-treatment, it has no effect on whether the patient takes the drug or not (D).\nTaking or not taking the drug (D) has an indirect effect on recovery (R) through Blood Pressure (B) along with a direct effect.  Â so our calculated effect will be unconfounded.\n\nSo with BP as the adjustment variable, the drug now has a small, negative effect (harmful), 0.78 - 0.83 = -0.05\n\nThe unconfounded, average causal effect for the population is negative, therefore the doctor should NOT prescribe the drug."
  },
  {
    "objectID": "qmd/causal-inference.html#sec-causinf-scms",
    "href": "qmd/causal-inference.html#sec-causinf-scms",
    "title": "Causal Inference",
    "section": "Structural Causal Models (SCMs)",
    "text": "Structural Causal Models (SCMs)\n\nYou add additional assumptions to your DAG to derive a causal estimator\nâ€œFull Luxuryâ€ Bayesian approach\n\nâ€œFull Luxuryâ€ is just a term coined by McElreath; itâ€™s just a bayesian model but bayesian models can fully model a DAG where standard regression approachs can fail (see examples)\nNo other approach will find something that the bayesian approach doesnâ€™t\n\nMain disadvantage is that it can be computationally intensive (same with all baysian models)\n\nProvides ways to add â€œcausesâ€ for missingness and measurement error\n\nExample (2 Moms)\n\nNotes from McElreath video\nHypothesis: a motherâ€™s family size is causal to her daughterâ€™s family size\n\nTruth: no relationship\n\nVariables:\n\nM - Motherâ€™s family size (i.e.Â number of children the birth)\nD - Daughterâ€™s family size\nB1 - Motherâ€™s birth order; binary, first born or not\nB2 - Daughtersâ€™ birth order; binary, first born or not\nU - unobserved confoundsÂ  (shown as curved dotted line)\n\n\n\nUnobserved confounds (economic status, education, cultural background, etc.) are causal to both Mother and Daughter (curved dotted line) which makes regression, D ~ M, impossible\n\nSee Baysian Two Moms example below for results of a typical regression\nStill possible to calculate the effect of M on D with SCMs\n\n\nAssumptions: Relationships are linear (i.e.Â linear system)\nCausal Effects\n\n\nWe want m which is the causal effect of M on D\nAssumes causal effect of birth order is the same on mother and daughter\nAside: There is no arrow/coefficient from M to B2 because itâ€™s not germane to the calculation of m\n\nCalculate linear effect (i.e.Â regression coefficient) without a regression model using a linear system of equations\n\nNote: a regression coefficent, Î² = cov(X,Y) / var(X)\nWe canâ€™t calculate the covariance of M and D directly because it depends on unobserved confounders but we can calculate the covariance between B1 and D and use that to get m.\nThe covariance for each path is the product of the path coefficients and the variance of the originating causal variable.\nPath B1 â†’ M: cov(B1, M) = b*var(B1)\nPath B1 â†’ D: cov(B1, D) = b*m*var(B1)\n2 equations and 2 unknowns, m and b\nSolve for b in the first equation, substitute b into the second equation, and solve for m\n\nm = cov(B1, D) / cov(B1, M)\n\nStill need an uncertainty of this value (e.g.Â bootstrap)\n\n\nExample (Bayesian 2 Moms)\n\nSee previous example for link, hypothesis, and definition of the variables\n\nFunctions (right side)\n\nEach variableâ€™s functionâ€™s inputs are variables that are causal influences (i.e.Â have arrows pointing at the particular variable\n\ne.g.Â M has two arrows pointing at it in the DAG: B1 and u\n\n\nCode\n\nThe assumption is that this is a lineary system, so M and D have Normal distributions for their functions with means as linear regression equations\nB1 and B2 are binary so they get bernoulli distributions\nU gets a standard normal prior\n\nAside: evidently this is a typical prior for latent variables in psychology\n\np, intercepts, sd, k get typical priors for bayesian regressions\n\nResults\n\n\nTruth: no effect\n1st 3 lm models shows how the unobserved confound biases the estimate when using a typical regression model to estimate the causal effect\n\nIncluding B2 adds precision to the biased estimate since it is causal to the outcome D while adding B1 increases the bias\n\nBayesian model isnâ€™t fooled because U is specified as an input to the functions for M and D\n\nInterpretation: There is no reliable estimate of an effect. The most likely effect is a moderately positive one but it could also be negative.\nAdding more simulated data to this example will move the point estimate towards zero\n\n\n\nExample (Bayesian Peer Bias)\n\nAlso see Causal Design &gt;&gt; Bad Adjustment Variables &gt;&gt; Peer Bias\nHypothesis: racial discrimination in acceptance of applicatioon to Berkeley grad schools\n\nTruth: moderate negative effect, -0.8\n\nVariables:\n\nX is race, E is department, Q is an unobserved confound (latent variable: student quality), Y is binary; Admission/No Admission\nR1 and R2 are proxy variables for Q (e.g.Â test scores, lab work, extracurriculars, etc.)\n\nAssumptions: System is linear\nDAG and Code\n\n\nXX is the race variable with X as the coefficient in the code\n\nThis code uses his {rethinking} package so some of this syntax is unfamiliar\n\nR1 and R2 are shown in the DAG to be influenced by student quality, Q\nEvery prior is normal except for Qâ€™s coefficient\n\nResults\n\n\nTruth: -0.8\n1st 3 glm models shows how the unobserved confound, Q, biases the estimate when using a typical logistic regression model to estimate the causal effect\nBayesian model isnâ€™t fooled because Q is specified as an input to the function for Y\n\nInterpretation: There is a reliably negative effect (no 0 in the CI). The most likely effect is a moderately negative one.\nNot quite equal to the truth but reliably negative and the point estimate is closer than the glms\n\n\n\nExample\n\nAssumptions: Relationships between variables are linear and error terms are independent\nEquations\n\n,Â  \n\n\nDAG 1 (left) shows the association DAG which represents the SCM\nmanipulated DAG 1 (middle) shows intervention where z is set to a constant\n\nincoming causal arrows get cutoff the intervening variable\n\nmanipulated DAG 1 (right) shows intervention where x is set to a constant\n\nSimulation of the SCM (n = 1000) (code in article)\n\n\nZ is more predictive of Y than X\n\nSimulate interventions (code in article)\n\n\nLeft - histogram of SCM for Y without an intervention\nMiddle - Intervention on Z\n\nconfirms the DAG which shows no effect on Y and Z is not causal\n\nRight - intervention on X\n\nconfirms the DAG which shows an intervention on X produces an effect on Y and X is causal\n\nAverage Causal Effect (ACE) can be determined by subtracting the expected values of interventions whereÂ  X = x +1 andÂ  X = x"
  },
  {
    "objectID": "qmd/causal-inference.html#sec-causinf-ctrfact",
    "href": "qmd/causal-inference.html#sec-causinf-ctrfact",
    "title": "Causal Inference",
    "section": "Counterfactuals",
    "text": "Counterfactuals\n\nExample(code in article): Test whether Grandmaâ€™s home remedy can speed recovery time for the common cold\n\nSCM\n\n\nT is 1/0, i.e.Â whether patient receives Grandmaâ€™s treatment, with p = 0.5; \nR is recovery time\nÎ¼ is the intercept\nÎ² is the average causal effect, since\n\n\nwhereÂ \n\n\nFrom fitting the model, we find Î¼ = 7, Î² = -2, Î¤ = 0, Îµ1 = 0.78\n\nTherefore, the Individual Causal Effect for patient 1\n\n\nJust plug and chug where we substitute T = 1 into the SCM and we already have the T = 0 part from the model\n\n\nIn this case, the SCM is linear, so the ICE = ACE."
  },
  {
    "objectID": "qmd/causal-inference.html#sec-causinf-medanal",
    "href": "qmd/causal-inference.html#sec-causinf-medanal",
    "title": "Causal Inference",
    "section": "Mediation Analysis",
    "text": "Mediation Analysis\n\n\nFigure\n\ncâ€™ is the direct effect of X on the outcome after the indirect path has been removed (i.e.Â conditioned upon, outcome ~ X + mediator)\nc is the to total effect (outcome ~ X)\nc - câ€™ equals the indirect effect\nSee definitions below\n\nAllows you to test for the influence of a third variable, the mediator, on the relationship between (i.e.Â effect of) the treatment variable, X, and the Outcome variable, Y.\nMisc\n\nNotes from: Mediation Models\n\nOverview of packages (Aug 2020)\n\n{brms} very flexible in terms of models. Youâ€™ll just have to calculate the effects by hand unless some outside package (e.g.Â sjstats) takes a brms model and does it for you.\n\nSee below for formulas. {mediation} papers should have other formulas for other types of models (e.g.Â poisson, binomial)\n\n{mediation} handles a lot for you. Method isnâ€™t bayesian but is very similar to it in a frequentist-bootstrappy-simulation way.\n\nPackage has been substantially updated since that article was written.\n\n\nAlso see\n\nOther Articles &gt;&gt; Frontdoor Adjustment\nStatistical Rethinking &gt;&gt; Chapter 11 &gt;&gt; Conclusion of Berkeley Admissions example\n\nalso Lecture 9 2022 video\n\nebook (w/brms) Introduction to Mediation, Moderation, and Conditional Process Analysis\n\nIncluding a mediator and the independent variable in a regression will result in the independent variable not being signficant and the mediator being significant.\n\nExample: Causal effect of education on income\n\nSay occupation is your mediator. Education has a big impact on your occupation, which in turn has a big impact on your income. You donâ€™t want to control for a mediator if you are interested in the full effect of X on Y! Because a huge part of how X impacts on Y is precisely through the mediation of C, in our case choice of and access to occupation, given a certain level of education. If you â€˜controlâ€™ for occupation you will be greatly underestimating the importance of education.\n\n\nWhen would you want to only measure the Direct Effect?\n\nExample: Determining the amount of remuneration for discrimination\n\nFrom Simulating confounders, colliders and mediators\nVariables\n\nOutcome: Pay Gap\nTreatment: Gender\n\nIn this case, this variable is actually â€œgender discrimination in the current workplace in making a pay decisionâ€ (for which we use actual, observed Gender as a proxy)\n\nMediators: Occupation and Experience\n\nWhen determining whether a type of descrimination exists, you donâ€™t want to condtion on the mediators, because the effect of gender will be underestimated. So, youâ€™d want the total effect. But here, discrimation is already determined and Gender is now a proxy variable. Under Genderâ€™s new definition, Occupation and Experience might influence the amount of â€œgender discrimiation,â€ so they canâ€™t be definitively labelled mediators any more.\nSo if you want to estimate that final â€œequal pay for equal workâ€ step of the chain then yes it is legitimate to control for occupation and experience.\n\n\nShould always compare a mediation model to a model without mediation\n\nAn unnecessary mediation model will almost certainly be weaker and probably more confusing than the model you would otherwise have.\n\nAverage Causal Mediation Effect (ACME) (aka Indirect Effect)- the expected difference in the potential outcome when the mediator took the value that it would have under the treatment condition as opposed to the control condition, while the treatment status itself is held constant.\n\nIf this isnâ€™t significant, there isnâ€™t a mediation effect\nIt is possible that the ACME takes different values depending on the baseline treatment status. Shown by analyzing the interaction between the treatment variable and the mediator\nÎ´(t) = E[Y (t, M(t1)) âˆ’ Y (t, M(t0))]\n\nwhere\n\nt, t1, t0 are particular values of the treatment T such that t1 â‰  t0,\nM(t) is the potential mediator\nY (t, m) is the potential outcome variable\n\n\n\nAverage Direct Effect (ADE) - the expected difference in the potential outcome when the treatment is changed but the mediator is held constant at the value that it would have if the treatment equals t.\n\nÎ¶(t) = E[Y (t1, M(t)) âˆ’ Y (t0, M(t))]\n\nThe Total Effect of the treatment on the outcome is ACME + ADE.\n\nConditions where you likely do NOT need mediation analysis :\n\nIf you cannot think of your model in temporal or physical terms, such that X necessarily leads to the mediator, which then necessarily leads to the outcome.\nIf you could see the arrows going either direction.\nIf when describing your model, everyone thinks youâ€™re talking about an interaction (a.k.a. moderation).\nIf there is NO strong correlation between key variables (variables of interest) and mediator, and if there is NO strong correlation between mediator and the outcome.\n\nSobel test - tests whether the suspected mediatorâ€™s influence on the independent variable is significant.\n\nPerforming the test in R via bda::mediation.test - article\n\nMethods\n\nBaron & Kennyâ€™s (1986) 4-step indirect effect method has low power\nProduct-of-Paths (or difference in coefficients)\n\nc - câ€™ = a*b (see figure at start of this section) where c - câ€™ is the indirect effect (aka ACME)\n\nif either a or b are nearly zero, then the indirect effect can only be nearly zero\nFormula only appropriate for the analysis of causal mediation effects when both the mediator and outcome models are linear regressions where treatment (IV) and moderator enter the models additively (e.g.Â without interaction)\n\nEffect formulas for models with an interaction between treatment and moderator (Paper)\n\nmediator: M = Î±2 + Î²2Ti + Î¾T2Xi + Îµi2(T~i`)\noutcome: Y = Î±~3 + Î²3Ti + Î³Mi + ÎºTiMi + Î¾T3Xi + Îµi3(Ti, Mi)\nACME = Î²2(Î³ + Îºt) where t = 0,1\nADE = Î²3 + Îº{Î±2 + Î²2t + Î¾T2Î•(Xi)}\nATE = Î²2Î³ + Î²3 +Îº{Î±2 + Î²2 + Î¾T2Î•(Xi)}\n\nAlternatively, fit Y = Î±1 + Î²1Ti + Î¾T1Xi + Î·TTiXi + Îµi1\n\nThen ATE = Î²1 + Î·TE(Xi)\n\n\nNotes\n\nVariables\n\nT is treatment, M is mediator, X is a set of adjustment variables\n\nThe exponentiated T in Î¾T is to let you know it can be a set of coefficients for a set of adjustment variables (I guess)\n\n\nCouldnâ€™t figure out why curly braces are being used\nACME with have two estimates (t=0, t=1)\nATE (average total effect)\nÎ•(Xi) is the sample average of each adjustment variable and itâ€™s multiplied by its associated Î¾2 coefficient\nSee paper for other types of models\n\n\n{lavaan}, {brms}\n\nTingley, Yamamoto, Hirose, Keele, & Imai, 2014\n\nQuasi-bayesian approach (paper ,esp Appendix D, for details)\n\nFits the mediation and outcome models (see 1st example)\nTakes the coefficients and vcov matrices from both models\n\nUses the coefs (means) and vcovs (variances) as inputs to a mvnorm function to simulate distributions for the coefficients.\nI do not understand what these are used forâ€¦ would have to look at the code.\n\nSamples predictions of each model K times for treatment = 1, then for treatment = 0\nCalcs difference between predictions for each set of samples, then averages to get the ACME\n\nAssumes Sequential Ignorability\n\nRequires treatment randomization or an equivalent assignment mechanism\nmediator is also ignorable given the observed treatment and pre-treatment confounders. This additional assumption is quite strong because it excludes the existence of (measured or unmeasured) post-treatment confounders as well as that of unmeasured pretreatment confounders. This assumption, therefore, rules out the possibility of multiple mediators that are causally related to each other (see Section 6 for the method that is designed to deal with such a scenario).\nCanâ€™t be tested but a sensitivity analysis can be conducted using mediation::medsens (see vignette)\n\n{mediation} (vignette)\n\nMultiple types of models for both mediator and outcome\n\nincluding multilevel model functions from {lme4} supported\n\nMethods for:\n\nâ€˜moderatedâ€™ mediation\n\nthe magnitude of the ACME depends on (or is moderated by) a pre-treatment covariate. Such a pre-treatment covariate is called a moderator. (see Moderator Analysis)\nACME can depend on treatment status (i.e.Â interaction between treatment and mediator), but this situation is talking about a separate variable moderating the effect of the treatment on the mediator.\n\nmultiple mediators (which violates sequential ingnorability but can be handled)\nvarious experimental designs (e.g.Â parallel, crossover)\ntreatment non-compliance\n\nUses MASS (so may have conflicts with dplyr)\nNo latent variable capabilities\n\n\nEtsy article calculates generalized average causal mediation effect (GACME) and generalized average direct effect (GADE) and uses a known mediator to measure the direct causal effect even when the DAG has multiple unknown mediators (paper, video, R code linked in article)\n\nExample: Tingley, 2014 Method\n\nEquations\n\n\n\nPredictions for â€œjob_seekâ€ in the mediator model (top) are used as predictor values in the outcome model (bottom).\n\nData: data(jobs, package = 'mediation')\n\ndepress2: outcome, numeric: Measure of depressive symptoms post-treatment. The outcome variable.\ntreat: treatment, binary: whether participant was randomly selected for the JOBS II training program.\n\n1 = assignment to participation.\n\njob_seek: mediator, ordinal: measures the level of job-search self-efficacy with values from 1 to 5.\necon_hard: adjustment, ordinal: Level of economic hardship pre-treatment with values from 1 to 5.\nsex: adjustment, binary: 1 = female\nage: adjustment, numeric: Age in years\n\n{mediation}\nmodel_mediator &lt;- lm(job_seek ~ treat + econ_hard + sex + age, data = jobs)\nmodel_outcomeÂ  &lt;- lm(depress2 ~ treat + econ_hard + sex + age + job_seek, data = jobs)\n\n# Estimation via quasi-Bayesian approximationÂ \nmediation_result &lt;- mediate(\nÂ  model_mediator,Â \nÂ  model_outcome,Â \nÂ  sims = 500,\nÂ  treat = \"treat\",\nÂ  mediator = \"job_seek\"\n)\n\nSummary - summary(mediation_result)\n\n\nerror bar plot also available via plot(mediation_result)\nSays ACME isnâ€™t significant, therefore no mediation effect detected.\nâ€œProp Mediatedâ€ is supposed to be the ratio of the indirect effect to the total.\n\nHowever this is not a proportion, and can even be negative, and so â€œit is mostly a meaningless number.â€\n\n\n\n\nExample: product-of-paths (or difference in coefficients)\n\n{lavaan}\nsem_model = '\nÂ  job_seek ~ a*treat + econ_hard + sex + age\nÂ  depress2 ~ c*treat + econ_hard + sex + age + b*job_seek\nÂ  # direct effect\nÂ  direct := c\nÂ  # indirect effect\nÂ  indirect := a*b\nÂ  # total effect\nÂ  total := c + (a*b)\n'\nmodel_sem = sem(sem_model, data=jobs, se='boot', bootstrap=500)\nsummary(model_sem, rsq=T)Â  # compare with ACME in mediation\nDefined Parameters:\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  EstimateÂ  Std.ErrÂ  z-valueÂ  P(&gt;|z|)\nÂ  Â  directÂ  Â  Â  Â  Â  -0.040Â  Â  0.045Â  -0.904Â  Â  0.366\nÂ  Â  indirectÂ  Â  Â  Â  -0.016Â  Â  0.012Â  -1.324Â  Â  0.185\nÂ  Â  totalÂ  Â  Â  Â  Â  Â  -0.056Â  Â  0.046Â  -1.224Â  Â  0.221\n\nAlso outputs the typical summary regression estimates, std.errors, pvals, R2 etc.\nBootstraps std.errors\nSame results for â€œindirectâ€ here as with {mediation} ACME estimate\nR2s are poor for both regression models which could be why no mediation effect is detected.\n\n{brms}\nmodel_mediator &lt;- bf(job_seek ~ treat + econ_hard + sex + age)\nmodel_outcomeÂ  &lt;- bf(depress2 ~ treat + job_seek + econ_hard + sex + age)\nmed_result = brm(\nÂ  model_mediator + model_outcome + set_rescor(FALSE),Â \nÂ  data = jobs\n)\nsummary(med_result) # regression results\n# using brms we can calculate the indirect effect as follows\nhypothesis(med_result, 'jobseek_treat*depress2_job_seek = 0')\n\nExact same brms syntax (except priors are specified) as in Statistical Rethinking &gt;&gt; Chapter 5 &gt;&gt; Counterfactual Plots\nExample has a mediator DAG as well.\nhypothesis tests H0: a*b == 0\n\npval &lt; 0.05 says there is a mediation effect.\n\n\n{sjstats}\n\nsjstats::mediation(med_result) %&gt;% kable_df()\n\nmediator (b): the effect of â€œjob_seekâ€ on â€œdepress2â€\nindirect (c-câ€™): ACME\ndirect (câ€™): ADE\nproportion mediated: See {mediation} example"
  },
  {
    "objectID": "qmd/causal-inference.html#sec-causinf-modanal",
    "href": "qmd/causal-inference.html#sec-causinf-modanal",
    "title": "Causal Inference",
    "section": "Moderation Analysis",
    "text": "Moderation Analysis\n\n\nMisc\n\nAlso see Introduction to Mediation, Moderation, and Conditional Process Analysis\n\nLike mediation analysis, it allows you to test for the influence of a third variable, Z (moderator), on the relationship between variables X and Y, but rather than testing a causal link between these other variables, moderation tests for when or under what conditions an effect occurs.\n\nModerators are conceptually different from mediators (â€œwhenâ€ (moderator) vs â€œhow/whyâ€ (mediator)).\n\nThere can be moderated mediation effect though. (see Mediation Analysis &gt;&gt; Methods &gt;&gt; {mediation})\n\nModerators can stengthen, weaken, or reverse the nature of a relationship.\nSome variables may be a moderator or a mediator depending on your question.\n\nAssumption: assumes that there is little to no measurement error in the moderator variable and that the DV did not CAUSE the moderator.\n\nIf moderator error is likely to be high, researchers should collect multiple indicators of the construct and use SEM to estimate latent variables.\nThe safest ways to make sure your moderator is not caused by your DV are to experimentally manipulate the variable or collect the measurement of your moderator before you introduce your IV.\n\nModeration can be tested by interacting variables of interest (moderator x IV) and plotting the simple slopes of the interaction, if present.\n\nSee Regression, Interactions for simple slopes/effects analysis\nMean center both your moderator and your IV to reduce multicolinearity and make interpretation easier. (â€œcâ€ in variable names indicates variable was centered)\n\nExample: academic self-efficacy (moderator)(confidence in ownâ€™s ability to do well in school) moderates the relationship between task importance (independent variable (IV)) and the amount of test anxiety (outcome) a student feels (Nie, Lau, & Liau, 2011).\n\nStudents with high self-efficacy experience less anxiety on important tests (task importance) than students with low self-efficacy while all students feel relatively low anxiety for less important tests.\nSelf-efficacy (Z) is considered a moderator in this case because it interacts with task importance (X), creating a different effect on test anxiety (Y) at different levels of task importance.\n\nExample: What is the relationship between the number of hours of sleep (X, independent variable (IV)) a graduate student receives and the attention that they pay to this tutorial (Y, outcome) and is this relationship influenced by their consumption of coffee (Z, moderator)\nmod &lt;- lm(Y ~ Xc + Zc + Xc*Zc)\nsummary(mod)\n## Coefficients:\n##Â  Â  Â  Â  Â  Â  Estimate Std. Error t value Pr(&gt;|t|)Â  Â \n## (Intercept) 48.54443Â  Â  1.17286Â  41.390Â  &lt; 2e-16 ***\n## XcÂ  Â  Â  Â  Â  5.20812Â  Â  0.34870Â  14.936Â  &lt; 2e-16 ***\n## ZcÂ  Â  Â  Â  Â  1.10443Â  Â  0.15537Â  7.108 2.08e-10 ***\n## Xc:ZcÂ  Â  Â  Â  0.23384Â  Â  0.04134Â  5.656 1.59e-07 ***\n\nSince we have significant interactions in this model, there is no need to interpret the separate main effects of either our IV or our moderator\nPlot the simple slopes (1 SD above and 1 SD below the mean) of the moderating effect\n\n\nFor details on this plot and analysis, see Regression, Interactions &gt;&gt; OLS &gt;&gt; numeric:numeric &gt;&gt; Calculate simple slopes for the IV at 3 representative values for the moderator variable\nInterpretation\n\nThose who drank less coffee (moderator, black line) paid more attention (outcome) with the more sleep (IV) that they got last night but paid less attention overall than average (the red line).\nThose who drank more coffee (moderator, green line) paid more attention (outcome) when they slept more (IV) as well and paid more attention than average.\nThe difference in the slopes for those who drank more or less coffee (moderator) shows that coffee consumption moderates the relationship between hours of sleep and attention paid"
  },
  {
    "objectID": "qmd/causal-inference.html#sec-causinf-sr",
    "href": "qmd/causal-inference.html#sec-causinf-sr",
    "title": "Causal Inference",
    "section": "Statistical Rethinking",
    "text": "Statistical Rethinking\n\nMisc\n\nArrows indicate directions of influence\nArrows in DAGs â€œcreateâ€ correlations\n\ni.e.Â if arrow, then correlation\nThe direction it points determines whether its association is causal or not.\n\nUnlike a statistical model, a DAG, if it is correct, will tell you the consequences of intervening to change a variable.\n** The data alone can never tell us when a DAG is right. But the data can tell us when a DAG is wrong. **\nMany dynamical systems cannot be usefully represented by DAGs, because they have complex behavior that is sensitive to initial conditions. But these models can still be analyzed and causal interventions designed from them.\nA DAG path means any series of variables you could walk through to get from one variable to another, ignoring the directions of the arrows.\nThe variable, U, in DAGs represents one or more unobserved variables\n\nUsually has circle around the U or is just represented by a dashed line\n\nâ€œConditioned upon,â€ â€œadjusted for,â€ or â€œcontrolled forâ€ is all the same thing\nâ€œaâ€ or â€œÎ±â€ is used in bayesian formulas to represent the intercept\nNotation\n\nX is not independent of Y, i.e \nconditional independence: Y is not associated with some variable X, after conditioning on some other variable Z, i.e.Â \n\nthey are statements of which variables should be associated with one another (or not) in the data.\nthey are statements of which variables become dis-associated when we condition on some other set of variables.\nThere is no other path of influence from X to Y except through Z\n\n\n(Total ) Causal Effect and Direct Causal Effect\n\n\nWeight (W) is the outcome, Height (H) and Sex (S) are explanatory\n(Total) Causal Effect is simply, W ~ S\nDirect Causal Effect shuts the backdoor paths, W ~ S + H\n\nSometimes we want the total causal effect and not the direct causal effect. (e.g.Â if H is a post-treatment variable, see SR, Ch.6)\n\n\n\n\n\nTestable Implications\n\nDiffering associations between plausible DAGs that are testable through statistical models\nAny DAG may imply that some variables are independent of others under certain conditions.\nNO conditional independencies â†’ NO testable implications\nwww.dagitty.net - Enter DAG and it will give you the Adjustment Set and Testable Implications\nExample\n\nQuestion: What is the causal relationship between Divorce Rate (D), Marriage Rate (M), and Median Age at Marriage (A)\nData:\n\n2 regressions are fit\n\nD ~ Î± + Î²M\n\nShows that M is positively correlated with D\n\nD ~ Î± + Î²A\n\nShows that A is negatively correlated with D\n\n\n\nPlausible DAGs (note: marriage cannot influence your ageâ€¦ technically)\n\n\n\nA directly influences D\nM directly influences D\nA directly influences M\nReasoning: First, Age can have a direct effect, perhaps because younger people change faster than older people and are therefore more likely to grow incompatible with a partner. Second, it can have an indirect effect by influencing the marriage rate. If people get married earlier, then the marriage rate may rise, because there are more young people. Consider for example if an evil dictator forced everyone to marry at age 65. Since a smaller fraction of the population lives to 65 than to 25, forcing delayed marriage will also reduce the marriage rate. If marriage rate itself has any direct effect on divorce, maybe by making marriage more or less normative, then some of that direct effect could be the indirect effect of age at marriage.\n\n\n\nSimilar to 1 except M does not directly influence D\nReasoning This DAG is plausible even though thereâ€™s a correlation between M and D (regression 1). It could be that M derives itâ€™s correlation with D through itâ€™s association with A.\n\nThe direction of influence doesnâ€™t prevent a correlation between M and D\n\n\n\nTestable implications\n\nDAG 1\n\nThe DAG shows all three are associated to each other, i.e.Â \nIt would be natural to think about measuring correlation and if a pair shows no correlation you could discard the DAG, but it is NOT a good test since there are many ways two variables can show correlation yet not be directly associated. (see reasoning under DAG 2 above and under DAG2 below)\nDAG1 has NO conditional independencies and therefore, NO testable implications\n\nDAG 2\n\nThis DAG also shows all three variables are associated with each other.\nD and M are associated with one another, because A influences them both. They share a cause, and this leads them to be correlated with one another through that cause. But suppose we condition on A. All of the information in M that is relevant to predicting D is in A. So once weâ€™ve conditioned on A, M tells us nothing more about D\nThe testable implication is that D is independent of M, conditional on A, i.e.Â \n\n(Conditioning on A does not make D independent of M, because M really influences D all by itself in this model.)\n\ni.e A and M are marginally dependent\n\n\n\nOnly difference between both DAGs is the conditional independence in DAG2.\n\nTest\n\nRun a multiple regression D ~ Î± + Î²MM + Î²AA\nIf the effect measured from regression 1 disappears in the multiple regression, then we can discard DAG 1. If the effect remains, then we discard DAG 2.\n\n\nDAGs that are consistent with the data associations (M & N are associated but the causal relationship isnâ€™t known)\n\nwhere U is an unknown variable. Unobserved variables are circled.\n\nAll three DAGs have no conditional independencies and therefore not testable implications\n\nA set of DAGs, each with the same conditional independencies known as a Markov Equivalence\n\nData cannot eliminate any of these DAGS. Domain knowledge must be used to reduce the number of Markov Equivalent DAGs.\n\n\n\n\nâ€œShutting the backdoorâ€ to potential confounding paths\n\nSection 6.4\nwww.dagitty.net - Enter DAG and it will give you the Adjustment Set and Testable Implications\nRecipe\n\nList all of the paths connecting X (the potential cause of interest) and Y (the outcome).\nClassify each path by whether it is open or closed. A path is open unless it contains a collider.\nClassify each path by whether it is a backdoor path. A backdoor path has an arrow entering X.\nIf there are any backdoor paths that are also open, decide which variable(s) to condition on to close it.\n\nIf you have a choice between two variables where conditioning on either will close a backdoor path and one of them is causal to the outcome variable, then condition on the variable that is causal to the outcome variable. It will add precision to the estimate of the treatment effect.\nAny frontdoor paths that lead to backdoor paths must also be closed (see Misc &gt;&gt; Table 2 fallacy)\n\n\nExamples:\n\n\n\nProblem: We want to measure the causal effect of X â€“&gt; Y\nPotential confounding paths: XUAC, XUBC\n\nXUAC doesnâ€™t have a collider so a variable needs conditioned on (aka adjusted for)\n\nU is unobserved, so either A or C. C directly influences Y, so itâ€™s more efficient and will â€œaid in precision.â€\n\nXUBC has a collider, B. So, no need to condition on any variable\n\nSolution: Y ~ a + X + C\n\nlibrary(dagitty)\ndag_6.1 &lt;- dagitty( \"dag {Â \nÂ  Â  U [unobserved]\nÂ  Â  X -&gt; Y\nÂ  Â  X &lt;- U &lt;- A -&gt; C -&gt; Y\nÂ  Â  U -&gt; B &lt;- C\n}\")\nadjustmentSets( dag_6.1 , exposure=\"X\" , outcome=\"Y\" )\n#&gt; { C }\n#&gt; { A }\n\n\nProblem: We want to measure the causal effect of the number of Waffle Houses, W, on Divorce, D.\nPotential confounding paths: WSM, WSA, WSMA (Also WSAM but McElreath on says there are 3. Maybe a combo of same letters is equivalent?)\n\nWSM doesnâ€™t have a collider and therefore either S or M needs conditioned on\nWSA doesnâ€™t have a collider and therefor either S or A needs conditioned on\nWSMA has a collider, M. So that path is blocked\nM is a choice for WSM but itâ€™s a collider so itâ€™s out. S is in both WSM and WSA, so conditioning on it kills two birds.\n\nSolution: D ~ a + W + S\n\nlibrary(dagitty)\ndag_6.2 &lt;- dagitty( \"dag {\nÂ  Â  A -&gt; D\nÂ  Â  A -&gt; M -&gt; D\nÂ  Â  A &lt;- S -&gt; M\nÂ  Â  S -&gt; W -&gt; D\n}\")\nadjustmentSets( dag_6.2 , exposure=\"W\" , outcome=\"D\" )\n#&gt; { A, M }\n#&gt; { S }\n\nEvidently conditioning on A and M is also a solution\n\nConditioning on M does close WSM but would then open WSMA. So, by then conditioning on A which is on a fork (or pipe depending on the path) it closes WSMA.\n\nIn his brms ebook, Kurz fits these regressions and a couple others for comparison. There wasnâ€™t a consensus point estimate for W in the regressions that adjust for S and A + M.\n\nMcElreath mentions, â€œThis DAG is obviously not satisfactoryâ€“it assumes there are no unobserved confounds, which is very unlikely for this sort of data.â€\nThe inconsistent point estimates are probably do to an omitted variable(s) that is confounding the regression.\n\nConditional independencies:\nimpliedConditionalIndependencies( dag_6.2 )\n#&gt; A _||_ W | S\n#&gt; D _||_ S | A, M, W\n#&gt; M _||_ W | S"
  },
  {
    "objectID": "qmd/causal-inference.html#sec-causinf-othart",
    "href": "qmd/causal-inference.html#sec-causinf-othart",
    "title": "Causal Inference",
    "section": "Other Articles",
    "text": "Other Articles\n\nFrontdoor Adjustment\n\nFrom http://arelbundock.com/posts/frontdoor/\nUseful when an unobserved confounder creates a backdoor path that prevents direct causal estimation\nIn a causal chain with three nodes Xâ†’Zâ†’Y, we can estimate the effect of X on Y indirectly by combining two distinct quantities:\n\nThe estimate of the effect of X on Z, P(Z|do(X))\nThe estimate of the effect of Z on Y, P(Y|do(Z), X)\n\nAssumptions\n\nFull mediation: there is no direct path from X to Y, except through Z.\nUn-confoundedness 1: There is no open backdoor from X to Z.\nUn-confoundedness 2: All backdoors from Z to Y are blocked by X\n\nExample: 1\n\nOur goal is to estimate P(Y|do(X)). Unfortunately, this relationship between X and Y is confounded by the unobserved variable U, via this backdoor path: Xâ†Uâ†’Y. Therefore, we cannot estimate the causal quantity of interest directly.\n\n\ncause X, a mediator Z, an outcome Y, and an unobserved confounder U\n\nlibrary(data.table)\nset.seed(731460)Â \nN = 1e5\nU = rbinom(N, 1, prob = .2)\nX = rbinom(N, 1, prob = .1 + U * .6)\nZ = rbinom(N, 1, prob = .3 + X * .5)\nY = rbinom(N, 1, prob = .1 + U * .3 + Z * .5)\ndat = data.table(X, Z, Y)\n\n# truth\ncoef(lm(Y ~ X + U))[\"X\"]\n## 0.2549541\nEstimate the effect of X on Z, P(Z|do(X))\nstep1 = lm(Z ~ X, dat)\nEstimate the effect of Z on Y, P(Y|do(Z), X)\nstep2 = lm(Y ~ Z + X, dat)\nCombine both estimates by multiplication\ncoef(step1)[\"X\"] * coef(step2)[\"Z\"]\n## 0.2496002\n\nExample 2\n\nSame as first example but using {dosearch} package\nlibrary('dosearch')\n   data1 &lt;- \"P(X, Y, Z)\"\nquery1 &lt;- \"P(Y | do(X))\"\ngraph1 &lt;- \"U -&gt; X\nÂ  Â  Â  Â  Â  U -&gt; Y\nÂ  Â  Â  Â  Â  X -&gt; Z\nÂ  Â  Â  Â  Â  Z -&gt; Y \"\n   # compute\n   frontdoor &lt;- dosearch(data1, query1, graph1)\n   frontdoor\n\nOutput:\n\nEstimate the causal effect\ndat[, `P(X)`Â  Â  := fifelse(X == 1, mean(X), 1 - mean(X)) ][\nÂ  Â  , `P(Z|X)`Â  := mean(Z), by = XÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ][\nÂ  Â  , `P(Y|Z,X)` := mean(Y), by = .(Z, X)Â  Â  Â  Â  Â  Â  Â  Â  ][\nÂ  Â  , `P(Z|X)`Â  := mean(Z), by = XÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ][\nÂ  Â  , Y := NULLÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ]\ndat = unique(dat)\ndat[, `P(Y|do(Z))` := sum(`P(Y|Z,X)` * `P(X)`), by = Z]\n`P(Y|do(X=0))` = with(dat[X == 0],Â \nÂ  `P(Z|X)`Â  Â  Â  Â  Â  [Z == 1] *Â \nÂ  `P(Y|do(Z))`Â  Â  Â  [Z == 1] +\nÂ  (1 - `P(Z|X)`)Â  Â  [Z == 0] *Â \nÂ  `P(Y|do(Z))`Â  Â  Â  [Z == 0]\n)\n`P(Y|do(X=1))` = with(dat[X == 1], {\nÂ  `P(Z|X)`Â  Â  Â  Â  Â  [Z == 1] *Â \nÂ  `P(Y|do(Z))`Â  Â  Â  [Z == 1] +\nÂ  (1 - `P(Z|X)`)Â  Â  [Z == 0] *Â \nÂ  `P(Y|do(Z))`Â  Â  Â  [Z == 0]\n})\n`P(Y|do(X=1))` - `P(Y|do(X=0))`\n## 0.249766\nComparison\n\nTruth: 0.2549541\nlm: 0.2496002\ndosearch: 0.249766"
  },
  {
    "objectID": "qmd/classification.html#sec-class-misc",
    "href": "qmd/classification.html#sec-class-misc",
    "title": "Classification",
    "section": "Misc",
    "text": "Misc\n\nAlso see Regression, Logistic\nGuide for suitable baseline models: link\nIf you have mislabelled target classes, try AdaSampling to correct the labels (article)\nSample size requirements\n\nLogistic Regression: (Harrell, link)\n\nThese are conservative estimates. Sample size estimates assume an event probability of 0.50.\nFor just estimating the intercept and a margin of error for predicted probabilities of 0.1\n\nWith no covariates (i.e.Â population is homogeneous), n = 96\nWith 1 categorical covariate, n = 96 for each level of the covariate\n\ne.g.Â For gender, you need 96 males and 96 females\n\n\nFor just estimating the intercept and a margin of error for predicted probabilities of 0.05\n\nWith no covariates (i.e.Â population is homogeneous), n = 384\nIf true probabilities of event (and non-event) are known to be extreme, i.e.Â \\(p \\notin [0.2, 0.8]\\), n = 246\n\nFor estimating predicted probabilities with 1 continuous predictor\n\nFor a margin of error of 0.1, n = 150\nFor a margin of error of 0.07, n = 300\n\n\nRF: 200 events per candidate feature (Harrell, link)\n\nUndersampling non-events(0s) is the popular way to balance the target variable in data sets but other ma be worth exploring while building the model.\nSpline â€” donâ€™t bin continuous, baseline, adjustment variables, where â€œbaselineâ€ means the patients measurements before treatment. Lack of fit will then come only from omitted interaction effects. (Harrell)\n\ne.g.: if older males are much more likely to receive treatment B than treatment A than what would be expected from the effects of age and sex alone, adjustment for the additive propensity would not adequately balance for age and sex.\nAlso see\n\nFeature Engineering, General &gt;&gt; Continuous &gt;&gt; Binning\nFeature Engineering, General &gt;&gt; Continuous &gt;&gt; Splines\n\n\nThe best information to present to the patient is the estimated individualized risk of the bad outcome separately under all treatment alternatives. That is because patients tend to think in terms of absolute risk, and differences in risks donâ€™t tell the whole story (Harrell)\n\nA risk difference (RD, also called absolute risk reduction) often means different things to patients depending on whether the base risk is very small, in the middle, or very large.\n\nRecommended metrics to be reported for medical studies (Harrell). This is perhaps generalizable to any RCT with a binary outcome.\n\nThe distribution of Risk Difference (RD)\nCovariate-Adjusted OR\nAdjusted marginal RD (mean personalized predicted risk as if all patients were on treatment A minus mean predicted risk as if all patients were on treatment B) (emmeans?)\nMedian RD"
  },
  {
    "objectID": "qmd/classification.html#sec-class-diag",
    "href": "qmd/classification.html#sec-class-diag",
    "title": "Classification",
    "section": "Diagnostics",
    "text": "Diagnostics\n\nAlso see\n\nDiagnostics, Classification\nRegression, Logistic &gt;&gt; Diagnostics\n\nâ€œDuring the initial phase of model building, a good strategy for data sets with two classes is to focus on the AUC statistics from these curves instead of metrics based on hard class predictions. Once a reasonable model is found, the ROC or precision-recall curves can be carefully examined to find a reasonable cutoff for the data and then qualitative prediction metrics can be used.â€ â€“ 3.2.2Â Classification Metricsâ€ Kuhn and Kjell\nâ€œStableâ€ AUC requirements for 0/1 outcome:\n\nPaper: Modern modelling techniques are data hungry: a simulation study for predicting dichotomous endpoints | BMC Medical Research Methodology | Full Text\nLogistic Regression: 20 to 50 events per predictor variable\nRandom Forest and SVM: greater than 200 to 500 events per predictor variable"
  },
  {
    "objectID": "qmd/classification.html#sec-class-imbal",
    "href": "qmd/classification.html#sec-class-imbal",
    "title": "Classification",
    "section": "Class Imbalance",
    "text": "Class Imbalance\n\nMisc\n\nAlso see\n\nModel Building, tidymodel &gt;&gt; Recipe &gt;&gt; up/down-sampling\nSurveys, Analysis &gt;&gt; Modeling &gt;&gt; Tidymodels &gt;&gt; Importance weights\nPaper: Subsampling without calibration will likely be more harmful than without subsampling\n\nSubsampling can refer to up/oversampling or down/undersampling.\nIt is perfectly ok to train a model on 1M negatives and 10K positives (i.e.Â plenty of events), as long as you avoid using accuracy as a metric\n\n1K to 10K events might be enough for the ML algorithm to learn from\nFor a RF model: 200 events per candidate feature (Harrell, link)\n\nUnless recalibration is applied, applying subsampling to correct class imbalance will lead to overpredicting the minority class (discrimination) and worse calibration when using logistic regression or ridge regression (paper)\n\nPaper used random undersampling (RUS), random oversampling (ROS), and SMOTE (Synthetic Minority Oversampling Technique)\nEvent Fractions: 1%, 10%, 30%\nN: 2500, 5000; p: 3, 6, 12, 24\nâ€œWe anticipate that risk miscalibration will remain present regardless of type of model or imbalance correction technique, unless the models are recalibrated. However, class imbalance correction followed by recalibration is only worth the effort if imbalance correction leads to better discrimination of the resulting models.â€\n\nThey used what looked to be Platt Scaling for recalibration\n\nAlso see Model Building, tidymodels &gt;&gt;Â Tune &gt;&gt; Tune Model with Multiple Recipes for an example of how downsampling (w/o calibration) + glmnet affects class prediction and GOF metrics\n\n\nIssues\n\nUsing Accuracy as a metric\n\nIf the positivity rate is just 1%, then a naive classifier labeling everything as negative has 99% accuracy by definition\n\nIf youâ€™ve used subsampling, then the training data is not the same as the data used in production\nLow event rate\n\nIf you only have 10 to 100 positive samples, the model may easily memorize these samples, leading to an overfit model that generalized poorly\nMay result in large CIs for your effect estimate (see Gelman post)\n\n\nCheck Imbalance\ndata %&gt;%\nÂ  count(class) %&gt;%\nÂ  mutate(prop = n / sum(n)) %&gt;%\nÂ  pretty_print()\nDownsampling\n\nUse cases for downsampling the majority class\n\nwhen the training data doesnâ€™t fit into memory (and your ML training pipeline requires it to be in memory), or\nwhen model training takes unreasonably long (days to weeks), causing too long iteration cycles, and preventing you from iterating quickly.\n\nUsing a domain knowledge filter for downsampling\n\na simple heuristic rule that cuts down most of the majority class, while keeping nearly all of the minority class.\n\ne.g.Â if a rule can retain 99% of positives but only 1% of the negatives, this would make a great domain filter\n\nExamples\n\ncredit card fraud prediction: filter for new credit cards, i.e.Â those without a purchase history.\nspam detection: filter for Emails from addresses that havenâ€™t been seen before.\ne-commerce product classification: filter for products that contain a certain keyword, or combination of keywords.\nads conversion prediction: filter for a certain demographic segment of the user population.\n\n\n\nCV\n\nIt is extremely important that subsampling occurs inside of resampling. Otherwise, the resampling process can produce poor estimates of model performance.\n\ndata leakage: if you first upsample the data and then split the data into training (aka analysis set) and validation (aka assessment set) folds, your model can simply memorize the positives from the training data and achieve artificially strong performance on the validation data, causing you to think that the model is much better than it actually is.\nThe subsampling process should only be applied to the analysis (aka training) set. The assessment (aka validation) set should reflect the event rates seen â€œin the wild.â€\n\nDoes {recipe} handle this correctly?\n\n\nProcess\n\nSubsample the data only in the analysis set\nPerform CV algorithm selection and tuning using a suitable metric for class imbalance\nUse same metric to get score on the test set\n\n\nML Methods\n\nSynthetic Data Approaches\n\nTabDDPM: Modeling Tabular Data with Diffusion Models (Raschka Thread)\n\nCategorical and Binary Features: adds uniform noise via multinomial diffusion\nNumerical Features: adds Gaussian noise using Gaussian diffusion\n\nSynthetic Data Vault (Docs)\n\nData generated with variational autoencoders adapted for tabular data (TVAE) (paper)\nCan Synthetic Data Boost Machine Learning Performance?\n\n\nImbalanced Classification via Layered Learning (ICLL)\n\nFor code, see article\nA hierarchical model composed of two levels:\n\nLevel 1: A model is built to split easy-to-predict instances from difficult-to-predict ones.\n\nThe goal is to predict if an input instance belongs to a cluster with at least one observation from the minority class.\n\nLevel 2: We discard the easy-to-predict cases. Then, we build a model to solve the original classification task with the remaining data.\n\nThe first level affects the second one by removing easy-to-predict instances from the training set.\n\n\nIn both levels, the imbalanced problem is reduced, which makes the modeling task simpler.\n\n\nTuning parameters (last resort)\n\nXGBoost and LightGBM have a parameter called scale_pos_weight, which up-weighs positive samples when computing the gradient at each boosting iteration\nUnlikely to have a major effect and probably wonâ€™t generalize well."
  },
  {
    "objectID": "qmd/classification.html#sec-class-calib",
    "href": "qmd/classification.html#sec-class-calib",
    "title": "Classification",
    "section": "Calibration",
    "text": "Calibration\n\nCalibrated- When the predicted probabilities from a model match the observed distribution of probabilities for each class.\n\nCalibration Plots visualize this comparison of distributions\n\nCalibration mesasure are important for validating a predictive model.\n\nLogistic Regression models are usually well-calibrated, but most ML and DL model predicted probabilities arenâ€™t directly produced by the algorithms and arenâ€™t usually calibrated\nRF models can also benefit from calibration although given enough data, they are already pretty well calibrated\nSVM, Naive Bayes, boosted tree algorithms, and DL models benefit most from calibration\n\nAn unintended consequence of applying calibration modeling can be the worsening of calibration for models that are already well calibrated\nIf a model isnâ€™t calibrated, then the magnitude of the predicted probability cannot be interpreted as the likelihood of an event\n\ne.g.Â If 0.66 and 0.67 are two predicted probabilities from an uncalibrated xgboost model, the 0.67 prediction cannot be interpreted as more likely to be an event than the 0.66 prediction.\nMiscalibrated predictions donâ€™t allow you to have more confidence in a label with a higher probability than a label with a lower probability.\nSee (below) the introduction in the paper, â€œA tutorial on calibration measurements â€¦â€ for examples of scenarios where calibration of risk scoring model is essential. If you canâ€™t trust the predicted probabilities (i.e.Â risk) then decision-making is impossible.\n\nalso this paper which also explains some sources of miscalibration (e.g.Â dataset from region with low incidence, measurement error, etc.)\n\n\nEven if a model isnâ€™t caibrated and depending on the metric, it can still be more accurate than a calibrated model.\n\nPoor calibration may make an algorithm less clinically useful than a competitor algorithm that has a lower AUC but is well calibrated (paper)\nCalibration doesnâ€™t affect the AUROC (does not rely on predicted probabilities) but does affect the Brier Score (does rely on predicted probabilities) (paper)(Harrell)\nSince thresholds are based on probabilities, I donâ€™t see how a valid, optimized threshold can be established for an uncalibrated model\nWonder how this affects model-agnostic metrics, feature importance, shap, etc.\n\n\n\nMisc\n\nAlso see Diagnostics, Classification &gt;&gt; Calibration\nPackages\n\n{probably} - tidymodels calibration package\n\nCalibrating Binary Probabilities - Nice tutorial\n\n\nNotes from:\n\nHow and When to Use a Calibrated Classification Model with scikit-learn\nCan I Trust My Modelâ€™s Probabilities? A Deep Dive into Probability Calibration\n\nPython, multiclass example\n\nA tutorial on calibration measurements and calibration models for clinical prediction models (paper)\n\nCalibration curves for nested cv (post)\nFor calibrated models, sample size affects the how well theyâ€™re calibrated\n\n\nEven for a logistic model, N &gt; 1000 is desired for good calibration\nFor a rf, closer to N = 10,000 is probably needed.\n\nDistributions of predicted probabilities\n\n\nRandom Forest pushes the probabilities towards 0.0 and 1.0, while the probabilities from the logistic regression are less skewed.\nDecision Trees are even more skewed than RF\nSays how rare a prediction is.\n\nIn a rf, really low or high probability predictions arenâ€™t rare. So, if the model gives you a 0.93, you shouldnâ€™t interpet it the way you normally would such a high probability (i.e.Â high certainty), because a rf inherently pushes its probabilities towards 0 and 1.\n\n\n\n\n\nBasic Workflow\n\nMisc\n\nIdeally youâ€™d want each model (i.e.Â tuning parameter combination) being scored in a CV or a nested CV to have its own calibration model, but itâ€™s not practical. But also, itâ€™s unlikely an algorithm with a slightly different parameter value combination with have a substantially different predicted probability distribution, and itâ€™s the algorithm itself thatâ€™s the salient factor. Therefore, for now, Iâ€™d go with 1 calibration model per algorithm.\n\nProcess\n\nSplit data into Training, Calibration, and Test\nFor each algorithm, train the algorithm on the Training set, and create the calibration model using it and the Calibration set.\n\nEach algorithm with have itâ€™s own calibration model\nSee below, Example: AdaBoost in Py using CV calibrator\n\nI like sklearnâ€™s ideas for training a calibration model\n\n\nUse the Training set for CV or Nested CV\nFor each split during CV or Nested CV (outer loop)\n\nTrain the algorithm on the training fold, predict on the validation fold, calibrate predictions with calibration model, score the calibrated predictions for that fold\n\nThe calibration model could be used in the tuning process in the inner loop of Nested CV as well\n\nScores should include calibration metrics (See Diagnostics, Classification &gt;&gt; Calibration &gt;&gt; Basic Workflow)\n\nThe rest is normal CV/Nested CV procedure\n\ni.e.Â average scores across the splits then select the algorithm with the best score. Predict and score algorithm on the Test set\nSee Calibration curves for nested cv for details and code on averaging calibration curves\n\n\n\n\n\nMethods\n\n\nMisc\n\nTLDR; Both Platt Scaling and Isotonic Regression methods are the essentially the same except:\n\nPlatt Scaling uses a logistic regression model as the calibration model\nIsotonic Regression uses an isotonic regression model on ordered data as the calibration model\n\n\nPlatt Scaling\n\nMisc\n\nSuitable for smaller data and for calibration curves with the S-shape.\nMay fail when model is already well calibrated (e.g.Â logistic regression models)\nPerforms best under the assumption that the predicted probabilities are close to the midpoint and away from the extremes\n\nSo, might be bad for tree models but okay for SVM, Naive Bayes, etc.\n\n\nProcess\n\nSplit data into Training, Calibration, and Test Sets\nTrain your model on the Training Set\nGet the predicted probabilities from your model on the Test Set\nFit a logistic regression using your modelâ€™s predicted probabilities for the Calibration Set as the predictor and the outcome variable in the Calibration Set as the outcome\nCalibrated probabilities are the predicted probabilities of the LR model using your modelâ€™s predicted probabilites on the Test set as new data.\n\nExample: SVM in R\nsvm_platt_recal = svm_platt_recal_func(model_fit, calib_dat, test_preds, test_dat)\n\nsvm_platt_recal_func = function(model_fit, calib_dat, test_preds, test_dat){\n\nÂ  # Predict on Calibration SetÂ \nÂ  cal_preds_obj &lt;- predict(model_fit,Â \nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  calib_dat[, -which(names(calib_dat) == 'outcome_var')],Â \nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  probability = TRUE)Â \nÂ  # e1071 has a funky predict output; just getting probabilitiesÂ \nÂ  cal_preds &lt;- as.data.frame(attr(cal_preds_obj, 'probabilities')[, 2])\nÂ  # Create calibration model\nÂ  cal_obs_preds_df = data.frame(y = calib_dat$outcome_var, yhat = cal_preds[, 1])\nÂ  calib_model &lt;- glm(y ~ yhat, data = cal_obs_preds_df, family = binomial)Â \n\nÂ  # Recalibrate classifiers predicted probabilities on the test setÂ  Â \nÂ  colnames(test_preds) &lt;- c(\"yhat\")\nÂ  recal_preds = predict(calib.model, test_preds, type='response')Â  Â \n\nÂ  return(recal_preds)\n\n}\n\nSee Istotonic Regression for â€œmodel_fitâ€, â€œcalib_datâ€, â€œtest_predsâ€, and â€œtest_datâ€\n\nExample: AdaBoost in py\nX_, X_test, y_, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\nX_train, X_calib, y_train, y_calib = train_test_split(X_, y_, test_size=0.4, random_state=42)\n\n# uncalibrated model\nclf = AdaBoostClassifier(n_estimators=50)\ny_proba = clf.fit(X_train, y_train).predict_proba(X_test)\n\n# calibrated model\ncalibrator = LogisticRegression()\ncalibrator.fit(clf.predict_proba(X_calib), y_calib)\ny_proba_calib = calibrator.predict_proba(y_proba)\n\nIsotonic Regression\n\nMore complex, requires a lot more data (otherwise it may overfit), but can support reliability diagrams with different shapes (is nonparametric).\n\nTried on palmer penguins (n = 332) and almost all the probabilites were pushed to the edges\nTried on mlbench::PimaIndiansDiabetes (n = 768). Probabilites were mostly clumped into 3 modes.\nPaper used a simulated (n = 5000) dataset. Probabilities moved a little more towards the center but the movement was much less dramatic that the other two datasets.\nI didnâ€™t calculate brier scores but Iâ€™d guess youâ€™d need over a 1000 or so observations for this method to have a significantly positive effect.\n\nLacks continuousness, because the fitted regression function is a piecewise function.\n\nSo, a slight change in the uncalibrated predictions can result in dramatic difference in the calibrated predictions (i.e.Â a change in step)\n\nProcess\n\nSplits: train (50%), Calibration (25%), Test (25%)\nTrain classifier model on train data\nGet predictions from the classifier on the test data\nCreate calibration model dataset from the calibration data\n\nGet predictions from the classifier on the calibration data\nCreate df with observed outcome of calibration data and predictions on calibration data\nOrder df rows according the predictions column\n\nLowest to largest probabilities in the paper but Iâ€™m not sure it matters\n\n\nFIt calibration model\n\nFit isotonic regression model on calibration model dataset\nCreate a step function using the isotonic regression fitted values and the predictions on the calibration data\n\nCalibrate the classifierâ€™s predictions on the test set with the step function\n\nExample: SVM in R\nlibrary(dplyr)\nlibrary(e1071)\ndata(PimaIndiansDiabetes, package = \"mlbench\")\n\n# isoreg doesn't handle NAs\n# e1071::svm doesn't identify the event correctly in non-0/1 factor variables\ndat_clean &lt;-Â \nÂ  PimaIndiansDiabetes %&gt;%Â \nÂ  mutate(diabetes = ifelse(as.numeric(diabetes) == 2, 1, 0)) %&gt;%Â \nÂ  rename(outcome_var = diabetes) %&gt;%Â \nÂ  tidyr::drop_na()\n\n# Data splits Training, Calibration, Test (50% - 25% - 25%)\nsmp_size &lt;- floor(0.50 * nrow(dat_clean))\nval_smp_size = floor(0.25 * nrow(dat_clean))\ntrain_idx &lt;- sample(seq_len(nrow(dat_clean)), size = smp_size)\ntrain_dat &lt;- dat_clean[train_idx, ]\ntest_val_dat &lt;- dat_clean[-train_idx, ]\nval_idx &lt;- sample(seq_len(nrow(test_val_dat)), size = val_smp_size)\ncalib_dat &lt;- test_val_dat[val_idx, ]\ntest_dat &lt;- test_val_dat[-val_idx, ]\nrm(list=setdiff(ls(), c(\"train_dat\", \"calib_dat\", \"test_dat\")))\n\n# Fit classifier; predict on Test Set\n# e1071::svm needs a factor outcome, probability=T to output probabilities\nmodel_fit &lt;- svm(factor(outcome_var) ~ .,\nÂ  Â  Â  Â  Â  Â  data = train_dat,Â \nÂ  Â  Â  Â  Â  Â  kernel = \"linear\", cost = 10, scale = FALSE, probability = TRUE)\ntest_preds_obj &lt;- predict(model_fit,\nÂ  Â  Â  Â  Â  Â  Â  Â  test_dat[, -which(names(test_dat) == 'outcome_var')],\nÂ  Â  Â  Â  Â  Â  Â  Â  probability = TRUE)\n# e1071 has a funky predict output; just getting probabilities\ntest_preds &lt;- as.data.frame(attr(test_preds_obj, 'probabilities')[, 2])\n\n# Predict on Calibration Set\ncal_preds_obj &lt;- predict(model_fit,\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  calib_dat[, -which(names(calib_dat) == 'outcome_var')],\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  probability = TRUE)\n# e1071 has a funky predict output; just getting probabilities\ncal_preds &lt;- as.data.frame(attr(cal_preds_obj, 'probabilities')[, 2])\n\n# Create Calibration Model dataset\ncal_obs_preds_mtx = cbind(y = calib_dat$outcome_var, yhat = cal_preds[, 1])\n# order training data by predicted probabilities\niso_train_mtx &lt;- cal_obs_preds_mtx[order(cal_obs_preds_mtx[, 2]), ]\n\n# Fit Calibration Model\n# (predicted probabilities, observed outcome)\ncalib_model &lt;- isoreg(iso_train_mtx[, 2], iso_train_mtx[, 1])Â \n# yf are the fitted values of the outcome variable\nstepf_data &lt;- cbind(calib_model$x, calib_model$yf)Â \nstep_func &lt;- stepfun(stepf_data[, 1], c(0, stepf_data[, 2]))Â \n# recalibrate classifiers predicted probabilities on the test set\nrecal_preds &lt;- step_func(test_preds[, 1])\n\nhead(recal_preds, n = 20)\nhist(recal_preds)\nhist(test_preds[, 1])\n\nisoreg doesnâ€™t handle NAs\nFor a binary classification model that outputs probabilities, e1071::svm needs:\n\nfactored 0/1 outcome variable\nprobability = TRUE\n\n\nExample: AdaBoost in Py using CV calibrator\nfrom sklearn import datasets\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn_evaluation import plot\nX, y = datasets.make_classification(10000, 10, n_informative=8,\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  class_sep=0.5, random_state=42)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\nclf = AdaBoostClassifier(n_estimators=100)\nclf_calib = CalibratedClassifierCV(base_estimator=clf,\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  cv=3,\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ensemble=False,\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  method='isotonic')\nclf_calib.fit(X_train, y_train)\ny_proba = clf_calib.predict_proba(X_test)\ny_proba_base = clf.fit(X_train, y_train).predict_proba(X_test)\nfig, ax = plt.subplots()\nplot.calibration_curve(y_test,\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  [y_proba_base[:, 1], y_proba[:, 1]],\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  clf_names=[\"Uncalibrated\", \"Calibrated\"],\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  n_bins=20)\nfig.set_figheight(4)\nfig.set_figwidth(6)\nfig.set_dpi(150)\n\nCalibratedClassifierCV has both platt scaling (â€œsigmoidâ€)(default) and isotonic (â€œisotonicâ€) calibration methods (Docs)\nCalibration model is built using the test fold (aka validation fold)\n\nensemble=True\n\nFor each cv split, the base estimator is fit on the training fold, and the calibration model is built using the â€œtestâ€ fold (aka validation fold)\n\nThe test (aka validation) fold is the calibration data described in the isotonic and platt scaling process sections above\n\nFor prediction, predicted probabilities are averaged across these individual calibrated classifiers.\n\nensemble=False\n\nLOO CV is performed using cross_val_predict, and those predictions are used to train the calibration model.\nThe base estimator is trained using all the data (i.e.Â training and test (aka validation)).\nFor prediction, there is only one classifier and calibration model combo.\nThe benefit is that itâ€™s faster, and since thereâ€™s only one combo, itâ€™s smaller in size as compared to ensemble = True which is k combos. (not as accurate or as well-calibrated though)\n\n\n\nOther forms\n\nFor logistic regression models, adjustment using the Calibration Intercept (and Calibration Slope)\n\nSeems similar to Platt Scaling, but has strong overfitting vibes so caveat emptor\nFor calculating the values, see Diagnostics, Classification &gt;&gt; Calibration &gt;&gt; Evaluation of Calibration Levels &gt;&gt; Weak &gt;&gt; Intercept, Slope\npaper, see supplemental material\nProcedure\n\nThe calibration intercept is added to the fitted modelâ€™s intercept\nThe calibration slope is multiplied times all (nonexponentiated) coefficients of the fitted model (including interactions)\nPredictions are then calculated using the new formula\n\n\nExample\n\nFrom How We Built Our COVID-19 Estimator\nTarget: Risk of Death (probabilities)\nPredictors: Age, Gender, Comorbidities\nModel: XGBoost\nâ€œEvery time our model makes a prediction, we compare the result to what the model would have returned for an individual of the same age and sex and only one of the listed comorbidities, or none at all. If the predicted risk is lower than the risk for a comorbidity taken on its ownâ€”such as, say, the estimated risk for heart disease alone being greater than the risk for heart disease and hypertension, or the risk for metabolic disorders being lower than the risk of someone with no listed comorbiditiesâ€”our tool delivers the higher number instead. We also smoothed our estimates and confidence intervals, using five-year moving averages by age and gender.â€"
  },
  {
    "objectID": "qmd/classification.html#sec-class-featimp",
    "href": "qmd/classification.html#sec-class-featimp",
    "title": "Classification",
    "section": "Feature Importance",
    "text": "Feature Importance\n\nMisc\n\nSee notebook and bookmarks for proper algorithms (e.g.Â permutation importance)Â to specify for variable importance plots\nPermutation Importance\n\nPermutation importance is generally considered as a relatively efficient technique that works well in practice\nImportance of correlated features may be overestimated\n\nIf you have highly correlated features, use partykit::varimp(conditional = TRUE)\n\nProcess\n\nTake a model that was fit to the training dataset\nEstimate the predictive performance of the model on an independent dataset (e.g., validation dataset) and record it as the baseline performance\nFor each feature i:\n\nrandomly permute feature column i in the original dataset\nrecord the predictive performance of the model on the dataset with the permuted column\ncompute the feature importance as the difference between the baseline performance (step 2) and the performance on the permuted dataset\n\n\n\nVariable Importance plots can be useful for model explaining to clients. Ex. Real estate: variables that are most influential in determining housing price are aspects of a house that could be emphasized by Realtors to their clients.\nMake sure to use â€œLossFunctionChangeâ€ importance type in Catboost.\n\nLooks at how much the loss function changes when a feature is excluded from the model.\nDefault method capable of giving importance to random noise\nRequires evaluation on a testing set\n\n\nxgboost\nxg_wf_best &lt;- xg_workflow_obj %&gt;%\nfinalize_workflow(select_best(xg_tune_obj))\nxg_fit_best &lt;- xg_wf_best %&gt;%\nfit(train)\nimportances &lt;- xgboost::xgb.importance(model = extract_fit_engine(xg_fit_best))\nimportances %&gt;%\nmutate(Feature = fct_reorder(Feature, Gain)) %&gt;%\nggplot(aes(Gain, Feature)) +\ngeom_point() +\nlabs(title = \"Importance of each term in xgboost\",\nÂ  subtitle = \"Even after turning direction numeric, still not *that* important\")\nUsing vip package\nlibrary(vip)\n\n# xg_wf = xgboost workflow object\nfit(xg_wf, whole_training_set) %&gt;%Â \npull_workflow_fit() %&gt;%Â \nvip(num_features = 20)\nglmnet\n\nlin_trained &lt;- lin_wf %&gt;%\nÂ  Â  finalize_workflow(select_best(lin_tune)) %&gt;%\nÂ  Â  fit(train) # or split_obj, test_dat, etc.\n\nlin_trained$fit$fit %&gt;%\nÂ  Â  broom::tidy %&gt;%\nÂ  Â  top_n(50, abs(estimate)) %&gt;%\nÂ  Â  filter(term != \"(Intercept)\") %&gt;%\nÂ  Â  mutate(ter = fct_reorder(term, estimate)) %&gt;%\nÂ  Â  ggplot(aes(estimate, term, fill = estimate &gt; 0)) +\nÂ  Â  geom_col() +\nÂ  Â  theme(legend.position = \"none\")"
  },
  {
    "objectID": "qmd/cli.html#sec-cli-misc",
    "href": "qmd/cli.html#sec-cli-misc",
    "title": "CLI",
    "section": "Misc",
    "text": "Misc\n\nResources\n\nData Science at the Command Line\n\nctrl-rshell command history search\n\nMcFly - intelligent command history search engine that takes into account your working directory and the context of recently executed commands. McFlyâ€™s suggestions are prioritized in real time with a small neural network\n\nPath to a folder thatâ€™s above root folder:\n\n1 level up: ../desired-folder\n2 levels up: ../../desired-folder\n\nDebian vs.Â Ubuntu (from ChatGPT)\n\nStability vs.Â Freshness:\n\nDebian: Debian is known for its stability and reliability. It has a rigorous testing process and a conservative approach to updates, which makes it suitable for servers and systems where stability is crucial.\nUbuntu: Ubuntu is based on Debian but tends to be more up-to-date with software packages. It follows a time-based release cycle, with regular releases every six months. This can be appealing if you want access to the latest features and software.\n\nPackage Management:\n\nDebian: Debian uses the Debian Package Management System (dpkg) and Advanced Package Tool (APT) for package management. It has a vast repository of software packages.\nUbuntu: Ubuntu also uses dpkg and APT but adds its own software management tools like Snap and Ubuntu Software Center. This can make software installation more user-friendly.\n\nCommunity and Support:\n\nDebian: Debian has a large and dedicated community, and itâ€™s known for its strong commitment to free and open-source software principles. It has a stable support structure, but community support may not be as user-friendly as Ubuntuâ€™s.\nUbuntu: Ubuntu has a large and active community, and it offers both free and paid support options. The Ubuntu community is known for its user-friendliness and helpful forums, making it a good choice for beginners.\n\nVariants and Flavors:\n\nDebian: Debian offers different flavors, known as â€œDebian spins,â€ catering to various needs, such as Debian Stable, Debian Testing, and Debian Unstable. These variants differ in terms of software stability and freshness.\nUbuntu: Ubuntu has several official flavors (e.g., Ubuntu Desktop, Ubuntu Server, Kubuntu, Xubuntu) that come with different desktop environments. This variety allows users to choose an environment that suits their preferences.\n\nLicensing:\n\nDebian: Debian has a strict commitment to free and open-source software, prioritizing software that adheres to its Free Software Guidelines.\nUbuntu: While Ubuntu also includes mostly free and open-source software, it may include some proprietary drivers and software by default, which can be a concern for users who prioritize a completely open-source system.\n\nPerformance (Google Search AI)\n\nDebian is considered lightweight and much faster than Ubuntu. It comes with few pre-installed software.\n\nHardware (Google Search AI)\n\nDebian works well on older hardware. Debian still offers a 32-bit version of the distro, while Ubuntu no longer offers a 32-bit version."
  },
  {
    "objectID": "qmd/cli.html#sec-cli-r",
    "href": "qmd/cli.html#sec-cli-r",
    "title": "CLI",
    "section": "R",
    "text": "R\n\nMake an R script pipeable (From link)\nparallel \"echo 'zipping bin {}'; cat chunked/*_bin_{}_*.csv | ./upload_as_rds.R '$S3_DEST'/chr_'$DESIRED_CHR'_bin_{}.rds\"\n#!/usr/bin/env Rscript\nlibrary(readr)\nlibrary(aws.s3)\n\n# Read first command line argument\ndata_destination &lt;- commandArgs(trailingOnly = TRUE)[1]\n\ndata_cols &lt;- list(SNP_Name = 'c', ...)\n\ns3saveRDS(\n  read_csv(\n        file(\"stdin\"), \n        col_names = names(data_cols),\n        col_types = data_cols \n    ),\n  object = data_destination\n)\n\nBy passing readr::read_csv the function, file(\"stdin\"), it loads the data piped to the R script into a dataframe, which then gets written as an .rds file directly to s3 using {aws.s3}.\n\nKilling a process\nsystem(\"taskkill /im java.exe /f\", intern=FALSE, ignore.stdout=FALSE)\nStarting a process in the background\n# start MLflow server\nsys::exec_background(\"mlflow server\")\nDelete an opened file in the same R session\n\nYou **MUST** unlink it before any kind of manipulation of object\n\nI think this works because readr loads files lazily by default\n\nExample:\nwisc_csv_filename &lt;- \"COVID-19_Historical_Data_by_County.csv\"\ndownload_location &lt;- file.path(Sys.getenv(\"USERPROFILE\"), \"Downloads\")\nwisc_file_path &lt;- file.path(download_location, wisc_csv_filename)\nwisc_tests_new &lt;- readr::read_csv(wisc_file_path)\n# key part, must unlink before any kind of code interaction\n# supposedly need recursive = TRUE for Windows, but I didn't need it\n# Throws an error (hence safely) but still works\nsafe_unlink &lt;- purrr::safely(unlink)\nsafe_unlink(wisc_tests_new)\n\n# manipulate obj\nwisc_tests_clean &lt;- wisc_tests_new %&gt;%\nÂ  Â  Â  janitor::clean_names() %&gt;%\nÂ  Â  Â  select(date, geo, county = name, negative, positive) %&gt;%\nÂ  Â  Â  filter(geo == \"County\") %&gt;%\nÂ  Â  Â  mutate(date = lubridate::as_date(date)) %&gt;%\nÂ  Â  Â  select(-geo)\n# clean-up\nfs::file_delete(wisc_file_path)\n\nFind out which process is locking or using a file\n\nOpen Resource Monitor, which can be found\n\nBy searching for Resource Monitor or resmon.exe in the start menu, or\nAs a button on the Performance tab in your Task Manager\n\nGo to the CPU tab\nUse the search field in the Associated Handles section\n\ntype the name of file in the search field and itâ€™ll search automatically\n35548"
  },
  {
    "objectID": "qmd/cli.html#sec-cli-awk",
    "href": "qmd/cli.html#sec-cli-awk",
    "title": "CLI",
    "section": "AWK",
    "text": "AWK\n\n\nMisc\n\nResources\n\nDocs\nAwk - A Tutorial and Introduction\n\n\nPrint first few rows of columns 1 and 2\nawk -F, '{print $1,$2}' adult_t.csv|head\nFilter lines where no of hours/ week (13th column) &gt; 98\nawk -F, â€˜$13 &gt; 98â€™ adult_t.csv|head\nFilter lines with â€œDoctorateâ€ and print first 3 columns\nawk '/Doctorate/{print $1, $2, $3}' adult_t.csv\nRandom sample 8% of the total lines from a .csv (keeps header)\n'BEGIN {srand()} !/^$/ {if(rand()&lt;=0.08||FNR==1) print &gt; \"rand.samp.csv\"}' big_fn.csv\nDecompresses, chunks, sorts, and writes back to S3 (From link)\n# Let S3 use as many threads as it wants\naws configure set default.s3.max_concurrent_requests 50\n\nfor chunk_file in $(aws s3 ls $DATA_LOC | awk '{print $4}' | grep 'chr'$DESIRED_CHR'.csv') ; do\n\n        aws s3 cp s3://$batch_loc$chunk_file - |\n        pigz -dc |\n        parallel --block 100M --pipe  \\\n        \"awk -F '\\t' '{print \\$1\\\",...\\\"$30\\\"&gt;\\\"chunked/{#}_chr\\\"\\$15\\\".csv\\\"}'\"\n\n        # Combine all the parallel process chunks to single files\n        ls chunked/ |\n        cut -d '_' -f 2 |\n        sort -u |\n        parallel 'cat chunked/*_{} | sort -k5 -n -S 80% -t, | aws s3 cp - '$s3_dest'/batch_'$batch_num'_{}'\n\n        # Clean up intermediate data\n        rm chunked/*\ndone\n\nUses pigz to parallelize decompression\nUses GNU Parallel (site, docs, tutorial1, tutorial2) to parallelize chunking (100MB chunks in 1st section)\nChunks data into smaller files and sorts them into directories based on a chromosome column (I think)\nAvoids writing to disk"
  },
  {
    "objectID": "qmd/cli.html#sec-cli-bash",
    "href": "qmd/cli.html#sec-cli-bash",
    "title": "CLI",
    "section": "Bash",
    "text": "Bash\n\nMisc\n\nNotes from\n\nBash for Data Scientists, Data Engineers & MLOps Engineers\n\nBunch of other stuff that I didnâ€™t take notes on\n\n\nResources\n\nBash Scripting Cheatsheet\nCurl Docs\n\nman &lt;command&gt; displays documentation for command\nSpecial Characters\n\n\nâ€œ&gt;â€ redirects the output from a program to a file.\n\nâ€œ&gt;&gt;â€ does the same thing, but itâ€™s appending to an existing file instead of overwriting it, if it already exists.\n\n\n\n\n\nCommands\n\nBasic Commands\n\n\necho $SHELL - prints the type of shell youâ€™re using\necho $PATH - prints all stored pathes\nexport PATH=\"my_new_path:$PATH\" - store a new path\nCommand Syntax: command -options arguments\nPiping Commands: cat user_names.txt|sort|uniq\n\n\n\nAliases\n\nCustom commands that you can define in order to avoid typing lengthy commands over and over again\nExamples\nalias ll=\"ls -lah\"\nalias gs=\"git status\"\nalias gp=\"git push origin master\"\nCreate safeguards for yourself\nalias mv=\"mv -i\"\n\nmv will automatically use the i flag, so the terminal will warn you if the file youâ€™re about to move does already exist under the new directory,\n\nThis way you donâ€™t accidentally overwrite files that you didnâ€™t mean to overwrite.\n\n\n\n\n\nFiles/Directories\n\nList\n\n\nList 10 most recently modified files: ls -lt | head\nList files sorted by file size: ls -l -S\n\nCreate/Delete Directories\nmkdir &lt;dir_name&gt;\nrmdir &lt;dir_name&gt;\nOutput to file: echo â€œThis is an example for redirectâ€ &gt; file1.txt\nAppend line to file: echo â€œThis is the second line of the fileâ€ &gt;&gt; file1.txt\nCreate/Delete file(s):\n# Create files\ntouch file1.txt\ntouch file1.txt file2.tx\n\n# Delete files\nrm file1.txt\nrm file1.txt file2.txt\nMove files/dir; Rename\n# Move single file\nmv my_file.txt /tmp\n# Move multiple files\nmv file1 file2 file3 /tmp\n# Move a directory or multiple directories\nmv d1 d2 d3 /tmp\n# Rename the file using move command\nmv my_file1.txt my_file_newname.txt\n\nFile(s) and directories being moved to â€œtmpâ€ directory\n\nSearch\n\nFind\n# syntax find &lt;path&gt; &lt;expression&gt;\n# Find by name\nfind . -name â€œmy_file.csv\"\n#Wildcard search\nfind . -name \"*.jpg\"\n# Find all the files in a folder\nfind /temp\n# Search only files\nfind /temp -type f\n# Search only directories\nfind /temp -type d\n# Find file modified in last 3 hours\nfind . -mmin -180\n# Find files modified in last 2 days\nfind . -mtime -2\n# Find files not modified in last 2 days\nfind . -mtime +2\n# Find the file by size\nfind -type f -size +10M\n\nLocate (faster)\n\nDocs\nInstall\nbash sudo apt install mlocate # Debian\nUsage\n\nsudo updatedb # update before using\nlocate .csv\nSplit files\n# default: 1000 lines per file, names of new files: xaa, xab, xac, etc.\nsplit my_file\n\n# add a prefix to new file names\nsplit my_file my_prefix\n\n# specify split threshold (e.g. 5000) by number of lines\nsplit --lines=5000 my_file\n\n# specify split threshold by size (e.g. 10MB)\nsplit --bytes=10 MB my_file\nPermissions\n\nls -l See list of files and the permissions\n-rwxrwxrwx - sytax of permissions for a folder or directory\n\nâ€œrwxâ€ stand for read, write, and execute rights, respectively\nThe 3 â€œrwxâ€ blocks are for (1) user, (2) user group, and (3) everyone else.\n\nIn the given example, all 3 of these entities have read, write, as well as execute permissions.\n\nThe dash indicates that this is a file. Instead of the dash, you can also see a â€œdâ€ for directory or â€œlâ€ for a symbolic link.\n\nchmod - edit permissions\n\nExample: chmod u+x my_program.py - makes this file executable for yourself\n\nsudo - â€œsuper userâ€ - using this prefix gives you all the permissions to all the files\n\nsudo su - opens a stand alone super user shell\n\n\n\n\n\nPrint\n\nPrint file content\ncat &lt; my_file.txt\n# or\ncat my_file.txt\nPrint 1 pg at a time: less my_file.txt\nPrint specific number of lines: head -n&lt;num_lines&gt; &lt;file.csv&gt;\nPrint file content from bottom to top: tac my_file.txt\ncat -b log.txt | grep error : shows all lines in log.txt that contain the string â€˜errorâ€™, along with the line number (-b)\n\n\n\nLogicals and Conditionals\n\nLogicals\n\n; : command1 ; command2\n\ncommand 1 and command 2 run independently of each other\n\n& : command1 & command2\n\ncommand 1 runs in the background and command 2 runs in the background\n\n&& : command1 && command2\n\nIf the first command errors out then the second command is not executed\n\n|| : command1 || command2\n\nThe second commmand is only execute if the first command errors\n\nExample\ncd my_dir && pwd || echo â€œNo such directory exist.Checkâ€\n\nIf the my_dir exists, then the current working directory is printed. If the my_dir doesnâ€™t exist, then the message â€œNo such directory exists. checkâ€ message is printed.\n\n\nConditionals\n\nUse [[ ]] for conditions in if / while statements, instead of [ ] or test.\n\n[[ ]] is a bash builtin, and is more powerful than [ ] or test.\nExample: if [[ -n \"${TRACE-}\" ]]; then set -o xtrace; fi\n\n\n\n\n\nString Matching\n\nExample: Search for â€œerrorâ€ and write to file\n#output to a file again\ncat file1 file2 file3 | grep error | cat &gt; error_file.txt\n#Append to the end\ncat file1 file2 file3 | grep error | cat &gt;&gt; error_file.txt\n\nPrints lines into grep which searches for â€œerrorâ€ in each line. Lines with â€œerrorâ€ get written to â€œerror_file.txtâ€\n\nFilter lines\ngrep -i â€œDoctorateâ€ adult_t.csv |grep -i â€œHusbandâ€|grep -i â€œBlackâ€|csvlook\n# -i, --ignore-case-IgnoreÂ  caseÂ  distinctions,Â  so that characters that differ only in case match each other.\n\nSelect all the candidates who have doctorates and a husband and race are Black\ncsvlook is pretty printing from csvkit package (see Big Data &gt;&gt; Larger Than Memory &gt;&gt; csvkit)\n\nCount how many rows fit the criteria\ngrep -i â€œDoctorateâ€ adult_t.csv | wc -l\n\nCounts how many rows have â€œDoctorateâ€\n\n-wc is â€œword countâ€\n\n\n\n\n\n\nVariables\n\nLocal Variable:\n\nDeclared at the command prompt\nUse lower case for name\nAvailable only in the current shell\nNot accessible by child processes or programs\nAll user-defined variables are local variables\n\nEnvironment (global) variables:\n\nCreate with export command\nUse upper case for name\nAvailable to child processes\n\nDeclare local and environment variables then access via â€œ$â€\n# local\nev_car=â€™Teslaâ€™\necho 'The ev car I like is' $ev_car\n\n# environment\nexport EV_CAR=â€™Teslaâ€™\necho 'The ev car I like is' $EV_CAR\n\nNo spaces in variable assignment\n\nAlways quote variable accesses with double-quotes.\n\nOne place where itâ€™s okay not to is on the left-hand-side of an [[ ]] condition. But even there Iâ€™d recommend quoting.\nWhen you need the unquoted behaviour, using bash arrays will likely serve you much better.\n\nFunctions\n\nUse local variables in functions.\nAccept multiple ways that users can ask for help and respond in kind.\n\nCheck if the first arg is -h or â€“help or help or just h or even -help, and in all these cases, print help text and exit.\n\nWhen printing error messages, please redirect to stderr.\n\nUse echo 'Something unexpected happened' &gt;&2 for this\n\n\n\n\n\nScripting\n\nUse the .sh (or .bash) extension for your script\nUse long options, where possible (like â€“silent instead of -s). These serve to document your commands explicitly.\nIf appropriate, change to the scriptâ€™s directory close to the start of the script.\n\nAnd itâ€™s usually always appropriate.\nUse cd \"$(dirname \"$0\")\", which works in most cases.\n\nUse shellcheck. Heed its warnings.\nShebang line\n\nContains the absolute path of the bash interpreter\n\nList paths to all shells: cat/etc/shells\n\nUse as the first line even if you donâ€™t give executable permission to the script file.\nStarts with â€œ#!â€ the states the path of the interpreter\nExample: #!/bin/bash\n\nInterpreter installed in directory â€œ/binâ€\n\nExample: #!/usr/bin/env bash\n\nCommands that should start your script\n\nUse set -o errexit\n\nSo that when a command fails, bash exits instead of continuing with the rest of the script.\n\nUse set -o nounset\n\nThis will make the script fail, when accessing an unset variable. Saves from horrible unintended consequences, with typos in variable names.\nWhen you want to access a variable that may or may not have been set, use \"${VARNAME-}\" instead of \"$VARNAME\", and youâ€™re good.\n\nUse set -o pipefail\n\nThis will ensure that a pipeline command is treated as failed, even if one command in the pipeline fails.\n\nUse set -o xtrace, with a check on $TRACE env variable.\n\nFor copy-paste: if [[ -n \"${TRACE-}\" ]]; then set -o xtrace; fi.\nThis helps in debugging your scripts, a lot.\nPeople can now enable debug mode, by running your script as TRACE=1 ./script.sh instead of ./script.sh .\n\n\nExample: Basic Execution a Bash Script\n\nCreate a directory bash_script: mkdir bash_script\nCreate a hello_world.sh file: touch hello_script.sh\nOpen hello_script.sh (text editor?)\nAdd code, save, and close\nÂ  Â  #!/bin/bash\nÂ  Â  echo â€˜Hello Worldâ€™\nMake file executable: chmod +x hello_world.sh\nExecute file: ./hello_world.sh\n\nTemplate\n#!/usr/bin/env bash\nset -o errexit\nset -o nounset\nset -o pipefail\nif [[ -n \"${TRACE-}\" ]]; then\nÂ  Â  set -o xtrace\nfi\nif [[ \"$1\" =~ ^-*h(elp)?$ ]]; then\nÂ  Â  echo 'Usage: ./script.sh arg-one arg-two\nThis is an awesome bash script to make your life better.\n'\nÂ  Â  exit\nfi\ncd \"$(dirname \"$0\")\"\nmain() {\nÂ  Â  echo do awesome stuff\n}\nmain \"$@\"\n\n\n\nJob Management\n\nPrograms/Scripts will by default run in the foreground, and prevent you from doing anything else until the program is done.\nWhile program is running:\n\ncontrol+c - Will send a SIGINT (signal interrupt) signal to the program, which instructs the machine to interrupt the program immediately (unless the program has a way to handle these signals internally).\ncontrol+z - Will pause the program.\n\nAfter pausing the program can be continued either by bringing it to the foreground (fg), or by sending it to the backgroud (bg).\n\n\nExecute script to run in the background: python run.py &\njobs - shows all running jobs and process ids (PIDS)\nkill - sends signals to jobs running in the background\n\nkill -STOP %1 sends a STOP signal, pausing program 1.\nkill -KILL %1 sends a KILL signal, terminating program 1 permanently.\n\n\n\n\ntmux (â€˜terminal multiplexerâ€™)\n\nEnables you to easily create new terminal sessions and navigate between them. This can be extremely useful, for example you can use one terminal to navigate your file system and another terminal to execute jobs.\nInstallation (if necessary): sudo apt install tmux\n\nTypically comes with the linux installation\n\nSessions\n\ntmux - starts an unnamed session\ntmux new -s moose creates new terminal session with name â€˜mooseâ€™\ntmux ls - lists all running sessions\ntmux kill-session -t moose - kills session named â€œmooseâ€\nexit - stops and quits the current session\nKill all sessions (various opinions on how to do this)\n\ntmux kill-session\ntmux kill-server\ntmux ls | grep : | cut -d. -f1 | awk '{print substr($1, 0, length($1)-1)}' | xargs kill\n\n\nAttach/Detach\n\nWhen you log out of a remote machine (either on purpose or accidentally), all of the programs that were actively running inside your shell are automatically terminated. On the other hand, if you run your programs inside a tmux shell, you can come simply detach the tmux window, log out, close your computer, and come back to that shell later as if youâ€™ve never been logged out.\ntmux detach - detach current session\ncontrol+bthen pressd`: When you have multiple sesssions running, this will allow you to select the session to detach\nFrom inside bash and not inside a session\n\ntmux a : attach to latest created session\ntmux a -t moose : attach to session called â€˜mooseâ€™\n\n\nPane Creation and Navigation\n\ncontrol+b then press â€ (i.e.Â shift+â€™): add another terminal pane below\ncontrol+b then press % (i.e.Â shift+5) : add another terminal pane to the right\ncontrol+b then press â†’ : move to the terminal pane on the right (similar for left, up, down)\n\n\n\n\nSSH\n\nTypically uses a key pair to log into remote machines\n\nKey pair consists of a public key (which both machines have access to) and a private key (which only your own machine has access to)\nâ€œssh-keygenâ€ is a program for generating such a key pair.\n\nIf you run ssh-keygen, it will by default create a public key named â€œid_rsa.pubâ€ and a private key named â€œid_rsaâ€, and place both into your â€œ~/.sshâ€ directory\nYouâ€™ll need to add the public key to the remote machine by piping together cat, ssh, and a streaming operator\n\ncat .ssh/id_rsa.pub | ssh user@remote 'cat &gt;&gt; ~/.ssh/authorized_keys'\n\n\n\nConnect to the remote machine: ssh remote -i ~/.ssh/id_rsa\nCreate a config file instead\n\nLocation: â€œ~/.ssh/configâ€\nContents\nHost dev\nÂ  HostName remote\nÂ  IdentityFile ~/.ssh/id_rsa\n\nConnect using config: ssh dev\nFor Windows and using Putty, see\n\nAWS &gt;&gt; EC2 &gt;&gt; Connect to/ Terminate Instance\nProjects Notebook &gt;&gt; Article, Nested Cross Validation &gt;&gt; Notes &gt;&gt; Running EC2 instances checklist\n\n\n\n\nVim\n\nCommand-line based text editor\nCommon Usage\n\nLogging into a remote machine and need to make a code change there. vim is a standard program and therefore usually available on any machine you work on.\nWhen running git commit, by default git opens vim for writing a commit message. So at the very least youâ€™ll want to know how to write, save, and close a file.\n\n2 modes: Navigation Mode; Edit Mode\n\nWhen Vim is launched youâ€™re in Navigation mode\nPress i to start edit mode, in which you can make changes to the file.\nPress Esc key to leave edit mode and go back to navigation mode.\n\nCommands (Cheatsheet)\n\nx deletes a character\ndd deletes an entire row\nb (back) goes to the previous word\nn (next) goes to the next word\n:wq saves your changes and closes the file\n:q! ignores your changes and closes the file\n\n\n\n\nPackages\n\nCommon package managers: apt, Pacman, yum, and portage\nAPT (Advanced Package Tool)\n\nInstall Packages\n# one pkg\nsudo apt-get install &lt;package_name&gt;\n# multiple\nsudo apt-get install &lt;pkg_name1&gt; &lt;pkg_name2&gt;\n\nInstall but no upgrade: sudo apt-get install &lt;pkg_name&gt; --no-upgrade\n\nSearch for an installed package: apt-cache search &lt;pkg_name&gt;\nUpdate package information prior to â€œupgradingâ€ the packages\nsudo apt-get update\n\nDownloads the package lists from the repositories and â€œupdatesâ€ them to get information on the newest versions of packages and their dependencies.\n\nUpgrade\n# all installed packages\nsudo apt-get upgrade\n\n# To upgrade only a specific program\nsudo apt-get upgrade &lt;package_name&gt;\n\n# Upgrades and handles dependencies; delete obsolete, add new\napt-get dist-upgrade\n\n# together\nsudo apt-get update && sudo apt-get dist-upgrade\n\n\n\n\nExpressions\n\nSort data, filter only unique lines, and write to file: cat adult_t.csv | sort | uniq -c &gt; sorted_list.csv"
  },
  {
    "objectID": "qmd/cli.html#sec-cli-powsh",
    "href": "qmd/cli.html#sec-cli-powsh",
    "title": "CLI",
    "section": "Powershell",
    "text": "Powershell\n\nComments: &lt;# comment #&gt;\nMulti-line Commands\nffmpeg -i input.mkv -map 0:v:0 `\n       -map 0:a:2 -map 0:a:0 -map 0:a:1 -map 0:a:3 `\n       -map 0:s -c copy `\n       -disposition:a:0 default `\n       reordered.mkv\n\nIn bash, itâ€™s a backslash (\\), but in Powershell, itâ€™s a backtick ( ` )\n*Donâ€™t forget that thereâ€™s a space between the last character and the backtick.*\nIn practice, this will look like\nffmpeg -i .input.mkv -map 0:v:0 `\n&gt;&gt; -map 0:a:2 -map 0:a:0 -map 0:a:1 -map 0:a:3 `\n&gt;&gt; -map 0:s -c copy `\n&gt;&gt; -disposition:a:0 default `\n&gt;&gt; reordered.mkv\n\nString Matching\n\nPrint line with pattern\nSelect-String -Path \"file*.txt\" -Pattern \"error\"\nfile1.txt:3:This is the error line of the file\nfile2.txt:3:This is the error line of the file\nfile3.txt:3:This is the error line of the file\n\nMatches the 3rd line of each file\n\n\nEnvironment Variables\n\nSet an environment variable\nSet-Item -Name PYTHONSTARTUP -Value C:\\path\\to\\pythonstartup.py\n\nSame expression to modify existing environment variable\n\nDelete environment variable\nRemove-Item -Name &lt;variable_name&gt;\nVerify value of an environment variable\n$env:&lt;variable_name&gt;\n\nPorts\n\nFind application using a port.\nnetstat -aon | findstr ':80'\nnetstat -anp | find \":80\"\n\nIf port 80 is being used by the application, it will return a PID. Then you can find it in Task Manager &gt;&gt; Processess\n\nList all Listening and Established ports\nnetstat -anob\nCheck for processes using a port\nGet-Process -Id (Get-NetTCPConnection -LocalPort 80).OwningProcess\nTest connection to local port to see if itâ€™s open\nTest-NetConnection -ComputerName localhost -Port 80 | Select-Object TcpTestSucceeded\nCheck firewall settings for an app\nnetsh advfirewall firewall show rule name=\"name_of_app\""
  },
  {
    "objectID": "qmd/cli.html#sec-cli-batscri",
    "href": "qmd/cli.html#sec-cli-batscri",
    "title": "CLI",
    "section": "Batch Scripting",
    "text": "Batch Scripting\n\nMisc\n\nResources\n\nWindows Batch Scripting\n\n\nExample: Create variables and execute\n@echo off\n\nrem Set the path to the Rscript executable\nset RSCRIPT=\"C:\\Users\\user\\AppData\\Local\\Programs\\R\\R-4.2.3\\bin\\Rscript.exe\"\n\nrem Set the path to the R script to execute\nset RSCRIPT_FILE=\"C:\\Users\\user\\my_r_script.R\"\n\nrem Execute the R script\n%RSCRIPT% %RSCRIPT_FILE%\n\nrem Pause so the user can see the output\nexit\n\n@echo off - This line turns off the echoing of commands in the command prompt window, making the output cleaner.\nrem - Keyword that denotes a comment in a batch file.\nset RSCRIPT= - This line assigns the path to the Rscript executable to the environment variable RSCRIPT.\nset RSCRIPT_FILE= - The path to the R script file is assigned to the environment variable RSCRIPT_FILE.\n%RSCRIPT% %RSCRIPT_FILE% - Executes the R script using the Rscript executable and passes the path to the R script file as an argument.\nexit - This command exits the batch file and closes the command prompt window.\n\nExample: Exit if script errors\nRscript \"C:\\Users\\ercbk\\Documents\\R\\Projects\\Indiana-COVID-19-Tracker\\R\\collection\\build-opentab-dat.R\"\n\nREM if the data building script errors, bat script terminates without running other scripts or commands\nif %errorlevel% neq 0 exit /b %errorlevel%\n\ncd \"C:\\Users\\ercbk\\Documents\\R\\Projects\\Indiana-COVID-19-Tracker\"\n\ngit add data/YoY_Seated_Diner_Data.csv\ngit commit -m \"opentab data update\"\ngit pull\ngit push\n\nEXIT"
  },
  {
    "objectID": "qmd/cli.html#sec-cli-wsl",
    "href": "qmd/cli.html#sec-cli-wsl",
    "title": "CLI",
    "section": "WSL",
    "text": "WSL\n\nResources\n\nDocs\nTo update password (link) using username\n\nLoad Linux: wsl -d Ubuntu-22.04 where -d is for â€“distribution\nWSL Help: wsl --help\nExit linux terminal back to command prompt or powershell: exit"
  },
  {
    "objectID": "qmd/cloud-services.html#misc",
    "href": "qmd/cloud-services.html#misc",
    "title": "6Â  Cloud Services",
    "section": "6.1 Misc",
    "text": "6.1 Misc\n\nSee Cloud Costs Every Programmer Should Knowfor various service estimates in order to perform back-of-the-napkin calculations of project costs\nRStudio Server on your docker image allows you to access an ide connected to the server through a browser. Useful so you can make sure the correct packages are installed.\nServerless computing is a method of providing backend services on an as-used basis.\n\nA serverless provider allows users to write and deploy code without the hassle of worrying about the underlying infrastructure\nCharged based on their computation and do not have to reserve and pay for a fixed amount of bandwidth or number of servers, as the service is auto-scaling\ne.g.Â AWS Lambda (i.e.Â resources only get spun-up when an event is triggered)\n\nNVIDIA GPU Guide (thread)\n\nRTX 20-series or 30-series GPUs are forbidden from inclusion in data centers\nGeneral Recommendations (Oct 2022)\n\nA100 for model training\nT4 for inference workloads\n\nK80\n\nReleased in 2015, the K80 contained a lot of VRAM for the time (24 GB)\nCame before tensor cores and is relatively weak by todayâ€™s standards\nOnly okay for learning purposes\n\nP4\n\nReleased in 2016\nValue came from its low power consumption\nMay find it priced higher than its upgraded version (the T4), so recommended to avoid it\n\nT4\n\nReleased in 2018\nSignificant upgrade for inference workloads compared to the P4\nExtremely low power consumption, tensor cores, and plenty (16GB) of VRAM\nCheap, so if you have an inference workload, recommended to strongly consider a T4\n\nP100\n\nBig improvement for model training workloads over the K80 when released\nLess RAM (16GB) than K80\nWay more computeÂ  than K80\n\nCan see memory savings from using mixed-precision training\n\nNo tensor cores\n\nV100\n\nHuge upgrade over the P100\nSame VRAM as P100 many but more CUDA cores\nIntroduces Tensor Cores\nMore cost-efficient than the P100\n\nA100\n\nnewest data center GPU\nupgraded tensor cores\nmost benchmarks show 3x+ faster training compared to the V100\n80GB VRAM\nPrice tag might be big, but itâ€™s usually worth it over the V100"
  },
  {
    "objectID": "qmd/cloud-services.html#price-management",
    "href": "qmd/cloud-services.html#price-management",
    "title": "6Â  Cloud Services",
    "section": "6.2 Price Management",
    "text": "6.2 Price Management\n\nspot instances for cheaper machines\nautoscaling (kubernetes?) to handle peak usage times (spin-up more machines) while saving during slow times (spin down excess machines)\nUse opensource project management tools (dvc, airflow, etc)\nGoogle\n\nThe Google Kubernetes Engine (GKE) control plane is free, whereas Amazonâ€™s (EKS) costs $0.20 an hour.\n\nAWS\n\nWith a well-defined framework of tag keys and values applied across different AWS resources, billing breakdowns by tag prove extremely useful for greater insight on the source of AWS charges â€” especially if resources are tagged by department, or team, or different layers of organizational granularity.\nReserved Instances - commit to specific configurations for one or three years at reduced cost\nSpot Instances - pay significantly lower costs but potential for applications to be interrupted\nSavings Plans\n\nEC2 Instance Savings Plans to reduce compute charges for specific instance types and AWS regions\n\nSavings of up to 72%\n\nCompute Savings Plans to reduce compute costs irrespective of type and region.\n\nSavings up to 66% and extends to ECS Fargate and Lambda functions.\n\n\nImage Management\n\nData Lifecycle Manager - automates the creation, retention, and deletion of images\n\nWill not manage images and snapshots created by other means, and it also excludes instance store-backed images.\nEC2 Recycle Bin - serves as a safety net to avoid the accidental deletion of resources â€” retaining images and snapshots for a configurable time where we may restore them before they are deleted permanently.\n\n\nLambda\n\nCloudwatch - Lambda automatically creates log groups for its functions, unless a group already exists matching the name /aws/lambda/[{functionName}]{style='color: #990000'}. These default groups do not configure a log retention period, leaving logs to accumulate indefinitely and increasing CloudWatch costs.\n\nExplicitly configure groups with matching names and a retention policy to maintain a manageable volume of logs.\n\nMemory Optimization - AWS Lambda Power Tuning can help to identify optimizations, albeit with notable initial costs given the underlying use of AWS Step Functions.\n\nLambda charges based on compute time in GB-seconds, where the duration in seconds is measured from when function code executes until it either returns or otherwise terminates, rounded up to the nearest millisecond. To reduce these times, we desire optimal memory configuration.\n\n\nS3 Lifecycle Configuration\n\nCharged for how much data stored, but also which S3 storage classes are utilized.\n\nStandard (default) class is the most expensive, permitting regular access to objects with high availability and short access times.\nInfrequent Access (IA) classes offer reduced cost for data which requires limited access (usually once per month)\nArchival options via Glacier deliver further cost reductions.\n\nConfiguring the lifecycle allows you to automatically transfer data to different storage classes and thereafter permanently delete it, X and Y days respectively after data creation"
  },
  {
    "objectID": "qmd/cloud-services.html#kaggle",
    "href": "qmd/cloud-services.html#kaggle",
    "title": "6Â  Cloud Services",
    "section": "6.3 Kaggle",
    "text": "6.3 Kaggle\n\nFree\n\n4-core CPU instances w/30 GB RAM\n2-core CPU, 2xT4 GPU w/13GB RAM\n\nT means tensor cores\n1 hour spent using 2xT4â€™s takes the same amount of your quota as a P100 (old free gpu offering)\n\nMeans 30-40 hours of free, multi-GPU compute per week"
  },
  {
    "objectID": "qmd/cloud-services.html#saturn-cloud",
    "href": "qmd/cloud-services.html#saturn-cloud",
    "title": "6Â  Cloud Services",
    "section": "6.4 Saturn Cloud",
    "text": "6.4 Saturn Cloud\n\nSaturn Cloud Recipes\n\nJSON files that specify your environment\nGood for keeping track of server dependencies (e.g.Â linux libraries)\n\nDunno about R packages"
  },
  {
    "objectID": "qmd/cloud-services.html#google-cloud-platform-gcp",
    "href": "qmd/cloud-services.html#google-cloud-platform-gcp",
    "title": "6Â  Cloud Services",
    "section": "6.5 Google Cloud Platform (GCP)",
    "text": "6.5 Google Cloud Platform (GCP)\n\nBigQuery sandbox is Googleâ€™s GCP free tier cloud SQL database. Itâ€™s free but your data only lasts 60 days at a time.\nGCP allows users to run deep learning workloads on TPUs\nSince data expires after 60 days, back-up the model coefficients and performance score tables to Google Sheets. Article suggested this is possible through WebUI.\nAs of Nov.19, regression, logistic regression, and k-nn are the only models available to be run with the sql query editor\nhttps://cloud.google.com/free/\n\n$300 credit for 12 months\nAlways free:\n\n2M requests for containers\n1 GB storage\n\nScalable NoSQL document database.\n50,000 reads, 20,000 writes, 20,000 deletes per day\n\nFunctions\n\n1 f1-micro instance per month (Available only in region: us-west1, Iowa: us-central1, South Carolina: us-east1)\n30 GB-months HDD\n5 GB-months snapshot in select regions\n1 GB network egress from North America to all region destinations per month (excluding China and Australia)\n\nKubernetes\n\nOne-click container orchestration via Kubernetes clusters, managed by Google.\nNo cluster management fee for clusters of all sizes\nEach user node is charged at standard Compute Engine pricing\n\nApp Engine\n\n28 instance hours per day\n5 GB Cloud Storage\nShared memcache\n1,000 search operations per day, 10 MB search indexing\n100 emails per day\n\nBigQuery\n\nFully managed, petabyte scale, analytics data warehouse.\n1 TB of querying per month\n10 GB of storage\n\nOther Stuff\n\nYour free trial credit applies to all GCP resources, with the following exceptions:\n\n* You canâ€™t have more than 8 cores (or virtual CPUs) running at the same time.\n* You canâ€™t add GPUs to your VM instances.\n* You canâ€™t request a quota increase. For an overview of Compute Engine quotas, see Resource quotas.\n* You canâ€™t create VM instances that are based on Windows Server images.\n\nYou must upgrade to a paid account to use GCP after the free trial ends. To take advantage of the features of a paid account (using GPUs, for example), you can upgrade before the trial ends. When you upgrade, the following conditions apply:\n\n* Any remaining, unexpired free trial credit remains in your account.\n* Your credit card on file is charged for resources you use in excess of whatâ€™s covered by any remaining credit.\nYou can upgrade your account at any time after starting the free trial. The following conditions apply depending on when you upgrade:\n* If you upgrade before the trial is over, your remaining credit is added to your paid account. You can continue to use the resources you created during the free trial without interruption.\n* If you upgrade within 30 days of the end of the trial, you can restore the resources you created during the trial.\n* If you upgrade more than 30 days after the end of the trial, your free trial resources are lost.\n\nSpot Instances (Preemptible VM)\n\nusage capped at 24 hrs\npricing is fixed and not market-driven\n\nGoogle price calculator:Â https://cloud.google.com/products/calculator/#id=3115f19f-4ff0-4c57-9028-69cb994fe7ca\nExample\n\ncreating a cluster with:\n\n1 x Dataproc cluster node with 30 GB of RAM\n3 x Dataproc worker nodes with 15 GB of RAM\nUsing less than 5 GB of disk space in a bucket\nAnd running the cluster for only 4 hrs\nWould cost only around $5 at the end of the month\n\n\nFree Tier\n\nincludes a 12-month free trial with $300 credit to use with any GCP services and an Always Free benefit, which provides limited access to many common GCP resources\nUse to test out, but KEEP EVERYTHING SMALL (data, hardware, etc). Need to upgrade it to see the true benefit. Free tier resources look like my desktop computer. Whatever cash is leftover should transfer to account.\nhttps://cloud.google.com/free/docs/gcp-free-tier#how-to-upgrade\nupgrade it from the free trial to a paid account through the GCP Console clicking the Upgrade button at the top of the page\n\n\n\nSteps for new project\n\nGo to interfaceÂ https://console.cloud.google.com/\ncreate a project. â€œselect a projectâ€ on top bar â€“&gt; â€œnew projectâ€ on top right â€“&gt; choose name (optionally a folder/organization if you have one) â€“&gt; create\n(article wasnâ€™t very reliable and went on talk about a python implementation so I stopped here\n\nTips\n\nApp Engine\n\nDonâ€™t use App Engine Standard environments â€” big brother G wants you to use rather Flex environments, otherwise, theyâ€™ll punish you.\nReview cost analysis regularly to make sure there are no surprising costs.\nMake sure you clean up redundant App Engine application versions to prevent G from robbing you."
  },
  {
    "objectID": "qmd/cloud-services.html#microsoft-azure",
    "href": "qmd/cloud-services.html#microsoft-azure",
    "title": "6Â  Cloud Services",
    "section": "6.6 Microsoft Azure",
    "text": "6.6 Microsoft Azure\n\nhttps://azure.microsoft.com/en-us/free/?WT.mc_id=Revolutions-blog-davidsmi\nhttps://visualstudio.microsoft.com/dev-essentials/\n\nstarts azure trial but gives you free sql server developer edition\n\nWonâ€™t be charged until you choose to upgrade.\n12 months access to $ services for free\n$200 credit for any service for 30 days\n\nAt the end of the 30 days, I think the remainder goes into your account after you change to a pay-to-play account\n\nAccess to the services that are always free\n\nAzure Kubernetes Service (AKS)\nFunctions\n\n1,000,000Â requests per month\na solution for easily running small pieces of code in the cloud. You can write just the code you need for the problem at hand, without worrying about a whole application or the infrastructure to run it.\nExample use case:Â for handling WebAPI requests and sending the different data and results to where it needs to go.\n\nApp Service\n\n10Â web, mobile, or API apps\n\nActive Directory B2C (identity)\n\n50,000Â authentications per month\n\nMachine Learning Server\n\nDevelop and run R and Python models on your platform of choice.\n\nSQL Server 2017 Developer Edition\n\nBuild, test, and demostrate applications in a non-production environment.\n\nOther stuff\n\nBlob storage\n\nobject storage solution for the cloud\noptimized for storing massive amounts of unstructured data\n\nSpot Instances (Low Priority VM)\n\nnot time limit on instance usage\nno warning on termination by Azure\n\nTips\n\nIf you canâ€™t create a service, because Azure servers are under maintenance for more than a couple of minutes â€” check out your permissions and registrations under the â€œResource providersâ€ panel.\nIf you see any strange errors on the Azure Portal â€” just change the filtersâ€™ values.\nIf you use Azure Machine Learning, and your scoring function cannot locate your source code â€” deliver the code as a Model and add it explicitly to the sys.path in the init function.\nIf you use Azure Machine Learning, donâ€™t use Batch Endpoints â€” it looks like they are not ready yet â€” just use the regular Published Pipelines. In fact, â€œBatch endpointâ€ is just a wrapper around a published pipeline.\nDonâ€™t include flask in your Azure conda environment specification."
  },
  {
    "objectID": "qmd/cloud-services.html#aws",
    "href": "qmd/cloud-services.html#aws",
    "title": "6Â  Cloud Services",
    "section": "6.7 AWS",
    "text": "6.7 AWS\n\nInstance types\n\nc-type instances are compute heavy\nr-type instances are RAM heavy\nm-type instances are balanced\nâ€œEach thread is represented as a virtual CPU (vCPU) on the instance. An instance has a default number of CPU cores, which varies according to instance type. For example, an m5.xlarge instance type has two CPU cores and two threads per core by defaultâ€”four vCPUs in total.â€\nspot prices from 03/24/2020, all calculations over the previous month\ngen purpose\n\nm6g.8xlarge\n\ngen purpose, 32 vcpu, 128 gb\nnewer graviton, didnâ€™t see any specs, but supposed to be much better than the xenon 1st gen\n\nm5.8xlarge\n\ngen purpose, 32 vcpu, 128 gb\nolder 3.1 ghz, xenon\non-demand $1.54/hr\n\nm5a.8xlarge\n\ngen purpose 32 vcpu, 128 gb\n2.4 ghz, slower processor speed than m5\n\nm5n.8xlarge\n\ngen purpose 32 vcpu, 128 gb\n3.1 ghz, xenon specialized for neural networks, ML tasks\nn.virg, 71% savings,Â &lt;5% interruption\nohio,Â 83% savings, &lt;5% interruption\non-demand $1.90/hr\npotential spot price = $0.32\n\nm5dn.8xlarge\n\nsame but with 2 ssd hard drives\n\nm4.10xlarge\n\ngen purpose 40 vcpu, 160 gb\n2.4 ghz\nsmaller write-up, get the sense these are older processors/instances\n\n\ncompute optimized\n\nRequires HVM AMIs that include drivers for ENA (network adaptor) and NVMe (ssd hard drives)\n\nseems standard on a lot of instances (gen purpose and here), shouldnâ€™ t be an issue\n\nc5.9xlarge\n\n36 vcpu, 72 gb\n3.4 ghz\non-demand $1.53/hr\n\nc5d.9xlarge\n\nsame but with ssd\n\nc5n.9xlarge\n\n36 vcpu, 96 gb\n3.0 ghz, built for task needing high throughput for networking\non-demand, $1.94/hr\n\nc4.8xlarge\n\n36 vcpu, 60 gb\n2.9 ghz\n67% savings, &lt;5% interruption\non-demand $1.59/hr\npotential spot price = $0.52\n\n\nmemory optimized\n\nr5.8xlarge\n\n32 vcpu, 256 gb\n3.1 ghz\nn.virg, 72% savings, 5-10% interruption\nn.cal,Â 76% savings, &lt;5% interruption\non-demand $2.02/hr\npotential spot price = $0.48\n\nr5a.8xlarge\n\n32 vpu, 256 gb\n2.5 ghz\n\nr5n.8xlarge\n\n32 vcpu, 256 gb\n3.1 ghz, neural network optimized\nus.west. oregon 76% savings, 5-10% interruption\non-demand $2.38/hr\npotential spot price = $0.57\n\nr4.8xlarge\n\n32 vcpu, 244 gb\n2.3 ghz\n\nz1d.6xlarge\n\n24 vcpu, 192 gb\n4.0 ghz\non-demand $2.23\n\n\naccelerated computing\n\ninf1.6xlarge\n\n24 vcpu, 48 gb\nbuilt for ML\non-demand $1.91/hr\n\n\n\nFree Tier (12 months after sign-up)\n\naws.amazon.comÂ â€“ pricing (top) â€“ free tier (mid) â€“ create a free account (mid)\nEC2\n\n750 hrs/mo of t2-micro instance usage\n\nfor Linux, Windows, RHEL, SLES AMIs\n\n\nElastic Block Storage (EBS)\n\n30 GB\ncan be connected to an ec2\n\nElastic Container Registry\n\n500 MB per month\n\nfor storing and retrieving Docker images\nexample in course was a basic nginx image and it was 50MB\n\n\nS3\n\n5 GB of standard storage (high availability/ high durability)\n20,000 Get Requests,Â 2000 Put Requests per month\n\nElastic Load Balancing\n\n750 hrs per month shared between classic and application load balancers\n\nno idea what the differences are between classic and application\n\n\n\nPricing\n\nPrice per GPU as of 29-06-2023\n\n\nExamples\n\nr3.4xlarge 16 CPUs, 122 GB RAM,Â 1 x 320 SSD,Â Spot Price: $0.1517/h\n\nTrained H2O GBM, RF, XGBoost, DeepLearning. Cluster ran for 2 hr 40 min. Total Cost = around $0.42\nhttps://www.daeconomist.com/post/2019-01-15-partii/\n\n\nStorage\n\nS3\n\ncharged by amount stored\n\n$0.023/GB for standard (for first 50 TB)\n0.004/GB for glacier and 0.00099/GB deep glacier\n\ntakes longer to retrieve and not always available\n\n\nfree inbound transfer\nfree transfer between aws services (e.g.Â S3 to EC2) within the same region\n\nAurora\n\nstorage + inbound/outbound: $0.20 per million requests\n\n\nConsolidated Biling\n\na separate account. All company individual accounts (marketing, sales, etc.) bills are pooled into this account\nhas no access to services\nhas no permissions to access services in other accounts\npooled bill counted towards potential discount billing\n\nCalculators\n\nTotal Cost of Ownership (TCO) calculator\n\ncompares cost of running a project on-premises to aws cloud\n\naws pricing calculator\n\ncalculates price of running a cloud application\ncalculator.aws.com\nestimates cost per service, per service group, and total infrastructure\nhelps find right ec2 instance and region\n\n\nBilling and Cost Management console\n\ncost explorer\n\nview and analysis costs and usage\n\n\n\n\nSpot Instances\n\nSummary\n\nGo to spot advisor and find instances that fit budget and compute requirements\nPrepare strategy for interruption\nOther services\n\nAs of Jan 01, 2019, cloudyrâ€™s aws.ec2 PKG didnâ€™t support all spot instances.\nno time limit on instance usage\nAWS gives a 2 min warning when it decides it needs your spot instance\npricing is market driven depending on capacity levels at the time\nAvailable actions when Amazon â€œinterruptsâ€ your instance:\n\nHibernation:\n\nâ€œlike closing your laptop displayâ€\nsaves data and memory and reboots once instance is available again\nRight before interruption, a daemon on the instance freezes the memory and stores it in Elastic Block Store (EBS) root volume\nYour EC2 will retain this root volume and any other EBS data volumes\nOnce market price falls below bid price, instance resumes with memory restored from disk to RAM\nYou arenâ€™t charged while instance is in hibernation, but EBS volumes do cost $.\nAvailable for instance types: C3, C4, M4, R3, and R4 with &lt; 100 GB RAM on Amazonâ€™s Linux, Ubuntu, and Windows\nAll this is done by something called the EC2 Hibernation Agent which sound like its just the name of the program on the servers\n\nStop\n\nâ€œlike shutting down your computer to be turned on laterâ€\nlose whatever is in RAM but retain EBS data volumes ($)\nrestores once bid price &lt; market price\n\nTerminate\n\n***default option***\neverything deleted\n\n\nSpot Advisor\n\n**always use this before spinning up spot instances **\nhttps://aws.amazon.com/ec2/spot/instance-advisor\nInput\n\nvCPUs\nMemory size\nPlatform (linux?)\navailability zone (region?)\namount required (number of instances?)\n\noutput\n\ninstance type\nvCPUs\nMemory (GB)\nSavings over On-Demand (%)\nFrequency of termination (%)\n\nliklihood your instance will get terminated\n\n\n\nRunInstance API\n\nFor requesting a spot instance through CLI I think\nLooks like you send something that looks like a python dict with max price, type, region, etc. to this API\n\nSpot Blocks\n\nallows you to set a finite duration that your instance will run for\n\n1 to 6 hrs\nno interruption during that time\n\ntypically 30 to 45% cheaper than on-demand and maybe an additional 5% cheaper during non-peak hours for the region\nrecommended for batch runs\n\nStrategy\n\nUse regions with largest pools of spot instances\n\nLargest pools\n\nus.east.1 (north.virginia)\neu.west.1(ireland)\n\nThese regions have most types/most instances available\nTypically can go uninterrupted for weeks\nless price fluctuation = more certainty\n\n\nSmallest pools\n\neu.central.1 (frankfort)\nap.south.1 (mumbai)\nap.southeast.1 (singapore)\n\ntypically get interrupted within days\n\n\n\nRun groups of instances that come from multiple spot pools\n\nTo used different compute types, jobs/tasks need to be in containers\nspot pools are instances with same region, type, OS, etc.\napplications running on instances from a least 5 different pools can cut interruptions by up to 80%\n\n\nManaging/preparing for interruptions\n\nOnly use for jobs that are short lived\n\ndevelopment and staging environments, short data processing, proof-of-concept, etc.\n\nBuild internal management system that automatically handles interruptions\n\nlook at spot pool historical prices for past 90 days\n\nlooking for least volatile pools\nolder generation (e.g.Â c-family, m-family) tend to be most stable\n\n\nUse 3rd party platform that manages spot instances and interruptions\n\nSpotinst - uses ML to choose and manage instances that optimizes price and provide continuous activity for apps that are without a single point of failure.\n\nUses on-demand as a fall-back.\nSLA guarantees 99.9% availability.\nSnapshots volumes to migrate data to new instances in case of interruption.\nworks with other services and platforms (kubernetes, codedeploy, etc.)\n\nSpot Fleet - aws service, automanages groups of spot instances according to either of the following strategies:\n\nstrategy options\n\nlowest price - lowest price instances\ndiversified - spread instances across pools\n\nAfter receiving 2 min warning,\n\ntake snapshots of AMI and any attached EBS volumes and use them to launch a new instance.\n\nsnapshot of AMI\n\non EC2 dashboard â€“ left panel â€“ instances â€“ instances\n\nright-click instance â€“ image â€“ create AMI\n\nimage is in left -panel â€“ Images â€“ AMIs\n\n\n\nActually both snapshots might be able to taken in left panel â€“ spot requests\n\nsee AWS note â€“ EC2 for further details\n\n\n\n\n\n\nneed to drain and detach instance from elastic load balancer if one is used\nIf using auto-scaling, need to create an on-demand group and a spot instance group\n\n\nKubernetes\n\nAfter receiving 2 minute interruption warning from AWS:\n\nDetach instance from elastic load balancer (ELB) is one is being used\nMark instance as unschedulable (?)\n\nprevents new pods (group of containers on an instance that performs a job) from being scheduled on that node\nunderlying compute capacity and scheduling of resources of the pods needs to be monitored. Compute capacity and pod resource requirements need to match.\n\n\n\n\n\nComparison\n\nMisc\n\nNotes from\n\nThe Top Clouds Evaluated Such That You Donâ€™t Need to Repeat Our Mistakes\nAWS vs GCP reliability is wildly different\n\nNo services for blockchain development, quantum computing, and graph databases in GCP (May 2022)\nhttps://cloud-gpus.com/ - tool for comparing gpu compute prices across vendors\n\nData centers\n\nCloser the resources are to your business, the less latency\n(May 2022) GCP has caught up and surpassed AWS in the number of data centers and regions that are available\n\nCompute\n\nCheapest vCPU\n\nGCP â€œe2-micro-preemptibleâ€ with 2 vCPU and 1 GB memory.\n\n48% lower than â€œt4g.nanoâ€ from AWS\n5 times lower than â€œA0â€ from Azure.\n\nAWS is in-between GCP and Azure in terms of price (i.e.Â Azure most expensive for cheap vCPUs)\n\nMore performant GCP instances usually cost approximately the same as their analogs from other cloud providers\n\nAzure servers cost the same or slightly less than AWS\n\nGCP: dedicated PostgreSQL server\n\nCheapest instances are 25% lower than the competitors\n\nGPU on-demand availability\n\nConclusion: Assuming you need on-demand boxes to succeed right when you need them, the consensus seems to clearly point to AWS. If you can stand to wait or be redundant to spawn failures, maybe Googleâ€™s hardware acceleration customizability can win the day.\nStats\n\nAWS consistently spawned a new GPU in under 15 seconds (average of 11.4s).\nGCP on the other hand took closer to 45 seconds (average of 42.6s).\nAWS encountered one valid launch error in these two weeks whereas GCP had 84\n\nCaveats\n\nGCP allows you to attach a GPU to an arbitrary VM as a hardware accelerator - you can separately configure quantity of the CPUs as needed.\nAWS only provisions defined VMs that have GPUs attached\n\n\n\nRecommendations\n\nAzure\n\nYou use the Microsoft Office stack (Word, Teams, OneDrive, SharePoint, etc.) and/or C# programming language.\nYou head neither for the cheapest servers nor for the most expensive ones â€” you need something in the middle.\nYou need a memory-optimized solution rather than a general-purpose or a compute-optimized one.\nYou read about the current bugs and inconsistencies in Azure, and it does not scare you.\n\nAWS\n\nYou are rich.\nYou have AWS experts in your team.\nYou build an enterprise-level long-term project.\nOR you just want to rent a cheap virtual machine, and you donâ€™t care about all the other facilities.\n\nGCP\n\nYou are a start-up company.\nYou canâ€™t invest much time in learning AWS and dealing with Azure bugs.\nYou donâ€™t need much flexibility and configuration facilities from the cloud.\nYou are ready to accept the approaches dictated by the platform.\nYou need either a general-purpose or a compute-optimized solution, but not a memory-optimized one."
  },
  {
    "objectID": "qmd/clustering-time-series.html#misc",
    "href": "qmd/clustering-time-series.html#misc",
    "title": "7Â  Time Series",
    "section": "7.1 Misc",
    "text": "7.1 Misc\n\nAKA Time Series Classification\nMethods\n\nEuclidean Distance Approaches\nDynamic Time Warping\nShapelet-Based\nKernel-Based\nFeature-Based"
  },
  {
    "objectID": "qmd/clustering-time-series.html#cluster-validity-indices-cvi",
    "href": "qmd/clustering-time-series.html#cluster-validity-indices-cvi",
    "title": "7Â  Time Series",
    "section": "7.2 Cluster Validity Indices (CVI)",
    "text": "7.2 Cluster Validity Indices (CVI)\n\nNotes from {dtwclust} vignette\nTypes\n\nFor either Hard (aka Crisp)Â  or Soft (aka Fuzzy) Partitioning\nCharacteristics\n\nInternal - try to define a measure of cluster purity\nExternal - compare the obtained partition to the correct one. Thus, external CVIs can only be used if the ground truth is known\nRelative - ?\n\nScores\n\n\nAvailable through dtwclust::cvi( )\nGlobal centroid is one thatâ€™s computed using the whole dataset\n\ndtwclust implements whichever distance method that originally used in the clustering computation\n\nSome CVIs require symmetric distance functions (distance from a to b = distance b to a)\n\nA warning is printed if an asymmetric distance method was used\n\n{clue} - compare repetitions of non-deterministic clustering methods (e.g.Â partitional) where random element means you get a different result each time\n\nIt uses a measure called â€œdissimilarities using minimal Euclidean membership distanceâ€ to compare different runs of a cluster method"
  },
  {
    "objectID": "qmd/clustering-time-series.html#dtwclust",
    "href": "qmd/clustering-time-series.html#dtwclust",
    "title": "7Â  Time Series",
    "section": "7.3 {dtwclust}",
    "text": "7.3 {dtwclust}\n\nSee vignette appendices from examples\nTakes named matrices, numeric vectors, named arrays, and maybe lists as inputs\nneed to pick a:\n\nclustering method\n\nPartitional\nHierarchical\nTADPole\nk-Shape\nFuzzy\n\ndistance algorithm\n\nDynamic Time Warping (DTW\nsoft-DTW\nSlope Based Distance (SBD)\nTriangular Global Alignment KernelÂ  (TGAK)\n\nprototype method\n\nPartition Around Medoids (PAM)\nDTW Barycenter Averaging (DBA)\nShape Extraction\nFuzzy-Based\n\n\nNot all methods, distances, prototypes are interchangeable with each other\nNot all methods, distances, prototypes handle multivariate series or unequal length series\n\nMultivariate series are mentioned throughout â€“ I think this means comparing 2 different dataframes of time series but not sure (maybe examples in appendices)\n\nThe documentation for each function specifies if they use parallelization and how, but in general,\n\nall distances use multi-threading (RcppParallel) (see TADPole method for more details on options)\nmulti-processing (foreach) is leveraged during clustering but no further parallelization on each node\n\nWorkflow\n\ncompare_clusterings( )\nProvides a way to efficiently grid search different settings for a clustering methodÂ  or multiple methods (I think)\n\nClustering Methods\n\nIn addition to whatâ€™s described below, validity measures in the next section can also be used to choose the best number of clusters\nHierarchical\n\nhierarchical_control( ) through tsclust( type = â€œhierarchicalâ€)\nMemory size needs to be a consideration. Depending on resources, might be limited to smaller datasets\ncreates a hierarchy of groups in which, as the level in the hierarchy increases, clusters are created by merging the clusters from the next lower level, such that an ordered sequence of groupings is obtained.\na (dis)similarity measure (linkage) between groups should be specified, in addition to the one that is used to calculate pairwise similarities.\na specific number of clusters does not need to be specified for the hierarchy to be created, and the procedure is deterministic, so it will always give the same result for a chosen set of (dis)similarity measures\nIf data are easily grouped, the type of linkage used doesnâ€™t make much of difference\nDendrogram - a binary tree where the height of each node is proportional to the value of the inter-group dissimilarity between its two daughter nodes\n\nvisually evaluate the dendrogram in order to assess the height at which the largest change in dissimilarity occurs, consequently cutting the dendrogram at said height and extracting the clusters that are created\n\n\n\n3rd level or 4th level from the top is where the disparities in segment length between the daughters start to even out\n\n\n\n\nPartitional\n\npartitional_control( ) through tsclust(type = â€œpartitionalâ€ )\nNumber of clusters, k, needs to specified beforehand\n\nsometimes during iterations, clusters become empty, and instability created. Try lower value of k or different distance method\n\nStochastic due to their random start. Thus, it is common practice to test different random starts to evaluate several local optima and choose the best result out of all the repetitions.\nTends to produce spherical clusters, but has a lower complexity, so it can be applied to very large datasets\nsteps\n\nk centroids are randomly initialized, usually by choosing k objects from the dataset at random; these are assigned to individual clusters.\nThe distance between all objects in the data and all centroids is calculated, and each object is assigned to the cluster of its closest centroid.\nA prototyping function is applied to each cluster to update the corresponding centroid.\nDistances and centroids are updated iteratively until a certain number of iterations have elapsed, or no object changes clusters any more\n\n\nTADPole\n\nTADPole( ) or tadpole_control( ) through tsclust(type= â€œtadpoleâ€)\nRequires series of equal length (uses Sakoe-Chiba window)\nMemory size needs to be a consideration. Depending on resources, might be limited to smaller datasets\nUtilizes DTW lower bound method soÂ  lb_keogh or lb_improved needs to be specified\n\nin order to use euclidean distance for the upper bound, symmetric1 step pattern needs to specified\n\ndc arg:Â  cutoff distance(s). Can be a vector with several values\n\nanything below this distance is a neighbor\nno details on how to determine the value(s) to use, may need to look at original paper\n\nDeterministic like hierarchical\nUses series from the data as centroids (like PAM)\nUses parallelization with certain specifications and will utilize all available threads\n\nuse RcppParallel::setThreadOptions( ) to adjust unless using doParellel pkg\n\n\nk-Shape\n\npartitional_control( ) through tsclust(type = â€œpartitionalâ€ )\nstochastic see the different component setting descriptions for more details\nSettings\n\npartitional as the method\nSBD as the distance measure\nshape extraction as the centroid function\nz -normalization as the preprocessing step\n\n\nFuzzy\n\nfuzzy_control( ) through tsclust(type = â€œfuzzyâ€ )\nsoft clustering method (like Gaussian Mixture Models)\n\na series can belong to more than one cluster\n\na percentage of belongedness to each cluster\n\n\n\n\nDistances\n\nDynamic Time Warping distance (DTW)\n\ndtw_basic( ) , dtw2( )\nCompares 2 series by â€œwarpingâ€ the time axis to bring them as close as possible to each other and measuring the sum of the distances between the points\nsymmetric (i.e.Â dist from A to B equals the distance from B to A) only if:\n\neither symmetric1 or symmetric2 step patterns are used\nseries are equal length after any constraints are used\n\nalgorithm compares two series by calculating a local cost matrix (LCM) and traversing it to find the optimal warping path (minimal cost)\nlist of components:\n\nstep pattern that determines how the alg traverses the rows of the LCM to find the optimal path\nwindow range that limits the number lcm calculations for each point\n\n\n\n\n\n\nFigure shows alignment of two series, x and y.\n\nThe initial and final points of the series must match, but other points along the axis may be â€œwarpedâ€ in order to minimize the distance/cost.\nThe dashed blue lines are the warp curves and show how some points are mapped to each other.\n\nX is the query (or test) series\nYis the reference series\nSteps\n\nCalc LCM matrix for series X and Y\nSimultaneously move along each row of the LCM using a chosen step patternÂ  (see window constraint to get part of a visual of this process)\n\nThe minimum lcm for each pointÂ  along x-axis is found. The sequence of minimum lcms or minimum alignment is Ï†.\nCalc the cost, DTWp, using the lcms in the minimum alignment\n\n\n\nmÏ† is a per-step weighting coefficient (edge weight in patterns fig)\nMÏ† is the normalization constant\nk is a pairs of points (or position along the x-axis) in the minimum alignment\ndtw_basic( ) sets p = 2 (the dtw in the dtw pkg doesnâ€™t use p in this equation)\n\n\n\nChoose the alignment with the lowest cost, DTWp (i.e.Â sum of lcm distances for that alignment)\n\nComponents\n\nLocal Cost Matrix (LCM)\n\nComputed for each pair of series that are compared\nThe Lp norm (distance) between the query series and reference series\n\n\n\nxi and yj are elements of the test and reference time series\nv stands for â€œvariableâ€ which is for comparing multivariate series\n\ni.e.Â the Lp norm for each pair of points is summed over all variables in the multivariate series\n\np is the order of the norm used\n\ne.g.Â 1 is Manhattan distance; 2 is Euclidean\n** Choice of p only matters if multivariate series are being used **\n\n\n\nEach lcm(i , j) value fills a spot in the n x m matrix, LCM (where 1 &lt; i &lt; n and 1 &lt; j &lt; m)\n\nStep Patterns\n\nstep.pattern arg\nDetermines how algorithm moves across the rows of the LCM to create alignments (time axis warps)\nEach pattern is a set of rules and weights\n\nthe rules are used to create different alignments of the LCM (i.e warping of the time axis)\nthe edge weights, mÏ†, are used the DTW calculation\n\n\n\nPatterns in fig\n\nsymmetric1Â  symmetric2 asymmetricÂ  rabinerJuangStepPattern(4, â€œcâ€, TRUE) (i.e., Rabiner-Juangâ€™s type IV with slope weighting)\n\nOnly some of the patterns are normalizable (i.e.Â MÏ† is used in the DTW equation below) (normalize arg)\n\nNormalization may be important when\n\ncomparing alignments between time series of different lengths, to decide the best match (e.g., for classification)\nWhen performing partial matches\n\nFor dtw_basic( ), doc says only supported with symmetric2\nrabinerJuangStepPattern() with slope weighting types c and d are normalizable\nsymmetricP* (where * is a number) are all normalizable (not shown in fig)\n\ndtwclust pkg author says symmetric1 most commonly used. dtw pkgÂ  and dtw_basic( ) use symmetric2 by default.\n\n\nWindow Constraints\n\nLimits the region that the lcm calculation takes place.\n\nReduces computation time but makes sense that you donâ€™t want to compare points that are separated by to large a time interval\n\nSakoe-Chiba window creates a calculation region along the diagonal of the LCM\n\n\n1 set of lcm calculations occurs within the horizontal, rectangular block of the query series and the vertical, rectangular block of the reference series.\nSakoe-Chiba requires equal length series but a â€œslanted bandâ€ is equivalent and works for unequal length series.\n\nâ€œSlanted bandâ€ is whatâ€™s used by dtwclust when the window constraint is used.\n\n\nOptimal window size needs to be tuned\n\nW, the window size, is ~half the size of the actual region covered\n\nvalue should be greater than 1 if used with series of different length\n\n[(i, j - w), (i, j + w)] which has 2w + 1 elements\n\n\nLower Bounds (LB)\n\ndtw_lb( )\nseries are supposed to be row-wise in a matrix (or in a df but not sure how you make a row-wise df)\nSteps\n\nCalculates an initial estimate of a distance matrix between two sets of time series using lb_improved( )\n\ninvolves the â€œlower boundâ€ calculation; didnâ€™t get into it\n\nUses the estimate to calculate the corresponding true DTW distance between only the nearest neighbors (row-wise minima of dist.matrix) of each series in x found in y\nUpdates distance matrix with DTW values\nContinues iteratively until no changes in the nearest neighbors occur\n\nOnly if dataset is very large will this method will be faster than dtw_basic( ) in the calculation of DTW\nNot symmetric, no multivariate series\nRequires\n\nboth series to be equal length\nwindow constraint defined\nnorm defined\n\nValue of LB (tightness of envelope around series) affected by step pattern which is set in dtw_basic( ) and included via â€¦ in dtw_lb\n\nsize of envelopes in general: LB_KeoghpÂ  &lt;Â  LB_Improvedp &lt;Â  DTWp\n\n\n\nSoft DTW\n\nsdtw( )\nâ€œregularizes DTW by smoothing itâ€Â  Â¯\\_(ãƒ„)_/Â¯\n\nâ€œsmoothnessâ€ controlled by gamma arg\n\ndefault: 0.01\nwith lower values resulting in less smoothing\n\n\nUses a gradient to efficiently calculate cluster prototypes\nnot recommended for stand-alone distance calculations\n\nnegative values can happen\n\nsymmetric and handles series of different lengths and multivariate series\n\nShape-based distance (SBD)\n\nSBD( )\nused in k-Shape Clustering\n\nbased on the cross-correlation with coefficient normalization (NCCc) sequence between two series\n\nfast (uses FFT to calc), competitive with other distance algorithms, and supports series with different lengths\nsymmetric, no multivariate series\nIn preprocessing arg, set to z-normalization\n\n\nTriangular global alignment kernel distance\n\nGAK( )\nâ€œregularizes DTW by smoothing itâ€Â  Â¯\\_(ãƒ„)_/Â¯\nsymmetric when normalized (dist a to b = dist b to a)\nsupports multivariate series and series of different length (as long as one series isnâ€™t half the length of the other)\nslightly more computationally expensive than DTW\n\n\n\nÏƒ can defined by the user but if left as NULL, the function estimates it\nT is the triangular constraint and is similar to the window constraint in DTW but there no arg for it, so I guess itâ€™s taken care of\nno idea what i and j refer to\n\nwould have to look it up in the original paper or there is a separate website and package for it\n\n\nif normalize = TRUE, then a distance is returned, can be compared with the other distance measure, and used in clustering\n\nif FALSE, a similarity is returned\n\n\nPrototypes/Average Series/Centroids\n\nDefines a time-series that effectively summarizes the most important characteristics of all series in a given cluster\nPrototypes could be used for visualization aid or to perform time-series classification\nMethods\n\nPartition Around Medoids (PAM)\n\nA series in the data whose average distance to all other objects in the same cluster is minimal\nPossible to calculate the distance matrix once and use it for each iteration. Takes less compute time and is done by default\n\npartitional_control(pam.precompute = TRUE)\nMentions that if done this way then the matrix is allocated to memory. So can be an issue for large datasets and the option would need to switched off.\n\nWhen switched off, sparse matrices are used.\n\npartitional_control(pam.precompute = FALSE, pam.sparse=TRUE)\n\n\n\n\nDTW Barycenter Averaging (DBA)\n\nDBA( )\nWith series of different lengths, either symmetric1 or symmetric2 should be used\nExpensive unless used in conjunction with the DTW distance algorithm\nsteps\n\nFor each cluster, a series in the data is chosen at random to be the reference series (or centroid)\nDTW alignments are calcâ€™d wrt to the reference series\nFor each point on the reference series, the point values from the other series that map to that point are averaged.\nAll the calculated average points become the new reference series.\nProcess is iterated until convergence or number of iterations is reached\n\n\nShape Extraction\n\nshape_extraction( )\nrequires z- normalized in preprocessing arg\nIf lengths unequal, a reference series is chosen\nsteps\n\n2 series are aligned based maximizing a NCCc score\n\nif unequal lengths zeros are append to the shorter series or the longer series is truncated (required for next step\n\nAligned series are entered row-wise into a matrix and the Rayleigh Quotient (::shrugs::) is maximized to obtain the final prototype\n\n\nFuzzy-Based\n\nc-means\n\nrequires series of equal lengths\ncentroid is like a weighted average using the cluster â€œbelongednessâ€ weight\n\n\n\nÎ¼ (mu) is the i-th element of the c-th centroid\nx is the i-th element of the p-th series in the data\nu is â€œbelongednessâ€ proportion of the p-th series to the c-th cluster\nm is known as the fuzziness exponent and should be greater than 1, with a common default value of 2\n\n\n\nc-medoids (FCMdd)\n\nCentroid selection similar to PAM\nhandles series of unequal lengths (as long as distance method permits)\n\n\nÎ¼ (mu) is the c-th centroid\nxq is the q-th series in the data\nu is â€œbelongednessâ€ proportion of the p-th series to the c-th cluster\nd represents the distance between the p-th series of the data and the c-th centroid\nsee vignette for details on the optimization procedure"
  },
  {
    "objectID": "qmd/clustering-general.html#sec-clust-gen-misc",
    "href": "qmd/clustering-general.html#sec-clust-gen-misc",
    "title": "General",
    "section": "Misc",
    "text": "Misc\n\nAlso see\n\nNotebook, pgs 57-58\nDiagnostics, Clustering\n\nFor static data, i.e., if the values do not change with time, clustering methods are usually divided into five major categories:\n\nPartitioning (or Partitional)\nHierarchical\nDensity-Based\nGrid-Based\nModel-Based Methods"
  },
  {
    "objectID": "qmd/clustering-general.html#sec-clust-gen-terms",
    "href": "qmd/clustering-general.html#sec-clust-gen-terms",
    "title": "General",
    "section": "Terms",
    "text": "Terms\n\nCluster Centroid - The middle of a cluster. A centroid is a vector that contains one number for each variable, where each number is the mean of a variable for the observations in that cluster. The centroid can be thought of as the multi-dimensional average of the cluster.\nHard (or Crisp) Clustering - each point belongs to a single cluster\nSoft (or Fuzzy) Clustering - each point is allowed to belong to multiple clusters"
  },
  {
    "objectID": "qmd/clustering-general.html#sec-clust-gen-clustdesc",
    "href": "qmd/clustering-general.html#sec-clust-gen-clustdesc",
    "title": "General",
    "section": "Cluster Descriptions",
    "text": "Cluster Descriptions\n\nPackages\n\n{parameters} - provides various functions for describing, analyzing, and visualizing clusters for various methods\n{clustereval} - compute the statistical association between the features and the detected cluster labels and whether they are significant.\n\nCategorical: Chi-Square, Fisherâ€™s Exact, or Hypergeometric tests\nContinuous: Mann-Whitney-U test\n\n\nExamine variable values at the centroids of each cluster\n\nA higher absolute value indicates that a certain variable characteristic is more pronounced within that specific cluster (as compared to other cluster groups with lower absolute mean values).\n\nDistributional statistics for each cluster\n\nNumeric variables: mean and sd for each variable in that cluster\nCategorical variables:\n\nbinary: percent where event = 1\nmultinomial: most prominent category\n\n\nRun a decision tree on clusters\n\n\nEach color (orange, blue, green, purple) represents a cluster\nExplains how clusters were generated\n{treeheatr}\n\n\nRadar charts\n\n\n3 clusters: blue (highlighted), red, green\nGuessing the mean values for each variable are the points\n\nScatter\n\nUse clustering variables of interest for a scatter plot then label the points with cluster id"
  },
  {
    "objectID": "qmd/clustering-general.html#sec-clust-gen-gmm",
    "href": "qmd/clustering-general.html#sec-clust-gen-gmm",
    "title": "General",
    "section": "Gaussian Mixture Models (GMM)",
    "text": "Gaussian Mixture Models (GMM)\n\nNotes from Serrano video: https://www.youtube.com/watch?v=q71Niz856KE&ab_channel=LuisSerrano\nSoft clustering algorithm\nComponents of the Algorithm\n\nâ€œColorâ€ points according to gaussians (clusters)\n\nThe closer a point is to the center of a gaussian the more intensely it matches the color of that gaussian\nPoints in between gaussians are a mixture or proportion of the colors of each gaussian\n\n\nFitting a Gaussian\n\nFind the center of mass\n\n2-dim: calculate the mean of x and the mean of y and thatâ€™s the coordinates of your center of mass\n\nFind the spread of the points\n\n2-dim: calculate the x-variance, y-variance, and covariance\n\n\nFirst Equation: Height of Gaussian (Multivariate Gaussian distribution equation).\nSecond Equation: 1-D gaussian equation thatâ€™s just being used for reference\n\nPartially â€œcoloredâ€ points affect spread and center of mass calculations\n\nFully colored points â€œweighâ€ more than partially colored points and pull the center of mass and change the orientation\n\n\n\n\n\nSteps\n\nStart with random Gaussians\n\nEach gaussian has random means, variances\n\nColor points according to distance to the random gaussians\n\nThe heights in the distributions pic above\n\n(Forget about old gaussians) Calculate new gaussians based on the colored points\n(Forget about old colors) Color points according to distance to the new gaussians\nRepeat until some threshold is reached (i.e.Â gaussians or colors donâ€™t change much)\n\nTuning\n\nInitial Conditions (i.e.Â Good starting points for the random gaussians at the beginning)\nLimits on the mean and variance calculations\nNumber of gaussians, k, can be chosen by minimizing the Davies-Bouldin score\n\nSee Diagnostics, Clustering &gt;&gt; Spherical/Centroid Based &gt;&gt; Davies-Bouldin Index\n\nRunning algorithm multiple times\n\nLike CV grid search algs or bootstrapping"
  },
  {
    "objectID": "qmd/clustering-general.html#sec-clust-gen-lpa",
    "href": "qmd/clustering-general.html#sec-clust-gen-lpa",
    "title": "General",
    "section": "Latent Profile Analysis (LPA)",
    "text": "Latent Profile Analysis (LPA)\n\nSort of like k-means + GMM\nk number of profiles (i.e.Â clusters) are chosen\nModel outputs probabilities that an observation belongs to any particular cluster\nGOF metrics available\nâ€œAs with Exploratory Factor Analysis (EFA )(and other latent-variable models), the assumption of LPA is that the latent (unobserved) factorâ€causesâ€ (Iâ€™m using the term loosely here) observed scores on the indicator variables. So, to refer back to my initial hypothetical example, a monster being a spell caster (the unobserved class) causes it to have high intelligence, low strength, etc. rather than the inverse. This is a worthwhile distinction to keep in mind, since it has implications for how the model is fit.â€\nBin variables that might dominate the profile. This way the profiles will represent a latent variable and not gradations of the dominate variable (e.g.Â low, middle, high values of the dominate variable).\nCenter other variable observations according to dominant variable bin those observations are in. (e.g.Â subtract values in bin1 from bin1â€™s mean)\n# From D&D article where challenge_rating is a likely dominant variable\nmons_bin &lt;- mons_df %&gt;%\nÂ  mutate(cr_bin = ntile(x = challenge_rating, n = 6))\nab_scores &lt;- c(\"strength\", \"dexterity\", \"constitution\", \"intelligence\", \"wisdom\", \"charisma\")Â \nmons_bin &lt;- mons_bin %&gt;%\nÂ  group_by(cr_bin) %&gt;%\nÂ  mutate(across(.cols = ab_scores, .fns = mean, .names = \"{.col}_bin_mean\")) %&gt;%\nÂ  ungroup()"
  },
  {
    "objectID": "qmd/clustering-general.html#sec-clust-gen-tsne",
    "href": "qmd/clustering-general.html#sec-clust-gen-tsne",
    "title": "General",
    "section": "tSNE",
    "text": "tSNE\n\nPackages\n\n{Rtsne}\n\nt-Distributed Stochastic Neighbor Embedding\nLooks at the local distances between points in the original data space and tries to reproduce them in the low-dimensional representation\n\nBoth UMAP and tSNE attempt to do this but fails (Lior Pachter paper thread, Doesnâ€™t preserve local structure, No theorem says that it preserves topology)\n\nResults depend on a random starting point\nTuning parameters: perplexity"
  },
  {
    "objectID": "qmd/clustering-general.html#sec-clust-gen-umap",
    "href": "qmd/clustering-general.html#sec-clust-gen-umap",
    "title": "General",
    "section": "UMAP",
    "text": "UMAP\n\nPackages:\n\n{umap}\n\nUniform Manifold Approximation and Projection\nSee tSNE section for Lior Pachter threads on why not to use tSNE or UMAP\nPreprocessing\n\nOnly for numeric variables\nStandardize\n\nProjects variables to a nonlinear space\nVariation of tSNE\n\nRandom starting point has less of an impact\n\nCan be supervised (give it an outcome variable)\nComputationally intensive\nLow-dimensional embedding cannot be interpreted\n\nNo rotation matrix plot like in PCA\n\nTry pca - linear method (fast)\n\nIf successful (good separation between categories), then prediction may be easier\nIf not, umap, tsne needed\n\nUMAP can taking training model and apply it to test data or new data (tSNE canâ€™t)\nTuning parameter: neighbors\n\nExample used 500 iterations (n_epochs) as limit for convergence"
  },
  {
    "objectID": "qmd/clustering-general.html#sec-clust-gen-kmeans",
    "href": "qmd/clustering-general.html#sec-clust-gen-kmeans",
    "title": "General",
    "section": "K-Means",
    "text": "K-Means\n\nSeeks to assign n points to k clusters and find cluster centers so as to minimize the sum of squared distances from each point to its cluster center.\nFor choosing the number of clusters, elbow method (i.e.Â WSS) is usually awful if there are more than few clusters. Recommended: Calinski-Harabasz Index and BIC then Silhouette Coefficient or Davies-Bouldin Index (See Diagnostics, Clustering &gt;&gt; Spherical/Centroid Based (article)\nBase R kmeans uses the Hartigan-Wong algorithm\n\nFor large k and larger n, the density of cluster centers should be proportional to the density of the points to the power (d/d+2). In other words the distribution of clusters found by k-means should be more spread out than the distribution of points. This is not in general achieved by commonly used iterative schemes, which stay stuck close to the initial choice of centers.\n\n{tidyclust}\n\nEngines\n\nstats and ClusterR run classical K-means\nlaRÂ runs K-Modes models which are the categorical analog to K-means, meaning that it is intended to be used on only categorical data\nclustMixTypeÂ to run K-prototypes which are the more general method that works with categorical and numeric data at the same time.\n\nExample: Mixed K-Means\nlibrary(tidymodels)\nlibrary(tidyclust)\n\ndata(\"ames\", package = \"modeldata\")\n\nkproto_spec &lt;- k_means(num_clusters = 3) %&gt;%\n  set_engine(\"clustMixType\")\n\nkproto_fit &lt;- kproto_spec %&gt;%\n  fit(~ ., data = ames)\n\nkproto_fit %&gt;%\n  extract_centroids() %&gt;%\n  select(11:20) %&gt;%\n  glimpse()\n#&gt; Rows: 3\n#&gt; Columns: 10\n#&gt; $ Lot_Config     &lt;fct&gt; Inside, Inside, Inside\n#&gt; $ Land_Slope     &lt;fct&gt; Gtl, Gtl, Gtl\n#&gt; $ Neighborhood   &lt;fct&gt; College_Creek, North_Ames, Northridge_Heights\n#&gt; $ Condition_1    &lt;fct&gt; Norm, Norm, Norm\n#&gt; $ Condition_2    &lt;fct&gt; Norm, Norm, Norm\n#&gt; $ Bldg_Type      &lt;fct&gt; OneFam, OneFam, OneFam\n#&gt; $ House_Style    &lt;fct&gt; Two_Story, One_Story, One_Story\n#&gt; $ Overall_Cond   &lt;fct&gt; Average, Average, Average\n#&gt; $ Year_Built     &lt;dbl&gt; 1989.977, 1953.793, 1998.765\n#&gt; $ Year_Remod_Add &lt;dbl&gt; 1995.934, 1972.973, 2003.035"
  },
  {
    "objectID": "qmd/clustering-general.html#sec-clust-gen-ann",
    "href": "qmd/clustering-general.html#sec-clust-gen-ann",
    "title": "General",
    "section": "Approximate Nearest Neighbor (ANN)",
    "text": "Approximate Nearest Neighbor (ANN)\n\nkNN runs at O(N*K), where N is the number of items and K is the size of each embedding. Approximate nearest neighbor (ANN) algorithms typically drop the complexity of a lookup to O(log(n)).\nMisc\n\nAlso see Maximum inner product search using nearest neighbor search algorithms\n\nIt shows a preprocessing transformation that is performed before kNN to make it more efficient\nIt might already be implemented in ANN algorithms\n\n\nCommonly used in Recommendation algs to cluster user-item embeddings at the end. Also, any NLP task where you need to do a similarity search of one character embedding to other character embeddings.\nGenerally uses one of two main categories of hashing methods: either data-independent methods, such as locality-sensitive hashing (LSH); or data-dependent methods, such as Locality-preserving hashing (LPH)\nLocality-Sensitive Hashing (LSH)\n\nHashes similar input items into the same â€œbucketsâ€ with high probability.\nThe number of buckets is much smaller than the universe of possible input items\nHash collisions are maximized, not minimized, where a collision is where two distinct data points have the same hash.\n\nSpotifyâ€™s Annoy\n\nUses a type of LSH, Random Projections Method (RPM) (article didnâ€™t explain this well)\nL RPM hashing functions are chosen. Each data point, p, gets hashed into buckets in each of the L hashing tables. When a new data point, q, is â€œqueried,â€ it gets hash into buckets like p did. All the hashes in the same buckets of p are pulled and the hashes within a certain threshold, c*R, are considered nearest neighbors.\n\nWiki article on LSH and RPM clears it up a little, but Iâ€™d probably have to go to Spotifyâ€™s paper to totally make sense of this.\n\nAlso the Spotify alg might bring trees/forests into this somehow\n\nFacebook AI Similarity Search (FAISS)\n\nHierarchical Navigable Small World Graphs (HNSW)\nHNSW has a polylogarithmic time complexity (O(logN))\nTwo approximations available Embeddings are clustered and centroids are calculated. The k nearest centroids are returned.\n\nEmbeddings are clustered into veroni cells. The k nearest embeddings in a veroni cell or a region of veroni cells is returned.\n\nBoth types of approximations have tuning parameters.\n\nInverted File Index + Product Quantization (IVFPQ)(article)"
  },
  {
    "objectID": "qmd/clustering-general.html#sec-clust-gen-dbscan",
    "href": "qmd/clustering-general.html#sec-clust-gen-dbscan",
    "title": "General",
    "section": "DBSCAN",
    "text": "DBSCAN\n\n\nMisc\n\nNotes from:\n\nUnderstanding DBSCAN and Implementation with Python\nClustering with DBSCAN, Clearly Explained video\n\nPackages\n\n{dbscan}\n{parameters}\n\nn_clusters_dbscan - Given a â€œmin_sizeâ€ (aka minPts?), the function estimates the optimal â€œepsâ€\ncluster_analysis - Shows Sum of Squares metrics and the (standardized) mean value for each variable within each cluster.\n\n\nHDBSCAN is the hierarchical density-based clustering algorithm\n\nTuning\n\neps - The maximum distance between two samples for one to be considered to be connected to the other\n\nLarge eps tend to include more points within a cluster,\nToo-large eps will include everything in the same single cluster\nToo-small eps will result in no clustering at all\n\nminPts (or min_samples) - The minimum number of samples in a neighborhood for a point to be considered as a core point\n\nToo-small minPts is not meaningful because it will regard every point as a core point.\nLarger minPts can be better to deal with noisy data\n\n\nAlgorithm\n\nFor each data point, find the points in the neighborhood within eps distance, and define the core points as those with at least minPts neighbors.\n\n\nThe orange circle represents the eps area\nIf minPts = 4, then the top 4 points are core points because they have at least 4 points overlapping the eps area\n\nDefine groups of connected core points as clusters.\n\n\nAll the green points have been labelled as core points\n\nAssign each non-core point to a nearby cluster if itâ€™s directly reachable from a neighboring core point, otherwise define it as an outlier.\n\n\nThe black points are non-core points but are points that overlap the eps area for the outer-most core points.\nAdding these black points finalizes the first cluster\n\nThis process is repeated for the next group of core points and continues until all thatâ€™s left are outliers.\n\nAdvantages\n\nDoesnâ€™t require users to specify the number of clusters.\nNot sensitive to outliers.\nClusters formed by DBSCAN can be any shape, which makes it robust to different types of data.\n\nExample: Nested Cluster Structure\n\nK-Means\n\n\nK-Means wants spherical clusters which makes it grab groups of points it shouldnâ€™t\n\nDBSCAN\n\n\nAble correctly identify the oblong shaped cluster\n\n\n\n\nDisadvantages\n\nIf the data has a very large variation in densities across clusters because you can only use one pair of parameters, eps and MinPts, on one dataset\nIt could be hard to define eps without the domain knowledge of the data\nClusters not totally reproducible. Clusters are defined sequentially so depending on which group of core points the algorithm starts with and hyperparameter values, some non-core points that are within the eps area of multiple clusters may be assigned to different clusters on different runs of the algorithm."
  },
  {
    "objectID": "qmd/code-debug.html#sec-code-debug-misc",
    "href": "qmd/code-debug.html#sec-code-debug-misc",
    "title": "Debugging",
    "section": "Misc",
    "text": "Misc\n\nRubber Duck Method\n\nBeg, borrow, steal, buy, fabricate or otherwise obtain a rubber duck (bathtub variety).\n- Note: In a pinch a coworker might be able to substitute for the duck, however, it is often preferred to confide mistakes to the duck instead of your coworker.\nPlace rubber duck on desk and inform it you are just going to go over some code with it, if thatâ€™s all right.\nExplain to the duck what your code is supposed to do, and then go into detail and explain your code line by line.\nAt some point you will tell the duck what you are doing next and then realise that that is not in fact what you are actually doing. The duck will sit there serenely, happy in the knowledge that it has helped you on your way."
  },
  {
    "objectID": "qmd/code-optimization.html#sec-code-opt-misc",
    "href": "qmd/code-optimization.html#sec-code-opt-misc",
    "title": "Optimization",
    "section": "Misc",
    "text": "Misc\n\nA while loop is faster than a recursive function\nungroup before performing calculations in mutate or summarize when that calculation doesnâ€™t need to be performed within-group (i.e.Â per factor level)\nString functions\n\nâ€œfixedâ€ searches, fixed = TRUE are fastest overall\n\nSearches involving fixed strings (e.g.Â â€œbananaâ€) that donâ€™t require regular expressions\n\nPCRE2, perl = TRUE , is fastest for regular expressions"
  },
  {
    "objectID": "qmd/code-optimization.html#sec-code-opt-bench",
    "href": "qmd/code-optimization.html#sec-code-opt-bench",
    "title": "Optimization",
    "section": "Benchmarking",
    "text": "Benchmarking\n\nMisc\n\nMastering Software Development in R, Ch. 2.71\n\n{bench}\n\nMisc\n\nAutomatically checks that each approach gives the same output, so that you donâ€™t mistakenly compare apples and oranges\n\nExample: Basic\nres &lt;-\nÂ  bench::mark(\nÂ  Â  approach_1 = Reduce(sum, numbers),\nÂ  Â  approach_2 = sum(unlist(numbers))\nÂ  )\n\nres %&gt;% select(expression, median)\n\n#&gt; # A tibble: 2 Ã— 2\n#&gt;Â  expressionÂ  median\n#&gt;Â  &lt;bch:expr&gt; &lt;bch:tm&gt;\n#&gt; 1 approach_1Â  2.25Âµs\n#&gt; 2 approach_2 491.97ns\n\n{microbenchmark}\n\nExample: Basic\n\nrecord_temp_perf &lt;- microbenchmark(find_records_1(example_data, 27),Â \nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  find_records_2(example_data, 27))\nrecord_temp_perf\n\n## Unit: microseconds\n##Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  exprÂ  Â  Â  minÂ  Â  Â  lqÂ  Â  Â  meanÂ  medianÂ  Â  Â  uq\n##Â  find_records_1(example_data, 27)Â  114.574Â  136.680Â  156.1812Â  146.132Â  163.676\n##Â  find_records_2(example_data, 27) 4717.407 5271.877 6113.5087 5867.701 6167.709\n##Â  Â  Â  Â  max neval\n##Â  Â  593.461Â  100\n##Â  11334.064Â  100\n\nlibrary(ggplot2)\nautoplot(record_temp_perf)\n\nDefault: 100 iterations\nTimes are given in a reasonable unit, based on the observed profiling times (units are given in microseconds in this case).\nOutput\n\nmin - min time\nlq - lower quartile\nmean, median\nuq - upper quartile\nmax - max time"
  },
  {
    "objectID": "qmd/code-optimization.html#sec-code-opt-prof",
    "href": "qmd/code-optimization.html#sec-code-opt-prof",
    "title": "Optimization",
    "section": "Profiling",
    "text": "Profiling\n\nMisc\n\nResources\n\nMastering Software Development in R, Ch. 2.72\nAdvanced R, Ch. 23\n\n\n{profvis}\n\nExample: Basic\n\nstep_1 &lt;- function() {\nÂ  pause(1)\n}\nstep_2 &lt;- function() {\nÂ  pause(2)\n}\nslow_function &lt;- function() {\nÂ  step_1()\n\nÂ  step_2()\n\nÂ  TRUE\n}\nresult &lt;- profvis(slow_function())\nresult\n\nBottom Row: outermost function\n2nd Row from the bottom are the functions in the next enviromental layer (e.g.Â â€œstep_1â€ and â€œstep_2â€)\n\nâ€œstep_2â€ takes about 2/3 of the total function execution time or twice the execution time of â€œstep_1â€\n\n3rd Row from the bottom (top row) are the functions in each of those other functions\n\nâ€œpauseâ€ in â€œstep_2â€ takes about 2/3 of the total function execution time or twice the execution time of â€œpauseâ€ in â€œstep_1â€"
  },
  {
    "objectID": "qmd/code-optimization.html#sec-code-opt-rftf",
    "href": "qmd/code-optimization.html#sec-code-opt-rftf",
    "title": "Optimization",
    "section": "Replacements for Tidyverse Functions",
    "text": "Replacements for Tidyverse Functions\n\nMisc\n\nNotes from: Writing performant code with tidy tools\n\nAlso provides links to more {vctrs} recipes from their tidymodels github pull requests\n\nFor code that relies on group_by()and sees heavy traffic, see vctrs::list_unchop(), vctrs::vec_chop(), and vctrs::vec_rep_each().\n\nselect\nbench::mark(\nÂ  dplyr = select(mtcars_tbl, hp),\nÂ  `[.tbl_df` = mtcars_tbl[\"hp\"]\n) %&gt;%\nÂ  select(expression, median)\n#&gt; # A tibble: 2 Ã— 2\n#&gt;Â  expressionÂ  median\n#&gt;Â  &lt;bch:expr&gt; &lt;bch:tm&gt;\n#&gt; 1 dplyrÂ  Â  Â  527.01Âµs\n#&gt; 2 [.tbl_dfÂ  Â  8.08Âµs\n\nWinner: base R subsetting\n\nfilter\nWres &lt;-\nÂ  bench::mark(\nÂ  Â  dplyr = filter(mtcars_tbl, hp &gt; 100),\nÂ  Â  vctrs = vec_slice(mtcars_tbl, mtcars_tbl$hp &gt; 100),\nÂ  Â  `[.tbl_df` = mtcars_tbl[mtcars_tbl$hp &gt; 100, ]\nÂ  ) %&gt;%\nÂ  Â  select(expression, median)\nres\n#&gt; # A tibble: 3 Ã— 2\n#&gt;Â  expressionÂ  median\n#&gt;Â  &lt;bch:expr&gt; &lt;bch:tm&gt;\n#&gt; 1 dplyrÂ  Â  Â  289.93Âµs\n#&gt; 2 vctrsÂ  Â  Â  Â  4.63Âµs\n#&gt; 3 [.tbl_dfÂ  Â  23.74Âµs\n\nWinner: vctrs::vec_sliceÂ \n\nmutate\nbench::mark(\nÂ  dplyr = mutate(mtcars_tbl, year = 1974L),\nÂ  `$&lt;-.tbl_df` = {mtcars_tbl$year &lt;- 1974L; mtcars_tbl}\n) %&gt;%\nÂ  select(expression, median)\n\n#&gt; # A tibble: 2 Ã— 2\n#&gt;Â  expressionÂ  median\n#&gt;Â  &lt;bch:expr&gt; &lt;bch:tm&gt;\n#&gt; 1 dplyrÂ  Â  Â  302.5Âµs\n#&gt; 2 $&lt;-.tbl_dfÂ  12.8Âµs\n\nWinner: base R assignment\n\nmutate and relocate\nbench::mark(\nÂ  mutate = mutate(mtcars_tbl, year = 1974L, .after = make_model),\nÂ  relocate = relocate(mtcars_tbl, year, .after = make_model),\nÂ  `[.tbl_df` =Â \nÂ  Â  Â  mtcars_tbl[\nÂ  Â  Â  Â  c(left_cols,Â \nÂ  Â  Â  Â  Â  colnames(mtcars_tbl[!colnames(mtcars_tbl) %in% left_cols])\nÂ  Â  Â  Â  )\nÂ  Â  Â  ],\nÂ  check = FALSE\n) %&gt;%Â \nÂ  select(expression, median)\n\n#&gt; # A tibble: 3 Ã— 2\n#&gt;Â  expressionÂ  median\n#&gt;Â  &lt;bch:expr&gt; &lt;bch:tm&gt;\n#&gt; 1 mutateÂ  Â  Â  Â  1.2ms\n#&gt; 2 relocateÂ  Â  804.3Âµs\n#&gt; 3 [.tbl_dfÂ  Â  19.1Âµs\n\nWinner: base R\n\npull\nbench::mark(\nÂ  dplyr = pull(mtcars_tbl, hp),\nÂ  `$.tbl_df` = mtcars_tbl$hp,\nÂ  `[[.tbl_df` = mtcars_tbl[[\"hp\"]]\n) %&gt;%\nÂ  select(expression, median)\n\n#&gt; # A tibble: 3 Ã— 2\n#&gt;Â  expressionÂ  median\n#&gt;Â  &lt;bch:expr&gt; &lt;bch:tm&gt;\n#&gt; 1 dplyrÂ  Â  Â  101.19Âµs\n#&gt; 2 $.tbl_dfÂ  615.02ns\n#&gt; 3 [[.tbl_dfÂ  Â  2.25Âµs\n\nWinner: base R bracket subsetting\n\nbind_*\nbench::mark(\nÂ  dplyr = bind_rows(mtcars_tbl, mtcars_tbl),\nÂ  vctrs = vec_rbind(mtcars_tbl, mtcars_tbl)\n) %&gt;%\nÂ  select(expression, median)\n\n#&gt; # A tibble: 2 Ã— 2\n#&gt;Â  expressionÂ  median\n#&gt;Â  &lt;bch:expr&gt; &lt;bch:tm&gt;\n#&gt; 1 dplyrÂ  Â  Â  Â  Â  44Âµs\n#&gt; 2 vctrsÂ  Â  Â  Â  14.3Âµs\n\nbench::mark(\nÂ  dplyr = bind_cols(mtcars_tbl, tbl),\nÂ  vctrs = vec_cbind(mtcars_tbl, tbl)\n) %&gt;%\nÂ  select(expression, median)\n#&gt; # A tibble: 2 Ã— 2\n#&gt;Â  expressionÂ  median\n#&gt;Â  &lt;bch:expr&gt; &lt;bch:tm&gt;\n#&gt; 1 dplyrÂ  Â  Â  Â  60.7Âµs\n#&gt; 2 vctrsÂ  Â  Â  Â  26.2Âµs\n\nWinners: vctrs::vec_cbind and vctrs::vec_rbind\n\nCreate Tibble\nbench::mark(\nÂ  tibble = tibble(a = 1:2, b = 3:4),\nÂ  new_tibble_df_list = new_tibble(df_list(a = 1:2, b = 3:4), nrow = 2),\nÂ  new_tibble_list = new_tibble(list(a = 1:2, b = 3:4), nrow = 2)\n) %&gt;%Â \nÂ  select(expression, median)\n#&gt; # A tibble: 3 Ã— 2\n#&gt;Â  expressionÂ  Â  Â  Â  Â  median\n#&gt;Â  &lt;bch:expr&gt;Â  Â  Â  Â  &lt;bch:tm&gt;\n#&gt; 1 tibbleÂ  Â  Â  Â  Â  Â  165.97Âµs\n#&gt; 2 new_tibble_df_listÂ  16.69Âµs\n#&gt; 3 new_tibble_listÂ  Â  Â  4.96Âµs\n\nWinner: new_tibble_list\n\nJoins\n\nQuestions:\n\nIf this join happens multiple times, is it possible to express it as one join and then subset it when needed?\n\ni.e.Â if a join happens inside of a loop but the elements of the join are not indices of the loop, itâ€™s likely possible to pull that join outside of the loop and then vctrs::vec_slice() its results inside of the loop. Am I using the complete outputted join result or just a portion? If I end up only making use of column names, or values in one column (as with joins approximating lookup tables), or pairings between two columns, I may be able to instead use $.tbl_df or [.tbl_df (see above, Pull).\n\n\nFor problems even a little bit more complex, e.g.Â if there were possibly multiple matching or if I wanted to keep all rows, then expressing this join with more bare-bones operations quickly becomes less readable and more error-prone. In those cases, too, joins in dplyr have a relatively small amount of overhead when compared to the vctrs backends underlying them. So, optimize carefully.\nExample: inner_join vs vctrs::vec_slice (Note: *only 0 or 1 match possible*)\nsupplement_my_cars &lt;- function() {\nÂ  # locate matches, assuming only 0 or 1 matches possible\nÂ  loc &lt;- vec_match(my_cars$make_model, mtcars_tbl$make_model)\n\nÂ  # keep only the matches\nÂ  loc_mine &lt;- which(!is.na(loc))\nÂ  loc_mtcars &lt;- vec_slice(loc, !is.na(loc))\n\nÂ  # drop duplicated join column\nÂ  my_cars_join &lt;- my_cars[setdiff(names(my_cars), \"make_model\")]\nÂ  vec_cbind(\nÂ  Â  vec_slice(mtcars_tbl, loc_mtcars),\nÂ  Â  vec_slice(my_cars_join, loc_mine)\nÂ  )\n}\nsupplement_my_cars()\n#&gt; # A tibble: 1 Ã— 13\n#&gt;Â  make_modelÂ  Â  mpgÂ  cylÂ  dispÂ  Â  hpÂ  dratÂ  Â  wtÂ  qsecÂ  Â  vsÂ  Â  amÂ  gearÂ  carb\n#&gt;Â  &lt;chr&gt;Â  Â  Â  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 Honda CivicÂ  30.4Â  Â  4Â  75.7Â  Â  52Â  4.93Â  1.62Â  18.5Â  Â  1Â  Â  1Â  Â  4Â  Â  2\n#&gt; # â„¹ 1 more variable: color &lt;chr&gt;\n\nbench::mark(\nÂ  inner_join = inner_join(mtcars_tbl, my_cars, \"make_model\"),\nÂ  manual = supplement_my_cars()\n) %&gt;%\nÂ  select(expression, median)\n#&gt; # A tibble: 2 Ã— 2\n#&gt;Â  expressionÂ  median\n#&gt;Â  &lt;bch:expr&gt; &lt;bch:tm&gt;\n#&gt; 1 inner_joinÂ  Â  438Âµs\n#&gt; 2 manualÂ  Â  Â  50.7Âµs\n\nnest\nbench::mark(\nÂ  nest = nest(mtcars_tbl, .by = c(cyl, am)),\nÂ  vctrs = {\nÂ  Â  res &lt;-Â \nÂ  Â  Â  vec_split(\nÂ  Â  Â  Â  x = mtcars_tbl[setdiff(colnames(mtcars_tbl), nest_cols)],\nÂ  Â  Â  Â  by = mtcars_tbl[nest_cols]\nÂ  Â  Â  )\n\nÂ  Â  vec_cbind(res$key, new_tibble(list(data = res$val)))\nÂ  }\n) %&gt;%\nÂ  select(expression, median)\n\n#&gt; # A tibble: 2 Ã— 2\n#&gt;Â  expressionÂ  median\n#&gt;Â  &lt;bch:expr&gt; &lt;bch:tm&gt;\n#&gt; 1 nestÂ  Â  Â  Â  1.81ms\n#&gt; 2 vctrsÂ  Â  Â  67.61Âµs\n\n# Results of nesting\n#&gt; # A tibble: 6 Ã— 3\n#&gt;Â  Â  cylÂ  Â  am dataÂ  Â  Â  Â  Â  Â  Â \n#&gt;Â  &lt;dbl&gt; &lt;dbl&gt; &lt;list&gt;Â  Â  Â  Â  Â  Â \n#&gt; 1Â  Â  6Â  Â  1 &lt;tibble [3 Ã— 10]&gt;Â \n#&gt; 2Â  Â  4Â  Â  1 &lt;tibble [8 Ã— 10]&gt;Â \n#&gt; 3Â  Â  6Â  Â  0 &lt;tibble [4 Ã— 10]&gt;Â \n#&gt; 4Â  Â  8Â  Â  0 &lt;tibble [12 Ã— 10]&gt;\n#&gt; 5Â  Â  4Â  Â  0 &lt;tibble [3 Ã— 10]&gt;Â \n#&gt; 6Â  Â  8Â  Â  1 &lt;tibble [2 Ã— 10]&gt;\nglue and paste0\nvec_paste0 &lt;- function (...) {\nÂ  args &lt;- vec_recycle_common(...)\nÂ  exec(paste0, !!!args)\n}\n\nname &lt;- \"Simon\"\nbench::mark(\nÂ  glue = glue::glue(\"My name is [{name}]{style='color: #990000'}.\"),\nÂ  vec_paste0 = vec_paste0(\"My name is \", name, \".\"),\nÂ  paste0 = paste0(\"My name is \", name, \".\"),\nÂ  check = FALSE\n) %&gt;%Â \nÂ  select(expression, median)\n\n#&gt; # A tibble: 3 Ã— 2\n#&gt;Â  expressionÂ  median\n#&gt;Â  &lt;bch:expr&gt; &lt;bch:tm&gt;\n#&gt; 1 glueÂ  Â  Â  Â  38.99Âµs\n#&gt; 2 vec_paste0Â  3.98Âµs\n#&gt; 3 paste0Â  Â  861.01ns\n\npaste0() has some tricky recycling behavior. vec_paste0 is a middle ground in terms of both performance and safety.\nUse glue() for errors, when the function will stop executing anyway.\nFor simple pastes that are intended to be called repeatedly, use vec_paste0()."
  },
  {
    "objectID": "qmd/code-snippets.html#sec-code-snippits-misc",
    "href": "qmd/code-snippets.html#sec-code-snippits-misc",
    "title": "Snippets",
    "section": "Misc",
    "text": "Misc\n\nCheck whether an environment variable is empty\nnzchar(Sys.getenv(\"blopblopblop\"))\n#&gt; [1] FALSE\nwithr::with_envvar(\n  new = c(\"blopblopblop\" = \"bla\"),\n  nzchar(Sys.getenv(\"blopblopblop\"))\n)\nUse a package for a single instance using {withr::with_package}\n\n\nUsing library() will keep the package loaded during the whole session, with_package() just runs the code snippet with that package temporarily loaded. This can be useful to avoid namespace collisions for example\n\nRead .csv from a zipped file\n# long way\ntmpf &lt;- tempfile()\ntmpd &lt;- tempfile()\ndownload.file('https://website.org/path/to/file.zip', tmpf)\nunzip(tmpf, exdir = tmpd)\ny &lt;- data.table::fread(file.path(tmpd,\n                       grep('csv$',\n                            unzip(tmpf, list = TRUE)$Name,\n                            value = TRUE)))\nunlink(tmpf)\nunlink(tmpd)\n\n# quick way\ny &lt;- data.table::fread('curl https://website.org/path/to/file.zip | funzip')\nLoad all R scripts from a directory: for (file in list.files(\"R\", full.names = TRUE)) source(file)\nView dataframe in View as html table using {kableExtra}\ndf_html &lt;- kableExtra::kbl(rbind(head(df, 5), tail(df, 5)), format = \"html\")\nprint(df_html)"
  },
  {
    "objectID": "qmd/code-snippets.html#sec-code-snippits-cleaning",
    "href": "qmd/code-snippets.html#sec-code-snippits-cleaning",
    "title": "Snippets",
    "section": "Cleaning",
    "text": "Cleaning\n\nRemove all objects except: rm(list=setdiff(ls(), c(\"train\", \"validate\", \"test\")))\nRemove NAs\n\ndataframes\ndf %&gt;% na.omit\ndf %&gt;% filter(complete.cases(.))\ndf %&gt;% tidyr::drop_na()\nvariables\ndf %&gt;% filter(!is.na(x1))\ndf %&gt;% tidyr::drop_na(x1)\n\nFind duplicate rows\n\n{datawizard} - Extract all duplicates, for visual inspection. Note that it also contains the first occurrence of future duplicates, unlike duplicated or dplyr::distinct. Also contains an additional column reporting the number of missing values for that row, to help in the decision-making when selecting which duplicates to keep.\ndf1 &lt;- data.frame(\n  id = c(1, 2, 3, 1, 3),\n  year = c(2022, 2022, 2022, 2022, 2000),\n  item1 = c(NA, 1, 1, 2, 3),\n  item2 = c(NA, 1, 1, 2, 3),\n  item3 = c(NA, 1, 1, 2, 3)\n)\n\ndata_duplicated(df1, select = \"id\")\n#&gt;   Row id year item1 item2 item3 count_na\n#&gt; 1   1  1 2022    NA    NA    NA        3\n#&gt; 4   4  1 2022     2     2     2        0\n#&gt; 3   3  3 2022     1     1     1        0\n#&gt; 5   5  3 2000     3     3     3        0\n\ndata_duplicated(df1, select = c(\"id\", \"year\"))\n#&gt; 1   1  1 2022    NA    NA    NA        3\n#&gt; 4   4  1 2022     2     2     2        0\ndplyr\ndups &lt;- dat %&gt;%Â \nÂ  group_by(BookingNumber, BookingDate, Charge) %&gt;%Â \nÂ  filter(n() &gt; 1)\nbase r\ndf[duplicated(df[\"ID\"], fromLast = F) | duplicated(df[\"ID\"], fromLast = T), ]\n\n##Â  Â  Â  Â  ID value_1 value_2 value_1_2\n## 2Â  ID-003Â  Â  Â  6Â  Â  Â  5Â  Â  Â  6 5\n## 3Â  ID-006Â  Â  Â  1Â  Â  Â  3Â  Â  Â  1 3\n## 4Â  ID-003Â  Â  Â  1Â  Â  Â  4Â  Â  Â  1 4\n## 5Â  ID-005Â  Â  Â  5Â  Â  Â  5Â  Â  Â  5 5\n## 6Â  ID-003Â  Â  Â  2Â  Â  Â  3Â  Â  Â  2 3\n## 7Â  ID-005Â  Â  Â  2Â  Â  Â  2Â  Â  Â  2 2\n## 9Â  ID-006Â  Â  Â  7Â  Â  Â  2Â  Â  Â  7 2\n## 10 ID-006Â  Â  Â  2Â  Â  Â  3Â  Â  Â  2 3\n\ndf[duplicated(df[\"ID\"], fromLast = F) doesnâ€™t include the first occurence, so also counting from the opposite direction will include all occurences of the duplicated rows\n\n\nRemove duplicated rows\n\n{datawizard} - From all rows with at least one duplicated ID, keep only one. Methods for selecting the duplicated row are either the first duplicate, the last duplicate, or the â€œbestâ€ duplicate (default), based on the duplicate with the smallest number of NA. In case of ties, it picks the first duplicate, as it is the one most likely to be valid and authentic, given practice effects.\ndf1 &lt;- data.frame(\n  id = c(1, 2, 3, 1, 3),\n  item1 = c(NA, 1, 1, 2, 3),\n  item2 = c(NA, 1, 1, 2, 3),\n  item3 = c(NA, 1, 1, 2, 3)\n)\n\ndata_unique(df1, select = \"id\")\n#&gt; (2 duplicates removed, with method 'best')\n#&gt;   id item1 item2 item3\n#&gt; 1  1     2     2     2\n#&gt; 2  2     1     1     1\n#&gt; 3  3     1     1     1\nbase R\ndf[!duplicated(df[c(\"col1\")]), ]\ndplyr\ndistinct(df, col1, .keep_all = TRUE)\n\nShowing all combinations present in the data and creating all possible combinations\n\nFuzzy Join (alt to case_when)\nref.df &lt;- data.frame(\nÂ  Â  Â  Â  Â  Â  bucket = c(â€œHighâ€, â€œMedium-Highâ€, â€œMedium-Lowâ€, â€œLowâ€),\nÂ  Â  Â  Â  Â  Â  value.high = c(max(USArrests$Assault), 249, 199, 149),\nÂ  Â  Â  Â  Â  Â  value.low = c(250, 200, 150, min(USArrests$Assault)))\nUSArrests %&gt;%Â \nÂ  fuzzy_join(ref.df,Â \nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  by = c(\"Assault\"=\"value.low\",\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  \"Assault\" = 'value.high'),Â \nÂ  Â  Â  Â  Â  Â  match_fun = c(`&gt;=`,`&lt;=`)) %&gt;%Â \nÂ  select(-c(value.high, value.low))\n\nAlso does partial matches\n\n\n\n\nRemove elements of a list by name\npurrr::discard_at(my_list, \"a\")\nlistr::list_remove"
  },
  {
    "objectID": "qmd/code-snippets.html#sec-code-snippits-func",
    "href": "qmd/code-snippets.html#sec-code-snippits-func",
    "title": "Snippets",
    "section": "Functions",
    "text": "Functions\n\nggplot\nviz_monthly &lt;- function(df, y_var, threshhold = NULL) {\n\nÂ  ggplot(df) +\nÂ  Â  aes(\nÂ  Â  Â  x = .data[[\"day\"]],\nÂ  Â  Â  y = .data[[y_var]]\nÂ  Â  ) +\nÂ  Â  geom_line() +\nÂ  Â  geom_hline(yintercept = threshhold, color = \"red\", linetype = 2) +\nÂ  Â  scale_x_continuous(breaks = seq(1, 29, by = 7)) +\nÂ  Â  theme_minimal()\n}\n\naes is on the outside\n\nThis was a function for a shiny module\nItâ€™s peculier. Necessary for function or module?\n\n\nCreate formula from string\nanalysis_formula &lt;- 'Days_Attended ~ W + School'\nestimator_func &lt;-Â  function(data) lm(as.formula(analysis_formula), data = data)\nRecursive Function\n\nExample\n# Replace pkg text with html\nreplace_txt &lt;- function(dat, patterns) {\n  if (length(patterns) == 0) {\n    return(dat)\n  }\n\n  pattern_str &lt;- patterns[[1]]$pattern_str\n  repl_str &lt;- patterns[[1]]$repl_str\n  replaced_txt &lt;- dat |&gt;\n    str_replace_all(pattern = pattern_str, repl_str)\n\n  new_patterns &lt;- patterns[-1]\n  replace_txt(replaced_txt, new_patterns)\n}\n\nArguments include the dataset and the iterable\nTests whether function has iterated through pattern list\nRemoves 1st element of the list\nreplace_text calls itself within the function with the new list and new dataset\n\nExample: Using Recall and tryCatch\nload_page_completely &lt;- function(rd) {\n  # load more content even if it throws an error\n  tryCatch({\n      # call load_more()\n      load_more(rd)\n      # if no error is thrown, call the load_page_completely() function again\n      Recall(rd)\n  }, error = function(e) {\n      # if an error is thrown return nothing / NULL\n  })\n}\n\nload_more is a user defined function\nRecall is a base R function that calls the same function itâ€™s in."
  },
  {
    "objectID": "qmd/code-snippets.html#sec-code-snippits-calcs",
    "href": "qmd/code-snippets.html#sec-code-snippits-calcs",
    "title": "Snippets",
    "section": "Calculations",
    "text": "Calculations\n\nCompute the running maximum per group\n(df &lt;- structure(list(var = c(5L, 2L, 3L, 4L, 0L, 3L, 6L, 4L, 8L, 4L),\nÂ  Â  Â  Â  Â  Â  Â  group = structure(c(1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L),\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  .Label = c(\"a\", \"b\"), class = \"factor\"),\nÂ  Â  Â  Â  Â  Â  Â  time = c(1L, 2L, 3L, 4L, 5L, 1L, 2L, 3L, 4L, 5L)),\nÂ  Â  Â  Â  Â  .Names = c(\"var\", \"group\",\"time\"),\nÂ  Â  Â  Â  Â  class = \"data.frame\", row.names = c(NA, -10L)))\n\ndf[order(df$group, df$time),]\n#Â  Â  var group time\n# 1Â  Â  5Â  Â  aÂ  Â  1\n# 2Â  Â  2Â  Â  aÂ  Â  2\n# 3Â  Â  3Â  Â  aÂ  Â  3\n# 4Â  Â  4Â  Â  aÂ  Â  4\n# 5Â  Â  0Â  Â  aÂ  Â  5\n# 6Â  Â  3Â  Â  bÂ  Â  1\n# 7Â  Â  6Â  Â  bÂ  Â  2\n# 8Â  Â  4Â  Â  bÂ  Â  3\n# 9Â  Â  8Â  Â  bÂ  Â  4\n# 10Â  4Â  Â  bÂ  Â  5\n\ndf$curMax &lt;- ave(df$var, df$group, FUN=cummax)\ndf\nvarÂ  |Â  groupÂ  |Â  timeÂ  |Â  curMax\n5Â  Â  Â  aÂ  Â  Â  Â  1Â  Â  Â  Â  5\n2Â  Â  Â  aÂ  Â  Â  Â  2Â  Â  Â  Â  5\n3Â  Â  Â  aÂ  Â  Â  Â  3Â  Â  Â  Â  5\n4Â  Â  Â  aÂ  Â  Â  Â  4Â  Â  Â  Â  5\n0Â  Â  Â  aÂ  Â  Â  Â  5Â  Â  Â  Â  5\n3Â  Â  Â  bÂ  Â  Â  Â  1Â  Â  Â  Â  3\n6Â  Â  Â  bÂ  Â  Â  Â  2Â  Â  Â  Â  6\n4Â  Â  Â  bÂ  Â  Â  Â  3Â  Â  Â  Â  6\n8Â  Â  Â  bÂ  Â  Â  Â  4Â  Â  Â  Â  8\n4Â  Â  Â  bÂ  Â  Â  Â  5Â  Â  Â  Â  8\nIntervals using {lubridate}\n\nLubridateâ€™s interval functions\nNotes from: Wrangling interval data using lubridate\nData\n(house_df &lt;- tibble(\n  person_id  = factor(c(\"A10232\", \"A10232\", \"A10232\", \"A39211\", \"A39211\", \"A28183\", \"A28183\", \"A10124\")),\n  house_id   = factor(c(\"H1200E\", \"H1243D\", \"H3432B\", \"HA7382\", \"H53621\", \"HC39EF\", \"HA3A01\", \"H222BA\")),\n  start_date = ymd(c(\"20200101\", \"20200112\", \"20211120\", \"19800101\", \"19900101\", \"20170303\", \"20190202\", \"19931023\")),\n  end_date   = ymd(c(\"20200112\", \"20211120\", \"20230720\", \"19891231\", \"20170102\", \"20180720\", \"20230720\", \"20230720\"))\n))\n\n#&gt;   A tibble: 8 Ã— 4\n#&gt;   person_id house_id start_date end_date  \n#&gt;   &lt;fct&gt;     &lt;fct&gt;    &lt;date&gt;     &lt;date&gt;    \n#&gt; 1 A10232    H1200E   2020-01-01 2020-01-12\n#&gt; 2 A10232    H1243D   2020-01-12 2021-11-20\n#&gt; 3 A10232    H3432B   2021-11-20 2023-07-20\n#&gt; 4 A39211    HA7382   1980-01-01 1989-12-31\n#&gt; 5 A39211    H53621   1990-01-01 2017-01-02\n#&gt; 6 A28183    HC39EF   2017-03-03 2018-07-20\n#&gt; 7 A28183    HA3A01   2019-02-02 2023-07-20\n#&gt; 8 A10124    H222BA   1993-10-23 2023-07-20\nCreate interval column\nhouse_df &lt;- \n  house_df |&gt; \n  mutate(\n    # create the interval\n    int = interval(start_date, end_date), \n    # drop the start/end columns\n    .keep = \"unused\"                      \n  )\n\nhouse_df\n#&gt;   A tibble: 8 Ã— 3\n#&gt;   person_id house_id int                           \n#&gt;   &lt;fct&gt;     &lt;fct&gt;    &lt;Interval&gt;                    \n#&gt; 1 A10232    H1200E   2020-01-01 UTC--2020-01-12 UTC\n#&gt; 2 A10232    H1243D   2020-01-12 UTC--2021-11-20 UTC\n#&gt; 3 A10232    H3432B   2021-11-20 UTC--2023-07-20 UTC\n#&gt; 4 A39211    HA7382   1980-01-01 UTC--1989-12-31 UTC\n#&gt; 5 A39211    H53621   1990-01-01 UTC--2017-01-02 UTC\n#&gt; 6 A28183    HC39EF   2017-03-03 UTC--2018-07-20 UTC\n#&gt; 7 A28183    HA3A01   2019-02-02 UTC--2023-07-20 UTC\n#&gt; 8 A10124    H222BA   1993-10-23 UTC--2023-07-20 UTC\nIntersection Function\n\nint_intersect &lt;- function(int, int_limits) {\n  int_start(int) &lt;- pmax(int_start(int), int_start(int_limits))\n  int_end(int)   &lt;- pmin(int_end(int), int_end(int_limits))\n  return(int)\n}\n\nThe red dashed line is the reference interval and the blue solid line is the interval of interest\nThe function creates an interval thats the intersection of both intervals (segment between black parentheses)\n\nProportion of the Reference Interval\n\nint_proportion &lt;- function(dat, reference_interval) {\n\n  # start with the housing data\n  dat |&gt; \n    # only retain overlapping rows, this makes the following\n    # operations more efficient by only computing what we need\n    filter(int_overlaps(int, reference_interval)) |&gt; \n    # then, actually compute the overlap of the intervals\n    mutate(\n      # use our earlier truncate function\n      int_sect = int_intersect(int, reference_interval),\n      # then, it's simple to compute the overlap proportion\n      prop = int_length(int_sect) / int_length(reference_interval)\n    ) |&gt; \n    # combine different intervals per person\n    summarize(prop_in_nl = sum(prop), .by = person_id)\n\n}\nExample\nint_2017  &lt;- interval(ymd(\"20170101\"), ymd(\"20171231\"))\nprop_2017 &lt;- \n  int_proportion(dat = house_df, \n                 reference_interval = int_2017)\n\nprop_2017\n\n#&gt; # A tibble: 3 Ã— 2\n#&gt;   person_id prop_in_nl\n#&gt;   &lt;fct&gt;          &lt;dbl&gt;\n#&gt; 1 A39211       0.00275\n#&gt; 2 A28183       0.832  \n#&gt; 3 A10124       1"
  },
  {
    "objectID": "qmd/code-style-guide.html#misc",
    "href": "qmd/code-style-guide.html#misc",
    "title": "Style Guide",
    "section": "Misc",
    "text": "Misc\n\nPackages\n\n{styler}"
  },
  {
    "objectID": "qmd/code-style-guide.html#naming",
    "href": "qmd/code-style-guide.html#naming",
    "title": "Style Guide",
    "section": "Naming",
    "text": "Naming\n\nCharacteristics\n\nGood names are a form of documentation\nNested loop variables should be names, not letters\nNames should be easily searchable\nUse prefixes and positive terms for booleans\nAdd extra detail for test functions\nNames should be pronouncable\nUse consistent lexicon throughout a project\n\nGood names are a form of documentation\n\nNested loop variables should be names, not letters\n\nNames should be easily searchable\n\nUse prefixes and positive terms for booleans\n\nAdd extra detail for test functions\n\nNames should be pronouncable\n\nUse consistent lexicon throughout a project"
  },
  {
    "objectID": "qmd/code-style-guide.html#refactoring",
    "href": "qmd/code-style-guide.html#refactoring",
    "title": "Style Guide",
    "section": "Refactoring",
    "text": "Refactoring\n\nFlatten nested code\n\n\n\n\n\nNested\n\n\n\n\n\n\n\nFlattened\n\n\n\n\n\n\n\nCondition on the negative"
  },
  {
    "objectID": "qmd/code-testing.html#sec-code-test-misc",
    "href": "qmd/code-testing.html#sec-code-test-misc",
    "title": "Testing",
    "section": "Misc",
    "text": "Misc\n\nIf the code is for a specific dataset/pipeline, then assertive testing makes more sense than traditional software testing.\nIf the code is general purpose, it should be in a package undergo traditional software testing\nAlso see\n\nPackage Development &gt;&gt; Testing\n\nPackages\n\n{assertthat}\n{testthat}\n{{pytest}} - It scales down, being super easy to use, but scales up, with mighty features and a rich ecosystem of plugins.\n\nDoesnâ€™t add the entry point directory to sys.path. However, you canÂ force it to do so with configuration. (See Make your Python life easier by learning how imports find things about entry points)\n\n{{tox}} and {{nox}} - Useful to run tests on different versions of Python to be sure it works with all of them. Both are good but nox is recommended"
  },
  {
    "objectID": "qmd/code-testing.html#sec-code-test-assert",
    "href": "qmd/code-testing.html#sec-code-test-assert",
    "title": "Testing",
    "section": "Assertive Testing",
    "text": "Assertive Testing\n\nTesting that happens within the function\nCheck for NAs in column\nif (anyNA(dataset$body_mass_g)) {\nÂ  rlang::abort(\"NAs are present in 'body_mass_g' column\")\n}\nAssert that I have not inadvertently changed the length of the output dataset either by accidentally dropping rows or accidentally introducing duplicates\nlibrary(testthat)\nmake_my_rectangle &lt;- function(dataset_a, dataset_b, dataset_c) {\n\nÂ  ... Do stuff\n\nÂ  expect_equal(nrow(output_dataset), nrow(dataset_a))\nÂ  expect_false(any(duplicated(output_dataset$id)))\n\nÂ  output_dataset\n}"
  },
  {
    "objectID": "qmd/confidence-and-prediction-intervals.html",
    "href": "qmd/confidence-and-prediction-intervals.html",
    "title": "8Â  Confidence & Prediction Intervals",
    "section": "",
    "text": "TOC\n\nMisc\nTerms\nDiagnostics\nBootstrapping\nConformal Prediction Intervals\n\nMisc\n\nAlso see Statistical Concepts &gt;&gt; Fundamentals &gt;&gt; Understanding CI, sd, and sem Bars\nSE used for CIs of the difference in proportions\n\nTerms\n\nConfidence Intervals:\n\nfrom https://staff.math.su.se/hoehle/blog/2017/06/22/interpretcis.html\nFrequentist: the confidence interval is constructed by a procedure, which, if you were to repeat the experiment and collecting samples many many times, in 95% of the experiments, the corresponding confidence intervals would cover the true value of the population mean.\n\nWhere\n\nt is the t-stat for\n\nn-k = sample size - number of predictors\n1 - Î± for 2-sided; 1 - (Î±/2) for 1 sided (I think)\n\nSE(B^i) is the sqrt of the corresponding value on the diagonal of the variance-covariance matrix for the coefficients.\n\n\nBayesian: the true value is in that interval with 95% probability\nJeffreyâ€™s Interval: Bayesian CIs for Binomial proportions (i.e.Â probability of an event)\n\n\n# probability of event\n# n_rain in the number of events (rainy days)\n# n is the number of trials (total days)\nmutate(pct_rain = n_rain / n,Â \nÂ  Â  Â  Â  Â  # jeffreys interval\nÂ  Â  Â  Â  Â  # bayesian CI for binomial proportions\nÂ  Â  Â  Â  Â  low = qbeta(.025, n_rain + .5, n - n_rain + .5),Â \nÂ  Â  Â  Â  Â  high = qbeta(.975, n_rain + .5, n - n_rain + .5))\n\nPrediction Intervals\n\nStandard Procedure for computing PIs for predictions (see link for examples and further details)\nWhere Y^0 is a single prediction\nt is the t-stat for\n\nn-p = sample size - number of predictors\n1 - Î± for 2-sided; 1 - (Î±/2) for 1 sided (I think)\n\nÏƒ^ is the variance given by residual standard error, summary(Model1)$sigma \n\nÏƒ^Â  = S\nI think this is also the MSE/dof that you sometimes see in other formulas\n\nx0 is new data for the predictor variable values for the prediction (also would need to include a 1 for the intercept)\n(Xâ€™X)-1 is the variance covariance matrix, vcov(model)\n\nTarget Coverage: The level of coverage you wantÂ  to attain on a holdout dataset\n\ni.e.Â the proportion of observations you want to fall within your prediction intervals\n\nExpected Coverage: The level of confidence in the model for the prediction intervals,\n\ni.e.Â setting Î± = 0.05\n\nEmpirical Coverage: The level of coverage actually observed when evaluated on a dataset, typically a holdout dataset not used in training the model.\n\nRarely will your model produce the Expected Coverage exactly\n\nAdaptive Coverage: Setting your Expected Coverage so that your Empirical Coverage = Target Coverage\n\nExample: 90% target coverage\n\nIf our model is slightly overfit, you might see that a 90% expected coverage leads to an 85% empirical coverage on a holdout dataset. To align your target and empirical coverage at 90%, may require setting expected coverage at something like 93%\n\n\n\nDiagnostics\n\nMean Interval Score (MIS)\n\n(Proper) Score of both coverage and interval width\n\nI donâ€™t think thereâ€™s a closed range, so itâ€™s meant for model comparison\nLower is better\n\ngreybox::MIS and (scaled) greybox::sMIS\n\nOnline docs donâ€™t have these functions, but docs in RStudio do\n\nAlso scoringutils::interval_score\n\nDocs have formula\n\nThe actual paper is dense Need to take the mean of MIS\n\n\n\nCoverage\n\nExample: Coverage %\n\n\nÂ  Â  coverage &lt;- function(df, ...){\nÂ  Â  Â  df %&gt;%\nÂ  Â  Â  Â  mutate(covered = ifelse(Sale_Price &gt;= .pred_lower & Sale_Price &lt;= .pred_upper, 1, 0)) %&gt;%Â \nÂ  Â  Â  Â  group_by(...) %&gt;%Â \nÂ  Â  Â  Â  summarise(n = n(),\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  n_covered = sum(\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  covered\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  ),\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  stderror = sd(covered) / sqrt(n),\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  coverage_prop = n_covered / n)\nÂ  Â  }\nÂ  Â  rf_preds_test %&gt;%Â \nÂ  Â  Â  coverage() %&gt;%Â \nÂ  Â  Â  mutate(across(c(coverage_prop, stderror), ~.x * 100)) %&gt;%Â \nÂ  Â  Â  gt::gt() %&gt;%Â \nÂ  Â  Â  gt::fmt_number(\"stderror\", decimals = 2) %&gt;%Â \nÂ  Â  Â  gt::fmt_number(\"coverage_prop\", decimals = 1)\n\nFrom Quantile Regression Forests for Prediction Intervals\nâ€œSale_Priceâ€ is the outcome variable\nâ€œrf_preds_testâ€ is the resulting object from predict with a tidymodels model as input\nExample: Test consistency of coverage across quintiles\n\nÂ  Â  preds_intervals %&gt;%Â  # preds w/ PIs\nÂ  Â  Â  mutate(price_grouped = ggplot2::cut_number(.pred, 5)) %&gt;%Â  # quintiles\nÂ  Â  Â  mutate(covered = ifelse(Sale_Price &gt;= .pred_lower & Sale_Price &lt;= .pred_upper, 1, 0)) %&gt;%Â \nÂ  Â  Â  with(chisq.test(price_grouped, covered))\n\np value &lt; 0.05 says coverage significantly differs by quintile\nâ€œSale_Priceâ€ is the outcome variable\nInterval Width\n\nNarrower bands should mean a more precise model\nExample: average interval width across quintiles\n\n\nÂ  Â  lm_interval_widths &lt;- preds_intervals %&gt;%Â \nÂ  Â  Â  mutate(interval_width = .pred_upper - .pred_lower,\nÂ  Â  Â  Â  Â  Â  interval_pred_ratio = interval_width / .pred) %&gt;%Â \nÂ  Â  Â  mutate(price_grouped = ggplot2::cut_number(.pred, 5)) %&gt;% # quintiles\nÂ  Â  Â  group_by(price_grouped) %&gt;%Â \nÂ  Â  Â  summarise(n = n(),\nÂ  Â  Â  Â  Â  Â  Â  Â  mean_interval_width_percentage = mean(interval_pred_ratio),\nÂ  Â  Â  Â  Â  Â  Â  Â  stdev = sd(interval_pred_ratio),\nÂ  Â  Â  Â  Â  Â  Â  Â  stderror = stdev / sqrt(n)) %&gt;%Â \nÂ  Â  Â  mutate(x_tmp = str_sub(price_grouped, 2, -2)) %&gt;%Â \nÂ  Â  Â  separate(x_tmp, c(\"min\", \"max\"), sep = \",\") %&gt;%Â \nÂ  Â  Â  mutate(across(c(min, max), as.double)) %&gt;%Â \nÂ  Â  Â  select(-price_grouped)Â \n\nÂ  Â  lm_interval_widths %&gt;%Â \nÂ  Â  Â  mutate(across(c(mean_interval_width_percentage, stdev, stderror), ~.x*100)) %&gt;%Â \nÂ  Â  Â  gt::gt() %&gt;%Â \nÂ  Â  Â  gt::fmt_number(c(\"stdev\", \"stderror\"), decimals = 2) %&gt;%Â \nÂ  Â  Â  gt::fmt_number(\"mean_interval_width_percentage\", decimals = 1)\n\nInterval width has actually been transformed into a percentage as related to the prediction (removes the scale of the outcome variable)\n\nBootstrapping\n\nMisc\n\nDo NOT bootstrap the standard deviation\n\narticle\nbootstrap is â€œbased on a weak convergence of momentsâ€\nif you use an estimate based standard deviation of the bootstrap, you are being overly conservative (i.e.Â overestimate the sd)\n\nbootstrapping uses the original, initial sample as the population from which to resample, whereas Monte Carlo simulation is based on setting up a data generation process (with known values of the parameters of a known distribution). Where Monte Carlo is used to test drive estimators, bootstrap methods can be used to estimate the variability of a statistic and the shape of its sampling distribution\nPackages\n\n{ebtools::get_boot_ci}\n\n\nSteps\n\nResample with replacement\nCalculate statistic of resample\nStore statistic\nRepeat 10K or so times\nCalculate mean, sd, and quantiles for CIs across all collected statistics\n\nCIs\n\nPlenty of articles for means and models, see bkmks\nrsample::reg_intervals is a convenience function for lm, glm, survival models\n\nPIs\n\nBootstrapping PIs is a bit complicated\n\nSee Shallowayâ€™s article (code included)\nonly use out-of-sample estimates to produce the interval\nestimate the uncertainty of the sample using the residuals from a separate set of models built with cross-validation\n\n\n\nConformal Prediction Intervals\n\nNotes from How to Handle Uncertainty in Forecasts: A deep dive into conformal prediction\nNormal PIs require iid data while conformal PIs only require the â€œidentically distributedâ€ part (not independent) and therefore should provide more robust coverage.\nSteps\n\nCV the model\nOn the hold-out set, compute the â€œconformal scoresâ€ for each observation\n\nThis is just the residual of sorts.\n\nLow = good, High = bad\n\nyi is the observed label (e.g.Â binary 0 or 1)\n\nGood model\n\nIf truth = 0, low predicted probability -&gt; score = low\nIf truth = 1, high predicted probability -&gt; score = low\n\nAwful model\n\nIf truth = 0, high predicted probability -&gt; score = high\nIf truth = 1, low predicted probability -&gt; score = high\n\n\nFor each observation, we calculate the conformal score\n\ne.g.Â for a binary target/label, each observation has two conformal scores.\n\nOnly need the magnitude, so take the absolute value. Therefore, the range is between 0 and 1\n\nOrder the conformal scores from highest to lowest\nCalculate the critical value for a chosen Î±\n\nx-axis corresponds to an ordered set of conformal scores\nIf Î± = 0.05, find the residual value at the the 95th percentile\nBlue: Conformal scores are not statistically significant. Theyâ€™re within our prediction interval.\nRed: Very large conformal scores indicate high divergence from the true label. These conformal scores are statistically significant and thereby outside of our prediction interval.\n\nCompute confusion matrix (e.g.Â binary target where labels are 0 and 1)\n\nInterpretation\n\nTop-left: predictions where both labels are not statistically significant (i.e.Â inside the â€œprediction intervalâ€).\n\nThe model predicts both classes well since both labels have low scores.\nDepending the threshold, maybe the model could be relatively agnostic (e.g.Â predicted probabilites like 0.50-0.50, 0.60-0.40)\n\nBottom-right: predictions where both labels are statistically significantÂ  (i.e.Â outside the â€œprediction intervalâ€).\n\nModel totally whiffs. Confident itâ€™s one label when itâ€™s actually another.\n\nExample\n\n1 (truth) - low predicted probability = high score -&gt; Red and significant\n0 - high predicted probability = high score -&gt; Red and significant\n\n\n\nTop-right: predictions where all 0 labels are not statistically significant.\n\nModel predicted the 0=class well (i.e.Â low scores) but the 1-class poorly (i.e.Â high scores)\n\nBottom-left: predictions where all 1 labels are not statistically significant. Here, the model predicted that 1 is the true class.\n\nVice versa of top-right\n\n\n\n\nThis can be extended to multinomial classification but it becomes more computationally intensive\n\nalso to continuous cases (link) using quantile regression\n\n\nhttps://towardsdatascience.com/how-to-handle-uncertainty-in-forecasts-86817f21bb54 https://towardsdatascience.com/mapie-explained-exactly-how-you-wished-someone-explained-to-you-78fb8ce81ff3 https://towardsdatascience.com/conformal-prediction-in-julia-351b81309e30 Notes\n\nCQR requires three sets of data:\n\nTraining data: data on which the quantile regression model learns.\nCalibration data: data on which CQR calibrates the intervals.\n\nMaybe â€œcalibrationâ€ data is the validation set\nIn the example, he split the data into 3 equal sets\n\nTest data: data on which we evaluate the goodness of intervals.\n\nSteps\n\nFit quantile regression model on training data.\nUse the model obtained at previous step to predict intervals on calibration data.\n\nPIs are predictions at the quantiles:\n\n(alpha/2)*100) (e.g 0.025, alpha = 0. 05)\n(1-(alpha/2))*100) (e.g.Â 0.975)\n\n\nCompute conformity scores on calibration data and intervals obtained at the previous step.\n\nresiduals are calculated for the PI vectors\nScores are calculated by taking the row-wise maximum of both (upper/lower quantile) residual vectors\n\nGet 1-alpha quantile from the distribution of conformity scores\n\nThis point will be the threshold\n\nUse the model obtained at step 1 to make predictions on test data.\n\nUse test data to compute PI vectors (i.e.Â predictions at the previously stated quantiles)\ni.e.Â same calculation as with the calibration data in step 2\n\nCompute lower/upper end of the interval by subtracting/adding the threshold from/to the quantile predictions (aka PIs)\n\nlower conformity interval is test set lower PI - threshold\nupper conformity interval is test set upper PI + threshold\n\n\nPy code\n\nimport numpy as np\nfrom skgarden import RandomForestQuantileRegressor\n\nalpha = .05\n\n# 1. Fit quantile regression model on training data\nmodel = RandomForestQuantileRegressor().fit(X_train, y_train)\n\n# 2. Make prediction on calibration data\ny_cal_interval_pred = np.column_stack([\nÂ  Â  model.predict(X_cal, quantile=(alpha/2)*100),Â \nÂ  Â  model.predict(X_cal, quantile=(1-alpha/2)*100)])\n\n# 3. Compute conformity scores on calibration data\ny_cal_conformity_scores = np.maximum(\nÂ  Â  y_cal_interval_pred[:,0] - y_cal,Â \nÂ  Â  y_cal - y_cal_interval_pred[:,1])\n\n# 4. Threshold: Get 1-alpha quantile from the distribution of conformity scores\n#Â  Â  Note: this is a single number\nquantile_conformity_scores = np.quantile(\nÂ  Â  y_cal_conformity_scores, 1-alpha)\n\n# 5. Make prediction on test data\ny_test_interval_pred = np.column_stack([\nÂ  Â  model.predict(X_test, quantile=(alpha/2)*100),Â \nÂ  Â  model.predict(X_test, quantile=(1-alpha/2)*100)])\n\n# 6. Compute left (right) end of the interval by\n#Â  Â  subtracting (adding) the quantile to the predictions\ny_test_interval_pred_cqr = np.column_stack([\nÂ  Â  y_test_interval_pred[:,0] - quantile_conformity_scores,\nÂ  Â  y_test_interval_pred[:,1] + quantile_conformity_scores])\n\nData usage summary\n\nThe model is fit on the training data\nThe scores are calculated on the calibration data along with the threshold\nFor the conformity intervals, predictions from a model fit on the test data add/subtract the threshold from those predictions\n\nPositive threshold means the PIs get widened by the threshold amount, upper and lower.\n\ne.g.Â quantile DL model\n\nNegative threshold means the PIs get shrank by the threshold amount, upper and lower.\n\ne.g quantile RF model\n\nSo future data (just like the test set) uses the threshold amount obtained using the training and calibration data to calculate the PIs for the those estimates\nDescription\n\nConformity scores express the (signed) distance between each observation and the nearest extreme of the interval.\nThe sign is given by the position of the point, whether it falls inside or outside the interval.\n\nWhen the point lies within the interval, the sign of conformity score is negative\nWhen the point lies outside the interval, the sign of conformity score is positive.\nWhen the point lies exactly on the interval, the conformity score is zero\n\nSince the maximum is taken to get the overall conformity score for that data point,\n\nBoth the lower quantile prediction and upper quantile predictions have to lie within the interval for the overall score to be negative at that data point\nEither or both quantile predictions have to lie outside the interval for the overall score to be positive at the data point.\nBoth quantile predictions have to lie exactly on the interval for the overall score to be zero at the data point\nTherefore, all things being equal, the threshold is more likely to expand the interval than contract it.\n\nThe alpha can be adjusted if need be to get the desired coverage\n\n\n\n\nBoosted models only able to fit one quantile at a time. no xgboost quantile regression gbm::gbm(distribution = list(name = \"quantile\",alpha = 0.25)\n\nwhere alpha is the quantile\n\nlightgbm\n\nargs: objective = â€œquantileâ€, alpha = 0.50,\n(maybe) metric = â€œquantileâ€\n\nâ€œmetric(s) to be evaluated on the evaluation set(s)â€\n\n\ncatboost\n\ncatboost.train(params = list(c(â€˜Quantile:alpha=0.1â€™))\n\nparams syntax might be wrong. R documentation isnâ€™t even half-assed. Itâ€™s like 1% -assed\n\n\nrf\n\n{grf}, {ranger}, {quantregForest}, {quantregRanger}\ngrfâ€™s quantile_forest method does not actually implement Meinshausenâ€™s quantile regression forest algorithm. A major difference is that grf makes splits that are sensitive to quantiles, whereas Meinshausenâ€™s method uses standard CART splits. (Paper)\n\nExample: ranger rf quantile regression (shalloway article)\n\nmodel\n\nrf_mod &lt;- rand_forest() %&gt;%\nÂ  set_engine(\"ranger\", importance = \"impurity\", seed = 63233, quantreg = TRUE) %&gt;%\nÂ  set_mode(\"regression\")\nset.seed(63233)\nrf_wf &lt;- workflows::workflow() %&gt;%Â \nÂ  add_model(rf_mod) %&gt;%Â \nÂ  add_recipe(rf_recipe) %&gt;%Â \nÂ  fit(ames_train)\n\nquantile predictions\n\npreds_bind &lt;- function(data_fit, lower = 0.05, upper = 0.95){\nÂ  predict(\nÂ  Â  Â  rf_wf$fit$fit$fit,Â  # parsnip::extract_fit_engine available now\nÂ  Â  Â  workflows::pull_workflow_prepped_recipe(rf_wf) %&gt;% bake(data_fit), # preprocessed data\nÂ  Â  Â  type = \"quantiles\",\nÂ  Â  Â  quantiles = c(lower, upper, 0.50)\nÂ  ) %&gt;%Â \nÂ  with(predictions) %&gt;%Â \nÂ  as_tibble() %&gt;%Â \nÂ  set_names(paste0(\".pred\", c(\"_lower\", \"_upper\",Â  \"\"))) %&gt;%Â \nÂ  mutate(across(contains(\".pred\"), ~10^.x)) %&gt;%Â \nÂ  bind_cols(data_fit) %&gt;%Â \nÂ  select(contains(\".pred\"), Sale_Price, Lot_Area, Neighborhood, Years_Old, Gr_Liv_Area, Overall_Qual, Total_Bsmt_SF, Garage_Area)\n}\nrf_preds_test &lt;- preds_bind(ames_holdout)\n\nVisualize\n\nset.seed(1234)\nrf_preds_test %&gt;%Â \nÂ  mutate(pred_interval = ggplot2::cut_number(Sale_Price, 10)) %&gt;%Â \nÂ  group_by(pred_interval) %&gt;%Â \nÂ  sample_n(2) %&gt;%Â \nÂ  ggplot(aes(x = .pred))+\nÂ  geom_point(aes(y = .pred, color = \"prediction interval\"))+\nÂ  geom_errorbar(aes(ymin = .pred_lower, ymax = .pred_upper, color = \"prediction interval\"))+\nÂ  geom_point(aes(y = Sale_Price, color = \"actuals\"))+\nÂ  scale_x_log10(labels = scales::dollar)+ # target variable is log10 transformed\nÂ  scale_y_log10(labels = scales::dollar)+\nÂ  labs(title = \"90% Prediction intervals on a holdout dataset\",\nÂ  Â  Â  subtitle = \"Random Forest Model\",\nÂ  Â  Â  Â  y = \"Sale_Price prediction intervals and actuals\")+\nÂ  theme_bw()+\nÂ  coord_fixed()\n\n\n\nCoverage and interval length tuning\ndef compute_coverage_len(y_test, y_lower, y_upper):\nÂ  Â  \"\"\" Compute average coverage and length of prediction intervals\nÂ  Â  Parameters\nÂ  Â  ----------\nÂ  Â  y_test : numpy array, true labels (n)\nÂ  Â  y_lower : numpy array, estimated lower bound for the labels (n)\nÂ  Â  y_upper : numpy array, estimated upper bound for the labels (n)\nÂ  Â  Returns\nÂ  Â  -------\nÂ  Â  coverage : float, average coverage\nÂ  Â  avg_length : float, average length\nÂ  Â  \"\"\"\nÂ  Â  in_the_range = np.sum((y_test &gt;= y_lower) & (y_test &lt;= y_upper))\nÂ  Â  coverage = in_the_range / len(y_test) * 100\nÂ  Â  avg_length = np.mean(abs(y_upper - y_lower))\nÂ  Â  return coverage, avg_length\n\nNotes\n\ntraining set_1 = 0.80, test set = 0.20\ntraining set_1 divided into train_2 and calibration sets (50/50)\n\nrow indexes: idx_train, idx_cal\nTheÂ  train_2 of the split is split further for figuring out the quantiles with correct coverage\n\ntrain_3/test = 95/5\n\n\nPredictions from tuned quantiles used to calculate scores\nUses alpha as the threshold index\nhardcoded values\n\ncoverage_factor would have to be something guess-timated or based on some previous knowledge about how â€œoffâ€ the algrithmâ€™s PI quantilesâ€™ coverage is\ninitial best_avg_length would require some knowledge of a baseline PI length and scale of the data\nMaybe these donâ€™t have to be hardcoded\n\nValues\n\nquantiles: [0.05, 0.95]\nquantile factor = 0.85 * (0.95 - 0.05) = 0.76\nalpha = 0.10 (i.e.Â acceptable miscoverage rate; â€œsignificanceâ€ in the code)\n\nasymmetric score function (QuantileRegAsymmetricErrFunc) available\n\nrun_icp (cqr/helper.py)\n\nnc, x_train, y_train, x_test, idx_train, idx_cal, alpha\ninstantiates IcpRegressor (nonconformist/icp.py)\n\ninput: nc_function = RegressorNC(QuantileForestRegressorAdapter (cqr/helper.py), QuantileRegErrFunc (nonconformist/nc.py))\n\nRegressorNC (nonconformist/nc.py)\n\ninherits\n\nBaseModelNc(model, errorfunc)\n\nmethods\n\nfit via model.fit\nscore\n\ncomputes nonconformity scores\n\n\n\n\n\n\ninherits\n\nBaseICP\n\ninherits sklearn BaseEstimater\n\nmethods: get_params, set_params\n\nmethod\n\nfit via nc.fit\ncalibrate\nother calibrate stuff\n\n\n\nmethod: predict\n\nRunsÂ  via IcpRegressor (nonconformist/icp.py)\n\nfit\n\ninput: X_train[idx_train,:], y_train[idx_train]\nQuantileForestRegressorAdapter (cqr/helper.py)\n\ninherits RegressorAdapter (noncomformist/base.py)\n\ninherits BaseModelAdaptor\n\ninherits BaseEstimator\nmethods: fit, predict, underlying_predict\n\nmethods: underlying_predict\n\nmethods\n\nfit\n\ninputs: X_train[idx_train,:], y_train[idx_train]\ntarget_coverage = quantiles[0] - quantiles[1] (quantiles = [0.05, 0.95]) = 0.90\ncoverage_factor = 0.85\ntarget_coverage = coverage_factor * target_coverage = 0.765\nrange_vals = 30\nnum_vals = 10\ngrid_q = grid of quantiles to search\n\nlower = 0.05 to 0.35 (num_vals = 10 evenly spaced values) (i.e.Â 0.05 to 0.05 + range_vals)\nupper = 0.95 to 0.65Â  (num_vals = 10 evenly spaced values) (i.e.Â 0.95 to 0.95 - range_vals)\nconcantenated pairwise (0.35, 0.65 gets fit, etc.), so I think heâ€™s fitting 10 models\n\ncalls CV_quntiles_rf (cqr/tune_params_cv.py)\n\ninputs: rf_params, x, y, target_coverage, grid_q, test_ratio, coverage_factor)\n\ntest_ratio = 0.05\nSo train/test =Â  95/5\n\nprocess\n\nrf fit on training data\nbest_avg_length = 1e10 (initial value)\nloop (row in row of grid_q)\npredicts lower and upper quantile vectors (i.e.Â row of the grid_q) on test data\ncalls compute_coverage_len (cqr/helper.py)\nÂ  Â  Â  Â  inputs: y_test, y_lower (i.e.Â lower quantile), y_upper (i.e.Â upper quantile)\nÂ  Â  Â  Â  computes coverage and avg length of interval\nÂ  Â  Â  Â  check if coverage &gt; target_coverage AND avg length &lt; best_avg_length\nÂ  Â  Â  Â  Â  Â  if so, set best_q = row of grid_q, best avg length = avg length\nreturn best_q\n\nstored in attribute, self.cv_quantiles Â  Â  Â  Â \n\n\npredict\n\nuses self.cv_quantiles\n\n\n\n\ncalibrate\n\ninput: X_train[idx_cal,:], y_train[idx_cal]\ninherits BaseIcp\n\nmethod:calibrate\n\nuses nc.function.score which is RegressorNc (nonconformist/nc.py)\n\ninherits BaseModelNc\n\nCalc predictions with QuantileForestRegressorAdapter (cqr/helper.py) predict method\n\nSo it does use tuned quantiles to compute scores\n\nsends predictions to QuantileRegErrFunc (nonconformist/nc.py) to calc scores\n\nmax(\\hat{q}_low - y, y - \\hat{q}_high)\n\n\n\n\nSorts (desc), and stores scores in self.cal_scores\n\n\npredict\n\ninput: X_test, alpha\nIcpRegressor.predict makes no sense to me. Looks to me that it would return an array of zeros. Â¯\\_(ãƒ„)_/Â¯\nIt uses nc.function.predict(X_test, self.cal_scores, significance (aka alpha)) in the code, but the conditional doesnâ€™t make sense to me\n\nRegressorNC.predict\n\nCalc predictions with QuantileForestRegressorAdapter (cqr/helper.py) predict method\nUses alpha to find the index of threshold score value through a QuantileRegErrFunc method\nhe has\n\nlower PI = lower_quantile_pred - threshold score\nupper PI = upper_quantile_pred + threshold score\n\n\n\n\n\n\n\nClassification\n\nNotes from paper, â€œClassification with Valid and Adaptive Coverageâ€\nMarginal Coverage\n\nThe probability that an future observed value is in a predicted set is greater than equal some miscoverage rate.\n\nConditional Coverage\n\nValid coverage conditional on a specific observed value of the features X.\n\nStronger statement than marginal coverage and cannot be achieved in theory without strong modeling assumptions\n\n\nThe only restrictions are the data are exchangeable and the training algorithm treats all samples exchangeably; i.e., it should be invariant to their order.\n\nNo typical ML algorithm fails this\n\nAdaptive classification with CV+ calibration\n\nSample Ui âˆ¼ Uniform(0, 1) for each i âˆˆ {1, . . . , n + 1}, independently of everything else\nTypical k-fold CV\n\ntrain model on all folds except k, etc.\n\nConstruct prediction set, CCV+n,Î±\n\nSays we sweep over all possible labels y âˆˆ Y and include y in the final prediction set CCV+ n,Î± (Xn+1) if the corresponding score E(Xn+1, y, Un+1; Ë†Ï€ k(i) ) is smaller than (1 âˆ’ Î±)(n + 1) hold-out scores E(Xi , Yi , Ui ; Ë†Ï€ k(i) ) evaluated on the true labeled data"
  },
  {
    "objectID": "qmd/cross-validation.html#sec-cv-misc",
    "href": "qmd/cross-validation.html#sec-cv-misc",
    "title": "Cross-Validation",
    "section": "Misc",
    "text": "Misc\n\nGuide for suitable baseline models: link\nIf N &gt; 20,000 & &gt; 60 predictors, use k-fold (Harrell)\nIf N &gt; 20,000 & &lt; 60 predictors, use k-fold or bootstrap resampling (Harrell)\nIf N &lt; 20,000, use bootstrap resampling (Harrell)\nIf N &lt; 10,000, use nested-cv (Raschka)\n\nIf using NCV for algorithm selection and hyperparameter tuning, the prediction error calculation is the standard CV predicition error (used for the algorithm selection) is biased in certain situations. See K-Fold &gt;&gt; Bates, Hastie, Tibshirani 2021 &gt;&gt; CV inflation\nMe: The NCV calculation of the prediction error involves the folds of inner loop that are used in the hyperparameter tuning. It might not be mathematically kosher to still calculate the prediction error using NCV method while tuning but it might still produce a less biased result and better coverage than the CV method. Itâ€™s still the same algorithm just with minor tweaks.\n\nSplits\n\nIf &lt; 12 samples per predictor\n\nThe test partition should be no less than 10% of the sample\n\nIf you have enough samples for reasonable predictive accuracy as determined by the sample complexity generalization error,\n\nA 50% test partition size is fine.\n\nBetween these two boundaries, adjust the test size to limit the generalization test error in a tradeoff with training sample size (Abu-Mostafa, Magdon-Ismail, & Lin, 2012, pg. 57).\n\nVehtari (Paper)\n\nRe Modeling Assumptions:\n\nIf you canâ€™t make any, then using CV (and WAIC) is appropriate even with the higher variance\nIf you can make assumptions, then you can reduce variance by examining directly the posterior or using reference models to filter out noise in the data (?) (see, e.g., Piironen, Paasiniemi and Vehtari (2018) and Pavone et al.Â (2020)).\n\nRe Number of Candidate Models\n\nFor a small number of models, performance bias at the model selection stage is usually negligible, that is, smaller than the standard deviation of the estimate or smaller than what is practically relevant.\nFor a large number of models, the performance bias at the model selection stage can be non-negligible, but this bias can be estimated using nested-CV or bootstrap\n\nThe paper reviews the concepts of selection-induced bias and overfitting, proposes a fast to compute estimate for the bias, and demonstrates how this can be used to avoid selection induced overfitting even when selecting among 10^30 models"
  },
  {
    "objectID": "qmd/cross-validation.html#sec-cv-kfold",
    "href": "qmd/cross-validation.html#sec-cv-kfold",
    "title": "Cross-Validation",
    "section": "K-Fold",
    "text": "K-Fold\n\nMisc\n\nAny preparation of the data prior to fitting the model occur on the CV-assigned training dataset within the loop rather than on the broader data set. This also applies to any tuning of hyperparameters. A failure to perform these operations within the loop may result in data leakage and an optimistic estimate of the model skill.\nIf the effective number of parameters of the model is much less than n, then with K&gt;10, the CV bias is usually negligible compared to the CV variance. (Vehtari)\n\nProcedure\n\nSplit the data into Train and Test sets\nWith the Train set\n\nShuffle the dataset randomly.\nSplit the dataset into k groups (aka folds)\nFor each unique fold:\n\nTake a fold as a validation dataset\nTake the remaining folds as a training dataset\nFit a model on the training set and calculate the performance metric score on the validation dataset\nRetain the performance metric score and discard the model\n\nSummarize the skill of the model using the sample of performance metric scores\n\ne.g.Â take the mean of the performance metric scores on the validation folds\nPrediction error\n\\[\n\\widehat {\\text{Err}}_{cv} := \\bar e = \\frac{1}{n} \\sum_{i=1}^n e_i\n\\]\nStandard Error of Prediction Error\n\\[\n\\widehat{\\text{SE}} := \\frac{1}{\\sqrt{n}} \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n (e_i - \\bar e)^2}\n\\]\nCI for prediction error\n\\[\n\\bar e \\pm z_{1-\\alpha/2} \\cdot \\hat{\\text{SE}}\n\\]\n\n\nRepeat for each model\nCompare models by their mean performance metric scores\nChoose best model and use it to predict on the Test set to get the out-of-sample error\n\nBates, Hastie, Tibshirani 2021\n\nThe estimand of CV is not the accuracy of the model fit on the data at hand, but is instead the average accuracy over many hypothetical data sets\nThe CV estimate of error has larger mean squared error (MSE) when estimating the prediction error of the final model than when estimating the average prediction error of models across many unseen data sets for the special case of linear regression\nCV inflation (the ratio of the true standard error of the point estimate compared to the CV estimate of standard error)\n\nThe number of folds has minimal impact on the inflation, although more folds gives moderately better coverage for small n.\nWe also find that even when n/p is less than/equal 20, there is appreciable inflation, and cross-validation leads to intervals with poor coverage\n\nCI for the prediction error\n\nUsing the variance to compute the width of the error interval does not account for the correlation between the error estimates in different folds, which arises because each data point is used for both training and testing\n\nEstimate of variance is too small and the intervals are too narrow\n\nCV has poor coverage until n &gt; 400.\nThe width of the NCV intervals relative to their CV counterpartsâ€”the usual ratio is not that large for samples sizes of n = 100 or greater.\nItâ€™s expected the standard CV intervals to perform better when n/p is larger and when more regularization is used\n\nRecommends Nested Cross-Validation to calculate the prediction error of the algorithm and the CI for that prediction error. (see details below)\nRegarding hyperparameter tuning and model selection\n\nâ€œ[35] suggests a bias correction for the model selected by cross-validation, [36] shows how to return a confidence set for the best model parameters, and [33, 18] show that selecting the best value of the tuning parameter is a statistically easier goal for CV than estimating the prediction error, in some sense.â€\n\nSee the paperâ€™s references for details on the papers being referred to."
  },
  {
    "objectID": "qmd/cross-validation.html#sec-cv-ncv",
    "href": "qmd/cross-validation.html#sec-cv-ncv",
    "title": "Cross-Validation",
    "section": "Nested Cross-Validation (NCV)",
    "text": "Nested Cross-Validation (NCV)\n\n\nAKA Double Cross-Validation\nPackages\n\n{glmnetr} - Performs nested cv using models from â€˜glmnetâ€™, â€˜survivalâ€™, â€˜xgboostâ€™, â€˜rpartâ€™ and â€˜torchâ€™. Fits relaxed Lasso and other models.\n\nHas many similarities to {glmnet}. Recommended that you read glmnet vignettes: â€œAn Introduction to glmnetâ€ and â€œThe Relaxed Lassoâ€\n\n{nestedcv} - Implements nested k*l-fold cross-validation for lasso and elastic-net regularised linear models via the {glmnet} package and other machine learning models via {caret}. Cross-validation of â€˜glmnetâ€™ alpha mixing parameter and embedded fast filter functions for feature selection are provided.\n\nWhen you have a smallish dataset such that having sufficient sized (hold-out) test set is unfeasible, using normal k-fold cv for BOTH tuning and error estimation (algorithm comparison) produces biased error estimation.\n\nThe data used for training in some folds are used for testing in other folds which increases the bias of the average error that is calculated across the folds. This bias means the average error rate across the folds canâ€™t be used as an estimate of the error rate of the model on an independent test set.\n\nRepeats should lower this bias and increase robustness in a nested cv framework so that both tuning and selection can be performed simultaneously. Number of repeats depends on how stable the results are.\nStandard deviations should be recorded along with mean scores on the outer loop. If the standard deviations widely vary, then more repeats may be required.\nIf repeats arenâ€™t included, then the nested cv should only be used for algorithm comparison and a separate k-fold cv should be performed afterwards to find the optimal hyperparameters.\n\n\nUse a larger k, number of outer folds, for smaller sample sizes\n\nIn Kuhnâ€™s example, he used 5 repeats of 10-fold cv with each fold comprised of 25 bootstrap resamples for just a 100 row dataset and the error estimate was pretty much balls on.\n\nKuhn used resamples for the inner-loop, but Raschka used folds. So either should be fine.\n\nSteps for algorithm selection:\n\nEntire nested cv procedure is executed for each candidate algorithm\nIf the algorithms make repeats too computationally intensive, then donâ€™t do the repeats. After the algorithm is selected, perform a k-fold cv for hyperparameter tuning.\nFor each algorithm (using repeats):\n\nA complete grid search of all hyperparameters is computed in each inner-resample of each fold of each repeat\n\nIf we have 5 repeats of 10-fold cv with each fold comprised of 25 bootstrap resamples, then there will be 5 * 10 * 25 = 1250 grid search sessions completed\nIn a fold of a repeat, the mean score (e.g.Â RMSE) and standard deviation are calculated for each hyperparameter value combination across all the resamplesâ€™ assessment sets.\n\nThe hyperparameter value combination with the best mean score (e.g.Â lowest average RMSE) is selected\n\nRepeat for each fold in each repeat\n\nIn a fold of a repeat, the selected hyperparameter values are used to fit on the entire analysis set of the fold.\n\nThe fitted model is scored on the assessment set of the fold\nRepeat for each fold in each repeat\n\nCalculate the average score and standard deviation across all folds for every repeat (i.e.Â one score value)(e.g.Â average RMSE)\n\nWith 5 repeats of 10-fold cv:\n\\[\n\\overline {\\text{RMSE}} = \\frac{1}{50} \\sum_{\\text{fold}=1}^{50} \\text{RMSE}_{\\text{fold}}\n\\]\n\n\nThis score is what will be compared during algorithm selection\nThe standard deviations can be used to get a sense of the stability of the scores\n\nRepeat for each algorithm\nChoose the algorithm with the lowest average score\nFor the final model, use the hyperparameter combination chosen most frequently as best during the inner-loop tuning of the winning algorithm.\n\nBates, Hastie, Tibshirani 2021\n\n.632 Bootstrap intervals are typically, but not always, wider than the NCV intervals. The bootstrap point estimates are typically more biased that the NCV point estimates.\nFound that a large number (e.g., 200) of random splits (aka Repeats) of nested CV were needed to obtain stable estimates of the standard error\nâ€œWe anticipate that nested CV can be extended to give valid confidence intervals for the difference in prediction error between two models.â€\n\nNo details given on this procedure. Not sure itâ€™s as straightforward as differencing each CI endpoint or what.\n\nProcedure for the prediction error estimate and its CI\n\nPer Fold\n\n\\(e_{\\text{in}}\\) - vector of errors (i.e loss function values) from the calculations of the errors on all validation sets in the inner loop\n\ni.e.Â Each validation set has a vector of errors, so \\(e_{\\text{in}}\\) is essentially of vector of vectors thatâ€™s been coerced into 1 vector\n\n\\(e_{\\text{out}}\\) - Vector of errors from the validation set of the outer loop using a model trained on the entire training set of the fold\n\\(a_{\\text{fold}_i} = (\\bar{e}_{\\text{in}} âˆ’ \\bar{e}_{\\text{out}})^2\\)\n\\(b_{\\text{fold}_i} = \\text{Var}(e_{\\text{out}})\\)\n\\(\\text{append}(a_{\\text{list}}, a_{\\text{fold}_i})\\)\n\\(\\text{append}(b_{\\text{list}}, b_{\\text{fold}_i})\\)\n\\(\\text{append}(\\text{es}, e_{\\text{in}})\\)\n\n\\(\\widehat{\\text{MSE}} = \\bar a_{\\text{list}} âˆ’ \\bar b_{\\text{list}}\\)\n\nStandard error of the prediction error estimate\n\n\\(\\widehat{Err}_{\\text{NCV}} = \\overline{\\text{es}}\\)\n\nPrediction error estimate\n\nCompute Bias correction value\n\nFit a standard k-fold CV and compute \\(\\widehat{Err}_{\\text{NCV}}\\)\n\nGuessing K (# of folds) for NCV and CV are equal\n\\(\\widehat{\\text{Err}}_{\\text{cv}}\\) would be the average error on the validation folds\n\\[\n\\widehat {\\text{Err}}_{cv} := \\bar e = \\frac{1}{n} \\sum_{i=1}^n e_i\n\\]\n\nBias\n\\[\n\\widehat {\\text{bias}} := \\left(1 + \\frac{K-2}{K}\\right)\\left(\\widehat{Err}_{\\text{NCV}} - \\widehat{Err}_{\\text{CV}}\\right)\n\\]\n\nCI for the prediction error estimate\n\nRegression\n\\[\n\\widehat{\\text{Err}}_{\\text{NCV}} - \\widehat{\\text{bias}} \\pm q_{1-\\alpha /2} \\cdot \\sqrt{\\frac{K-1}{K}} \\cdot \\sqrt{\\widehat{\\text{MSE}}}\n\\]\n\nThe \\(\\sqrt{\\frac{K-1}{K}} \\cdot \\sqrt{\\widehat{\\text{MSE}}}\\) term needs to be restricted to be between \\(\\widehat{\\text{SE}}\\) and \\(K \\cdot \\widehat{\\text{SE}}\\)\nK is the number of folds\n\nFor binary classification using 0/1 loss\n\\[\n\\sin^{-1}\\left(\\sqrt{\\widehat{\\text{Err}}_{\\text{NCV}}}\\right)\\pm z_{1-\\alpha/2} \\cdot \\frac{\\sqrt{\\widehat{\\text{MSE}}}}{\\widehat{\\text{SE}}} \\cdot \\sqrt{\\frac{1}{4n}}\n\\]\n\nn is the sample size\nWhere \\(\\widehat{\\text{SE}}\\) is the estimate of the width of the confidence interval using standard k-fold CV\n\n\\[\n\\widehat{\\text{SE}} := \\frac{1}{\\sqrt{n}} \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n (e_i - \\bar e)^2}\n\\]\nNot sure if this would be viable for some other loss function like cross-entropy loss or not. See Appendix E for a few more details.\n\n\n\nNotes\n\nNo initial split where thereâ€™s a hold out set that doesnâ€™t go through the ncv procedure\nPaper acknowledges that hyperparameter tuning might be useful for ncv, but says theyâ€™re not getting into it in this paper."
  },
  {
    "objectID": "qmd/cross-validation.html#sec-cv-swcv",
    "href": "qmd/cross-validation.html#sec-cv-swcv",
    "title": "Cross-Validation",
    "section": "Sliding Window Cross-Validation",
    "text": "Sliding Window Cross-Validation\n\n\nInstead of each fold using a different block of observations for the validation/test set, each successive fold slides the interval of the target variable interval (e.g.Â 3 weeks) from the previous fold\nSplit the data before creating the folds to avoid leakage.\nExample: Classification\n\nSee Algorithms, Marketing &gt;&gt; Propensity Model\nFrom Scoring Customer Propensity using Machine Learning Models on Google Analytics Data\nEach row is a customer\nEach fold\n\nFeature values are values that have been aggregated over a 3 month window for each customer\nTarget variable is purchase/no purchase which is detected over the next 3 weeks\n\n\nExample: Multi-step time series forecasting with XGBoost\n\nCodes a sliding-window (donâ€™t think itâ€™s a cv approach though) and forecasts each window with the model using the Direct method\nNot exactly sure how this works. No helpful figs in the article or the paper if follows, so would need to examine the code"
  },
  {
    "objectID": "qmd/cross-validation.html#sec-cv-ts",
    "href": "qmd/cross-validation.html#sec-cv-ts",
    "title": "Cross-Validation",
    "section": "Time Series",
    "text": "Time Series\n\nMisc\n\nUsually, the forecast horizon (prediction length),Â H,Â equals the number of time periods in the testing data\nAt a minimum, the training dataset should contain at least three times as many time periods as the testing dataset\nIn time series model comparison, CV variance is likely to dominate, and it is more important to reduce the variance than bias. Leave-Few-Observations with joint log score is better than use of Leave-Future-Out (LFO). Cooper et al.Â (2023) (Vehtari)\n\nTypes\n\n\n(Blue) Training set (including validation fold); (Orange) Test set; (White) Unused\n\nPaper: Evaluating time series forecasting models: An empirical study on performance estimation methods\n\nCode in R\nAlgorithm used: rule-base regression system Cubist (trees)\n\nâ€œother learning algorithms were tested, namely the lasso and a random forest. The conclusions drawn using these algorithms are similar to the ones reported in the next sections.â€\nData:\n\nEach of the 62 time series had lengths of either 949, 1338, or 3000.\nhalf-hourly, hourly, and daily frequencies\nIndustries: bike sharing, stock prices from aerospace companies, air quality, solar radiation, and water consumption\n\nBest Performing (i.e.Â cv error closest hold-out error) (i.e better estimation of generalized performance)\n\nRep-Holdout - Repeated holdout. Itâ€™s similar to holdout, but it consists of n estimates. Moreover, differently from holdout, not all the observations are used in a single estimate. A block (e.g.Â 70%) of the dataset is randomly selected, the first part of that block is used for training and the subsequent part for testing. This procedure is repeated n (e.g.Â 5) times\n\nVaried length of dataset from a size of 100 to a size of 3000, by intervals of 100 (100, 200, â€¦, 3000). The experiments did not provide any evidence that the size of the synthetic time series had a noticeable effect on the error of estimation methods.\nIn a stationary setting the cross-validation variants are shown to have a competitive estimation ability. However, when non-stationarities are present, they systematically provide worse estimations than the out-of-sample approaches.\nBest method based on summary stats of time series\n\n\nSummary Stats used in the Decision Tree (Tree only used 4 of the stats as splits)\n\nSkewness: For measuring the symmetry of the distribution of the time series;\n5th and 95th Percentiles (Perc05, Perc95) of the standardized time series;\nAcceleration (Accel.): As the average ratio between a simple moving average and the exponential moving average;\nInter-Quartile Range (IQR): as a measure of the spread of the standardized time series;\nSerial Correlation - Estimated using a Box-Pierce test statistic;\nLong-Range Dependence: A Hurst exponent estimation with wavelet transform;\nMaximum Lyapunov Exponent: A measure of the level of chaos in the time series;\nStationary: A boolean variable, indicating whether or not the respective time series is stationary according to the wavelet spectrum test\n\nInterpretation: Rep-Holdout, which is the best method most of the time across the 62 time series, should be used unless the acceleration is below 1.2. Otherwise, the tree continues with more tests in order to find the most suitable estimation method for each particular scenario.\n\n\n\n\n\n\nArticle: 12 Ways to Test Your Forecasts like A Pro\n\nCode in Python\nAlgorithm used: Linear Regression with lags and day-of-the-week variable.\nData: Hyndman datasets - â€œTSDL consists of 648 datasets, but I selected only the univariate time-series with at least 1,000 observations.â€ (Turns out to be around 58 datasets I think)\nExperiment: 90% used to perform cv method (cv error); model trained on 90% and tested on the 10% hold-out (hold-out error)\nBest Performing (i.e.Â cv error closest hold-out error) (i.e better estimation of generalized performance)\n\nInverse holdout: (13 wins) Itâ€™s the opposite of holdout: the latest part of the data (usually 75% or 80%) is used for training and the preceding part is used for testing.\npreq_grow: (7 wins) Prequential growing window. n iterations are carried out. At the first iteration, the test set is made of the latest part of observations (e.g.Â the latest 20%), whereas the first part is used for training. In the following iterations, the test set dimension is progressively reduced, and all the remaining observations are used for training.\npreq_sld_bls: (7 wins) Prequential sliding blocks. Each fold is made only of adjacent points. For each model, one fold is used for training and the subsequent fold is used for testing.\n\n\nExpanding Window or Forward Chaining or Online Training\n\n\nSimulates production environment\npreq-grow method above"
  },
  {
    "objectID": "qmd/css-general.html#sec-css-gen-misc",
    "href": "qmd/css-general.html#sec-css-gen-misc",
    "title": "General",
    "section": "Misc",
    "text": "Misc\n\n\nResources\n\nhttps://css-tip.com/\nWidget testing parameter values for css styling a div box\n\nEqual Column Widths\n\nCSS comment - /* comment */\nSelector formats\n\nSyntax: #&lt;class&gt;.&lt;id&gt;&lt;additional-stuff&gt;\nExample:\n\nCSS\n#header.fluid-row::before{\n}\nHTML\n&lt;div class=\"fluid-row\" id=\"header\"&gt; == $0\n::before\n&lt;/div&gt;\n\n\nInclude css styling directly into a html page\n\nExample: Via HTML style tag\n&lt;style&gt;\nbody {\nÂ  padding: 50px 25px 0px 25px;\nÂ  font-family: 'Roboto', sans-serif;\nÂ  font-size: 19px;\n}\n&lt;/style&gt;\nExample: Via R chunk\nhtmltools::tags\\$link(href = \"https://fonts.googleapis.com/css2?family=Libre+Baskerville:ital,wght\\@0,400;0,700;1,400&display=swap\",\n                      rel = \"stylesheet\")\nExample: styling of a legend html div\n&lt;style type='text/css'&gt;\nÂ  .my-legend .legend-title {\nÂ  Â  text-align: left;\nÂ  Â  margin-bottom: 8px;\nÂ  Â  font-weight: bold;\nÂ  Â  font-size: 90%;\nÂ  Â  }\nÂ  .my-legend .legend-scale ul {\nÂ  Â  margin: 0;\nÂ  Â  padding: 0;\nÂ  Â  float: left;\nÂ  Â  list-style: none;\nÂ  Â  }\nÂ  .my-legend .legend-scale ul li {\nÂ  Â  display: block;\nÂ  Â  float: left;\nÂ  Â  width: 50px;\nÂ  Â  margin-bottom: 6px;\nÂ  Â  text-align: center;\nÂ  Â  font-size: 80%;\nÂ  Â  list-style: none;\nÂ  Â  }\nÂ  .my-legend ul.legend-labels li span {\nÂ  Â  display: block;\nÂ  Â  float: left;\nÂ  Â  height: 15px;\nÂ  Â  width: 50px;\nÂ  Â  }\nÂ  .my-legend .legend-source {\nÂ  Â  font-size: 70%;\nÂ  Â  color: #999;\nÂ  Â  clear: both;\nÂ  Â  }\nÂ  .my-legend a {\nÂ  Â  color: #777;\nÂ  Â  }\n&lt;/style&gt;\n\nSee link for details on the legend div element that uses this CSS"
  },
  {
    "objectID": "qmd/css-recipes.html",
    "href": "qmd/css-recipes.html",
    "title": "Recipes",
    "section": "",
    "text": "Fit div to text length and center div in page\n\n\nh4, .h4 {\n  border-left: 4px solid #dee2e6;\n  padding-left: 20px;\n  border-right: 4px solid #dee2e6;\n  padding-right: 3px;\n  width: -moz-max-content;\n  width: -webkit-max-content;\n  width: max-content;\n  margin: 0 auto;\n}\n\nI wanted to create the effect with 2 bars on both sides of the text. Since the container stretched across the page, the right bar would be at the edge of the page.\n\nCan also do other things like creating a box or an effect around the text and not across the page\n\nwidth: max-content dynamically adjusts the length of the container to the length of the content\n\nThe multiple width attributes are so this works for different browsers (min-content works for Chrome).\nSee Understanding min-content, max-content, and fit-content in CSS for explanation of other similar attributes\n\nmargin: 0 auto positions the container in the center of the page\n\nNavbar Styling\n.navbar-inverse {\nÂ  background-color: #000000;\nÂ  border-color: #000000;\nÂ  font-family: 'Roboto', sans-serif;\n}\nImport Font\n@import url('https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,400;0,700;1,400&display=swap');\n\nCopied from google fonts site"
  },
  {
    "objectID": "qmd/data.table.html#sec-dt-misc",
    "href": "qmd/data.table.html#sec-dt-misc",
    "title": "data.table",
    "section": "Misc",
    "text": "Misc\n\nsetDT(df)- fast conversion of a data frame or list to a data.table without copying\n\nUse when working with larger data sets that take up a considerable amount of RAM (several GBs) because the operation will modify each object in place, conserving memory.\nas.data.table(matrix) should be used for matrices\ndat &lt;- data.table(df) can be used for small datasets but thereâ€™s no reason to.\nsetDT(copy(df)) if you want to work with a copy of the df instead of converting the original object.\n\nChaining: see Pivoting &gt;&gt; melt &gt;&gt; Multiple variables stored in column names for an example\nPiping\ndt |&gt; \n   _[, do_stuff(column), by = group] |&gt; \n   _[, do_something_else(othr_col), by = othr_grp]\n\nThe _ placeholder allows you to use Râ€™s native pipe."
  },
  {
    "objectID": "qmd/data.table.html#keys",
    "href": "qmd/data.table.html#keys",
    "title": "data.table",
    "section": "Keys",
    "text": "Keys\n\nFast filtering mechanism; reorders rows (increasing) to group by the values in the key columns. Reordered rows make them easier to find and subset.\n\nAll types of columns can be used except list and complex\n\nOperations covered in this section\n\nFiltering\nFilter, select\nFilter, groupby, summarize\nIf-Else\n\nSet Keys - Says order in the increasing direction according to origin and then dest.\nsetkey(flights, origin, dest)\nhead(flights)\n#Â  Â  year month day dep_delay arr_delay carrier origin dest air_time distance hour\n# 1: 2014Â  Â   1Â   2Â  Â  Â  Â  -2Â  Â  Â   -25Â  Â  Â  EVÂ  Â  EWRÂ  ALBÂ  Â  Â  30Â  Â  Â  143Â  Â  7\n# 2: 2014Â  Â   1Â   3Â  Â  Â  Â  88Â  Â  Â  Â  79Â  Â  Â  EVÂ  Â  EWRÂ  ALBÂ  Â  Â  29Â  Â  Â  143Â   23\n# 3: 2014Â  Â   1Â   4Â  Â  Â   220Â  Â  Â   211Â  Â  Â  EVÂ  Â  EWRÂ  ALBÂ  Â  Â  32Â  Â  Â  143Â   15\n# 4: 2014Â  Â   1Â   4Â  Â  Â  Â  35Â  Â  Â  Â  19Â  Â  Â  EVÂ  Â  EWRÂ  ALBÂ  Â  Â  32Â  Â  Â  143Â  Â  7\n# 5: 2014Â  Â   1Â   5Â  Â  Â  Â  47Â  Â  Â  Â  42Â  Â  Â  EVÂ  Â  EWRÂ  ALBÂ  Â  Â  26Â  Â  Â  143Â  Â  8\n# 6: 2014Â  Â   1Â   5Â  Â  Â  Â  66Â  Â  Â  Â  62Â  Â  Â  EVÂ  Â  EWRÂ  ALBÂ  Â  Â  31Â  Â  Â  143Â   23\nFilter by origin == â€œJFKâ€ and dest == â€œMIAâ€\nflights[.(\"JFK\", \"MIA\")]\n#Â  Â  Â  year month day dep_delay arr_delay carrier origin dest air_time distance hour\n#Â  Â  1: 2014Â  Â  1Â   1Â  Â  Â  Â  -1Â  Â  Â   -17Â  Â  Â  AAÂ  Â  JFKÂ  MIAÂ  Â  Â  161Â  Â  1089Â   15\n#Â  Â  2: 2014Â  Â  1Â   1Â  Â  Â  Â   7Â  Â  Â  Â  -8Â  Â  Â  AAÂ  Â  JFKÂ  MIAÂ  Â  Â  166Â  Â  1089Â  Â  9\n#Â  Â  3: 2014Â  Â  1Â   1Â  Â  Â  Â   2Â  Â  Â  Â  -1Â  Â  Â  AAÂ  Â  JFKÂ  MIAÂ  Â  Â  164Â  Â  1089Â   12\n#Â  Â  4: 2014Â  Â  1Â   1Â  Â  Â  Â   6Â  Â  Â  Â   3Â  Â  Â  AAÂ  Â  JFKÂ  MIAÂ  Â  Â  157Â  Â  1089Â  Â  5\n#Â  Â  5: 2014Â  Â  1Â   1Â  Â  Â  Â   6Â  Â  Â   -12Â  Â  Â  AAÂ  Â  JFKÂ  MIAÂ  Â  Â  154Â  Â  1089Â   17\n#Â  ---Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â \n# 2746: 2014Â  Â 10Â  31Â  Â  Â  Â  -1Â  Â  Â   -22Â  Â  Â  AAÂ  Â  JFKÂ  MIAÂ  Â  Â  148Â  Â  1089Â   16\n# 2747: 2014Â  Â 10Â  31Â  Â  Â  Â  -3Â  Â  Â   -20Â  Â  Â  AAÂ  Â  JFKÂ  MIAÂ  Â  Â  146Â  Â  1089Â  Â  8\n# 2748: 2014Â  Â 10Â  31Â  Â  Â  Â   2Â  Â  Â   -17Â  Â  Â  AAÂ  Â  JFKÂ  MIAÂ  Â  Â  150Â  Â  1089Â  Â  6\n# 2749: 2014Â  Â 10Â  31Â  Â  Â  Â  -3Â  Â  Â   -12Â  Â  Â  AAÂ  Â  JFKÂ  MIAÂ  Â  Â  150Â  Â  1089Â  Â  5\n# 2750: 2014Â  Â 10Â  31Â  Â  Â  Â  29Â  Â  Â  Â   4Â  Â  Â  AAÂ  Â  JFKÂ  MIAÂ  Â  Â  146Â  Â  1089Â   19\nFilter by only the first key column (origin): flights[\"JFK\"]\nFilter by only the second key column (dest)\nflights[.(unique(), \"MIA\")]\n#Â  Â  Â  year month day dep_delay arr_delay carrier origin dest air_time distance hour\n#Â  Â  1: 2014Â  Â  1Â   1Â  Â  Â  Â  -5Â  Â  Â   -17Â  Â  Â  AAÂ  Â  EWRÂ  MIAÂ  Â  Â  161Â  Â  1085Â   16\n#Â  Â  2: 2014Â  Â  1Â   1Â  Â  Â  Â  -3Â  Â  Â   -10Â  Â  Â  AAÂ  Â  EWRÂ  MIAÂ  Â  Â  154Â  Â  1085Â  Â  6\n#Â  Â  3: 2014Â  Â  1Â   1Â  Â  Â  Â  -5Â  Â  Â  Â  -8Â  Â  Â  AAÂ  Â  EWRÂ  MIAÂ  Â  Â  157Â  Â  1085Â   11\n#Â  Â  4: 2014Â  Â  1Â   1Â  Â  Â  Â  43Â  Â  Â  Â  42Â  Â  Â  UAÂ  Â  EWRÂ  MIAÂ  Â  Â  155Â  Â  1085Â   15\n#Â  Â  5: 2014Â  Â  1Â   1Â  Â  Â  Â  60Â  Â  Â  Â  49Â  Â  Â  UAÂ  Â  EWRÂ  MIAÂ  Â  Â  162Â  Â  1085Â   21\n#Â  ---Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â \n# 9924: 2014Â  Â 10Â  31Â  Â  Â   -11Â  Â  Â  Â  -8Â  Â  Â  AAÂ  Â  LGAÂ  MIAÂ  Â  Â  157Â  Â  1096Â   13\n# 9925: 2014Â  Â 10Â  31Â  Â  Â  Â  -5Â  Â  Â   -11Â  Â  Â  AAÂ  Â  LGAÂ  MIAÂ  Â  Â  150Â  Â  1096Â  Â  9\n# 9926: 2014Â  Â 10Â  31Â  Â  Â  Â  -2Â  Â  Â  Â  10Â  Â  Â  AAÂ  Â  LGAÂ  MIAÂ  Â  Â  156Â  Â  1096Â  Â  6\n# 9927: 2014Â  Â 10Â  31Â  Â  Â  Â  -2Â  Â  Â   -16Â  Â  Â  AAÂ  Â  LGAÂ  MIAÂ  Â  Â  156Â  Â  1096Â   19\n# 9928: 2014Â  Â 10Â  31Â  Â  Â  Â   1Â  Â  Â   -11Â  Â  Â  USÂ  Â  LGAÂ  MIAÂ  Â  Â  164Â  Â  1096Â   15\nFilter by origin and dest values, then select a arr.delay column: flights[.(\"LGA\", \"TPA\"), .(arr_delay)]\nFilter by origin and dest values, then summarize and pull maximum of arr_delay\nflights[.(\"LGA\", \"TPA\"), max(arr_delay)]\n# [1] 486\nFilter by origin value, group_by month, summarize( max(dep_delay))\nans &lt;- flights[\"JFK\", max(dep_delay), keyby = month]\nhead(ans)\n#Â  Â  monthÂ  V1\n# 1:Â  Â  1Â  881\n# 2:Â  Â  2 1014\n# 3:Â  Â  3Â  920\n# 4:Â  Â  4 1241\n# 5:Â  Â  5Â  853\n# 6:Â  Â  6Â  798\nkey(ans)\n# [1] \"month\"\n\nkeyby groups and sets the key to month\n\nFilter by three origin values, one dest value, return the last row for each match\nflights[.(c(\"LGA\", \"JFK\", \"EWR\"), \"XNA\"), mult = \"last\"]\n#Â  Â  year month day dep_delay arr_delay carrier origin dest air_time distance hour\n# 1: 2014Â  Â   5Â  23Â  Â  Â   163Â  Â  Â   148Â  Â  Â  MQÂ  Â  LGAÂ  XNAÂ  Â  Â  158Â  Â  1147Â  18\n# 2:Â   NAÂ  Â  NAÂ  NAÂ  Â  Â  Â  NAÂ  Â  Â  Â  NAÂ  Â    NAÂ  Â  JFKÂ  XNAÂ  Â  Â   NAÂ  Â  Â  NAÂ  NA\n# 3: 2014Â  Â   2Â   3Â  Â  Â   231Â  Â  Â   268Â  Â  Â  EVÂ  Â  EWRÂ  XNAÂ  Â  Â  184Â  Â  1131Â  12\n\nFiltering by more than one key value returns combinations of the first key and second key\nRemember setting a key reorders (increasing)\n\nIfelse using hour\nsetkey(flights, hour) # hour has values 0-24\nflights[.(24), hour := 0L]\n\nifelse(hour == 24, 0, TRUE)\nConsequence: since a key column value has changed, hour is no longer a key"
  },
  {
    "objectID": "qmd/data.table.html#sec-dt-pivot",
    "href": "qmd/data.table.html#sec-dt-pivot",
    "title": "data.table",
    "section": "Pivoting",
    "text": "Pivoting\n\npivot_longer and melt\n\nBasic\nrelig_income |&gt;\nÂ  pivot_longer(!religion, # keep religion as a column\nÂ  Â  Â  Â  Â  Â  Â  names_to = \"income\", # desired name for new column\nÂ  Â  Â  Â  Â  Â  Â  values_to = \"count\") # what data goes into the new column?\nmelt(DT, id.vars = \"religion\",\nÂ  Â  variable.name = \"income\",\nÂ  Â  value.name = \"count\",\nÂ  Â  variable.factor = FALSE) # added to keep output consistent with tidyr\nColumns have a common prefix and missing values are dropped\nbillboard |&gt;\nÂ  pivot_longer(\nÂ  Â  cols = starts_with(\"wk\"),\nÂ  Â  names_to = \"week\",\nÂ  Â  names_prefix = \"wk\",\nÂ  Â  values_to = \"rank\",\nÂ  Â  values_drop_na = TRUE\nÂ  )\nmelt(DT,\nÂ  Â  measure.vars = patterns(\"^wk\"),\nÂ  Â  variable.name = \"week\",\nÂ  Â  value.name = \"rank\",\nÂ  Â  na.rm = TRUE)\nMultiple variables stored in column names\nwho |&gt;\nÂ  Â  pivot_longer(\nÂ  Â  Â  cols = new_sp_m014:newrel_f65,\nÂ  Â  Â  names_to = c(\"diagnosis\", \"gender\", \"age\"),\nÂ  Â  Â  names_pattern = \"new_?(.*)_(.)(.*)\", # Whoa\nÂ  Â  Â  values_to = \"count\"\nÂ  Â  )\nDT[,melt(.SD, measure.vars = 5:60, value.name = \"count\")\n# returns a new variable column, which needs splitting up into more columns\nÂ  Â  Â  Â  Â  Â  ][,variable := gsub(\"new_?\",\"\", variable)\n# get rid of \"new_\" using gsub and replace it with nothing at all\nÂ  Â  Â  Â  Â  Â  Â  ][,c(\"diagnosis\", \"temp\") := tstrsplit(variable,\"_\", fixed = TRUE)\n# create a diagnosis column and a â€œtempâ€ column, which will be split up\nÂ  Â  Â  Â  Â  Â  Â  Â  ][, gender := tstrsplit(temp, \"[^mf]+\", fixed = FALSE)\n# split out the gender column from the new temp column that has values â€œmâ€ or â€œfâ€.\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  ][, age := tstrsplit(temp, \"[mf]+\", fixed = FALSE, keep = 2)\n# retrieve age from the temp column. This generates two vectors, but only want the second one, hence the keep = 2\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  ][, !c(\"variable\",\"temp\")\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ][,c(1:4,6:8,5)][]\n# last two lines discard the variable and temp columns, and sort the columns into the same order as that returned by pivot_longer\n\n##Â  Â  Â  Â  Â  Â  country iso2 iso3 year diagnosis gender age count\n##Â  Â  Â  1: AfghanistanÂ  AFÂ  AFG 1980Â  Â  Â  Â  spÂ  Â  Â  m 014Â  Â  NA\n##Â  Â  Â  2: AfghanistanÂ  AFÂ  AFG 1981Â  Â  Â  Â  spÂ  Â  Â  m 014Â  Â  NA\n##Â  Â  Â  3: AfghanistanÂ  AFÂ  AFG 1982Â  Â  Â  Â  spÂ  Â  Â  m 014Â  Â  NA\n##Â  Â  Â  4: AfghanistanÂ  AFÂ  AFG 1983Â  Â  Â  Â  spÂ  Â  Â  m 014Â  Â  NA\n##Â  Â  Â  5: AfghanistanÂ  AFÂ  AFG 1984Â  Â  Â  Â  spÂ  Â  Â  m 014Â  Â  NA\n##Â  Â  ---Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â \n## 405436:Â  Â  ZimbabweÂ  ZWÂ  ZWE 2009Â  Â  Â   relÂ  Â  Â  fÂ  65Â  Â  NA\n## 405437:Â  Â  ZimbabweÂ  ZWÂ  ZWE 2010Â  Â  Â   relÂ  Â  Â  fÂ  65Â  Â  NA\n## 405438:Â  Â  ZimbabweÂ  ZWÂ  ZWE 2011Â  Â  Â   relÂ  Â  Â  fÂ  65Â  Â  NA\n## 405439:Â  Â  ZimbabweÂ  ZWÂ  ZWE 2012Â  Â  Â   relÂ  Â  Â  fÂ  65Â  Â  NA\n## 405440:Â  Â  ZimbabweÂ  ZWÂ  ZWE 2013Â  Â  Â   relÂ  Â  Â  fÂ  65Â   725\nMatrix to long\nanscombe |&gt;\nÂ  pivot_longer(\nÂ  Â  everything(),\nÂ  Â  cols_vary = \"slowest\",\nÂ  Â  names_to = c(\".value\", \"set\"),\nÂ  Â  names_pattern = \"(.)(.)\"Â \nÂ  )\nDT[,melt(.SD,\nÂ  Â  Â  Â  Â  Â  variable.name = \"set\",\nÂ  Â  Â  Â  Â  Â  value.name = c(\"x\",\"y\"),\nÂ  Â  Â  Â  Â  Â  variable.factor = FALSE,\nÂ  Â  Â  Â  Â  Â  measure.vars = patterns(\"^x\",\"^y\"))]\n\n\n\npivot_wider and dcast\n\nData in examples\n\nfish_encounters\n## # A tibble: 114 Ã— 3\n##Â  Â  fishÂ  stationÂ  seen\n##Â  Â  &lt;fct&gt; &lt;fct&gt;Â    &lt;int&gt;\n##Â  1 4842Â  ReleaseÂ  Â   1\n##Â  2 4842Â  I80_1Â  Â  Â   1\n##Â  3 4842Â  LisbonÂ  Â  Â  1\n##Â  4 4842Â  RstrÂ  Â  Â  Â  1\n##Â  5 4842Â  Base_TDÂ  Â   1\n##Â  6 4842Â  BCEÂ  Â  Â  Â   1\n##Â  7 4842Â  BCWÂ  Â  Â  Â   1\n##Â  8 4842Â  BCE2Â  Â  Â  Â  1\n##Â  9 4842Â  BCW2Â  Â  Â  Â  1\n## 10 4842Â  MAEÂ  Â  Â  Â   1\n## # â€¦ with 104 more rows\n\nBasic\nfish_encounters |&gt;\nÂ  pivot_wider(names_from = station, values_from = seen)\n\ndcast(DT, fish ~ station, value.var = \"seen\")\nFill in missing values\nfish_encounters |&gt;\nÂ  pivot_wider(names_from = station, values_from = seen, values_fill = 0)\n\ndcast(DT, fish ~ station, value.var = \"seen\", fill = 0)\n# alt\nDT[, dcast(.SD, fish ~ station, value.var = \"seen\", fill = 0)]\n\nRather than have the DT inside dcast, we can use .SD and have dcast inside DT, which is helpful for further chaining. (see applied to melt above)\n\nGenerate column names from multiple variables\nus_rent_income |&gt;\nÂ  pivot_wider(\nÂ  Â  names_from = variable,\nÂ  Â  values_from = c(estimate, moe)\nÂ  )\n\ndcast(DT, GEOID + NAME ~ variable,Â \nÂ  Â  Â  Â  Â  value.var = c(\"estimate\",\"moe\"))\n# alt\ndcast(DT, ... ~ variable,Â \nÂ  Â  Â  value.var = c(\"estimate\",\"moe\"))\n\nAlternative: pass â€œâ€¦â€ to indicate all other unspecified columns\n\nSpecify a different names separator\nus_rent_income |&gt;\nÂ  pivot_wider(\nÂ  Â  names_from = variable,\nÂ  Â  names_sep = \".\",\nÂ  Â  values_from = c(estimate, moe)\nÂ  )\n\ndcast(DT, GEOID + NAME ~ variable,\nÂ  Â  Â  value.var = c(\"estimate\",\"moe\"),Â \nÂ  Â  Â  sep = \".\")\n# alt\nDT[, dcast(.SD, GEOID + NAME ~ variable,\nÂ  Â  value.var = c(\"estimate\",\"moe\"),Â \nÂ  Â  Â  Â  Â  sep = \".\")]\n\nAlternative: Rather than have the DT inside dcast, we can use .SD and have dcast inside DT, which is helpful for further chaining. (see applied to melt above)\n\nControlling how column names are combined\nus_rent_income |&gt;\nÂ  pivot_wider(\nÂ  Â  names_from = variable,\nÂ  Â  values_from = c(estimate, moe),\nÂ  Â  names_vary = \"slowest\"\nÂ  ) |&gt; names()\n\nDT[, dcast(.SD, GEOID + NAME ~ variable,\nÂ  Â  Â  Â  Â  value.var = c(\"estimate\",\"moe\"))\nÂ  ][,c(1:3,5,4,6)] |&gt; names()\n\n## [1] \"GEOID\"Â  Â  Â  Â  Â  \"NAME\"Â  Â  Â  Â  Â  Â  \"estimate_income\" \"moe_income\"Â  Â  Â \n## [5] \"estimate_rent\"Â  \"moe_rent\"\n\nSee {tidyr::pivot_wider} docs and the names_vary arg\n\nAggregation\nwarpbreaks %&gt;%\nÂ  pivot_wider(\nÂ  Â  names_from = wool,\nÂ  Â  values_from = breaks,\nÂ  Â  values_fn = mean\nÂ  )\ndcast(DT, tension ~ wool,Â \nÂ  Â  Â  Â  Â  value.var = \"breaks\", fun = mean)\n# alt\nDT[, dcast(.SD, tension ~ wool,Â \nÂ  Â  Â  value.var = \"breaks\", fun = mean)]\n\n## # A tibble: 3 Ã— 3\n##Â  tensionÂ  Â  AÂ  Â  B\n##Â  &lt;fct&gt;Â  &lt;dbl&gt; &lt;dbl&gt;\n## 1 LÂ  Â  Â  Â  44.6Â  28.2\n## 2 MÂ  Â  Â  Â  24Â  Â  28.8\n## 3 HÂ  Â  Â  Â  24.6Â  18.8\n\nAlternative: Rather than have the DT inside dcast, we can use .SD and have dcast inside DT, which is helpful for further chaining. (see applied to melt above)"
  },
  {
    "objectID": "qmd/data.table.html#sec-dt-rec",
    "href": "qmd/data.table.html#sec-dt-rec",
    "title": "data.table",
    "section": "Recipes",
    "text": "Recipes\n\nOperations covered in this section\n\ngroup_by, summarize (and arrange)\ncrosstab\n\ngroup_by, summarize (and arrange)\ndt_res &lt;- dtstudy[, .(n = .N, avg = round(mean(y), 1)), keyby = .(male, over65, rx)]\n\ntb_study &lt;- tibble::as_tibble(dtstudy)\ntb_res &lt;- tb_study |&gt;\nÂ  summarize(n = n(),\nÂ  Â  Â  Â  Â  Â  avg = round(mean(y), 1),\nÂ  Â  Â  Â  Â  Â  .by = c(male, over65, rx)) |&gt;\nÂ  arrange(male, over65, rx)\n\ndt automatically orders by the grouping variables, so to get the exact output, you have to add an arrange\n\nCrosstab using cube (Titanic5 dataset)\n# Note that the mean of a 0/1 variable is the proportion of 1s\nmn &lt;- function(x) mean(x, na.rm=TRUE)\n# Create a function that counts the number of non-NA values\nNna &lt;- function(x) sum(! is.na(x))\n\ncube(d, .(Proportion=mn(survived), N=Nna(survived)), by=.q(sex, class), id=TRUE)\n\n#&gt; Â  Â  groupingÂ  Â  sex class ProportionÂ  Â  N\n#&gt; 1:Â  Â  Â  Â   0 femaleÂ  Â   1Â  0.9652778Â  144\n#&gt; 2:Â  Â  Â  Â   0Â   maleÂ  Â   1Â  0.3444444Â  180\n#&gt; 3:Â  Â  Â  Â   0Â   maleÂ  Â   2Â  0.1411765Â  170\n#&gt; 4:Â  Â  Â  Â   0 femaleÂ  Â   2Â  0.8867925Â  106\n#&gt; 5:Â  Â  Â  Â   0Â   maleÂ  Â   3Â  0.1521298Â  493\n#&gt; 6:Â  Â  Â  Â   0 femaleÂ  Â   3Â  0.4907407Â  216\n#&gt; 7:Â  Â  Â  Â   1 femaleÂ  Â  NAÂ  0.7274678Â  466\n#&gt; 8:Â  Â  Â  Â   1Â   maleÂ  Â  NAÂ  0.1909846Â  843\n#&gt; 9:Â  Â  Â  Â   2Â   &lt;NA&gt;Â  Â   1Â  0.6203704Â  324\n#&gt; 10:Â  Â  Â  Â  2Â   &lt;NA&gt;Â  Â   2Â  0.4275362Â  276\n#&gt; 11:Â  Â  Â  Â  2Â   &lt;NA&gt;Â  Â   3Â  0.2552891Â  709\n#&gt; 12:Â  Â  Â  Â  3Â   &lt;NA&gt;Â  Â  NAÂ  0.3819710 1309"
  },
  {
    "objectID": "qmd/db-dbt.html",
    "href": "qmd/db-dbt.html",
    "title": "9Â  DB, dbt",
    "section": "",
    "text": "TOC\n\nMisc\nSet-up\nDescription\nOptimizations\nComponents\nPackages\nData Validation and Unit Tests\n\nMisc\n\nList of available DB Adaptors\n\nRuns on Python, so adaptors are installed via pip Notes from\nAnatomy of a dbt project\nWhat is dbt? Docs\n\nResources\n\nCreate a Local dbt Project\n\nUses docker containers to set up a local dbt project and a local postgres db to play around with Typical Workflow\n\n\n\ndbt deps\ndbt seed\ndbt snapshot\ndbt run\ndbt run-operation {{ macro_name }}\ndbt test\n\nStyle Guide Components\n\nNaming conventions (the case to use and tense of the column names)\nSQL best practices (commenting code, CTEs, subqueries, etc.)\nDocumentation standards for your models\nData types of date, timestamp, and currency columns\nTimezone standards for all dates\n\nCastor - tool that takes your project and autofills much of the documentation\n\nHas a free tier\nVery helpful if you have the same column name in multiple datasets, you donâ€™t have to keep defining it\nTribal Knowledge\n\nWhen a dataset is discussed in a team slack channel, Castor pulls the comments and adds them to the documentation of the dataset\n\n\nLightdash - BI tool for dbt projects - free tier for self hosting\n\nSet-up\n\nBasic set-up: Article\n\nExample uses postgres adaptor\n\nWithin a python virtual environment\n\nCreate: python3 -m vevn dbt-venv\nActivate: source dbt-venv/bin/activate\n\nShould be able to see a (dbt-venv) prefix in every line on the terminal\n\nInstall dbt-core: pip install dbt-core\n\nSpecific version: pip install dbt-core==1.3.0\nConfirm installation by checking version: dbt --version\n\nInstall plugins\n\n\npip install dbt-bigquery\npip install dbt-spark\n# etc...\nDescription\nbuilt for data modeling\n    **models** are like sql queries\n    \nmodularizes SQL code and makes it reusable across \"models\"\n![](./_resources/DB,_dbt.resources/1_uf-Jo4ybZT0qdsCHNZavGg (2).png)\n* Running the orders \"model\" also runs the base\\_orders model and base\\_payments model (i.e. dependencies for orders)\n    * Not sure this exactly right. Seems like doing this would result in wasting time rerunning the same dependencies multiple times\n* base\\_orders and base\\_payments are independent in that they can also be used in other models\n* creates more dependable code because youâ€™re using the same logic in all your models\n* Makes runs faster since you arenâ€™t wasting time and resources running the same blocks of code over and over again\n\nYou can schedule running sets of models by tagging them (e.g.Â #daily, #weekly)\nVersion Control\n\nsnapshots provide mechanism for versioning datasets\nWithin every yaml file is an option to include the version\n\npackage add-ons that allow you to interact with spark, snowflake, duckdb, redshift, etc.\nDocumentation for every step of the way\n\n.yml files can be used to generate a website (localhost:8080) around all of your dbt documentation.\n\n\ndbt docs generate\ndbt docs serve\nCurrent understanding\n    Data is brought in from warehouses via base models and basic transformations are performed (models &gt;&gt; staging directory)\n    \n    Then the data is transformed to the desired state via intermediate models and calculations performed (models &gt;&gt; marts directory)\n    \n    Then the final product is stored in the data directory\n    \nOptimizations\n\nusing an M1, the new Apple laptops with Appleâ€™s own CPUs improved speed by 3x\nupgrading from dbt 0.15.0 -&gt; 0.20.0 resulted in another 3x speed increase\nmoving dbt out of a container resulted in a 2x speed increase for those using a Intel CPU MacBook Pro\nRuns parallelized\n\nModels that have dependencies arenâ€™t run until their upstream models are completed but models that donâ€™t depend on one another are run at the same time.\nthreadÂ  parameter in your dbt_project.yml specifies how many models are permitted to run in parallel\n\nBigQuery\n\n\n\n\nComponents\nProject Templates\n    [Style Guide](https://github.com/dbt-labs/corp/blob/master/dbt_style_guide.md)\n        More detailed: [link](https://discourse.getdbt.com/t/how-we-structure-our-dbt-projects/355)\n        \n    Example - [Starter Project](https://github.com/dbt-labs/dbt-init/tree/master/starter-project)\n    ![](./_resources/DB,_dbt.resources/1_bgUI3LTLdY19EFRwSyZh1A (2).png)\n* \n\nprofiles.yml\n\nNot included in project directory\nOnly have to worry about this file if you set up dbt locally.\ndoc\nCreated by dbt init in ~/.dbt/\nContents\n\ndatabase connection, database credentials that dbt will use to connect to the data warehouse\nIf you work on multiple projects locally, the different project names (configured in the dbt_project.yml file) will allow you to set up various profiles for other projects.\nâ€¦ something about â€œtargetsâ€ but not sure what this is or how itâ€™s used\n\n\ndbt_project.yml\n\ndoc\nmain configuration file for your project\nFields where you need to change default values to your project name:\n\nproject name, profile name\nvariables\n\nDefined in the project yaml and used in models\n\nAccessed using var\n\nExample: Assigning States to Regions\n\nDefine variables in dbt_project.yml\n\n\n\n\n\nvars:\nÂ  state_lookup:\nÂ  Â  Northeast:\nÂ  Â  Â  - CT\nÂ  Â  Â  - ME\nÂ  Â  Midwest:\nÂ  Â  Â  - IL\nÂ  Â  Â  - IN\n\nUsing the variables in a model\n\n{# Option 1 #}\nSELECT state,\nÂ  Â  Â  CASE {% for k, v in var(\"state_lookup\").items() %}\nÂ  Â  Â  Â  Â  Â  WHEN state in ({% for t in v %}'{{ t }}'{% if not loop.last %}, {% endif %}{% endfor %}) THEN {{ k }}{% endfor %}\nÂ  Â  Â  Â  Â  Â  ELSE NULL END AS region\nÂ  FROM {{ ref('my_table') }}\n\n{# Option 2 #}\nSELECT state,\nÂ  Â  Â  CASE {% for k, v in var(\"state_lookup\").items() %}\nÂ  Â  Â  Â  Â  Â  WHEN state in ({{ t|csl }}) THEN {{ k }}{% endfor %}\nÂ  Â  Â  Â  Â  Â  ELSE NULL END AS region\nÂ  FROM {{ ref('my_table') }}\n\nThis is a complicated example, see docs for something simpler\n{% ... %} are used to encapsulate for-loops and if-then conditions, see docs\n\n{# ... #}Â  is for comments\n\nOption 2 uses a csl filter (comma-separated-list)\nmodels section\n\nmy_new_project\n\nmodels Misc * key components of a well-written data model * modularity where possible * Same as the functional mindset: â€œif thereâ€™s any code thatâ€™s continually repeated, then it should be a function(i.e.Â its own separate model in dbt).â€ * readability * Comment * Use CTEs instead of subqueries * Use descriptive names * Example: if you are joining the tables â€œusersâ€ and â€œaddressesâ€ in a CTE, you would want to name it â€œusers_joined_addressesâ€ instead of â€œuser_addressesâ€ * Example: Comments, CTE, Descriptive Naming\n\nWITH\nActive_users AS (\nÂ  SELECT\nÂ  Â  Name AS user_name,\nÂ  Â  Email AS user_email,\nÂ  Â  Phone AS user_phone,\nÂ  Â  Subscription_id\nÂ  FROM users\nÂ  --- status of 1 means a subscription is active\nÂ  WHERE subscription_status = 1\n),\nActive_users_joined_subscriptions AS (\nÂ  SELECT\nÂ  Â  Active_users.user_name,\nÂ  Â  active_users.user_email,\nÂ  Â  Subscriptions.subscription_id,\nÂ  Â  subscriptions.start_date ,\nÂ  Â  subscriptions.subscription_length\nÂ  FROM active_users\nÂ  LEFT JOIN subscriptions\nÂ  Â  ON active_users.subscription_id = subscriptions.subscription_id\n)\nSELECT * FROM Active_users_joined_subscriptions\nModel categories: staging, marts, base/intermediate\n* Staging\n    * Contains all the individual components of your project that the other layers will use in order to craft more complex data models.\n    * Each model bears a one-to-one relationship with the source data table it represents (i.e. 1 staging model per source)\n    * Typical Transformations: recasting, column renaming, basic computations (such as KBs to MBs or GBs), categorization (e.g. using CASE WHEN statements).\n        * Aggregations and joins should also be avoided\n    * Usually materialized as views.\n        * Allows any intermediate or mart models referencing the staging layer to get access to fresh data and at the same time it saves us space and reduces costs.\n    * Example\n        * Both the Stripe and Braintree payments are recast into a consistent shape, with consistent column names.\n* Marts\n    * Where everything comes together in a way that business-defined entities and processes are constructed and made readily available to end users via dashboards or applications.\n    * Since this layer contains models that are being accessed by end users it means that performance matters. Therefore, it makes sense to materialize them as tables.\n        * If a table takes too much time to be created (or perhaps it costs too much), then you may also need to consider configuring it as an incremental model.\n    * A mart model should be relatively simple and therefore, too many joins should be avoided\n    * Example\n        * A monthly recurring revenue (MRR) model that classifies revenue per customer per month as new revenue, upgrades, downgrades, and churn, to understand how a business is performing over time.\n            * It may be useful to note whether the revenue was collected via Stripe or Braintree, but they are not fundamentally separate models.\n    Base/Intermediate\n        Base\n            basic transformations (e.g. cleaning up the names of the columns, casting to different data types)\n            \n            Other models use these models as data sources\n                prevents errors like accidentally casting your dates to two different types of timestamps, or giving the same column two different names.\n                    two different timestamp castings can cause all of the dates to be improperly joined downstream, turning the model into a huge disaster\n                    \n            Usually occuring in staging\n            \n            read directly from a source, which is typically a schema in your data warehouse\n                Source object\n                    `{{ source('campaigns', 'channel'){style='color: goldenrod'}[}}]{style='color: goldenrod'}`\n                    \n                * campaigns is the name of the source in the .yml file\n                * channel is the name of a table from that source\n        Intermediate\n            Brings together the atomic building blocks that reside on staging layer such that more complex and meaningful models are constructed\n            \n            Usually occuring in marts\n            \n            Additional transformations that particular marts-models require\n            * Created to isolate complex operations\n                Typically used for joins between multiple base models\n                \n        * Should not be directly exposed to end users via dashboards or applications\n        * Other models should reference them as Common Table Expressions although there may be cases where it makes sense to materialize them as Views\n            * Macros called via `run-operation` cannot reference ephemeral objects such as CTEs\n            * Recommended to start with ephemeral objects unless this doesnâ€™t work for the specific use case\n            * Whenever you decide to materialize them as Views, it may be easier to to do so in a [custom schema](https://docs.getdbt.com/docs/build/custom-schemas), that is a schema outside of the main schema defined in your dbt profile.\n        * If the same intermediate model is referenced by more than one model then it means your design has probably gone wrong.\n            * Usually indicates that you should consider turning your intermediate model into a macro.\n            reference the base models rather than from a source\n            * Reference object\n                * `{{ ref('base_campaign_types'){style='color: goldenrod'}[}}]{style='color: goldenrod'}`\n                * base\\_campaign\\_types is a base model\n\nTagging\n\nAllows you to run groups of models\n\nExample dbt run --models tag:daily\n\n\nDirectories models Sources (i.e.Â data sources) are defined in src_ .yml files in your models directory * .yml files contain definitions and tests * .doc files contain source documentation * Models (i.e.Â sql queries) are defined stg__yml * .yml files contain definitions and tests * .doc files contain source documentation * The actual models are the .sql files Example  * staging: * Different data sources will have separate folders underneath staging (e.g.Â stripe). * marts: * Use cases or departments have different folders underneath marts (e.g.Â core or marketing) data contains all manual data that will be loaded to the database by dbt.\n      To load the .csv files in this folder to the database, you will have to run the `dbt seed` command.\n\n      For github or other repos, do not put large files or files with sensitive information here\n          acceptable use cases: yearly budget, status mappings, category mappings, etc\n\nsnapshots\n\ncaptures of the state of a table at a particular time\ndoc\nbuild a slowly changing dimension (SCD) table for sources that do not support change data capture (CDC)\nExample\n\nEvery time the status of an order change, your system overrides it with the new information. In this case, there we cannot know what historical statuses that an order had.\nDaily snapshots of this table builds a history and allows you to track order statuses\n\n\n\nMacros\n\nsimilar to functions in excel\ndefine custom functions in the macros folder or override default macros and macros from a package\nSee bkmks for tutorials on writing custom macros with jinja\n{dbtplyr} macros\n\ndplyr tidy selectors, across, etc.\n\n\nSeeds\n\nSeeds are csv files that you add to your dbt project to be uploaded to your data warehouse.\n\nUploaded into your data warehouse using the dbt seed command\n\nBest suited to static data which changes infrequently.\n\nExamples of use cases:\n\nA list of unique codes or employee ids that you may need in your analysis but is not present in your current data.\nA list of mappings of country codes to country names\nA list of test emails to exclude from analysis\n\n\nReferenced in downstream models the same way as referencing models â€” by using the ref function\n\n\nPackages\n\npackages.yml\n\nlist of external dbt packages you want to use in your project\n\navailable packages: link\nformat\n\nÂ  Â  Â  Â  packages:\nÂ  Â  Â  Â  Â  Â  - package: dbt-labs/dbt_utils\nÂ  Â  Â  Â  Â  Â  Â  version: 0.7.3\n\nInstall packages - dbt deps\nsome of the packages (there are a lot of packages and I didnâ€™t get through them all)\n\n{audit-helper}\n\ncompares columns, queries; useful if refactoring code or migrating db\n\n{codegen}\n\ngenerate base model, barebones model and source .ymls\n\n{dbt-athena} - adaptor for AWS Athena\n{dbt-expectations}\n\ndata validation based on great expectations py lib\n\n{dbt-utils}\n\nton of stuff for tests, queries, etc.\n\n{dbtplyr} macros\n\ndplyr tidy selectors, across, etc.\n\n{dbt-duckdb} - adapter for duckdb\n{external-tables}\n\ncreate/replace/refresh external tables\nGuessing this means any data source (e.g.Â s3, spark, google, another db like snowflake, etc.) that isnâ€™t the primary db connected to the dbt project\n\n{logging}\n\nprovides out-of-the-box functionality to log events for all dbt invocations, including run start, run end, model start, and model end.\ncan slow down runs substantially\n\n{re_data}\n\ndashboard for monitoring, macros, models\n\n{profiler}\n\nimplements dbt macros for profiling database relations and creating doc blocks and table schemas (schema.yml) containing said profiles\n\n{spark-utils}\n\nenables use of (most of) the {dbt-utils} macros on spark\n\n\n\nData Validation and Unit Tests \n\nBuilt-in support for CI/CD pipelines to test your â€œmodelsâ€ and stage them before committing to production\n\nMisc\n\nSee also\n\nDocs\npackages (see above)\nHow to do Unit Testing in dbt\n\n\n\nMost of the tests are defined in a models-type .yml file in the models directory\nUses pre-made or custom macros\n\ncustom (aka singular) tests should be located in a tests folder.\n\ndbt will evaluate the SQL statement.\nThe test will pass if no row is returned and failed if at least one or more rows are returned.\nUseful for testing for some obscurity in the data\nExample: Check for duplicate rows when joining two tables\n\n\n\nselectÂ \na.idÂ \nfrom {{ ref(â€˜table_aâ€™) }} aÂ \nleft join {{ ref(â€˜table_bâ€™) }} bÂ \non a.b_id = b.idÂ \ngroup by a.idÂ \nhaving count(b.id)&gt;1\n\ni.e.Â If I join table a with table b, there should only be one record for each unique id in table a\nProcess\n\njoin the tables on their common field\ngroup them by the id that should be distinct\ncount the number of duplicates created from the join.\n\nThis tells me that something is wrong with the data.\nadd a having clause to filter out the non-dups\n\n\nCan be applied to a model or a column\nRun test - dbt test\nMock data\n\nData used for unit testing SQL code\nTo ensure completeness, itâ€™s best if analysts or business stakeholders are the ones provide test cases or test data\nStore in the â€œdataâ€ folder (typically .csv files)\n\neach CSV file represents one source table\nshould be stored in a separate schema (e.g.Â unit_testing) from production data\ndbt seed (see below, Other &gt;&gt; seeds) command is used to load mock data into the data warehouse\n\n\nTests\n\nfreshness (docs) - used to define the acceptable amount of time between the most recent record, and now, for a table\n\nExample\n\n\n\nsources:\nÂ  - name: users\nÂ  Â  freshness:\nÂ  Â  Â  warn_after:\nÂ  Â  Â  Â  count: 3\nÂ  Â  Â  Â  period: day\nÂ  Â  Â  error_after:\nÂ  Â  Â  Â  count: 5\nÂ  Â  Â  Â  period: day\n\nExample\nExample: Unit Test\n\nFig shows the mock data (.csv) files\nAdd test to dbt_project.yml\n\n\nseeds:\nÂ  unit_testing:\nÂ  Â  revenue:\nÂ  Â  Â  schema: unit_testing\nÂ  Â  Â  +tags:\nÂ  Â  Â  Â  - unit_testing\n\nEvery file in the unit_testing/revenue folder will be loaded into unit_testing\nExecuting dbt build -s +tag:unit_testing will run all the seeds/models/tests/snapshots with tag unit_testing and their upstreams\nCreate macro that switches the source data in the model being tested from production data (i.e.Â using {{ source() }} ) to mock data (i.e.Â using ref ) when a unit test is being run\n\n{% macro select_table(source_table, test_table) %}\nÂ  Â  Â  {% if var('unit_testing', false) == true %}\n\nÂ  Â  Â  Â  Â  Â  {{ return(test_table) }}\nÂ  Â  Â  {% else %}\nÂ  Â  Â  Â  Â  Â  {{ return(source_table) }}\nÂ  Â  Â  {% endif %}\n{% endmacro %}\n\nArticle calls this file â€œselect_table.sqlâ€\n2 inputs: â€œsource_tableâ€ (production data) and â€œtest_tableâ€ (mock data)\nmacro returns the appropriate table based on the variable in the dbt command\n\nIf the command doesnâ€™t provide unit_testing variable or the value is false , then it returns source_table , otherwise it returns test_table.\n\nAdd macro code chunk to model\n\n{{ config\nÂ  Â  (\nÂ  Â  Â  Â  materialized='table',\nÂ  Â  Â  Â  tags=['revenue']\nÂ  Â  )\n}}\n{% set import_transaction = select_table(source('user_xiaoxu','transaction'), ref('revenue_transaction')) %}\n{% set import_vat = select_table(source('user_xiaoxu','vat'), ref('revenue_vat')) %}\nSELECT\nÂ  Â  date\nÂ  Â  , city_name\nÂ  Â  , SUM(amount_net_booking) AS amount_net_booking\nÂ  Â  , SUM(amount_net_booking * (1 - 1/(1 + vat_rate)))Â  AS amount_vat\nFROM {{ import_transaction }}\nLEFT JOIN {{ import_vat }} USING (city_name)\nGROUP BY 1,2\n\nInside the {%...%} , the macro â€œselect_tableâ€ is called to set the local variables, â€œimport_transactionâ€ and â€œimport_vatâ€ which are later used in the model query\nModel file is named â€œrevenue2.sqlâ€\nRun model and test using mock data: dbt build -s +tag:unit_testing --vars 'unit_testing: true'\n\nRun model with production data (aka source data): dbt build -s +tag:revenue --exclude tag:unit_testing\n\nCompare output\n\nversion: 2\nmodels:\nÂ  - name: revenue\nÂ  Â  meta:\nÂ  Â  Â  owner: \"@xiaoxu\"\nÂ  Â  tests:\nÂ  Â  Â  - dbt_utils.equality:\nÂ  Â  Â  Â  Â  compare_model: ref('revenue_expected')\nÂ  Â  Â  Â  Â  tags: ['unit_testing']\n\nmodel properties file thatâ€™s named â€œrevenue.ymlâ€ in the models directory\nBy including tags: ['unit_testing'] we can insure that we donâ€™t run this test in production (see build code above with --exclude tag:unit_testing\nMacro for comparing numeric output\n\n{% test advanced_equality(model, compare_model, round_columns=None) %}\n{% set compare_columns = adapter.get_columns_in_relation(model) | map(attribute='quoted') %}\n{% set compare_cols_csv = compare_columns | join(', ') %}\n{% if round_columns %}\nÂ  Â  {% set round_columns_enriched = [] %}\nÂ  Â  {% for col in round_columns %}\nÂ  Â  Â  Â  {% do round_columns_enriched.append('round('+col+')') %}\nÂ  Â  {% endfor %}\nÂ  Â  {% set selected_columns = '* except(' + round_columns|join(', ') + \"), \" + round_columns_enriched|join(', ') %}\n{% else %}\nÂ  Â  {% set round_columns_csv = None %}\nÂ  Â  {% set selected_columns = '*' %}\n{% endif %}\nwith a as (\nÂ  Â  select {{compare_cols_csv}} from {{ model }}\n),\nb as (\nÂ  Â  select {{compare_cols_csv}} from {{ compare_model }}\n),\na_minus_b as (\nÂ  Â  select {{ selected_columns }} from a\nÂ  Â  {{ dbt_utils.except() }}\nÂ  Â  select {{ selected_columns }} from b\n),\nb_minus_a as (\nÂ  Â  select {{ selected_columns }} from b\nÂ  Â  {{ dbt_utils.except() }}\nÂ  Â  select {{ selected_columns }} from a\n),\nunioned as (\nÂ  Â  select 'in_actual_not_in_expected' as which_diff, a_minus_b.* from a_minus_b\nÂ  Â  union all\nÂ  Â  select 'in_expected_not_in_actual' as which_diff, b_minus_a.* from b_minus_a\n)\nselect * from unioned\n{% endtest %}\n\nFile called â€œadvanced_equality.sqlâ€"
  },
  {
    "objectID": "qmd/db-duckdb.html#sec-db-duckdb-misc",
    "href": "qmd/db-duckdb.html#sec-db-duckdb-misc",
    "title": "DuckDB",
    "section": "Misc",
    "text": "Misc\n\nHigh performance embedded database for analytics which provides a few enhancements over SQLite such as increased speed and allowing a larger number of columns\n\nFaster than sqlite for most analytics queries (sums, aggregates etc).\n\nVectorizes query executions (columnar-oriented), while other DBMSs (SQLite, PostgreSQLâ€¦) process each row sequentially\n\nAlso has fast JSON extension\n\nExample: from hrbrmstr drop\nINSTALL 'json';\nLOAD 'json';\n\nCOPY (\n  SELECT * FROM (\n    SELECT DISTINCT\n      cve_id,\n      unnest(\n        regexp_split_to_array(\n          concat_ws(\n            ',',\n            regexp_extract(case when cweId1 IS NOT NULL THEN cweId1 ELSE regexp_replace(json_extract_string(problem1, '$.description'), '[: ].*$', '') END, '^(CWE-[0-9]+)', 0),\n            regexp_extract(case when cweId2 IS NOT NULL THEN cweId2 ELSE regexp_replace(json_extract_string(problem2, '$.description'), '[: ].*$', '') END, '^(CWE-[0-9]+)', 0)\n          ),\n          ','\n        )\n      ) AS cwe_id\n    FROM (\n      SELECT \n        json_extract_string(cveMetadata, '$.cveId') AS cve_id, \n        json_extract(containers, '$.cna.problemTypes[0].descriptions[0]') AS problem1,\n        json_extract(containers, '$.cna.problemTypes[0].descriptions[1]') AS problem2,\n        json_extract_string(containers, '$.cna.problemTypes[0].cweId[0]') AS cweId1,\n        json_extract_string(containers, '$.cna.problemTypes[0].cweId[1]') AS cweId2\n      FROM \n        read_json_auto(\"/data/cvelistV5/cves/*/*/*.json\", ignore_errors = true) \n    )\n    WHERE \n      (json_extract_string(problem1, '$.type') = 'CWE' OR\n       json_extract_string(problem2, '$.type') = 'CWE')\n    )\n  WHERE cwe_id LIKE 'CWE-%'\n) TO '/data/summaries/cve-to-cwe.csv' (HEADER, DELIMETER ',')\n\nProcesses a nested json\nClones the CVE list repo, modify the directory paths and run it. It burns through nearly 220K hideous JSON files in mere seconds, even with some complex JSON operations.\n\n\nUnlike some other big data tools it is entirely self-contained. (aka embedded, in-process)\n\nno external dependencies, or server software to install, update, or maintain\n\n\nCan directly run queries on Parquet files, CSV files, SQLite files, postgres files, Pandas, R and Julia data frames as well as Apache Arrow sources\nVS Code extension\n\nConnect to a local DuckDB instance\nCreate new in-memory DuckDB instance\nView DuckDB tables, columns, and views\nRun SQL queries on open DuckDB connections\nAttach SQLite database files to in-memory DuckDB instances\nQuery remote CSV and Parquet data files with DuckDB HTTPFS extension\nCreate in-memory DuckDB tables from remote data sources and query results\nManage DuckDB connections in SQLTools Database Explorer\nAutocomplete SQL keywords, table names, column names, and view names on open database connections in VSCode SQL editor\nSave named SQL query Bookmarks\nUse SQL Query History\nExport SQL query results in CSV and JSON data formats\nintegrate with the equally spiffy SQL Tools extension"
  },
  {
    "objectID": "qmd/db-duckdb.html#sec-db-duckdb-setup",
    "href": "qmd/db-duckdb.html#sec-db-duckdb-setup",
    "title": "DuckDB",
    "section": "Set-up",
    "text": "Set-up\n\nInstallation: install.packages(\"duckdb\")\nCreate db and populate table from csv\n\nExample \nExample\n# includes filename/id\nwithr::with_dir(\"data-raw/files/\", {\n  dbSendQuery(\n    con, \"\n    CREATE TABLE files AS\n    SELECT *, regexp_extract(filename, '\\\\d{7}') AS file_number\n    FROM read_csv_auto('*Control*File-*.txt', FILENAME = TRUE);\"\n  )\n})"
  },
  {
    "objectID": "qmd/db-duckdb.html#sec-db-duckdb-dbplyr",
    "href": "qmd/db-duckdb.html#sec-db-duckdb-dbplyr",
    "title": "DuckDB",
    "section": "dbplyr",
    "text": "dbplyr\n\nExample Connect to db; Write a df to table; Query it\nlibrary(dbplyr)\n\nduck = DBI::dbConnect(duckdb::duckdb(), dbdir=\"duck.db\", read_only=FALSE)\nDBI::dbWriteTable(duck, name = \"sales\", value = sales)\nsales_duck &lt;- tbl(duck, \"sales\")\n\nsales_duck %&gt;%\n  group_by(year, SKU) %&gt;%\n  mutate(pos_sales = case_when(\n          sales_units &gt; 0 ~ sales_units,\n          TRUE ~ 0)) %&gt;%\n  summarize(total_revenue = sum(sales_units * item_price_eur),\n            max_order_price = max(pos_sales * item_price_eur),\n            avg_price_SKU = mean(item_price_eur),\n            items_sold = n())\n\nDBI::dbDisconnect(duck)"
  },
  {
    "objectID": "qmd/db-duckdb.html#sec-db-duckdb-arrow",
    "href": "qmd/db-duckdb.html#sec-db-duckdb-arrow",
    "title": "DuckDB",
    "section": "Apache Arrow",
    "text": "Apache Arrow\n\nto_duckdb() and to_arrow(): Converts between using {arrow} engine and {duckdb} engieg in workflow without paying any cost to (re)serialize the data when you pass it back and forth\n\nUseful in cases where something is supported in one of Arrow or DuckDB but not the other\n\nBenefits\n\nutilization of a parallel vectorized execution engine without requiring any extra data copying\nLarger Than Memory Analysis: Since both libraries support streaming query results, we are capable of executing on data without fully loading it from disk. Instead, we can execute one batch at a time. This allows us to execute queries on data that is bigger than memory.\nComplex Data Types: DuckDB can efficiently process complex data types that can be stored in Arrow vectors, including arbitrarily nested structs, lists, and maps.\nAdvanced Optimizer: DuckDBâ€™s state-of-the-art optimizer can push down filters and projections directly into Arrow scans. As a result, only relevant columns and partitions will be read, allowing the system to e.g., take advantage of partition elimination in Parquet files. This significantly accelerates query execution.\n\nExample (using a SQL Query; method 1)\n# open dataset\nds &lt;- arrow::open_dataset(dir_out, partitioning = \"species\")\n# open connection to DuckDB\ncon &lt;- dbConnect(duckdb::duckdb())\n# register the dataset as a DuckDB table, and give it a name\nduckdb::duckdb_register_arrow(con, \"my_table\", ds)\n# query\ndbGetQuery(con, \"\n  SELECT sepal_length, COUNT(*) AS n\n  FROM my_table\n  WHERE species = 'species=setosa'\n  GROUP BY sepal_length\n\")\n\n# clean up\nduckdb_unregister(con, \"my_table\")\ndbDisconnect(con)\n\nfiltering using a partition, the WHERE format is â€˜&lt;partition_variable&gt;=&lt;partition_value&gt;â€™\n\nExample (using SQL Query; method 2)\nlibrary(duckdb)\nlibrary(arrow)\nlibrary(dplyr)\n\n# Reads Parquet File to an Arrow Table\narrow_table &lt;- arrow::read_parquet(\"integers.parquet\", as_data_frame = FALSE)\n\n# Gets Database Connection\ncon &lt;- dbConnect(duckdb::duckdb())\n\n# Registers arrow table as a DuckDB view\narrow::to_duckdb(arrow_table, table_name = \"arrow_table\", con = con)\n\n# we can run a SQL query on this and print the result\nprint(dbGetQuery(con, \"SELECT SUM(data) FROM arrow_table WHERE data &gt; 50\"))\n\n# Transforms Query Result from DuckDB to Arrow Table\nresult &lt;- dbSendQuery(con, \"SELECT * FROM arrow_table\")\nExample (using dplyr)\nlibrary(duckdb)\nlibrary(arrow)\nlibrary(dplyr)\n\n# Open dataset using year,month folder partition\nds &lt;- arrow::open_dataset(\"nyc-taxi\", partitioning = c(\"year\", \"month\"))\n\nds %&gt;%\n  # Look only at 2015 on, where the number of passenger is positive, the trip distance is\n  # greater than a quarter mile, and where the fare amount is positive\n  filter(year &gt; 2014 & passenger_count &gt; 0 & trip_distance &gt; 0.25 & fare_amount &gt; 0) %&gt;%\n  # Pass off to DuckDB\n  to_duckdb() %&gt;%\n  group_by(passenger_count) %&gt;%\n  mutate(tip_pct = tip_amount / fare_amount) %&gt;%\n  summarize(\n    fare_amount = mean(fare_amount, na.rm = TRUE),\n    tip_amount = mean(tip_amount, na.rm = TRUE),\n    tip_pct = mean(tip_pct, na.rm = TRUE)\n  ) %&gt;%\n  arrange(passenger_count) %&gt;%\n  collect()\n\nIn the docs, the example has to_duckdb after the group_by. Not sure if that makes a difference in speed.Â \n\nExample (Streaming Data)\n# Reads dataset partitioning it in year/month folder\nnyc_dataset = open_dataset(\"nyc-taxi/\", partitioning = c(\"year\", \"month\"))\n\n# Gets Database Connection\ncon &lt;- dbConnect(duckdb::duckdb())\n\n# We can use the same function as before to register our arrow dataset\nduckdb::duckdb_register_arrow(con, \"nyc\", nyc_dataset)\n\nres &lt;- dbSendQuery(con, \"SELECT * FROM nyc\", arrow = TRUE)\n# DuckDB's queries can now produce a Record Batch Reader\nrecord_batch_reader &lt;- duckdb::duckdb_fetch_record_batch(res)\n\n# Which means we can stream the whole query per batch.\n# This retrieves the first batch\ncur_batch &lt;- record_batch_reader$read_next_batch()"
  },
  {
    "objectID": "qmd/db-duckdb.html#sec-db-duckdb-sql",
    "href": "qmd/db-duckdb.html#sec-db-duckdb-sql",
    "title": "DuckDB",
    "section": "SQL",
    "text": "SQL\n\nMisc\nStar Expressions\n\nAllows you dynamically select columns\n-- select all columns present in the FROM clause\nSELECT * FROM table_name;\n-- select all columns from the table called \"table_name\"\nSELECT table_name.* FROM table_name JOIN other_table_name USING (id);\n-- select all columns except the city column from the addresses table\nSELECT * EXCLUDE (city) FROM addresses;\n-- select all columns from the addresses table, but replace city with LOWER(city)\nSELECT * REPLACE (LOWER(city) AS city) FROM addresses;\n-- select all columns matching the given expression\nSELECT COLUMNS(c -&gt; c LIKE '%num%') FROM addresses;\n-- select all columns matching the given regex from the table\nSELECT COLUMNS('number\\d+') FROM addresses;"
  },
  {
    "objectID": "qmd/db-engineering.html#sec-db-eng-misc",
    "href": "qmd/db-engineering.html#sec-db-eng-misc",
    "title": "Engineering",
    "section": "Misc",
    "text": "Misc\n\nIf youâ€™re developing an application, a good rule of thumb is to write your frequently run queries in such a way that they return a response within 500 ms\nAWS Athena ($5/TB scanned)\n\nAWS Athena is serverless and intended for ad-hoc SQL queries against data on AWS S3\n\nParquet format supports indexing such that every pg has min-max stat\nColumn storage files (parquet) are more lightweight, as adequate compression can be made for each column. Row storage doesnâ€™t work in that way, since a single row can have multiple data types.\n\n\n(See below) Apache Avro is smaller file size than most row format file types (e.g.Â csv)\n\nAthena doesnâ€™t support indexed parquet formats\n{pins}\n\nConvenient storage method\nUse when:\n\nObject is less than a 1 Gb\n\nUsed {butcher} for large model objects\n\nSome model objects store training data\n\n\n\nBenefits\n\nJust need the pins board name and name of pinned object\n\nThink the set-up is supposed to be easy\n\nEasy to share; donâ€™t need to understand databases"
  },
  {
    "objectID": "qmd/db-engineering.html#sec-db-eng-terms",
    "href": "qmd/db-engineering.html#sec-db-eng-terms",
    "title": "Engineering",
    "section": "Terms",
    "text": "Terms\n\nACID - A database transaction, by definition, must be atomic, consistent, isolated and durable. These are popularly known as ACID properties.Â  These properties can ensure the concurrent execution of multiple transactions without conflict. Guarantees data validity despite errors and ensure that data does not become corrupt because of a failure of some sort.\n\nCrucial to business use cases that require a high level of data integrity such as transactions happening in banking.\n\nBatch processing - performing an action on data, such as ingesting it or transforming it, at a given time interval.\nBTEQ - Batch Teradata Query (like SQL) is simply a utility and query tool for Teradata which is a relational database system Creating a BTEQ script to load data from a flat-file.\nConcurrency - multiple computations are happening at the same time\nData Dump - A file or a table containing a significant amount of data to be analysed or transferred. A table containing the â€œdata dumpâ€ of all customer addresses.\nData Mart - A subset of a data warehouse, created for a very specific business use case. Finance data mart storing all the relevant financial information required by the Accounting team to process their month-end cycles.\nData Integration - Usually, the hardest part of the project, where multiple sources of data are integrated into a singular application/data warehouse. Integrating finance and customer relationship systems integrating into an MS SQL server database.\nData Lake - A repository for all kinds of structured and unstructured data. Mainly based on Hadoop storage technology. Called a lake as it is flexible enough to store anything from raw data to unstructured email files. Hadoop Data Lake. Storing logs of all customers called into the inbound call centre including call duration.\nData Mesh - Decentralized design where data is owned and managed by teams across the organisation that understands it the most, known as domain-driven ownership. tl;dr - Each department controls theyâ€™re own data from ingestion to â€œdata products.â€ This data product is then made a available to the other departments for them to use in their projects. Each department has their own engineers, scientists, and analysts.\n\nEach business unit or domain aims to infuse product thinking to create quality and reusable data products â€” a self-contained and accessible data set treated as a product by the dataâ€™s producers â€” which can then published and shared across the mesh to consumers in other domains and business units â€” called nodes on the mesh.\nEnables teams to work independently with greater autonomy and agility, while still ensuring that data is consistent, reliable and well-governed.\nYou donâ€™t have to figure out whoâ€™s in charge of what data, who gets to access it, who needs to protect it and what controls and monitoring is in place to ensure things donâ€™t go wrong.\nExample: Banking\n\nCredit risk domainâ€™s own data engineers can independently create and manage their data pipelines, without relying on a centralised ingestion team far removed from the business and lacking in credit expertise. This credit team will take pride in building and refining high-quality, strategic, and reusable data products that can be shared to different nodes (business domains) across the mesh.\n\n\nData Models - A way of organising the data in a way that it can be understood in a real-world scenario. Taking a huge amount of data and logically grouping it into customer, product and location data.\nData Quality - A discipline of measuring the quality of the data to improve and cleanse it. Checking Customer data for completeness, accuracy and validity.\nData Replication - There are multiple ways to do this, but mainly it is a practice of replicating data to multiple servers to protect an organisation against data loss. Replicating the customer information across two databases, to make sure their core details are not lost.\nDenormalization - database optimization technique in which we add redundant data to one or more tables. Designers use it to tune the performance of systems to support time-critical operations. Done in order to avoid costly joins. Me: Seems like itâ€™s kind of like a View except a View might have calculated columns in it.\nDimensions - A data warehousing term for qualitative information. Name of the customer or their country of residence.\nDistributed SQL -Â  a single logical database deployed across multiple physical nodes in a single data center or across many data centers if need be; all of which allow it to deliver elastic scale and resilience. Billions of transactions can be handled in a globally distributed database.\nEDW - The same as a data warehouse except it includes all the data within an organisation. This means that the entire enterprise can rely on this warehouse for their business decisions. Organising sales, customer, marketing and finance data in an enterprise data warehouse to be able to create several key management reports.\nEmbedded aka In-Process\n\nEmbedded database as in a database system particularly designed for the â€œembeddedâ€ space (mobile devices and so on.) This means they perform reasonably in tight environments (memory/CPU wise.)\nEmbedded database as in databases that do not need a server, and are embedded in an application (like SQLite.) This means everything is managed by the application.\n\nFacts - A data warehousing term for quantitative information. The number of orders placed by a customer.\nFlat File - Commonly used to transfer data due to their basic nature; flat files are a single table storing data in a plain text format. All customer order numbers stored in a comma-separated value (.csv) file\nHTAP - Hybrid Transactional Analytical Processing - System that attempts be good at both OLAP and OLTP\nMaster Data - This is data that is the best representation of a particular entity in the business. This gives you a 360 view of that data entity by generally consolidating multiple data sources. Best customer data representation from multiple sources of information.\nMulti-Master - allows data to be stored by a group of computers, and updated by any member of the group. All members are responsive to client data queries. The multi-master replication system is responsible for propagating the data modifications made by each member to the rest of the group and resolving any conflicts that might arise between concurrent changes made by different members.\n\nAdvantages\n\nAvailability: If one master fails, other masters continue to update the database.\nDistributed Access: Masters can be located in several physical sites, i.e.Â distributed across the network.\n\nDisadvantages\n\nConsistency: Most multi-master replication systems are only loosely consistent, i.e.Â lazy and asynchronous, violating ACID properties. (mysqlâ€™s multi-master is acid compliant)\nPerformance: Eager replication systems are complex and increase communication latency.\nIntegrity: Issues such as conflict resolution can become intractable as the number of nodes involved rises and latency increases.\n\nCan be contrasted with primary-replica replication, in which a single member of the group is designated as the â€œmasterâ€ for a given piece of data and is the only node allowed to modify that data item. Other members wishing to modify the data item must first contact the master node. Allowing only a single master makes it easier to achieve consistency among the members of the group, but is less flexible than multi-master replication.\n\nNiFi - It is an open-source extract, transform and load tool (refer to ETL), this allows filter, integrating and joining data. Moving postcode data from a .csv file to HDFS using NiFi.\nNormalization - A method of organizing the data in a granular enough format that it can be utilised for different purposes over time. Organizing according to data attributes reduces or eliminates data redundancy (i.e.Â having the same data in multiple places). Usually, this is done by normalizing the data into different forms such as 1NF (normal form) or 3NF (3rd normal form) which is the most common. (See DB, Relational &gt;&gt; Normalization)\n\nTaking customer order data and creating granular information model; order in one table, item ordered in another table, customer contact in another table, payment of the order in another table. This allows for the data to be re-used for different purposes over time.\n\nNULL indexes - These are the indexes that contain a high ratio of NULL values\nObject-Relational Mapping (ORM) - Allows you to define your data models in Python classes, which are then used to create and interact with the database. See {{SQLAlchemy}}\nODS - Operational data store generally stores limited and current information to help simple queries. Unable to handle historical or complex data queries. An ODS for daily stock fluctuations in a warehouse help the warehouse manager decide what to prioritise in the next order delivery.\nOLAP - Online Analytical Processing - large chunks of tables are read to create summaries of the stored data\n\nUse chunked-columnar data representation\n\nOLTP - Online Transactional Processing - rows in tables are created, updated and removed concurrently\n\ntraditionally use a row-based data representation\npostgres excels at this type of processing\n\nRDBMS - Relational database management system. All of the above examples are RDBMS, meaning they store data in a structured format using rows and columns.\n\nA Microsoft SQL server database.\n\nReal-Time Processing (aka Event Streaming) - each new piece of data that is picked up triggers an event, which is streamed through the data pipeline continuously\nReverse ETL - Instead of ETL where data is transformed before itâ€™s stored or ELT where data is stored and transformed while in storage, Reverse ETL performs transformations in the pipeline between Storage and the Data Product.\n\nSCD Type 1â€“6 - A method to deal with changes in the data over time in a data warehouse. Type 1 is when history is overwritten whereas Type 2 (most common) is when history is maintained each time a change occurs.\n\nWhen a customer changes their address; SCD Type 1 would overwrite the old address with the new one, whereas Type 2 would store both addresses to maintain history.\n\nSchemas - A term for a collection of database objects. These are generally used to logically separate data within the database and apply access controls.\n\nStoring HR data in HR schema allows logical segregation from other data in the organisation.\n\nSharding - Horizontal Partitioning â€” divides the data horizontally and usually on different database instances, which reduces performance pressure on a single server.\n\nStaging - The name of a storage area that is temporary in nature; to allow for processing of ETL jobs (refer to ETL).\n\nA staging area in an ETL routine to allow for data to be cleaned before loading into the final tables.\n\nTransactional Data - This is data that describes an actual event.\n\nOrder placed, a delivery arranged, or a delivery accepted.\n\nUnstructured Data - Data that cannot be nicely organised in a tabular format, like images, PDF files etc.\n\nAn image stored on a data lake cannot be retrieved using common data query languages."
  },
  {
    "objectID": "qmd/db-engineering.html#sec-db-eng-datqual",
    "href": "qmd/db-engineering.html#sec-db-eng-datqual",
    "title": "Engineering",
    "section": "Data Quality",
    "text": "Data Quality\n\nAlso see Production, Data Validation\nAccuracy - addresses the correctness of data, ensuring it represents real-world situations without errors. For instance, an accurate customer database should contain correct and up-to-date addresses for all customers.\nCompleteness - extent your datasets have all the required information on every record\n\nMonitor: missingness\n\nConsistency - extent that no contradictions in the data received from different sources. Data should be consistent in terms of format, units, and values. For example, a multinational company should report revenue data in a single currency to maintain consistency across its offices in various countries.\nTimeliness - Data should be available at the time itâ€™s required in the system\nValidity - ensuring that data adheres to the established rules, formats, and standards.\n\nMonitor: variable types/classes, numeric variable: ranges, number of decimal places, categorical variable: valid categories, spelling\n\nUniqueness - no replication of the same information twice or more. They appear in two forms; duplicate records and information duplication in multiple places.\n\nMonitor: duplicate rows, duplicate columns in multiple tables"
  },
  {
    "objectID": "qmd/db-engineering.html#sec-db-eng-costopt",
    "href": "qmd/db-engineering.html#sec-db-eng-costopt",
    "title": "Engineering",
    "section": "Cost Optimization",
    "text": "Cost Optimization\n\nAlso see\n\npage 53 in notebook\nGoogle, BigQuery &gt;&gt; Optimization\n\nAvoid disk operations, make sure that you look out for hints & information in the EXPLAIN PLAN of your query. (e.g.Â using SORT without an index)\n\nWhen you see filesort, understand that it will try to fit the whole table in the memory in many chunks.\n\nIf the table is too large to fit in memory, it will create a temporary table on disk.\n\nLook out for a using filesort with or without a combination of using temporary.\n\nSplit tables with many columns Might be efficient to split the less-frequently used data into separate tables with a few columns each, and relate them back to the main table by duplicating the numeric ID column from the main table.\n\nEach small table can have a primary key for fast lookups of its data, and you can query just the set of columns that you need using a join operation.\n\nPrimary keys should be global integers.\n\nIntegers consume less memory than strings, and they are faster to compare and hash\n\nJoins\n\nWith correlated keys\n\nThe query planner wonâ€™t recognize the correlated keys and do nested loop join when a hash join is more efficient\nI donâ€™t fully understand what correlated keys on a join are, but see SQL &gt;&gt; Terms &gt;&gt; Correlated/Uncorrelated queries\n\nIn the example below, a group of merge_commit_ids will only be from 1 repository id, so the two keys are associated in a sort of traditional statistical sense.\n\nSolutions\n\nUse LEFT_JOIN instead of INNER_JOIN\nUse extended statistics\nCREATE STATISTICS ids_correlation ON repository_id, merge_commit_id FROM pull_requests;\n\nâ€œrepository_idâ€ and â€œmerge_commit_idâ€ are the correlated keys\nIâ€™m not sure if â€œids_correlationâ€ is a function or just a user-defined name\nPostgreSQL â‰¥13 will recognize correlation and the query planner will make the correct calculation and perform a hash join\n\n\n\n\nPre-join data before loading it into storage\n\nIf a group of tables is frequently joined and frequently queried, then pre-joining will reduce query costs\ncan be done using an operational transform system such as Spark, Flow, or Flink (dbt can parallelize runs and work w/Spark)\n\nIndexes{#sec-db-eng-costopt-index}\n\nIndexes help in filtering data faster as the data is stored in a predefined order based on some key columns.\n\nIf the query uses those key columns, the index will be used, and the filter will be faster.\n\nSuitable for any combination of columns that are used in filter, group, order, or join\nMySQL Docs\nDonâ€™t use indexes with LIKE\nCluster a table according to an index\n\nAlso see Google, BigQuery &gt;&gt; Optimization &gt;&gt; Partition and Cluster\nRearranges the rows of a table on the disk\nDoesnâ€™t stay â€œclusteredâ€ if table is updated\n\nSee pg_repack for a solution\n\nExample\n-- create index\nCREATE INDEX pull_requests_repository_id ON pull_requests (repository_id, number)\n-- cluster table\nCLUSTER pull_requests USING pull_requests_repository_id\n\n\nUseful for queries such as\nSELECT *\nFROM pull_requests\nWHERE repository_id IN (...) AND number &gt; 1000\nBest Pactices\n\nAvoid too many indexes\n\nA copy of the indexed column + the primary key is created on disk\nIndexes add to the cost of inserts, updates, and deletes because each index must be updated\nBefore creating an index, see if you can repurpose an existing index to cater to an additional query\nCreate the least possible number of indexes to cover most of your queries (i.e.Â Covering Indexes).\n\nMakes effective use of the index-only scan feature\nAdd INCLUDE to the create index expression\nExample\n-- query\nSELECT y FROM tab WHERE x = 'key';\n-- covering index, x\nCREATE INDEX tab_x_y ON tab(x) INCLUDE (y);\n-- if the index, x, is unique\nCREATE UNIQUE INDEX tab_x_y ON tab(x) INCLUDE (y);\n\ny is called a non-payload column\n\nDonâ€™t add too many non-payload columns to an index. Each one duplicates data from the indexâ€™s table and bloat the size of the index.\n\n\nExample: Query with function\n-- query\nSELECT f(x) FROM tab WHERE f(x) &lt; 1;\n-- covering index, x\nCREATE INDEX tab_f_x ON tab (f(x)) INCLUDE (x);\n\nWhere f() can be MEAN, MEDIAN, etc.\n\n\n\nFix unusable indexes\n\nIssues related to data types, collation (i.e.Â how itâ€™s sorted), character set (how the db encodes characters), etc\nSometimes you can make the indexes work by explicitly forcing the optimizer to use them. (?)\n\nRepurpose or delete stale indexes\n\nIndexes are designed to serve an existing or a future load of queries on the database\nWhen queries change, some indexes originally designed to serve those queries might be completely irrelevant now\nAutomate stale index removal. Dbs keep statistics. Write a script to either notify you or just delete the index if itâ€™s older and not been used past a certain threshold\n\nUse the most cost efficient index type\n\nExample: If your use case only needs a regular expression search, youâ€™re better off having a simple index than a Full Text index.\n\nFull Text indexes occupy much more space and take much more time to update\n\n\nDonâ€™t index huge tables (&gt; 100M rows), partition instead\n\nThen prune the partitions (partition pruning) you donâ€™t need and create indexes for the partitioned tables you do keep.\n\n\nPartitioning\n\nAlso see Google, BigQuery &gt;&gt; Optimization &gt;&gt; Partition and Cluster\nSplits your table into smaller sub-tables under the hood\n\nNot viewable unless you check the table directory to see the multiple files that have been created\n\nThe same goes for indexes on that table.\n\n\nUse on tables with at least 100 million rows (BigQuery recommends &gt; 1 GB) Partitioning helps reduce table size and, in turn, reduces index size, which further speeds up the Data Warehouse (DWH) operations. But, partitioning also introduces complexity in the queries and increases the overhead of managing more data tables, especially backups. So try a few of the other performance techniques before getting to Sharding.\nPartition columns should always be picked based on how you expect to use the data, and not depending on which column would evenly split the data based on size.\n\nExample: partition on county because your analysis or transformations will largely be done by county even though since some counties may be much larger than others and will cause the partitions to be substantially imbalanced.\n\n\nUse ELT (e.g.Â load data from on-prem server to cloud, then transform) instead of ETL (transform data while on-prem, then load to cloud) for data pipelines\n\nMost of the time you have a lot of joins involved in the transformation step\n\nSQL joins are one of the most resource-intensive commands to run. Joins increase the queryâ€™s runtime exponentially as the number of joins increases.\nExample\n\nRunning 100+ pipelines with some pipelines having over 20 joins in a single query.\nEverything facilitated by airflow (see bkmk for code)\nETL: postgres on-prem server, sql queries with joins, tasks ran 12+ hours, then the transformed data is loaded to google storage\n\n13+ hrs for full pipeline completion\n\nELT: running the queries with the joins, etc. with bigquery sql on the data after itâ€™s been loaded into google storage.\n\n6+ hrs for full pipeline completion\n\n\n\n\nUse Materialized Views\n\nA smaller data object that contains the subset of data resulting from a specific query\nWhereas a query happens after data is loaded, a materialized view is a precomputation\nThe computation is done once, and changes to the data are incorporated as they occur, making subsequent updates to the view much cheaper and more efficient than querying the entire database from scratch\n\nFetching a large table will be slower if you try to use multiple cores.\n\nYou have to divide up the table and recombine it. Plus setting up parallel network processes takes time.\nThe time used to fetch some data from the internet depends massively on the internet bandwidth available on your router/network.\n\nUse Random Access via http range header + sparse-hilbert index to optimize db for query searches\nCITEXT extension makes it so you donâ€™t have use lower or upper which are huge hits on performance (at least they are in WHERE expressions) GIN custom indexes for LIKE and ILIKE\nCREATE EXTENSION IF NOT EXISTS btree_gin;\nCREATE EXTENSION IF NOT EXISTS pg_trgm;\nCREATE INDEX index_users_on_email_gin ON users USING gin (email gin_trgm_ops);\n\nCREATE EXTENSION adds btree and pg_trgm extensions\nindex_users_on_email_gin is the name of the index\nusers is the table\nUSING gin (email gin_trgm_ops)\n\ngin specifies that itâ€™s a gin index\nemail is the field\ngin_trgm_ops is from the pg_trgm extension. It splits the index into trigrams which is necessary for the gin index to work with LIKE or ILIKE\n\nSlower to update than the standard ones. So you should avoid adding them to a frequently updated table.\n\nGiST indexes are very good for dynamic data and fast if the number of unique words (lexemes) is under 100,000, while GIN indexes will handle 100,000+ lexemes better but are slower to update.\n\n\nNULLS LASTputs the NULLS in a field in any sorting operations at the end\n\nThe default behavior of ORDER BY will put the NULLS first, so if you use LIMIT , you might get back a bunch of NULLS.\nUsing NULLS LAST fixes this behavior but its slow even on an indexed column\n\nExample: ORDER BY email DESC NULLS LAST LIMIT 10\n\nInstead use two queries\nSELECT *\nFROM users\nORDER BY email DESC\nWHERE email IS NOT NULL LIMIT 10;\n\nSELECT *\nFROM users\nWHERE email IS NULL LIMIT 10;\n\nThe first one would fetch the sorted non-null values. If the result does not satisfy the LIMIT, another query fetches remaining rows with NULL values.\n\n\nRebuild Null Indexes\nDROP INDEX CONCURRENTLY users_reset_token_ix;\nCREATE INDEX CONCURRENTLY users_reset_token_ix ON users(reset_token)\nWHERE reset_token IS NOT NULL;\n\nDrops and rebuilds an index to only include NOT NULL rows\nusers_reset_token_ix is the name of the index\nusers is the table\nI assume â€œreset_token has to be the field\n\nWrap multiple db update queries into a single transaction\n\nImproves the write performance unless the database update is VERY large.\nA large-scale update performed by a background worker process could potentially timeout web server processes and cause a user-facing app outage\nFor large db updates, add batching\n\n[Example]{.ribbon-highlight: db update has a 100K rows, so update 10K at a time.\nUPDATE messages SET status = 'archived'\nÂ  WHERE id IN\nÂ  (SELECT ID FROM messages ORDER BY ID LIMIT 10000 OFFSET 0);\nUPDATE messages SET status = 'archived'\nÂ  WHERE id IN\nÂ  (SELECT ID FROM messages ORDER BY ID LIMIT 10000 OFFSET 10000);\nUPDATE messages SET status = 'archived'\nÂ  WHERE id IN\nÂ  (SELECT ID FROM messages ORDER BY ID LIMIT 10000 OFFSET 20000);\n\nmessages is the table name\nI guess OFFSET is whatâ€™s key here."
  },
  {
    "objectID": "qmd/db-engineering.html#sec-db-eng-schema",
    "href": "qmd/db-engineering.html#sec-db-eng-schema",
    "title": "Engineering",
    "section": "Schema",
    "text": "Schema\n\nMisc\n\nDonâ€™t use external IDs as primary keys\n\nSince you donâ€™t control those IDs, they can change the format and break your queries.\n\n\nStar\n\n\nThe simplest way to model data into different quantitative and qualitative data called facts and dimensions. Usually, the fact table is interpreted with the help of a dimensions table resembling a star. A Star schema of sales data with dimensions such as customer, product & time.\nData sources are logically structured into smaller tables with join keys in each table"
  },
  {
    "objectID": "qmd/db-engineering.html#sec-db-eng-ets",
    "href": "qmd/db-engineering.html#sec-db-eng-ets",
    "title": "Engineering",
    "section": "Event Tracking Systems",
    "text": "Event Tracking Systems\n\nEvents are queued, then batch inserted into your db.\n\nStreaming events does not scale very well and is not fault tolerant.\n\nCommercial Services\n\nSegment\n\nMost popular option\nVery expensive\nSusceptible to ad blockers\nOnly syncs data once per hour or two\nMissing a few key fields in the schema it generates (specifically, session and page ids).\n\nFreshpaint is a newer commercial alternative that aims to solve some of these issues.\n\nOpen Source (each with a managed offering if you donâ€™t feel like hosting it yourself)\n\nSnowplow is the oldest and most popular, but it can take a while to setup and configure.\nRudderstack is a full-featured Segment alternative.\nJitsu is a pared down event tracking library that is laser focused on just getting events into your warehouse as quickly as possible."
  },
  {
    "objectID": "qmd/db-engineering.html#sec-db-eng-stream",
    "href": "qmd/db-engineering.html#sec-db-eng-stream",
    "title": "Engineering",
    "section": "Streaming",
    "text": "Streaming\n\nStreaming or near real-time data\nData Characteristics\n\nIt is ingested near-real-time.\nUsed for real-time reporting and/or calculating near-real-time aggregates. Aggregation queries on it are temporal in nature so any aggregations defined on the data will be changed over time as the data comes.\nIt is append-only data but can have high ingestion rates so needs support for fast writes.\nHistorical trends can be analyzed to forecast future metrics. Algorithms like ARIMA are used to do time series forecasting.\n\nRelational databases canâ€™t handle high ingestion rates and near-real-time aggregates without extensions.\nArchitectures\n\n\nTimeScale DB\n\nOpen source extension for postgresql\nSupport all things postgresql like relational queries, full SQL support(not SQL-like) as well as the support of real-time queries\nSupports an ingestion of 1.5M+ metrics per second per server\nNear-real-time aggregation of tables\nProvides integration with Kafka, kinesis, etc for data ingestion.\nCan be integrated with any real-time visualization tool such as Graphana\n\nPipeline DB\n\nOpen source extension for postgresql\nSimilar features as TimeScale DB\nEfficiency comes from it not storing raw data\n\nUsually, itâ€™s recommended to store raw data"
  },
  {
    "objectID": "qmd/db-engineering.html#sec-db-eng-otools",
    "href": "qmd/db-engineering.html#sec-db-eng-otools",
    "title": "Engineering",
    "section": "Other Tools",
    "text": "Other Tools\n\nDataFold monitors your warehouse and alerts you if there are any anomalies (e.g.Â if checkout conversion rate drops suddenly right after a deploy).\nHightouch lets you sync data from your warehouse to your marketing and sales platforms.\nWhale is an open source tool to document and catalog your data.Â \nRetool lets you integrate warehouse data into your internal admin tools.\nGrowth Book that plugs into your data warehouse and handles all of the complicated querying and statistics required for robust A/B test analysis."
  },
  {
    "objectID": "qmd/db-feature-stores.html",
    "href": "qmd/db-feature-stores.html",
    "title": "10Â  DB, Feature Stores",
    "section": "",
    "text": "TOC\n\nMisc\nBrands\n\nMisc\n\nBenefits\n\nIncreases Reproducibility\n\nEasily track the version of the features used in each model and reproduce the modelâ€™s results if needed.\nUseful in a collaborative environment where multiple people are working on the same project.\n\nDiscovery and Testing Features Easier\n\nHaving features in a centralized location makes comparing the performance of various features and versions of those features easier.\n\nEasier to Scale\n\nItâ€™s easier to share features between ML models which means fewer resources (e.g.Â development, deployment) will be required. Allowing more models to be added more efficiently and cheaply.\n\n\nFeatures of a Feature Store\n\nDesigned with ML modelling in mind\n\nCan handle large amounts of data and perform feature engineering at scale\n\nHandles versioning of features\n\nEasy to track which features were used for a particular model, making it simple to reproduce or deploy the model in the future\n\nAllows for different levels of access control\n\nA data scientist can work on a feature without worrying about affecting other users which canâ€™t be said about warehouses\n\n\nBest used if you have a substantial number of features that are computationally expensive, frequently improved, and used in many ML models.\n\nHere â€œTransformâ€ is referring to something like an AWS lambda function thatâ€™s triggered to transform the data\nCases where adding a feature store adds unnecessary complexity:\n\nFeature value needs to â€œseenâ€ by the client (e.g.Â app)\n\nNot exactly sure why this matters or what â€œseenâ€ means\nMaybe this is a latency thing?\n\nFeature is in a data warehouse.\nFeature isnâ€™t time dependent\n\nSo only streaming and not batch serving I think\n\nComputationally inexpensive\n\nExample\n\nEmbedding of a song, artist, and user features in a music streaming service.\n\nThere is a team updating user and song embeddings on a daily basis. Every time the model that consumes this feature, it is retrained â€” high commercial value use cases will need to re-train periodically â€” the training code will need to fetch the values of this feature that align with the training labels and the latest version of the embedding algorithm.\n\n\n\nPositioning within a pipeline\n\nLooks like something dbt (â€œFeature Pipelinesâ€) would write to.\nReminds me of the description of a data mart.\n\nConnectors\n\nTensorFlowâ€™s TFXI (TensorFlow Extended Input/Output)\n\nModule allows you to easily read data from Feature store and feed it into your TensorFlow model.\nSupports data preprocessing, so you can do things like normalization and feature selection right within TensorFlow.\n\nPyTorchâ€™s DataLoader\n\nClass that allows you to easily read data from Feature store, process, and feed it into your PyTorch model.\n\n\n\nBrands\n\nGoogle Vertex AI feature store\n\ndocs\n\nAmazon SageMaker Feature Store\n\ndocs\n\nDatabricks Feature Store\nHopsWorks Feature Store\ntecton.ai\n\nsite\ncloud agnostic\n\nbytehub\n\ngithub\n\nFeast\n\nIt is a standalone, open-source feature store that organizations use to store and serve features consistently for offline training and online inference.\n\nDataRobot\nAlgoworks\nHugging Face: A feature store for natural language processing (NLP) models that allows for easy sharing and management of pre-trained models and features."
  },
  {
    "objectID": "qmd/db-lakes.html#sec-db-lakes-misc",
    "href": "qmd/db-lakes.html#sec-db-lakes-misc",
    "title": "Lakes",
    "section": "Misc",
    "text": "Misc\n\nData is stored in structured format or in its raw native format without any transformation at any scale.\n\nHandling both types allows all data to be centralized which means it can be better organized and more easily accessed.\n\nOptimal for fit for bulk data types such as server logs, clickstreams, social media, or sensor data.\nlower storage costs due to their more open-source nature and undefined structure\nHadoop\n\ntraditional format for data lakes\n\nObject Storage Systems\n\nExamples\n\nAmazon S3\nMicrosoft Azure Data Lake Storage (ADLS)\n\ncloud data lakes provide organizations with additional opportunities to simplify data management by being accessible everywhere to all applications as needed\norganized as collections of files within directory structures, often with multiple files in one directory representing a single table.\n\nPros: highly accessible and flexible\nMetadata Catalogs are used to answer these questions:\n\nWhat is the schema of a dataset, including columns and data types\nWhich files comprise the dataset and how are they organized (e.g., partitions)\nHow different applications coordinate changes to the dataset, including both changes to the definition of the dataset and changes to data\n\nHive Metastore (HMS) and AWS Glue Data Catalog are two popular catalog options\n\ncontain the schema, table structure and data location for datasets within data lake storage\n\n\nIssues:\n\ndoes not coordinate data changes or schema evolution between applications in a transactionally consistent manner.\n\nCreates the necessity for data staging areas and this extra layer makes project pipelines brittle\n\n\n\nApache Iceberg\n\nopen source table format\nFeatures\n\nTransactional consistency between multiple applications where files can be added, removed or modified atomically, with full read isolation and multiple concurrent writes\nFull schema evolution to track changes to a table over time\nTime travel to query historical data and verify changes between updates\nPartition layout and evolution enabling updates to partition schemes as queries and data volumes change without relying on hidden partitions or physical directories\nRollback to prior versions to quickly correct issues and return tables to a known good state\nAdvanced planning and filtering capabilities for high performance on large data volumes\nthe full history is maintained within the Iceberg table format and without storage system dependencies\n\nsupports common industry-standard file formats, including Parquet, ORC and Avro\nsupported by major data lake engines including Dremio, Spark, Hive and Presto\n\nComparison"
  },
  {
    "objectID": "qmd/db-other.html#sec-db-other-misc",
    "href": "qmd/db-other.html#sec-db-other-misc",
    "title": "Other",
    "section": "Misc",
    "text": "Misc"
  },
  {
    "objectID": "qmd/db-other.html#sec-db-other-nonrel",
    "href": "qmd/db-other.html#sec-db-other-nonrel",
    "title": "Other",
    "section": "Non-relational databases",
    "text": "Non-relational databases\n\nMore scalable than relational databases, but the lack of SQL support\nExamples: MongoDB"
  },
  {
    "objectID": "qmd/db-other.html#sec-db-other-mart",
    "href": "qmd/db-other.html#sec-db-other-mart",
    "title": "Other",
    "section": "Data Mart",
    "text": "Data Mart\n\nRequired for real-time artificial intelligence, and data analysis for different subsets of business operations.\nScaled down version of a data warehouse with a more limited scope for groups of end users in different business units or departments\n\nGives groups more control to customize but also silos the data apart from the rest of the company\n\nTypes\n\nDependent - Gets populated from a Data Warehouse\nIndependent - Gets populated from an Operational Data Store (ODS)\n\nProcess\n\nCreate a initial list of questions the data mart will be used to answer\nCreate Schema documents\n\nInclude as much information as possible in the schema document because it can be used as a reference if anyone has questions about the data in the future instead of asking you\nAdd any business logic that needs to be applied when reading in the data such as filters and transformation logic as well as noting the time frame of data needed and frequency of update\nExamples\n\n\n\nNotice the business logic for Field Name: wau (weekly active users) is the distinct count of users where the login date is current date-1 and current date-6. The reason we use current date-1 is because the most recent data is typically from yesterday and taking yesterday minus 6 days gives us 7 days to calculate wau.\nField Name: update_date should be set to the last time the ETL was run for this table to let the user know when the data was last updated. Occasionally ETL jobs may fail and this can help troubleshoot if the table was refreshed for the day\n\n\nCreate sample tables according to the schema document\n\nSource production data for you to validate the tables\nAfter the sample tables pass your QA (quality assurance) checks, you can work with the data engineer to back run any history if needed and then have them put the ETL code into production"
  },
  {
    "objectID": "qmd/db-other.html#sec-db-other-trans",
    "href": "qmd/db-other.html#sec-db-other-trans",
    "title": "Other",
    "section": "Transactional Store",
    "text": "Transactional Store\n\nOptimized for row-based operations such as reading and writing individual records while maintaining data integrity\nNot specifically built for analytics but can be used for analytic queries as well as low latency information monitoring\nACID (atomicity, consistency, isolation, durability) compliant, meaning they guarantee data validity despite errors and ensure that data does not become corrupt because of a failure of some sort.\n\ncrucial to business use cases that require a high level of data integrity such as transactions happening in banking.\n\nRow-based makes it better at writing data.\n\nIn contrast to data warehouses which are column bases and better for reading data"
  },
  {
    "objectID": "qmd/db-other.html#sec-db-other-opstore",
    "href": "qmd/db-other.html#sec-db-other-opstore",
    "title": "Other",
    "section": "Operational Data Stores",
    "text": "Operational Data Stores\n\nLike a staging area for data required for projects\nProvide fine-grained non-aggregated data\nUsually complimentary to a data warehouse\nGeneral purpose is to integrate data from different sources into a single structure via data cleaning, resolving redundancies, and establishing business rules."
  },
  {
    "objectID": "qmd/db-postgres.html#sec-db-pstgr-misc",
    "href": "qmd/db-postgres.html#sec-db-pstgr-misc",
    "title": "postgres",
    "section": "Misc",
    "text": "Misc\n\nNotes from\n\nCourse: linkedin.learning.postgresql.client.applications-xqzt\nCourse: Linux.Academy.PostgreSQL.Administration.Deep.Dive-APoLLo\n\nEverything is case sensitive, so use lowercase for db and table names\nCheck postgres sql version - psql --version or -V\nSee flag options - psql --help\nIf thereâ€™s a â€œ#â€ in the prompt after logging into a db, then that signifies you are a super-user\nMeta commands (i.e.Â commands once youâ€™re logged into the db)\n\n\\du - list roles (aka users + permissions)\n\\c  - switches databases\n\\password  - assign a password to a user (prompt will ask for the password twice)\n\nCan also use ALTER ROLE for this but the password will then be in the log\n\n\nUnlogged Table - Data written to an unlogged table will not be logged to the write-ahead-log (WAL), making it ideal for intermediate tables and considerably faster. Note that unlogged tables will not be restored in case of a crash, and will not be replicated."
  },
  {
    "objectID": "qmd/db-postgres.html#sec-db-pstgr-pgadm",
    "href": "qmd/db-postgres.html#sec-db-pstgr-pgadm",
    "title": "postgres",
    "section": "pgAdmin",
    "text": "pgAdmin\n\nCreate a server\n\nRight-click on servers &gt;&gt; create &gt;&gt; server\n\nGeneral tab &gt;&gt; enter name\nConnection tab\n\nHost name/address: computer name or ip address where the server is running\n\nlocal: localhost or 127.0.0.1\n\nPort: default = 5432\nMaintenance database: db you want to connect to\n\nIf you havenâ€™t created it yet, just use default â€œpostgresâ€ which autmatically created during installation\n\nusername/password\n\nu: default is postgres\np: installation password\nTick Save password\n\n\nClick Save\n\n\nCreate a db\n\nRight-click databases &gt;&gt; create &gt;&gt; databases &gt;&gt; enter name (lowercase) and click save\n\nCreate a table\n\nVia gui\n\nClick db name &gt;&gt; schema &gt;&gt; public &gt;&gt; right-click tables &gt;&gt; create &gt;&gt; tables\nGeneral tab\n\nEnter the table name (lower case)\n\nColumns tab\n\nEnter name, data type, whether there should be a â€œNot Nullâ€ constraint, and whether itâ€™s a primary key\nAdd additional column with â€œ+â€ icon in upper right\nIf youâ€™re going to fill the table with a .csv file, make sure the column names match\n\nClick save\nTable will be located at db name &gt;&gt; schema &gt;&gt; public &gt;&gt; tables\n\nVia sql\n\nOpen query tool\n\nRight-click  or Schemas or Tables &gt;&gt; query tool\nClick Tools menu dropdown (navbar) &gt;&gt; query tool\n\nRun CREATE TABLE statement\n\nIf you donâ€™t include the schema as part of the table name, pgadmin automatically places it into the â€œpublicâ€ schema directory (e.g.Â public.table_name)\n\n\n\nImport csv into an empty table\n\nMake sure the column names match\nRight-click table name &gt;&gt; import/export\nOptions tab\n\nMake sure import is selected\nSelect the file\nIf you have column names in your csv, select Yes for Header\nSelect â€œ,â€ for the Delimiter\n\nColumns tab\n\nCheck to make sure all the column names are there\n\nClick OK\n\nQuery Table\n\nRight-click table &gt;&gt; query editor\nQuery editor tab\n\nType query &gt;&gt; click â–¶ to run query"
  },
  {
    "objectID": "qmd/db-postgres.html#sec-db-pstgr-rds",
    "href": "qmd/db-postgres.html#sec-db-pstgr-rds",
    "title": "postgres",
    "section": "AWS RDS",
    "text": "AWS RDS\n\nMisc\n\nNotes from Create an RDS Postgres Instance and connect with pgAdmin\n\nSteps\n\nSearch AWS services for â€œRDSâ€ (top left navbar)\nCreate Database\n\nClick â€œCreate Databaseâ€\n\nCreate Database\n\nChoose Standard create or Easy Create\n\nEasy Create - uses â€œbest practicesâ€ settings\n\nSelect postgres\n\nAlso available: Amazon Aurora, MySQL, MariaDB, Oracle, Microsoft SQL Server\n\nTemplates\n\nProduction\n\nMulti-AZ Deployment - Multiple Availability Zones\nProvisioned IOPS Storage - Increased output\n\nDev/Test\nRree tier\n\n750 hrs of Amazon RDS in a Single-AZ db.t2.micro Instance.\n20 GB of General Purpose Storage (SSD).\n20 GB for automated backup storage and any user-initiated DB Snapshots.\n\nRDS pricing page\n\nSettings\n\nDB Instance Identifier - enter name\nSet master username, master username password\n\nDB Instance\n\ndb.t3.micro or db.t4g.micro for free tier\n\ndev/test, production has many other options\n\n\nStorage\n\nDefaults: SSD with 20GB\nAutoscaling can up the storage capacity to a default 1000GB"
  },
  {
    "objectID": "qmd/db-relational.html#sec-db-rel-misc",
    "href": "qmd/db-relational.html#sec-db-rel-misc",
    "title": "12Â  Relational",
    "section": "12.1 Misc",
    "text": "12.1 Misc\n\nWhen you start to have multiple datasets or when you want to make use of several columns in one table and other columns in another table you should consider going the local database route.\nUse db â€œnormalizationâ€ (that thing adam was doing?) to figure out a schema\nOptimized for a mix of read and write queries that insert/select a small number of rows at a time and can handle up to 1TB of data reasonably well.\nThe main difference between a â€œrelational databaseâ€ and a â€œdata warehouseâ€ is that the former is created and optimised to â€œrecordâ€ data, whilst the latter is created and built to â€œreact to analyticsâ€.\nTypes\n\nEmbedded aka In-Process (see Databases, Engineering &gt;&gt; Terms): DuckDB (analytics) and SQLite (transactional)\nServer-based: postgres, mysql, SQL Server\n\nMix of transactional and analytical\nDistributed SQL (database replicants across regions or hybrid (on-prem + cloud)\n\nmysql, postgres available for both in AWS Aurora (See below)\npostgres available using yugabytedb\nSQL Server on Azure SQL Database\nCloud Spanner on GCP\n\n\n\nSQLite vs MySQL as transactional dbs (article)\n\nSQLite:\n\nembedded, size ~600KB\nlimited data types\nbeing self-contained, other clients on a network would not have access to the database (no multi-users) unlike with MySQL\nno built-in authentication that is supported\nmultiple processes are able to access the database at the same time, but making changes at the same time is not something supported\nUse Cases\n\nData being confined in the files of the device is not a problem\nNetwork access to the db is not needed\nApplications that will minimally access the database and not require heavy calculations\n\n\nMySQL:\n\nopposites of the sqlite stuff\nSize ~600MB\nsupports replication and scalability\nSecurity is a large; built-in features to keep unwanted people from easily accessing data\nUse cases\n\ntransactions are more frequent like on web or desktop applications\nif network capabilities are a must\nmulti-user access and therefore security and authentication\nlarge amounts of data\n\n\n\nWrapper for db connections (e.g.Â con_depA &lt;- connect_databaseA(username = ..., password = ...) )\n# ... other stuff including code for \"connect_odbc\" function\n\n# connection attempt loop\nwhile(try &lt; retries) {\nÂ  Â  con &lt;- connect_odbc(source_db = \"&lt;database name&gt;\"\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  username = username,\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  password = password)\nÂ  Â  if(class(con) == \"NetexxaSQL\") {\nÂ  Â  Â  Â  try &lt;- retries + 1\nÂ  Â  } else if (!\"NetezzaSQL\" %in% class(con) & try &lt; retries {\nÂ  Â  Â  Â  warning(\"&lt;database name&gt; connection failed. Retrying...\")\nÂ  Â  Â  Â  try &lt;- try + 1\nÂ  Â  Â  Â  Sys.sleep(retry_wait)\nÂ  Â  } else {\nÂ  Â  Â  Â  try &lt;- try + 1\nÂ  Â  Â  Â  warning(\"&lt;database name&gt; connection failed\")\nÂ  Â  }\n}\n\nGuessing â€œNetezzaSQLâ€ is some kind of error code for a failed connection to the db\n\nMySQL\n\nInstallation docs\nBasic intro\nSee SQL notebook\n\nCloud SQL - Google service to provide hosting services for relational dbs (see Google, BigQuery &gt;&gt; Misc). Can use postgres, mysql, etc. on their machines.\n\nCloud SQL Insights - good query optimization tool\n\nAWS RDS for db instances (see Database, postgres &gt;&gt; AWS RDS)\n\nAvailable: Amazon Aurora, MySQL, MariaDB, postgres, Oracle, Microsoft SQL Server\nRDS (Relational Database Service)\n\nBenefits over hosting db on EC2: AWS handles scaling, availability, backups, and software and operating system updates\n\nS3 is like googledrive or dropbox\n\nCon: only contains data about the files, not whatâ€™s inside them, i.e.Â no querying\nIdeal use cases\n\nbackup for logs,\nraw sensor data for your IoT application,\ntext files from user interviews\nimages\ntrained machine learning models (with the database simply storing the path to the object)\n\nAlternative: Minio\n\nOpen-Source alternative to AWS S3 storage.\nGiven that S3 often stores customer PII (either inadvertently via screenshots or actual structured JSON files), Minio is a great alternative to companies mindful of who has access to user data.\n\nOf course, AWS claims that AWS personnel doesnâ€™t have direct access to customer data, but by being closed-source, that statement is just a function of trust.\n\n\n\n\nAWS Aurora - MySQL- and PostgreSQL-compatible enterprise-class database\n\nstarting at &lt;$1/day.\nsupports up to 64TB of auto-scaling storage capacity, 6-way replication across three availability zones, and 15 low-latency read replicas.\nCreate MySQL and Postgres instances using AWS Cloudformation\n\nAWS DynamoDB - for creating and querying NoSQL databases.\nRelational databases do not keep all data together but split it into multiple smaller tables. That separation into sub-tables has several advantages:\n\nall information is stored only once, avoiding repetition and conserving memory\nall information is updated only once and in one place, improving consistency and avoiding errors that may result from updating the same value in multiple locations\nall information is organized by topic and segmented into smaller tables that are easier to handle\n\nSeparation of data, thus, helps with data quality, and explains the continuing popularity of relational databases in production-level data management.\ndplyr\n\ncompute stores results in a remote temporary table\ncollect retrieves data into a local tibble.\ncollapse doesnâ€™t force computation, but instead forces generation of the SQL query.\n\nsometimes needed to work around bugs in dplyrâ€™s SQL generation.\n\n\n{dm}\n\nCan join multiple tables from a db, but keeps the meta info such as table names, primary and foreign keys, size of original tables etc.\n\nSQLite\n\n{RSQLite}\n\nApache Avro\n\nrow storage file format unlike parquet\na single Avro file contains a JSON-like schema for data types and the data itself in binary format\n4x slower reading than csv but 1.5x faster writing than csv\n1.7x smaller file size than csv\n\nBenchmarks\n\nExample\n\nData\n\n~54,000,000 rows and 6 columns\n10 .rds files with gz compression is 220MB total,\n\nIf they were .csv, 1.5 GB\n\nSQLite file is 3 GB\nDuckDB file is 2.5 GB\nArrow creates a structure of directories, 477 MB total\n\nOperation: read, filter, group_by, summarize\nResults\n##Â  formatÂ  Â  Â  Â  Â  median_time mem_alloc\n##Â  &lt;chr&gt;Â  Â  Â  Â  Â  Â  Â  &lt;bch:tm&gt; &lt;bch:byt&gt;\n## 1 R (RDS)Â  Â  Â  Â  Â  Â  Â  1.34mÂ  Â  4.08GB\n## 2 SQL (SQLite)Â  Â  Â  Â  Â  5.48sÂ  Â  6.17MB\n## 3 SQL (DuckDB)Â  Â  Â  Â  Â  1.76sÂ  104.66KB\n## 4 Arrow (Parquet)Â  Â  Â  1.36sÂ  453.89MB\n\n\nTradional relational db solutions balloon up the file size\n\nSQLite 2x, DuckDB 1.66x (using csv size)\n\nIndexing on SQLite and DuckDB and partitioning with Arrow help with speed"
  },
  {
    "objectID": "qmd/db-relational.html#sec-db-rel-transspr",
    "href": "qmd/db-relational.html#sec-db-rel-transspr",
    "title": "12Â  Relational",
    "section": "12.2 Transitioning from Spreadsheet to DB",
    "text": "12.2 Transitioning from Spreadsheet to DB\n\nMisc\n\nWhen you start to have multiple datasets or when you want to make use of several columns in one table and other columns in another table you should consider going the local database route.\nUse db â€œnormalizationâ€ to figure out a schema\nAlso see\n\nDatabases, Engineering &gt;&gt; Schema\nDatabases, Warehouses &gt;&gt; Design a Warehouse\n\n\nDB advantages over spreadsheets:\n\nEfficient analysis: Relational databases allow information to be retrieved quicker to then be analyzed with SQL (Structured Query Language), to then run queries.\n\nOnce spreadsheets get large, they can lag or freeze when opening, editing, or performing simple analyses in them.\n\nCentralized data management: Since relational databases often require a certain type or format of data to be input into each column of a table, itâ€™s less likely that youâ€™ll end up with duplicate or inconsistent data.\nScalability: If your business is experiencing high growth, this means that the database will expand, and a relational database can accommodate an increased volume of data.\n\nStart documenting the spreadsheets\n\nfile names, file paths\nUnderstand where values are coming from\n\nsource (e.g.Â department, store, sensor), owner\n\nHow rows of data are being generated\n\nwho/what is inputting the data\n\nHow does each spreadsheet/notebooks/set of spreadsheets fit in the companyâ€™s business model\n\nHow are they being used and by whom\n\nMap the spreadsheets relationships to one another\n\nSee Databases, Warehouses &gt;&gt; Design a Warehouse"
  },
  {
    "objectID": "qmd/db-relational.html#sec-db-rel-norm",
    "href": "qmd/db-relational.html#sec-db-rel-norm",
    "title": "12Â  Relational",
    "section": "12.3 Normalization",
    "text": "12.3 Normalization\n\nOrganizing according to data attributes to reduce or eliminate data redundancy (i.e.Â having the same data in multiple places).\n\nIt gives you a set of rules to be able to start categorizing your data and forming a layout\n\nBy establishing structure in a database, you are able to help establish a couple of important things: data integrity and scalability.\n\nIntegrity ensures that data is entered correctly and accurately.\nScalability ensures you have organized the data in a way that it is more computationally efficient when you start to run SQL queries.\n\nMisc\n\nNotes from When Spreadsheets Arenâ€™t Good Enough: A Lesson in Relational Databases\n\nGives an example of normalizing a dataset through a MySQL analysis\n\nPackages\n\n{{autonormalize}} - analyzes transaction df and creates relational tables - python library for automated dataset normalization\n\n\nForms\n\nDatabases are often considered as â€œnormalizedâ€ if they meet the third normal form\nSee A Complete Guide to Database Normalization in SQL for details on the other 4 forms.\n\nAlso gives an example of normalizing a dataset through a posgresSQL analysis\n\nFirst normal form (1NF)\n\nEvery value in each column of a table must be reduced to its most simple value, also known as atomic.\n\nAn atomic value is one where there are no sets of values within a column. (i.e.Â 1 value per cell)\n\nThere are no repeating columns or rows within the database.\nEach table should have a primary key which can be defined as a non-null, unique value that identifies each row insertion. Second normal form (2NF)\nConforms to first normal form rules.\nAdjust columns so that each table only contains data relating to the primary key.\nForeign keys are used to establish relationships between tables. Third normal form (3NF)\nConforms to both first and second normal form rules.\nNecessary to shift or remove columns (attributes) that are transitively dependent, which means they rely on other columns that arenâ€™t foreign or primary keys."
  },
  {
    "objectID": "qmd/db-vector.html#sec-db-vect-misc",
    "href": "qmd/db-vector.html#sec-db-vect-misc",
    "title": "Vector Databases",
    "section": "Misc",
    "text": "Misc\n\nVector databases store embeddings and provide fast similarity searches\nComparison (link)\n\n\nOpen-Source and hosted cloud: If you lean towards open-source solutions, Weviate, Milvus, and Chroma emerge as top contenders. Pinecone, although not open-source, shines with its developer experience and a robust fully hosted solution.\nPerformance: When it comes to raw performance in queries per second, Milvus takes the lead, closely followed by Weviate and Qdrant. However, in terms of latency, Pinecone and Milvus both offer impressive sub-2ms results. If nmultiple pods are added for pinecone, then much higher QPS can be reached.\nCommunity Strength: Milvus boasts the largest community presence, followed by Weviate and Elasticsearch. A strong community often translates to better support, enhancements, and bug fixes.\nScalability, advanced features and security: Role-based access control, a feature crucial for many enterprise applications, is found in Pinecone, Milvus, and Elasticsearch. On the scaling front, dynamic segment placement is offered by Milvus and Chroma, making them suitable for ever-evolving datasets. If youâ€™re in need of a database with a wide array of index types, Milvusâ€™ support for 11 different types is unmatched. While hybrid search is well-supported across the board, Elasticsearch does fall short in terms of disk index support.\nPricing: For startups or projects on a budget, Qdrantâ€™s estimated $9 pricing for 50k vectors is hard to beat. On the other end of the spectrum, for larger projects requiring high performance, Pinecone and Milvus offer competitive pricing tiers."
  },
  {
    "objectID": "qmd/db-vector.html#sec-db-vect-bran",
    "href": "qmd/db-vector.html#sec-db-vect-bran",
    "title": "Vector Databases",
    "section": "Brands",
    "text": "Brands\n\nQdrant - open source, free, and easy to use (example)\nChroma - can be used as a local in-memory (example)\nPinecone - Data Elixir is using this store for their chatbot; has a free tier\npostgres with pgvector: Supports exact and approximate nearest neighbor search; L2 distance, inner product, and cosine distance; any language with a Postgres client"
  },
  {
    "objectID": "qmd/db-warehouses.html#sec-db-ware-misc",
    "href": "qmd/db-warehouses.html#sec-db-ware-misc",
    "title": "Warehouses",
    "section": "Misc",
    "text": "Misc\n\nThe main difference between a â€œrelational databaseâ€ and a â€œdata warehouseâ€ is that the former is created and optimized to â€œrecordâ€ data, whilst the latter is created and built to â€œreact to analytics.â€\nOptimized for read-heavy workloads that scan a small number of columns across a very large number of rows and can easily scale to petabytes of data\nCons\n\nCan become expensive when an organization needs to scale them\nDo not perform well when handling unstructured or complex data formats.\n\nPros\n\nIntegrating multiple data sources in a single database for single queries\nMaintaining data history, improving data quality, and keeping data consistency\nProviding a central view for multiple source system across the enterprise\nRestructuring data for fast performance on complex queries"
  },
  {
    "objectID": "qmd/db-warehouses.html#olap-vs-oltp",
    "href": "qmd/db-warehouses.html#olap-vs-oltp",
    "title": "Warehouses",
    "section": "OLAP vs OLTP",
    "text": "OLAP vs OLTP\n\n\nOLAP (Online Analytical Processing)(aka the Cube)(Data Warehouses)\n\ndb designed to optimize performance in analysis-intensive applications\nAggregates transactions to be less frequent but more complex\nExamples: Snowflake, Bigquery\n\nOLTP (Online Transaction Processing) db designed for frequent, small transactions\n\nExecutes a number of transactions occurring concurrently (i.e.Â at the same time)\nUse cases: online banking, shopping, order entry, or sending text messages\n\nData model: OLTP systems typically use a normalized data model, which means that data is stored in multiple tables and relationships are defined between the tables. This allows for efficient data manipulation and ensures data integrity. OLAP systems, on the other hand, often use a denormalized data model, where data is stored in a single table or a small number of tables. This allows for faster querying, but can make data manipulation more difficult.\nData volume: OLTP systems typically deal with smaller amounts of data, while OLAP systems are designed to handle large volumes of data.\nQuery complexity: OLTP systems are designed to handle simple, short queries that involve a small number of records. OLAP systems, on the other hand, are optimized for more complex queries that may involve aggregating and analyzing large amounts of data.\nData updates: OLTP systems are designed to support frequent data updates and insertions, while OLAP systems are optimized for read-only access to data.\nConcurrency: OLTP systems are designed to support high levels of concurrency and handle a large number of transactions simultaneously. OLAP systems, on the other hand, are optimized for batch processing and may not perform as well with high levels of concurrency."
  },
  {
    "objectID": "qmd/db-warehouses.html#sec-db-ware-brands",
    "href": "qmd/db-warehouses.html#sec-db-ware-brands",
    "title": "Warehouses",
    "section": "Brands",
    "text": "Brands\n\nAmazon Redshift, DynamoDB, RDS, S3\n\nRedshift is best when you have data engineers who want control over infrastructure costs and tuning.\nRDS (Relational Database Service)\n\nBenefits over hosting db on EC2: AWS handles scaling, availability, backups, and software and operating system updates\n\nS3 is like googledrive or dropbox\n\nCon: only contains data about the files, not whatâ€™s inside them, i.e.Â no querying\nIdeal use cases\n\nBackup for logs,\nRaw sensor data for your IoT application,\nText files from user interviews\nImages\nTrained machine learning models (with the database simply storing the path to the object)\n\nAlternative: Minio\n\nOpen-Source alternative to AWS S3 storage.\nGiven that S3 often stores customer PII (either inadvertently via screenshots or actual structured JSON files), Minio is a great alternative to companies mindful of who has access to user data.\n\nOf course, AWS claims that AWS personnel doesnâ€™t have direct access to customer data, but by being closed-source, that statement is just a function of trust.\n\n\n\n\nGoogle BigQuery\n\nBest when you have very spiky workloads.\n\nSnowflake\n\nA cloud data warehouse for analytics. Itâ€™s columnar, which means that data is stored (under the hood) in entire columns instead of rows; this makes large analytical queries faster, so itâ€™s a common choice for how to build analytical DBs.\nBest when you have a more continuous usage pattern\nSupport for semi-structured data, data sharing, and data lake integration\nResource: Snowflake Data Warehouse Tutorials\n\nAzure Synapse Analytics\n\nFully managed, cloud-based data warehousing service offered by Microsoft Azure. It offers integration with Azure Machine Learning and support for real-time analytics.\n\nData Bricks\n\nCompany behind spark technology and have built a cloud-based data warehousing service.\n\nTeradata\nSAP HANA\nClickHouse\n\nOpensource, built by Yandex (Russian search engine)\n\nApache Hadoop running Apache Hive\n\nHive: an open-source data warehouse solution for Hadoop infrastructure. It is used to process structured data of large datasets and provides a way to run HiveQL queries.\n\nResource: Apache Hive Tutorial with Examples"
  },
  {
    "objectID": "qmd/db-warehouses.html#sec-db-ware-strat",
    "href": "qmd/db-warehouses.html#sec-db-ware-strat",
    "title": "Warehouses",
    "section": "Strategies",
    "text": "Strategies\n\nInmon\n\n\nPrioritizes accuracy and consistency of data above all else.\nQuerying is pretty fast (data marts)\nTends to be a lot of upfront work, however subsequent modifications and additions are quite efficient.\nRecommended if:\n\nData accuracy is the most important characteristic of your warehouse\nYou have time/resources to do a lot of upfront work\n\n\nKimball\n\n\nLess structured approach, which speeds up the initial development cycle.\nFuture iterations require the same amount of work, which can be costly if youâ€™re constantly updating the warehouse Fast querying but very few quality checks\nRecommended if:\n\nIf youâ€™re business requirements are well-defined and stable\nYou are querying lots of data often\n\n\nData Vault\n\n\nTrys to fix disadvantages of Kimball and Inmon strategies by waiting to the last minute to develop any kind of structure\nWorkflow: Sources â€“&gt; unstructured storage (data lake) â€“&gt; Staging which supports operations such as batch and streaming processes â€“&gt; data vault which stores all raw data virtually untouched (non-relational db?)\nAdvantages: efficient, fast to implement, and highly dynamic\nDisadvantages: querying can be quite slow\n\nUh doesnâ€™t seem to be much cleaning either\n\nRecommended if:\n\nYour business goals change often\nYou need cheap server and storage costs"
  },
  {
    "objectID": "qmd/db-warehouses.html#sec-db-ware-dsgn",
    "href": "qmd/db-warehouses.html#sec-db-ware-dsgn",
    "title": "Warehouses",
    "section": "Design a Warehouse",
    "text": "Design a Warehouse\n\nMisc\n\nOracle Data Model Documentation\n\nConsiderations\n\n7 Vs\n\nVolume: How big is the incoming data stream and how much storage is needed?\nVelocity: Refers to speed in which the data is generated and how quickly it needs to be accessed.\nVariety: What format the data needs to be stored? Structured such as tables or Unstructured such as text, images, etc.\nValue: What value is derived from storing all the data?\nVeracity:How trustworthy the data source, type and its processing are?\nViscosity: How the data flows through the stream and what is the resistance and the processability?\nVirality: Ability of the data to be distributed over the networks and its dispersion rate across the users_\n\nData Quality (See Database, Engineering &gt;&gt; Data Quality) completeness, uniqueness, timeliness, validity, accuracy, and consistency\n\nComponents\n\n\nMetamodeling:\n\nDefines how the conceptual, logical, and physical models are consistently linked together.\nProvides a standardized way of defining and describing models and their components (i.e.Â grammar, vocabulary), which helps ensure consistency and clarity in the development and use of these models.\nData ownership should be assigned based on a mapping of data domains to the business architecture domains (i.e.Â market tables to the marketing department?)\n\nConceptual Modeling - Involves creating business-oriented views of data that capture the major entities, relationships, and attributes involved in particular domains such as Customers, Employees, and Products.\nLogical Modeling - Involves refining the conceptual model by adding more detail, such as specifying data types, keys, and relationships between entities, and by breaking conceptual domains out into logical attributes, such as Customer Name, Employee Name, and Product SKU.\nPhysical Data Modeling - Involves translating the logical data model into specific database schemas that can be implemented on a particular technology platform\n\nProcess (article, article, article)\n\nUnderstand the Core Business Requirements\n\nCreate a catalogue of reporting stories for each stakeholder to an idea of the reports that each will want generated\n\nThese will inform you of the data requirements\ne.g.Â â€œAs a marketing manager, I need to know the number of products the customer bought last year in order to target them with an upsell offer.â€\n\nFrom the story above, I can determine that we will need to aggregate the number of products per customer based on sales from the previous year.\n\n\n\nSelect the tools and technologies:\n\nUsed to build and manage the data warehouse. This may include selecting a database management system (DBMS), data integration and extraction tools, and analysis and visualization tools.\nWarehouses - See Brands\nSee Production, Tools &gt;&gt;\n\nOrchestration\nELT/ETL Operations\n\n\nChoose a data model\n\nIdentify Business Processes\n\nFocus on business process and not business departments as many departments share the same business process\nIf we focus on department, we might end up with multiple copies of models and have different sources of truth.\n\nChoose a data model from the Business Process\n\nStart with the most impactful model with the lowest risk\n\nConsult with the stakeholders\n\nShould be used frequently and be critical to the business and also it must be built accurately\n\nDecide on the data granularity\n\nMost atomic level is the safest choice since all the types of queries is typically unknown\nNeed to consider the size and complexity of the data at the various granularities, as well as the resources available/costs for storing and processing it.\nExamples\n\nCustomer Level - easy to answer questions about individual customers, such as their purchase history or demographic information.\nTransaction Level - easy to answer questions about individual transactions, such as the products purchased and the total amount spent.\nDaily or Monthly?\n\n\n\nCreate Conceptual Data Models (Tables)\n\nThese represent abstract relationships that are part of your business process.\nExplains at the highest level what respective domains or concepts are, and how they are related.\nThe elements within the reporting stories should be consistent with these models\n\nExample: Retail Sales\n\nTime, Location, Product, and Customer.\n\nTime might be used to track sales data over different time periods (e.g.Â daily, monthly, yearly).\nLocation might be used to track sales data by store or region.\nProduct might be used to track sales data by product category or specific product.\nCustomer might be used to track sales data by customer demographics or customer loyalty status.\n\n\n\nExample:\n\n\nTransactions form a key concept, where each transaction can be linked to the Products that were sold, the Customer that bought them, the method of Payment, and the Store the purchase was made in â€” each of which constitute their own concept.\nConnectors show that each individual transaction can have at most one customer, store, or employee associated with it, but these in turn can be associated with many transactions (multi-prong connector into Transactions)\n\nExample:\n\n\nEach Customer (1 prong connector) can have 0 or more Orders (multi-prong connector)\nEach Order can have 1 or more Products\nEach Product can have 0 or more Orders\n\n\nCreate Logical Data Models\n\nBreakdown each entity of the conceptual model into attributes\n\nExample:\n\nExample:\n\n\n\nCreate Physical Data Models\n\nDetails are added on where exactly (e.g., in what table), and in what format, these data attributes exist.\n\ne.g.Â finalizing table names, column names, data types, indexes, constraints, and other database objects\n\nTranslate the logical data model into specific database schemas that can be implemented on a particular technology platform\n\ne.g.Â dimensional modelling in a star schema or normalisation in a 3rd normal form in a snowflake model.\n\nExample:\n\nExample: Dimension model in a star schema\n\n\nfact_ (quantitative) and dim_ (qualitative)\n\n\nMake Design and Environment decisions\n\nDecide on:\n\nPhysical data models\nHistory requirements\nEnvironment provisions & set up\n\n\nBuild a prototype (aka wireframe) of the end product\n\nThe business end-user may have a vision, they couldnâ€™t coherently articulate at the requirement phase.\nThe prototype need not use real-world data or be in the reporting tool.\n\n** Profile known sources data **\n\nLearn about the data quality issues, and try and remediate those issues before designing your data pipelines.\n\nIf an issue cannot be resolved, you will have to handle it in your data pipeline\n\n\nBuild, Test, and Iterate\n\nCreate ETL jobs or data pipelines\n\nIteratively need to unit test the individual components of the pipeline.\n\nThe data will need to be moved from the source system into our physical warehouse\nProfile data\n\nData types, and if conversion is required\nThe amount of history that needs to be pulled\n\nValidate the modelâ€™s output numbers with the business end-user\n\nProgress towards Data Maturity (see Job, Organizational and Team Development &gt;&gt; Data Maturity)"
  },
  {
    "objectID": "qmd/db-warehouses.html#sec-db-ware-trig",
    "href": "qmd/db-warehouses.html#sec-db-ware-trig",
    "title": "Warehouses",
    "section": "Database Triggers",
    "text": "Database Triggers\n\n\nA database trigger is a function that gets triggered every time a record is created or updated (or even deleted) in the source table (in this case, a transactional table)\nDatabase triggers provide an effective, solution to extracting data from the transactional system and seamlessly integrating it into the data warehouse while also not adversely impacting that system.\nUse case â€” You see a couple of data points in your transactional systemâ€™s tables that you would require for your reporting metrics but these data points are not being provided by your transactional systemâ€™s API endpoints. So, there is no way you can write a script in Python or Java to grab these data points using the API. You cannot use direct querying on your transactional system as it can negatively impact its performance.\nMisc\n\nNotes from Harnessing Triggers in the Absence of API Endpoints\n\nProvides a detailed step-by-step\n\nIf your transactional system does not have a lot of traffic (or) is not directly used by end-user applications, then it can be set up as a synchronous process. In that case, the lambda or the Azure functions would need to have the trigger event as the transactional databaseâ€™s staging table. The appropriate database connection information would also need to be provided.\n\nDatabase Triggers\n\nDDL Triggers - Set up whenever you want to get notified of structural changes in your database\n\nUseful when you wish to get alerted every time a new schema is defined; or when a new table is created or dropped. Hence, the name DDL (Data Definition Language) triggers.\n\nDML Triggers - Fired when new records are inserted, deleted, or updated\n\ni.e.Â Youâ€™re notified anytime a data manipulation change happens in a system.\n\n\nSyntax: &lt;Timing&gt; &lt;Event&gt;\n\nTrigger Event - The action that should activate the trigger.\nTrigger Timing - Whether you need the trigger to perform an activity before the event occurs or after the event occurs.\n\nSpecialized triggers provided by cloud services\n\nAWS\n\nLambda Triggers: These triggers help initiate a lambda function when a specified event happens. Events can be internal to AWS, or external in nature. Internal events can be related to AWS services such as Amazon S3, Amazon DynamoDB streams, or Amazon Kinesis. External events can come in from the database trigger of a transactional system outside of AWS or an IoT event.\nCloudwatch Events: If you have used standalone relational databases such as Microsoft SQL Server and SQL Server Management Studio (SSMS), you may have used SQL Server Agent to notify users of a job failure. Cloudwatch is specific to AWS and is used not only to notify users of a job failure but also to trigger Lambda functions and to respond to events. The important difference between a CloudWatch Event and a Lambda Trigger is that while Lambda triggers refer to the capability of AWS Lambda to respond to events, CloudWatch Events is a broader event management service that can handle events from sources beyond Lambda. On a side note, while SQL Server Agent requires an email server to be configured, Cloudwatch has no such requirement.\n\nAzure\n\nBlob Trigger: Azure blobs are similar to S3 buckets offered by AWS. Similar to how Amazon S3 notifications can be used to get alerts about changes in S3 buckets; blob triggers can be used to get notified of changes in Azure blob containers.\nAzure Function Trigger: These are the Azure equivalent of AWS Lambda Function Triggers. These triggers can be used to initiate an Azure function in response to an event within Azure or an external event, such as an external transactional database trigger, an HTTP request, or an IoT event hub stream. Azure functions can also be initiated based on a pre-defined schedule using a Timer Trigger.\n\n\nExample: Transfer data from a transactional database to a warehouse (See article for further details)\n\nIdentify table in transactional db with data you want\nCreate a staging table thatâ€™s exactly like the transaction table\n\nEnsure that you donâ€™t have any additional constraints copied over from the source transactional table. This is to ensure as minimal impact as possible on the transactional system.\nFor a bulk data transfer of historical transaction data:\n\nCREATE TABLE AS SELECT (SELECT * INTO in SQL Server) while creating the staging table. This will create the staging table pre-populated with all the data currently available in the transaction table.\nDo an empty UPDATE on all the records in the transaction table\n\ne.g.Â UPDATE TABLE Pricing_info SET OperationDate=OperationDate\nThis is not a recommended approach as it could bog down the transactional system due to the number of updates and undo statements generated. Moreover, the transaction table will also be locked during the entire update operation and will be unavailable for other processes thus impacting the transactional system. This method is okay to use if your transaction table is extremely small in size.\n\n\nIn addition to that, also have a column to indicate the operation performed such as Insert, Update, Delete).\n\nSet up a DML trigger directly on the transaction table\n\nAll DML events namely Insert, Delete, and Update in the transaction table should have a separate trigger assigned to them.\n\nThe below example shows the trigger for Insert. The rest of the triggers are created similarily â€” just by substituting 2 INSERTs (trigger event, select statement) for DELETE or UPDATE (See article for code) and using a different name in CREATE\n\nInsert trigger in (SQL Server)\n-- Create the trigger\nCREATE TRIGGER TransactionTrigger_pricing_Insert\nON Pricing_info\n--Trigger Event\nAFTER INSERT\nAS\nBEGIN\n    -- Insert new records into the staging table\n    INSERT INTO StagingTable_pricing (ID, Column1, Column2, OperationType)\n    SELECT ID, Column1, Column2, 'INSERT'\n    FROM inserted\nEND;\n\nâ€œPricing_infoâ€ is the name of transactional table with the data you want\nâ€œStagingTable_pricingâ€ is the name of the staging table\nAFTER INSERT where AFTER is the trigger timing and INSERT is the trigger event\nIn the SELECT statement, â€œINSERTâ€ is the value for that extra column in the staging table that tells us which type of operation this was.\n\n\nSet-up the specialized trigger in the warehouse\n\nAWS \n\nA database DML trigger in the transactional systemâ€™s database. Whenever a new record comes into the transactional database table, the trigger would insert the new data into a staging table within the transactional database.\n\nIf you based it on a schedule (using AWS Cloudwatch events), the Lambda trigger would trigger a lambda function to grab the data from the staging table to a table in the datawarehouse (Redshift)\n\n\nAzure \n\nWhen the timer trigger activates, it would run the Azure Function which would then pick up the new/updated/deleted records from the staging table."
  },
  {
    "objectID": "qmd/decision-intelligence.html#sec-decint-misc",
    "href": "qmd/decision-intelligence.html#sec-decint-misc",
    "title": "13Â  Decision Intelligence",
    "section": "13.1 Misc",
    "text": "13.1 Misc\n\nAlso see\n\nEconometrics, Discrete Choice Models\nRegression, Multinomial\n\nNotes from\n\nDecision Theory video playlist"
  },
  {
    "objectID": "qmd/decision-intelligence.html#sec-decint-terms",
    "href": "qmd/decision-intelligence.html#sec-decint-terms",
    "title": "13Â  Decision Intelligence",
    "section": "13.2 Terms",
    "text": "13.2 Terms\n\nAlternative - The levels of a polytomous response in Random Utility Models.\nChoice Probability - The probability that a decision-maker will chose a particular alternative. The predicted response for a Random Utility Model.\nCertainty Equivalent - Amount someone would pay to enter a lottery\nLottery - decision-making situation usually involving a probability for each outcome\nMarket Share - The percentage of total sales in an industry or product category that belong to a particular company during a discrete period of time. For a Random Utility Model, when the data is at the market level instead of the individual level, the predicted response is the Market Share.\nRisk/ Insurance Premium - The difference between the Certainty Equivalent and Expected Reward\n\nItâ€™s how insurance companies make their money\n\nUtility - total usefulness or enjoyment or satisfaction received from a decision or action\nUtility Theory - The idea that people behave in line with self-interset where self-interest reflects peoplesâ€™ needs to save time and economize on effort.\nValue of Perfect Information (VPI) - The difference between 2 different influence diagrams.\n\nIf one diagram has an additional variable or the same variables but different probabilities of states, then the difference can tell us the value of that extra information (i.e.Â new variable) or you can compare different scenarios based on the different probabilities for particular states."
  },
  {
    "objectID": "qmd/decision-intelligence.html#sec-decint-matnot",
    "href": "qmd/decision-intelligence.html#sec-decint-matnot",
    "title": "13Â  Decision Intelligence",
    "section": "13.3 Mathematical Notation",
    "text": "13.3 Mathematical Notation\n\nD( ) : Decision-making situation (e.g.Â Deciding whether to purchase insurance or not)\nVal(A) : Set of all possible actions (e.g.Â performing a biopsy, taking a drug)\nVal(X) : Set of all possible states or outcomes (e.g.Â discovery of cancer, death)\nP(X|A) : Probability of a state given an action has taken place\nU(X,A) : Utility function; assigns a numerical utility for each state and action combination\nEU[D(a)] : Expected utility of a decision-making situation (aka lottery) for a particular action, a\n\n\nThe goal is to choose an action that maximizes this value (aka Maximized Expected Utility (MEU))"
  },
  {
    "objectID": "qmd/decision-intelligence.html#sec-decint-frmwrk",
    "href": "qmd/decision-intelligence.html#sec-decint-frmwrk",
    "title": "13Â  Decision Intelligence",
    "section": "13.4 Framework",
    "text": "13.4 Framework\n\n\nMisc\n\nNotes from A Framework for Embedding Decision Intelligence into your Organization\n\nArticle seems to draw heavily from Lorien Prattâ€™s book, â€œLinkâ€\n\nPrattâ€™s Website has a lot of free resources\n\n\nBasics of Causal Design Diagrams (CDD) Link\n\nThe example used is a product manager responsible for the first production run of a new product\n\n\nEnabling Factors\n\nAny DI framework is unlikely to succeed without a corporate environment that enables the application of decision intelligence techniques to a corporate context\nAn organizationâ€™s decision culture (values, desire for transparency in decision-making, culture of continuous improvement, etc.) is a critical enabling factor that supports a DI approach to decision-making\nOrganizational processes such as governance, risk management, and program/project management provide a vehicle for building decision-supportive practices into corporate processes\nAn organizationâ€™s technological capabilities and infrastructure are critical to ensuring that data assets can be stored, transformed, visualized, analyzed, etc. Similarly, these capabilities enable storage/retrieval of decision assets, and provide collaboration capabilities and tooling that are critical to holding facilitated discussions that are critical to decision intelligence\n\nPre-Decision Activities\n\nDecision Framing\n\nSets the boundaries of the decision â€” what are my actions (levers), outcomes (desired or otherwise), and contingencies (risks and externalities)?\nQuestions to address before looking at the data\nDecision Mapping Create the CDD by graphically represent causal relationships between actions, intermediaries, and desired outcomes See link above for the basics of CDDs\n\nIt presents causal links that serve as points of integration for data and analytic products, metrics, information, predictive models, etc. and in this way orchestrate multiple inputs into a single decision\nIt presents an opportunity to revisit the entire decision process for post-decision review and provides for the re-use of decision artifacts\n\n\nData and Analytics Integration\n\nidentify where and how data or analytics can support downstream pre-decision processes (e.g.Â modeling), or how to determine whether an action will have a desired outcome\nAttach specific data, metrics, analytics, predictive models, etc. to sub-components of a decision.\nConsider blocks of sub-components (for example, clusters of action-link-outcome or similar) that are known to have relationships that are governed by single aggregate relationship that describes actions and outcomes\nUse CDD as a scaffold for integrating data and analytics.\n\nIdentifying Uncertainty\n\nBy proactively identifying areas of uncertainty, we are preparing ourselves to acknowledge the limitations of our decision process and giving ourselves a chance to identify systematically unresolvable uncertainty, or areas where we might reduce the level of uncertainty in our decision-making.\nSources of uncertainty can be external to your organization, internal, or driven by inherent issues with data-quality\nIn the CDD, look at each causal link\n\nIdentify uncertainty from a Rumsfeldian perspective\n\nWhere data and analytics have been integrated\n\nFrom a statistical perspective â€” whatâ€™s the error? what does the quality of the data look like?\n\n\nApproaches for making decisions under uncertainty\n\nModeling\n\nQuantitatively model aggregated action-to-outcome links within the CDD\nPotentially can assess the aggregate impact of action(s) on decision outcomes or intermediates\n\nApproach\n\n\nPost-Decision Activities\n\nDecision Retrospective\n\nEven the â€œbestâ€ decisions can be wrong and the most effective decisions eventually become obsolete.\nDonâ€™t evaluate the decision totally on the outcome (i.e.Â outcome bias) but also on the decision-making process\n\nRetention of Decision Artifacts\n\nA well documented decision provides a rich opportunity to retain and re-use all these decision artifacts for future decisions\nMain Components\n\nclear frame, a clear design, potentially a decision map and link to data and analytics, lessons learned\n\nSub Components\n\nany information that has been hypothesized and tested through an action â€” causation â€” outcome link"
  },
  {
    "objectID": "qmd/decision-intelligence.html#sec-decint-maxexput",
    "href": "qmd/decision-intelligence.html#sec-decint-maxexput",
    "title": "13Â  Decision Intelligence",
    "section": "13.5 Maximizing Expected Utility",
    "text": "13.5 Maximizing Expected Utility\n\nInfluence Diagram\n\nShapes\n\nOvals: random variables\nRectangles: actions\nDiamond: Utility\n\n\nExample (found a start-up or get a job)\n\nMarket and Found both influence utility\nMarket gives the probability of there being a bad, moderate, or good market for the start-up, m0, m1, or m2 respectively\nFound is binary action; Found (f1) or Donâ€™t Found (f0) a start-up (i.e.Â get a job)\nU is the utility\n\nSee Utility Functions for details on how this is calculated\n\nExpected utility calculation for a bad market and founding a start-up:\n\nU = m0 * U(m0, f1) = 0.5 * -7 = -3.5\n\nExpected Utility for f1, EU(f1) = (0.5 * -7) + (0.3 * 5) + (0.2 * 20) = 2 which is higher than EU(f0) = 0, so the person should found the start-up"
  },
  {
    "objectID": "qmd/decision-intelligence.html#sec-decint-meuudr",
    "href": "qmd/decision-intelligence.html#sec-decint-meuudr",
    "title": "13Â  Decision Intelligence",
    "section": "13.6 Maximizing Expected Utility Using Decision Rules",
    "text": "13.6 Maximizing Expected Utility Using Decision Rules\n\nInformation is used create a Decision Rule to inform the agent on which Action maximizes Utility\nExpected Utility with prior information\n\n\nTo calculate the Expected Utility this equation gets manipulated into\n\n\n** See the Example for a clearer understanding of how to utilize all of this **\nBasically there are two parts:\n\nthe decision rule distribution (left summation, over Z and A)\nthe parent variable to the action (right summation, over W)\nwhere\n\n\nSays Z is the parent variable to the Action and W is the rest of the variables that arenâ€™t Z\n\n\nStep function the Decision Rule\n\nSays for an action value, a, given a parent variable value, z, perform that action (1) if it has the max utility and otherwise donâ€™t perform that action (0)\n\n\n\nExample (Decision Rule based on prior information)\n\n\nSee Maximizing Expected Utility &gt;&gt; Example for details on all variables (except Survey)\nSurvey is added and used to create a decision rule, Î´, which will maximize the utility and determine the action (found/not found) either probabilistically (more than 1 agent making a decision) or deterministically (1 agent)\n\nIt surveys people to determine what the probability of bad, moderate, or good market will be\nExample for reading the Survey conditional probability distribution matrix:\n\nIf the true probability that market will be bad is m0 = 0.5, then the probability that survey will say that it will be a bad market, s0, is 0.6.\n\n\nThe decision rule, Î´(F,S), is a conditional probability distribution shown as the matrix on the bottom with values of S as rows and values of F as columns\n\nThe decision rule is:\n\nif s0 â€“&gt; f0 (since 0 &gt; -1.25)\nif s1 â€“&gt; f1 (since 1.15 &gt; 0)\nif s2 â€“&gt; f1 (since 2.1 &gt; 0)\n\n\nThe maximized expected utility = max_utility(s0) + max_utility(s1) + max_utility(s2) = 0 + 1.15 + 2.10 = 3.25\n\nNote how adding Survey increased the maximized expected utility from 2 (from Maximizing Expected Utility &gt;&gt; Example) to 3.25\nValue of Perfect Information, VPI = 3.25 - 2.00 = 1.25\n\nTherefore, the agent should be willing to spend less that 1.25 utility points to conduct the survey\n\n\nThe Expected Utility w/prior information equation gets values substituted into it and is factored a bit.\n\nPÎ´A(x, a) becomes P(M) * P(S|M) * Î´F(F|S) which is Market * Survey * Found\nU(x, a) becomes U(F, M) which is Utility\nThe decision rule distribution is factored out of the group leaving a group that is summed over the parent variable to the Action which is Found\n\nComputing the values to the Decision Rule matrix\n\nSteps\n\nChoose a value for each decision rule distribution variable (e.g.Â S, F)\nFor each value of M, calculate P(M) * P(S|M) * U(F, M) then sum all the products\nRepeat for each combination of the decision rule distribution variables\n\nExample\n\nLet S = s0 an F = f1\nFor m0: 0.5 * 0.6 * -7 = -2.1\nFor m1: 0.3 * 0.3 * 5 = 0.45\nFor m2: 0.2 * 0.1 * 20 = 0.40\n-2.1 + 0.45 + 0.4 = -1.25"
  },
  {
    "objectID": "qmd/decision-intelligence.html#sec-decint-utfun",
    "href": "qmd/decision-intelligence.html#sec-decint-utfun",
    "title": "13Â  Decision Intelligence",
    "section": "13.7 Utility Functions",
    "text": "13.7 Utility Functions\n\nMisc\n\nAll attibutes/tradeoffs (e.g.Â money, time, pleasure) that affect the agentâ€™s preferences must be included into the utility function\nFor complicated systems, the Utility is decomposed into separate sub-utility functions with various numbers of variables and actions influencing each. The Total Expected Utility for an action is the sum of the separate expected utilities for that action.\n\nExample:Â  The action of studying at school\n\nDifferent utilities arise from studying (e.g.Â getting good grades feels good, good job because grades are good feels good, college life may suffer because of increased studying) with different variables influencing each. To get the total expected utility from studying or not studying, youâ€™d need to sum the individual utilities.\n\nExample: Prenatal diagnosis\n\nAttributes\n\nTesting - how painful is the testing procedure\nKnowledge - knowing whether your baby has down syndrome\nDown Syndrome - probability the baby has down syndrome\nLoss of Fetus - testing procedure comes with a risk of losing the fetus\nFuture Pregnancy - Any issues found may affect future pregnancies\n\nTotal Utility = U(T) + U(K) + U(DS, LoF) + U(LoF, FP)\n\n\nUtility Curve Types\n\nRisk Averse - concave (most common)\nRisk Neutral - straight line\nRisk Seeking - convex\n\nExpected Reward &lt; Certainty Equivalent which means the Risk Premium is negative\n\nPeople willing to pay more for low probability of winning a large reward\n\ne.g.Â Gambling at a casino\n\n\nMicromort - A 1/1M chance of death is worth $20 (1980 dollars)\n\nUseful for including death into a utility function and ranking utilities for various attributes\n\nQALY (quality adjusted life year) - attribute thatâ€™s common in medical field\n\nExample (Risk Averse Curve)\n\n\nD = Decision-making scenario (aka lottery)\np = 0.50\nExpected Utility line for the lottery is a linear combination of p and expected reward (represented by the dotted line)\n\nLow p â€“&gt; expected utility is closer to zero\nHigh p â€“&gt; expected utility is closer to the point on the curve above the reward value (e.g.Â $1000)\n\nExpected Reward for the lottery = p * reward1 + p* reward2 + â€¦ = 0.50 * $1000 + 0.50 * $0 = $500\nThe Certainty Equivalent = $400\n\nItâ€™s the Reward value according to the point on the Utility curve thatâ€™s horizonally across from the point thatâ€™s above the Expected Reward on the Expected Utility line .\n\nRisk Premium = Expected Reward of lottery - Certainty Equivalent = $100"
  },
  {
    "objectID": "qmd/decision-intelligence.html#sec-decint-conjanal",
    "href": "qmd/decision-intelligence.html#sec-decint-conjanal",
    "title": "13Â  Decision Intelligence",
    "section": "13.8 Conjoint Analysis",
    "text": "13.8 Conjoint Analysis\n\nNotes from\n\nModeling Consumer Decisions: Conjoint Analysis\n\nMisc\n\nUsed to understand the relative importance/preference of attributes and quantify the utility a consumer gains from each attribute of a product\n\nModels the trade-offs a consumer might make while making a purchase decision\n\n\nAssumptions\n\nConsumers purchase the product which gives them the highest total utility (sum of individual attribute utilities)\nConsumers follow a compensatory decision-making process. Simply speaking, this means that a positive attribute of a product can compensate for a negative attribute, i.e., customers are willing to make trade-offs.\n\nSteps\n\nConduct a market research study\n\nBrainstorm on attributes that the consumer might use to make a decision on purchases your product\n\ne.g.Â Smartphone: Ram, Storage, Camera, Screen, Brand, and Price\n\nStratify sample and survey on these attributes\n\n\nFor each Set (â€œChoice Setâ€), a partipant selects one group of attributes\nActual questionaire will have 10 to 20 choice sets based on the number of attributes of the product\n\nRecord answers\n\n\nâ€œChoiceâ€ = 1 if that â€œAlternativeâ€ is selected by the participant\n\nâ€œAlternativeâ€ being the particular group of attributes in that Choice Set\n\nIâ€™d add whichever participant characteristics were used in the stratified sampling for mixed modeling (see next step)\n\nFit a logistic regression\nmodel &lt;- glm(Choice ~ 0 + Ram + Storage + Camera + Screen + Brand + Price, Data = Data, Family = Binomial)\n\nIntercept is forced to 0 so when all the dependent variables are 0, there should technically be 0 utility for the product\nSeems like since we stratify sampled, we should use a mixed model approach to get more out of the data\n\n\nSummarize utilities for each attribute\n\n\nthe log-odds that we model using Logistic Regression represent the utility the consumer gains from an attribute\n\ne.g.Â a $1 increase in â€˜Priceâ€™ results in a 0.08 unit decrease in utility on average for our customers\n\n\nCalculate the total utility and probability of purchase\n\n\nPotential attribute value combinations can be fed to the model and probability of purchase can be predicted\nI donâ€™t think this is correct for â€œTotal Utility.â€ Utilities have to be on the same scale and therefore all the variables would have to be on the same scale, i.e.Â all continuous + standardized or all discrete, in order for this sum to be correct.\n\nMaybe standardized continuous and discrete variables would work"
  },
  {
    "objectID": "qmd/decision-intelligence.html#sec-decint-deccurv",
    "href": "qmd/decision-intelligence.html#sec-decint-deccurv",
    "title": "13Â  Decision Intelligence",
    "section": "13.9 Decision Curves",
    "text": "13.9 Decision Curves\n\nDecision Curve Analysis (DCA)\n\nCompares prediction models by looking at net benefit across a range of threshold probabilities\ntl;dr\n\nDesigned for medical research\nDCA Net Benefit focuses on TP and FP which wonâ€™t necessarily be an optimal metric for general use outside the medical field.\nDCA Net Benefit has an advantage over Traditional Decision Analysis (DA) Net Benefit (see last section) in that utilities for various scenarios doenâ€™t have to be estimated.\nThis is used for model comparison and not (necessarily) choosing a probability threshold\n\nThe threshold is chosen according to the risk tolerance of the patient (or doctor).\n\nExample: how large a probability of cancer is acceptable until acceding to having a painful biopsy in order to confirm the presence of the cancer.\n\n\nChoice of model (by highest net benefit) may depend on the threshold you choose.\n\nOne model may be best only for a certain range of probability thresholds.\n\n\nMisc\n\nTutorial\n\nI only read the section for binary treatment variables but survival models were also covered Decision curve analysis for quantifying the additional benefit of a new marker (code button at VERY top of page)\n\n\nGeneral interpretations for all Net Benefit vs Threshold Probability Charts\n\nWhere the â€œtreat everyoneâ€ base modelâ€™s net benefit = 0 (i.e line crosses the x-axis), that threshold probability is the (disease) prevalence.\n\nIf you have a representative sample, then the prevalence can also be calculated by number_with_disease / sample size\n\nWhere the net benefit for a binary predictor variable = 0, the threshold probability is the positive predictive value.\n\nThis is because for a binary variable, a patient with the characteristic is given a risk at the positive predictive value. (?)\n\nMedical interpretation of net benefit\n\nExample: Biopsy is taken when the probability of cancer (outcome) is above a certain threshold\n\nNet benefit is calculated using a probability of cancer model with a genetic marker as the predictor.\nFor Net Benefit = 0.03 at a threshold probability of 20%\nInterpretation: â€œCompared to conducting no biopsies, biopsying on the basis of the marker is the equivalent of a strategy that found 3 cancers per 100 patients without conducting any unnecessary biopsies.â€\n\n\n\nReporting\n\nIncrease in net benefit = net benefit of your model - net benefit of the treat-all model (aka base model 1)\noutput = dca(data=data.set, outcome=\"cancer\", predictors=\"marker\", probability=F,\nÂ  Â  Â  Â  Â  Â  xstart=0.05, xstop=0.35, xby=0.05, graph=F)\nincrease_in_nb = output$net.benefit$marker - output$net.benefit$all\n\n\nDCA prediction models are compared to two base models:\n\nbase model 1: assume that all patients are test positive and therefore treat everyone\nbase model 2: assume that all patients are test negative and offer treatment to no one\n\nThe range of threshold probabilities where the net benefit of the predictor &gt; than net benefit for two base models is a measure of the value that the predictor adds\nThe dca function isnâ€™t part of a package. It was in a zip file for the tutorial. Script located in Code &gt;&gt; diagnostics-classification\ndca &lt;- function(data, outcome, predictors, xstart=0.01, xstop=0.99, xby=0.01,Â \nÂ  ymin=-0.05, probability=NULL, harm=NULL,graph=TRUE, intervention=FALSE,Â \nÂ  interventionper=100, smooth=FALSE,loess.span=0.10)\n\nxstop: ending threshold\nxstart: starting threshold (default: 0)\nxby: stepsize for threshold range\npredictors: variables can be conditonal probabilities (i.e.Â P(y|X), the fitted values from a logistic regression) or not\n\nIf a predictor is NOT conditional probability, then the probability arg needs set to FALSE for that variableÂ \n\nprobability: whether each predictorâ€™s values are conditional probabilities or not\n\nvector of TRUEs/FALSEs.\ndefault: TRUE for each predictor (even though it says NULL in the arg)\nLength of the vector is the number of predictors\n\n\nSteps: dca function (for each predictor, m, and threshold value, t)\n\nCalculate net benefit for base model 1\nevent.rate=colMeans(data[outcome])\nnb[\"all_treated\"]=event.rate - (1-event.rate)*nb$threshold/(1-nb$threshold)\n\nvectorized calculation where threshold has all the threshold values\nevent.rate is a constant but I think heâ€™s using colMeans because data[outcome]Â  is a df with one column\n\nSet net benefit for base model 2 to 0: nb[\"none_treated\"]=0\nIf probability = FALSE for a predictor\n\nA simple logistic regression is fit (Y~X) and the fitted probabilties are used\n\n(avg?) TP and FP are calculated for each threshold and predictorÂ  Â  Â  Â \ntp = mean( data[data[[predictors[m]]] &gt;= nb$threshold[t], outcome]) * sum( data[[predictors[m]]] &gt;= nb$threshold[t] )\nfp = (1 - mean( data[data[[predictors[m]]] &gt;= nb$threshold[t], outcome])) * sum( data[[predictors[m]]] &gt;= nb$threshold[t] )\n\ntp = mean( outcome (where predictor &gt;= threshold) * sum( predictor (where predictor &gt;= threshold))\nHeâ€™s taking an average here and normally TP and FP are counts, so Iâ€™m not sure how to interpret these\n\nNet benefit is calculated by looping (nested) through each threshold and predictor\n# net benefit\nnb[t,predictors[m]] = tp/N - fp/N*(nb$threshold[t]/(1-nb$threshold[t])) - harm[m]\n\n(model based) net benefit = [probability_threshold / (1 - probability_threshold)] * [(TP/N) - (FP/N)]\n\nIf the â€œharmâ€ argument is used, (model-based) net benefit w/harm = (model-based) net benefit - harm\n\nN is the number of observations\nharm is an arg for adding an extra penalty\nData for examples\n\n\nMarker is a numeric\ncancerpredmarker is fitted probabilities from a logistic regression\n\nExample: Compare Multivariable model to Univariate model\n\n# fit multivariable logistic regression\nmodel = glm(cancer ~ marker + age + famhistory, family=binomial(link=\"logit\"))Â \n#save predictions in the form of probabilitiesÂ \ndata.set$cancerpredmarker = predict(model, type=\"response\")\n# dca for each model\ndca(data=data.set, outcome=\"cancer\", predictors=c(\"cancerpredmarker\",\"famhistory\"),Â \nxstop=0.35)\nMultivariable DCA involves getting conditional probabilities from the multivariable model to use as a predictor in the dca function\n\nmarker is the binary treatment variable with age, family history as adjustment variables\n\nInterpretation\n\nThe multivarible model with the genetic marker has substantially more benefit than the univariate family history model at low probability thresholds (risk averse) and moderate risk thresholds (risk neutral?).\n\nExample: A Bad Model\n\n\nInterpretation\n\nThe â€œBrown modelâ€ (fictitious) only shows a positive net benefit for lower threshold probabilities and is harmful (i.e.Â negative net benefit) even for moderate threshold probabilities.\n\ni.e.Â itâ€™s only has limited benefit for the most risk averse patients\n\n\n\nExample: Conditional and Joint application\n\nApproaches (besides base models):\n\nBiopsy everyone that was determined to be at high risk of cancer; donâ€™t use the marker\nMeasure the marker for everyone, then biopsy anyone who is either at high risk of cancer or who was determined to have a probability of cancer past a certain level, based on the marker\n\njoint approach part is risk_group == â€œhigh riskâ€ or marker probability &gt; some_amount)\n\nBiopsy everyone at high risk; measure the marker for patients at intermediate risk and biopsy those with a probability of cancer past a certain level, based on the marker\n\nconditional approach part is risk_group == â€œintermediateâ€ and marker probability &gt; some_amount)\n\n\nCreate indicator variables for each approach\n# univariate (marginal?) approach\n# Treat only risk_group == high risk\n# This will be 1 for treat and 0 for donâ€™t treat\ndata.set$high_risk = ifelse(risk_group==\"high\", 1, 0)\n\n# Treat based on Joint Approach\ndata.set$joint = ifelse(risk_group==\"high\" | cancerpredmarker &gt; 0.15, 1, 0)\n\n# Treat based on Conditional Approach\ndata.set$conditional = ifelse(risk_group==\"high\" | (risk_group==\"intermediate\" &\ncancerpredmarker &gt; 0.15), 1, 0)\n\nRun dca dca(data=data.set, outcome=\"cancer\", predictors=c(\"high_risk\", \"joint\", \"conditional\"), xstop=0.35)\n\nInterpretation\n\nâ€œLess than 5%, the clinical option of treating all would be superior to any other option, though rarely would treatment thresholds be so lowâ€\nJoint approach is best for low risk thresholds (i.e.Â threshold probabilities) until around 0.26 when the Conditional approach is best, then all 3 seem be minimally beneficial for moderate risk thresholds\n\nThe disadvantage of the joint test is that the marker needs to be measured for everyone, and such tests may be expensive and time consuming\n\nTreat high risk has pretty near constant net benefit but minimal\n\nExample: Incorporating Harm\n\nSame as previous example except the clinician says only willing to conduct 30 genetic marker tests per 1 positive test result\n\nPotential reasons: difficulty in conducting the test; cost, etc.\nHarm is set to 1 / 30 = 0.03333â€¦\n\n\nharm_marker = 0.0333\n\n# conditional approach: only patients at intermediate risk have their marker measured\nintermediate_risk = ifelse(risk_group==\"intermediate\", c(1), c(0))\n\n# harm of the conditional approach is proportion of patients who have immediate risk multiplied by the harm\n# 0.014874\nharm_conditional = mean(intermediate_risk)*harm_marker\n\n#Run the decision curve\ndca(data=data.set, outcome=\"cancer\", predictors=c(\"high_risk\", \"joint\",\n\"conditional\"), harm=c(0, harm_marker, harm_conditional),\nxstop=0.35)\n\nharm arg\n\nharm = 0 for predictor == high_risk (i.e.Â harm not applied)\nharm_marker for predictor == joint (i.e.Â constant)\nharm_conditional for predictor == conditional (i.e.Â proportion of patients who have immediate risk multiplied by the harm_marker)\n\n\nInterpretation\n\nConditional approach now consistently has the higher net benefit but at a risk threshold around 0.34 it has less benefit than Treat All\n\nFewer genetic marker tests means less harm for the Conditional approach\nHarm has a sizeable effect on the Joint approach by lowering itâ€™s net benefit a few points.\n\nJoint approach has negative benefit at about 0.31 risk threshold.\n\n\nExample Unnecessary interventions avoided\ndca(data=data.set, outcome=\"cancer\", predictors=\"marker\", probability=FALSE,\nintervention=TRUE, xstart=0.05, xstop=0.35)\n\nThis seems to be the FP calculation (See Steps: dca function)\nInterpretation: â€œAt a probability threshold of 15%, the net reduction in interventions is about 33 per 100 patients. In other words, at this probability threshold, biopsying patients on the basis of the marker is the equivalent of a strategy that reduced the biopsy rate by 33%, without missing any cancers.â€\n\nComparing Traditional Decision Analysis with Model-Based DCA (article)\n\n\nIn traditional decision analysis, all potential outcomes must be assigned a utility.\n\nTrue Negative â†’ no biopsy, no cancer: utility = 1\n\nthe outcome of no biopsy in a patient without cancer, the best possible outcome\n\nFalse Positive â†’ biopsy, no cancer: utility = 0.95\n\nUnnecessary intervention\nAssigning this 0.95 is confusing to me since usually higher utility is related to higher satisfaction. It may be that if youâ€™re given a positive test, and even with the painful biopsy, itâ€™s extremely satisfying to get a confirmation of a negative result.\n\nTrue Positive â†’ biopsy, cancer: utility = 0.80\n\nfinding a high-grade cancer\n\nFalse Negative â†’ no biopsy, cancer: utility = 0.35\n\nMissing a cancer increases the risk of toxic treatment, metastasis and death\n\n\n(traditional) net benefit = (Expected utility of the model â€“ utility of false negative) Ã· (utility of true positive â€“ utility of false negative)\nTraditional Net Benefit vs DCA Net Benefit\n\nTraditional net benefit equals the DCA net benefit at the rational risk threshold (bolded row)\n\nRational risk threshold Pt can be obtained as Pt Ã· (1 - Pt ) = (utility of true positive â€“ utility of false negative) Ã· (utility of true negative â€“ utility of false positive)\nPt Ã· (1 - Pt ) = (utility of 0.8 â€“ 0.35) Ã· (1 â€“ 0.95) = 9 and hence Pt = 10%\n\nAs shown in the chart, the â€œtreat-allâ€ model (blue) can only be used when the threshold probability is less than the prevalence (i.e.Â treat-all net benefit &gt; 0). This rule is violated when using traditional DA. Therefore, DCA is better suited for model comparison.\n\nI didnâ€™t include the table that shows this. For details, see table 1 at the 25% cut-point in the article."
  },
  {
    "objectID": "qmd/decision-intelligence.html#sec-decint-mentmod",
    "href": "qmd/decision-intelligence.html#sec-decint-mentmod",
    "title": "13Â  Decision Intelligence",
    "section": "13.10 Mental Models",
    "text": "13.10 Mental Models\n\nMisc\n\nNotes from Data Science Mental Models â€“ Optimizing your Thinking and Decision-Making\n\nTypes\n\nConcept Map (system mapping)\nThe Iceberg Model (system mapping)\nSix Thinking Hats (problem solving)\nThe Cynefin Framework (problem solving)\nFirst Principles (problem solving)\nInversion Approach (problem solving)\n\nConcept Map Allows us to visually display a system and pinpoint how the linkages between its parts\n\nSteps\n\nFormulate a problem question\n\nWhat are the exact questions that you need to answer to be able to visually represent the system in which the problem is situated? â€œHow does X work?â€, â€œWhatâ€™s the context of X in which it exists?â€ and â€œHow is X linked to Y?â€\nExample: App plays music to induce mood of user. App not performing as well in production as it did during CV How does your algorithm work exactly?\n\nWhatâ€™s the app ecosystem like in which the algorithm exists?\nHow are the algorithmâ€™s music curations linked to the userâ€™s current mood?\n\n\nIdentify key entities and sort them Create a list of the key entities that impact the problem and are linked to it. These entities might be people, algorithms, processes, places, protocols, and more. (commonly around 20 entities)\n\nSort the list by specificity and/or importance.\nHelps you to uncover the hierarchy\n\nOutline the map and fill it in Start adding entities according to your hierarchy and understanding of the problem\n\nWrite the actual action of the said connection by adding phrases to the arrows like â€œadds toâ€, â€œcreatesâ€, â€œselects fromâ€, â€œpicks according toâ€ Some of these boxes (e.g.Â algorithm) might have their specific concept maps. It all depends on the level of specificity you need/want to use.\nExample (see example description above)\n\n\n\n\nIceberg Model\n\nHelps you to notice what are the underlying causes and implications of a system or event\nComponents Events: What is happening right now? What is being asked? What do we know for sure?\n\nPatterns: Are there any trends? Has this happened before? What historical data do we have?\nStructures: What might produce the patterns? How are the parts connected? Where do the parts originate from and where do they end?\nMental Models: Are there any beliefs, assumptions, or other mental models on which the system is built? What kind of Mental Models are they and how do they behave?\n\nExample: Bank needs forecast of the amount of money taken out from their ATMs, 30 days in advance, and daily Events Follow-up questions: Why? For what use case? Why 30 days? Why daily? How often do they provision the ATM?\n\nFollow-up answers:\n\n30 days seems like a convenient number so they picked that one\n\nknowing when an ATM will run out would make them fill it sooner and/or prioritize it hence the daily request\nthey provision the ATMs every week\nSince theyâ€™re provisioned weekly, should the forecast be made weekly instead of daily\n\nDoes making more forecasts (daily) add more uncertainty to the prediction than a weekly forecast?\n\nPatterns and Structions\n\nsalary â€“ when people get paid, ATMs get used more often\nlocation â€“ ATMs at urban locations will be used more often\nweather â€“ if the weather is horrible people wonâ€™t go outside that much\ncalendar â€“ before events like Christmass people might use the ATMs more. What about other ethnicities and religions?\n\n\n\nSix Thinking Hats Approaching the problem from different standpoints should lead us to the right decision\n\nEach team member get one type of hat\n\ncreativity â€“ brainstorm ideas and let them run wild in many directions.\npositivity â€“ ponder all the benefits of an approach/decision.\nnegativity â€“ ponder all the downsides and look for weaknesses.\nemotions â€“ how do you feel about this? What does your gut say? Why?\nanalytical â€“ focus on the data and be VERY rational.\ncontrolling â€“ moderate the other hats so that you make progress. Watch out if one of them becomes too prominent and blocks the others from speaking.\n\n\nThe Cynefin Framework\n\nProblem categories\n\nClear - everything is clearly defined if straightforward cause-and-effect knowledge\n\nOften solely requires the use of best practices to solve a problem and the solutions are easy to spot.\nCourse of action: understand the problem - categorize it - respond\n\nComplicated - requires some pondering and might have multiple competing solutions\n\nCharacterized by known unknowns that often require some domain expert guidance\nCourse of action: understand the problem - analyze it - respond\n\nComplex - obscure problems that arenâ€™t clear enough at first and require investigation of the problem and its context\n\nCourse of action: investigate and bring the problem to the Complicated category\n\nChaotic - causal relationships are unclear The goal is to bring the problem down to the Complex category\nDisorder - you donâ€™t know in what category your problem is in.\n\nCourse of action: dissect the problem into multiple smaller ones and try categorizing those\n\n\n\nFirst Principles\n\nBoil down the problem to the most basic components that donâ€™t need/canâ€™t be boiled down further What is the first principle (aka basic truth) for a domain expert might not be the one for you\nProcess:\n\nBreak the problem down into basic truths\n\nSocratic Method: you start with one question and pose another one when you get the answer and you go until you reach the basic truth. 5 Whys: Ask ourselves 5 times Why is something the case\n\nUse the basic truths to find the solution\n\nExample: see Iceberg Model &gt;&gt; Example: Bank ATM forecasting\n\nFirst Principles\n\nATMs need to be refilled\nsome ATMs need to be refilled sooner than others\nATMs differ in the amount of money in them\nthe bank wants something that works\nthe problem weâ€™re facing is (?)\n\nPotential approaches based on first principles\n\nRegression\n\nTotal money left in an ATM daily\nDaily prediction of taken out money\n\nClassification\n\nDaily % chance for the ATM to empty out\n\nAggregation\n\nSummed amount of predicted ATM money take-outs\n\nOptimization\n\nThe best route to take for money transportation\n\nSurvival\n\nHow many days until an ATM is empty?\n\n\n\n\nInversion Approach\n\nView the problem from a different angle and/or consider the worst possible scenarios Start thinking of bad solutions and ask yourself why is it bad and how can it be improved upon\nExample: Imagine a project/solution failed. Ask why did it fail. Ask the team to brainstorm together why it failed, what was done/gone wrong, what mistakes we made, what we didnâ€™t consider, and more."
  },
  {
    "objectID": "qmd/diagnostics-bayes.html",
    "href": "qmd/diagnostics-bayes.html",
    "title": "14Â  Diagnostics, Bayes",
    "section": "",
    "text": "TOC\n\nMisc\nCorrelation between parameter posteriors\nConvergence\n\nMetrics\n\nRHat\nESS\n\nAutocorrelation plots for chains\n\nMixing\n\nPosteriors and Trace plots\nTrank Plots\n\nPosterior predictive check\nGOF Plots\nResiduals\nPrediction Accuracy\n\nPSIS\nWAIC\nWeights\nBayes Factor\n\n\nMisc\n\nPrior sensitivity analysis\n\n{priorsense}\n\nVideo, Thread\n\n\n\nCorrelations between parameter posteriors\n\nCorrelated parameters and their uncertainties will co-vary within the posterior distribution\n\ne.g.Â high intercepts will often mean high slopes\nCentering/standardization of predictors can remove correlation between parameters\n\nWithout independent parameters\n\nParameters canâ€™t be interpreted independently\nParameter effects on prediction arenâ€™t independent\n\nbrms::pairs(model_fit) (SR Ch 4)\nExample: SR Ch 8,9\n\npost &lt;- posterior_samples(mod_obj)\npost %&gt;%\nÂ  select(-lp__ ) %&gt;%\nÂ  ggally::ggpairs()\n\nignore first â€œb_â€; no idea why that got added\na_cid1 is the intercept for factor variable, cid = 1\nb_cid1 is the slope for the predictor variable, geological ruggedness,Â  when cid = 1\nslope and intercept conditional on cid = 1 has the highest correlation at 0.174\n\nConvergence\n\nMetrics\n\nMisc\n\nNotes from\n\nRank-normalization, folding, and localization: An improved RË† for assessing convergence of MCMC\n\nLots of detailed convergence analysis examples\n\n\nbayestestR::diagnostic_posterior has â€œESSâ€, â€œRhatâ€, â€œMCSEâ€\n\nAccepts rstanarm, brms models\n\nValues potentially indicate multimodal distribution (Vehtari, Thread)\n\nâ€œChain stacking might help, but would need to know more about the posterior to be more confident on recommendationâ€\n\n\nRhat - Gelman-Rubin convergence diagnostic\n\nestimate of the convergence of Markov chains to the target distribution\n\nChecks if the start and end of each chain explores the same region\nChecks that independent chains explore the same region\n\nCan require long chains to work well\n** This diagnostic can fail for more complex models (i.e.Â bad chains even when value = 1) **\n\nNew metric called R* might be better (docs) but thereâ€™s arenâ€™t any guidelines on the values, so probably just useful for model comparison for now.\n\nRatio of variances\n\nAs total variance among all chains shrinks to the average variance within chains, R-hat approaches 1\nIf converges, Rhat = 1+\n\nGuideline\n\nIf value is above 1.00, it usually indicates that the chain has not yet converged, and probably you shouldnâ€™t trust the samples.\n\nSolution\n\nIf you draw more iterations, it could be fine, or it could never converge.\n\n\nAutocorrelation Metrics\n\nMarkov chains are typically autocorrelated, so that sequential samples are not entirely independent.\nEffects of Autocorrelation\n\nCan be an indicator of non-convergence\nIncreases uncertainty (standard errors)\nWhen chains have high autocorrelation, they can get stuck in regions of the parameter space making the sampling inefficient.\n\nI understand this to mean that less of the parameter space gets sampled\n\n\nSolutions\n\nIf you get warnings, taking more samples usually helps\nIncreasing max tree depth helps if max tree depth is continually being reached\n\nMCMCvis::MCMCdiag(fit, round = 2) produces diagnostics and shows sampler settings your model\n\nAccepts rstan, nimble, rjags, jagsUI, R2jags, rstanarm, and brms model objects\n\n\n\nEffective Sample Size (ESS)\n\nMeasures the amount by which autocorrelation in samples increases uncertainty (standard errors) relative to an independent sample.\n\nTells you how many samples the chain would have if there was 0 autocorrelation between samples in the chain\n\nMore autocorrelation means fewer effective number of samples.\n\nGuidelines for all ESS Metrics (tail or bulk)\n\nlarger is better\nBad: ESSÂ  &lt; 400 indicates convergence problemsÂ  (Vehtari)\nOkay: ESS â‰ˆ 800 corresponds to low relative efficiency of 1% (Vehtari)\nGood: ESS &gt; 1000 is sufficient for stable estimates (BÃ¼rkner, 2017)\nVery Good: ESS â‰¥ iteration amount\n\nGreater than means that something called anti-correlation is going on which is good\n\nExample: 2000 total samples with 1000 of those used for warm-up which is brms default. 4 chains x 1000 samples = 4000 post-warm-up samples. So for each parameter, the ESS should be around that or above\n\n\nn_eff in {rethinking} precis output\n\nSame as Bulk_ESS\n\nBulk_ESS - effective sample size around the bulk of the posterior (i.e.Â around the mean or median) (same as McElreathâ€™s n_eff)\n\nâ€œassesses how well the center of the distribution is resolvedâ€\n\ni.e.Â measures how well HMC sampled the posterior around the bulk of the distribution in order to determine its shape.\n\nExample: Summary will give ESS stats\n\n\n\n\n\nas_draws_rvars(brms_fit) %&gt;%Â  Â \nÂ  Â  summarize_draws()\nvariable mean median Â  sd mad Â  q5 q95Â  rhat ess_bulk ess_tailÂ \nlp__ Â  Â  Â  -38.56Â  -38.20 1.30 1.02Â  -41.09Â  -37.21Â  Â  1Â  Â  1880Â  Â  2642Â \nalpha_c Â  Â  Â  Â  9.32 9.32 0.14 0.14 9.09 9.55Â  Â  1Â  Â  3364Â  Â  2436Â \nbeta Â  Â  Â  Â  0.02 0.02 0.01 0.01 0.01 0.03Â  Â  1Â  Â  3864Â  Â  2525Â \nsigma Â  Â  Â  Â  1.12 1.11 0.10 0.10 0.97 1.29Â  Â  1Â  Â  3014Â  Â  2776\n\nExample: Select variable and ESS Values for Quantiles\n\nas_draws_rvars(brms_fit) %&gt;%\nÂ  subset_draws(\"beta100\") %&gt;%\nÂ  summarize_draws(ess_mean, ~ess_quantile(.x, probs = c(0.05, 0.95)))\nvariableÂ  ess_meanÂ  ess_q5Â  ess_q95\nbeta100Â  Â  Â  3816Â  Â  2525Â  Â  3153\n\nThese are ESS values for\n\nThe summary estimate (aka point estimate) which is the mean of the posterior in this case\nAnd the CI values of that summary estimate\n\nSo it makes sense youâ€™d have lower numbers of effective samples in the tails of the posterior than in the bulk since itâ€™s going to get sampled less than the bulk\nTail_ESS - effective sample size in the tails of the posterior\n\nmeasures how well HMC sampled the posterior in the tails of the distribution in order to determine their shape.\n\nAutocorrelation plots for chains\n\npost &lt;- posterior_samples(mod_obj)\npost %&gt;%Â \nmcmc_acf(pars = vars(b_a_cid1:sigma),\nÂ  Â  Â  lags = 5) +\ntheme_pomological_fancy(base_family = \"Marck Script\")\n\nExample from Ch 8,9 Statistical Rethinking\nL-shaped autocorrelation plots like these are good.\n\nThose are the kinds of shapes youâ€™d expect when you have reasonably large effective samples.\n\n\nMixing\n\nPosteriors and Trace Plots\n\nbrms::plot(mod_obj)\n\nExample from Ch 8,9 Statistical Rethinking\n\nSee above for parameter descriptions\nTrace plots with fat, lazy caterpillars like these are good\n\nTrank Plot (Trace Rank Plot)\n\npost &lt;- posterior_samples(b9.1b, add_chain = T)\npost %&gt;%Â \nÂ  bayesplot::mcmc_rank_overlay(pars = vars(b_a_cid1:sigma)) +\nÂ  Â  Â  scale_color_pomological() +\nÂ  Â  Â  ggtitle(\"My custom trank plots\") +\nÂ  Â  Â  coord_cartesian(ylim = c(25, NA)) +\nÂ  Â  Â  theme_pomological_fancy(base_family = \"Marck Script\") +\nÂ  Â  Â  theme(legend.position = c(.95, .2))\nPosterior Predictive Check\n\nExample\n\nBad Fit\n\n\nbrms::pp_check(model, nsamples = 100) + xlim(0, 20)\n\nGood Fit\nWith bayesplot\n\nbayesplot::ppc_rootogram(y = testdata$observedResponse,\nÂ  Â  Â  Â  Â  Â  Â  yrep = posterior_predict(model4, nsamples = 1000)) +\nÂ  xlim(0, 20)\nGOF Plots\n\nMisc\n\nAlso see Statistical Rethinking &gt;&gt; Ch. 5 &gt;&gt; inferential plots\n\nPredictor Residual, Counterfactual Plots, and Posterior Predictive Plots\n\n\nData with regression line\n\n# scatter plot of observed pts with the regression line from the model\n# Defined the by the alpha and beta estimate\ndat %&gt;%\nÂ  ggplot(aes(x = weight, y = height)) +\nÂ  geom_abline(intercept = fixef(b4.3)[1],\nÂ  Â  Â  Â  Â  Â  Â  slopeÂ  Â  = fixef(b4.3)[2]) +\nÂ  geom_point(shape = 1, size = 2, color = \"royalblue\") +\nÂ  theme_bw() +\nÂ  theme(panel.grid = element_blank())\n\nPredicted vs Observed with PI and CI around regression line\n\n# x value (weight) range we want for the CI of the line\nweight_seq &lt;- tibble(weight = seq(from = 25, to = 70, by = 1))\n\n# predicted values (height) for each x value\n# 95% CIs generated by default\nmu_summary &lt;-Â  fitted(b4.3, newdata = weight_seq) %&gt;%\nÂ  Â  as_tibble() %&gt;%\nÂ  Â  # let's tack on the `weight` values from `weight_seq`\nÂ  Â  bind_cols(weight_seq)\n\n# 95% PIs generated by default\npred_height &lt;-Â  predict(b4.3,\nÂ  Â  Â  newdata = weight_seq) %&gt;%\nÂ  Â  as_tibble() %&gt;%\nÂ  Â  bind_cols(weight_seq)\n\n# includes regression line, CI, and PI\ndat %&gt;%Â  ggplot(aes(x = weight)) +\nÂ  # PIs\nÂ  geom_ribbon(data = pred_height, aes(ymin = Q2.5,\nÂ  Â  Â  Â  Â  Â  Â  ymax = Q97.5), fill = \"grey83\") +\nÂ  # CIs\nÂ  geom_smooth(data = mu_summary, aes(y = Estimate, ymin = Q2.5, ymax = Q97.5),\nÂ  Â  Â  Â  Â  Â  Â  stat = \"identity\", fill = \"grey70\", color = \"black\",\nÂ  Â  Â  Â  Â  Â  Â  alpha = 1, size = 1/2) +\nÂ  geom_point(aes(y = height), color = \"navyblue\", shape = 1,\nÂ  Â  Â  Â  Â  Â  size = 1.5, alpha = 2/3) +\nÂ  coord_cartesian(xlim = range(d2$weight),\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  ylim = range(d2$height)) +\nÂ  theme(text = element_text(family = \"Times\"),\nÂ  Â  Â  Â  panel.grid = element_blank())\nResiduals\n\nDHARMa package\n\nvignettes:\n\nhttps://cran.r-project.org/web/packages/DHARMa/vignettes/DHARMaForBayesians.html\nhttps://cran.r-project.org/web/packages/DHARMa/vignettes/DHARMa.html\n\nExample\n\nBad Fit\n\n\n\npacman::p_load(brms, DHARMa)\n\nmodel.check &lt;- createDHARMa(\nÂ  simulatedResponse = t(posterior_predict(model)),\nÂ  observedResponse = testdata$observedResponse,\nÂ  fittedPredictedResponse = apply(t(posterior_epred(model)), 1, mean),\nÂ  integerResponse = TRUE)\nplot(model.check)\n\nHierarchical dataset fit with a poisson model. Everything is bad in this fit.\nGood Fit\n\nHierarchical dataset fit with a hierarchical negative binomial model.\n\nExample (group variation check)\n\nÂ  Â  plot(model.check, form = testdata$group)\n\nThis is hierarchical dataset fit with a hierarchical poisson model. The within-group box-plots show that model has captured the group variance sufficiently as both tests have non-significant (n.s.) results.\nExample (overdispersion visualization)\n\nBad fit\n\n\nÂ  Â  testDispersion(model.check)\n\nThis is hierarchical dataset fit with a hierarchical poisson model. In the previous chart (left panel), the overdispersion test failed. This histogram shows how much the model is off.\nGood Fit\n\nPredictive Accuracy\n\nMisc\n\nSee Statistical Rethinking &gt;&gt; Ch. 7 for details\nloo package website has some nice CV workflows\n\nAlso ensembling, time series\n\nDonâ€™t compare models with different numbers of observations (SR, Ch 11)\n\ne.g.Â 1/0 logistic regression model vs aggregated logistic regression model\n\nPSIS-LOO (IS-LOO, WAIC, etc) has difficulties if each observation has their own parameter(s) (aka â€œrandom effectsâ€) (Vehtari thread + post)\n\nModel Comparison\n\nMcElreath: To judge whether two models are â€œeasy to distinguishâ€ (i.e.Â kinda like whether their scores are statistically different), we look at the differences between the model with the best WAIC and the WAICs of the other models along with the standard error of the difference of the WAIC scores\n\nbrms::loo_compare(loo_obj, loo_obj)\n\nWhere a â€œloo_objâ€ is a brms::loo(fit) or brms::waic(fit) object\nCan also take a list of loo objects\nsimplify = FALSE gives a more detailed summary\n\nIf the difference in ELPD is much larger or several times the estimated standard error of the difference, then the top model is expected to have better predictive performance\n\n\nPareto-Smoothed Importance Sampling Cross-Validation (PSIS)\n\nWeights observations based on influence on the posterior\nUses highly influential observations to formulate a pareto distribution and sample from it\nbrms::loo - wrappers for loo::loo (docs)\nEstimates out-of-sample LOO-CV lppd\n\nloo pkg\n\nâ€œelpd_looâ€ - larger is better\nâ€œlooicâ€ - is just (-2 * elpd_loo) to convert it to the deviance scale, therefore smaller is better\nMay need to use add_criterion(brms_fit, \"loo\") in order to use the loo function\n\nRethinking pkg: smaller is better\n\nThe shape parameter of the distribution, k, is estimated. When k &gt; 0.5, then the distribution has infinite variance. PSIS weights perform well as long as k &lt; 0.7. Large k values can be used to identify influential observations (i.e.Â rare observations/potential outliers).\n\nFor brms, warnings for high k values will show when using add_criterion(brms_mod, \"loo\")Â \nOutliers make it tough to estimate out-of-sample accuracy, since rare values are unlikely to be in the new sample. (i.e.Â overfitting risk)\nAlso, warnings about high k values can occur when the sample size is small\n\nWhen looking at the posterior, keep in mind that â€œinfluentialâ€ data values might be significantly affecting the posterior distribution.\n\nSolutions\n\nIf there are only a few outliers, and you are sure to report results both with and without them, dropping outliers might be okay.\nIf there are several outliers, then a form of Robust Regression can be used or a Mixture Model.\n\nCommon to use a Studentâ€™s T distribution instead of a Gaussian for the outcome variable specification\n\nThe Student-t distribution arises from a mixture of Gaussian distributions with different variances. If the variances are diverse, then the tails can be quite thick.\nHas an extra shape parameter, Î½, that controls how thick the tails are.\n\nÎ½ = âˆ is a Gaussian distribution\nAs v â€“&gt; 1+ , tails start becoming fat\nÎ½ can be estimated with very large datasets that have plenty of rare events\n\n\n\n\nExample\n\n\n\n# shows k values for all data points below 0.5 threshold\nloo::loo(b8.3) %&gt;%Â \nÂ  plot()\n# K values\ntibble(k = b8.3$criteria$loo$diagnostics$pareto_k,Â \nÂ  Â  Â  row = 1:170) %&gt;%Â \nÂ  arrange(desc(k))\n# k value diagnostic table - shows how many are points have bad k values and that group's min n_eff\nloo(b8.3) %&gt;% loo::pareto_k_table()\n\nWidely Applicable Information Criterion (WAIC)\n\nDeviance with a penalty term based on the variance of the outcome variableâ€™s observation-level log-probabilities from the posterior\nEstimates out-of-sample deviance\n\nloo pkg: brms::waic\n\nâ€œelpd_waicâ€: larger is better\nâ€œwaicâ€:Â  is just (-2 * elpd_waic) to convert it to deviance scale, therefore smaller is better\nMay need to use add_criterion(brms_fit, \"waic\") in order to use the waic function\n\nRethinking pkg: smaller is better\n\nEffective number of parameters, pwaic (aka the penalty term or overfitting penalty)\n\nSays compute the variance in log-probabilities for each observation i, and then sum up these variances to get the total penalty.\nCalled such because in ordinary linear regressions the sum of all penalty terms from all points tends to be equal to the number of free parameters in the model\n\nWhen the sum is larger than the number of free parameters, it can indicate an outlier is present which will increase the overfitting risk.\n\nSee Solutions for outliers under PSIS &gt;&gt;Â  Shape parameter, k\n\n\nWeights\n\nThese weights can be a quick way to see how big the differences are among models.\nEach model weight is essentially a proportion of itâ€™s WAIC or PSIS difference compared to the total of all the WAIC or PSIS differences.\n\nLarger is better\n\nExample\n\n\nbrms::model_weights(b8.1b, b8.2, b8.3, weights = \"loo\") %&gt;%\nÂ  Â  Â  Â  round(digits = 2)\n## b8.1bÂ  b8.2Â  b8.3Â \n##Â  0.00Â  0.03Â  0.97\n\nInterpretation:\n\nb8.3 has more than 95% of the model weight. Thatâ€™s very strong support for including the interaction effect, if prediction is our goal.\nThe modicum of weight given to b8.2 suggests that the posterior means for the slopes in b8.3 are a little overfit.\n\nBayes Factor\n\nThe ratio (or difference when logged) of the average likelihoods (the denominator of bayes theorem) of two models.\nSince the average likelihood has been averaged over the priors, it has a natural penalty for more complex models\nProblems\n\nEven when priors are weak and have little influence on posterior distributions within models, priors can have a huge impact on comparisons between models.\nNot always possible to compute the average likelihood"
  },
  {
    "objectID": "qmd/diagnostics-classification.html#sec-diag-class-misc",
    "href": "qmd/diagnostics-classification.html#sec-diag-class-misc",
    "title": "Classification",
    "section": "Misc",
    "text": "Misc\n\nAlso see Diagnostics, Regression &gt;&gt; Residuals\nTwo main types of quantities to validate (Harrell RMS Ch.5)\n\nCalibration (aka reliability): ability to make unbiased estimates of response (Y^ vs.Â Y)\nDiscrimination: ability to separate responses\n\nBinary logistic model: e.g.Â AUROC\n\n(more optional) Centrality of errors: e.g.Â Brier Score"
  },
  {
    "objectID": "qmd/diagnostics-classification.html#sec-diag-class-terms",
    "href": "qmd/diagnostics-classification.html#sec-diag-class-terms",
    "title": "Classification",
    "section": "Terms",
    "text": "Terms\n\nCalibration - When the predicted probabilities from a model match the observed distribution of probabilities for each class. A well-calibrated model is one that minimizes residuals.\nCalibration-in-the-Large - mean calibration\n\nHarrell: â€œcompares the average Y-hat with the average Y (does the model get the right result on average).â€\n\naverage predicted risk compared with the overall event rate\n\nConcerned with gross measurements of calibration, such as:\n\nWhether the modelâ€™s overall expected number of events exceeds the observed number\nWhether the proportion of expected over observed events departs significantly from â€œ1â€\n\n\nCalibration-in-the-Small - means that each of your modelâ€™s predicted probabilities, i.e.Â 0.01, 0.02, â€¦, 0.99, occur exactly at that proportion in the observed data/real world\n\ni.e., for all the times when the predicted risk was 0.4, the outcome happened about 0.4 of the time.\nMore important than calibration-in-the-large. Required for good decision making using risk models.\nHarrell: Assesses the absolute forecast accuracy at the individual levels of Y-hat.\nSmooth calibration curves visualizes this (See Calibration &gt;&gt; Calibration Plots &gt;&gt; Harrell)\n\nA curve close to the diagonal indicates that predicted risks correspond well to observed proportions\n\nâ€œcalibration-in-the-tinyâ€ the next step: for males in which the prediction was 0.4 was the outcome present 0.4 of the time, then for females.\n\nDiscrimination - The ability of the predictive model to predict the observed class. Itâ€™s not a particular performance measure. Typically AUROC, Somersâ€™ Dxy\n\nPredictive discrimination is the ability to rank order subjects\nExample: The higher the ratio of patients who have high predicted risk probabilities / patients who actually have the disease, the better the discrimination by the model\n\nâ€œHighâ€ is determined by a cutpoint or threshold\n\n\nExternal Validity - The extent to which your results can be generalized to other contexts. Assesses the applicability or generalizability of the findings to the real world. So, your study had significant findings in a controlled environment. But will you get the same results outside of the lab?\nInternal Validity - The degree of confidence that the causal relationship you are testing is not influenced by other factors or variables. Evaluates a studyâ€™s experimental design and methods. Studies that have a high degree of internal validity provide strong evidence of causality.\n\nUsing CV or bootstrapping is sometimes referred to as performing internal validation\n\nProper Scoring Rule (SO discussion + Harrell links) - A score assesses whether a probabilistic forecast is close to the true distribution. A score that is minimized in expectation if the predictive density is the true density.\n\nLoss functions that map predicted probabilities and corresponding observed outcomes to loss values, which are minimized in expectation by the true probabilities (p,1âˆ’p). The idea is that we take the average over the scoring rule evaluated on multiple (best: many) observed outcomes and the corresponding predicted class membership probabilities, as an estimate of the expectation of the scoring rule\nAlso\n\nStrictly proper scoring rules -rules that are only minimized in expectation if the predictive density is the true density\nImproper scoring rules - (e.g.Â Accuracy)"
  },
  {
    "objectID": "qmd/diagnostics-classification.html#confusion-matrix-statistics",
    "href": "qmd/diagnostics-classification.html#confusion-matrix-statistics",
    "title": "Classification",
    "section": "Confusion Matrix Statistics",
    "text": "Confusion Matrix Statistics\n\nFalse Discovery Rate (FDR)\n\\[\nFDR = \\frac{FP}{TP + FP} = 1 âˆ’ PPV\n\\]\n\nExpected ratio of the number of false positive classifications (false discoveries) to the total number of positive classifications (rejections of the null)\nFDR-controlling procedures provide less stringent control of Type I errors compared to family-wise error rate (FWER) controlling procedures (such as the Bonferroni correction), which control the probability of at least one Type I error.\n\nThus, FDR-controlling procedures have greater power, at the cost of increased numbers of Type I errors.\n\n\nFalse Positive Rate (FPR)\n\\[\nFPR = 1 - TNR = \\frac{FP}{FP + TN}\n\\]\nFalse Negative Rate (FNR)\n\\[\nFNR = 1 - TPR = \\frac{FN}{FN + TP}\n\\]\nNegative Likelihood Ratio (LR-)\n\\[\nLR_- = \\frac{FNR}{TNR}\n\\]\nNegative Predictive Value (NPV)\n\\[\nNPV = \\frac{TN}{TN + FN}\n\\]\n\n\\(PPV\\) and \\(NPV\\) are useful after the result of the test in know (unlike Sensitivity and Specificity which are useful before the results are known)\nBoth \\(PPV\\) and \\(NPV\\) are useful GoF metrics since they describe performance on future data.\n{yardstick}\n\nPositive Likelihood Ratio (LR+)\n\\[\nLR_+ = \\frac{TPR}{FPR}\n\\]\nPrecision or True Positive Rate (TPR) or Positive Predictive Value (PPV)\n\\[\n\\mbox{Precision} = \\frac{TP}{TP + FP}\n\\]\n\nThe fraction of observations that the model identified as positive that were correct (i.e.Â the fraction of all the actual positives (\\(TP+FN\\)) that you model labels as positives.)\nFocuses on Type-I error (\\(FP\\))\nInterpretation: 1 is perfect, &lt; 0.5 is low\nMaximize when \\(FPs\\) are more costly\n\nExamples\n\nAn event is mistakenly detected that results in a high value customer getting their account suspended.\nYou want to reduce the number of unsuccessful marketing calls that the model predicts will be conversions.\n\n\n\nPrevalence\n\\[\n\\mbox{Prevalence} = \\frac{P}{P+N}\n\\]\n\nProportion of a particular population found to be affected by a medical condition (typically a disease or a risk factor such as smoking or seatbelt use) at a specific time\nPrevalence Threshold: The prevalence level below which a testâ€™s positive predictive value (PPV) declines most sharply relative to disease prevalence - and thus the rate of false positive results/false discovery rate increases most rapidly.\n\nRecall or Sensitivity\n\\[\n\\mbox{Recall} = \\frac{TP}{TP + FN}\n\\]\n\nThe fraction of all true positives that the model got correct\nFocuses on Type-II error (\\(FN\\))\nProbability of detecting a condition when itâ€™s truly present\n\ni.e.Â normally want high sensitivity and low specificity\n\nAs the number of \\(FNs\\) towards 0, the value of Recall will tend to \\(TP/TP = 1\\)\nInterpretation: 1 is perfect, &lt; 0.5 is low\nMaximize when \\(FNs\\) are more costly\n\nExamples\n\nA customer gets their card stolen and incurs fraudulent charges, but the model doesnâ€™t detect it.\n\n\n\nSpecificity or True Negative Rate (TNR)\n\\[\n\\mbox{Specificity} = \\frac{TN}{TN + FP}\n\\]\n\nThe fraction of all the true negatives the model got correct\nProbability of not detecting a condition when itâ€™s truly present\n\ni.e.Â normally want high sensitivity and low specificity\n\n\nType I Error: False Positives - If \\(FPs\\) are more costly than \\(FNs\\), then the threshold is raised to increase the accuracy of the model in predicting positive cases\nType II Error: False Negatives - If \\(FNs\\) are more costly than \\(FPs\\), then the threshold is lowered to increase the accuracy of the model in predicting negative cases"
  },
  {
    "objectID": "qmd/diagnostics-classification.html#sec-diag-class-scores",
    "href": "qmd/diagnostics-classification.html#sec-diag-class-scores",
    "title": "Classification",
    "section": "Scores",
    "text": "Scores\n\nMisc\n\nTypes\n\nThreshold (e.g.Â F-Scores, Accuracy, Precision, Recall)\n\nDetermining thresholds depends on being able to correctly quantify the costs of type 1 and 2 errors\n\nRanking (ROC, PR) Costs not included An expected utility canâ€™t be calculated\n\nNot intuitive to a non-technical audience\n\n\nComparison of similar scoring models\n\nNotes from https://evidentlyai.com/blog/tutorial-2-model-evaluation-hr-attrition\nIf thereâ€™s no clear winner in terms of scores, then you need to drill down.\n\nIf models differ significantly on one or couple scores but not the others. Determine what that score optimizes for (e.g.Â precision, recall, etc.) and if that fits your use case.\nHow does changing the classification threshold effect the scores?\n\nMakes sure whichever measure (e.g TP, FP, etc.) youâ€™re maximizing for has plenty of observations, so you know youâ€™re seeing a real effect.\n\nExamine how the models predict different groups (aka cohorts).\n\nExample: If itâ€™s a employee churn model, how does the model perform on employees from different departments, skill level, roles, etc.\nOne cohort might be more important to keep from churning than another.\n\n\n\nOut-of-Bag Error (OOB) - Mean prediction error on each training sample (row) using only trees that didnâ€™t use that sample in their bootstrap resample\n\nUseful when there isnâ€™t enough data for a validation set.\n\nCustom Cost Functions\n\nAlso see\n\nLogistics &gt;&gt; Decision Impact Metrics\n\nExamples\n\nCalculating the Business Value of a Data Science Project\nSee notebook\nVisualizing Machine Learning Thresholds to Make Better Business Decisions\n\n\nUses a Telecom subscriptions churn example and incorporates available resources (queue rate) that can review flagged events in order to choose a threshold\n\ni.e.Â if you can only review 50 cases, then you model need only flag 50 cases\n\nAdds uncertainty by using multiple train/test splits and creating quantiles for CIs\nOptimizes resources, costs, precision, and recall to produce a threshold\n\nExample of model ROI calculation: see Banking/Credit &gt;&gt; Fraud &gt;&gt; Misc\n\n\n\nBalanced Accuracy\n\\[\nBA = \\frac{cTP}{P} + \\frac{(c-1)TN}{N}\n\\]\n\n\\(c\\) is a cost weight between 0 and 1\n\\(P\\) and \\(N\\) are observed Positives and Negatives\nCan also be used when the negative class is more important to get right\n\ne.g.Â Cost Reductions: Reduce unnecessary treatments of healthy patients that got an FP on their disease test\n\n{yardstick}\n\nBrier Score\n\nHow far your predictions lie from the true values\n\nA mean square error in the probability space\n\nSmaller Brier scores are better\nRandom guess model (i.e.Â all predicted probabilities = 0.50) has a Brier score of 0.25\n\nWhen the outcome incidence is lower (i.e.Â event rate, proportion of 1s), the maximum score for a random guess model is lower, eg, for 10%: 0.1 x (1 - 0.1)2 + (1 - 0.1) x 0.12 = 0.090.\n\nWhere the formula is:\n\\[\nY(1- p)^2 + (1-Y)p^2\n\\]\n\n\nIssue: Inadequate for very rare (or very frequent) events, because it does not sufficiently discriminate between small changes in forecast that are significant for rare events.\n\nn &gt; 1000 required for higher-skill forecasts of relatively rare events, whereas only quite modest sample sizes are needed for low-skill forecasts of common events (wiki)\n\nBootstrapping code (article)\nBinary Outcomes\n\\[\nBS = \\frac{1}{N}\\sum_{t=1}^N (f_t - o_t)^2\n\\]\n\n\\(f\\) is the predicted probability\n\\(o\\) is the observed outcome\nRange is from 0 to 1\nMean of brier loss\n\nPolytomous Outcomes\n\\[\nBS = \\frac{1}{N}\\sum_{t=1}^N\\sum_{i=1}^R (f_{ti} - o_{ti})^2\n\\]\n\nR is the number of categories\nRange is from 0 to 2\n\nBrier Skill Score (BSS)\n\\[\nBSS = 1 - \\frac{BS}{BS_{\\text{ref}}}\n\\]\n\nPercentage improvement in the BS compared to the reference model\nStill a strictly proper scoring rule\nValues between 0 and 1\nHigher is better\n\\(BS\\) is the brier score\n\\(BS_{\\text{ref}}\\) is a reference/baseline brier score that your trying to beat\n\nDefault value (no-skill/naive model value)\n\\[\nBS_{\\text{ref}} = \\frac{1}{N} \\sum_{t=1}^N (\\bar o - o_t)^2\n\\]\n\n**for binary outcomes\n\\(o\\) is the observed outcome\n\\(\\bar o\\) is the average observed outcome (i.e.Â overall proportion of 1s)\n\n\n\nScaled Brier Score\n\\[\n\\begin{align}\n&SBS = 1 - \\frac{BS}{BS_{\\text{max}}} \\\\\n&\\text{where} \\;\\; B_{\\text{max}} = \\bar p (1 - \\bar p)\n\\end{align}\n\\]\n\narticle\nCode\nscaled_brier &lt;- function(x, p, ...){Â \nÂ  Â  format(round(1 - (x / (mean(p) * (1 - mean(p)))), digits = 2), nsmall = 2)\n}\nRange between 0% and 100%\n\n\nCohenâ€™s Kappa\n\nA similar measure to accuracy, but is normalized by the accuracy that would be expected by chance alone and is very useful when one or more classes have large frequency distributions.\nExtends naturally to multiclass scenarios\nyardstick::kap, docs\nA measure of agreement between categorical variables X and Y\n\nCalculated from the observed and expected frequencies on the diagonal of a square contingency table\nRange: 0 &lt; \\(\\kappa\\) &lt; 1, although negative values do occur on occasion.\nSuited for nominal (non-ordinal) categories.\n\nWeighted kappa can be calculated for tables with ordinal categories.\n\nSee Penn Stats Stats 509 for further details, equation\n\n\nF scores\n\nEach score weights \\(FP\\) and \\(FN\\) differently. A trade-off between Precision and Recall\nRange: 0 to 1\n\n1 - Perfect precision and recall\n0 - Either precision or recall are zero.\n\ni.e.Â A low F-score doesnâ€™t tell you whether itâ€™s precision or recall that is the problem\nMaximizing FPR along with F1 will help curb Type-I errors and youâ€™ll get an idea about the villain behind your low F1-score\n\nRule of Thumb\n\n&gt; 0.9 Very good\n0.8 - 0.9 Good\n0.5 - 0.8 OK\n&lt; 0.5 Not good\n\n\nFine for imbalanced classes as long as the positive class (TP and FP) is whatâ€™s important and not the negative class (e.g.Â FN and TN) (article)\n\\(F_\\beta\\)\n\\[\nF_\\beta = \\frac{1+\\beta^2}{\\frac{1}{\\mbox{Precision}}+\\frac{\\beta^2}{\\mbox{Recall}} + }\n\\]\n\nA generalized F score\nChoose your own ratio\n\nThe more you care about recall over precision the higher beta you should choose\ne.g.Â If you want precision to be twice as important than recall, youâ€™d set \\(\\beta\\) to 0.5\n\nUse when you care more about the positive class\n\n\\(F_{1}\\)\n\\[\n\\begin{align}\nF_1 &= \\frac{2}{\\frac{1}{\\mbox{Precision}} + \\frac{1}{\\mbox{Recall}}} \\\\\n      &= \\frac{TP}{TP + 0.5(FP + FN)}\n\\end{align}\n\\]\n\nHarmonic mean of precision and recall\nRecall and precision are of the same importance\n\n\\(F_2\\)\n\\[\n\\begin{align}\nF_2 &= \\frac{5}{\\frac{4}{\\mbox{Precision}} + \\frac{1}{\\mbox{Recall}}} \\\\\n      &= \\frac{TP}{TP + 0.8FP + 0.2FN}\n\\end{align}\n\\]\n\n\\(F_2\\): 80/20\n\nÎ² = 2. Therefore you care about Recall twice as much as Precision\n\n\n\\(F_3\\)\n\\[\nF_3 = \\frac{TP}{TP + 0.9FP + 0.1FN}\n\\]\n\n\\(F_3\\): 90/10\n\nÎ² = 3. Therefore you care about Recall 3 times as much as Precision\n\n\n\nH measure\n\n{hmeasure}\n\nSee bkmks for vignette\n\nInstead of using misclassification cost (c, see above) as a constant it defines it as a distribution\n\nJ index (a.k.a. Youdenâ€™s J statistic)\n\\[\nJ = sensitivity + specificity - 1\n\\]\n\nValues near one are best. lower for models with pathological distributions for the class probabilities (i.e.Â uncalibrated)\n\nLift Score\n\\[\n\\mbox{Lift} = \\frac{\\frac{TP}{TP+FP}}{\\frac{TP+FN}{TP+TN+FP+FN}}\n\\]\n\nThe ratio of correctly predicted positive examples and the actual positive examples in the test dataset.\nRange: 0 to \\(\\infty\\)\nValues greater than 1 represent a valuable model\nExample: Using lift score within {sklearn::GridSearchCV}\nfrom mlxtend.evaluate import lift_score\nscorer = {\nÂ  Â  'lift_score': make_scorer(lift_score)\n}\nclf = GridSearchCV(SVC(), hyperparameters, cv=10,\n                   scoring=lift_scorer)\n\nFrom Raschka library\n\nAlso has examples on how to implement\nmake_scorer is from {{sklearn}}\n\n\n{yardstick::lift_curve}\n{ROCR}\n\nMatthewâ€™s Correlation Coefficient (MCC)\n\\[\nMCC = \\frac{(TP \\cdot TN)-(FP \\cdot FN)}{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}\n\\]\n\nItâ€™s a correlation between predicted classes and ground truth.\n\nAka Phi Coefficient (i.e.Â Pearson Correlation for 2x2 tables, i.e.Â binary variables)\n\n{yardstick}\nRange: -1 to 1\n\n0 is equivalent to a random prediction\n-1 predictions are perfectly negatively correlated with the truth\n1 predictions are perfectly postiviely correlated with the truth (model is perfect)\n\nUses all confusion matrix values\nRobust to class imbalance\n\nMcNemarâ€™s Test\n\nChi-Squared test for symmetry\nSee Post-Hoc Analysis, General &gt;&gt; Dichotomous Data &gt;&gt; for details and code\nUses this matrix format\n\nmatrix(c(9945, 25, 15, 15),\nÂ  Â  Â  Â  nrow = 2,\nÂ  Â  Â  Â  dimnames = list(\"model 1\" = c(\"Correct\", \"Wrong\"),\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  \"model 2\" = c(\"Correct\", \"Wrong\")))\n\nIf â€œwrong-correctâ€ or â€œcorrect-wrongâ€ have counts &lt; 50, then use the Exact Tests to get accurate p-values\n\nFor more than 2 models, Cochranâ€™s Q test can be used (Generalized McNemarâ€™s Test)\n\nMean Log Loss (aka Cross-Entropy)\n\\[\n\\mbox{MeanLogLoss} = -\\frac{1}{N} \\sum_{i=1}^N y_i \\cdot \\log (\\hat{p}_i) + (1-y_i) \\cdot \\log(1 - \\hat{p}_i))\n\\]\n\n\\(\\hat p\\) is the predicted probability and \\(y\\) is the numeric observed label (0/1)\nThe difference between ground truth and predicted score for every observation and average those errors over all observations\nLower is better\n\nA perfect model has a mean log loss of 0.\n\n{yardstick::mn_log_loss}\nAlso see Loss Functions for issues with this metric\n\nMisclassification Cost\n\\[\n\\mbox{Misclassification Cost} = c_1FP + c_2FN\n\\]\n\nWhere \\(c_1\\) and \\(c_2\\) are costs per \\(FP\\) and per \\(FN\\) respectively; \\(FP\\) is total False Positives and \\(FN\\) is total False Negatives\nSimple method to assign a business value to a classification model\nSee {yardstick::classification_cost}\n\nMisclassification Rate\n\\[\n\\begin{align}\n&\\mbox{Misclassification Rate} = 1 - Accuracy \\\\\n&\\text{where} \\;\\; \\mbox{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n\\end{align}\n\\]\n\nUses Accuracy so terrible for unbalanced classes.\n\nNo-information rate\nno_information_rate &lt;- function(data) {\nÂ  data %&gt;%\nÂ  Â  count(outcome_var) %&gt;%\nÂ  Â  mutate(proportion = n / sum(n)) %&gt;%\nÂ  Â  top_n(1, proportion) %&gt;%\nÂ  Â  pull(proportion)\n}\n\nThe proportion of the most common class\nUsed as a baseline metric for comparing models in multinomial classification\n\nPR-AUC\n\nThe area under the Precision-Recall curve\nValues near one indicate very good results while values near 0.5 would imply that the model is very poor\nBetter for imbalanced classes\nPR AUC focuses mainly on the positive class (PPV and TPR) it cares less about the frequency of the negative class\n\nUse when you care more about positive than negative class\n\n\nPseudo R2\nlibrary(DescTools)\nPseudoR2(model, which = c(\"CoxSnell\", \"Nagelkerke\", \"Tjur\"))\n##Â  CoxSnell NagelkerkeÂ  Â  Â  TjurÂ \n##Â  0.6583888Â  0.9198193Â  0.8795290\n\nExample: How much of the likelihood of promotion does my model explain?\n\nOur model explains more than two thirds of the variation in the promotion outcome.\n\n\nROC-AUC\n\naka AUROC, concordance statistic, or c-statistic\nThe area under the ROC is an overall assessment of performance across all cutoffs.\n\nWhere ROC is Sensitivity vs (1 - Specificity) (aka TPR vs FPR) chart\n\nValues near one indicate very good results while values near 0.5 would imply that the model is very poor.\nUse it when you care equally about positive and negative classes\nDo NOT use it if you have imbalanced classes\n\nSomersâ€™ \\(D_{xy}\\)\n\\[\nD_{xy} = 2(c - 0.5)\n\\]\n\nWhere c is the concordance probability or AUROC\nA pure discrimination measure which is a rank correlation between \\(Y\\) and \\(\\hat Y\\)"
  },
  {
    "objectID": "qmd/diagnostics-classification.html#sec-diag-class-climb",
    "href": "qmd/diagnostics-classification.html#sec-diag-class-climb",
    "title": "Classification",
    "section": "Class Imbalance",
    "text": "Class Imbalance\n\nMisc\n\nClass imbalance sampling methods tend to greatly improve metrics based on the hard class predictions (i.e., the categorical outcome variables and not binned numeric variables) because the default cutoff tends to be a better balance of sensitivity and specificity.\n\ne.g J index vs AUC\n\nROC curves and PR curves have inflated performance under strong class imbalance\n\nScores: F-Scores, MCC, PR-AUC, Cohenâ€™s Kappa, precision at fixed recall, recall at fixed precision\nPrecision-Recall\n\nIn cases of balanced classes, Sensitivity and Specificity are often used\nFor less-than-perfect models (FP = FN = 0), if you increase Precision, you reduce Recall and vice versa.\nSensitive to class-imbalance\n\nYou canâ€™t compare curves that have been tested on two different test sets if the class proportions differ\nA test-set with a 50:50 proportion will have higher scores in general as compared to a test-set with a 10:90 proportion for the primary class."
  },
  {
    "objectID": "qmd/diagnostics-classification.html#sec-diag-class-multinom",
    "href": "qmd/diagnostics-classification.html#sec-diag-class-multinom",
    "title": "Classification",
    "section": "Multinomial",
    "text": "Multinomial\n\nMisc\n\nPackages\n\n{MultiClassROC} - computing and visualizing Receiver Operating Characteristics (ROC) and Area Under the Curve (AUC) for multi-class classification problems\n\nAlso see\n\nScores\n\n&gt;&gt; No-information Rate for a generic accuracy baseline\n&gt;&gt; Brier-Score &gt;&gt; Polytomous Outcomes\n&gt;&gt; Cohenâ€™s Kappa\n&gt;&gt; Mean Log Loss\n\n\nsklearn::classification_report calculates the maps and waps for precision, recall, and f1-score\n\nâ€œSupportâ€ in this report refers to label and total sample sizes.\nSee Model building, sklearn &gt;&gt; Misc &gt;&gt; Score Model for code\n\n\nConfusion Matrix\n\n\nClass labels are numbers 0-9\nThe decimal places are confusing but these are counts of the predicted label matching each observed label\n\nPrecision for Label 9 = \\(TP / (TP + FP) = 947/ (947 + 1 + 0 + 0 + 0 + 0 + 38 + 0 + 40 + 2) = 0.92\\)\n\nColumns are used for precision calculation\n\nRecall for label 9 = \\(TP / (TP + FN) = 947 / (947 + 0 + 0 + 0 + 0 + 0 +14 + 0 + 36 + 3) = 0.947\\)\n\nRows are used for recall calculation\n\nMacro Average Precision - simple arithmetic average of the precision of all the labels\n\n\\(map = (0.80 + 0.95 + 0.77 + 0.88 + 0.75 + 0.95 + 0.68 + 0.90 + 0.93 + 0.92) / 10 = 0.853\\)\n\nWeighted Average Precision - the precision of each label is multiplied by their sample size and the weighted sum is divided by the total number of samples\n\n\\(wap = (760*0.80 + 900*0.95 +535*0.77 + 843*0.88 + 801*0.75 + 779*0.95 + 640*0.68 + 791*0.90 + 921*0.93 + 576*0.92) / 7546 = 0.86\\)\n\nWhere 760 = sample size for label 0, 900 = sample size for label 1, etc., and 7546 is the total sample size\n\n\nSame techniques can be used for F-Scores and others.\n\nExample: The F-Score would be calculated for each label. Then, the map and wap can be calculated from those individual label scores."
  },
  {
    "objectID": "qmd/diagnostics-classification.html#sec-diag-class-curv",
    "href": "qmd/diagnostics-classification.html#sec-diag-class-curv",
    "title": "Classification",
    "section": "Curves",
    "text": "Curves\n\nMisc\n\n{rtichoke} - Interactive ROC/PR curves; all kinds of additional information provided in the pop-up\n{yardstick}\n{ROCR} - ROC curves, precision/recall plots, lift charts, cost curves, custom curves by freely selecting one performance measure for the x axis and one for the y axis, handling of data from cross-validation or bootstrapping, curve averaging (vertically, horizontally, or by threshold), standard error bars, box plots, curves that are color-coded by cutoff, printing threshold values on the curve, tight integration with Rs plotting facilities (making it easy to adjust plots or to combine multiple plots), fully customizable, easy to use (only 3 commands)\n\nReceiver Operating Curve (ROC)\n\nIf a model is poorly calibrated, the ROC curve value might not show diminished performance. However, the J-index would be lower for models with pathological distributions for the class probabilities.\nA ROC curve plots TPR as a function of FPR at different decision thresholds. We want to minimize our FPR while maximizing our TPR.\nA good model will bend towards the upper-left corner of plot.\nA bad model (i.e., random) will hug the diagonal.\nA ludicrously bad model (i.e., worse than random) will bend towards the lower-right corner of the plot.\nPackages\n\n{runway}\n\n\nPrecision-Recall (PR)\n\n\nWhen the observations with true positive labels are rare, then PR curves are preferred (Also see Class Imbalance)"
  },
  {
    "objectID": "qmd/diagnostics-classification.html#sec-diag-class-thresh",
    "href": "qmd/diagnostics-classification.html#sec-diag-class-thresh",
    "title": "Classification",
    "section": "Threshold Analysis",
    "text": "Threshold Analysis\n\nPackages\n\n{runway}\n\n{pROC}\n{ROCR} - Interesting metrics about various threshold points\n\nNet Benefit and Decision Curves\n\nSee also\n\nDecison Intelligence\nA simple, step-by-step guide to interpreting decision curve analysis\n\n\\(\\text{net benefit} = \\text{sensitivity} \\times \\text{prevalence} â€“ (1 â€“ \\text{specificity)} \\times (1 â€“ \\text{prevalence}) Ã— w\\)\n\nWhere w is the odds at the threshold probability.\nFor a prediction model that gives predicted probability, \\(\\hat p\\), of disease, sensitivity and specificity at a given threshold probability \\(p_t\\) is calculated by defining test positive as \\(\\hat p \\geq p_t\\).\n\n\nDetermine the Probability Threshold Using Expected Utility (article)\n\\[\n\\begin{align}\n\\text{expected utility} &= p \\cdot TPR(t) \\cdot \\nu_{TP} \\\\\n&+ p \\cdot (1-TPR(t)) \\cdot \\nu_{FN} \\\\\n&+ (1-p) \\cdot FPR(t) \\cdot \\nu_{FP} \\\\\n&+ (1-p) \\cdot (1-FPR(t)) \\cdot \\nu_{TN}\n\\end{align}\n\\]\n\nWhere\n\n\\(t\\) is the probability threshold for classifying a 1/0 outcome\n\\(\\nu\\) is the utility assigned to a TP, FN, FP, TN prediction\n\nCan be positive or negative values\nCould be dollar amounts or dollar amounts normalized into weights\n\n\\(p\\) is observed proportion of events (i.e.Â 1s) in the sample.\n\nSee also\n\nEconometrics, Discrete Choice Models\nDecison Intelligence\n\nAlgebraically equivalent expected utility equation\n\\[\nc = TPR(t) \\cdot p \\cdot (\\nu_{TP} - \\nu_{FN}) + FPR(t) \\cdot (1-p) \\cdot (\\nu_{FP} - \\nu_{TN})\n\\]\n\n\\(c\\) is the expected utility\nLinear equation in the form, \\(c = yb + xa\\) where slope, \\(s = -a/b\\)\nSlope\n\\[\ns = \\frac{1-p}{p} \\cdot \\frac{\\nu_{TN} - \\nu_{FP}}{\\nu_{TP} - \\nu_{FN}}\n\\]\n\n\\(s &gt; 1\\): negative classes outweigh positive ones, or correctly classifying a negative class outweighs correctly classifying a positive class in terms of utility or both\n\\(s &lt; 1\\): positive classes outweigh negative ones, or correctly classifying a positive class outweighs correctly classifying a negative class in terms of utility or both\n\n\nOptimizing utility for different values of \\(t\\)\n\nDetermine the blue baseline in the bottom charts for different ranges of \\(s\\)\n\nGiven a range of \\(s\\) and setting \\(t\\) to 0 or 1, we can infer \\(FPR\\) and \\(TPR\\). Then, we use these values to calculate the baseline expected utility\n\nFor \\(s &gt; 1\\): Set \\(t = 1\\), meaning we always reject, resulting in \\(FPR = 0\\) and \\(TPR = 0\\)\nFor \\(s &lt; 1\\): Set \\(t = 0\\), meaning we always accept, resulting in \\(FPR = 1\\) and \\(TPR = 1\\)\n\n\n\nUtility vs Threshold is all you really need. I think the top charts are just there to show how this type of baseline is more difficult (and realistic) bar to pass when compared to the typical 45 degree line used in ROC charts."
  },
  {
    "objectID": "qmd/diagnostics-classification.html#sec-diag-class-calib",
    "href": "qmd/diagnostics-classification.html#sec-diag-class-calib",
    "title": "Classification",
    "section": "Calibration",
    "text": "Calibration\n\nMisc\n\nAlso see\n\nClassification &gt;&gt; Calibration\nTerms section\nA tutorial on calibration measurements and calibration models for clinical prediction models (paper)\n\nrms::val.prob(predicted, observed) (Harrellâ€™s function) outputs most of the calibration metrics mentioned\n\n\npackages\n\n{runway}\n{rtichoke}\n{probably} - tidymodels calibration package\n\nCalibrating Binary Probabilities - Nice tutorial\n\n{CalibrationCurves}\n{{binclass-tools}}\n\nStratification and calibration metrics (e.g.Â low risk, medium risk, and high risk)\n\nUseful if you want to choose the model that performs best on a specific risk group.\nYou can add a categorical variable to the calibration set and group observations based on the predicted probabilities\nAfter grouping the predicted probabilites, calculate calibration metrics for each group\nSee ICI paper for more details\n\n\n\n\nBasic Workflow\n\nCompute Tests for Miscalibration using 2 vectors\n\nClassifierâ€™s predicted probabilities\nNumeric (0/1) observed outcome\n\nCompute Mean, Weak, and Moderate levels of Calibration\nCreate Calibration Plots\nInterpret\n\nIf model isnâ€™t well-calibrated, then\n\nWhich region of the probability range is being over/under-predicted?\n\nAre these regions important to the use case?\n\nIs the model useful with this level of calibration\nCalibrate predictions (see Classification &gt;&gt; Calibration &gt;&gt; Methods &gt;&gt; Platt Scaling/Isotonic Regression)\n\nIf model wasnâ€™t well-calibrated, but now the predictions have been calibrated,\n\nIs model/predictions still not sufficiently calibrated?\nAre there any regions still being over/under-predicted?\n\nAre these regions important to the use case?\n\nIs the model useful with this level of calibration\n\n\n\n\n\nTests for Miscalibration\n\nMisc\n\nThe ROC curve value might not show diminished performance.\n\nHowever, theÂ JÂ index would be lower for models with pathological distributions for the class probabilities.\n\nThe Hosmer-Lemeshow (H-L) test is popular but has major shortcomings. Not a good test to trust by itself\n\nBased on artificially grouping patients into risk strata, p-value that is uninformative with respect to the type and extent of miscalibration, and low statistical power\nSee Spiegelhalter z statistic &gt;&gt; Manually &gt;&gt; paper for more details\nAlso see RMS Ch 10.5 (end of section, listen to audio)\n\n\n\n\nSpiegelhalter Z Statistic\n\\[\nz_s = \\frac{\\sum_{i=1}^N (O_i - E_i)(1-2E_i)}{\\sqrt{\\sum_{i=1}^N (1-2E_i)^2(1-E_i)E_i}}\n\\]\n\nWhere\n\n\\(E\\) is the predicted probability\n\\(O\\) is the observed outcome (0/1)\n\\(N\\) is the number of observations\n\nStata: This tests â€œwhether an individual Brier score is extremeâ€\np-value &lt; 0.05 suggests an improperly calibrated model (lower z-stats = more calibrated)\nNull hypothesis is that the estimated probabilities are equal to the true class probabilities\nrms::val.prob(predicted, observed)\n\nGives â€œthe Spiegelhalter Z-test for calibration accuracy, and its two-tailed P-valueâ€\nz-score is under â€œS:zâ€ and itâ€™s p-value is under â€œS:pâ€\n\nManually (paper, github)\nSpiegelhalter_z = function(y, prob){\nÂ  alpha = 0.05\nÂ  z_score = sum((y-prob)*(1-2*prob))/sqrt(sum(((1-2*prob)^2)*prob*(1-prob)))\nÂ  print(z_score)\nÂ  if (abs(z_score) &gt; qnorm(1-alpha/2)){\nÂ  Â  print('reject null. NOT calibrated')\nÂ  } else{\nÂ  Â  print('fail to reject. calibrated')\nÂ  }\nÂ  cat('z score: ', z_score, '\\n')\nÂ  cat('p value: ', 1-pnorm(abs(z_score)), '\\n')\nÂ  return(z_score)\n}\n\ny: observed outcome (1/0)\nprob: predicted probabilities from your classifier\nUses upper-tail p-value, but I think a two-sided p-value is probably more correct (See below)\n\nHypothesis tests\n\nHarrell uses a two-sided p-value but a couple other papers I read just use the upper-tail. I asked Harrell and he tagged Spiegelhalter in a tweet to see which is correct (tweet). Havenâ€™t heard back yet.\nSAS squares it to get a ChiSq distribution, which is equivalent to a 2-tail Z-test, and tests it that way: pchisq(z_stat^2), lower.tail = F)\nStata uses the upper tail probability\n\n\n\n\nIntegrated Calibration Index (ICI, Eavg)\n\npaper\nMisc\n\nInterpreted as weighted mean absolute difference between observed and predicted probabilities, in which observations are weighted by the empirical density function (i.e.Â empirical distribution) of the predicted probabilities.\nMotivated by Harrellâ€™s Emax index (see below), which is the maximum absolute difference between a smooth calibration curve and the diagonal line of perfect calibration\nLower is better for each metric (ICI, E50, E90, and Emax)\n\nThese metrics are used for model comparison. There arenâ€™t guidelines for metric values that delineate well-calibrated models for poorly calibrated ones.\n\nSee below for ICI bootstrap CIs\nIn general, the larger the sample size the lower the values for each metric\n\ni.e.Â donâ€™t compare models that were fit with different sized datasets.\n\nEmax is the least stable out of the four metrics\n\nProcess\n\nFit a LOESS model for observed_outcome ~ predicted probabilities\nTake the average of the abs difference between the loess predicted probabilities and your modelâ€™s predicted probabilities\n\nLoess predicted probabilities are predicted using your modelâ€™s predicted probabilities as new data.\n\n\nManually (paper, github)\nici = function(Y, P){\n\nÂ  loess.calibrate &lt;- loess(Y ~ P)Â \n\nÂ  # Estimate loessâ€based smoothed calibration curve\nÂ  P.calibrate &lt;- predict(loess.calibrate, newdata = P)\n\nÂ  # This is the point on the loess calibration curve corresponding to a given predicted probability.\nÂ  ICI &lt;- mean(abs(P.calibrate - P))\nÂ  return(ICI)\n\nÂ  # plot\nÂ  plot(P, P.calibrate)\nÂ  lines(P.calibrate, x=P)\n}\n\n\n\nE50, E90, and Emax\n\nCode\n# Let Y denote a vector of observed binary outcomes.\n# Let P denote a vector of predicted probabilities.\nloess.calibrate &lt;â€ loess(Y âˆ¼ P)\nP.calibrate &lt;â€ predict(loess.calibrate, newdata = P)\n\nE50 &lt;â€ median(abs(P.calibrate â€“ P))\nE90 &lt;â€ quantile(abs(P.calibrate â€“ P), probsâ€‰=â€‰0.9)\nEmax &lt;â€ max(abs(P.calibrate â€“ P))\nNotes from ICI paper above\nLower is better for each metric\nEmax - maximal absolute difference between observed and predicted probabilities of the outcome (Harrellâ€™s)\n\ni.e.Â The maximal absolute vertical distance between the calibration curve and the diagonal line denoting perfect calibration\nIssue: The greatest distance between observed and predicted probabilities may occur at a point at which the distribution of predicted probabilities is sparse.\n\nWhen comparing two competing prediction methods, it is possible that one method, despite having a greater value of Emax, actually displays greater calibration in the region of predicted probabilities in which most predicted probabilities lie.\n\n\nE50 - denotes the median absolute difference between observed and predicted probabilities\n\ni.e.Â same as ICI (aka Eavg) except uses the median instead of the mean\nLess influenced by a small minority of subjects/observations for whom there is a large discrepancy between observed and predicted probabilities (i.e.Â outliers)\n\nE90 - denotes the 90th percentile of the absolute difference between observed and predicted probabilities.\n\nSummarizes the limit to the absolute discrepancy for the large majority of subjects in the sample\n\nBootstrap CIs for ICI, E50, E90, Emax\n\n{CalibrationCurves}\n\nHarrellâ€™s val.prob function but only requires the numeric observed outcome and predicted probabilities, so you donâ€™t need to enter Harrellverse\nHas a ton of extra features, but the documentation is kind of poor.\nval.prob.ci.2(preds, observed_y)\n\nExample: Using rms::val.prob with a glm model (article)\n\nAlso code for viz of the bootstrap distributions of each metric\n\nProcedure\n\nDraw bootstrap sample from the training dataset\nDraw bootstrap sample from the calibration dataset\nFit classifier to the training bootstrap sample\nGet predictions from classifier on calibration sample\nCalculate calibration metrics using calibration setâ€™s observed outcomes and classifierâ€™s predictions on the calibration set.\n\nPaper used 2000 bootstrap replicates (B = 2000).\nPercentileâ€based bootstrap confidence intervals were then constructed using these 2000 bootstrap replicates.\n\ne.g.Â the endpoints of the estimated 95% confidence interval were the empirical 2.5th and 97.5th percentiles of the distribution of the given calibration metric across the 2000 bootstrap replicates.\n\nTo calculate CIs for the difference in calibration metrics between models, I think each type of classifier would participate for each replicate and the differences would be calculated in step 5. (glossed over in the paper)\n\nIf the difference CI contains 0, then it can be said that the calibration for both models is comparable.\n\n\n\n\n\nExpected Calibration Error (ECE)\n\nSee Evaluation of Calibration Levels &gt;&gt; Binning Method for details on whatâ€™s happening here. Almost all the calculations mentioned in that section are included in this metric.\nAlso described in Expected Calibration Error: A Visual Explanation With Python\nBinary\n\\[\n\\begin{aligned}\n&ECE = \\sum_{m=1}^M \\frac{|B_m|}{n}|\\mbox{acc}(B_m) -  \\mbox{conf}(B_m)|\n\\\\\n&\\begin{aligned}\n\\mbox{where}\\;\\; &\\mbox{acc}(B_m) = \\frac{1}{|B_m|} \\sum_{i\\in B_m} \\mathbb{1} (\\hat y_i = y_i)   \\;\\; \\mbox{and} \\\\\n&\\mbox{conf}(B_m) = \\frac{1}{|B_m|} \\sum_{i \\in B_m} \\hat p_i\n\\end{aligned}\n\\end{aligned}\n\\]\n\n\\(|B_m|\\) - The number of observations in bin \\(m\\)\n\\(\\mbox{acc}(B_m)\\) - The average accuracy of the predictions in bin \\(m\\)\n\nA prediction is the label determined by the predicted probability and the threshold.\n\n\\(\\mbox{conf}(B_m)\\) - The average predicted probability of the predictions in bin \\(m\\)\n\nPolytomous\n\nThere will be a predicted probability for each label.\nFor \\(\\hat p_i\\), the maximum probability will be selected since that probability will determine the predicted label. Then, the calculation for \\(\\mbox{conf}(B_m)\\) will be the same as the binary case.\nFor \\(\\mbox{acc}(B_m)\\), the maximum probability for the observation determines the label. Then, the calculation will be the same as the binary case â€” correct label = 1, incorrect label = 0.\n\n\n\n\n\nEvaluation of Calibration Levels\n\nLevels\n\nMean - See Calibration-in-the-Large, (Y vs Yhat)\n\naverage predicted risk compared with the overall event rate\n\nWeak - means that, on average, the model does not over or underestimate risk and does not give overly extreme (too close to 0 and 1) or modest (too close to disease prevalence or incidence) risk estimates.\n\nintercept - a target value of 0 (Calibration-in-the-Large)\n\nA logistic regression model, Y ~ offset(Yhat), is fit to get the calibration intercept\n&lt; 0 suggests overestimation\n&gt; 0 suggests underestimation\n\nslope - target value of 1\n\nA logistic regression model, Y ~ Yhat, is fit to get the calibration slope (paper, see supplemental material)\n&lt;â€‰1 suggests that estimated risks are too extreme\n\ni.e., too high for patients who are at high risk and too low for patients who are at low risk\n\nâ€‰&gt;â€‰1 suggests the opposite\n\ni.e., that risk estimates are too moderate\n\n\nA calibration intercept close to 0 and a calibration slope close to 1 do not guarantee that the flexible calibration curve (Calibration-in-the-Small) is close to the diagonal\nAcceptable to stop evaluation at weak calibration for small datasets\n\nModerate - implies that estimated risks correspond to observed proportions\n\ne.g., among patients with an estimated risk of 10%, 10 in 100 have or develop the event\nSee Calibration-in-the-Small\n\nStrong - the predicted risk corresponds to the observed proportion for every possible combination of predictor values; this implies that calibration is perfect and is a utopic goal\n\n\n\nCalibration Plots (Reliability Diagrams)\n\nMisc\n\nSee Misc &gt;&gt; packages for calibration curve functions\nCalibration curves for nested cv (post)\n\nA loess curve is estimated for each split in the outer loop. Those curves are then â€œaveragedâ€ by fitting another loess curve onto those curves.\n\nHarrell re calibration curves showing miscalibration at the tails: â€œyou can ignore extreme tails where there are almost no data, with the exception that youâ€™d need to mention that if future predictions ever appear in such tails such predictions are of unknown accuracy.\n{probably}\n\nCalibration Curve:\n\npreds &lt;- tibble(\n            truth = data$class,\n            truth_int = as.integer(data$class) - 1,\n            estimate = predict_prob(unbalanced_model, data)\n         )\ncal_plot_breaks(preds, truth = truth_int, estimate = estimate)\n\n\n\n\nLOESS\n\nHarrell uses a loess curve, loess(y ~ yhat, iter = 0)\nBecause binning subjects into strata based on predicted risk may result in a loss of information\nWith rms::calibrate, he also bootstraps the process to create a bias-corrected curve\n\nSee RMS Ch 10.11 pgs 269-270 for details\ncalibrate code on github\n\nBe aware there is more that one â€œcalibrateâ€ function in the repo\n\n\nEvaluating a logistic regression based prediction tool in R\n\nCode for a simple loess smoothed calibration plot and a binned one, both in ggplot2\nThe binned one uses an â€œlmâ€ smoother. The intercept (and slope?) should measure Calibration-in-the-Large (see Terms)\nThe loess curve vs the 45 degress ab-line should be a visualization of Calibration-in-the-Small (see Terms)\n\n{CalibrationCurves}\n\nHarrellâ€™s val.prob function but only requires the numeric observed outcome and predicted probabilities, so you donâ€™t need to enter Harrellverse\nHas a ton of extra features, but the documentation is kind of poor.\nval.prob.ci.2(preds, observed_y)\n\n\n\n\nBinning Method\n\nx-axis\n\nThe predicted probabilities are divided up into a fixed number of bins along the x-axis.\n\nToo few bins and there wonâ€™t be enough points on the curve. Too many bins and there will be too few observations in each bin leading to more noise. It is common to select 10 bins.\nFreedman-Diaconis rule - a statistical rule designed for finding the number of bins that makes the histogram as close as possible to the theoretical probability distribution\n\nFor each bin, calculate the average predicted probability. This will be your x-axis value.\n\ny-axis\n\nObserved proportion of positive events (1) for each x-axis bin\n\nCode\nGetCalibrationCurve &lt;- function(y, y_pred, bins = 10) {\nÂ  Â  data.frame(y = y, y_pred = y_pred) %&gt;%\nÂ  Â      arrange(y_pred) %&gt;%\nÂ  Â      mutate(pos = row_number() / n(),\nÂ  Â  Â  Â  Â       bin = ceiling(pos * bins)) %&gt;%\nÂ  Â      group_by(bin) %&gt;%\nÂ  Â      summarize(pred_prob = mean(y_pred),\nÂ  Â  Â  Â  Â  Â  Â      obs_prob = mean(y))\n}\n\ndf &lt;- GetCalibrationCurve(y, y_pred, bins = 10)\n\nggplot(df, aes(x = pred_prob, y = obs_prob)) +\nÂ  geom_point() +\nÂ  geom_line() +\nÂ  geom_abline(slope = 1, intercept = 0, linetype = 2) +\nÂ  coord_cartesian(xlim = c(0, 1), ylim = c(0, 1)) +\nÂ  theme_bw() +\nÂ  labs(title = \"Calibration Curve\", x = \"Predicted Probability\",\nÂ  Â  Â   y = \"Observed Probability\")\nAlso see Evaluating a logistic regression based prediction tool in R\n\n\n\nInterpretation\n\nThe better calibrated or more reliable a forecast, the closer the points will appear along the main diagonal from the bottom left to the top right of the plot.\nBelow the diagonal: The model has over-forecast; the probabilities are too large.\nAbove the diagonal: The model has under-forecast; the probabilities are too small.\nExample 1\n\n\nSystematic overestimation (red): Compared to the true distribution, the distribution of predicted probabilities is pushed towards the right. This is common when you train a model on an unbalanced dataset with very few positives.\nSystematic underestimation (blue): Compared to the true distribution, the distribution of predicted probabilities is pushed leftward.\nCenter of the distribution is too heavy (green): This happens when â€œalgorithms such as support vector machines and boosted trees tend to push predicted probabilities away from 0 and 1â€ (quote from Predicting good probabilities with supervised learning).\nTails of the distribution are too heavy (black): For instance, â€œOther methods such as naive bayes have the opposite bias and tend to push predictions closer to 0 and 1â€ (quote from Predicting good probabilities with supervised learning).\n\nExample 2 (paper)\n\n\nBased on an outcome with a 25% event rate and a model with an area under the ROC curve (AUC or c-statistic) of 0.71.\na: General over- or underestimation of predicted risks.\nb: Predicted risks that are too extreme or not extreme enough"
  },
  {
    "objectID": "qmd/diagnostics-clustering.html#sec-diag-clust-misc",
    "href": "qmd/diagnostics-clustering.html#sec-diag-clust-misc",
    "title": "Clustering",
    "section": "Misc",
    "text": "Misc\n\nAlso see Notebook, pg 57\nFor K-Means, elbow method (i.e.Â WSS) is awful. Recommended: Calinski-Harabasz Index and BIC then Silhouette Coefficient or Davies-Bouldin Index\n{{sklearn.metrics.cluster}}"
  },
  {
    "objectID": "qmd/diagnostics-clustering.html#sec-diag-clust-sphcent",
    "href": "qmd/diagnostics-clustering.html#sec-diag-clust-sphcent",
    "title": "Clustering",
    "section": "Spherical/Centroid Based",
    "text": "Spherical/Centroid Based\n\nWithin-Cluster Sum of Squares (WSS) (aka Inertia)\n\nMeasures the variability of the observations within each cluster. In general, a cluster that has a small sum of squares is more compact than a cluster that has a large sum of squares.\nTo calculate WCSS, you first find the Euclidean distance between a given point and the centroid to which it is assigned. You then iterate this process for all points in the cluster, and then sum the values for the cluster and divide by the number of points\nInfluenced by the number of observations. As the number of observations increases, the sum of squares becomes larger. Therefore, the within-cluster sum of squares is often not directly comparable across clusters with different numbers of observations.\n\nTo compare the within-cluster variability of different clusters, use the average distance from centroid instead.\n\nTypically used in the elbow method for kmeans for choosing the number of clusters\n\nAverage Distance from Centroid\n\nThe average of the distances from observations to the centroid of each cluster.\nThe average distance from observations to the cluster centroid is a measure of the variability of the observations within each cluster. In general, a cluster that has a smaller average distance is more compact than a cluster that has a larger average distance. Clusters that have higher values exhibit greater variability of the observations within the cluster.\n\nSilhouette Coefficient\n\nComputationally expensive\nFormula\n\\[\ns(i) = \\frac{b(i) - a(i)}{\\max \\{a(i), b(i)\\}},\\; \\text{if}\\; |C_I| &gt; 1\n\\]\n\nIntra-Cluster Distance\n\\[\na(i) = \\frac{1}{|C_I| - 1} \\sum_{j \\in C_I, i \\neq j} d(i,j)\n\\]\n\nThe mean distance between i and all the other data points within C.\n\\(|C_I|\\) is the number of points belonging to cluster i, and d(i , j) is the distance between data points i and j in the cluster CI\n\nInter-Cluster Distance\n\\[\nb(i) = \\min_{J \\neq I} \\frac{1}{C_J} \\sum_{j \\in C_J} d(i, j)\n\\]\n\nThe mean distance between i to all the points of its nearest neighbor cluster\n\n\nRange: -1 to 1\nGood: 1\nBad: -1\n\nSays inter-cluster distances are not comparable to the intra-cluster distances\n\n\nCalinski-Harabasz Index (aka Variance Ratio Criterion)\n\nCompares the variance between-clusters to the variance within each cluster\nNOT to be used to the density based methods, such as mean-shift clustering, DBSCAN, OPTICS, etc.\n\nClusters in density based methods are unlikely to be spherical and therefore centroids-based distances will not be that informative to tell the quality of the clustering algorithm\nMuch faster than Silhouette score calculation\n\nRatio of the squared inter-cluster distance sum and the squared intra-cluster distance sum for all clusters\nFormula\n\\[\nCH = \\frac{\\sum_{k=1}^K n_k \\lVert c_k - c \\rVert^2}{K-1} \\cdot \\frac{N-K}{\\sum_{k=1}^K \\sum_{i=1}^{n_k} \\lVert d_i - c_k \\rVert^2}\n\\]\n\n\\(n_k\\) is the size of the kth cluster\n\\(c_k\\) is the feature vector of the centroid of the kth cluster\n\\(c\\) is the feature vector of the global centroid of the entire dataset\n\\(d_i\\) is the feature vector of data point i\n\\(N\\) is the total number of data points\n\nRange: no upper bound\nHigher is better and means the clusters are separated from each other\n\nDavies-Bouldin Index\n\nAverage similarity between each cluster and its most similar one.\nNOT to be used to the density based methods, such as mean-shift clustering, DBSCAN, OPTICS, etc.\n\nClusters in density based methods are unlikely to be spherical and therefore centroids-based distances will not be that informative to tell the quality of the clustering algorithm\n\nMuch faster than Silhouette score calculation\nRange [0,1] and lower is better\nThe type of norm used in the formula should probably match the distance type used in the clustering algorithm. See the wiki for details.\nFormula\n\\[\nDB = \\frac{1}{N} \\sum_{i=1}^N D_i\n\\]\n\nAn averaged similarity score across all clusters with its nearest neighbor cluster\n\\(D_i\\) is the ith clusterâ€™s worst (i.e.Â largest) similarity score, \\(R_{i,j}\\) across all other clusters\n\\[\nD_i = \\max_{j\\neq i} R_{i,j}\n\\]\nSmaller similarity score indicates a better cluster separation\n\\[\n\\begin{aligned}\n&R_{i,j} = \\frac{S_i + S_j}{M_{i,j}}\\\\\n&\\begin{aligned}\n\\text{where} \\;\\; &S_i = \\left(\\frac{1}{T_i} \\sum_{j=1}^{T_i} \\lVert X_j - A_i \\rVert_{p}^q \\right)^{\\frac{1}{q}} \\;\\; \\text{and} \\\\\n&M_{i,j} = \\lVert A_i - A_j \\rVert_p = \\left(\\sum_{k=1}^n |a_{k,i} - a_{k,j}|^p\\right)^{\\frac{1}{p}}\n\\end{aligned}\n\\end{aligned}\n\\]\n\n\\(X_j\\) : n-dimensional feature vector assigned to Cluster \\(C_i\\)\n\\(T_i\\): The size of cluster \\(C_i\\)\n\\(A_i\\): Centroid of \\(C_i\\) and \\(a_{k,i}\\) is an element of the kth feature vector of \\(A_i\\) which has \\(n\\) elements\n\\(P\\): the degree of the norm; typically 2 for the euclidean norm (i.e.Â distance)\n\\(q\\): Not sure what this is (something to do with a statistical moment), but wiki only describes the parameter when itâ€™s equal to 1 which makes S the average distance between the feature vectors and the centroid. So thatâ€™s probably the typical value."
  },
  {
    "objectID": "qmd/diagnostics-clustering.html#sec-diag-clust-featred",
    "href": "qmd/diagnostics-clustering.html#sec-diag-clust-featred",
    "title": "Clustering",
    "section": "Feature Reduction",
    "text": "Feature Reduction\n\nBayesian Information Criteria (BIC)\n\nHigher is better\nGoF metric in this situation thatâ€™s typically used for GMMs\nNot on the same scale as WSS\nPy Code\ndef bic_score(X, labels):\nÂ  \"\"\"\nÂ  BIC score for the goodness of fit of clusters.\nÂ  This Python function is directly translated from the GoLang code made by the author of the paper.Â \nÂ  The original code is available here: https://github.com/bobhancock/goxmeans/blob/a78e909e374c6f97ddd04a239658c7c5b7365e5c/km.go#L778\nÂ  \"\"\"\n\nÂ  n_points = len(labels)\nÂ  n_clusters = len(set(labels))\nÂ  n_dimensions = X.shape[1]\nÂ  n_parameters = (n_clusters - 1) + (n_dimensions * n_clusters) + 1\nÂ  loglikelihood = 0\nÂ  for label_name in set(labels):\nÂ  Â  X_cluster = X[labels == label_name]\nÂ  Â  n_points_cluster = len(X_cluster)\nÂ  Â  centroid = np.mean(X_cluster, axis=0)\nÂ  Â  variance = np.sum((X_cluster - centroid) ** 2) / (len(X_cluster) - 1)\nÂ  Â  loglikelihood += \\\nÂ  Â  Â  n_points_cluster * np.log(n_points_cluster) \\\nÂ  Â  Â  - n_points_cluster * np.log(n_points) \\\nÂ  Â  Â  - n_points_cluster * n_dimensions / 2 * np.log(2 * math.pi * variance) \\\nÂ  Â  Â  - (n_points_cluster - 1) / 2\n\nÂ  bic = loglikelihood - (n_parameters / 2) * np.log(n_points)\n\nÂ  return bic"
  },
  {
    "objectID": "qmd/diagnostics-cv.html#sec-diag-cv-misc",
    "href": "qmd/diagnostics-cv.html#sec-diag-cv-misc",
    "title": "CV",
    "section": "Misc",
    "text": "Misc\n\nA better validation set score than the training set score (Notes from link):\n\nYou donâ€™t have that much data and itâ€™s luck.\n\nCan be diagnosed by changing the seed (random_state in py) in data split function\n\n\nGap between them shrinks over time\n\nMay be do to regularization (if itâ€™s being used).\nDuring validation and testing, your loss function only comprises prediction error\n\nGap between them stays the same and training loss has fluctuations\n\nDL: dropout is only applicable during the training process, so it only affects training loss\n\nValidation loss lower than training loss at first but has similar or higher values later on\n\nDL: Training loss is calculated during each epoch, but validation loss is calculated at the end of each epoch\n\n\nCompare Training vs Test\n\nExample: {gt} table, {yardstick} forecast metrics\nbind_rows(\nÂ  yardstick::mape(rf_preds_train, Sale_Price, .pred),\nÂ  yardstick::mape(rf_preds_test, Sale_Price, .pred)\n) %&gt;%Â \nÂ  mutate(dataset = c(\"training\", \"holdout\")) %&gt;%Â \nÂ  gt::gt() %&gt;%Â \nÂ  gt::fmt_number(\".estimate\", decimals = 1)"
  },
  {
    "objectID": "qmd/diagnostics-cv.html#sec-diag-cv-reg",
    "href": "qmd/diagnostics-cv.html#sec-diag-cv-reg",
    "title": "CV",
    "section": "Regression",
    "text": "Regression\n\nFor prediction, if coefficients vary significantly across the test folds their robustness is not guaranteed (see coefficient boxplot below), and they should probably be interpreted with caution.\n\n\nBoxplots show the variance of the coefficient across the folds of a repeated 5-fold cv.\nThe â€œCoefficient importanceâ€ in the example is just the coefficient value of the standardized variable in a ridge regression\nNote outliers beyond the whiskers for Age and Experience\n\nIn this case, the variance is caused by the fact that experience and age are strongly collinear.\n\nVariability in coefficients can also be explained by collinearity between predictors\n\nPerform sensitivity analysis by removing one of the collinear predictors and re-running the CV. Check if the variance of the variable that was kept has stabilized (e.g.Â fewer outliers past the whiskers of a boxplot)."
  },
  {
    "objectID": "qmd/diagnostics-dl.html#sec-diag-dl-misc",
    "href": "qmd/diagnostics-dl.html#sec-diag-dl-misc",
    "title": "DL",
    "section": "Misc",
    "text": "Misc\n\nPackages\n\n{{weightwatcher}}\n\nRaschka (thread)\n\nGeneral\n\nMake sure training loss converged\n\nWant to see a plateau in the loss (y-axis)\n\nLeft: bad; Right: better\n\n\nCheck for overfitting\n\nDonâ€™t want the gap between training and validation accuracy to be too large\n\nLeft: bad; Right: better\n\n\nCompare accuracy to a zero-rule baseline\n\nCheck that the validation accuracy is substantially better than a baseline based on always predicting the majority class (aka zero-rule classifier)\n\nTop chunk of code is just to determine which class is the majority class, which is class 1 with 1135 observations (aka examples)\nBottom chunk calculates the accuracy if a model just choose to classify each observation as class 1\n\n\nLook at failure cases\n\nAlways useful to check what cases the model gets wrong.\nAnalysis of these cases might detect things like mislabeled data\n\nPlot at a confusion matrix\n\nExample: PyTorch digit classifier\nimport matplotlib\n\nfrom mlxtend.plotting import plot_confusion_matrix\nfrom torchmetrics import ConfusionMatrix\n\ncmat = ConfusionMatrix(num_classes=len(class_dict))\n\nfor x, y in dm.test_dataloader():\n\n  with torch.inference.mode():\n    pred = lightning_model(x)\n  cmat(pred, y)\n\ncmat_tensor = cmat.compute()\ncmat = cmat_tensor.numpy()\n\nfig, ax = plot_confusion_matrix(\n  conf_mat=cmat,\n  class_names=class_dict.values(),\n  norm_colormap=matplotlib.colors.LogNorm()\n)\nplt.xticks(rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n\nplt.savefig('cm.pdf')\nplt.show()\n\n\n\nNew Architecture\n\nCheck that you can overfit 1000 data points, by using the same training and validation.\n\nPyTorch Lightning has this flag\nThe loss should be near zero (because the network should be able to memorize it); if not, thereâ€™s a bug in your code.\n\n\nRun {{weightwatcher}}}} and check that the layers have converged individually to a good alpha, and exhibit no rank collapse or correlation traps."
  },
  {
    "objectID": "qmd/diagnostics-forecasting.html#sec-diag-fcast-misc",
    "href": "qmd/diagnostics-forecasting.html#sec-diag-fcast-misc",
    "title": "Forecasting",
    "section": "Misc",
    "text": "Misc\n\nAlso see\n\nLogistics &gt;&gt; Decision Impact Metrics\nLoss Functions\n\nLoss Functions If you care about errors measured in percent (i.e.Â â€œrelativeâ€) and your data are strictly positive, then â€œrelativeâ€ metrics such as the MALE and RMSLE MAPE and sMAPE are also in this class of metrics but have issues (See Loss Functions)\n\nIf you care about errors measured in real units (e.g.Â number of apples), or your data can be zero or negative, then â€œrawâ€ metrics such as MAE or MSE are more appropriate.\n\nSee Loss Functions &gt;&gt; Misc &gt;&gt; re Stochastic Gradient Descent\n\nIf you want to compare or aggregate performance metrics across time series, then you might want to use scaled metrics such as MASE, MASLE\n\nUsing MASLE will require your data are strictly positive\n\n\nHow does the amount of data affect prediction\n\nCould be useful for choosing a training window for production\nExample: Relative Absolute Error vs number of rows in the training set\n\n\nInterpretation: No pattern?\n\nMightâ€™ve been useful to only look at the values from 0.5. Looks like a lot more points a cluster at data sizes between 1000 and 1200 rows.\n\n\nExample: MAE vs prediction horizon (colored by the number of weeks of data in training set)\n\n\nInterpretation for this data and model: For prediction horizons greater than a couple weeks, having mored data in the training set leads to worse performance\n\n\nDoes the model predict values close to zero\n\n\nâ€œLabelsâ€ are the observed values\nBad performance with values close to zero can be the result of the loss function used where lower losses are not penalized as much as higher losses"
  },
  {
    "objectID": "qmd/diagnostics-forecasting.html#sec-diag-fcast-prob",
    "href": "qmd/diagnostics-forecasting.html#sec-diag-fcast-prob",
    "title": "Forecasting",
    "section": "Probabilistic",
    "text": "Probabilistic\n\nContinuous Ranked Probability Score (CRPS)\n\nfabletools::accuracy\nMeasures forecast distribution accuracy\nCombines a MAE score with the spread of simulated point forecasts\nSee notebook (pg 172)\n\nWinkler Score\n\nfabletools::accuracy\nMeasures how well a forecast is covered by the prediction intervals (PI)\nSee notebook (pg 172)"
  },
  {
    "objectID": "qmd/diagnostics-glm.html#sec-diag-glm-misc",
    "href": "qmd/diagnostics-glm.html#sec-diag-glm-misc",
    "title": "GLM",
    "section": "Misc",
    "text": "Misc\n\nThe degrees of freedom are related to the number of observations, and how many predictors you have used. If you look at the mean value in the prostate dataset for recurrence, it is 0.1708861, which means that 17% of the participants experienced a recurrence of prostate cancer. If you are calculating the mean of 315 of the 316 observations, and you know the overall mean of all 315, you (mathematically) know the value of the last observation - recurrence or not - it has no degrees of freedom. So for 316 observations, you have n-1 or 315, degrees of freedom. For each predictor in your model you â€˜use upâ€™ one degree of freedom. The degrees of freedom affect the significance of the test statistic (T, or chi-squared, or F statistic).\n\nShould be in the summary of the model\n\nChi Square test for the deviance only works for nested models\n** The formulas for the deviances for a logistic regression model are slightly different since the deviance for the saturated logistic regression model is 0 **"
  },
  {
    "objectID": "qmd/diagnostics-glm.html#sec-diag-glm-dev",
    "href": "qmd/diagnostics-glm.html#sec-diag-glm-dev",
    "title": "GLM",
    "section": "Deviance Metrics",
    "text": "Deviance Metrics\n\nMisc\n\nNotes from Saturated Models and Deviance video\nDeviance is 2 * Log Likelihood\n\nLog likelihood can usually be extracted from the model object (e.g.Â ll_proposed &lt;- mod$logLik)\n\nSaturated Model also called the Full model (Also see Regression, Discrete &gt;&gt; Misc)\n\nThe full model has a parameter for each observation and describes the data perfectly while the reduced model provides a more concise description of the data with fewer parameters.\nUsually calculated from the data themselves\ndata(wine, package = \"ordinal\")\ntab &lt;- with(wine, table(temp:contact, rating))\n## Get full log-likelihood (aka saturated model log-likelihood)\npi.hat &lt;- tab / rowSums(tab)\n(ll.full &lt;- sum(tab * ifelse(pi.hat &gt; 0, log(pi.hat), 0))) ## -84.01558\n\nGOF: as a rule of thumb, if the deviance about the same size as the difference in the number of parameters (i.e.Â pfull - pproposed), there is NOT evidence of lack of fit. ({ordinal} vignette, pg 14)\n\nExample (have doubts this is correct)\n\nLooking at the number of params (â€œno.parâ€) for fm1 in Example: {ordinal}, model selection with LR tests below and the model summary in Proportional Odds (PO) &gt;&gt; Example: {ordinal}, response = wine rating (1 to 5 = most bitter), the number of parameters for the reduced model is the number of regression parameters (2) + number of thresholds (4)\nFor the full model (aka saturated), the number of thresholds should be the same, and there should be one more regression parameter, an interaction between â€œtempâ€ and â€œcontactâ€. So, 7 should be the number of parameters for the full model\nTherefore, for a good-fitting model, the deviance should be close to pfull - preduced = 7 - 6 = 1\nThis example uses â€œnumber of parametersâ€ which is the phrase in the vignette but I think itâ€™s possible he might mean degrees of freedom (dof) which he immediatedly discusses afterwards. In the LR Test example below, under LR.Stat, which is essentially what deviance is, the number is around 11 which is quite aways from 1. Not exactly an apples to apples comparison, but the size after adding 1 parameter just makes me wonder if dof would match this scale of numbers for deviances better.\n\n\n\nResidual Deviance (G2): Dresid = Dsaturated - Dproposed\n\n2*log_likelihood between a saturated model and the proposed model\n\n2 *(LL(sat_mod) - LL(proposed_mod))\n-2 * (LL(proposed_mod) - LL(sat_mod))\n\nSee example 7, pg 13 ({ordinal} vignette) for (manual) code\nYour residual deviance should be lower than the null deviance. You can even measure whether your model is significantly better than the null model by calculating the difference between the Null Deviance and the Residual Deviance. This difference [281.9 - 246.8 = 35.1] has a chi-square distribution. You can look up the value for chi-square with 2 degrees (because you had 2 predictors) of freedom. Or you can calculate this in R with pchisq(q = 35.1, df=2, lower.tail = TRUE) which gives you a p value of 1.\n\nNull Deviance: Dnull = Dsaturated - Dnull\n\nAs a GOF for a single model, a model can be compared to the Null model (aka intercept-only model)\n2*log_likelihood between a saturated model and the intercept-only model (aka Null model)\n\n2 *(LL(sat_mod) - LL(null_mod))\n-2 * (LL(null_mod) - LL(sat_mod))\n\n\nMcFaddenâ€™s Pseudo R2 = (LL(null_mod) - LL(proposed_mod)) / (LL(null_mod) - LL(saturated_mod))\n\nThe p-value for this R2 is the same as the p-value for:\n\n2 * (LL(proposed_mod) - LL(null_mod))\nNull Deviance - Residual Deviance\n\nFor the dof, use proposed_dof - null_dof\n\ndof for the null model is 1\n\n\n\nExample: Getting the p-value\nm1 &lt;- glm(outcome ~ treat)\nm2 &lt;- glm(outcome ~ 1)\n(ll_diff &lt;- logLik(m1) - logLik(m2))\n## 'log Lik.' 3.724533 (df=3)\n1 - pchisq(2*ll_diff, 3)\n\nLikelihood Ratio Test (LR Test) - For a pair of nested models, the difference in âˆ’2ln L values has a Ï‡2 distribution, with degrees of freedom equal to the difference in number of parameters estimated in the models being compared.\n\nRequirement: Deviance tests are fine if the expected frequencies under the proposed model are not too small and as a general rule they should all be at least five.\n\nAlso see Discrete Analysis notebook\n\nExample\n\nÏ‡2 = (-2)*log(model1_likelihood) - (-2)*log(model2_likelihood) = 4239.49 â€“ 4234.02 = 5.47\n\n-2*log can probably be factored out\n\ndegrees of freedom = model1_dof - model2_dof = 12 â€“ 8 = 4\npval &gt; 0.05 therefore the likelihoods of these models are not signficantly different\n\n\nCompare nested models\n\nExample: LR Test Manually\n\nModels\nmodel1 &lt;- glm(TenYearCHD ~ ageCent + currentSmoker + totChol,Â \nÂ  Â  Â  Â  Â  Â  Â  data = heart_data, family = binomial)\nmodel2 &lt;- glm(TenYearCHD ~ ageCent + currentSmoker + totChol +Â \nÂ  Â  Â  Â  Â  Â  Â  Â  as.factor(education),Â \nÂ  Â  Â  Â  Â  Â  Â  data = heart_data, family = binomial)\n\nAdd Education or not?\n\nExtract Deviances\n# Deviances\n(dev_model1 &lt;- glance(model1)$deviance)\n## [1] 2894.989\n(dev_model2 &lt;- glance(model2)$deviance)\n## [1] 2887.206\nCalculate difference and test significance\n# Drop-in-deviance test statistic\n(test_stat &lt;- dev_model1 - dev_model2)\n## [1] 7.783615\n\n# p-value\n1 - pchisq(test_stat, 3)Â  # 3 = number of new model terms in model2 (i.e. 3(?) levels of education)\n## [1] 0.05070196\n\n\nExample: LR test with anova\nanova(fm2, fm1)\n\nLikelihood ratio tests of cumulative link models:\nÂ  Â  formula:Â  Â  Â  Â  Â  Â  Â  Â  link:Â  threshold:\nfm2 rating ~ tempÂ  Â  Â  Â  Â  logitÂ  flexible\nfm1 rating ~ temp + contact logitÂ  flexible\nÂ  Â  no.par AICÂ  Â  logLikÂ  LR.stat df Pr(&gt;Chisq)\nfm2 5Â  Â  Â  194.03 -92.013\nfm1 6Â  Â  Â  184.98 -86.492Â  11.043Â  1Â  0.0008902 ***"
  },
  {
    "objectID": "qmd/diagnostics-glm.html#sec-diag-glm-resid",
    "href": "qmd/diagnostics-glm.html#sec-diag-glm-resid",
    "title": "GLM",
    "section": "Residuals",
    "text": "Residuals\n\nNotes from Deviance Residuals video\nThe Deviance Residuals should have a Median near zero, and be roughly symmetric around zero.\n\nIf the median is close to zero, the model is not biased in one direction (the outcome is not over- nor under-estimated).\n\nDeviance residuals are like the values from computing the residual deviance at each data point\n\n\nTop line: â€œ3.3â€ is the likelihood for a data point in the saturated model and â€œ1.8â€ is the likelihood for that same data point in the proposed model\nTherefore, squaring each residual and summing them would give you the Residual Deviance for the model."
  },
  {
    "objectID": "qmd/diagnostics-mixed-effects.html#sec-diag-me-misc",
    "href": "qmd/diagnostics-mixed-effects.html#sec-diag-me-misc",
    "title": "Mixed Effects",
    "section": "Misc",
    "text": "Misc\n\nTest all optimizers\n\nTo assess whether convergence warnings render the results invalid, or to the contrary, the results can be deemed valid in spite of the warnings, Bates et al.Â (2023) suggest refitting models affected by convergence warnings with a variety of optimizers. (article)\n\nExample: {lme4}\nlibrary(lme4)\nlibrary(dfoptim)\nlibrary(optimx)\n\nfit &lt;- lmer(fatigue ~ spin * reg + (1|ID),\n           data = testdata, REML = TRUE)\n\n# Refit model using all available algorithms\nmulti_fit &lt;- allFit(fit)\n#&gt; bobyqa : [OK]\n#&gt; Nelder_Mead : [OK]\n#&gt; nlminbwrap : [OK]\n#&gt; nmkbw : [OK]\n#&gt; optimx.L-BFGS-B : [OK]\n#&gt; nloptwrap.NLOPT_LN_NELDERMEAD : [OK]\n#&gt; nloptwrap.NLOPT_LN_BOBYQA : [OK]\nsummary(multi_fit)$fixef\n#&gt;                               (Intercept)      spin       reg  spin:reg\n#&gt; bobyqa                          -2.975678 0.5926561 0.1437204 0.1834016\n#&gt; Nelder_Mead                     -2.975675 0.5926559 0.1437202 0.1834016\n#&gt; nlminbwrap                      -2.975677 0.5926560 0.1437203 0.1834016\n#&gt; nmkbw                           -2.975678 0.5926561 0.1437204 0.1834016\n#&gt; optimx.L-BFGS-B                 -2.975680 0.5926562 0.1437205 0.1834016\n#&gt; nloptwrap.NLOPT_LN_NELDERMEAD   -2.975666 0.5926552 0.1437196 0.1834017\n#&gt; nloptwrap.NLOPT_LN_BOBYQA       -2.975678 0.5926561 0.1437204 0.1834016\n\nArticle also has a custom plotting function to visually compare the results"
  },
  {
    "objectID": "qmd/diagnostics-mixed-effects.html#sec-diag-me-resid",
    "href": "qmd/diagnostics-mixed-effects.html#sec-diag-me-resid",
    "title": "Mixed Effects",
    "section": "Residuals",
    "text": "Residuals\n\nMisc\n\n{DHARMa} - Built for Mixed Effects Models for count distributions but also handles lm, glm (poisson) and MASS::glm.nb (neg.bin)\n\nBinned Residuals\n\nIt is not useful to plot the raw residuals, so examine binned residual plots\nMisc\n\n{arm} will mask some {tidyverse} functions, so donâ€™t load whole package\n\nLook for :\n\nPatterns\nNonlinear trend may be indication that squared term or log transformation of predictor variable required\nIf bins have average residuals with large magnitude\nLook at averages of other predictor variables across bins\nInteraction may be required if large magnitude residuals correspond to certain combinations of predictor variables\n\nProcess\n\nExtract raw residuals\n\nInclude type.residuals = \"response\" in the broom::augment function to get the raw residuals\n\nOrder observations either by the values of the predicted probabilities (or by numeric predictor variable)\nUse the ordered data to create g bins of approximately equal size.\n\nDefault value: g = sqrt(n)\n\nCalculate average residual value in each bin\nPlot average residuals vs.Â average predicted probability (or average predictor value)\n\nExample: vs Predicted Values\n\narm::binnedplot(x = risk_m_aug$.fitted, y = risk_m_aug$.resid,\nÂ  Â  Â  Â  Â  Â  Â  Â  xlab = \"Predicted Probabilities\",\nÂ  Â  Â  Â  Â  Â  Â  Â  main = \"Binned Residual vs. Predicted Values\",\nÂ  Â  Â  Â  Â  Â  Â  Â  col.int = FALSE)\nExample: vs Predictor\n\narm::binnedplot(x = risk_m_aug$ageCent,\nÂ  Â  Â  Â  Â  Â  Â  Â  y = risk_m_aug$.resid,\nÂ  Â  Â  Â  Â  Â  Â  Â  col.int = FALSE,\nÂ  Â  Â  Â  Â  Â  Â  Â  xlab = \"Age (Mean-Centered)\",\nÂ  Â  Â  Â  Â  Â  Â  Â  main = \"Binned Residual vs. Age\")\n\nCheck that residuals have mean zero: mean(resid(mod))\nCheck that residuals for each level of categorical have mean zero\nrisk_m_aug %&gt;%\nÂ  group_by(currentSmoker) %&gt;%\nÂ  summarise(mean_resid = mean(.resid))\nCheck for normality.\n# Normal Q-Q plot\nqqnorm(resid(mod))\nqqline(resid(mod))\nCheck normality per categorical variable level\n\nExample: 3 levels\n## by level\npar(mfrow=c(1,3))\n\nqqnorm(resid(mod)[1:6])\nqqline(resid(mod)[1:6])\n\nqqnorm(resid(mod)[7:12])\nqqline(resid(mod)[7:12])\n\nqqnorm(resid(mod)[13:18])\nqqline(resid(mod)[13:18])\n\nData should be sorted by random variable level before modeling. Otherwise you could column bind the residuals to the original data. Then, group by random variable and make q-q plots for each group"
  },
  {
    "objectID": "qmd/diagnostics-model-agnostic.html#sec-diag-modagn-misc",
    "href": "qmd/diagnostics-model-agnostic.html#sec-diag-modagn-misc",
    "title": "Model Agnostic",
    "section": "Misc",
    "text": "Misc\n\nOther packages\n\n{pdp} - has ggplot option\n\nIndividual prediction interpretation uses:\n\nLook at extreme prediction values and see what predictor variable values are driving those predictions\nExamine distribution of prediction values (on observed or new data)\n\nMulti-Modal? Which variables are driving the different modeâ€™s predicitions\nBreak predictions down by cat variable. If differences between levels are apparent, which predictor variable values are driving those differences in predictions\n\nML model predicts customer in observed data has high probability of conversion yet customer hasnâ€™t converted. Develop strategy around predictor variables (increase or decrease, do or stop doing something) that contributed to that prediction to hopefully nudge that customer into converting"
  },
  {
    "objectID": "qmd/diagnostics-model-agnostic.html#sec-diag-modagn-dalex",
    "href": "qmd/diagnostics-model-agnostic.html#sec-diag-modagn-dalex",
    "title": "Model Agnostic",
    "section": "DALEX",
    "text": "DALEX\n\nMisc\n\nNotes from Explanatory Model Analysis\n\nBook shows code snippets for R and Python\n\nAlso available for Python models\nModels from packages handled by {DALEXtra}: scikit-learn, keras, H2O, tidymodels, xgboost, mlr or mlr3\nThe first step is always going to be creating an â€œexplainâ€ object\ntitanic_glm_model &lt;- \n  glm(survived~., \n      data = titanic_imputed, \n      family = \"binomial\")\nexplainer_glm_titanic &lt;- \n  explain(titanic_glm_model, \n          data = titanic_imputed[,-8],\n          y = titanic_imputed$survived)\n\nCustom explainers can be constructed for models not supported by the package. You need to specify directly the model-object, the data frame used for fitting the model, the function that should be used to compute predictions, and the model label.\n{DALEXtra} has helper functions set-up to support {{sklearn}}, {{keras}}, {h2o}, {mlr3}, {tidymodels}, and {xgboost} models.\n\n\n\n\nExplanatory Levels\n\n\nInstance Level\n\nThe model exploration for an individual instance starts with a single number â€” a prediction. This is the top level of the pyramid.\nTo this prediction we want to assign particular variables, to understand which are important and how strongly they influence this particular prediction. One can use methods as SHAP, LIME, Break Down, Break Down with interactions. This is the second from the top level of the pyramid.\nMoving down, the next level is related to the sensitivity of the model to change of one or more variablesâ€™ values. Ceteris Paribus profiles allow to explore the conditional behaviour of the model.\nGoing further, we can investigate how good is the local fit of the model. It may happen, that the model is very good on average, but for the selected observation the local fit is very low, errors/residuals are larger than on average. The above pyramid can be further extended, i.e.Â by adding interactions of variable pairs.\n\nDataset Level\n\nThe exploration for the whole model starts with an assessment of the quality of the model, either with F1, MSE, AUC or LIFT/ROC curves. Such information tells us how good the model is in general.\nThe next level helps to understand which variables are important and which ones make the model work or not. A common technique is permutation importance of variables.\nMoving down, methods on the next level help us to understand what the response profile of the model looks like as a function of certain variables. Here you can use such techniques as Partial Dependence Profiles or Accumulated Local Dependence.\nGoing further we have more and more detailed analysis related to the diagnosis of the errors/residuals.\n\n\n\n\nInstance Level\n\nUse Case:\n\nInvestigate extreme response values\nInvestigate observations not predicted well by your model\n\n\n\nBreak-Down (BD)\n\nAllows you to see how predictor variables contribute to prediction.\nBD Charts shows additive contributions of variables when they are sequentially added to the model\nBreak-down (BD) plots and Shapley values are most suitable for models with a small or moderate number of explanatory variables. Neither of those approaches is well-suited for models with a very large number of explanatory variables, because they usually determine non-zero attributions for all variables in the model.\n\nFor data with many predictors, use LIME.\n\nInterpretation\n\n\nModel predicts survival probability of a person on the Titanic\nintercept is the mean of all the predictions from of the model on the training data\nFixing age = 8 adds 0.27 probability points to the mean overall prediction\nFixing age = 8 and class = 1st adds \\((0.27 + 0.086 = 0.356)\\) to the mean overall prediction\nprediction is the point prediction for an observation that has all of these predictors fixed at these values\n\nWhen interactions, whether explicit like in linear regression models or implicit like in Random Forest models, are present in the model, the order that the predictors are presented to the DALEX function affects the estimated contribution.\n\n\nWhen interactions arenâ€™t present in the model, then contributions should be the same regardless of ordering of the predictors.\nHere class and age have been presented to the DALEX function is different orders and have different contributions which shows that the RF model is using an interaction of the two predictors.\n\nExample: Assume No Interactions\nlibrary(\"randomForest\")\nlibrary(\"DALEX\")\nexplain_rf &lt;- \n  DALEX::explain(model = titanic_rf,  \n                 data = titanic_imputed[, -9],\n                 y = titanic_imputed$survived == \"yes\", \n                 label = \"Random Forest\")\nbd_rf &lt;- \n  predict_parts(explainer = explain_rf,\n                new_observation = henry,\n                type = \"break_down\")\nbd_rf\n##                                     contribution\n## Random Forest: intercept                   0.235\n## Random Forest: class = 1st                 0.185\n## Random Forest: gender = male              -0.124\n## Random Forest: embarked = Cherbourg        0.105\n## Random Forest: age = 47                   -0.092\n## Random Forest: fare = 25                  -0.030\n## Random Forest: sibsp = 0                  -0.032\n## Random Forest: parch = 0                  -0.001\n## Random Forest: prediction                  0.246\n\nplot(bd_rf) will plot the BD chart\ntype - The method for calculation of variable attribution; the possible methods are â€œbreak_downâ€ (the default), â€œshapâ€, â€œoscillationsâ€, and â€œbreak_down_interactionsâ€\norder - A vector of characters (column names) or integers (column indexes) that specify the order of explanatory variables to be used for computing the variable-importance measures; if not specified (default), then a one-step heuristic is used to determine the order\nkeep_distributions - A logical value (FALSE by default); if TRUE, then additional diagnostic information about conditional distributions of predictions is stored in the resulting object and can be plotted with the generic plot() function.\n\nWhen this is TRUE, plot will output the BD plot, but instead of point estimates of the contributions for each predictor, violin plots visualize the distribution of contribution values.\nThere are also lines that connect predictions from one distribution to the next distribution which indicates how each model prediction changed wthen the next predictor was added.\nNot very aesthetically pleasing. If you want to use these distributions in a presentation and arenâ€™t concerned with the lines, Iâ€™d recommend getting the values and making a raincloud plot with {{ggdist}} or some other package.\n\n\nExample: Assume Interactions\n\n##                                             contribution\n## Random Forest: intercept                           0.235\n## Random Forest: class = 1st                         0.185\n## Random Forest: gender = male                      -0.124\n## Random Forest: embarked:fare = Cherbourg:25        0.107\n## Random Forest: age = 47                           -0.125\n## Random Forest: sibsp = 0                          -0.032\n## Random Forest: parch = 0                          -0.001\n## Random Forest: prediction                          0.246\n\nSame code as before except in predict_parts, type = â€œbreak_down_interactionsâ€\n\nFinds an ordering of the predictors in which the most important predictors are placed at the beginning\n\niBD detects an important interaction between fare and class\nCan be time consuming as the running time is quadratic depending on the number of predictors, \\(O(p^2)\\). \\(\\frac{p(p+1)}{2}\\) net contributions for single variables and pairs of variables have to be calculated.\nFor datasets with a small number of observations, the calculations of the net contributions will be subject to a larger variability and, therefore, larger randomness in the ranking of the contributions.\n\n\n\n\nLime\n\nArtificial data is generated around the neighborhood of instance of interest. Predictions are generated by the black-box model. A glass-box model (e.g.Â lasso, decision-tree, etc.) is trained on the artificial data and predictions. Then, the glass-box model is used as a proxy model for the black-box model for interpreting that instance.\nWidely adopted in the text and image analysis but also suitable for tabular data with many predictors\n\nFor models with large numbers of variables, sparse explanations with a small number of variables offer a useful alternative to BD and SHAP.\n\nIssues\n\nRequires numeric predictors, so different LIME methods will have different transformations of the categorical variables and will provide different results.\nThe glass-box model is selected to approximate the black-box model, and not the data themselves, the method does not control the quality of the local fit of the glass-box model to the data. Thus, the latter model may be misleading.\nSometimes even slight changes in the neighborhood where the artificial data is generated can strongly affect the obtained explanations.\n\nSee Ch. 9.6 for discussion of some of the differences in LIME implementation between {lime}, {localModel}, and {iml}.\nExample: {localModel}\nlibrary(\"localModel\")\nlocMod_johnny &lt;- \n  DALEXtra::predict_surrogate(explainer = explain_rf, \n                              new_observation = johnny_d, \n                              size = 1000, \n                              seed = 1,\n                              type = \"localModel\")\nlocMod_johnny[,1:3]\n##     estimated                        variable original_variable\n## 1  0.23530947                    (Model mean)                  \n## 2  0.30331646                     (Intercept)                  \n## 3  0.06004988                   gender = male            gender\n## 4 -0.05222505                    age &lt;= 15.36               age\n## 5  0.20988506     class = 1st, 2nd, deck crew             class\n## 6  0.00000000 embarked = Belfast, Southampton          embarked\n\ntype = â€œlocalModelâ€ says which package to use\n\n{lime} and {iml} available\n\nestimated shows the coefficients of a LASSO glass-box model\nNote the dichotomization of the continuous variable, age\n\nCutpoint chosen using ceteris-paribus profiles. The point at which the largest change in the response is chosens as the cutpoint.\n\nCP Profile: localModel::plot_interpretable_feature(locMod_johnny, \"age\")\n\n\n{iml} doesnâ€™t transform continuous variables and {lime} uses quartile cutpoints\n\nVisualizaton: plot(locMod_johnny)\n\n\n\n\n\nCeteris Paribus (CP)\n\nA CP profile shows how a modelâ€™s prediction would change if the value of a single exploratory variable changed\n\nA pdp is the average of cp profiles for all observations.\n\nIf calculated for multiple observations, CP profiles are a useful tool for sensitivity analysis.\nThe larger influence of an explanatory variable on prediction for a particular instance, the larger the fluctuations of the corresponding CP profile. For a variable that exercises little or no influence on a modelâ€™s prediction, the profile will be flat or will barely change.\nFor categoricals or discrete variables, bar graphs are used to visualize the CP values.\nIssues\n\nCorrelated explanatory variables may lead to misleading results , as it is not possible to keep one variable fixed while varying the other one.\nIf an interaction is present, whether explicit like in linear regression models or implicit like in Random Forest models, results can be misleading.\n\nPairwise interactions require the use of two-dimensional CP profiles that are more complex than one-dimensional ones.\n\n\nProcess\n\nSupply a single observation with values of your choice and a predictor.\nThat predictor varies while the others in the model are held constant, and predictions are calculated.\nResponses are plotted vs the predictor.\n\nExample: Compare 2 observations\n\nvariable_splits = list(age = seq(0, 70, 0.1), \n                       fare = seq(0, 100, 0.1))                      )\ncp_titanic_rf2 &lt;- \n  predict_profile(explainer = explain_rf, \n                  new_observation = rbind(henry, johnny_d),\n                  variable_splits = variable_splits)\nlibrary(ingredients)\nplot(cp_titanic_rf2, \n     color = \"_ids_\", \n     variables = c(\"age\", \"fare\")) + \n  scale_color_manual(name = \"Passenger:\", \n                     breaks = 1:2, \n                     values = c(\"#4378bf\", \"#8bdcbe\"), \n                     labels = c(\"henry\" , \"johny_d\")) \n\npredict_profile\n\nvariables - (Not used in this example) Names of explanatory variables, for which CP profiles are to be calculated. By default, variables = NULL and the profiles are constructed for all variables, which may be time consuming.\nvariable_splits is an optional argument for providing a custom range of predictor values.\n\nBy default the function uses the range of values in the training data which is obtained throught the explainer object.\nAlso, limits the computations to the variables specified in the data.frame\n\n\nplot\n\ncolor = â€œ_ids_â€ specifies that more than one observation is being compared\n\n\nExample: Compare Multiple Models\n\nplot(cp_titanic_rf, cp_titanic_lmr, color = \"_label_\",  \n     variables = c(\"age\", \"fare\")) +\n     ggtitle(\"Ceteris-paribus profiles for Henry\", \"\") \n\nage has a non-linear shape because a spline transform was used\n\nProfile Oscillations\n\nMethod to assign importance to individual CP Profiles. Useful when there are a large number of variables used in your model.\n\nFor models with hundreds of variables or discrete variables like zip codes, visual examination can be daunting.\n\nThe method estimates the area under the CP curve by summing the differences between the CP values and the prediction at that instance.\n\nExample: Basic\n\noscillations_uniform &lt;- \n  predict_parts(explainer = explain_rf, \n                new_observation = henry, \n                type = \"oscillations_uni\")\n\noscillations_uniform\n##    _vname_ _ids_ oscillations\n## 2   gender     1   0.33700000\n## 4    sibsp     1   0.16859406\n## 3      age     1   0.16744554\n## 1    class     1   0.14257143\n## 6     fare     1   0.09942574\n## 7 embarked     1   0.02400000\n## 5    parch     1   0.01031683\n\noscillations_uniform$`_ids_` &lt;- \"Henry\"\nplot(oscillations_uniform) +\n    ggtitle(\"Ceteris-paribus Oscillations\", \n            \"Expectation over uniform distribution (unique values)\")\n\nGender is a CP Profile that you should definitely look at. Followed by sibsp, age, and probably class\nvariable_splits is optional. Available if you want to use a custom grid of variables and values.\ntype\n\ntype = â€œoscillationsâ€ is used when variable_splits is used. An average residual is whatâ€™s being calculated, and one main difference in the types is the divisor (e.g.Â number of unique values, sample size) thatâ€™s used. Here, Iâ€™m guessing this argument value sets the divisor to the number of grid values in variable_splits.\ntype = â€œoscillations_uniâ€ assumes a uniform distribution of the â€œresidualsâ€ which are differences between the CP value and prediction for that instance.\n\nFilters variables to only unique values\nQuicker to compute\n\ntype = â€œoscillations_empâ€ assumes an empirical distribution of the â€œresidualsâ€\n\nPreferred when there are enough data to accurately estimate the empirical distribution and when the distribution is not uniform.\n\n\n\n\n\n\nLocal Diagnostics\n\nCheck how the model behaves locally for observations similar to the instance of interest.\nNeighbors of the instance of interest are chosen using a distance metric that includes all variables â€” default being the gower distance which handles categorical and continuous variables.\nLocal Fidelity\n\nCompares the distribution of residuals for the neighbors with the distribution of residuals for the entire training dataset.\nA histogram with both sets of residuals is created.\n\nBoth distributions should be similar. But, if the residuals for the neighbors are shifted towards positive or negative values, then the model has issues predicting instances with variable values in this space.\n\nNon-parametric tests such as Wilcoxon or Kolmogorov-Smirnov test can also be used to test for differences.\n\nLocal Stability\n\nChecks whether small changes in the explanatory variables, as represented by the changes within the set of neighbors, have got much influence on the predictions.\nCP profiles are calculated for each of the selected neighbors\n\nAdding residuals to the plot allows for evaluation of the local model-fit.\n\nInterpretation\n\nParallel lines suggests the relationship between the variable and the response is additive.\nA small distance between the lines suggests model predictionss are stable for values of that variable around the supplied value.\n\n\nExample\n\nLocal-Fidelity\n\nid_rf &lt;- \n  predict_diagnostics(explainer = explain_rf,\n                      new_observation = henry,\n                      neighbours = 100)\nid_rf\n##  Two-sample Kolmogorov-Smirnov test\n## \n## data:  residuals_other and residuals_sel\n## D = 0.47767, p-value = 4.132e-10\n## alternative hypothesis: two-sided\nplot(id_rf)\n\nKS-Test indicates a statistically significant difference between the two distributions\nThe plot suggests that the distribution of the residuals for Henryâ€™s neighbours might be slightly shifted towards positive values, as compared to the overall distribution.\nI donâ€™t think the y-axis values are informative. This is a split axis histogram where neighbor residuals are on top and the rest of the observations are on the bottom.\nThere is a nbinsargument which has a default of twenty. This chart many need a few more bins\n\nLocal-Stability\n\nid_rf_age &lt;- \n  predict_diagnostics(explainer = explain_rf,\n                      new_observation = henry,\n                      neighbours = 10,\n                      variables = \"age\")\nplot(id_rf_age)\n\nThe vertical segments correspond to residuals\n\nShorter the segment, the smaller the residual and the more accurate prediction of the model.\nGreen segments correspond to positive residuals, red segments to negative residuals.\n\nThe profiles are relatively close to each other, suggesting the stability of predictions.\nThere are more negative than positive residuals, which may be seen as a signal of a (local) positive bias of the predictions.\n\n\n\n\n\n\nDataset Level\n\nFitted vs Observed\n\ndiag_ranger &lt;- model_diagnostics(explainer_ranger)\nplot(diag_ranger, variable = \"y\", yvariable = \"y_hat\") +\n  geom_abline(colour = \"red\", intercept = 0, slope = 1)\n\nRed line is the perfect fitting model\nShows that, for large observed values of the dependent variable, the predictions are smaller than the observed values, with an opposite trend for the small observed values of the dependent variable. (Also see Residuals vs Observed)\n\n\n\nResidual Plots\n\nMisc\n\nFor ML models, can help in detecting groups of observations for which a modelâ€™s predictions are biased.\nFrom Chapter 19\n\nHistogram Plot\n\nmr_lm &lt;- model_performance(explain_apart_lm)\nmr_rf &lt;- model_performance(explain_apart_rf)\nplot(mr_lm, mr_rf, geom = \"histogram\") \n\nThe split into two separate, normal-like parts, which may suggest omission of a binary explanatory variable in the model.\n\nBoxplot\n\nplot(mr_lm, mr_rf, geom = \"boxplot\")\n\nRed dot represents RMSE (i.e.Â mean of the residuals)\nThe residuals for the random forest model are more frequently smaller than the residuals for the linear-regression model. However, a small fraction of the random forest-model residuals is very large, and it is due to them that the RMSE is comparable for the two models.\nThe separation of the red dot (mean) and line (median) indicate that the residual distribution is skewed to the right.\n\nResiduals vs Observation ID\n\nmd_rf &lt;- model_diagnostics(explainer_ranger)\nplot(md_rf, variable = \"ids\", yvariable = \"residuals\")\n\nShows an asymmetric distribution of residuals around zero, as there is an excess of large positive (larger than 500) residuals without a corresponding fraction of negative values (i.e.Â right-skewed distribution)\n\nResiduals vs Fitted - plot(md_rf, variable = \"y_hat\", yvariable = \"residuals\")\n\n\nShould be symmetric around the horizontal at zero\nSuggests that the predictions are shifted (biased) towards the average.\n\nResiduals vs Observed - plot(md_rf, variable = \"y\", yvariable = \"residuals\")\n\n\nShould be symmetric around the horizontal at zero\nShows that, for the large observed values of the dependent variable, the residuals are positive, while for small values they are negative. This trend is clearly captured by the smoothed curve included in the graph. Thus, the plot suggests that the predictions are shifted (biased) towards the average. (See Fitted vs Observed)\n\nAbs Residuals vs Fitted - plot(md_rf, variable = \"y_hat\", yvariable = \"abs_residuals\")\n\n\nVariation of the Scale-Location plot\nShould be symmetric scatter around a horizontal line to indicate homoskedastic variance of the residuals\nLarge concern for linear regression models, but potentially not a concern for tree models like RF\n\nFor tree models, you have to decide whether the bias is acceptable\n\n\n\n\n\nFeature Importance\n\nFrom Chapter 16\nModel-agnostic method used allows comparing an explanatory-variableâ€™s importance between models with different structures to check for agreement.\n\nIf variables are ranked in importance differently in different models, then compare in pdp, factor, or ale plots. See if one of the models that has the variable ranked higher captures a different (e.g.Â non-linear) relationship with response better than the other models\n\nThe permutation step means there some randomness involved, so it should be repeated many times. This will give you a distribution of importance values for each variable. Then, you can calculate a an interval based on a chosen quantile to represent the uncertainty.\nOverview\n\nTakes a variable, randomizes its rows, measures change in loss function from full model.\nVars that have largest changes are of greater importance (longer bars).\n\nSome of the Args\n\nN = 1000 (default), = NULL to use whole dataset (slower).\nB = 10 (default) - Number of iterations (i.e.Â number of times you go through the permutation process for each variable)\nType = â€œrawâ€ (default) is just the value of loss function. You can use â€œdifferenceâ€ or â€œratioâ€ (shown in Steps section), but the ranking isnâ€™t affected\n\nSteps for any given loss function\n\nCompute the value for the loss function for original model\nFor variable i in {1,â€¦,p}\n\nPermute values of explanatory variable\nApply given ML model\nCalculate value of loss function\nCompute feature importance (permuted loss / original loss)\n\nSort variables by descending feature importance\n\nExample\n\nvars &lt;- c(\"surface\",\"floor\",\"construction.year\",\"no.rooms\",\"district\")\nmodel_parts(explainer = explainer_rf, \n        loss_function = loss_root_mean_square,\n                    B = 50,\n            variables = vars)\nlibrary(\"ggplot2\")\nplot(vip.50) +\n  ggtitle(\"Mean variable-importance over 50 permutations\", \"\") \n\nBox-plot represents the distribution of importance values for that variable\n\n\n\n\nPartial Dependence Profiles (PDPs)\n\nMisc\n\nFrom Chapter 17\nThe PDP curve represents the average prediction across all observations while holding a predictor, x, at a constant value\n\nKeep a new data pt constant and calculate a prediction for each observed value of the other predictors then take the average of the predictions\nPartial Dependence profiles are averages of Ceteris-Paribus profiles\n\nWhen comparing PDPs between Regression and Tree models, expect to see flatter areas for tree models as they tend to shrink predictions towards the average and they are not very good for extrapolation outside the range of values observed in the training dataset.\nFor additive models, CP profiles are parallel. For models with interactions, CP profiles may not be parallel.\n\nUse Cases\n\nAgreement between profiles for different models is reassuring. Some models are more flexible than others. If PD profiles for models, which differ with respect to flexibility, are similar, we can treat it as a piece of evidence that the more flexible model is not overfitting and that the models capture the same relationship.\nDisagreement between profiles may suggest a way to improve a model. If a PD profile of a simpler, more interpretable model disagrees with a profile of a flexible model, this may suggest a variable transformation that can be used to improve the interpretable model. For example, if a random forest model indicates a non-linear relationship between the dependent variable and an explanatory variable, then a suitable transformation of the explanatory variable may improve the fit or performance of a linear-regression model.\n\nExample:\n\npdp_lmr &lt;- \n  model_profile(explainer = explainer_lmr, \n                         variables = \"age\")\npdp_rf &lt;- \n  model_profile(explainer = explainer_rf, \n                        variables = \"age\")\nplot(pdp_rf, pdp_lmr) +\n    ggtitle(\"Partial-dependence profiles for age for two models\") \n\nLeft: Indicates that the linear regression isnâ€™t capturing u-shape relationship\nRight: Indicates that the RF may be underestimating the effect\n\n\nEvaluation of model performance at boundaries. Models are known to have different behaviour at the boundaries of the possible range of a dependent variable, i.e., for the largest or the lowest values. For instance, random forest models are known to shrink predictions towards the average, whereas support-vector machines are known for a larger variance at edges. Comparison of PD profiles may help to understand the differences in modelsâ€™ behaviour at boundaries.\n\nChecks\n\nHighly Correlated Variables\n\nPD profiles inherit the limitations of the CP profiles. In particular, as CP profiles are problematic for correlated explanatory variables may offer a crude and potentially misleading summarization\nIf highly correlated, use Accumulated Local Profies\n\nCheck CP profile for consistent behavior\n\nPD profiles are estimated by the mean of the CP profiles for all instances (observations) from a dataset (i.e.Â CP profiles are instance-level PD profiles).\nThe mean (i.e.Â PD) may not be representative of the relationship of the variable and the response\nIf the CP prole lines are parallel, then the PD profile is representative\nSolutions (If not parallel):\n\nCluster CP profiles - Cluster the CP profiles (e.g.Â k-means) and the entroids will be the PD lines\n\nExample\n\npdp_rf_clust &lt;- \n  model_profile(explainer = explainer_rf, \n                variables = \"age\", \n                k = 3)\nplot(pdp_rf_clust, \n     geom = \"profiles\") + \n    ggtitle(\"Clustered partial-dependence profiles for age\") \n\nGrouped-by CP profiles - Group CP profiles by a moderator variable\n\nDistinctive PD lines can indicate an interaction\nExample\n\npdp_rf_gender &lt;- \n  model_profile(explainer = explainer_rf, \n                variables = \"age\", \n                groups = \"gender\")\nplot(pdp_rf_gender, \n     geom = \"profiles\") + \n    ggtitle(\"Partial-dependence profiles for age, grouped by gender\") \n\nGender looks like a good candidate for an interaction\n\n\n\nExample: RF survival model\n\n\nThe shape of the PD profile does not capture, for instance, the shape of the group of five CP profiles shown at the top of the panel.\nIt does seem to reflect the fact that the majority of CP profiles (predicted probabilities below 0.75) suggest a substantial drop in the predicted probability of survival for the ages between 2 and 18.\n\n\n\nSteps\n\nDetermine grid space of j evenly spaced values across distribution of selected predictor, x\nFor value i in {1,â€¦,j} of grid space\n\nset x == i for all n observations (x is a constant variable)\napply given ML model\nestimate n predicted values\ncalculate the average predicted value\n\ngraph y_hat vs x\n\nCode\npdp_rf &lt;- model_profile(explainer = explainer_rf, variables = \"age\")\nlibrary(\"ggplot2\")\nplot(pdp_rf) +  ggtitle(\"Partial-dependence profile for age\") \n\nOther Args\n\nN - The number of (randomly sampled) observations that are to be used for the calculation of the PD profiles (N = 100 by default); N = NULL implies the use of the entire dataset included in the explainer-object.\nvariable_type - A character string indicating whether calculations should be performed only for â€œnumericalâ€ (continuous) explanatory variables (default) or only for â€œcategoricalâ€ variables.\ngroups - The name of the explanatory variable that will be used to group profiles, with groups = NULL by default (in which case no grouping of profiles is applied).\nk - The number of clusters to be created with the help of the hclust() function, with k = NULL used by default and implying no clustering.\ngeom = â€œprofilesâ€ in the plot function, we add the CP profiles to the plot of the PD profile.\n\n\n\n\n\nFunnel Plot\n\nFrom {DALEXtra}, lets us find subsets of data where one of models is significantly better than other ones.\nVery useful in situations where we have models that have similiar overall performance, but where predictive performance of units with certain characteristics are more important to our business than others.\nCan be used to create a sort of ensemble model where predictions from different models are used depending on the values of the predictors.\nExample\n\nexplainer_lm &lt;- \n  explain_mlr(model_lm, \n              apartmentsTest, \n              apartmentsTest$m2.price, \n              label = \"LM\", \n              verbose = FALSE, \n              precalculate = FALSE)\nexplainer_rf &lt;- \n  explain_mlr(model_rf, \n              apartmentsTest, \n              apartmentsTest$m2.price, \n              label = \"RF\",\n              verbose = FALSE, \n              precalculate = FALSE)\n\n plot_data &lt;- \n   funnel_measure(explainer_lm, \n                  explainer_rf,\n                  partition_data = cbind(apartmentsTest,\n                                         \"m2.per.room\" = apartmentsTest$surface/apartmentsTest$no.rooms),\n                  nbins = 5, \n                  measure_function = DALEX::loss_root_mean_square, \n                  show_info = FALSE)\n\nnbins is the number of bins to partition continuous variables into.\nmeasure_function is the metric used to judge each model. Here RMSE is used.\nLength of segment is the difference in prediction performance. The longer the segments are, the better that model is at predicting for those subsets of data.\nFor the most expensive and the cheapest (i.e.Â first and last bin of m2.price), the linear regression model performs best, while the random forest model performs best average priced (i.e.Â middle 3 bins of m2.price) apartments.\nThe linear regression is extremely better at predicting prices for apartments in the Srodmiescie district."
  },
  {
    "objectID": "qmd/diagnostics-model-agnostic.html#sec-diag-modagn-triplot",
    "href": "qmd/diagnostics-model-agnostic.html#sec-diag-modagn-triplot",
    "title": "Model Agnostic",
    "section": "triplot",
    "text": "triplot\n\nMisc\n\nGithub, Docs\nTakes into account the correlation structure when assessing variable importance; global and local explanations\nTriplot Features:\n\nThe importance of every single feature,\nHierarchical aspects importance,\nOrder of grouping features into aspects.\n\n\nGlobal Triplot\n\nmod_explainer &lt;- \n  DALEX::explain(model_obj, \n                 data = dat_without_target, \n                 y = target_var, \n                 verbose = FALSE)\ntriplot_global &lt;- \n  triplot::model_triplot(mod_explainer, \n                         B = num_permutations, \n                         N = num_rows_sampled, \n                         corr_method)\nplot(triplot_global)\n\nCurrently only correlation methods for numeric features supported\nUsing small numbers of rows for permutations (N arg) will cause unstable results\nLeft Panel\n\nThe global importance of every single feature\nPermutation feature importance used\n\nCenter Panel\n\nThe importance of groups of variables determined by the hierarchical clustering\nImportance calcâ€™d by permutation feature importance\nNumbers to left of the split point is the group importance\n\nRight Panel\n\nCorrelation structure visualized by hierarchical clustering\nGuess this is the same as the middle panel but a correlation method is used instead of feature importance\n\nInterpretation: Use the middle panel to see if adding correlated features increases group importance.\n\nAdding too many for little gain in importance may not be worth it, depending on sample size.\nMight be useful for deciding whether or not to create a combined feature\n\nUnclear on the technical details on how or what exactly is being clustered.\n\npredict_triplot (local)\n\nLike breakdown or shapley in that itâ€™s goal is to assess the feature contribution to the prediction\n\nvariables can have negative or positive contributions to the prediction value\n\nCombines the approach to explanations used by LIME methods and visual techniques introduced in a global triplot\n# slice a row of original dataset that you want an explanation of the model prediction\ntarget_row &lt;- df %&lt;% slice(1)\ntriplot_local &lt;- triplot::predict_triplot(mod_explainer, target_row, N = num_rows_sampled, corr_method)\nplot(triplot_local)\nsame interpretation as model_triplot but for explaining the prediction of a target observation\nleft panel\n\nthe contribution to the prediction of every single feature\n\nmiddle panel\n\nthe contribution of aspects, that are built in the order determined by the hierarchical clustering\n\nright panel\n\ncorrelation structure of features visualized by hierarchical clustering\n\n\npredict_aspects (local)\n\naspects are groups of variables that can be thought of as latent variables\n\ncan use the middle or right panel from predict_triplot to get ideas on how to group your variables\nthereâ€™s also a group_variables helper function that can group vars by a correlation cutoff value\n\n# Example\n# group variables\nfifa_aspects &lt;- list(\nÂ  \"age\" = \"age\",\nÂ  \"body\" = c(\"height_cm\", \"weight_kg\"),\nÂ  \"attacking\" = c(\"attacking_crossing\",\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  \"attacking_finishing\",Â \nÂ  Â  Â  Â  Â  Â  Â  Â  Â  \"attacking_heading_accuracy\",\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  \"attacking_short_passing\",Â \nÂ  Â  Â  Â  Â  Â  Â  Â  Â  \"attacking_volleys\"))\n# Compare aspect importances from different models\npa_rf &lt;- predict_aspects(rf_explainer,\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  new_observation = target_row,\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  variable_groups = fifa_aspects)\npa_gbm &lt;- predict_aspects(gbm_explainer,\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  new_observation = top_player,\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  variable_groups = fifa_aspects)\nplot(pa_rf, pa_gbm)\n\noutput is two, side-by-side aspect importance plots\n\nbut theyâ€™re more like contribution plots, where aspects can have positive or negative contributions to the predicted value\n\nCan be used to compare models\n\nexamples\n\nif one model underpredicts a target observation more than another model, how do the feature contributions to that prediction differ between the two models?\nare there different aspects more important in one model than the other?\nif theyâ€™re the same aspects at the top, does one aspect stand out in one model while in the other model the importance values are more evenly spread?"
  },
  {
    "objectID": "qmd/diagnostics-model-agnostic.html#sec-diag-modagn-auditor",
    "href": "qmd/diagnostics-model-agnostic.html#sec-diag-modagn-auditor",
    "title": "Model Agnostic",
    "section": "auditor",
    "text": "auditor\n\nFor GOF measure, model similarity comparison using residuals. Also, uses residual plots and scores to check for asymmetry (around zero) in the distribution, trends, and heteroskedacity.\nAuditor objects only require a predict function and response var in order to be created.\nUsually scores use score(auditor_obj, type = â€œ?â€, â€¦), plots use plot(auditor_obj, type = â€œ?â€, â€¦ )\nPlot function can facet multiple plots, e.g.Â plot(mod1, mod2, mod3, type = c(â€œModelRankingâ€, â€œPredictionâ€), variable = â€œObserved responseâ€, smooth = TRUE, split = â€œmodelâ€). smooth = TRUE adds a trend line. Split arg splits prediction vs observed plot into plots for each model.\nRegression Error Characteristic (REC) plot- GOF measure - type = â€œRECâ€ - The y-axis is the percentage of residuals less than a certain tolerance (i.e.Â size of the residual) with that tolerance on the x-axis. The shape of the curve illustrates the behavior of errors. The quality of the model can be evaluated and compared for different tolerance levels. The stable growth of the accuracy does not indicate any problems with the model. A small increase of accuracy near 0 and the areas where the growth is fast signalize bias of the model predictions (jagged curve).\nArea Over the REC Curve (AOC) score - GOF measure - type = â€œRECâ€ - is a biased estimate of the expected error for a regression model. Provides a measure of the overall performance of regression model. Smaller is better Iâ€™d think.\nRegression Receiver Operating Characteristic (RROC) plot - type = â€œRROCâ€ - for regression to show model asymmetry. The RROC is a plot where on the x-axis we depict total over-estimation and on the y-axis total under-estimation.\nArea Over the RROC Curve score - GOF measure - type = â€œRROCâ€ - equivalent to the error variance. Smaller is better Iâ€™d think\nModel Ranking plot and table - Multi-GOF measure - type = â€œModelRankingâ€ - radar plot of potentially five scores: MAE, MSE, RMSE, and the AOC scores for REC and RROC. Theyâ€™ve been scaled in relation to the model with the best score for that particular metric. Best model for a particular metric will be farthest away from the center of the plot. In the table, larger scaled score is better while lower is better in the plain score column. You can also add a custom score function but you need to make sure that a lower value = best model.\nResiduals Boxplot - asymmetry measure - type = â€œResidualBoxplotâ€ - Pretty much the same as a regular boxplot except thereâ€™s a red dot which stands for the value of Root Mean Square Error (RMSE). Values are the absolute value of the residuals. Best models will have medians around zero and small spreads (i.e.Â short whiskers). Example in the documentation has a good example showing how a long whisker pulls the RMSE which is sensitive to outliers towards the edge of the box.\nResidual Density Plot - asymmetry measure - type = â€œResidualDensityâ€ - detects the incorrect behavior of residuals. For linear regressions, residuals should be normally distributed around zero. For other models, non-zero centered residuals can indicate bias. Plot has a rug which makes it possible to ascertain whether there are individual observations or groups of observations with residuals significantly larger than others. Can specify a cat predictor variable and see the shape of the density of residuals w.r.t. the levels of that variable.\nTwo-sided ECDF Plot - type = â€œTwoSidedECDFâ€ - stands for Empirical Cumulative Distribution Functions. Thereâ€™s an cumulative distribution curve for each positive and negative residuals. The plot shows the distribution of residuals divided into groups with positive and negative values. It helps to identify the asymmetry of the residuals. Points represent individual error values, what makes it possible to identify â€˜outliersâ€™\nResiduals vs Fitted and autocorrelation plots - type = â€œResidualâ€, type = â€œAutocorrelationâ€ - same thing as base R plot. Any sort of grouping, trend, or pattern suggests an issue. Looking for randomness around zero. Can specify a cat predictor variable and see the trend of residuals w.r.t. the levels of that variable.\nAutocorrelation Function Plot (ACF) - autocorrelation check - type = â€œACFâ€ - same evaluation as a ACF plot for time series\nScale-Location Plot - type = â€œScaleLocationâ€ - The y-axis is the sqrt(abs(std_resids)) and x-axis is fitted values. Different from Resid vs Fitted since that plot uses raw residuals instead of scaled. Equation shows the resids are divided by their std.dev, so that should fix the variance at 1. The presence of any trend suggests that the variance depends on ï¬tted values, which is against the assumption of homoscedasticity. Not every model has an explicit assumption of homogeneous variance, however, the heteroscedasticity may indicates potential problems with the goodness-of-ï¬t. Residuals formed into separate groups suggest a problem with model structure (specification?)\nPeak Test - - type = â€œPeakâ€ - tests for heteroskedacity. Scoreâ€™s range is (0, 1]. Close to 1 means heteroskedacity present.\nHalf-Normal Plot - GOF measure - type = â€œHalfNormalâ€ - graphical method for comparing two probability distributions by plotting their quantiles against each other. Method takes info from the model and simulates response values. Your model is then fitted with the simulated response variable and residuals created. A dotted line 95% CI envelope is created from these simulated residuals. If your residuals come from the normal distribution, they are close to a straight dotted line. However, even if there is no certain assumption of a speciï¬c distribution, points still show a certain trend. Simulated envelopes help to verify the correctness of this trend. For a good-ï¬tted model, diagnostic values should lay within the envelope.\nHalf-Normal Score - GOF measureÂ - scoreHalfNormal(auditor_obj)\n\nCount the number of simulated residuals for observation_i that are greater or equal than resid_i. If the number is around 0.5 the number of simulated residuals, m, then the model residual doesnâ€™t differ that much from the simulated residuals which is a good thing.\nThat calc is repeated for each residual.\nScore = sum(abs(count_i - (m/2)))\nScoreâ€™s range = [0, (nm)/2], where n is the number of observations\nLower value indicates better model fit.\n\nModel PCA Plot - Similarity of models comparison - type = â€œModelPCAâ€ - vector arrows represent the models and the gray dots are the model residuals. The smaller the angle between the models, the closer their residual structure. Arrows perpendicular the residual dots mean that modelâ€™s residuals likely represent that structure. Parallel means not likely to represent that structure.\nModel Correlation Plot - Similarity of models comparison - type = â€œModelCorrelationâ€ - Densities in the diagonal are each models fitted response values. Correlations in upper right triangle are between models and between each model and the observed response.\nPredicted Response Plot (aka predicted vs observed) - GOF measure - type = â€œPredictionâ€ - Should randomly envelop the diagonal line. Trends can indicate values of the response where the model over/under predicts. Groupings of residuals suggest problems in model specification.\nReceiver Operating Characteristic (ROC) curve - classification GOF measure - type = â€œROCâ€ - True Positive Rate (TPR) (y-axis) vs False Positive Rate (FPR) (x-axis) on a threshold t (probability required to classify something as an event happening) where t has the range, [0,1]. Each point on the ROC curve represents values of TPR and FPR at different thresholds.Â The closer the curve is to the the left border and top border of plot, the more accurate the classiï¬er is.\nAUC scoreÂ - GOF measure - type = â€œROCâ€ - guideline is &gt; 0.80 is good.\nLIFT charts - classification GOF measure - type = â€œLIFTâ€ - Rate of Positive Prediction (RPP) plotted (y-axis) vs number of True Positives (TP) (x-axis) on a threshold t.\n where P is the total positive classifications (TP + FP) predicted for that threshold and N is the total negative (TN + FN). The ideal model is represented with a orange/yellow curve. The model closer to the ideal curve is considered the better classifier.\nCookâ€™s Distances Plot - influential observationsÂ -Â type = â€œCooksDistanceâ€ - a tool for identifying observations that may negatively affect the model. They can be also used for indicating regions of the design space where it would be good to obtain more observations. Data points indicated by Cookâ€™s distances are worth checking for validity. Cookâ€™s Distances are calculated by removing the i-th observation from the data and recalculating the model. It shows an inï¬‚uence of i-th observation on the model.Â The 3 observations (default value) with the highest values of the Cookâ€™s distance are marked with the row number of the observation. The guideline seems to be a Cookâ€™s Distance (y-axis) &gt; 0.5 warrants further investigation."
  },
  {
    "objectID": "qmd/diagnostics-model-agnostic.html#sec-diag-modagn-shap",
    "href": "qmd/diagnostics-model-agnostic.html#sec-diag-modagn-shap",
    "title": "Model Agnostic",
    "section": "SHAP",
    "text": "SHAP\n\nDescription\n\nDecomposes predictions into additive contributions of the features\nBuilds model explanations by asking the same question for every prediction and feature: â€œHow does prediction i change when feature j is removed from the model?â€\nQuantifies the magnitude and direction (positive or negative) of a featureâ€™s effect on a prediction.\nTheoretical foundation in game theory\n\nIssues\n\nCannot be used for causal inference\nHighly correlated features\n\nMay be indictators for a latent feature.\nThese correlated features will have lower shap values than they would if only 1 were in the feature space.\nSince shap values are additive, we can add the shap values of highly correlated variables to get an estimate of the importance of this potential latent feature. (seems like the sum might be an overestimation unless the variables are Very highly correlated.)\n**See triplot section for handling models with highly correlated features\n\n\nPackages\n\n{shapper} - a wrapper for the Python library\n{fastshap} - uses Monte-Carlo sampling\n{treeshap}\n\nfast\nHandles correlated features by explicitly modeling the conditional expected prediction\n\nDisadvantage of this method is that features that have no influence on the prediction can get a TreeSHAP value different from zero\n\nable to compute interaction values\n\nComputing â€œresidualsâ€ might indicate how well shapley is capturing the contributions if the features are independent (py article)\n\n\n{kernelshap}\n\npermute feature values and make predictions on those permutations. Once we have enough permutations, the Shapley values are estimated using linear regression\nSlow\nDoesnâ€™t handle feature correlation. Leads to putting too much weight on unlikely data points.\n\n\nArticles\n\nExplaining Machine Learning Models: A Non-Technical Guide to Interpreting SHAP Analyses\n\nUltimate explainer\n\nInterpretable Machine Learning (ebook)\n\nmath, compares packages\n\n\nSteps for the approximate Shapley estimation method (used in IML package below):\n\nChoose single observation of interest\nFor variables j in {1,â€¦,p}\n\nm = random sample from data set\nt = rbind(m, ob)\nf(all) = compute predictions for t\nf(!j) = compute predictions for t with feature j values randomized\ndiff = sum(f(all) - f(!j))\nphi = mean(diff)\n\nsort phi in decreasing order\n\nInteractions\n\nSHAP values for two-feature interactions\nResource\nExample: Years on hormonal contraceptives (continuous) interacts with STDs (binary)\n\nInterpretation\n\nIn cases close to 0 years, the occurence of a STD increases the predicted cancer risk.\nFor more years on contraceptives, the occurence of a STD reduces the predicted risk.\nNOT a causal model. Effects might be due to confounding (e.g.Â STDs and lower cancer risk could be correlated with more doctor visits).\n\n\nCluster by shap value\n\n\nSee ebook chapter for more details\nPlot is all the force plots ordered by similarity score\n\nI think the â€œsimilarity scoreâ€ might come from the cluster model (i.e.Â a distance measure from from a hierarchical cluster)\n\nInterpretation: group of force plots on the far right shows that the features similarily contributed to that group of predictions\n\nWaterfall plots\n\n\nAn example waterfall plot for the individual case in the Boston Housing Price dataset that corresponds to the median predicted house\nInterpretation example\n\n\nforce plot - Examines influences behind one predicted value that you input to the function.\n\n\nRed arrows represent feature effects (SHAP values) that drives the prediction value higher while blue arrows are those effects that drive the prediction value lower.\nEach arrowâ€™s size represents the magnitude of the corresponding featureâ€™s effect.\nThe â€œbase valueâ€ (see the grey print towards the upper-left of the image) marks the modelâ€™s average prediction over the training set. The â€œoutput valueâ€ is the modelâ€™s prediction.\nThe feature values for the largest effects are printed at the bottom of the plot.\n\ndecision plot - Examines influences behind one predicted value that you input to the function. Pretty much the same exact thing as the breakdown plot in DALEX.\n\nTheÂ straight vertical line marks the modelâ€™s base value. The colored line is the prediction.\nStarting at the bottom of the plot, the prediction line shows how the SHAP values (i.e., the feature effects) accumulate from the base value to arrive at the modelâ€™s predicted output at the top of the plot.\nThe feature name on the y-axis is bordered by 2 horzontal grid lines. In the graph, the way the prediction line behaves in between these horizontal grid lines of the feature visualizes the SHAP value, e.g.Â a negative slope would be equal to the blue arrow in the force plot.\nThe feature values for the largest effects are in parentheses.\nFor multi-variate classification\n\nUsing lines instead of bars (like in breakdown plots) allows it to visualize the influences behind a multi-class outcome prediction where each line is for a category level probability. But the x-axis actually represents the raw (SHAP?) score, not a probability. (I think thereâ€™s an option for x-axis as probabilities)\nThe magnitude of score shows strength of confidence in the prediction. A negative score says the model thinks that category level is not the case. For example, if the category level is â€œno diseaseâ€, then a large negative score means the model says thereâ€™s strong evidence that disease is present (score was -2.12 so I guess thatâ€™s strong).\n\nCan vary one predictor between a range or set of specific values and keep the rest of predictors constant. For classification, you could ask which predictors dominate influence for high probability predictions? Then use the plot to compare predictor behavior.\nCan compare how influences are different for an observation by multiple models."
  },
  {
    "objectID": "qmd/diagnostics-model-agnostic.html#sec-diag-modagn-iml",
    "href": "qmd/diagnostics-model-agnostic.html#sec-diag-modagn-iml",
    "title": "Model Agnostic",
    "section": "IML",
    "text": "IML\n\nUses ggplot, so layers and customizations can be added. Package also has pdp, feature importance, lime, and shapley. Note fromÂ https://www.brodrigues.co/blog/2020-03-10-exp_tidymodels/, Bruno wrapped his predict function when he created a IML Predictor object. Also see article if working with workflow and cat vars, he an issue and workaround.\npredict_wrapper2 &lt;- function(model, newdata){\nÂ  predict(object = model, new_data = newdata)\n}\n\npredictor2 &lt;- Predictor$new(\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  model = best_model_fit,\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  data = pra_test_bake_features,\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  y = target,\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  predict.fun = predict_wrapper2\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  )\nAcumulated Local Effect (ALE) - shows how the prediction changes locally, when the feature is varied\n\naka Partial Dependence Plot\nChart has a distribution rug which shows how relevant a region is for interpretation (little or no points mean that we should not over-interpret this region)\n\nIndividual Conditional Expectation (ICE)\n\nThese are curves for a chosen feature that illustrate the predicted value for each observation when we force each observation to take on the unique values of that feature.\nSteps for a selected predictor, x:\n\nDetermine grid space of j evenly spaced values across distribution of x\nFor value i in {1,â€¦,j} of grid space\n\nset x == i for all n observations (x is a constant variable)\napply given ML model\nestimate n predicted values\n\n\nThe only thing thatâ€™s different here from pdp is that the predicted values, y_hats,Â  for a particular x value arenâ€™t averaged in order to produce a smooth line across all the x values. It allows us to see the distribution of predicted values for each value of x.\n\nFor a categorical predictor: each category has boxplot and the boxplots are connected at their medians\nFor a numerical predictor: Its a multi-line graph with each line representing row in the training set.\nThe pdp line is added and highlighted with arg, method = â€œpdp+iceâ€.\n\n\nInteraction PDP\n\nVisualizes the pdp for an interaction. Shows the how the effect of a predictor on the outcome varies when conditioned on another variable.\n\nExample:\n\noutcome (binary) = probability of getting overtime\npredictor (numeric) = monthly_income\ninteraction_variable (character) = gender\npdp shows a multi-line chart with a distinct gap between male and female\n\n\nCan be used in conjunction using the H-statistic below to find a strong interaction to examine with this plot\n\nH-statistic\n\nMeasures how much of the variation of the predicted outcome depends on the interaction of the features.\nTwo approaches:\n\nSort of an overall measure of a variableâ€™s interaction strength\n\nSteps:\n\nfor variable i in {1,â€¦,m}\n\nf(x) = estimate predicted values with original model\n\nThink this is a y ~ i model only\n\npd(x) = partial dependence of variable i\npd(!x) = partial dependence of all features excluding i\n\nGuessing each non-i variable gets itâ€™s own pd(!x)\n\nupper = sum(f(x) - pd(x) - pd(!x))\nlower = variance(f(x))\nÏ = upper / lower\n\nSort variables by descending Ï (interaction strength)\n\nÏ = 0 means none of variation in the predictions is dependent on the interactions involving the predictor\nÏ = 1 means all of the variation in the predictions is dependent on the interactions involving the predictor\n\nMeasures the 2-way interaction strength of feature\n\nBreaks down the overall measure into strength measures between a variable and all the other variables\nTop variables in the first approach or variables of domain interest are usually chosen to be further examined with this method. This method will show if there are strong co-dependency relationships in the model\nsteps:\n\ni = a selected variable of interest\nfor remaining variables j in {1,â€¦,p}\n\npd(ij) = interaction partial dependence of variables i and j\npd(i) = partial dependence of variable i\npd(j) = partial dependence of variable j\nupper = sum(pd(ij) - pd(i) - pd(j))\nlower = variance(pd(ij))\nÏ = upper / lower\n\nSort interaction relationship by descending Ï (interaction strength)\n\n\n\nComputationally intensive as feature set grows\n\n30 predictors for 3 different models =Â a few minutes\n80 predictors for 3 different models = around an hour\n\n\nSurrogate model\n\nSteps:\n\nApply original model and get predictions\nChoose an interpretable â€œwhite boxâ€ model (linear model, decision tree)\nTrain the interpretable model on the original dataset with the predictions of the black box model as the outcome variable\nMeasure how well the surrogate model replicates the prediction of the black box model\nInterpret / visualize the surrogate model\n\n\nFeature Importance\n\nthe ratio of the model error after permutation to the original model error\n\nYou can specify the loss function\n\nâ€œrmseâ€ â€œmseâ€ â€œmaeâ€\n\nÂ Anything 1 or less is interpreted as not important\nHas error bars\n\nCan also used difference instead of ratio"
  },
  {
    "objectID": "qmd/diagnostics-model-agnostic.html#sec-diag-modagn-easalluv",
    "href": "qmd/diagnostics-model-agnostic.html#sec-diag-modagn-easalluv",
    "title": "Model Agnostic",
    "section": "easyalluvial",
    "text": "easyalluvial\n\npdp plots using alluvial flows to display more than 2 dimensions at a time\nhelps to get an intuitive understanding how predictions of a certain ranges can be generated by the model\n\nConstricted alluvial paths indicate specific (or maybe a small range) predictor variable values are responsible for a certain range of outcome variable values\nSpread-out alluvial paths indicate the value of the predictor probably isnâ€™t that important in determining that range of values of the outcome variable\n\nThere are built-in functions for caret and parsnip models, but there are methods that allow you to use any model\npackages\n\n{easyalluvial}\n{parcats} - converts easyalluvial charts into interactive htmlwidget\n\ntooltip also shows probability and counts\n\n\nSteps\n\nBuild model\nCalculate pdp prediction values - p = alluvial_model_response_parsnip(m, df, degree = 4, method = \"pdp\")\n\narguments\n\nm = model\ndf = data\ndegree = number of top importance variables to use\nmethod =\n\nâ€œmedianâ€ is default which sets variables that are not displayed to median mode, use with regular predictions.\nâ€œpdpâ€ uses pdp method\n\nbins = number of flows for numeric predictors (not intervals, so not sure why they used â€œbinsâ€)\nparams_bin_numeric_pred = list, Default: list(bins = 5, center = T, transform = T, scale = T)\n\nâ€œpredâ€ = predictions so these are binned predictions of the outcome variable\ntransform = apply Yeo Johnson Transformation to predictions\nThese are params to another function, and I think you can adjust the binning function there.\n\nbin_labels: labels for the bins from low to high, Default: c(â€œLLâ€, â€œMLâ€, â€œMâ€, â€œMHâ€, â€œHHâ€)\n\nHigh, High (HH)\nMedium, High (MH)\nMedium (M)\nMedium, Low (ML)\nLow, Low (LL)\n\n\nFor numerics, 5 values (bins arg) in the range of each predictor is chosen\n\nThese arenâ€™t IQR values or evenly spaced so not sure what the process is\nFor categoricals, I guess each level is used. Not sure how a cat var with many levels is treated.\n\npredictions are calculated using the method in the method arg\npredictions are transformed and binned into 5 range intervals (params_bin_numeric_pred arg)\n\nPlot p (just have to call the above function)\n\ncan also add importance and marginal histograms\n\np_grid &lt;- p %&gt;%\nÂ  add_marginal_histograms(data_input = df, plot = FALSE) %&gt;%\nÂ  add_imp_plot(p = p, data_input = df)\n\nExtreme values\n\nCheck if extreme values of the sample distribution are covered by the alluvial\n\npred_train = predict(m)Â \nplot_hist('pred', p, df,\n  Â  Â  Â  Â  pred_train = pred_train, # pred_train can also be passed to add_marginal_histograms()\n          scale = 50)\n\nDistributions\n\nlstat is the sample distribution for the outcome variable\npred_train is the model predictions of sample data\npred is the predictions by the alluvial method using top importance variables\n\nMost of the extreme values are covered by the model predictions (pred_train), but the not the alluvial method.\nIf you want an alluvial with preds for the extreme values, see What feature combinations are needed to obtain predictions in the lower and higher ranges in the docs.\nExample - mlbench::BostonHousing; lstat(outcome var) percentage of lower status of the population; rf parsnip model\n\npredictor variable labels:Â \n\ntop number: value of the variable used in predictions\nrest: fraction of the flows of that color (predictions bin range) pass through that stratum\n\ne.g.Â 49% of the medium-high lstat predictions involve medv = 5\n\n\nmethod = â€œmedianâ€, note bottom where it shows that predictions are calculated using the median/mode values of variable\nExample: mtcars; disp (outcome var)\n\nhistograms/density charts\n\nâ€œpredictionsâ€: density shows the shape of the predictions distribution and sample distribtution; areas for HH, MH, M, ML, LL\npredictors: shows sample density/histogram and location of the variable values used in the pdp predictions\n\npercent importance: variable importance for the model; â€œtotal_alluvialâ€ is the total importance of the predictor variables used in the pdp alluvial.\nWhen comparing the distribution of the predictions against the original distribution of disp we see that the range of the predictions in response to the artificial dataspace do not cover all of the range of disp. Which most likely means that all possible combinations of the 4 plotted variables in combination with moderate values for all other predictors will not give any extreme values"
  },
  {
    "objectID": "qmd/diagnostics-nlp.html#sec-diag-nlp-misc",
    "href": "qmd/diagnostics-nlp.html#sec-diag-nlp-misc",
    "title": "NLP",
    "section": "Misc",
    "text": "Misc\n\nAlso see\n\nDiagnostics, Model Agnostic &gt;&gt; DALEX &gt;&gt; Instance Level &gt;&gt; LIME\nFeature Engineering, Tokenization\nFeature Engineering, Embeddings\nEDA, Text\nNLP, General"
  },
  {
    "objectID": "qmd/diagnostics-nlp.html#sec-diag-nlp-cohmeas",
    "href": "qmd/diagnostics-nlp.html#sec-diag-nlp-cohmeas",
    "title": "NLP",
    "section": "Coherence Measures",
    "text": "Coherence Measures\n\nThere goal is to evaluate topics quality with respect to interpretability\nMisc\n\nPaper uses Twitter data to show lack of performance of these measures (paper)\n\nNormalized Pointwise Mutual Information (NPMI)\n\\[\n\\mbox{NPMI} = \\frac{\\log(p(x)p(y))}{\\log (p(x,y))} - 1\n\\]\n\nNormalized Pointwise Mutual Information in Collocation Extraction\nEstimates how likely is the co-occurrence of two words x and y than we would expect by chance\nRange: -1 to 1\nNPMI = 0 means independence between the occurrences of x and y\n\nThis makes it sound like a correlation measure\n\nIf this is a correlation type of measure, then Iâ€™m guessing you want something close to 1 for each topic. As this would imply, all the words are likely to appear together."
  },
  {
    "objectID": "qmd/diagnostics-nlp.html#sec-diag-nlp-exppred",
    "href": "qmd/diagnostics-nlp.html#sec-diag-nlp-exppred",
    "title": "NLP",
    "section": "Explore Predictions",
    "text": "Explore Predictions\n\nScenario: Feature importance from a text model indicates a word or phrase from a text predictor variable is highly predictive of the outcome variable\n\nExplore which values of the outcome variable are associated with high tfidf values of the value of the text variable\n\nExample\n\nThe token, course, for the text variable, improvements, scores high in feature importance when predicting the course satisfaction rating\nAll text variables were tokenized, ngram engineered and the values assigned tfidf scores\n\nbake(prep(text_recipe), testing(splits)) %&gt;%\nÂ  Â  select(tfidf_improvements_course) %&gt;% # tfidf naming format is tfidf_textColumn_token\nÂ  Â  bind_cols(\nÂ  Â  Â  Â  testing(splits) %&gt;% select(satisfaction_rating)\nÂ  Â  ) %&gt;%\nÂ  Â  group_by(satisfaction_rating) %&gt;%\nÂ  Â  summarize(mean_tfidf_course = mean(tfidf_improvements_course)) %&gt;%\nÂ  Â  ungroup()\n\nInterpretation\n\nCustomers that give a satisfaction rating (outcome var) of 6 use the word, â€œcourse,â€ a lot (i.e.Â higher mean tfidf score)"
  },
  {
    "objectID": "qmd/diagnostics-nlp.html#sec-diag-nlp-behtest",
    "href": "qmd/diagnostics-nlp.html#sec-diag-nlp-behtest",
    "title": "NLP",
    "section": "Behavioral Tests",
    "text": "Behavioral Tests\n\nMisc\n\nNotes from: Metrics are not enough â€” you need behavioral tests for NLP\nPackages: {{checklist}}\n\nRobustness Criteria\n\nSex/Ethnicity Bias - does your model discriminate against males/females or a specific nationality?\nEquivalent Words/Synonyms - If a candidate replaces â€œGood python knowledgeâ€ with â€œgood python 3 knowledgeâ€ how does your model react?\nSkill Grading - do your model assign a higher score for â€œvery good knowledgeâ€ vs.Â â€œgood knowledgeâ€ vs.Â â€œbasic knowledgeâ€. Are adjectives adequately understood? Candidates with â€œexceptional skillâ€ should not be rated below one with â€œbasic skillâ€.\nSentence Ordering - If we reverse the order of job experience, is the model prediction consistent?\nTypos - Iâ€™ve seen a lot of models where a typo in a completely unimportant word changed the model prediction completely. We may argue that job applications should not contain typos, but we can all agree that, in general, this is an issue in NLP\nNegations - I know that is difficult. But if your task requires understanding them, do you measure it? (for example, â€œI have no criminal recordsâ€ vs.Â â€œI have criminal recordsâ€ or â€œI finished vs.Â I did not finishâ€. How about double negations?"
  },
  {
    "objectID": "qmd/diagnostics-probabilistic.html#sec-diag-prob-misc",
    "href": "qmd/diagnostics-probabilistic.html#sec-diag-prob-misc",
    "title": "Probabilistic",
    "section": "Misc",
    "text": "Misc\n\nAIC vs BIC (paper)\n\n\nAIC\n\nPenalizes parameters by 2 points per parameter\nIdeal AIC scenario\n\nNumerous hypotheses are considered\nYou have a conviction that all of them are to differing degrees wrong\n\n\nBIC\n\nPenalizes parameters by ln(sample size) points per parameter and ln(20) = 2.996\nAlmost always a stronger penalty in practice\nIdeal BIC scenario\n\nOnly a few potential hypotheses are considered\nOne of the hypotheses is (essentially) correct"
  },
  {
    "objectID": "qmd/diagnostics-probabilistic.html#sec-diag-prob-scor",
    "href": "qmd/diagnostics-probabilistic.html#sec-diag-prob-scor",
    "title": "Probabilistic",
    "section": "Scores",
    "text": "Scores\n\nContinuous Ranked Probability Score (CRPS)\n\nfabletools::accuracy\n{loo} - crps(), scrps(), loo_crps(), and loo_scrps() for computing the (Scaled) Continuously Ranked Probability Score\nManual calculation (article)\nMeasures forecast distribution accuracy\nCombines a MAE score with the spread of simulated point forecasts\nSee notebook (pg 172)\n\nWinkler Score\n\nfabletools::accuracy\nMeasures how well a forecast is covered by the prediction intervals (PI)\nSee notebook (pg 172)"
  },
  {
    "objectID": "qmd/diagnostics-probabilistic.html#sec-diag-prob-vizinsp",
    "href": "qmd/diagnostics-probabilistic.html#sec-diag-prob-vizinsp",
    "title": "Probabilistic",
    "section": "Visual Inspection",
    "text": "Visual Inspection\n\nCheck how well the predicted distribution matches the observed distribution\n{topmodels} currently supported models:\n\nlm, glm, glm.nb, hurdle, zeroinfl, zerotrunc, crch, disttree, and models from {disttree}, {crch}\nAlso see video, Probability Distribution Forecasts: Learning from Random Forests and Graphical Assessment\n\nautoplot produces a ggplot object that can be used for further customization\n(Randomized) quantile-quantile residuals plot\n\nqqrplot(distr_forest_fit)\n\nQuantiles of the standard normal distribution vs quantile residuals (regular ole q-q plot)\nInterpretation\n\nPretty good fit as the points stick pretty close to the line (red dot is the laser pointer from the dude giving the talk)\nLeft and right tails show deviation.\nThe left tail also shows increased uncertainty due the censored distribution that was used to fit the model\n\nCompare with a bad model\n\nc(qqrplot(distr_forest_fit, plot = FALSE), qqrplot(lm_fit, plot = FALSE)) |&gt; autoplot(legend = TRUE, single_graph = TRUE, col = 1:2)\n\n(Randomized) quantile-quantile residuals plot\n\npithist(distr_forest_fit)\n\nCompares the value that the predictive CDF attains at the observation with the uniform distribution\nThe flatter the histogram, the better the model.\nInterpretation: As with the q-q, this model shows some deviations at the tails but is more or less pretty flat\nCompare with a bad model\n\nc(pithist(distr_forest_fit, plot = FALSE), pithist(lm_fit, plot = FALSE) |&gt; autoplot(legend = TRUE, style = \"lines\", single_graph = TRUE, col = 1:2)\n\n(Hanging) Rootogram\n\nrootogram(distr_forest_fit)\n\nCompares whether the observed frequencies match the expected frequencies\nObserved frequencies (bars) are hanging off the expected frequencies (model predictions, red line)\nrobs is the outcome values\nInterpretation: Near perfect prediction for 0 precipitation (outcome variable), underfitting values of â€œ1â€ precipitation\nCompare with a bad model\n\nc(rootogram(distr_forest_fit, breaks = -9:14), rootogram(lm_fit,\nbreaks = -9:14) |&gt; autoplot(legend = TRUE)\n\nlm model shows overfitting of outcome variable values 1-5 and underfitting the zeros.\nThe lm model doesnâ€™t use a censored distribution so thereâ€™s an expectation of negative values\n\n\nReliability Diagram\n\nreliagram(fit)\n\nForecasted probabilities of an event vs observed frequencies\n\nBasically a fitted vs observed plot\nForecast probabilites are binned (points on the line), 10 in this example, and averaged\n\nClose to the dotted line indicates a good model\n\nWorm plot\n\nwormplot(fit)\n\n? ( he didnâ€™t describe this chart)\nGuessing the dots on the zero line indicates a perfect model and dots inside the dashed lines indicates a good model\n\nHe said this model fit was reasonable but doesnâ€™t look that great to me."
  },
  {
    "objectID": "qmd/diagnostics-regression.html#sec-diag-reg-gof",
    "href": "qmd/diagnostics-regression.html#sec-diag-reg-gof",
    "title": "Regression",
    "section": "GOF",
    "text": "GOF\n\n{performance}\n\nHandles all kinds of models including mixed models, bayesian models, econometric models\nperformance::check_model\n\n\nCan take a tidymodel as input.\n\nperformance::model_performance\n\nScores model using AIC, AICc, BIC, R2, R2adj, RMSE, Sigma\n\nâ€œSigmaâ€ is the standard deviation of the residuals (aka Residual standard error, see below)\n\n\nperformance::compare_performance\n\nOutputs table with scores for each model\n\n\n\n\nsummary\n\nStandard Errors: An estimate of how much estimates would â€˜bounce aroundâ€™ from sample to sample, if we were to repeat this process over and over and over again.\n\nMore specifically, it is an estimate of the standard deviation of the sampling distribution of the estimate.\n\nt-score: Ratio of parameter estimate and its SE\n\nUsed for hypothesis testing, specifically to test whether the parameter estimate is â€˜significantlyâ€™ different from 0.\n\np-value: The probability of finding an estimated value that is as far or further from 0, if the null hypothesis were true.\n\nNote that if the null hypothesis is not true, it is not clear that this value is telling us anything meaningful at all.\n\nF-statistic: This â€œsimultaneousâ€ test checks to see if the model as a whole predicts the response variable better than that of the intercept only (a.k.a. mean) model.\n\ni.e.Â Whether or not all the coefficient estimates should jointly be considered unable to be differentiated from 0\nAssumes homoskedastic variance\nIf this statisticâ€™s p-value &lt; 0.05, then this suggests that at least some of the parameter estimates are not equal to 0\nMany authors recommend ignoring the P values for individual regression coefficients if the overall F ratio is not statistically significant. This is because of the multiple testing problem. In other words, your p-value and f-value should both be statistically significant in order to correctly interpret the results.\nUnless you only have an intercept model, you have multiple tests (e.g.Â variables + intercept p-values). There is no protection from the problem of multiple comparisons without this.\n\nBear in mind that because p-values are random variablesâ€“whether something is significant would vary from experiment to experiment, if the experiment were re-runâ€“it is possible for these to be inconsistent with each other.\n\n\nResidual standard error - Variation around the regression line.\n\\[\n\\text{Residual Standard Error} = \\sqrt{\\frac{\\sum_{i=1}^n (Y_i-\\hat Y_i)^2}{dof}}\n\\]\n\n\\(\\text{dof} = n âˆ’ k^*\\) is the modelâ€™s degrees of freedom\n\n\\(k^*\\) = The numbers of parameters youâ€™re estimating including the intercept\n\nAside from model comparison, can also be compared to the sd of the observed outcome variable\n\n\n\n\nKolmogorovâ€“Smirnov test (KS)\n\nGuessing this can be used for GOF to compare predictions to observed\nMisc\n\nSee Distributions &gt;&gt; Tests for more details\nVectors may need to be standardized (e.g.Â normality test) first unless comparing two samples\n\nPackages\n\n{KSgeneral} has tests to use for contiuous, mixed, and discrete distributions; written in C++\n{stats} and {dgof} also have functions, ks.test\nAll functions take a numeric vector and a base R density function (e.g.Â pnorm, pexp, etc.) as args\n{KSgeneral} docs donâ€™t say you can supply your own comparison sample (2nd arg) only the density function but with stats and dgof, you can.\n\nAlthough they have function to compute the CDFs, so if you need speed, it might be possible to use their functions and do it man\n\n\n\n\n\ng-index\n\nFrom Harrellâ€™s rms (doc)\n\nNote: Harrell often recommends using Giniâ€™s mean difference as a robust substitute for the s.d.\nFor Giniâ€™s mean difference, see Feature Engineering, General &gt;&gt; Continuous &gt;&gt; Transformations &gt;&gt; Standardization\n\nI think the g-index for a model is the total of all the partial g-indexes\n\nEach independent variable would have a partial g-index\nHe also supplies 3 different ways of combining the partial indexes I think\nHarrell has a pretty thorough example in the function doc that might shed light\n\nPartial g-Index\n\nExample: A regression model having independent variables, age + sex + age*sex, with corresponding regression coefficients \\(\\beta_1\\), \\(\\beta_2\\), \\(\\beta_3\\)\n\ng-indexage = Giniâ€™s mean difference (age * (\\(\\beta_1\\) + \\(\\beta_3\\)*w))\n\nWhere w is an indicator set to one for observations with sex not equal to the reference value.\nWhen there are nonlinear terms associated with a predictor, these terms will also be combined."
  },
  {
    "objectID": "qmd/diagnostics-regression.html#sec-diag-reg-res",
    "href": "qmd/diagnostics-regression.html#sec-diag-reg-res",
    "title": "Regression",
    "section": "Residuals",
    "text": "Residuals\n\nMisc\n\nPackages\n\n{gglm} - Diagnostic plots for residuals\n\ninfluence.measures(mod) wll calculate DFBETAS for each model variable, DFFITS, covariance ratios, Cookâ€™s distances and the diagonal elements of the hat matrix. Cases which are influential with respect to any of these measures are marked with an asterisk.\n\nStandardized Residuals:\n\\[\nR_i = \\frac{r_i}{SD(r)}\n\\]\n\nWhere r is the residuals\nFollows a Chi-Square distribution\nAny \\(|R|\\) &gt; 2 or 3 is an indication that the point may be an outlier\nIn R: rstandard(mod)\nIt may be a good idea to run the regression twice â€” with and without the outliers to see how much they have an effect on the results.\nInflation of the MSE due to outliers will affect the width of CIs and PIs but not predicted values, hypothesis test results, or effect point estimates\n\nStudentized Residuals\n\\[\nt_i = r_i \\left(\\frac{n-k-2}{n-k-1-r_i^2}\\right)^{\\frac{1}{2}}\n\\]\n\nWhere \\(r_i\\) is the ith standardized residual, \\(n\\) = the number of observations, and \\(k\\) = the number of predictors.\nSome outliers wonâ€™t be flagged as outlierrs because they drag the regression line towards them. These residuals are for detecting them.\n\nTherefore, generally better at detecting outliers than standardized residuals.\n\nThe studentized residuals, \\(t\\), follow a student t distribution with dof = nâ€“kâ€“2\n\nRule of thumb is any \\(|t_i|\\) &gt; 3 is considered an outlier but you can check the residual against the critical value to be sure.\n\nIn R, rstudent(mod)\nIt may be a good idea to run the regression twice â€” with and without the outliers to see how much they have an effect on the results.\nInflation of the MSE due to outliers will affect the width of CIs and PIs but not predicted values, hypothesis test results, or effect point estimates\n\nCheck for Normality\n\nResiduals vs Fitted scatter\n\nLooking for data to centered around 0\nHelpful to have a horizontal and vertical line at the zero markers on the X & Y axis.\n\nResiduals historgram\n\nLook for symmetry\nHelpful to have a gaussian overlay\n\n\nCheck for heteroskedacity or Non-Linear Patterns\n\nResiduals vs Fitted scatter\n\nHeteroskedastic\n\n\n\nSubtler but present (reverse megaphone shape)\n\nSolutions:\n\nLog transformations of a predictior, outcome or both\nHeteroskedastic robust standard errors (See Econometrics, General &gt;&gt; Standard Errors)\nGeneralized Least Squares (see Regression, Other &gt;&gt; Misc)\nWeighted Least Squares (see Regression, Other &gt;&gt; Weighted Least Squares)\n\nAlso see Feasible Generalized Least Squares (FGLS) in the same note\nExample: Real Estate &gt;&gt; Appraisal Methods &gt;&gt; CMA &gt;&gt; Market Price &gt;&gt; Case-Shiller method\n\nScale model (greybox::sm ) models the variance of the residuals or greybox::alm will call sm and fit a model with estimated residual variance\n\nSee article for an example\nCan be used with other distributions besides gaussian\nThe unknown factor or function to describe the residual variance is a problem w/WLS\n\n{gamlss} also models location, scale, and shape\n\nAlso can be used with other distributions besides gaussian\n\n\n\nNonlinear\n\nBreusch Pagan test (lmtest::bptest or car::ncvtest )\n\nH0: No heteroskedacity present\nbptest takes data + formula or lm model; ncvtest only takes a lm model\n\n\n\nCheck for Autocorrelation\n\nRun Durbin-Watson, Breusch-Godfrey tests: forecast::checkresiduals(fit)Â  to look for autocorrelation between residuals.\n\nRange: 1.5 to 2.5\nClose to 2 which means youâ€™re in the clear.\n\n\nCheck for potential variable transformations\n\nResidual vs Predictor\n\nRun every predictor in the model and every predictor that wasnâ€™t used.\nShould look random.\nNonlinear patterns suggest non linear model should be used that variable (square, splines, gam, etc.). Linear patterns in predictors that werenâ€™t use suggest they should be used.\n\ncar::residualPlots - Plots predictors vs residuals and performs curvature test\n\np &lt; 0.05 â€“&gt; Curvature present and need a quadratic version of the variable\nOr car::crPlots(model) for just the partial residual plots\n\n{ggeffects}\n\nIntroduction: Adding Partial Residuals to Marginal Effects Plots\n\nShows how to detect not-so obvious non-linear relationships and potential interactions through visualizing partial residuals\n\n\nHarrellâ€™s {rms}\n\nLogistic regression example (link)\n\nEnd of section 10.5; listen to audio"
  },
  {
    "objectID": "qmd/diagnostics-regression.html#sec-diag-eg-plot",
    "href": "qmd/diagnostics-regression.html#sec-diag-eg-plot",
    "title": "Regression",
    "section": "plot(fit)",
    "text": "plot(fit)\n\nFor each fitted model object look at residual plots searching for non-linearity, heteroskedacity, normality, and outliers. Correlation matrix (see bkmk in stats) for correlation and VIF score for collinearity among predictors.\nIf non-linearity is present in a variable run poly function to determine which polynomial produces the least cv error\nIs heteroscedasticity is present use a square root or log transform on the variable. Not sure if this is valid with multiple regression. If one variable is transformed the others might have to also be transformed in order to maintain interpretability. *try sandwich estimator in regtools, car, or sandwich pkgs*\nOutliers can be investigated further with domain knowledge or other statistical methods\nExample: Diamonds dataset from {ggplot2}\n\nprice ~ carat + cut\n\n\nBad fit\nResiduals vs Fitted: Clear structure in the residuals, not white noise. They curve up at the left (so some non-linearity going on), plus they fan out to the right (heteroskedasticity)\nScale-Location: The absolute scale of the residuals definitely increases as the expected value increases â€” a definite indicator of heteroskedasticity\nNormal Q-Q: Strongly indicates the residuals arenâ€™t normal, but has fat tails (e.g.Â when they theoretically would be about 3 on the standardised scale, they are about 5 - much higher)\n\nlog(price) ~ log(carat) + cut\n\n\nGood fit\nResiduals vs Fitted: Curved shape and the fanning has gone and weâ€™re left with something looking much more like white noise\nScale-Location: Looks like solid homoskedasticity\nNormal Q-Q: A lot more â€œnormalâ€ (i.e.Â straight line) and apart from a few outliers the values of the standardized residuals are what youâ€™d expect them to be for a normal distribution"
  },
  {
    "objectID": "qmd/diagnostics-regression.html#sec-diag-reg-othdiag",
    "href": "qmd/diagnostics-regression.html#sec-diag-reg-othdiag",
    "title": "Regression",
    "section": "Other Diagnostics",
    "text": "Other Diagnostics\n\nMisc\n\nTutorial for modeling with Harrellâ€™s {Hmisc}\n{kernelshap} and {fastshap} can handle complex lms, glms (article)\n\nAlso see Diagnostics, Model Agnostic &gt;&gt; SHAP\n\n\nCheck for influential observations - outliers that are in the extreme x-direction\nCheck VIF score for collinearityÂ \nFor prediction, if coefficients vary significantly across the test folds their robustness is not guaranteed (see coefficient boxplot below), and they should probably be interpreted with caution.\n\n\nBoxplots show the variance of the coefficient across the folds of a repeated 5-fold cv.\nThe â€œCoefficient importanceâ€ in the example is just the coefficient value of the standardized variable in a ridge regression\nNote outliers beyond the whiskers for Age and Experience\n\nIn this case, the variance is caused by the fact that experience and age are strongly collinear.\n\nVariability in coefficients can also be explained by collinearity between predictors\n\nPerform sensitivity analysis by removing one of the collinear predictors and re-running the CV. Check if the variance of the variable that was kept has stabilized (e.g.Â fewer outliers past the whiskers of a boxplot)."
  },
  {
    "objectID": "qmd/diagnostics-regression.html#sec-diag-reg-mlpred",
    "href": "qmd/diagnostics-regression.html#sec-diag-reg-mlpred",
    "title": "Regression",
    "section": "ML Prediction",
    "text": "ML Prediction\n\nMisc\n\nGet test predictions from tidymodels workflow fit obj\npreds_tbl &lt;- wflw_fit_obj %&gt;%\nÂ  Â  predict(testing(splits)) %&gt;%\nÂ  Â  bind_cols(testing(splits), .)\n\nGOF\n\nRMSE of model vs naive\npreds_tbl %&gt;%\nÂ  Â  yardstick::rmse(outcome_var, .pred)\npreds_tbl %&gt;%\nÂ  Â  mutate(.naive = mean(outcome_var)) %&gt;%\nÂ  Â  yardstick::rmse(outcome_var, .naive\n\nR2\npreds_tbl %&gt;%\nÂ  Â  yardstick::rsq(outcome_var, .pred)\n\nSquared correlation between truth and estimate to guarantee a value between 0 and 1\n\nObserved vs Predicted plot\npreds_tbl %&gt;%\nÂ  Â  ggplot(aes(outcome_var, .pred)) +\nÂ  Â  # geom_jitter(alpha = 0.5, size = 5, width = 0.1, height = 0) + # if your outcome var is discrete\nÂ  Â  geom_smooth()\nFeature Importance\n\nExample: tidymodel xgboost\nxgb_feature_imp_tbl &lt;- workflow_obj_xgb %&gt;%\nÂ  Â  extract_fit_parsnip() %&gt;%\nÂ  Â  pluck(\"fit\") %&gt;%\nÂ  Â  xgboost::xgb.importance(model = .) %&gt;%\nÂ  Â  as_tibble() %&gt;%\nÂ  Â  slice(1:20)\n\nxgb_feature_imp_tbl %&gt;%\nÂ  Â  ggplot(aes(Gain, fct_rev(as_factor(Feature)))) %&gt;%\nÂ  Â  geom_point()"
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-terms",
    "href": "qmd/distributions.html#sec-distr-terms",
    "title": "Distributions",
    "section": "Terms",
    "text": "Terms\n\nConditional Probability Distributions\n\nNotes from https://www.causact.com/joint-distributions-tell-you-everything.html#joint-distributions-tell-you-everything\nNotation: \\(P(Y | X) = P(Y \\text{and} X) / P(X) = P(Y, X) / P(X)\\)\n\ni.e.Â ratio of 2 marginal distributions\n\nExample: two tests for cancer are conducted to determine whether a biopsy should be performed\n\nConditional approach: Biopsy everyone at determined to be high risk from test 1; measure the genetic marker (aka test 2) for patients at intermediate risk and biopsy those with a probability of cancer past a certain level based on the marker\n\n\nEmpirical CDF\n\\[\nF_n (x) = \\frac {1}{n} \\sum_{i = 1}^n I(X_i \\leq x)\n\\]\n\nWhere \\(X_1, X_2,\\ldots,X_n\\) are from a population with CDF, \\(F_n (x)\\)\nProcess\n\nTake n samples from an unknown distribution. The more samples you take, the closer the empirical distribution will resemble the true distribution.\nSort these samples, and place them on the x-axis.\nStart plotting a â€˜step-functionâ€™ style line â€” each time you encounter a datapoint on the x-axis, increase the step by 1/N.\n\nExample\n\n\nThe CDF of a normal distribution (green) and its empirical CDF (blue)\n\n\nJoint Probability Distribution - assigns a probability value to all possible combinations of values for a set of random variables.\n\nNotation: \\(P(x_1, x_2, ... ,x_n)\\)\nPlugging in a value for each random variable returns a probability for that combination of values\nExample: Two tests for cancer are conducted to determine whether a biopsy should be performed\n\nJoint approach: biopsy anyone who is either at high risk of cancer (test 1) or who was determined to have a probability of cancer past a certain level, based on the marker from the genetic test (test 2)\nCompare with example in Conditional Probability Distributions\n\n\nLocation - Distribution parameter determines the shift of the distribution\n\ne.g.Â mean, mu, of the normal distribution.\n\nMarginal Probability Distribution - assigns a probability value to all possible combinations of values for a subset of random variables\n\nNotation: \\(P(x_1)\\)\n\n\\(P(x_1,x_2)\\) is sometimes called the Joint Marginal Probability Distribution\n\nThe marginal distribution, \\(P(Y)\\) where \\(Y\\) is a subset of random variables, is calculated from the joint distribution, \\(P(Y = y, Z = z)\\) where \\(Z\\) is the subset of random variables not in \\(Y\\) .\n\n\\(P(Y) = \\sum_{Z=z} P(Y = y, Z = z)\\)\n\nIf \\(Y\\) is just one variable\n\nSays sum all the joint probabilities for all the combinations of values for the variables in \\(Z\\) while holding \\(Y\\) constant\nRepeat for each value of \\(Y\\) to get this summed probability value\nThe marginal distribution is made up of all these values, one for each value of \\(Y\\) (or combination of values if \\(Y\\) is a subset of variables)\n\n\nWhen the joint probability distribution is in tabular form, one just sums up the probabilities in each row where \\(Y = y\\).\n\n\nScale - Distribution parameter; the larger the scale parameter, the more spread out the distribution\n\ne.g.Â s.d., sigma, \\(\\sigma\\) of the normal distribtution\nRate Parameter: the inverse of the scale parameter (see Gamma distribution)\n\nShape - Distribution parameter that affects the shape of a distribution rather than simply shifting it (as a location parameter does) or stretching/shrinking it (as a scale parameter does).\n\ne.g.Â â€œPeakednessâ€ refers to how round the main peak is"
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-tests",
    "href": "qmd/distributions.html#sec-distr-tests",
    "title": "Distributions",
    "section": "Tests",
    "text": "Tests\n\nWhy normality tests are greatâ€¦ as a teaching example and should be avoided in research\n\ntl;dr; KS test has very low power as a Normality test as compared to Shapiro-Wilk, and Shapiro-Wilk isnâ€™t very good for n &lt; 100\nFor detecting moderate skew, you want at least n &gt; 75 to get 80% power for Shapiro-Wilk\nShapiro-Wilk can detect very fat tails at n &lt; 100, but would require larger sample sizes to detect more moderately thick tails.\nKS is worthless in detecting fat tails and near-worthless at detecting skew\nWhen n gets large (e.g.Â 1000s), these types of tests will almost always reject the null even when the practical deviation from normality is not practically significant.\n\nKolmogorovâ€“Smirnov test (KS)\n\nUsed to compare distributions\n\nCan be used as a Normality test or any distribution test\nCan compare two samples\n\nMisc\n\nVectors may need to be standardized (e.g.Â normality test) first unless comparing two samples H0: Both distributions are from the same distribution\n\nPackages\n\n{KSgeneral} has tests to use for contiuous, mixed, and discrete distributions written in C++\n{stats} and {dgof} also have functions, ks.test\n\nBoth handle continuous and discrete distributions\n\nAll functions take a numeric vector and a base R density function (e.g.Â pnorm, pexp, etc.) as args\n\nKSgeneral docs donâ€™t say you can supply your own comparison sample (2nd arg) only the density function but with stats and dgof, you can.\nAlthough they have function to compute the CDFs, so if you need speed, it might be possible to use their functions and do it manually\n\n\n2-sample test as the greatest distance between the CDFs (Cumulative Distribution Function) of each sample\n\nSpecifically, this test determines the distribution of your unknown data sample by constructing and comparing the sampleâ€™s empirical CDFÂ  (see Terms) with the CDF you hypothesized. If the two CDFs are close, your unknown data sample likely follows the hypothesized distribution.\n\nKS statistic, \\(D_{n,m} = \\max|\\text{CDF}_1 - \\text{CDF}_2|\\) where \\(n\\) as the number of observations on Sample 1 and \\(m\\) as the number of observations in Sample 2\nCompare the KS statistic with the respective KS distribution based on parameter â€œenâ€ to obtain the p-value of the test\n\n\\(en = (m \\times n) / (m + n)\\)"
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-beta",
    "href": "qmd/distributions.html#sec-distr-beta",
    "title": "Distributions",
    "section": "Beta",
    "text": "Beta\n\n\nDefined on the interval [0,1]\nThe key difference between the Binomial and Beta distributions is that for the Beta distribution the probability, x, is a random variable, however for the Binomial distribution the probability, x, is a fixed parameter.\nShape parameters are \\(\\alpha\\) and \\(\\beta\\), usually.\n\n\\(\\alpha\\) and \\(\\beta\\) are two positive parameters that appear as exponents of the random variable\n\npdf\n\\[\nf(x) = \\frac {x^{\\alpha - 1} (1-x)^{\\beta - 1}} {B(\\alpha, \\beta)}\n\\]\n\\(\\mathbb{E}(X) = \\frac {\\alpha} {\\alpha + \\beta}\\)\n\\(\\text{Var}(X) = \\frac {\\alpha \\cdot \\beta} {(\\alpha + \\beta)^2 \\cdot (\\alpha + \\beta + 1)}\\)"
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-betbin",
    "href": "qmd/distributions.html#sec-distr-betbin",
    "title": "Distributions",
    "section": "Beta-Binomial",
    "text": "Beta-Binomial\n\n\n\n\n\nWhere k is the number of events in n trials\n\n\n\n\n\n\n\nWhere \\(\\theta\\) is the probability of an event\n\n\n\n\n\n\n\nUsed when the probability of success, p, in a fixed number of Bernoulli trials is unknown or random and can change from trial to trial.\nShape parameters Î± and Î² define the probability of success (i.e.Â the success parameter is modeled by the Beta Distribution).\n\nFor large values of Î± and Î², the distribution approaches a binomial distribution.\nWhen Î± and Î² both equal 1, the distribution equals a discrete uniform distribution from 0 to n\n\nAccuracy analysis data from psychology follow beta-binomial distributions (Jaeger, 2008; Kruschke, 2014)"
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-dirichlet",
    "href": "qmd/distributions.html#sec-distr-dirichlet",
    "title": "Distributions",
    "section": "Dirichlet",
    "text": "Dirichlet\n\nA family of continuous multivariate probability distributions parameterized by a vector Î± of positive reals"
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-exp",
    "href": "qmd/distributions.html#sec-distr-exp",
    "title": "Distributions",
    "section": "Exponential",
    "text": "Exponential\n\n\nNotes from\n\nStatistical Rethinking &gt;&gt; Chapter 10\n\nConstrained to be zero or positive\nFundamental distribution of distance and duration, kinds of measurements that represent displacement from some point of reference, either in time or space.\nIf the probability of an event is constant in time or across space, then the distribution of events tends towards exponential.\nIts shape is described by a single parameter, the rate of events \\(\\lambda\\), or the average displacement \\(\\lambda âˆ’1\\) .\nThis distribution is the core of survival and event history analysis"
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-gamma",
    "href": "qmd/distributions.html#sec-distr-gamma",
    "title": "Distributions",
    "section": "Gamma",
    "text": "Gamma\n\n\nNotes from\n\nStatistical Rethinking &gt;&gt; Chapter 10\n\nConstrained to be zero or positive\nLike Exponential but can have a peak above zero\nIf an event can only happen after two or more exponentially distributed events happen, the resulting waiting times will be gamma distributed.\n\ne.g.Â age of cancer onset is approximately gamma distributed, since multiple events are necessary for onset.\n\nThe gamma can be viewed as the sum of iid n exponential random variables. Exponential random variables have a rate parameter, so it makes sense for the Gamma to inherit a rate parameterization. The rate parameter also happens to be related to a scale parameter, so it makes sense for the Gamma to have a scale parameterization.\nShape parameter \\(k\\) and a scale parameter \\(\\theta\\)\n\\(\\mathbb{E}[X] = k\\theta = \\frac{\\alpha}{\\beta}\\)\n\nShape parameter \\(\\alpha = k\\) and an\nInverse Scale parameter (aka Rate Parameter) \\(\\beta = \\frac {1}{\\theta}\\)\nTherefore if you want a gamma distributions with a certain â€œmeanâ€ and â€œstandard deviation,â€ youâ€™d:\n\nSet your mean to \\(\\mathbb{E}[X]\\), your standard deviation to \\(\\theta\\) (probably but maybe itâ€™s \\(\\beta\\))\nCalculate \\(\\beta\\)\nCalculate \\(\\alpha\\)\nprior(gamma(alpha, beta))\n\n\nExample: Gamma distribution as the sums of random exponential variables\n\nn &lt;- 12\nbeta &lt;- 1.2\n\nrvs &lt;- replicate(1000, {\n  sum(rexp(n, beta))\n})\n\nhist(rvs, freq = F)\ncurve(dgamma(x, shape = n, rate = beta), col='red', add=T)\n\nGamma distribution density overlayed with a histogram of exponential variable sums\n\nUsed in Survival Regression"
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-gauss",
    "href": "qmd/distributions.html#sec-distr-gauss",
    "title": "Distributions",
    "section": "Gaussian",
    "text": "Gaussian\n\nSpecial case of Studentâ€™s t-distribution with the \\(\\nu\\) parameter (i.e.Â degree of freedom) set to infinity."
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-gumb",
    "href": "qmd/distributions.html#sec-distr-gumb",
    "title": "Distributions",
    "section": "Gumbel",
    "text": "Gumbel\n\n\nKnown as the type-I generalized extreme value distribution\n\nEVT says it is likely to be useful if the distribution of the underlying sample data is of the normal or exponential type.\n\nUsed to model the distribution of the maximum (or the minimum) of a number of samples of various distributions.\n\nTo model minimums, use the negative of the original data.\n\nUse Cases\n\nRepresent the distribution of the maximum level of a river in a particular year if there was a list of maximum values for the past ten years.\nPredicting the chance that an extreme earthquake, flood or other natural disaster will occur.\nDistribution of the residuals in Multinomial Logit and Nested Logit models\n\nParameters\n\nGumbel(\\(\\mu, \\beta\\)) (location, scale)\nMean: \\(\\mu + \\beta\\gamma\\) where \\(\\gamma\\) is Eulerâ€™s constant (\\(\\approx\\) 0.5772)\nMedian: \\(\\mu - \\beta \\ln(\\ln(2))\\)\nMode: \\(\\mu\\)\nVariance: \\(\\frac{\\pi^2}{6}\\beta^2\\)\nStandard Gumbel: When \\(\\mu = 0\\), mean = \\(\\gamma\\), median = \\(-\\ln(\\ln(2)) \\approx 0.3665\\) and the standard deviation = \\(\\pi/\\sqrt{6}\\)"
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-multgauss",
    "href": "qmd/distributions.html#sec-distr-multgauss",
    "title": "Distributions",
    "section": "Multivariate Gaussian",
    "text": "Multivariate Gaussian\n\nIf the random variable components in the vector are not normally distributed themselves, the result is not multivariate normally distributed.\nVariance-Covariance matrix must be semi-definite and therefore symmetric\n\nExample of not symmetric for two random variables"
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-pareto",
    "href": "qmd/distributions.html#sec-distr-pareto",
    "title": "Distributions",
    "section": "Pareto",
    "text": "Pareto\n\nAlso see Extreme Value Theory &gt;&gt; Distribution Tail Classification\nâ€œGaussian distributions tend to prevail when events are completely independent of each other. As soon as you introduce the assumption of interdependence across events, Paretian distributions tend to surface because positive feedback loops tend to amplify small initial events.â€\nPareto has similar relationship with the exponential distribution as lognormal does with normal \\[\nY_{exp} = \\log \\frac {X_{pareto}} {x_m}\n\\]\n\nWhere \\(X_{pareto} = x_m e^{Y_{\\text{exp}}}\\)\n\n\\(x_m\\) is the (positive) minimum of the randomly distributed pareto variable, X that has index Î±\n\\(Y_{exp}\\) is exponentially distributed with rate \\(\\alpha\\)\n\n\nSome theoretical statistical moments may not exist\n\nIf the theoretical moments do not exist, then calculating the sample moments is useless\nExample: Pareto (\\(\\alpha\\) = 1.5) has a finite mean and an infinite variance\n\nNeed \\(\\alpha &gt; 2\\) for a finite variance\nNeed \\(\\alpha &gt; 1\\) for a finite mean\nIn general you need \\(\\alpha &gt; p\\) for the pth moment to exist\nIf the nth moment is not finite, then the (n+1)th moment is not finite.\n\n\nFat Tails \\[\n\\bar{F} = x^{-\\alpha} L(x)\n\\]\n\n\\(L(x)\\) is just characterized as slowly varying function that gets dominated by the decaying inverse power law element, \\(x-\\alpha\\). as \\(x\\) goes to infinity\n\n\\(\\alpha\\) is a shape parameter, aka â€œtail indexâ€ aka â€œPareto indexâ€"
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-poisson",
    "href": "qmd/distributions.html#sec-distr-poisson",
    "title": "Distributions",
    "section": "Poisson",
    "text": "Poisson\n\nObtained as the limit of the binomial distribution when the number of attempts is high and the success probability low. Or the Poisson distribution can be approximated by a normal distribution when Î» is large\nProbability Mass Function \\[\n\\text{Pr}(Y = y) = f(y; \\lambda) = \\frac {e^{-\\lambda} \\cdot \\lambda^y} {y!}\n\\]\n\n\\(\\mathbb{E}[Y] = \\text{Var}(Y) = \\lambda\\)\n\n{distributions3}\n\nStats\nY &lt;- Poisson(lambda = 1.5) \nprint(Y) \n## [1] \"Poisson distribution (lambda = 1.5)\"\n\nmean(Y) \n## [1] 1.5 \nvariance(Y) \n## [1] 1.5 \npdf(Y, 0:5) \n## [1] 0.22313 0.33470 0.25102 0.12551 0.04707 0.01412 \ncdf(Y, 0:5) \n## [1] 0.2231 0.5578 0.8088 0.9344 0.9814 0.9955 \nquantile(Y, c(0.1, 0.5, 0.9)) \n## [1] 0 1 3 \nset.seed(0) \nrandom(Y, 5) \n## [1] 3 1 1 2 3\n\nVisualize\n\nplot(Poisson(0.5), main = expression(lambda == 0.5), xlim = c(0, 15)) \nplot(Poisson(2),   main = expression(lambda == 2),   xlim = c(0, 15)) \nplot(Poisson(5),   main = expression(lambda == 5),   xlim = c(0, 15)) \nplot(Poisson(10),  main = expression(lambda == 10),  xlim = c(0, 15))"
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-studt",
    "href": "qmd/distributions.html#sec-distr-studt",
    "title": "Distributions",
    "section": "Studentâ€™s t-distribution",
    "text": "Studentâ€™s t-distribution\n\nStandard Deviation\n\\[\n\\text{sd} = \\sqrt {\\frac {\\nu} {\\nu - 2}}\n\\]\n\n\\(\\nu\\) = degrees of freedom\n\nWhen Î½ is small, the Studentâ€™s t-distribution is more robust to multivariate outliers\nThe smaller the degree of freedom, the more â€œheavy-tailedâ€ it is\n\n\n-3 on the y-axis says that the probability of being in the tail is 1 in 103\n\nDonâ€™t pay attention to the x-axis. Just note how much the probability of being in the tail gets larger as the dof get smaller\n\nAs the degrees of freedom goes to 1, the t distribution goes to the Cauchy distribution\nAs the degrees of freedom goes to infinity, it goes to the Normal distribution."
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-tri",
    "href": "qmd/distributions.html#sec-distr-tri",
    "title": "Distributions",
    "section": "Triangular",
    "text": "Triangular\n\nTriangle shaped distribution\nUseful when you have a known min and max value\nextraDistr::rtriang(n, a, b, c) %\\&gt;% hist()\n\n# Discrete distribution\nextraDistr::rtriang(n, a, b, c) %\\&gt;% round() \\`\\`\\`\n\nn is the number of random values you wish to draw\na is the min value\nb is the max value\nc is the mode\n\nCan use to adjust the skew of the distribution"
  },
  {
    "objectID": "qmd/dl-audio.html",
    "href": "qmd/dl-audio.html",
    "title": "15Â  DL, Audio",
    "section": "",
    "text": "TOC\n\nMisc\n\nMisc\n\nPackages\n\n{carelesswhisper} - Speech-to-Text; Automatic speech recognition in R using whisper.cpp; Translation; Can use huggingface and ggerganov models; no dependencies\n\nStrong Baseline Model: ResNet/EffNet\n\nTypically these are image models, but converting the audio to a spectrogram and using these models produces good results\nAlso see this guide for suitable baseline models: link"
  },
  {
    "objectID": "qmd/dl-general.html",
    "href": "qmd/dl-general.html",
    "title": "16Â  DL, General",
    "section": "",
    "text": "TOC\n\nMisc\nTerms\nActivation Functions\nRegularization\nReinforcement Learning\n\nMisc\n\nGuide for suitable baseline models: link\nDL model cost calculator (github) (article)\nUse Adam and AdamW optimizers\nKeras also provides out-of-the-box preprocessing layers. This way, when the model is saved, the preprocessing steps will automatically be part of the model.\n\ni.e.Â the same preprocessing steps applied in training are applied in production\nBut preprocessing steps will be wastefully repeated on each iteration through the training dataset. The more expensive the computation, the more this adds up.\n\nDL architectures\n\nTerms\n\nFully-Connected Layers (aka Linear Layers) - connect every input neuron to every output neuron. Each neuron applies a linear transformation to the input vector through a weights matrix. As a result, all possible connections layer-to-layer are present, meaning every input of the input vector influences every output of the output vector. Three parameters define a fully-connected layer: batch size, number of inputs, and number of outputs. (see article)\nModalities\n\nunimodal models: text-only, image-only, etc.\nmultimodal models: text, image, continuous sensor data, etc.\n\n\nActivation functions \n\nActivation Function: after the node calculates the weighted sum of the input, the activation function transforms the output which is fed to the next layer.\nMisc\n\nThe choice of activation function has a large impact on the capability and performance of the neural network, and\nDifferent activation functions may be used in different layers of the model.\n\nCommonly the same activation function is used for the hidden layers and a different one for the outer layer that makes the prediction (e.g.Â softmax)\n\nMost activation functions add non-linearity to the neural network\nBoth the sigmoid and Tanh functions can make the model more susceptible to problems during training, via the so-called vanishing gradients problem.\nArchitectures\n\nHidden layers\n\nMultilayer Perceptron (MLP): ReLU activation function.\nConvolutional Neural Network (CNN): ReLU activation function.\nRecurrent Neural Network (RNN): Tanh and/or Sigmoid activation function\n\nOuter layer\n\nRegression: One node, linear activation\nBinary Classification: One node, sigmoid activation.\nMulticlass Classification: One node per class, softmax activation.\nMultilabel Classification: One node per class, sigmoid activation.\n\n\n\nBase Types\n\nReLU: if the input value (x) is negative, then a value 0.0 is returned, otherwise, the value is returned\nSigmoid: Logistic function; output is 0 to 1\n\nA perceptron is called the logistic regression model if the activation function is sigmoid.\nGood practice to use a â€œXavier Normalâ€ or â€œXavier Uniformâ€ weight initialization (aka Glorot initialization)) and scale input data to the range 0-1 (e.g.Â the range of the activation function) prior to training.\n\ntanh: takes any real value as input and outputs values in the range -1 to 1\n\nThe larger the input (more positive), the closer the output value will be to 1.0, whereas the smaller the input (more negative), the closer the output will be to -1.0.\nGood practice to use a â€œXavier Normalâ€ or â€œXavier Uniformâ€ weight initialization (aka Glorot initialization) and scale input data to the range -1 to 1 (e.g.Â the range of the activation function) prior to training.\n\n\nComprehensive Survey and Performance Analysis of Activation Functions in Deep Learning, paper\n\nRegularization\n\nMisc\n\nOther methods\n\nData Augmentation\n\nMore data reduces variance\nComputer vision: Gain data by flipping, zooming, and translating the original images\nDigits Recognition: Gain data by mposing distortion on the images\n\nEarly Stopping\n\nStopping the training phase before a defined number of iterations\nFor an overfitting model, if we plot the cost function on both the training set and the validation set as a function of the iterations.\n\nThe training error always keeps decreasing but the validation error might start to increase after a certain number of iterations.\nWhen the validation error stops decreasing, that is exactly the time to stop the training process.\nBy stopping the training process earlier, we force the model to be simpler, thus reducing overfitting.\n\n\n\n\nL1 and L2 regularization\n\nShrinks model weights\nRegularization process\n\nThe weights of some hidden units become closer (or equal) to 0. As a consequence, their effect is weakened and the resulting network is simpler because itâ€™s closer to a smaller network. As stated in the introduction, a simpler network is less prone to overfitting.\nFor smaller weights, also the input z of the activation function of a hidden neuron becomes smaller. For values close to 0, many activation functions behave linearly.\n\nL1\n\nCost Function, J\nL is the loss function\nm is the number of training â€œexamplesâ€\nw and b are the weight and bias terms in the output of the node\nRegularization term\n\nThe norm of the weights, w, is\nL is the number of layers\nÎ» is the regularization factor\n\n\nL2\n\nCost Function, J\n\nSee L1 for definitions of terms\n\nRegularization term\n\nThe norm of the weights, w, is\n\nn^{[l]} rows and n^{[l-1]} columns (?)\n\n\n\n\nDropout\n\nRandomly remove some nodes in the network\n\nPerformed separately for each training example.\nTherefore, each training example might be trained on a different network.\n\nRegularization Process\n\nHas the effect of temporarily transforming the network into a smaller one, and we know that smaller networks are less complex and less prone to overfitting.\nBecause some of its inputs may be temporarily shut down due to dropout, the unit canâ€™t always rely on them during the training phase. As a consequence, the hidden unit is encouraged to spread its weights across its inputs. Spreading the weights has the effect of decreasing the squared norm of the weight matrix, resulting in a sort of L2 regularization.\n\nProcess\n\nSet a probability, p, for each node of the network.\n\nTypically, the keeping probability is set separately for each layer of the neural network\nFor layers with a large weight matrix, a smaller keeping probability because, at each step, we want to conserve proportionally fewer weights with respect to smaller layers\n\nDuring the training phase, each node has a p probability to be turned off.\n\n\n\nReinforcement Learning\n\nInvolves training a smart agent that can learn to perform a goal through trial & error in an environment and at the end of the training we have an agent that can perform the goal in real life independently\nA type of machine learning problem where, rather than making a single decision, you have to make multiple sequential decisions as part of a strategy\nRL does not require any explicit labels to be provided unlike in supervised learning techniques\nUse Cases\n\nEmail\n\nWhatâ€™s the most optimal time for each user as to when theyâ€™ll want to read it.\nHow many is too many emails per user?\n\nDynamic paywall metering\n\nHelp make the decision about a tradeoff between making revenue through serving ads by allowing users to read articles for free and making revenue through subscriptions by blocking free access with a digital paywall (after a certain number of free articles), inducing the user to subscribe\n\n\nDeep Q Network (DQN)\n\na combination of the principles of deep learning and Q-learning\n\nQ-learning is an algorithm of a class of RL solutions called tabular solutions which aims to learn the q-values for each state.\n\nQ-value of a state is the cumulative (discounted) reward from all the states that the agent could go in the future.\nAn elegant solution for problems that have a finite state spaces such as frozen lake problem.\n\n\nFor larger state spaces, this Q-learning gets unwieldy and needs to adopt an approximate way of estimating state value and this class of solution is called â€˜approximate methodsâ€™.\n\nDQN is the most popular algorithm among the approximate methods.\n\nIn DQN, the deep learning network serves as a function approximation that estimates the value for a given state/action\nThe solution design, the algorithm and the setup would be the same for all the use cases, but the configuration of the MDP (Markov Decision Process)â€” the states spaces, rewards, actions to be taken would vary for each use case.\n\nExample States â€” The NL opens/clicks pattern for the last (1/2) months.\n  _Action â€” 1â€“24 hours of the day. This could further be reduced to 12 action values with each action representing a 2 hour period when the email could be send._\n\n  _Reward â€” +2 for a NL click, +1 for a NL open, 0 otherwise_"
  },
  {
    "objectID": "qmd/dl-graph.html",
    "href": "qmd/dl-graph.html",
    "title": "17Â  DL, Graph",
    "section": "",
    "text": "TOC\n\nMisc\nTerms\nTests\n\nMisc\n\n\n\nTerms\n\nIsomorphism - two graphs are isomorphic if there is a mapping between their nodes in which we can conclude that these graphs are in fact the same\n\ntwo graphs H and G are isomorphic if and only if, for any pair of nodes u and v from H that are adjacent, there is a transformation f where f(u) is adjacent to f(v) in G\nno polynomial-time solution and the problem may as well be considered NP-Complete\nExample\n\n\nTests\n\nWeisfeiler-Lehman Test\n\ntests if two graphs are isomorphic or not\nH0: Not Isomorphic; Ha: May be Isomorphic\n\nGiven that the problem might be NP-Complete, the test can fail in many situations\n\nExample: Non-Isomorphic but passes WL Test\n\n\nProcess\n\nWe start by setting an initial value to every node on the graph. Letâ€™s say â€˜1â€™.\nFor each node, we get the value of every neighbor and concatenate it together with the node value\nWe take the hash of this value to set the new value of the node\nWe repeat the process until there is no further change in the distribution of the values (not the values themselves)\nIf the distribution of the values is the same for both graphs, H0 is rejected\n\nGNNs are, at most (which means that they can be worse) as powerful as a WL-Test on its ability to tell if two graphs are isomorphic\n\nk-Weisfeiler-Lehman Test\n\nFor a 2-WL Test:\n\nLet [K] be the set of nodes from K. Let now KÂ² be the set of tuples of size 2 comprised of every permutation of nodes from [K]\nRepeat the WL-Test, but with these 2-tuples instead of the nodes\nNeighbors are tuples with at least 2 common node with the original tuple"
  },
  {
    "objectID": "qmd/dl-image.html",
    "href": "qmd/dl-image.html",
    "title": "18Â  DL, Image",
    "section": "",
    "text": "TOC\n\nMisc\nData-Centric\nConvolutional Neural Network (CNN)\n\nMisc\n\nImproving your data will likely improve your model more than testing different algorithms or tuning more hyperparameters\n\nGood Data:\n\nis defined consistently (definition of labels, y, is unambiguous)\n\ne.g.Â consistent object labeling procedures\n\nhas coverage of important cases (good coverage of inputs, x)\n\ne.g.Â  samples of every combination of features the model will encounter\n\nhas timely feedback from production data (distribution covers data drift/concept drift)\nis sized appropriately\n\nExamples\n\nImage\n\nImprove consistency of labeling procedures\n\n\n(top-left) clear separation of boxes; (top-right)tail of left iguana included; (bottom) include all iguanas in one bounding box\n\nfyi 1 is probably best\n\nsolution: Improve procedure by making sure only 1 of these ways is used by labellers given this type of picture\n\n\nAudio\n\nYou discover you speech recognition model performs poorly when thereâ€™s car noise in the background\n\nsolution: get more training data with car noise in the background\n\n\n\nSteps\n\nTrain a model\nError analysis: investigate which types of data you model performs poorly on. The points the model performs really poorly on will be edge cases or errors in data collection, measurement, entry, etc.\nImprove these data\n\nFor edge cases, collect more data, augment your data, generate more data (simulation?) (i.e.Â change inputs of x)\nFor errors, make labeling procedure more consistent if some are found to be ambiguous (i.e.Â change the labels of y)\n\n\n\n\nAblation Study - a procedure where certain parts of the network are removed, in order to gain a better understanding of the networkâ€™s behaviour. Usually pertains to DL research\nTransfer Learning - Shortens time and resources required for training by using a feature representation technique over pre-trained models. These pre-trained models are generally trained using high-end computational resources and on massive datasets.\n\nMethods\n\nUsing the pre-trained weights and directly making predictions on the test data\nUsing the pre-trained weights for initialization and training the model using the custom dataset\nUsing only the architecture of the pre-trained network, and training it from scratch on the custom dataset\n\nPre-trained CNN Models\n\nVGG\nXception\nResNet\nInceptionV3\nInceptionResNet\nMobileNet\nDenseNet\nNasNet\nEfficientNet\nConvNEXT\n\nAll these pre-trained models can be loaded as keras models using the keras.application API\n\nSpecifications of these models and instantiation code are in this article\n\n\nStrong Baseline Model: ResNet/EffNet\n\nResNet18 and EffNet-B0 are small, quick models that are effective for nearly any type of image data.\nOnce youâ€™ve squeezed all the juice out of those, you can scale up to their bigger versions and almost always get better accuracy.\nAlso see this guide for suitable baseline models: link\n\n\nData-Centric\n\nImage (or Data) Boosting Notes from How I Won Andrew Ngâ€™s First Data-Centric AI Competition\n\nAugmented training images are used to get embeddings. Apply nearest neighbors to misclassified images with embeddings to get new data. Add to training data\nSteps\n\nGenerate a very large set of randomly augmented images from the training data (treat these as â€œcandidatesâ€ to source from).\nTrain an initial model and predict on the validation set.\nUse another pre-trained model to extract features (aka embeddings) from the validation images and augmented images.\nFor each misclassified validation image, retrieve the nearest neighbors (based on cosine similarity) from the set of augmented images using the extracted features. Add these nearest neighbor augmented images to the training set. I will call this procedure â€œData Boostingâ€.\nRetrain model with the added augmented images and predict on validation set.\nRepeat steps 4â€“6 until weâ€™ve reached the limit of 10K images.\n\nNotes\n\nAlthough augmented images were used for this competition, in practice any large set of images as candidates to source from.\nI generated ~1M randomly augmented images from the training set as candidates to source from\nThe data evaluation spreadsheet is used to keep track of inaccuracies (misclassified images) and to annotate the data. Alternatively, I also spun up an instance of Label Studio with a PostgreSQL backend but I decided not to use it for this competition due to the unnecessary overhead.\nFor the pre-trained model, I used ResNet50 trained on ImageNet.\nI used the Annoy package to perform approximate nearest neighbor search.\nThe number of nearest neighbors to retrieve per misclassified validation image is a hyper-parameter.\n\n\n\nConvolutional Neural Network (CNN) \n\nMisc\n\nNotes from Convolutional Neural Network (CNN) Architecture Explained in Plain English Using Simple Diagrams\n\nEspecially designed to work with images. They are widely used in the domain of computer vision\n\nTo use MLPs with images, we need to flatten the image. If we do so, spatial information (relationships between the nearby pixels) will be lost. So, accuracy will be reduced significantly. CNNs can retain spatial information as they take the images in the original format.\nCNNs can reduce the number of parameters in the network significantly. So, CNNs are parameter efficient.\n\nImages\n\ngrayscale image is represented as (height, width, 1) or simply (height, width) since the 3rd dim is 1\nRGB image is represented as (height, width, 3), where 3 is the number of color channels in the image (3D array)\n\nLayers\n\nConvolutional layers\n\nThe first convolutional layer takes the images as the input and begins to process\n\nThere can be multiple convolutional layers\nThe ReLU activation is used in each convolutional layer\n\nExtracts a set of features from the image while maintaining relationships between the nearby pixels\nComponents and Operations (fig: Gray scale, 1 Filter)\n\nConvolutional Operation is a partial matrix multiplication: 1st row (image) * 1st col (kernel) + 2nd row (image) * 2nd col (kernel) +â€¦\n\ne.g.Â (0*0 + 3*0 + 0*1) + (2*1 + 0*1 + 1*0) + (0*1 + 1*0 + 3*0) = 3\nExample is row*col but evidently it can also be row*row or col*col.Â Probably doesnâ€™t matter as long as itâ€™s consistent.\n\nKernel (aka Filter or Feature Detector)\n\nThere can be multiple filters in a single convolutional layer\n\nMultiple filters are used to identify a different set of features in the image\n\nThe number of filters increases in each convolutional layer\n\ne.g if we use 16 filters in the first convolutional layer, we usually use 32 filters in the next convolutional layer, and so on\n\nThe size of the filter and the number of filters should be specified by the user as hyperparameters.\nThe size should be smaller than the size of the input image.\nThe elements inside the filter define the filter configuration.\n\nImage section\n\nSection of the input image matrix that is multiply-summed by the kernel\nThe size of the image section should be equal to the size of the filter(s) we choose.\nWe can move the filter(s) vertically and horizontally on the input image to create different image sections.\nThe number of image sections depends on the stride we use.\n\nStride: The number of steps (pixels) that we shift the filter over the input image\n\nstride = 1 is moving the filter on the image horizontally by one step to the right (see 2nd row of images)\n\n\n\nFeature map\n\nOutput matrix from the Convolutional Operations\n\nStores the outputs of different convolution operations between different image sections and the filter(s)\nSize depends on the stride\n\nThe number of elements in the feature map is equal to the number of different image sections that we obtained by moving the filter(s) on the image.\nInput for the next pooling layer.\n\nImage Input is the input matrix\n\nPadding adds additional pixels with zero values to each side of the image which helps to keep the feature map of the same size as the input\n\nIf there are several convolutional layers in the CNN, the size of the feature map will be further reduced at the end so that we cannot do other operations on the feature map\nExample: new size of the input image is (8,8)\n\nIf we do the convolution operation now with Stride=1, we get a feature map of size (6x6) that is equal to the size of the original image before applying Padding\n\n\n\nExample: Gray Scale, Multiple Filters\n\nFeature map is now an array\n\nExample: RGB image\n\nBecause a RGB image has 3 color channels, 3-channel filters are needed to do the calculations\nfinal result (i.e.Â for one cell in the feature map) is obtained by adding all outputs of each channelâ€™s convolution operation calculations\n\nExample: RGB, mulitple filters\n\nFeature map is now an array\n\n\n\nPooling layers\n\nConvolution and pooling layers are used together as pairs (i.e.Â one is followed by the other)\nObjectives\n\nExtract the most important (relevant) features by getting the maximum number or averaging the numbers.\nReduce the dimensionality (number of pixels) of the output returned from previous convolutional layers.\nReduce the number of parameters in the network.\nRemove any noise present in the features extracted by previous convolutional layers.\nIncrease the accuracy of CNNs.\n\nComponents and Operations (max pooling, stride = 2)\n\nThe Pooling Operation happens between a section of the feature map and the filter. It outputs the pooled feature map\n\nFor RGB, pooling operations will be done on each channel independently.\nTypes\n\nMax pooling: Get the maximum value in the feature map section where the filter is applied.\nAverage pooling: Get the average of the values in the feature map section where the filter is applied.\nglobal-average-pooling: similar to average-pooling but instead of using NÃ—N patches of the feature maps, it uses all feature maps area at once\n\n\nFilter: empty so no transformation, it only sets the size of the image section where the pooling operation takes place\n\nSize of the filter should be specified by the user as a hyperparameter.\nSize should be smaller than the size of the feature map\nIf the feature map has multiple channels, we should use a filter with the same number of channels\n\nFeature Map: Input to the pooling layer\n\nSize of the feature map sections should be equal to the size of the filter we choose\nThe number of sections depends on the Stride\n\nPooled Feature Map: Output of the pooling layer\n\nInput for the next convolution layer (if any) or for the flatten operation.\nFor RGB, there are same number of channels in the feature map and the pooled feature map (see examples in Covolutional layers section)\n\nOther hyperparameters\n\nStride: The Stride is usually equal to the size of the filter. If the filter size is (2x2), we use Stride=2. (also see Convolutional layers section)\nPadding: Padding is applied to the feature map to adjust the size of the pooled feature map (also see Convolutional layers section)\n\nThe Flatten Operation flattens the pooled feature map output to a single column so that it can be fed the Multiayer Perceptron (MLP) and the image can be classified\n\nOccurs after the final pooling layer is completed\nUnlike flattening the original image (i.e if a MLP was used to classify an image), important pixel dependencies are retained when pooled maps are flattened.\nExample: Grayscale\nExample: RGB\n\n\n\nFully connected (dense) layers\n\nLayers of the MLP thatâ€™s used to classify the image\nInput is the flattened, pooled feature map\nThe ReLU activation is used in each fully connected layer except in the final layer in which we use the Softmax activation for multiclass classification.\n\n\nArchitectures\n\nAlexNet\n\n5 convolution layers with non-increasing kernel sizes\n\nLRN is applied on the first and second convolution layers after applying ReLU\n\n3 fully connected layers\n\nReLU activation in the hidden layers\nsoftmax activation for the output layer"
  },
  {
    "objectID": "qmd/dl-tabular.html#sec-dl-tab-misc",
    "href": "qmd/dl-tabular.html#sec-dl-tab-misc",
    "title": "19Â  Tabular",
    "section": "19.1 Misc",
    "text": "19.1 Misc\n\nResources\n\nRaschkaâ€™s list of DL tabular papers\n\nSummaries, links to code if available\n\n\nGuide for suitable baseline models: link\nQuestions that I should be able to answer\n\nHow to manage the convergence mechanism in the training process?\nHow to apply transfer learning on a pre-trained network?\nHow to minimize redundant computation?\nHow to reduce the sensitivity of a deep learning technique?\n\nNumerics\n\nneural networks treat numerical inputs as continuous variables. Meaning:\n\nhigher numbers are â€œgreater thanâ€ lower numbers\nnumbers that are similar are treated as being similar items.\n\nOkay for a variable like â€œageâ€ but is nonsensical when the numbers represent a categorical variable (embeddings solve the categorical encoding problem)\n\n\n\nAblation Study - a procedure where certain parts of the network are removed, in order to gain a better understanding of the networkâ€™s behaviour. Usually pertains to DL research."
  },
  {
    "objectID": "qmd/dl-tabular.html#sec-dl-tab-preproc",
    "href": "qmd/dl-tabular.html#sec-dl-tab-preproc",
    "title": "19Â  Tabular",
    "section": "19.2 Preprocessing",
    "text": "19.2 Preprocessing\n\nContinuous\n\nStandardize\nBin\n\na network with discretized features may have an advantage because it doesnâ€™t have to spend any of its parameter budget learning to partition the input space\nâ€œGradient Boosted Decision Tree Neural Networkâ€ (paper) and Uber (See Uber ETA below) found quantile buckets provided better accuracy than equal-width buckets\n\nPotential reason: â€œmaximized entropy: for any fixed number of buckets, quantile buckets convey the most information (in bits) about the original feature value compared to any other bucketing schemeâ€\n\n\nLogging can create more compact ranges, which then enables more efficient neural network training"
  },
  {
    "objectID": "qmd/dl-tabular.html#sec-dl-tab-papsum",
    "href": "qmd/dl-tabular.html#sec-dl-tab-papsum",
    "title": "19Â  Tabular",
    "section": "19.3 Paper Summaries",
    "text": "19.3 Paper Summaries\n\nFrom pytorch-widedeep, deep learning for tabular data IV: Deep Learning vs LightGBM\n\nExperiment compared LightGBM performance vs a bunch of tabular data designed DL algorithms from pytorch-widedeep LIB (including TabNet)\n\nLightGBM destroyed them\n\nâ€œthe DL algorithm that achieves similar performance to that ofÂ LightGBMÂ is a simple Multilayer Perceptron (MLP)â€\nâ€œIn my experience, DL models on tabular data perform best on sizeable dataset that involve many categorical features and these have many categories themselves.â€\nCases where DL models can complement ML models for tabular data\n\nUsing the categorical feature embeddings from DL models as features in other models\n\nI donâ€™t get this. This article makes it sound like theyâ€™re an artefact of the DL model itself. I just always thought this was a preprocessing step.\n\nI wonder how these are produced and extracted from the model.\n\nâ€œthe embeddings acquire a more significant value, i.e.Â we learn representations of those categorical features that encode relationships with all other features and also the target for a specific dataset. Note that this does not happen when using GBMs. Even if one used target encoding, in reality there is not much of a learning element there (still useful of course).â€\nâ€œAssume that you have a dataset with metadata for thousands of brands and prices for their corresponding products. Your task is to predict how the price changes over time (i.e.Â forecasting price). The embeddings for the categorical featureÂ brandÂ will give you information about how a particular brand relates to the rest of the columns in the dataset and the target (price). In other words, if given a brand you find the closest brands as defined by embeddings proximity you would beâ€naturallyâ€ and directly finding competitors within a given space (assuming that the dataset is representative of the market).â€\n\nImprove performance on a small dataset by â€œtransferringâ€(?) whats learned from using a DL model on a similar much larger dataset\n\nThe transferring comes from Transfer learning which I have no idea how it works.\nâ€œAssume you have a large dataset for a given problem in one country but a much smaller dataset for the exact same problem in another country. Letâ€™s also assume that the datasets are, column-wise, rather similar. One could train a DL model using the large dataset andâ€transfer the learningsâ€ to the second, much smaller dataset with the hope of obtaining a much higher performance than just using that small dataset alone.â€\n\n\n\n(Raschka summary) On Embeddings for Numerical Features in Tabular Deep Learning (paper, code)\n\nInstead of designing new architectures for end-to-end learning, the authors focus on embedding methods for tabular data: (1) a piecewise linear encoding of scalar values and (2) periodic activation-based embeddings. Experiments show that the embeddings are not only beneficial for transformers but other methods as well â€“ multilayer perceptrons are competitive to transformers when trained on the proposed embeddings.\nUsing the proposed embeddings, ResNet, multilayer perceptrons, and transformers outperform CatBoost and XGBoost on several (but not all) datasets.\nSmall caveat: I would have liked to see a control experiment where the authors trained CatBoost and XGboost on the proposed embeddings.\n\nÂ  (Raschka summary) Why do tree-based models still outperform deep learning on tabular data? (paper)\n\nThe main takeaway is that tree-based models (random forests and XGBoost) outperform deep learning methods for tabular data on medium-sized datasets (10k training examples). The gap between tree-based models and deep learning becomes narrower as the dataset size increases (here: 10k -&gt; 50k).\nSolid experiments and thorough investigation into the role of uninformative features: uninformative features harm deep learning methods more than tree-based methods.\nSmall caveats: some of the recent tabular methods for deep learning were not considered; â€œlargeâ€ datasets are only 50k training examples (small in many industry domains.)\nExperiments based on 45 tabular datasets; numerical and mixed numerical-categorical; classification and regression datasets; 10k training examples with balanced classes for main experiments; 50k datasets for â€œlargeâ€ dataset experiments."
  },
  {
    "objectID": "qmd/dl-tabular.html#sec-dl-tab-arch",
    "href": "qmd/dl-tabular.html#sec-dl-tab-arch",
    "title": "19Â  Tabular",
    "section": "19.4 Architectures",
    "text": "19.4 Architectures\n\nDeepETA: Uberâ€™s ETA model \n\nModel for residual calculation\n\nEncoder-Decoder architecture with self-attention (article)\n\nTransformer type of architecture (Also see NLP, Transformers)\n\nProcessing\n\nContinuous features were quantile binned. Then both binned numerics and categoricals are embedded.\nLatitude and longitude was binned and multi-feature hashed (See Feature Engineering, Geospatial)\n\nSelf-attention in transformers is a sequence-to-sequence operation that takes in a sequence of vectors and produces a reweighted sequence of vectors\n\ny is the outcome, x is the predictor, K is the number of features\nAttention matrix calculation has quadratic complexity, O(K2d) (I think d is the number of rows).\n\nFaster alternatives that linearize the self-attention calculation: linear transformer, linformer, performer\n\nlinear transformerâ€™s time complexity is O(Kd2) and uses kernel trick to bypass attention matrix calculation\n\nIf K &gt; d, then the linear transformer is faster\n\n\n\n\nUtilizes feature sparsity for speed (any one prediction touches only about 0.25% of the modelâ€™s parameters)\n\nâ€œHandfulâ€ of layers\n\nMost parameters are in embedding lookup tables\n\nDiscretizing numerics\nMulti-feature hashing\n\nDeepETA simply quantizes the coordinates and performs a hash lookup, which takes O(1) time.\n\nby precomputing partial answers in the form of large embedding tables learned during training, we reduce the amount of computation needed at serving time.\n\nIn comparison, storing embeddings in a tree data structure would require O(log N) lookup time, while using fully-connected layers to learn the same mapping would require O(N2) lookup time\n\n\nDecoder is a fully connected neural network with a segment bias adjustment layer\n\nBias adjustment layers improve raw predictions when thereâ€™s a lot of variance in the outcome between groups\n\ne.g.Â distribution of absolute errors varies by a lot across delivery trips vs rides trips, long vs short trips, pick-up vs drop-off trips, and also across global mega-regions\nOther approaches\n\nAdding group features - outperformed by the bias adj layer\nmulti-task decoder - didnâ€™t meet latency requirements\n\n\nReLU at output to force predicted ETA to be positive;\nClamping to reduce the effect of extreme values"
  },
  {
    "objectID": "qmd/dl-training.html",
    "href": "qmd/dl-training.html",
    "title": "18Â  DL, Training",
    "section": "",
    "text": "TOC\nMisc\n\nUse early stopping and set the number of iterations arbitrarily high. This removes the chance of terminating learning too early.\nStrategies\n\nIncludes Active Learning, Transfer Learning, Few-Shot Learning, Weakly-Supervised Learning, Self-Supervised Learning, Semi-Supervised Learning\n\nCommon Variations of Gradient Descent\n\nBatch Gradient Descent: Use the entire training dataset to compute the gradient of the loss function. This is the most robust but not computationally feasible for large datasets.\nStochastic Gradient Descent: Use a single data point to compute the gradient of the loss function. This method is the quickest but the estimate can be noisy and the convergence path slow.\nMini-Batch Gradient Descent: Use a subset of the training dataset to compute the gradient of the loss function. The size of the batches varies and depends on the size of the dataset. This is the best of both worlds of both batch and stochastic gradient descent.\n\nUse the largest batch sizes possible that can fit inside your computer's GPU RAM as they parallelize the computation\n\n\nGPU methods\nData Storage\n\nData samples are iteratively loaded from a storage location and fed into the DL model\n\nThe speed of each training step and, by extension, the overall time to model convergence, is directly impacted by the speed at which the data samples can be loaded from storage\n\nWhich metrics are important depends on the application\n\nTime to first sample might be important in a video application\nAverage sequential read time and Total processing time would be important for DL models\n\nFactors\n\nData location: The location of the data and its distance from the training machine can impact the latency of the data loading.\nBandwidth: The bandwidth on the channel of communication between the storage location and the training machine will determine the maximum speed at which data can be pulled.\nSample size: The size of each individual data sample will impact the number of overall bytes that need to be transported.\nCompression: Note that while compressing your data will reduce the size of the sample data, it will also add a decompression step on the training instance.\nData format: The format in which the samples are stored can have a direct impact on the overhead of data loading.\nFile sizes: The sizes of the files that make up the dataset can impact the number of pulls from the data storage. The sizes can be controlled by the number of samples that are grouped together into files.\nSoftware stack: Software utilities exhibit different performance behaviors when pulling data from storage. Among other factors, these behaviors are determined by how efficient the system resources are utilized.\n\nMetrics\n\nTime to first sample â€” how much time does it take until the first sample in the file is read.\nAverage sequential read time â€” what is the average read time per sample when iterating sequentially over all of the samples.\nTotal processing time â€” what is the total processing time of the entire data file.\nAverage random read timeâ€” what is the average read time when reading samples at arbitrary offsets.\n\nReading from S3 (article)"
  },
  {
    "objectID": "qmd/docker-aws.html#sec-docker-aws-misc",
    "href": "qmd/docker-aws.html#sec-docker-aws-misc",
    "title": "AWS",
    "section": "Misc",
    "text": "Misc\n\nNotes from Linkedin Learning Docker on AWS\n\nThe example used in this class is for a web server"
  },
  {
    "objectID": "qmd/docker-aws.html#sec-docker-aws-summ",
    "href": "qmd/docker-aws.html#sec-docker-aws-summ",
    "title": "AWS",
    "section": "Summary",
    "text": "Summary\n\nPush docker image to ECR\nPush app code and build instructions (buildspec yaml) to CodeCommit\nCreate CodeBuild project that executes image building instructions\nCreate a Pipeline that triggers CodeBuild (automatic image build when new code is committed)\nChoose a Cluster method (fargate or manual EC2), then create and start cluster instances\n\nonly able to specify number of instances to create with the EC2 method\nfargate handles most of the configuration (cost extra?)\n\nCreate a task or a service\n\nA task is for short running jobs, no load balancer or autoscaling. Its definition details the container configuration; how much of the resources you want your workloads (e.g.Â app) to be able to use; communication between containers, etc.\nA service is for long running jobs. Creates tasks and autoscales number of instances and load balances trafficÂ \n\nAdd a storage container and update task definition to include a shared volume"
  },
  {
    "objectID": "qmd/docker-aws.html#sec-docker-aws-gloss",
    "href": "qmd/docker-aws.html#sec-docker-aws-gloss",
    "title": "AWS",
    "section": "Glossary of AWS Services Used",
    "text": "Glossary of AWS Services Used\n\nECR stores your images that you build\nCodeCommit (CC) is like a github (code storage)\nCodeBuild sets up the process of using a yaml file in your CC repo as instructions to build the images\nPipeline is the CI/CD part. Triggers an image build every time thereâ€™s a new push to CodeCommitÂ \nRoute 53 takes your domain name (www.store.com/app), creates a DNS ip address and reroutes traffic from that domain to your load balancer.\nContainer Networking Models\n\nHost - direct mapping to host networking (EC2)\n\nuse when performance is prime concern\nonly 1 container per task per port\n\nAwsvpc - ENI per task, required for fargate\n\nmost flexibility\nrecommended for most use cases\n\nBridge - â€œClassicâ€ docker networking\n\ndidnâ€™t get discussed\n\nNone - multi-container localhost and storage\n\nonly local connectivity (i.e.Â communication between containers within a task)\n\nSecurtiy groups available for Host and AWSvpc\n\nsecurity groups allow for tracking container performance and limiting access"
  },
  {
    "objectID": "qmd/docker-aws.html#sec-docker-aws-meths",
    "href": "qmd/docker-aws.html#sec-docker-aws-meths",
    "title": "AWS",
    "section": "Two Methods For Running Containers on AWS",
    "text": "Two Methods For Running Containers on AWS\n\nManaging the EC2 instances yourself (see section below for set-up instructions)\n\nif you understand the capacity you need and want greater control, this might be better\nYou pay for unused capacity\nBilling is like the standard billing for using an EC2 instanceÂ \n\nFargate (see section below for set-up instructions)\n\nManaged by Amazon, less control, less to deal with\nYou donâ€™t have to deal with starting, stopping, choosing compute sizes, capacities etc. of EC2 instances\nBilled by CPU/Memory and Time thatâ€™s used by your container"
  },
  {
    "objectID": "qmd/docker-aws.html#sec-docker-aws-ecr",
    "href": "qmd/docker-aws.html#sec-docker-aws-ecr",
    "title": "AWS",
    "section": "Elastic Container Registry (ECR)",
    "text": "Elastic Container Registry (ECR)\n\nCreate an ECR account\n\nLog into your account\nsearch ecr\nClick create â€œget startedâ€ under Create a Repository (mid right)\n\nSays you pay for the amount of data you store in the repository and data transferred to the internet.\n\nReason for doing this is latency. The repo is regional and you want your image/app to be close to the host\n\n\nAssign a name\n\nfirst part is a hash + region + amazon.com\nyou add a name. whatever you want\n\nmutable/immutable\n\nIf youâ€™re going to be storing multiple versions of the same image, you should choose mutable.\n\nClick create repository (bottom right)\n\nPush image to ECR repo\n\ncopy the URI for your repo from the ecr console (ecr â€“ left panel â€“ repositories â€“ images)\n\nsave it to registry-tag.txt file in your local image directory\nAlso include it as the tag to your docker image\n\ndocker build . -t &lt;URI&gt;\n\nauto-appends â€œ:latestâ€\n\n\n\nIn terminal\n\n(aws ecr get-login --no-include-email --region &lt;region&gt;}\n\n*with parentheses\nregion is whatever you have in your profile e.g.Â us-east-2\ngets login from the aws profile youâ€™ve already set-up\nprints some kind of warning, he didnâ€™t act like it was meaningful\n\ndocker push &lt;tag&gt;\n\nwhich is your URI:\n\nin the example, the version is â€œlatestâ€\n\n\nIf something doesnâ€™t work, the instructions are at aws\n\nIn repository console\n\nclick repo name\nclick view push commands (top right)\nshows how to connect to repository and â€œpushâ€ instance\n\n\n\nIn console, hit refresh (mid-right) to see that the image is loaded into the repo"
  },
  {
    "objectID": "qmd/docker-aws.html#sec-docker-aws-codcom",
    "href": "qmd/docker-aws.html#sec-docker-aws-codcom",
    "title": "AWS",
    "section": "CodeCommit",
    "text": "CodeCommit\n\nCreate a CodeCommit git repository - benefit is having (image/app) code live near hosting service, less latency for CI/CD processes\n\nDeveloper tools â€“ CodeCommit â€“ (maybe left panel â€“ Source â€“ Repositories)\nClick create repository (top right)\n\nEnter name\n\nDoesnâ€™t have to match the name of the image repo, but might be worth doing\nalso a box for entering a description\n\nClick create\n\nConnection Steps\n\nhttps or ssh\n\nclick ssh\n\nfollow these directions to gitbash and create SSH keys for windows, enter them into config file, clone repository, etc. etc.\n\nhttps://docs.aws.amazon.com/codecommit/latest/userguide/setting-up-ssh-windows.html\nHe cloned the repo, one directory above the .ssh directory\n\n\n\nPush Container Code to CodeCommit repo\n\ncd to cloned repo directory\ncopy code files to that directory\ngit add *, git commit -m â€œblah blahâ€, git push -u origin master\n\n-u is for upstream\nâ€œ-u origin masterâ€Â  necessary for a first push\n\nFiles should be present in developer tools â€“ CodeCommit â€“ Source â€“ Repositories â€“ repo"
  },
  {
    "objectID": "qmd/docker-aws.html#sec-docker-aws-codcomproj",
    "href": "qmd/docker-aws.html#sec-docker-aws-codcomproj",
    "title": "AWS",
    "section": "Create a CodeBuild Project",
    "text": "Create a CodeBuild Project\n\nBuild the CodeCommit repo into a docker container\nbuildspec.yml (see hostname folder in exercise files)\n\nyaml script that automates building docker, logging into ECR, building an image, and pushing it to ECR\ncodebuild version used was 2.0 (which is at the top of the yaml script)\n\ncodebuild must be some aws tool you can use to do this\n\nadd, commit, push to CodeCommit repo\n\ndeveloper tools â€” codecommit â€“ left panel â€“ build â€“ build projects\n\nclick create build project (upper right)\n\nanything not listed below, just used defaults\n\nenter project name\n\nhe gave same name as CC repo\n\nUnder Source, make sure it says CodeCommit, enter repo name in box\nMake sure Manage Image box is ticked\nOperating System\n\nhe used Ubuntu\n\nRuntime\n\nselect Standard\n\nImage\n\nstandard 2.0\n\nPriviledged\n\ntick box â€œEnable this flag if you want to build Docker images or want your builds to have elevated priviledgesâ€\n\nLogs\n\nUsing cloudwatch\n\ngroup name - codebuild\nStream Name\n\nhe used the name of the CC repo\n\n\n\nClick Create build project (bottom right)\n\nGoto IAM console â€“ left panel â€“ Roles\n\nWhen the â€œbuild projectâ€ was created a role was also created\n\nUnder Role name -Â  click â€œcodebuild--service-roleâ€\nclick attach policy (mid left)\nSearch for AmazonEC2ContainerRegistryPowerUser\ntick box to select it\nclick attach policy (bottom right)\n\n\nGoto Developer tools â€“ CodeBuild â€“ left panel â€“ Build â€“ Build Project â€“ project name\n\nClick Start Build (top right)\nkeep all defaults, Click Start Build (bottom right)\n\nProject builds and under Build Status, status should say â€œsucceededâ€ when it finishes\n\nWhich means there are now two images in the ECR repo\n\noriginal push and image built from this project build process (duplicate)\n\n\nAutomate building container when new code is pushed (CI/CD)\n\ndeveloper tools â€“ codebuild â€“ left panel â€“ pipeline â€“ pipelines\nclick create pipeline\n\nenter pipeline name\n\nhe named it the CC repo name, hostname\nclick next\n\nAdd source stage\n\nchoices\n\nCodeCommit\nECR\nS3\nGithub\nChoose codecommit\n\nSelect repo name\n\nexample: â€œhostnameâ€\n\nSelect branch\n\nexample â€œmasterâ€\n\nDetection option\n\nselect CloudWatch\n\nClick next\n\nAdd build stage\n\nCodeBuild or Jenkins\n\nchoose CodeBuild\n\nRegion\n\nexample US East - (Ohio)\n\nProject Name\n\nname of the build project from last section\nexample hostname\n\nClick next\n\nAdd deploy stage\n\nskipped, because something I didnâ€™t understand. Sound like another level of automation that might be used in the future\nclick skip deploy stage\n\nReview\n\nClick create pipeline (bottom right)\n\n\nOnce created, it will start building the pipeline from the CodeCommit source\n\nProcess takes a few minutes\ndetects the buildspec.yml in CC and executes it\nunder Build Section, there will be a details link, you can right-click and open it in a new tab\n\nShould result in a 3rd image (duplicate images) in the ECR repo\n\nSo anytime a new commit is pushed to CodeCommit, an image will be built and stored in ECR"
  },
  {
    "objectID": "qmd/docker-aws.html#sec-docker-aws-ec2user",
    "href": "qmd/docker-aws.html#sec-docker-aws-ec2user",
    "title": "AWS",
    "section": "Create Cluster: EC2 (User-Managed)",
    "text": "Create Cluster: EC2 (User-Managed)\n\nCreate Cluster: Set-up instructions for running containers using EC2 method\n\nSearch for ECS\nleft panel â€“ Under Amazon ECS: Clusters\n\nClick create cluster\nChoose Linux + Networking\n\nWindows + Networking and Networking-only (Fargate see below) options also available\nclick next (bottom right)\n\nConfigure Cluster\n\nEnter Cluster name\n\nexample: ecs-ec2\n\nProvisioning\n\nOn demand instance\nspot instance\n\nEC2 instance type (size of compute)\n\nexample: t2.medium\n\nNumber of instances\n\nhe chose 1\n\nEC2 AMI id\n\nLinux-1, linux-2\n\nhe chose linux-2; didnâ€™t give a reason\n\n\nDefaults kept for Virtual Private Cloud (VPC), Security Group, storage, etc.\nCloudWatch container insights\n\ntick enable container insights\nso you can monitor stats in Cloudwatch and help you tune compute resources in the future\n\nClick create\n\ntakes a minute or two to spin up the instance"
  },
  {
    "objectID": "qmd/docker-aws.html#sec-docker-aws-ec2ssh",
    "href": "qmd/docker-aws.html#sec-docker-aws-ec2ssh",
    "title": "AWS",
    "section": "Create Cluster: EC2 with SSH Access",
    "text": "Create Cluster: EC2 with SSH Access\n\nCreate Cluster using EC2 method with SSH access (not possible with a Fargate cluster) and connect to it\nSteps\n\nFind your ssh public key\n\ngo into git bash and type â€œcat ~/.ssh/id_ed25519.pubâ€\n\noptions\n\nid_rsa.pub\nid_ecdsa.pub\nid_ed25519.pub\n\nI have 2, rsa that I created when linking rstudio to github and ed25519 when I created gitlab acct\n\nCopy everything (including the ssh-filename beginning part) all the way until your email (donâ€™t include)\n\nGoto EC2 services page (open new tab)\n\nUnder Resources (mid), click Key Pairs\n\nClick import\npaste key into Public Key Contents box\nenter a name\n\nexample ecs-ec2-key\n\nclick import\n\n\nGo back to the ECS services page and create another cluster\n\nSame as before. (create cluster - EC2 method above) except:\n\ncluster name - ecs-ec2-ssh\nkey pair - chose newly imported key pair\nNetworking\n\nvpc\n\ndrop down\n\nchoose vpc created by prev. cluster (some big long hash)\n\n\nsubnets\n\ndropdown\n\nchoose subnet created by prev. cluster\nspawns another dropdown to add another subnet\n\ndropdown\n\nchoose second subnet created by prev.cluster\n\nShould only be 2, theyâ€™re names should gray-out after you choose them\n\nsecurity group\n\nchoose the one created by the prev. cluster\n\n\nHaving SSH available will allow us to go into the container and view docker ressource\n\n\nCopy public ip address and open SSH port (also see AWS notebook â€“ EC2 â€“ connect/terminate instance)\n\nClick on Cluster name\nclick on ECS instances tab (mid left)\nright-click EC2 Instance id and open in new tab\n\ncopy IPv4 Public IP (lower right)\nclick security group link (lower left)\n\nclick inbound tab (lower left)\nclick edit\n\nclick add rule\nunder Type, click dropdown and select SSH\n\nautomatically chooses port 22\n\nSource\n\nkept 0.0.0.0Â  (â€œv4 addressâ€, guess 4 is for the 4 numbers in the address)\n\nDescription\n\nkept default\n\nclick save\n\n\n\n\nOpen terminal\n\nssh -i ~/.ssh/id_rsa ec2-user@\n\nasks if youâ€™re sure, say yes\n\nCheck container status on instance\n\nsudu su -\n\nswitches to being a root user\n\ndocker ps\n\nshows container id and name, image, status, etc.\n\n\nexec into container\n\ndocker exec -it  sh\n\nonly works if linux image has a shell environment\ninstead of sh, can try bash\nor docker runÂ  â€“rm â€“name linux -it alpine:latest sh\nctrl + d\n\nleave shell\n\nexit\n\nto exit as root user\n\nexit\n\nleaves instance, closes connection"
  },
  {
    "objectID": "qmd/docker-aws.html#sec-docker-aws-farg",
    "href": "qmd/docker-aws.html#sec-docker-aws-farg",
    "title": "AWS",
    "section": "Create Cluster: Fargate (AWS-Managed)",
    "text": "Create Cluster: Fargate (AWS-Managed)\n\nSet-up instructions for running containers using Fargate method\n\nSearch for ECS\nleft panel â€“ Under Amazon ECS: Clusters\n\nClick create cluster\n\nChoose Network-only (amazon fargate)\nEnter Cluster name\n\nexample ecs-fargate\n\ntick box for Enable Container insights (cloudwatch)\nclick create (bottom right)\n\n\nCluster created instantaneously\n\nclick view cluster"
  },
  {
    "objectID": "qmd/docker-aws.html#sec-docker-aws-catd",
    "href": "qmd/docker-aws.html#sec-docker-aws-catd",
    "title": "AWS",
    "section": "Creating a Task Defintion",
    "text": "Creating a Task Defintion\n\nIf using load balancer goto the â€œcreate appplication load balancerâ€ andÂ  â€œadd ecs service and taskâ€ below\ndetails the images to use, the CPU and memory to allocate, environment variables, ports to expose, and how the containers interact.\n\nassociates the cluster created in the previous section with the app or workload\n\n1 task definition can be used in multiple containers\nsearch for ECS\nleft panel â€“ Clusters â€“ task definitions\n\nclick create new task definition\nselect cluster method (Fargate or EC2)\n\nchoose fargate\nclick next step\n\ncreate task-definition name\n\neg hostname-fargate\n\ntask role\n\nused if workload creates other resources inside aws\nleave blank\nsome kind of warning about the network settings, he ignored it.\n\ntask execution iam role\n\ngives permission for cluster to use task defintion\nkeep default\n\ntask size\n\nmemory size choice effects available choices for cpu\ndepends on your application needs, if running multiple containers with this definition, etc.\n\nhe chose the smallest for each just because this is for illustrative purposes\n\n\ncontainer definitions\n\nassigns which containers will be using this definition\nclick add container\n\ncontainer name\n\nwhatever you want, he chose hostname\n\nimage\n\ngoto services (top left) (open new tab) â€“ left panel â€“ ecr â€“ left panel â€“ repositories\n\nclick image repo name\ncopy image uri that you want to associate with the definition\ngo back to the task definitions tab\n\npaste uri into the box\n\nIf you want to always use the latest image and your uri has build version tag, replace the build tag with â€œlatestâ€\n\nbuild tag starts at â€œbuildâ€ and goes to the end of the uri\n\n\n\nauthentication only necessary if ecr repo is private\nsoft memory limit\n\nspecify a memory limit for the container\nleaving blank says only limit will be the memory size of the task definition (see 6.)\n\nport mappings\n\nwhatever is specified in dockerfile/image\nexample nginx image exposes 80 tcp\n\nAdvanced options\n\nhealthcheck\n\nsome cli code that allows you to check if your container is running properly at the container level\nhe already has this in his buildspec.yml code (see create CodeBuild project section)\n\nhealthcheckz is a check at the system level\n\n\nEnvironment, network settings, volumes\n\nseems like a bunch of stuff that would be used in a docker run command or in a docker-compose file\nall left blank\n\n\nclick add\n\n\nvolumes\n\nexternal volume to be shared\nleft blank\n\nclick create\n\nTakes you to launch status\n\nclick view task definition"
  },
  {
    "objectID": "qmd/docker-aws.html#sec-docker-aws-utdtsdvbc",
    "href": "qmd/docker-aws.html#sec-docker-aws-utdtsdvbc",
    "title": "AWS",
    "section": "Update Task Definition to Share Data Volumes Between Containers",
    "text": "Update Task Definition to Share Data Volumes Between Containers\n\nFargate cluster example\n\nAlso see the Data Volumes, Sharing Data between containers, Docker-Compose sections of part 1 of this note\n\nthink a lot of what happens in those sections is automated by using the this task definition\n\nSearch ECS â€“ Left panel â€“ Task Definitions\n\nclick on fargate task definition\n\nclick on latest revision of the definition\n\ndefinitions are versioned\nclick on create new revision\n\nscroll down to click on add container\n\ncontainer name\n\nwhatever, fargate-storage (he called his hostname-v2)\n\nimage\n\nadd image uri (see creating task definition above)\nhe used the same image as the first container. This becomes a problem because he has two containers using the same port since both nginx containers are using 80. See troubleshooting section below. Also mentioned in part 1 â€“ running containers â€“ flags â€“ p\nThink for data science weâ€™d use a postgressql, redis, etc. image\n\nEnvironment\n\nThink this was for display purposes. He added one just so when he went to the webpage and it displayed the container names, we could tell the difference. The first container said version 1 and this one says version two.\nenvironment variables\n\nkey\n\nexample VERSION\n\nvalue\n\nexample versionTwo\n\n\n\nclick add\n\nVolumes\n\nclick add volume\n\nname\n\nwhatever\nexample shared-volume\n\nclick add\n\n\nGO BACK to container section\n\nDo this for each container: click on the container name\n\nStorage and Logging\n\nmount points\n\nsource volume\n\nclick dropdown and select volume name\n\nexample from above: shared-volume\n\n\ncontainer path\n\nhe added the shared folder path and it was the same for both containers\nsee part 1 Data Volumes and Sharing Data between containers sections\n\nI think using that example, weâ€™d specifyÂ â€œ/app/public/â€ (no quotes) for the app container. *** This dude said to add a trailing â€œ/â€ to the paths ***\nfor storage container, example redis, itâ€™d be â€œ/data/â€ which is designated by the redis image authors.\n\n\n\n\nclick update\n\n\nClick create\n\nNote the revision number thatâ€™s given\n\n\n\nleft panel â€“ Clusters\n\nclick on fargate cluster\n\nclick services tab (mid left)\n\nclick service name (example is hostname) using the task definition\n\nThis was created in the Add ECS service and task section below\nclick update button (top right)\n\nConfigure Service\n\nTask Definition\n\nrevision\n\nselect revision number of the updated definition\n\n\nclick next\n\nclick next all the way to review\n\nclick update service\n\nclick view service\n\n\n\nclick tasks tab (mid left)\n\nrefresh (mid right) and watch new task start up with â€œprovisioningâ€ status and then â€œrunningâ€\n\n\n\ncan go to ip address with volume path appended to the address to see that the volumes are up and running\n\nnot sure if this would work with a database example or not"
  },
  {
    "objectID": "qmd/docker-aws.html#sec-docker-aws-ratwaad",
    "href": "qmd/docker-aws.html#sec-docker-aws-ratwaad",
    "title": "AWS",
    "section": "Running a Task with an Available Definition",
    "text": "Running a Task with an Available Definition\n\nleft panel â€“ Clusters\n\nClick Cluster your using for the task definition\n\nhe used the fargate one he created\n\nclick tasks tab (mid left)\nclick run new task\n\ntick fargate launch type\ncluster vpc,Â subnets\n\nclick dropdown boxes\nitâ€™ll show the ones that were made during cluster creation\n\nchoose vpc and both subnets\n\n\nSecurity group\n\ncreates one for you with default rules which you can keep\n\nAlso can manipulate after created by going to EC2 â€“ left panel â€“ Network and Security â€“ Security Groups\n\nOr click edit button to specify ports, choose existing security group, etc\n\nadd additional port\n\ntype\n\nselect custom with tcp protocol\n\nport range\n\n81-90\ncontainer must be configured to be able to listen on the range of ports\n\nSource\n\ncan choose a group that allows you to connect with other tasks in the environment and limit access\nhe kept Anywhere\n\nclick save\n\n\n\nclick run task (bottom right)\n\nclick the task hash under Task column\n\nat the bottom, you can watch the status turn from â€œpendingâ€ to â€œrunningâ€\n\nrefresh button (right)\n\ncopy the public ip under Network section\n\npaste into address bar + port\n\nexample 3.15.13.43:80\nexample: for his nginx server, it just displayed the private ip and the image name\n\n\nSimple way for a minor scale up the access to the application is to duplicate the task definition (also see autoscaling section below)\n\nleft panel â€“ Clusters\n\nselect the same cluster\nclick tasks tab again\nclick task hash id again\n\nclick â€œrun more like thisâ€ (top right)\n\ntick fargate again\nselect the same vpc and subnets again\nclick run task\n\nget the public ip the same way as before\nnow two ips are available for the app"
  },
  {
    "objectID": "qmd/docker-aws.html#sec-docker-aws-calb",
    "href": "qmd/docker-aws.html#sec-docker-aws-calb",
    "title": "AWS",
    "section": "Create Application Load Balancer (ALB)",
    "text": "Create Application Load Balancer (ALB)\n\nAlso see AWS &gt;&gt; EC2 &gt;&gt; Configure Load Balancer and Application Ports\nsearch ec2\nleft panel â€“ load balancing â€” load balancers\n\nclick create load balancer (top left)\n\nConfigure Load Balancer\n\nselect type\n\napplication, network or classic\n\napplication is for http, https\n\nguess this is for internet traffic coming into (and out of?) application\n\nnetwork is for tcp, tls, udp\n\nguess this would be for communication between containers\n\nclassic is for http, https, and tcp\n\nsomething about an app running on an ec2 classic network\n\nhe chose application\n\n\ngive it a name\n\nexample ecs-alb\n\nip address type\n\nipv4 (default)\n\nscheme\n\ninternal or internet facing\n\nkept internet-facing (default)\n\n\nListeners\n\nhttp, port 80\ncan add other ports if you want\n\nfor production should add a https, 80\n\n\nAvailability zones\n\nvpc, subnets\n\nselect those asscociated with the cluster\nsubnets have region specification (us-east-2a, b)\n\n\nclick next: configure security settings (bottom right)\n\nif you havenâ€™t add https port, itâ€™ll give you a warning\n\nclick next if donâ€™t care about https\n\n\n\nConfigure Security Settings\n\ntick box that has the name of the security group that was created during the cluster creation\n\nExample: EC2ContainerServic-ecs-ec2-EcsSecurityGroup-somehash\n\ndescription: ECS Allowed Ports\n\n\nclick Next\n\nConfigure Routing\n\ntarget group (backend of load balancer)\n\nnew target group (default)\n\nName\n\n(literally) â€œdefaultâ€\n\ntarget type\n\nInstance, IP, Lambda\nchose IP\n\nsomething about being able to use on EC2 and Fargate\n\n\nprotocol\n\nkept http\n\nport\n\nkept 80\n\nhealth check\n\nkept defaults\n\nclick next\n\nRegister targets\n\nkeep defaults\ngoing to specify this info through ecs in the next section\nclick next to review\n\nReview\n\nclick create\n\n\n\nEdit the default forwarding target\n\nA listener rule is comprised of a target (or group of targets) and conditions. When the load balancer receives a request, it checks it against the conditions in the listener rules. For whichever condition the request meets, the load balancer then sends the request to the target (e.g.Â ip address(instance) or lambda function (code scripts)) associated with that condition.\nSearch ec2 â€“ left panel â€“ load balancing â€“ load balancerÂ \n\ntick the load balancer you want\nclick listener tab (mid left)\nFor listener id = http 80 and under the Rules column it will say Default: forwarding to default\n\nclick view/edit rules\n\nunder the IF column (ie the condition) it says, Requests otherwise not routed. Which means any request that doesnâ€™t meet any of the other conditions\nclick the edit pencil icon (top left) â€“ click pencil icon next to http 80: default action\n\nUnder the THEN column â€“ click the trash can to delete â€œforward to defaultâ€\nclick add action\nselect return fixed response\n\nkeep response code 503\n\nmeans server had an issue responding\n\nin Response body, type message\n\nexample: sorry, no one is home right now\nclick check mark\nclick update (top right)\n\n\n\nclick back arrow (top left)\n\n\ntest it out by clicking description tab (mid left)\n\nGet DNS (Domain Name Service) name\n\nExample blahblahamazonaws.com\nNormally, you take your domain name (www.store.com/app) and give it the Amazon Route 53 service which translates your domain name into an ip address.\nYou then reroute traffic from your domain ip address to this dns name.\n\npaste it in the browser and the error message displays"
  },
  {
    "objectID": "qmd/docker-aws.html#sec-docker-aws-cesat",
    "href": "qmd/docker-aws.html#sec-docker-aws-cesat",
    "title": "AWS",
    "section": "Create ECS Service and Task",
    "text": "Create ECS Service and Task\n\nAlso see create task definition and run task sections above\nSteps\n\nsearch ecs â€“ left panel â€“ clusters â€“ click fargate cluster\nclick tasks tab (mid left) â€“ select task that was created in Create task section â€“ click stop button (mid left)\nclick services tab (mid left) â€“ click create service\n\nConfigure Service\n\nConfigure Service\n\nselect launch type\n\nchoose fargate\n\nTask Definiton and Cluster\n\nkept the ones created in sections above\n\nenter a service name\n\nexample hostname\n\nnumber of tasks\n\nexample 2\n\nclick next step\n\nDeployments\n\nkeep default, rolling update\n\nallows you to upgrade the task definition from version 1 to version 2 in a rolling fashionÂ \n\ndonâ€™t know what heâ€™s talking about here with versions\n\n\n\nclick next step\n\nConfigure Network\n\nService\n\ncluster vpc, subnets\n\nselect the ones that are associated with this fargate cluster\n\nÂ security group\n\nkeep default (allows traffic in)\n\nauto-assign public ip\n\nkeep default ENABLED\nwith a load balancer, we could choose to use only used private ips though\n\n\nHealth check grace period\n\nset to 60 (in seconds)\ngives the container/cluster a chance to get up an running before it tests it to see if everything is working\n\nLoad Balancing\n\ntick application load balancer\ncontainer to load balancer\n\nshows container name port:port\nclick add to load balancer\n\nproduction listener port\n\nclick dropdown â€“ select 80 HTTP\n\n80 is our port of the container and we chose http when we created the load balancer\n\n\npath pattern\n\nit was /hostname but he changed it to /* which is every pattern\n\nI think /hostname would that â€œ/hostnameâ€ would be ip address pattern associated with this container (i.e.Â the condition or rule)\nand /* means route any request from  no matter what pattern is attached to it.\n\n\nevaluation order\n\nas soon as the first rule/condition is matched, traffic goes to that target and no other rules are considered. Lower the evaluation order, the sooner the rule is considered\nhe chose 1\n\nhealth check path\n\ndefault was /hostname\nsince heâ€™s using /*, he changed it to /hostname/ so the healthcheck will get a webpage (code 200) and not a redirect (code 300 error)\n\nService Discovery\n\nEnables Route 53 to create a local network dns address for your container.\nUseful for when you have multiple applications talking to each other\nuntick box for enable service discovery integation since this is only one application\n\n\n\n\nclick next step\n\nSet Autoscaling\n\nSee next section on adding autoscaling\nThis can be added after this service has been created by updating\nclick next step\n\nReview\n\nclick create service\n\nCreates target group, rule/condition\n\nclick view service\nshould see two tasks starting up\ngoto the dns address on a webpage (see end of create load balancer section)\n\nbefore it displayed the error message (default target), now it shows a webpage (like in the Create Task section above)\nrefresh and it shows the second dns address associated with having a second task\n\n2 tasks means it can handle more traffic (just like the end of the create task section above)"
  },
  {
    "objectID": "qmd/docker-aws.html#sec-docker-aws-utstaa",
    "href": "qmd/docker-aws.html#sec-docker-aws-utstaa",
    "title": "AWS",
    "section": "Updating the Service to Add Autoscaling",
    "text": "Updating the Service to Add Autoscaling\n\nsearch ecs â€“ left panel â€“ clusters\nClick on your cluster that you want to update its service\n\nservices tab (mid left) â€“ click service name or id\n\nclick update\n\nClick next until you get to Set Autoscaling\ntick configure service autoscaling\nminimum number of tasks\n\nhe chose 1\n\ndesired number of tasks\n\nhe chose 3\n\nmaximum number of tasks\n\nhe chose 5\n\nIAM role\n\nuse default ecsautoscalerole\nuse create new role if there isnâ€™t already one available\n\nclick Add scaling policy\n\ntick step scaling\nenter policy name\n\nexample stepUp\n\nexecute policy when\n\ntick create new alarm\nalarm name\n\nexample upAlarm\n\nECS service metric\n\nCPU Utilization\n\nAlarm threshold\n\navg cpu utilization &gt; 10 (%)\nconsecutive period = 1\nperiod = 8 min\n\nhe chose 1 just for illustrative purposes\n\nclick save\n\n\nscaling action\n\nadd 1 task\nwhen cpu utilization &gt; 10\n\ncountdown period\n\namount of time it takes to make a decision\n30 sec\n\nclick save\n\nclick Add scaling policy (again)\n\nsame thing but for scaling down\navg cpu utilization\n\nhe chose &lt;= 10 but Iâ€™m not sure if thatâ€™s what youâ€™d do in real life. Iâ€™d think youâ€™d want some separation between the up and down scaling, but maybe not\n\nscaling action\n\nremove 1 task\n\n\nClick next to review\nclick update service\n\n\nClick tasks tab (mid left) to see how many task are currently running\nclick autoscaling tab to see the both upAlarm and downAlarm condition info\nTo see status of the targets (ip addresses of instances/containers)\n\nGoto EC2 â€“ left panel â€“ load balancing â€“ target groups\n\ntick the target group name of the load balancer\nUnder Registered Targets (Bottom)\n\nshows ips, status\n\nstatus == draining when auto-scaling taking a resource offline"
  },
  {
    "objectID": "qmd/docker-aws.html#sec-docker-aws-troub",
    "href": "qmd/docker-aws.html#sec-docker-aws-troub",
    "title": "AWS",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n\nNotes:\n\nnormal for an active cluster without any running services or tasks to have 1 active container instance. Itâ€™s called the container management instance.\n\nExample: you notice your cluster is running 5 containers when you only desire 3\n\nCan see this in Clusters â€“ fargate â€“ services tab, under the desired tasks and running tasks columns\n\nits says 5 for desired but he chose that for his max in the autoscaling section, so I donâ€™t know if he adjusted it for demonstration purposes or if this something confusing that AWS does.\n\nAnswer: he had both containers trying to bind to port 80 (see logs below)\nService level\n\nclick service name\n\ntasks tab\n\ncan see the task ids and definitions that the various active tasks are using\n\nexample: tasks are alternating between running and provisioning. Why are some shutting down and others starting in their place? The container is running for some time and then being stopped for some reason.\n\nclick task id\n\nlogs tab â€“ select container\n\nshows errors that have occurred\n\nDetails tab\n\nContainers (bottom)\n\nclick expand-arrow on desired container\n\nclick view logs in CloudWatch\n\ntakes you to CloudWatch console\n\nview the logs of the task\n\nable to filter log by events\n\ngo up one level to see log streams\n\ncan match containers and tasks to see if it might be a task issue\nend hash is the task id\n\n\n\n\n\n\n\n\nDetails tab\n\nLoad Balancing â€“ click target group name\n\ntargets tab\n\nshows the individual targets (ip addresses), ports, statuses\nstatus by region (if you have resources in different zones)\n\nexample: there were 2 zones -Â  us.east.2a and 2b and one had all healthy and the other had zero healthy, but he didnâ€™t mention anything about it. Think thatâ€™s just how nodes are taken on and offline and not that thereâ€™s a regional issue.\n\n\nhealth checks tab\n\nhealthy threshold\n\nnumber of code 200s i.e.Â healthy responses required in order for a node to be considered healthy\n\nunhealthy threshold\n\nnumber of code 300s i.e.Â error responses required in order for the node to be considered unhealthy\n\n\n\n\nLogs tab\n\nshows the aggregate of the logs for each container (all tasks included)\nselect a container from the dropdown\n\ntimestamp, message, task id\nexample: shows a â€œbindâ€ error that says the container canâ€™t a bind to port 80 because itâ€™s already in use.Â  In theÂ Update task definition to share data volumes section, the second container he added was a duplicate of the nginx container and both were trying to bind to port 80. Hence the error\n\n\n\n\nCluster metrics\n\nClusters â€“ EC2 cluster â€“ metrics tab\n\nonly useful for EC2 clusters\ncompute and memory resources being used\n\nshows time series of min, max, and average percent usage\n\n\nClusters â€“ fargate cluster â€“ services tab\n\nclick service name â€“ metrics tab\n\nsame stuff as EC2 metrics tab\ncan click on the different metrics (cpu, memory utilization) and create custom metric functions, change period length, etc.\n\nleft panel has alarms (autoscaling trigger history), events, logs, and settings"
  },
  {
    "objectID": "qmd/docker-fundamentals.html",
    "href": "qmd/docker-fundamentals.html",
    "title": "21Â  Docker, Fundamentals",
    "section": "",
    "text": "TOC\n\nâ€œA Cloud Guru: Docker Fundamentalsâ€\nMisc\nCreating a Dockerfile\nImages\nDocker Hub\nRunning Containers\nNetworking Containers\nData Volumes\nOptimizing container file size\nRunning scripts when a container starts\nDocker utility functions\nDocker Compose\nManaging a web app with Docker-Compose\n\nMisc\n\nNotes from the course, â€œA Cloud Guru: Docker Fundamentalsâ€\nMisc\n\nDocker file linting tools\n\nhadolint\nSynk\nTrivy\nClaire\nAnchore\n\nDocker help commands (every command preceded by â€œdocker  â€)\n\nEvery management command has its own subset of commands associated with it\nmanagement\n\ncontainer\nimage\nnetwork\nnode\nplugin\nsecret\nservice\nstack\nswarm\nsystem\nvolume\nExample docker image --help\n\nshows sub-commands for management command image and short descriptions\nFor even more information on a sub-command, add â€“help after the sub-command\n\nExample: docker image build --help\n\n\n\n\nLinux OS stuff\n\nDebian is a very stable Linux distribution\n\nDebian-Jessie is the latest version of Debian\n\n(As of July 6 2019, itâ€™s called buster)\nSlim is a light-weight Debian-Jessie\n\nDebian-Wheezy is the version before Jessie\n\nAlpine is a very small Linux distribution (much smaller than even Slim)\n\nAn instance of an image or the result of running an image is called container\n\nAny changes made while running the container is lost once that container has been stopped.\n\nIf you create or add a file while in the container, stop the container, rerun the container, then it will no longer be there.\n\n\nAn image is the setup of the virtual computer. A combination of a file system and parameters. A package that rolls up everything you need to run an application.\n\nComposed of stacked layers where the layers are self-contained files\nYou can download, build, and run image, but they cannot be changed (immutable)\n\nYou can have many running containers of the same image.\nDocker Hub is a registry for docker repositories. Itâ€™s like a github for images. Each repo has 1 image but the repo can store many tagged versions of that image (version control)\nPull and run a docker image from Docker Hub from local cli\n\ndocker run url/repo/image\n\nexample: docker run docker.io/library/hello-world\n\ndocker.io = docker hub\nlibrary = repo name for all â€œofficialâ€ images\nhello-world = name of image\n\n\n\nTwo ways to create a image\n\nWhile inside a container, make changes. Then use commit command to create the new image with the changes\n\nNever used. Creating a dockerfile is superior.\n\nUsing a dockerfile\n\nimages on docker hub\n\nmdancho/h2o-verse\n\nh20, tidyverse, tensorflow, kerasÂ \n~2 GB\n\n\n\nCreating a dockerfile (example of a toy python flask app)\n\nArchitectureÂ \n\nstart with FROM base:tag\n\nEssentially copy-pastes the base image\nusually good to start with a base image\n\nFROM python:2.7-alpine\n\npython is the base image and 2.7-alpine is the tag\n\n\n\nRUN executes commands or scripts as if you were in the containerâ€™s OS\n\nRUN mkdir /app\n\nmakes a directory called â€œappâ€\n\n\nWORKDIR sets the working directory for everything that happens after this command\n\nWORKDIR /app\n\nCOPYÂ has source path+file (local) and a destination path+file (container) as args\n\nCOPY requirements.txt requirements.txt\n\nthe first requirements.txt is in the same dir as the docker file so no â€œ/other-dir/â€ required\nthe second â€œrequirements.txtâ€ says the file is to be placed into the /app dir\n\nequivalent to â€œ/app/requirements.txtâ€ because /app is our working directory\n\n\nCannot use .. to move above the dockerfile directory. Every path must be below it.\n\nInstall packages\n\nRUN pip -installÂ  -r requirements.txt\n\ninside requirements.txt says Flask==0.12\n\n\nCopy entire local directory to the working directory\n\nCOPY . .\n\nfirst period says copy everything is the current local directory\nsecond period says put everything in the working dir\n\n\nLABELs have key-value pairs. can be useful in production settings. The can be retrieved with a command later on.Â \n\nSome uses:\n\nfilter a containerâ€™s output based on a label Â \ncan include scripts for automatic load balancing (also see aws load balancer section below)\n\nLABEL maintainer=â€œEric Book ericbook@email.comâ€\\\n\n\n\n\nÂ Â  Â Â Â  Â Â Â  Â Â Â  Â Â Â  Â Â Â  Â Â version=â€œ1.0â€Â Â  Â \n\nOften changed or added to, so they should be close to the end of the dockerfile (but not the last line)\nCMD gives the default instruction when the image gets ran which is different from RUN commands, which are executed when the image is built.\n\nCMD flask run â€“host=0.0.0.0 â€“port=5000\nUnder-the-hood: CMD instructions are arguments to an entrypoint script and get run through a default docker entrypoint (see waaaaay below for info about entrypoints)\n\nA \\\\ is Linux operator that chains together instructions so they can be in separate lines of code for easier reading. Think a space also does the same thing, but you canâ€™t see it in the code.\nThe ordering of the inputs in the dockerfile will affect its size and efficiency\n\nThe source code is much more likely to change in the future than the package dependencies. Therefore even though there would be fewer COPY commands, itâ€™s best to install the dependencies before copying the source code.\nWhenever changes to the source code occur Docker has a caching mechanism, so that it doesnâ€™t rebuild everything above the layer where the changes occur. COPY . . executes in millisecs so the rebuild happens almost instantly while installing dependencies could take minutes.\n\nAttaching packages and libraries\n\nDownload and install R packages\n\nRUN R -e â€œinstall.packages(c(â€˜shinydashboardâ€™, â€˜reticulateâ€™, â€˜shinyâ€™))â€\n\nDownload, install Python libraries (ubuntu image)\n\nRUN ln -s /usr/bin/python3 /usr/bin/python && \\\n\n\n\nÂ Â  Â Â Â  Â Â Â  Â Â Â  Â Â Â  Â Â Â  Â Â Â  Â Â Â  Â Â Â  Â Â Â  Â Â Â  Â Â  Â Â Â  Â Â Â Â Â  Â Â Â  Â Â Â ln -s /usr/bin/pip3 /usr/bin/pipÂ Â  Â \n\nRUNÂ apt-get update\nRUN apt-get install -y libpython-dev\nRUN apt-get install -y libpython3-dev\nImages\n\nBuild image from dockerfile (** donâ€™t forget the dot at the end **)\n\ndocker image build -t web1 .\n\nimage is the management command\nbuild is the sub-command of image\n-t is the flag for â€œtagâ€\nweb1 is the tag. This allows us to to refer to this image as â€œweb1â€ instead of a hash\n. says build the image and place in current directory\n\nEnd of build shows hash id â€œsuccessfully built â€\ndocker image build -t web1:1.0\n\nversioning the image\nfor just â€œweb1â€ the version will be web1:latest\n\n\nInspect the image\n\ndocker image inspect web1\ninfo in json format\nAt the top complete hash id\nshows the versions of the image under â€œrepotagâ€\nvarious info about how the layers were created\nthe number of layers created is shown at the bottom by the lines preceded by a â€œshaâ€ and a hash\n\nList of images in local docker host\n\ndocker image ls\n\nbase images loaded from dockerfiles listed alongside the images we create\n images are â€œdangling images.â€ They are failed image builds or images that were built on top of already existing images\n\nsafe to delete; frees up disk space\n\n\n\nDeleteÂ local image\n\nUsing name and version tag\n\ndocker image rm web1:1.0\n\nUsing id\n\ndocker image rm -f 633f\n\n-f is the â€œforceâ€ flag\n\nnecessary when image has been tagged/copied (like when pushing to docker hub) and you want to remove both images.\n\n633f - only need the first four characters of the hash id\n\nids in docker image ls\n\n\n\n\n\nDocker Hub\n\nLogin to Docker Hub\n\nHave to do it once then a config file is created so you donâ€™t have to do it again\nInput your username and password for your Docker Hub acct\ndocker login\n\nPush image to Docker Hub\n\nTag image with docker hub username\n\ndocker image tag web1 ercbk/web1:latest\n\nweb1 is the image we want to push\nercbk/web1:latest\n\nercbk is the username\nweb1 is the repo\nlatest is the tag\n\n\ndocker image ls will show the newly tagged image\n\nPush image to docker hub\n\ndocker image push ercbk/web1:latest\n\n\n\nRunning Containers\n\n** Unless you include -d in the run command, you will need to open a separate terminal (i.e.Â different from the one you ran the run command in) in order run more docker cli commands while an app is running **\n** container names and ports need to be unique for each running container **\ndocker container ls\n\nlists all running containers, container_id, image, creation time, runtime, name\n-a\n\nshows all stopped containers\n\n\ndocker container rm &lt;name&gt;\n\ndeletes a stopped container\ncan also use 1st four characters of &lt;container_id&gt;\n\nshown in ls (see above)\n\n\ndocker container run\n\nthe basic run command\nhardly ever want to use just the basic commmand\n\nsee flags and examples below\n\n\ndocker container stop &lt;name&gt;\n\nstops a container\nadd more s to stop more than one container\n\nwith spaces between the names\n\nalso, ctrl + c\n\nuse it in the same terminal you used the run command in.\nonly works if you included the -it flag when you started it.\n\nalso see â€“rm below\ndocker container stop $( docker container ls -q)\n\nStops all running containers\n-q flag says list them quietly, so it doesnâ€™t print them out\nif you a â€œstop requires at least one argumentâ€ error, then there arenâ€™t any containers running\n\n\ndocker container logs &lt;name&gt;\n\ncan also use 1st four characters of &lt;container_id&gt;\nfor active or inactive containers\nshows times container was accessed\n-f\n\nruns log in the foreground (i.e.Â you can view accesses in real-time)\nctrl + c to kill it\n\n\ndocker container stats\n\nreal-time monitoring of active containers\nshows cpu and memory usage for each container\nnetwork input/output\n\nflags\n\n** Donâ€™t think order matters except for that the name of the image needs to be last **\n-d\n\nruns the container in the background\nallows you to run commands in the same terminal window that you used the container run command in\n\n-e\n\nallows you to pass an environment variable into the container\nNot always necessary\n\nFlask requires one (e.g.Â -e FLASK_APP=app.py)\n\napp.py is the name of the app script\n\n\nmultiple -e flags are allowed\n\n-e FLASK_DEBUG=1\n\nturns debug mode on when you run the container\nuse along with -v to make real-time changes to the app while the container is running\n\n\n\n-it\n\nallows for unix commands such as ctrl c to kill a process (such as a running container)\nmakes the docker container interactive. Which allows you to go into the container, navigate the file system, and make changes.\n\n--name\n\ndocker automatically provide a name for a container but this flag allows you to provide your own\n--name moose\n\n-pÂ \n\nports\n\nmap ports from local machine to ports within docker\n\nexpected to supply two ports separated by a colon\n-p 5000:5000\n\nThe first 5000 is the host port. The port you use to interact with the app in your browser (e.g.Â localhost:5000)\n\nAll containers running on your docker host need to have unique ports to run on. (e.g.Â -p 5001:8000)\nSo, I think this can be any port you want as long something else isnâ€™t already using it.\n\nThe second 5000 is the container port that was specified in the dockerfile\n\nTried 5000:5000 and the app didnâ€™t run in the browser even when the dockerfile had 5000 specified, but after specifying 800 in the dockerfile, -p 5000:8000 did work\n\nThis was run on the default docker network\n\nRunning -p 5000:5000 DID work on a custom network.\n\nAlso these are 2 different apps. The 1st one was stand-alone (03-lecture cloud guru docker fundamentals), and the 2nd ran in conjunction with a redis server (see custom network section below)(09-lecture in cloud guru docker fundamentals). Not sure if that makes a difference\n\n\n\nWhen running the container, if itâ€™s an app, it can be viewed in a browser at localhost:5000 (i.e.Â the first port specified)\n-p 5000\n\ndocker will attach a random port number\n\nYou can specify more than one port mapping e.g.Â -p 5000:5000 -p 3838:3838\nports (second port specified) for common images\n\nredis (storage) - 6379\nnginx (web server) - 80\n\nopen source, handles a lot of connections efficiently, used to host a lot of websites\n\nRStudio: -p 3838:3838\nShiny apps launched from within RStudio: -p 8787:8787\n\n\n--restart\n\nwith value, on-failure\n\nsays restart the container if thereâ€™s some catastrophic failure (e.g.Â docker daemon dies and restarts)\nuseful in production\n\ncannot be included if â€“rm is also used\n\n--rm\n\ndeletes the container once it has been stopped\ncannot be included if â€“restart is also used\n\n-v\n\nvolumes\nUse cases\n\nwhile developing. Makes changes inside the container in real-time.\nstore data in a db\nMapping to a folder on your local machine allows you to work on projects stored within the container\n\n(1st half) requires address (local machine) to the directory with the script (e.g.Â app script not dockerfile) where youâ€™re making changes  (2nd half) address inside the container where this directory should be â€œmountedâ€ (i.e.Â where the script youâ€™re making changes to is located inside the container)\n\n1st half\n\nFor Linux, you can used the shortcut, â€œ$PWDâ€ which is a linux environment variable that stores the path to the working directory. It has the same value as the command â€œpwd -Lâ€ which Prints Working Directory.\n\nfyi the -L has something to do with â€œsymlinksâ€ which are thinks that can be created that point to another directory. If youâ€™re in in the symlink directory -L will print that directory and -P will print the directory that the symlink points to.\nsame thing for Mac, except the quotes might need to be included\n\nFor windows, â€œ/c/users/&lt;user_name&gt;/path/to/directoryâ€\n\n2nd half\n\nâ€œ/appâ€ which is where the app script is (also the working directory specified in the dockerfile for the example)\n\nExample in Linux, docker container run -it -p 5000:5000 -e FLASK\\_APP=app.py --rm -name web1 -e FLASK\\_DEBUG=1 -v $PWD:/app web1\nExample in Windows,Â docker container run --rm -itd -p 5000:5000 -e FLASK\\_APP=1 -e FLASK\\_DEBUG=1 --name web2 -v /C/Users/tbats/Documents/R/Projects/Docker/Courses/cloud-guru-docker-fund/09-linking-containers-with-docker-networks:/app web2\n\nWhile running, if you open a new terminal and docker container inspect &lt;name&gt;, then there should be a â€œmountâ€ section with type = â€œbindâ€\nTroubleshooting if changes donâ€™t show up in the container in real-time\n\nitâ€™s probably the path specifications for the value of the -v tag. Docker is picky, esp w/WIndows.\nMake sure code script and docker are on same drive\nNext try replacing Alpine Linux with Slim Linux in the dockerfile â€“&gt; rebuild image â€“&gt; run container with volume flag + the addresses like stated above\n\nSomething about inotify in Alpine not doing something\n\nsee exec command below\n\n\n\nExamples:\n\ndocker container run -it -p 5000:8000 -e FLASK\\_APP=app.py web1\n\nweb1 is the name of the image.\nThis is probably least number of flags necessary to run an app in a container\n\ndocker container run -it --rm --name web2 -p 5000:8000 -e FLASK\\_APP=app.py -d --restart on-failure web2\n\nexec\n\nexecutes an interactive bash session in the container\nContainer must be running, so open a new terminal window and type the code line:\n\ndocker container exec -it web1 bash\n\nwhere web1 in the name of the container\nbash is for the Slim distribution of Linux; use sh for Alpine\n\nOr run a bash command detached in the background docker exec -t -d web1 bash \"ls -al\"\n\nyouâ€™ll be logged in under â€œrootâ€ and dropped into wherever you designated the working directory in the dockerfile\nType ls -la to view the files\nExample of a debug\n\nPython app isnâ€™t showing changes after running it with a volume flag (-v)\nyou exec a bash session inside the container\ndelete some .pyc files (might be corrupted somehow) that are created when python runs flask\n\nrm *.pyc\nls -la to confirm\n\ngo to local script and make changes, save\ngo to terminal window where container is running and see changes to scripts detected in the terminal\ngo to browser where app is running and refresh\nwait a few secs and changes show up. Yay.\ngo back to bash terminal window, ctrl + d to kill it\n\n\n\nNetworking Containers\n\nInternal networks (LANs), external networks (WAN)\n\nWAN is a wide area network. Can be public or private. Stretches across city or industrial park, etc\n\nAccess addresses\n\nservers bound to 0.0.0.0: give access to any computers on your LAN or WAN\nif localhost:, then only laptop running the server can connect to it\nif : then any computer on your LAN, WAN, or on the internet can connect\n\ne.g.Â 192.168.1.4:5000\n\n\nList of Docker networks\n\ndocker network ls\nipconfig (windows) ifconfig (Linux, Mac)\n\nshows info about networks\n\nbridge network is docker0 which is the docker supplied network\n\nfyi docker0 didnâ€™t show up on Windows for me\n\n\nPing from one container to another\n\nNote: ping and ifconfig removed from Alpine and Slim image\n\nto reinstall, add this to 2nd line of dockerfile\n\nAlpine\n\nRUN apk update && apk add iputils\n\nSlim\n\nRUN apt-get update && apt-get install -y net-tools iputils-ping\n\n\n\ndocker exec web2 ping 172.17.0.2\n\nwhere 172.17.0.2 is the other containerâ€™s eth0 inet address\n\nfound by docker exec &lt;container name&gt; ifconfig\n\n\nctrl + c to stop the pinging\n\nView etc file\n\ndocker exec &lt;container name&gt; cat /etc/hosts\nshows eth0 inet address is mapped to container id\n\nCreate custom network\n\nallows us to connect containers by name which means if the addresses change, the apps wonâ€™t break.\ndocker network create --driver bridge &lt;name&gt;\n\nthe bridge driver is used for networking containers on the same docker host\nfor networking across multiple docker hosts, the overlay driver is used. (would need to research this further)\n\nInspect network\n\ndocker network inspect &lt;name&gt;\n\nadd container to custom network\n\nadd â€“net  to the run-container instruction\n\nexample:Â docker container run -it --rm name web2 -p 5000:5000 -e FLASK\\_APP=app.py -d --net firstnetwork web2\nexample:Â docker container run -it --rm name redis -p 6379:6379 -d --net firstnetwork redis:3.2-alpine\n\nThis containers only linked up with the app running on the browser when they were run on the custom network and not the default docker bridge network\nin debug mode:Â docker container run -it --name web2 -p 5000:5000 -e FLASK\\_APP=app.py -e FLASK\\_DEBUG=1 -v /C/Users/tbats/Documents/R/Projects/Docker/Courses/cloud-guru-docker-fund/09-linking-containers-with-docker-networks:/app -d --rm --net firstnetwork web2\n\ncontainers will show up when you inspect firstnetwork\ncan now ping using container names (assuming both containers have been added to the network)\n\ndocker exec web2 ping redis\n\nweb2 is the container doing the pinging\nredis being pingedÂ \n\n\n\n\n\n\nData Volumes\n\nallows data to persist on docker host after the container is stopped\n\nShould save on the host for apps because they should be portable\nFor databases, not so bad\n\nA volume is nothing more than a folder on your computer that is linked to a folder inside the Docker container.\nDefault volume path on host, â€œ/var/lib/docker/volume/â€\ndocker volume create web2\\_redis\n\nweb2_redis is the name of the volume\n\ngood idea to pick a name thatâ€™s relevant to job\n\n\ndocker volume ls\n\nlist of volumes\n\ndocker volume inspect web2\\_redis\n\nshows info about volume\nâ€œMountpointâ€ shows where the volume will be stored on the host machine\n\ndocker volume rm &lt;volume1 name&gt; &lt;volume2 name&gt;\n\nremove specific volumes by name\n\ndocker volume prune\n\nremoves all volumes\n\ndocker rm -v &lt;container name&gt;\n\nremoves container and anonymous volume\n\nWill not remove a named volume\n\n-v required else a â€œdanglingâ€ is created\n\ndocker volume ls -qf dangling=true docker volume rm $(docker volume ls -qf dangling=true)\n\nremoves dangling volumes\n\nAdd volume flag, data volume name, and destination to container\n\ndocker container run -it --rm name redis -p 6379:6379 -d --net firstnetwork -v web2\\_redis:/dataÂ  redis:3.2-alpine\n\nweb2_redis is the name we gave to the data volume\n/data is designated by the redis people\n\nThis technique works for mysql, postgres, elasticsearch, etc. You just have to figure out the WHERE they decided that they want you to store your data (i.e.Â the /data part)\n\ngo to their docker hub image page â€“&gt; view readme â€“&gt; look for section on persistent storage\nExample I went to postgres page and did an ctrl+f â€œpersistentâ€ and found a section describing when to use /data or /pgdata\n\n\n\n\nSaving the data\n\nredis does it automatically every 30 sec\nManual save if you need to save right away:\n\ndocker exec redis redis-cli SAVE\n\n\nExample: Named Volume\n\n\nÂ  Â  Â  Â  Â  Â  version: '3.8'\nÂ  Â  Â  Â  Â  Â  services:\nÂ  Â  Â  Â  Â  Â  Â  db:\nÂ  Â  Â  Â  Â  Â  Â  Â  image: mysql\nÂ  Â  Â  Â  Â  Â  Â  Â  restart: always\nÂ  Â  Â  Â  Â  Â  Â  Â  environment:\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  MYSQL_ROOT_PASSWORD: root\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  MYSQL_DATABASE: test_db\nÂ  Â  Â  Â  Â  Â  Â  Â  ports:\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  - \"3306:3306\"\nÂ  Â  Â  Â  Â  Â  Â  Â  volumes:\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  - db_data:/var/lib/mysql\nÂ  Â  Â  Â  Â  Â  volumes:\nÂ  Â  Â  Â  Â  Â  Â  db_data:\n\nâ€œdb_dataâ€ is the name\nâ€œ/var/lib/mysqlâ€ is the path inside the container\nAdvantages\n\nData persists after we restart or remove a container\nAccessible by other containers\n\nExample: Unnamed (aka Anonymous) Volume\n\nÂ  Â  Â  Â  Â  Â  version: '3.8'\nÂ  Â  Â  Â  Â  Â  services:\nÂ  Â  Â  Â  Â  Â  Â  db:\nÂ  Â  Â  Â  Â  Â  Â  Â  image: mysql\nÂ  Â  Â  Â  Â  Â  Â  Â  restart: always\nÂ  Â  Â  Â  Â  Â  Â  Â  environment:\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  MYSQL_ROOT_PASSWORD: root\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  MYSQL_DATABASE: test_db\nÂ  Â  Â  Â  Â  Â  Â  Â  ports:\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  - \"3306:3306\"\nÂ  Â  Â  Â  Â  Â  Â  Â  volumes:\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  - /var/lib/mysql\n\nNo volume name here but still has the path inside the container\nData will persist on restart of the container, but not after the container is stopped and removed\nNot accessible by other containers\nActuallyâ€¦ even without the â€œvolumesâ€ instruction, the â€œmysqlâ€ image/dockerfile has a â€œVOLUMEâ€ instruction, so an anonymous volume would still be created.\nExample (Bind Mounts)\n\nÂ  Â  Â  Â  Â  Â  version: '3.8'\nÂ  Â  Â  Â  Â  Â  services:\nÂ  Â  Â  Â  Â  Â  Â  db:\nÂ  Â  Â  Â  Â  Â  Â  Â  image: mysql\nÂ  Â  Â  Â  Â  Â  Â  Â  restart: always\nÂ  Â  Â  Â  Â  Â  Â  Â  environment:\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  MYSQL_ROOT_PASSWORD: root\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  MYSQL_DATABASE: test_db\nÂ  Â  Â  Â  Â  Â  Â  Â  ports:\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  - \"3306:3306\"\nÂ  Â  Â  Â  Â  Â  Â  Â  volumes:\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  - $PWD/data:/var/lib/mysql\n\nInstead of using the default host directory for the volume, you can specify a location yourself\nfirst half (before colon) : where on the host machine to mount (i.e.Â create) the volume\n\nIn the example, itâ€™s located in the working directory + /data\nFor Linux, you can used the shortcut, â€œ$PWDâ€ which is a linux environment variable that stores the path to the working directory. It has the same value as the command â€œpwd -Lâ€ which Prints Working Directory.\n\nfyi the -L has something to do with â€œsymlinksâ€ which are thinks that can be created that point to another directory. If youâ€™re in in the symlink directory -L will print that directory and -P will print the directory that the symlink points to.\nsame thing for Mac, except the quotes might need to be included\n\nFor windows, â€œ/c/users/&lt;user_name&gt;/path/to/directoryâ€\n\nsecond half (after colon): specify the path inside the container you want to use for the volume. Usually specified by the db software.\n\nIn the example, itâ€™s â€œ/var/lib/mysqlâ€ which has been specified by the mysql image\n\nSharing data between containers\n\nContainers sharing and receiving need to be on the same docker host\nAdd to app dockerfile\n\nVOLUME [â€œ/app/publicâ€]\n\nLine location in the dockerfile wasnâ€™t specified but he put his right before the CMD line\nlocal directory with app has a folder named â€œpublicâ€ that will be shared with other containers.\n\n\nSteps\n\nbuild app image\nrun app image\nrun redis image with flags, â€“volume -from Â \n\ndocker container run -it --rm name redis -p 6379:6379 -d --net firstnetwork -v web2\\_redis:/data --volume -from web2Â  redis:3.2-alpine\n\n\nFiles in appâ€™s public folder will be in redisâ€™s container\n\nSteps\n\ngo into redis container\n\ndocker container exec -it redis sh\n\nchange directory to the public folder\n\ncd /app/public\n\ncontents of appâ€™s public folder are in this folder too\n\nexamine contents by printing to the terminal\n\ncat main.css\n\n\n\n\nWhile containers are running, you can exec into volume container (e.g.Â web2) add files, make changes to files, etc., and the files in the other containers will be updated in real time.\nAlternate method (*not recommended for production*)\n\nDonâ€™t put the VOLUME instruction in the dockerfile\nAdd -v /app/public to the app run command\nAdd the â€“volume -from flags to the redis container run command like before\n\n\nOptimizing container file size\n\n.dockerignore\n\nContains file paths to files in the local project directory that you donâ€™t want on the image (e.g.Â files with private data, .git files can be huge)\nDuring the image build, when docker runs the COPY/ADD instructions it will bypass the files in the .dockerignore file\nFile paths in the .dockerignore file begin wherever youâ€™ve designated the working directory in WORKDIR in the dockerfile\nExamples:\n\n.git/\n\nadding a trailing â€œ/â€ isnâ€™t necessary but lets people know itâ€™s a directory and not a file\n\nmay want to include the .dockerignore itself\nfolder/*\n\nThe folder itself will be added to the image but all the content will be ignored\n\n**/*.txt\n\nsays ignore all files with the .txt extension\n\n!name.txt\n\nthis grants an exception to the name.txt file. It will be added to the image even with the **/*.txt\n\nYou can negate the .dockerignore file\n\nJust a * on line 1\nbegin each file path with a â€œ!â€ to specify which files to include\n\n\n\nRemoving the build dependency files of the system dependency files (** only for Alpine Linux **)\n\nIn other words the files used to build the system dependency files\nHe uses the example of a postgres dependency in the video, but no dependencies are actually listed in the dockerfile in the files folder.\n\n2 hypotheses on what heâ€™s talking about:\n\nThe package you use in python to interact with the sql db has dependencies and files are needed to build those dependencies (build dependencies). After the dependencies are built, the build dependencies get deleted.\nThis image with include the sql db and that db has build dependencies. They get deleted\n\nI think itâ€™s 1., but I dunno. In the Installing packages section above, there are examples of python-dev files being installed, but Iâ€™m not sure which hypothesis that favors.\n\n\n\nItâ€™s a bear of a bash script, so see dockerfile for details in cloud guru docker fund course, 012 lecture files folder on optimizing\n\nscript stays above the COPY . . instruction\nIf you use this script, thoroughly check everything and make sure the packages are working as intended.\n\nSteps\n\nfirst line (indent of the lines is the same in the file)\n\n\n\n\nRUN apk add --no-cache --virtual .build-deps \\\n\nsecond line, add dependencies\n\nÂ  Â  postgressql-dev dependency2 dependency3 \\\n\nThen add the rest of the bash gunk\n\nÂ  Â  && pip install -r requirements.txt \\\nÂ  Â  && find /usr/local \\\nÂ  Â  Â  Â  \\( -type d -a -name test -o -name tests \\) \\\nÂ  Â  Â  Â  -o \\( -type f -a -name '*.pyc' -o -name '*.pyo' \\) \\\nÂ  Â  Â  Â  -exec rm -rf '{}' + \\\nÂ  Â  && runDeps=\"$( \\\nÂ  Â  Â  Â  scanelf --needed --nobanner --recursive /usr/local \\\nÂ  Â  Â  Â  Â  Â  Â  Â  | awk '{ gsub(/,/, \"\\nso:\", $2); print \"so:\" $2 }' \\\nÂ  Â  Â  Â  Â  Â  Â  Â  | sort -u \\\nÂ  Â  Â  Â  Â  Â  Â  Â  | xargs -r apk info --installed \\\nÂ  Â  Â  Â  Â  Â  Â  Â  | sort -u \\\nÂ  Â  )\" \\\nÂ  Â  && apk add --virtual .rundeps $runDeps \\\nÂ  Â  && apk del .build-deps\n\nRunning scripts when a container starts\n\nInstead of making multiple similar dockerfiles/images, you can use entry pointsÂ into one dockerfile/image\n\nExamples:\n\nFor a postgressql image, you pass a run instruction inside the dockerfile that sets up your user authorization and password as an environment variable. It also has an ENTRYPOINT, so that if you have other projects that use a postgres sql db, they can gain access to that information through the entry point.\nRunning a db migration after a container starts\n\nrun_db_migration1 as environment variable but with a default value set to off. You can control that action through entrypoint and a script\n\ncontrol stuff in a nginx config to set an external ip after the container is running\n\n\nThe docker_entrypoint scripts arenâ€™t in your dockerfile so they donâ€™t add layers to an image\nSteps\n\nAdd lines to dockerfile\n\n\n\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  COPY docker-entrypoint.sh /\n\ndocker-entrypoint.sh should be a file in the root project directory\n\nNot in app directory, because itâ€™s best practice to keep entrypoint files separate.\nFor details see section below\n\n\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  RUN chmod +x /docker-entrypoint.sh\n\ntells Linux to give the entrypoint script permission to run\n\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  ENTRYPOINT [\"/docker-entrypoint.sh\"]\n\nentry point instruction that points to where the script is located.\nRun redis container as before\nBuild app image as before (gave it the name, â€œwebentrypointâ€)\nRun app container\n\ndocker container run -it --name webentrypoint -p 5000:5000 -e FLASK_APP=app.py -e FLASK_DEBUG=1 --rm --net firstnetwork webentrypoint\n\nname and image values changed to webentrypoint\nremoved -d flag because we want it to run in the foreground\nIf you go to localhost:5000 it has some message printed from the docker-entrypoint.sh script\nStop the app container\nRe-run the app container\n\ndocker container run -it --name webentrypoint -p 5000:5000 -e FLASK_APP=app.py -e FLASK_DEBUG=1 -e WEB2_COUNTER_MSG=\"Docker fans will have visited this page\" --rm --net firstnetwork webentrypoint\n\nWEB2_COUNTER_MSG is an environment variable that was given a default value inside the docker-entrypoint.sh script.\nSo we were able to change the environment variable without having to run the container, exec into the container, then change it.\nDetails on the dockerentrypoint.sh file\n\nWhen you run the container, this script gets run before the dockerfile\nExports an environment variable that gets accessed by the app.py script\n\n\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  #!/bin/sh\n\nComment that tells us that weâ€™re running a shell script\n\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  set -e\n\nsays abort if thereâ€™s an error in the script\n\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  echo \"The Dockerfile ENTRYPOINT has been executed!\"\n\nLine that prints in the terminal when we run the container\n\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  export WEB2_COUNTER_MSG=\"${WEB2_COUNTER_MSG:-carbon based life forms have sensed this website}\"\n\nWhere the custom scripting takes place\nIn this case, WEB2_COUNTER_MSG environment variable is created and a default value set\n\nI think the syntax after the = has something to do with making the value a default value\n\n\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  exec \"$@\"\n\nSays that after running everything in this script, then run the CMD stuff in the dockerfile\nDocker utility functions (can be run in any directory)\n\ndocker system df\n\nshows resources being used by docker\nadding -v tag at the end, produces a verbose output all the info is broken down further\n\ndocker system info\n\ninfo about your docker installation\n\nuseful when reporting bugs, creating issues\nverify docker installation\n\n\ndocker system prune\n\ndeletes all the crud\n\nstopped containers that you forgot to include â€“rm\nvolumes not used by at least one container\nnetworks not used by at least one container\nall dangling images\n\nadd -f flag to execute the prune without the confirmation message\n\nuseful for automating through cron jobs, etc.\n\nadd -a flag to remove ALL unused images\n\n**only run if want every image not being used by a running container to be deleted**\n\n\n\nDocker Compose (note dash between docker and compose in commands)\n\ndocker-compose.yml properties\n\nyaml files do not need to be in the same directory as your dockerfiles. You just have to give the path in the build property (see below)\n\n\n\nversion: '3'\n\n\n# pound sign is for comments\nservices:\nÂ  redis:\nÂ  Â  image: 'redis:3.2-alpine'\nÂ  Â  ports:\nÂ  Â  Â  - '6379:6379'\nÂ  Â  volumes:\nÂ  Â  Â  - 'redis:/data'\n\nÂ  web:\nÂ  Â  build: '.'\nÂ  Â  depends_on:\nÂ  Â  Â  - 'redis'\nÂ  Â  env_file:\nÂ  Â  Â  - '.env'\nÂ  Â  ports:\nÂ  Â  Â  - '5000:5000'\nÂ  Â  volumes:\nÂ  Â  Â  - '.:/app'\n\nÂ  worker:\nÂ  Â  build: '.'\nÂ  Â  command: celery &lt;command&gt;\nÂ  Â  depends_on:\nÂ  Â  Â  - 'redis'\nÂ  Â  env_file:\nÂ  Â  Â  - '.env'\nÂ  Â  volumes:\nÂ  Â  Â  - '.:/app'\n\nvolumes:\nÂ  redis: {}\n\nversion is the version of the compose api\nservices are the containers weâ€™re building\n\nservice names (e.g.Â redis, web) will end up being the container and image names (see below for the exception)\nimage property\n\nuses the base:tag format like the value for the FROM instruction in the dockerfile\n\nbuild property\n\nâ€˜.â€™ says build an image from the current directory\nIf your app has itâ€™s own folder then you need to specify the path\n\nexample â€˜./webâ€™Â \n\n\nimage and build property in the same service\n\ndocker will build 2 of the same image\n\none with project name (build) and the other with the image value as the name for the image\n\nexample build: â€˜.â€™ and image: â€™ercbk/web:1.0\n\nsays build image using dockerfile in current directory and name it ercbk/web:1.0\nuseful if the image is going to be pushed to the docker hub\n\n\nports\n\nports to be used for the container (see -p flag in running containers section for more details)\n\nthe â€œbindâ€ port (right side) supplied here needs to match the dockerfile\n\na  indicates a list in yamls.\n\nexample: forward 2 sets of ports\n\n\n\n\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  - '6379:6379'\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  - '5348:5348'\n\nnetwork\n\ntakes list inputs\nused when multiple networks are created\n\nexample: databases/workers communicate on one network while apps communicate on both networks\n\n\n\nÂ  result:\nÂ  Â  build: ./result\nÂ  Â  command: nodemon --debug server.js\nÂ  Â  volumes:\nÂ  Â  Â  - ./result:/app\nÂ  Â  ports:\nÂ  Â  Â  - \"5001:80\"\nÂ  Â  networks:\nÂ  Â  Â  - front-tier\nÂ  Â  Â  - back-tier\n\nÂ  worker:\nÂ  Â  build: ./worker\nÂ  Â  networks:\nÂ  Â  Â  - back-tier\n\nÂ  db:\nÂ  Â  image: postgres:9.4\nÂ  Â  volumes:\nÂ  Â  Â  - \"db-data:/var/lib/postgresql/data\"\nÂ  Â  networks:\nÂ  Â  Â  - back-tier\n\nvolumes\n\nsee data volumes section for more details\n means, just like for ports, values for this property are in list format\nIn the app service,\n\nspecify which directory to share\n\nâ€™.:/appâ€ says share the current local directory which will be the app directory in the container\n\n\nin the redis service,Â \n\nweb2_redis is the name we give to the data volume\n/data is designated by the redis people\n\n\ndepends_on\n\nnecessary if one service depends on another. Indicates if one container needs to start before another.\n\nexample: web (app) depends on redis (db)\n\ntakes a value in list format\n\nenvironment\n\nmethod 1 for setting environment variables\nname: value pair\n\nexample:\n\n\n\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  environment:\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  FLASK_DEBUG: 'true'\n\nenv_file\n\nmethod 2 takes a list of environment files to load\nloads from top to bottom\n\nif any variables in a lower listed file match those in a file listed higher up, then the values in the earlier file get overwritten to the values in the later file\nuseful if you have a stock env file, decide to put the containers into production, then you can just add a production env file to the directory and the yaml\n\nexample: .env is the name of the file in the current directory ( is in the actual name)\nfile example\n\n\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  COMPOSE_PROJECT_NAME=web2\n\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  PYTHONBUFFERED=trueÂ \nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  FLASK_APP=app.py\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  FLASK_DEBUG=1\n\nThe flask variables are the ones we passed in the run containers section\nCan configure Docker Compose options here\n\nexample: supply a project name. Otherwise Docker would just use the current directory for the name of the project. Also gets added as a prefix for networks, etc. that Compose will create.\n\nThe Python buffer variable is necessary if you want to see the output from the terminal through Compose\nworker (service)\n\nUse the same dockerfile for both services\n\neg worker uses the same dockerfile as web in the example docker-compose.yml\n\nuseful for background services for your app\n\ncelery is a python library used this task\n\ndifferences\n\nno need for port to be exposed (therefore overriding the CMD instruction in the dockerfile)\nuses command property which replaces the CMD instruction in the dockerfile\n\n\nvolumes\n\nthe name given here needs to match the name of the volume given in the services property\n\nexample: for this yaml the name of the volume created is â€œredisâ€\n\n{} - curly brackets are for adding options to the volume, such as being read-only (see docker docs for more options)\n\nin this example no options are supplied so itâ€™s empty (still necessary though)\n\n\nManaging a web app with Docker-ComposeÂ (note dash between docker and compose in commands)\n\nCurrent directory need to have the docker-compose.yml file\n\nunless you use the -f flag\n\ndocker-compose --help\n\ninfo on commands\n\ndocker-compose build\n\nbuilds an image of any service in the yaml file with a build property\ndocker image ls will show the built images with the project name as the prefix\n\nproject name set in the env file (see above)\n\n\ndocker-compose pull\n\npulls any other images specified with the image property in the yaml file\n\nÂ not just local but also pulls from docker hub I think\n\n\ndocker-compose up\n\nruns the project (everything created with project name prefix)\n\ncreates network\n\neg web2_defaults\n\ncreates volume\n\neg web2_redis\n\nstarts containers\n\neg web2_redis_1, web2_web_1\n\nthe â€œ_1â€ suffix is in case the project calls for multiple instances of the same container\n\nreminder: multiple apps require different ports (binding?) (see -p in running-a-container sections). Would also require setting up a load balancer. (see aws load balancer section below)\n\n\n\n\ndocker-compose up &lt;service&gt;\n\nstarts a specific service\nif you start a service with a dependency (â€œdepends_onâ€ specified in yaml), it will also start that dependency service.\ndocker compose web\n\nstarts web but also redis, because web has a specified redis dependency\n\n\n\ndocker-compose stop\n\nstops all containers\n\ncan probably specify a container\n\neg docker-compose stop web\n\n\ncan also use ctrl+c, but (as of 2017) thereâ€™s a bug that throws an error and aborts instead\n\ndocker-compose up --build -d\n\nÂ runs both build and up commands at once\n-d says run in the background\n\ndocker-compose ps\n\nsimilar info as docker container ls but presented slightly differently\n\ndocker-compose logs -f\n\nsince containers running in the background -f needed to logs of the container activity\nItâ€™s a realtime log of all containers running in the project, so will require ctrl+c to exit\n\ndocker-compose restart\n\nrestarts all containers\nÂ can also just specify one container\n\neg docker-compose restart redis\n\n\ndocker-compose exec web\n\nfor a running container, you can execute commands inside the container from the outside\n\ndocker-compose exec web ls -la\n\nshows file contents inside container\n\n\nopening a shell inside the container\n\ndocker-compose exec web sh\n\nno -it flag necessary like for docker container exec (see running-a-container section)\n\n\nexit\n\nexits the shell\n\n\ndocker-compose run &lt;service&gt; &lt;command&gt;\n\nallows you to instantly run the container, execute a command inside the container, and exit the shell\nequivalent to the command sequence: up â€“&gt; exec â€“&gt; exit\nexample: docker-compose run redis redis-server â€“version\n\ndocker-compose rm\n\ndeletes all containers (only stopped ones I assume)\nThese will also be removed using the docker system prune from the docker-utility-functions section"
  },
  {
    "objectID": "qmd/docker-misc.html",
    "href": "qmd/docker-misc.html",
    "title": "22Â  Docker, Misc",
    "section": "",
    "text": "TOC\n\nDocs\nCommands\nImages\nBuildKit\nMulti-Stage Dockerfile\nDocs\n\nCommand Reference for CLI, dockerfiles, etc.\n\nCommands\n\ndocker history --no-trunc &lt;image&gt; shows image hash, date created, command used to create the image, size, comments\n\nImages\n\npython:3.9-slim-bullseye\n\nnon-bulky python base image\nDonâ€™t use python alpine. Youâ€™ll need to install all sorts of dependencies for the simplest libraries. jAlso, alpine versions of common libraries are sometimes horribly outdated.\n\n\nOne-liner to run a R script\n\nFor someone that doesnâ€™t have R but has Docker\n\nADD from a github repo\n\nSee Misc &gt;&gt; COPY things (e.g.Â executables) from an image in a registry (below) for a mention about this not interacting with cache correctly\n\nCOPY things (e.g.Â executables) from an image in a registry (Thread)\n\nAlso see Multi-Stage Dockerfile &gt;&gt; Stage 2\nUnlike ADD https://â€¦ or RUN wget https://â€¦ this interacts correctly with caching, both not downloading anything unless necessary *and* updating always when necessary.\nUse Cases\n\nâ€œuse it for installing python wheels and deb packages from private images in ECR on top of a FastAPI Dockerfile without needing private repositories for them.â€\nâ€œa nice way to fetch external dependencies. Eg. Instead of downloading the tool with curl, copy it from the official image. This can also allow tools like dependabot to monitor upstream for updates and bump the tag in your dockerfile in a PR when a new release drops.â€\n\n\nAWS Elastic Beanstalk automatically handles things like capacity provisioning, load balancing, auto-scaling and application health monitoring. AWS Elastic Beanstalk itself uses EC2 instances for running your application and S3 storage for storing data, but you as a user donâ€™t have to bother with these things. You can simply upload your Docker image and AWS Elastic Beanstalk handles the remaining stuff for you.\n\nA â€œdocker-compose.ymlâ€ file is used by AWS Elastic Beanstalk to create and run the Docker container and has to be placed in the same directory as the Dockerfile\n\n\nversion: '3'\nservices:\nÂ  deploy:\nÂ  Â  build:\nÂ  Â  Â  context: .\nÂ  Â  Â  dockerfile: Dockerfile\nÂ  Â  ports:\nÂ  Â  Â  - '80:5000'\n\nport 80 is mapped to the port 5000 of the container. This is due to the reason that the webpage, which then will be hosted on AWS Elastic Beanstalk, listens to incoming connections on port 80 per default and the created container listens on port 5000.\nRequires you upload a zipped folder with the application and the Dockerfile.\n\nExample: GH Action yml step\n\n\nname: Generate Deployment Package\nÂ  Â  Â  run: zip -r deploy.zip *\n\nSecure public images with Docker Content Trust (DCT)\n\nArticle\nGives you the ability to verify the integrity of the image you are using and the integrity of the publisher of that image\n\nBuildKit\n\nAllows you to use external caching sources and build mounts to speed up image builds through caching (requires Docker version â‰¥18.09)\n\nAble to supply a previously built image in your registry where Docker will check the manifest of the image, and pull any layers that can be used as local cache.\n\nNotes from Fast Docker Builds With Caching\nMust have the environment variable, DOCKER_BUILDKIT=1\nExternal Cache\n\nDocs\nExample (single stage build)\n\n\n\nDOCKER_BUILDKIT=1 docker build \\\nÂ  --cache-from my-repo.com/my-image \\\nÂ  --build-arg BUILDKIT_INLINE_CACHE=1 \\\n\nUse --build-arg BUILDKIT_INLINE_CACHE=1 and --cache-from arguments when building the image\n\nâ€œmy-repo.com/my-imageâ€ is the url of the image you want Docker to pull dependencies (aka layers) that can be used as a local cache\n\nExample (multi-stage build)\n\nexport DOCKER_BUILDKIT=1\nIMAGE=my-repo.com/my-image\n# Build image for the build stage\ndocker build \\\nÂ  --target build-stage \\\nÂ  --cache-from \"$[{IMAGE}]{style='color: #990000'}:build-stage\" \\\nÂ  --tag \"$[{IMAGE}]{style='color: #990000'}:build-stage\" \\\nÂ  --build-arg BUILDKIT_INLINE_CACHE=1 \\\nÂ  .\n# Build the final image\ndocker build \\\nÂ  --cache-from \"${IMAGE_NAME}:build-stage\" \\\nÂ  --cache-from \"${IMAGE_NAME}:latest\" \\\nÂ  --tag \"${IMAGE_NAME}:latest\" \\\nÂ  --build-arg BUILDKIT_INLINE_CACHE=1 \\\nÂ  .\n\n# Push the build-stage image too so that it can be reused for cache\ndocker push \"${IMAGE_NAME}:build-stage\"\ndocker push \"${IMAGE_NAME}:latest\"\n\nThis shell script that gets referenced in the docker file (another example in this note; search for â€œshell scriptâ€)\nâ€œexportâ€ creates the environment variable; IMAGE is a variable storing the URL of the externally cached image\n--targetÂ  in the first build command to stop at the build-stage stage, and that\nThe second build command referenced both the build-stage and latest images as cache sources\nBuild Mounts\n\nThis type of caching is only available:\n\nlocally and cannot be reused across machines\nduring a single RUN instruction, so you need to either:\n\ncopy the files to a different location in the image before the RUN instruction finishes (e.g., with cp) or\nCOPY the cache directory from another image, e.g., a previously built build-stage image.\n\n\nSee the article for an example\n\nCredentials\n\nSteps\n\nPrepare an auth.toml file with your credentials\n\nExample (poetry LIB credentials for installing deps from a private repo)\n\n\n\n\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  [http-basic]\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  [http-basic.my_repo]\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  username = \"my_username\"\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  password = \"my_ephemeral_password\"\n\nPlace it outside of your Docker context or exclude it in .dockerignore (the cache would still get invalidated otherwise).\nUpdate your Dockerfile to include â€œ# syntax=docker/dockerfile:1.3â€ as the very first line, and\nAdjust install commands that require the credentials (e.g.Â poetry install command becomes:)\n\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  RUN --mount=type=secret,id=auth,target=/root/.config/pypoetry/auth.toml \\\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  poetry install --no-dev --no-interaction --remove-untracked\n\nbuild the image with docker build --secret id=auth,src=auth.toml ...\nMulti-Stage Dockerfile\n\nMisc\n\nCan drastically reduce the size of working images\nNotes from\n\nUsing multi-stage builds to make your docker image 10x smaller\n\n\nExample: 2-Stage\n\n\nFROM ubuntu:20.04 AS final\nFROM ubuntu:20.04 as build\n# BUNDLE LAYERS\nRUN apt-get update -y && apt install -y --no-install-recommends \\\nÂ  curl \\\nÂ  osmium-tool \\\n&& rm -rf /var/lib/apt/lists/*\nRUN mkdir /osmfiles \\\n&& mkdir /merged \\\n&& curl http://download.geofabrik.de/europe/monaco-latest.osm.pbf -o /osmfiles/monaco.osm.pbf \\\n&& curl http://download.geofabrik.de/europe/andorra-latest.osm.pbf -o /osmfiles/andorra.osm.pbf \\\n&& osmium merge /osmfiles/monaco.osm.pbf /osmfiles/andorra.osm.pbf -o /merged/merged.osm.pbf\n\nFROM final\nRUN mkdir /merged\nCOPY --from=build /merged /merged\n\nStage 1: build\n\nStarts at FROM ubuntu:20.04 as build\nDownloads a couple geospatial files, then merges them and stores them in the â€œmergedâ€ folder\n\nStage 2: final\n\nStarts at FROM final\nCreates a â€œmergedâ€ dir and copies merged file from stage 1 (build) to the â€œmergedâ€ dir\nThe curl and osmium-tool dependencies that are installed in Stage 1 are not included in Stage 2 which reduces the size of the final image.\n\nIâ€™m not sure if FROM ubuntu:20.04 AS final being the first line (instead of replacing the FROM final line) is necessary or not. It looks kind of redundant.\nIf a slimmer ubuntu image is used in the last stage, the size of the image can reduced further"
  },
  {
    "objectID": "qmd/econometrics-discrete-choice-models.html#sec-econ-dcm-misc",
    "href": "qmd/econometrics-discrete-choice-models.html#sec-econ-dcm-misc",
    "title": "Discrete Choice Models",
    "section": "Misc",
    "text": "Misc\n\nAKA Random Utility or Choice Models\ntl;dr:\n\nThe Utility equation is the model equation.\n\nUtility (\\(U\\)) is made up of Observed Utility (\\(V\\)) and Unobserved Utility (\\(\\epsilon\\)).\n(Total) Utility is the LHS of the model equation\nLogit models are typically used to estimate utility as predicted logits.\n\nThe predicted probabilities are called Choice Probabilities\nIf the data is aggregated to the Market Level instead of the individual level, then the predicted logits are now called Market Shares.\n\nObserved Utility is modeled using our data, and Unobserved Utility is the residual of that model.\nThe coefficients (\\(\\beta\\)s) (log odds ratios) of the model are called Marginal Utilities.\n\nThe observed response is typically polytomous and the categories are called Alternatives which are the potential choices of the Decision Maker.\n\nNotes from ResEcon 703 Video Course\n\nWeeks 3, 4, 9, 10, 12, and 13\nVideo lectures talk through mathematics (interpretations, motivations, benefits, limitations, etc.) of the approaches. Each week ends with a coding session illustrating the approaches thatâ€™s not included in the videos but is included in the slides.\n\nThe slides are updated with new material while the videos have not.\n\ngithub with slides and problem sets/solutions with R code. Slides provide a deeper introduction to the application the algorithms and the econometrics. Problem set solutions have similar material but written out more clearly. {mlogit} used throughout.\n\nAlso see:\n\nDecision Intelligence\nRegression, Multinomial\nDiagnostics, Classification &gt;&gt; Multinomial\nClassification &gt;&gt; Discriminant Analysis &gt;&gt; Linear Discriminant Analysis (LDA)\nModel Building, brms &gt;&gt; Logistic Regression\n\nUse Cases:\n\nRespondents to a social survey are classified by their highest completed level of education, taking on the values (1) less than highschool, (2) highschool graduate, (3) some post-secondary, or (4) post-secondary degree.\nWomenâ€™s labor-force participation is classified as (1) not working outside the home, (2) working part-time, or (3) working full-time.\nVoters in Quebec in a Canadian national election choose one of the (1) Liberal Party, (2) Conservative Party, (3) New Democratic Party, or (4) Bloc Quebecois."
  },
  {
    "objectID": "qmd/econometrics-discrete-choice-models.html#sec-econ-dcm-terms",
    "href": "qmd/econometrics-discrete-choice-models.html#sec-econ-dcm-terms",
    "title": "Discrete Choice Models",
    "section": "Terms",
    "text": "Terms\n\nAlternative - The levels of a polytomous response in Random Utility Models.\nChoice Probability - The probability that a decision-maker will chose a particular alternative. The predicted response for a Random Utility Model.\nChoice Settings - Can describe in general the choice set itself, or can also be used to describe the context in which the choice by the decision-maker is taking place.\n\ne.g.Â binary, ordered multinomial, unordered multinomial, and count data frameworks\ne.g.Â educational setting, marketing setting\ne.g.Â costs associated with each alternative for each individual\n\nConsumer Surplus - Monetary gain to a consumer from â€œpurchasingâ€ a good for less than the value the consumer places on the good\nCost Trade-Offs - How changes in one in a choice attribute can be offset by another.\n\ne.g.Â If the operating cost were to increase by $1, what reduction the purchase price would result in the same utility/choice probability for the consumer.\n\nDiscounted Utility - The utility of some future event, such as consuming a certain amount of a good, as perceived at the present time as opposed to at the time of its occurrence. It is calculated as the present discounted value of future utility, and for people with time preference for sooner rather than later gratification, it is less than the future utility.\nMarginal Utility - Coefficients in the logit model (log odds ratios). The added satisfaction that a consumer gets from having one more unit of a good or service. The concept of marginal utility is used by economists to determine how much of an item consumers are willing to purchase.\nMarket Level - Environment or category where a class or brand of products share the same attributes\n\ne.g.Â All Cokes should cost the same in Indianapolis but that price may be different from the price of Cokes in Nashville. Therefore, Indianapolis and Nashville are separate markets.\n\nMarket Share - The percentage of total sales in an industry or product category that belong to a particular company during a discrete period of time. For a Random Utility Model, when the data is at the market level instead of the individual level, the predicted response is the Market Share.\nOutside Product (aka Outside Option) - Typically a â€œpurchase nothingâ€ indicator variable/variable category\n\nCan just be a vector of 0s\n\nRandom Utility Models - These models rely on the hypothesis that the decision maker is able to rank the different alternatives by an order of preference represented by a utility function, the chosen alternative being the one which is associated with the highest level of utility. They are called random utility models because part of the utility is unobserved and is modeled as the realization of a random deviate."
  },
  {
    "objectID": "qmd/econometrics-discrete-choice-models.html#sec-econ-dcm-randutmod",
    "href": "qmd/econometrics-discrete-choice-models.html#sec-econ-dcm-randutmod",
    "title": "Discrete Choice Models",
    "section": "Random Utility Models",
    "text": "Random Utility Models\n\nMisc\nRandom Utility - utility theory can readily be understood as the idea that people behave in line with self-interset where self-interest reflects peoplesâ€™ needs to save time and economize on effort.\n\nThese models rely on the hypothesis that the decision maker is able to rank the different alternatives by an order of preference represented by a utility function, the chosen alternative being the one which is associated with the highest level of utility. They are called random utility models because part of the utility is unobserved and is modeled as the realization of a random deviate.\n\nData sets used to estimate random utility models have therefore a specific structure that can be characterized by three indexes: the alternative, the choice situation, and the individual. The distinction between choice situation and individual indexes is only relevant if we have repeated observations for the same individual.\n\nExamples of variable types\n\nData Descriptions\n\nEach individual has responded to several (up to 16) scenarios.\nFor every scenario, two train tickets A and B are proposed to the user, with different combinations of four attributes: price (the price in cents of guilders), time (travel time in minutes), change (the number of changes) and comfort (the class of comfort, 0, 1 or 2, 0 being the most comfortable class).\n\nChoice Situation Specific\n\ndata1: Length of the vacation and the Season\ndata2: values of dist, income and urban are repeated four times.\n\ndist (the distance of the trip)\nincome (household income)\nurban (a dummy for trips which have a large city at the origin or the destination)\nnoalt the number of available alternatives\n\n\nIndividual Specific\n\ndata1: Income and Family Size\ndata2: None\n\nAlternative Specific\n\ndata1: Distance to Destination and Cost\ndata2:\n\ntransport modes (air, train, bus and car)\ncost for monetary cost\nivt for in vehicle time\novt for out of vehicle time\nfreq for frequency\n\n\n\nThe unit of observation is typically the choice situation, and it is also the individual if there is only one choice situation per individual observed, which is often the case\n\nAgent/Decison-Maker gets some amount of utility from each of the alternatives (set of actions the user can choose from)\nThe amount of utility for each alternative and each decision maker can depend on:\n\nobserved characteristics of the alternative themselves\nobserved characteristics of the decision maker\nunobserved characteristics\n\nUtility is not observed but can be inferred by factors that are observed (and how each factor affects Utility):\n\nChosen alternative, i, by each decision maker, n\nSome attributes about each alternative\nSome attributes about each decision maker.\n\nTotal Utility: \\(U_{nj} = V_{nj} + \\epsilon_{nj}\\)\n\nWhere Representative (or Observed) Utility, \\(V_{nj} = V(x_{nj}, s_n)\\)\n\n\\(x_{nj}\\) is the vector of attributes for alternative j specific to decison maker n\n\\(s_n\\) is the vector of attributes for decision maker n\\\n\n\\(\\epsilon_{nj}\\) is called the â€œRandom Utilityâ€ which is unobserved (i.e.Â the stuff we didnâ€™t adjust for).\n\nUtility Model: \\(U_{nj} = \\hat{\\beta}x_{nj} + \\epsilon_{nj}\\)\n\n\\(V_j = \\hat{\\beta} \\boldsymbol{x}\\)\nSince \\(U_{nj}\\) is unobserved, decision maker choice from alternatives is used as the response variable to proxy Utility.\n\\(\\beta\\) is called a â€œstructural parameterâ€\n\nChoice Probabilities\n\\[\nP_{ni} = \\int_{\\epsilon} \\mathbb{1} (\\epsilon_{nj} - \\epsilon_{ni} &lt; V_{ni} - V_{nj} \\;\\; \\forall j \\neq i) f(\\boldsymbol{\\epsilon}_n) d \\boldsymbol{\\epsilon}_n\n\\]\n\nAssuming the decision maker makes choices that maximize utility allows us to use choice probabilities to model which alternative maximizes utility.\nSee video for the derivation from \\(\\text{PR}(U_{ni} &gt; U_{nj} \\;\\; \\forall j \\neq i)\\)\nThis is the cumulative distribution for \\(\\epsilon_{nj} - \\epsilon_{ni}\\) which is a multidimensional integral over the density of unobserved utility, \\(f(\\epsilon_n)\\).\nAssumptions about \\(f(\\epsilon_n)\\) are what yields different discrete choice models"
  },
  {
    "objectID": "qmd/econometrics-discrete-choice-models.html#sec-econ-dcm-binlog",
    "href": "qmd/econometrics-discrete-choice-models.html#sec-econ-dcm-binlog",
    "title": "Discrete Choice Models",
    "section": "Binary Logit",
    "text": "Binary Logit\n\nMisc\nChoice Probability\n\\[\n\\begin {align}\nP_{n1} &= \\frac {1}{1 + e^{-(V_{n1} - V_{n0})}} \\\\\n&= \\frac{1}{1 + e^{-(\\hat \\beta \\boldsymbol{x})}}\n\\end {align}\n\\]\n\nThe probability of decision maker, \\(n\\), selecting choice, \\(1\\).\n\ni.e.Â predicted probability\n\nThe estimated utility of decision maker, \\(n\\), selecting choice, \\(1\\) is the predicted logit not the predicted probability\n\nCost Trade Offs\n\nSee Terms\nExample: An increase in â€œoperating costâ€ would need to be offset in how much of a reduction in purchasing price in order for the decision makerâ€™s utility not to change?\n\\[\n\\begin {align}\nU_{n1} &= \\beta_0 + \\beta_1 P_n + \\beta_2 C_n + \\epsilon_{n1} \\\\\ndU_{n1} &= \\beta_1 dP_n + \\beta_2 dC_n \\\\\ndU_{n1} &= 0 \\implies \\frac{dP_n}{dC_n} = -\\frac{\\beta_2}{\\beta_1}\n\\end {align}\n\\]\nAlso see Example: Preferences for Air Conditioning\n\nImplied Discount Rate\n\nSee Terms\nExample:\n\\[\n\\begin {align}\n(1) \\;\\; U_{n1} &= \\alpha_0 + \\alpha_1(P_n + \\frac{1}{\\gamma}C_n) + \\omega_n \\\\\n(2) \\;\\; U_{n1} &= \\beta_0 + \\beta_1P_n + \\beta_2C_n + \\epsilon_{n1} \\\\\n&\\therefore \\; \\alpha_1 = \\beta_1 \\;\\&\\; \\frac{\\alpha_1}{\\gamma} = \\beta_2 \\\\\n&\\implies \\gamma = \\frac{\\beta_1}{\\beta_2}\n\\end {align}\n\\]\n\n\\(P_n\\) is product price and \\(C_n\\) is the productâ€™s operating cost\n\\(\\gamma\\) is the discount rate\n\\(\\alpha_1\\) is supposed to be the marginal utility for household income but it doesnâ€™t play into the discount rate calculation\n(1) is the general formula for a householdâ€™s expected utility after purchasing the product which assumes infinite time horizon for operating cost\n(2) binary logistic model equation for utility\n\n\nExample: Student preferences for either driving their car or riding the bus to campus\n\nModel\nmodel_1a &lt;- \n  glm(formula = car ~ cost.car + time.car + time.bus,\n      family = 'binomial',\n      data = data_binary)\n\ncar - TRUE/FALSE binary variable that indicates whether the decision maker chose to drive (TRUE) or take the bus (FALSE)\ncost.car - continuous, cost (in dollars) to drive to campus\ntime.car - continuous, time (in minutes) to drive to campus\ntime.bus - time (in minutes) to ride the bus to campus\n\nsummary Results\n#&gt; Coefficients:\n#&gt;             Estimate Std. Error z value Pr(&gt;|z|)    \n#&gt; (Intercept)  2.23327    0.34662   6.443 1.17e-10 ***\n#&gt; cost.car    -2.07716    0.73245  -2.836  0.00457 ** \n#&gt; time.car    -0.33222    0.03534  -9.400  &lt; 2e-16 ***\n#&gt; time.bus     0.13257    0.03240   4.092 4.28e-05 ***\n\nCoefficents (log ORs) are marginal utilities\nThe cost of driving and the time spent driving both decrease the utility of driving, and the time spent riding the bus increases theutility of driving relative to riding the bus.\nIntercept: Driving a car generates 2.23 â€œutilsâ€ of utility\ncost.car: Each additional dollar of driving cost reduces utility of driving by 2.08\ntime.car: Each additional minute of driving reduces utility of driving by 0.33\ntime.bus: Each additional minute riding on the bus increases utility of driving by 0.13\n\nThe dollar value that a student places on each hour spent driving and on each hour spent on the bus.\n## Calculate hourly time-value for each commute mode\nabs(coef(model_1a)[3:4] / coef(model_1a)[2]) * 60\n#&gt; time.car time.bus\n#&gt; 9.596257 3.829494\n\nEach hour of driving has a dollar value of $9.60 and each hour of bus riding has a dollar value of $3.83.\n\nIn other words, a student would be willing to pay $9.60 to spend one less hour commuting by car but only $3.83 to spend one less hour commuting by bus.\n\nSince the time variables are in minutes, I wouldâ€™ve guessed that you would divide by 60 to get hours. Dividing by 60 gives you values of only 1000ths of a dollar though, so I guess not.\n\nAllowing cost to vary inversely by income (aka Heterogeneous Parameter)\nmodel_1b &lt;- \n  glm(formula = car ~ I(cost.car / income) + time.car + time.bus,\n      family = 'binomial',\n      data = data_binary)\n\nsummary(model_1b)\n#&gt; Coefficients:\n#&gt;                     Estimate Std. Error z value Pr(&gt;|z|)    \n#&gt; (Intercept)          2.26541    0.33110   6.842 7.81e-12 ***\n#&gt; I(cost.car/income) -53.63314   14.54884  -3.686 0.000227 ***\n#&gt; time.car            -0.33521    0.03484  -9.622  &lt; 2e-16 ***\n#&gt; time.bus             0.13589    0.02880   4.719 2.37e-06 ***\n\nDividing cost by income takes into account that students with different incomes might have different sensitivities to cost\n\nincome - student annual income (in 1000 dollars)\n\nA higher level of income yields a lower marginal utility of cost.\nIntercept: Driving a car generates 2.27 â€œutilsâ€ of utility\ncost.car/income: An additional 0.1 percentage point of cost.car (i.e.Â driving cost) as a share of income reduces utility by -53.63\n\nI think a â€œshareâ€ means that proportion of car.cost to income increases by 0.1 of a percentage point\n\nCalculate marginal utility of car cost at different incomes\n-coef(model_1b)[2] / c(15, 25, 35)\n#&gt; [1] 3.575543 2.145326 1.532375\n\nHe doesnâ€™t give units when stating these results, but did so when taking the ratio of 2 marginal utilities.\nHe takes the negative here, but Iâ€™m not sure if thatâ€™s because you always take the negative or heâ€™s just looking for the magnitude (i.e.Â abs).\n\nI feel like itâ€™s the latter. If thatâ€™s the case though, then it will always be the case that a larger income means less absolute marginal utility no matter the value of the coefficient.\nSo this seems that your making that assumption beforehand by choosing to divide by income instead of multiplying by it, and the goal is to just calculate the amounts by which the effect of cost is affected by larger incomes.\n\n\nCalculate hourly time-value for each commute mode at different incomes\nrep(abs(coef(model_1b)[3:4] / coef(model_1b)[2]), 3) * \n  c(rep(15, 2), rep(25, 2), rep(35, 2)) * 60\n#&gt; time.car  time.bus  time.car  time.bus  time.car  time.bus \n#&gt; 5.625096  2.280260  9.375160  3.800433 13.125224  5.320607 \n\nAt each of these incomes, each hour of driving has a dollar value of $5.63, $9.38, and $13.13, respectively; and each hour of bus riding has a dollar value of $2.28, $3.80, and $5.32, respectively.\n\n\n\nExample: Visualization\n\nKernel Density of Estimated Utilities\n\n## Plot density of utilities\nac_data %&gt;%\nggplot(aes(x = utility_ac_logit)) +\ngeom_density() +\nxlab('Utility of air conditioning') +\nylab('Kernel Density')\n\nWhere utility_ac_logit = predict(binary_logit_mod)\n\nCan also do this with choice probabilities\n\n\nFraction of Choice (Adoption) vs Estimated Utility\n\nac_data %&gt;%\n  mutate(bin = cut(utility_ac_logit,\n                   breaks = seq(-3, 2, 0.25),\n                   labels = 1:20)) %&gt;%\n  group_by(bin) %&gt;%\n  summarize(fraction_ac = mean(air_conditioning), .groups = 'drop') %&gt;%\n  mutate(bin = as.numeric(bin),\n         bin_mid = 0.25 * (bin - 1) - 2.875) %&gt;%\n  ggplot(aes(x = bin_mid, y = fraction_ac)) +\n    geom_point() +\n    xlab('Utility of air conditioning') +\n    ylab('Fraction with air conditioining')\n\nFrom Week 4 slides at around slide 59\nShows the fraction of adoption of air conditioning (choice 1) within each bin of estimated utility.\n\nCan also do this with choice probabilities\n\nIn general, we expect decision makers with greater estimated utility to choose air conditioning more often which is what is seen.\n\nThere are a couple of exception around bin -1.5, but maybe those are due to there not being enough individuals in those bins to be representative. Otherwise, the model is missing something for these individuals.\n\nIâ€™m not sure if this should be linear or maybe sigmoid â€” itâ€™s a proportion vs a logit. Think Iâ€™m leaning towards sigmoid.\n\nHe did do this with choice probabilities and that one looked linear which is what youâ€™d expect since itâ€™s like a observed vs predicted plot.\n\n\n\nExample: Marginal Effects and Elasticities\n\nThese formulas are the same as the ones shown in the Multiomial Logit section\nThese values are on the observation level so one useful thing to look at would be summary statistics and density plots\nMarginal Effects\nac_data &lt;- ac_data %&gt;%\nmutate(marg_eff_system = coef(binary_logit_mod)[2] *\n          probability_ac_logit * (1 - probability_ac_logit),\n       marg_eff_operating = coef(binary_logit_mod)[3] *\n          probability_ac_logit * (1 - probability_ac_logit))\nElasticities\nac_data &lt;- ac_data %&gt;%\nmutate(elasticity_system = coef(binary_logit_mod)[2] *\n          cost_system * (1 - probability_ac_logit),\n       elasticity_operating = coef(binary_logit_mod)[3] *\n          cost_operating * (1 - probability_ac_logit))\n\nExample: Preferences for Air Conditioning\n\nModel\nbinary_logit &lt;- \n  glm(formula = air_conditioning ~ cost_system + cost_operating,\n      family = 'binomial',\n      data = ac_data)\n\nair_conditioning: Binary TRUE/FALSE, the choice between buying an AC (TRUE) or not (FALSE)\ncost_system: Cost of buying the AC\ncost_operating: Cost of operating the AC\n\nCost Trade-Offs\n-coef(binary_logit)[3] / coef(binary_logit)[2]\n## cost_operating\n## -5.186972\n\nSays a dollar increase in AC purchasing cost would need to be offset by a decrease of $5.19 in operating cost in order for the customer utility for buying an AC to remain the same.\n\nImplied Discount Rate\ncoef(binary_logit)[2] / coef(binary_logit)[3]\n## cost_system\n## 0.1927907"
  },
  {
    "objectID": "qmd/econometrics-discrete-choice-models.html#sec-econ-dcm-mnl",
    "href": "qmd/econometrics-discrete-choice-models.html#sec-econ-dcm-mnl",
    "title": "Discrete Choice Models",
    "section": "Multinomial Logit (MNL)",
    "text": "Multinomial Logit (MNL)\n\nSee Regression, Multinomial &gt;&gt; Multinomial Logit for examples\nAKA Generalized Logit\nWhen the polytomous response has m levels (aka Alternatives), the multinomial logit model comprises mâˆ’1 log-odds comparisons with a reference level, typically the first or last.\n\nThe likelihood under the model and the fitted response probabilities that it produces are unaffected by choice of reference level, much as choice of reference level for dummy regressors created from a factor predictor doesnâ€™t affect the fit of a regression model.\n\nChoice Probability for alternative, i, and decision-maker, n:\n\\[\nP_{ni} = \\frac {e^{V_{ni}}}{\\sum_j e^{V_{nj}}} = \\frac {e^{\\hat{\\beta}x_{ni}}}{\\sum_j e^{{\\hat{\\beta}x_{nj}}}}\n\\]\n\nThe probability that decision-maker, \\(n\\), chooses alternative, \\(i\\)\nPredicted probability output from the model\n\nMarket Shares\n\nMarket Share for alternative, i\n\\[\nS_i = \\frac {e^{V_i}}{\\sum_j e^{V_j}}\n\\]\n\nWhat distinguishes this from a choice probability is that unit is at the market level and not the individual level (see \\(\\vec{x}\\) in the next section), hence no â€œnâ€ in the equation\n\nThe difference in market share of products i and j\n\\[\n\\ln (S_i) - \\ln (S_j) = \\hat{\\beta} (x_i - x_j)\n\\]\n\n\\(x_i\\) is a set of variables that describe product i.\n\nTherefore these will be the same for every decision-maker.\n\n\\(j\\) is typicially a reference level (aka outside option)\n\nIf the products were a product such as cereal, then the outside option would typically be â€œnot buying cerealâ€ where the utility is 0.\n\n\n\nConsumer Surplus\n\nThe expected consumer surplus that decision maker n obtains when faced with a choice setting is\n\\[\n\\nabla \\mathbb{E}(CS_n) = \\frac{1}{\\alpha_n} \\left[\\ln \\left(\\sum_{j=1}^{J^1} e^{V^1_{ng}} \\right) - \\ln \\left(\\sum_{j=0}^{J^1} e^{V^0_{ng}} \\right) \\right]\n\\]\n\nAssumptions\n\nExogenity of the alternative attribute variables of the alternatives: \\(\\mathbb{E}[\\epsilon \\; | \\; \\vec {x}] = 0\\) (i.e mean of the residuals is 0)\n\nExample of Endogenity\n\nTaking a car to work or taking the bus to work are the alternatives.\nIf a commuter likes to drive, they wonâ€™t care about living close to a bus stop\nIf a commuter likes to take the bus, they are more likely to live close to a bus stop\nTherefore your logit model will be biased because any attribute variable that is related to the distance a decision-maker is from a bus stop will be endogenous.\n\ne.g.Â Time each mode of transportation takes to get to work will be endogenous because distance from the bus stop affects time and distance to bus stop is correlated with a decision maker already biased towards buses and not with a decision maker who would choose bus or car.\n\ni.e.Â distance isnâ€™t randomly distributed among all decision makers.\n\n\n\nSolution: TBD in a later lecture\n\niid residuals following a Gumbel distribution\n\nResiduals are the unobserved utility (i.e.Â random utility).\nFor panel data, itâ€™s unlikely that each unobserved alternative attribute for a decision maker is independent across a time component which makes this a bad model for such data.\n\ne.g.Â If a person chooses Count Chocula one day and Fruit Loops the next, itâ€™s unlikely that the unobserved components of utility that led to the Count Chocula choice arenâ€™t correlated with the unobserved components of utility that led to the Fruit Loops choice.\n\n\n\nSubstitution Patterns\n\nIndependence of Irrelavent Alternatives (IIA) Property of Logit models\n\nExample: car, blue bus , red bus\n\nIn the beginning, the decision maker only has 2 choices: drive car or ride blue bus.\nThe probability of choosing the alternative car is the same for a decision maker as choosing the alternative blue bus: \\(P_c = P_{bb} = 1/2\\)\nThen a red bus is added to the choices, but the decision maker doesnâ€™t care whether the bus is red or blue. Therefore the blue bus and the red bus have the same choice probability: \\(P_{bb} = P_{rb}\\)\nBut from the IIA property, the ratio of choice probability between car and blue bus is not changed after introduction of the red bus. Therefore, \\(P_c = P_{bb} = P_{rb} = 1/3\\)\nIn reality, the choice probability for car has NOT decreased, since for decision maker, the choice is actually between car and bus â€” color isnâ€™t a relative factor\n\nExample: Hummer, Escalade, Prius\n\nHummer lowers price.\nWill Hummer attract more Escalade drivers than Prius?\nThe logit model says that â€œproportional substitution to the Hummer will be equal for the Escalade and the Prius.\n\ni.e.Â The lowering of the Hummer price will have an equal effect on Escalade sales and Prius sales which is unrealistic\n\n\n\nProportional Substitution\n\nNote the logit cross-elasticity equation for alternative, i, given a change in alternative, j (See below, Elasticity &gt;&gt; Cross Elasticity). The change in choice probability for alternative i only depends on the change to the attribute for alternative, j.\nThis means that we can substitute any alternative for i (other than j) and the cross-elasticity will be the same and therefore the change in choice probability will be the same.\n\ni.e.Â We change the value of an attribute for alternative j. Then, all choice proabilities for the other alternatives (other than j) change by the same proportion. Doesnâ€™t matter if alternative j is correlated more strongly with another alternative or not.\n\nShows a lack of flexibility in the logit model â€” there is no structure to model the correlation between alternatives. (See Mixed Logit)\n\n\nMarginal Effects\n\\[\n\\text{ME} = \\frac {\\partial P_{ni}} {\\partial z_{ni}} = \\beta_z P_{ni}(1-P_{ni})\n\\]\n\nThe change in the probability of choosing alternative, i, after a change in the attribute of alternative, i.\n\\(P_{ni}\\) is the probability of decision maker, n, choosing alternative i.\nCross Marginal Effect \\[\n\\text{CME} = \\frac {\\partial P_{ni}} {\\partial z_{nj}} = -\\beta_z P_{ni}P_{nj}\n\\]\n\nThe change in the probability of choosing alternative, \\(i\\), after a change in the attribute of alternative, \\(j\\).\n\n\nElasticity\n\\[\nE_{iz_{ni}} = \\beta_z z_{ni} (1-P_{ni})\n\\]\n\nMarginal Effects and Elasticities are similar except elasticities are percent change.\n\ne.g.Â a percentage change in a regressor results in this much of a percentage change in the response level probability\n\n\\(E_{iz_{ni}}\\) is the elasticity of the probability of alternative, i, with respect to \\(z_{ni}\\), an observed attribute of alternative, i\n\\(P_{ni}\\) is the predicted probability of alternative, i, for decision maker, n.\nCross Elasticity \\[\nE_{iz_{ni}} = -\\beta_z z_{nj} P_{nj}\n\\]"
  },
  {
    "objectID": "qmd/econometrics-discrete-choice-models.html#sec-econ-dcm-nestlog",
    "href": "qmd/econometrics-discrete-choice-models.html#sec-econ-dcm-nestlog",
    "title": "Discrete Choice Models",
    "section": "Nested Logit",
    "text": "Nested Logit\n\nFits separate models for each of a hierarchically nested set of binary comparisons among the response categories. The set of mâˆ’1 models comprises a complete model for the polytomous response, just as the multinomial logit model does.\nMisc\n\nIIA still holds for two alternatives in the same dichotomy, but doesnâ€™t hold for alternatives of different different dichotomies\nBoth MNL and Nested Logit methods have have pÃ—(mâˆ’1) parameters. The models are not equivalent, however, in that they generally produce different sets of fitted category probabilities and hence different likelihoods.\n\nMultinomial logit model treats the response categories symmetrically\n\nExtensions\n\nPaired Combinatorial Logit\n\nAllows alternatives to be in multiple dichotomies and multiple hierarchies and for them to have more complex correlation structures\n\nIn nested logits, only alternatives within the same hierarchy are modeled as being correlated with each other.\n\nCreates pairwise dichotomies for each combination of alternatives (i.e.Â each alternative appears in J-1 nests)\nBest for data with fewer alternatives since the parameter space can explode fairly quickly\n\nGeneralized Nested Logit\n\nAllow alternatives to be in any dichotomy in any hierarchy, but assign a weight to each alternative in each dichotomy.\nEstimate Parameters: \\(\\lambda_k\\) (See Choice Probability below), \\(\\alpha_{jk}\\) : â€œweightâ€ or proportion of alternative, \\(j\\), in dichotomy, \\(k\\)\nNeed to be careful about how many dichotomies that you place each alternative, since the parameter space can explode fairly quickly\n\nHeteroskedastic Logit\n\nHeteroskedacity in this sense means that the variance of the unobserved utility (aka residuals) can be different for each alternative\n\\[\n\\mbox{Var}(\\epsilon_{nj}) = \\frac {(\\theta_j \\pi)^2}{6}\n\\]\nSince thereâ€™s no closed form solution, simulation methods must be usded to get the choice probabilities and model parameters.\n\n\n\nConstruction of Dichotomies\n\nBy the construction of nested dichotomies, the submodels are statistically independent (because the likelihood for the polytomous response is the product of the likelihoods for the dichotomies), so test statistics, such as likelihood ratio (G2) and Wald chi-square tests for regression coefficients can be summed to give overall tests for the full polytomy.\nNested dichotomies are not unique and alternative sets of nested dichotomies are not equivalent: Different choices have different interpretations. Moreover, and more fundamentally, fitted probabilities and hence the likelihood for the nested-dichotomies model depend on how the nested dichotomies are defined.\nExample: 2 methods of splitting a 4-level response into dichotomies\n\n\nLeft: Y = {1, 2, 3, 4} â†’ {1,2} vs {3,4} â†’ {1} vs {2} and {3} vs {4}\nRight: (Continuous Logit) Y = {1, 2, 3, 4} â†’ {1} vs {2, 3, 4} â†’ {2} vs {3, 4} â†’ {3} vs {4}\n\n{1} vs.Â {2,3,4} could represent highschool graduation\n{2} vs.Â {3,4} could represesnt enrollment in post-secondary education\n{3} vs.Â {4} could represent completion of a post-secondary degree.\n\n\n\nAssumptions\n\nAssumes Gumbel distribution of errors but relax the i.i.d. hypothesis\n\nAllows for the unobserved (and random) components of utility, \\(\\epsilon_{nj}\\) , to be correlated for the same decision maker\nExample: {drive} vs {carpool} and {bus} vs {train}\n\nModel allows from unobserved components of utility (i.e.Â residuals of the model) of drive and carpool to correlated for the same decision maker but not the unobserved components of utility for carpool and train.\n\n\nAssumes Exogenity (See Multinomial Logit &gt;&gt; Assumptions)\n\nThis model is also bad for panel data\n\n\nChoice Probability for alternative, i, and decision-maker, n\n\\[\nP_{ni} = \\frac {e^{V_{ni}/\\lambda_k}(\\sum_{j\\in B_k}e^{V_{nj}/\\lambda_k})^{\\lambda_k - 1}}{\\sum_{\\ell=1}^K (\\sum_{j\\in B_\\ell}e^{V_{nj}/\\lambda_\\ell})^{\\lambda_\\ell}}\n\\]\n\nThe probability that decision-maker, \\(n\\), chooses alternative, \\(i\\)\n\nPredicted probability output from the model\nWhen data is at the market level, the \\(n\\) index (i.e.Â individual) is removed from the equation above and the LHS becomes Market Share (See Terms). The difference in market share between alternatives (e.g.Â brands) is:\n\\[\n\\ln(S_i) - \\ln(S_m) = \\hat{\\beta}(x_i - x_m) + (1-\\lambda_k)\\ln S_{i|B_k} - (1-\\lambda_\\ell)S_{m|B_\\ell}\n\\]\n\nThe index, \\(m\\), is typically a reference level called the â€œoutside optionâ€ (See Terms) which is alternative of NOT doing an action where the utility is 0.\n\nIf the products were a product such as cereal, then the outside option would typically be â€œnot buying cerealâ€ which would have a utility is 0.\n\n\n\n\\(\\lambda_k\\) is a measuer of independence in dichotomy, \\(k\\) that gets estimated\n\\(1-\\lambda_k\\) is a measure of the correlation within dichotomy, \\(k\\)\nThe nested logit is consistent with the random utility model when \\(\\lambda_k \\in (0,1] \\; \\forall k\\)\n\nElasticity\n\\[\nE_{iz{ni}} = \\beta_z z_{ni} \\left(\\frac{1}{\\lambda_k} - \\frac{1-\\lambda_k}{\\lambda_k}P_{ni|B_k} - P_{ni}\\right)\n\\]\n\nMarginal Effects and Elasticities are similar except elasticities are percent change.\n\ne.g.Â a percentage change in a regressor results in this much of a percentage change in the response level probability\n\nElasticity of alternate, \\(i\\), in dichotomy, \\(k\\), for decision-maker, \\(n\\), with respect to its attribute, \\(z_{ni}\\)\n\nCross-Elasticity\n\\[\nE_{iz_{nm}} =\n\\left\\{ \\begin{array}{lcl}\n-\\beta_z z_{zm}P_{nm} \\left(1+ \\frac{1-\\lambda_k}{\\lambda_k} \\frac {1}{P_{nB_k}} \\right) & \\mbox{if} \\; m \\in \\beta_k \\\\\n-\\beta_z z_{zm}P_{nm} & \\mbox{if} \\; m \\notin \\beta_k\n\\end{array}\\right.\n\\]\n\nWhere\n\\[\nP_{nB_k} = \\sum \\limits_{j \\in B_k} P_{nj} \\;\\; \\text{and} \\;\\; P_{ni|B_k} = \\frac {P_{ni}}{P_{nB_k}} = \\frac {P_{ni}}{\\sum_{j \\in B_k} P_{nj}}\n\\]\nThe percent change in the probability of choosing alternative, \\(i\\), after a percent change in the attribute of alternative, \\(m\\).\nCross elasticity depends on whether both alternatives are in the same dichotomy."
  },
  {
    "objectID": "qmd/econometrics-discrete-choice-models.html#sec-econ-dcm-mixlog",
    "href": "qmd/econometrics-discrete-choice-models.html#sec-econ-dcm-mixlog",
    "title": "Discrete Choice Models",
    "section": "Mixed Logit",
    "text": "Mixed Logit\n\nIndividual heterogeneity can be introduced in the parameters associated with the covariates entering the observable part of the utility or in the variance of the errors.\n\nDoesnâ€™t exhibit IIA (See Multinomial Logit &gt;&gt; Substitution Patterns) since correlations between alternatives are modeled.\nModel creates a distribution of \\(\\beta\\)s, so \\(\\beta\\) is allowed to vary throughout a population.\n\nMisc\n\nMarket Level Data\n\nObserve price, market share, and characteristics of a product\n\n\nChoice Probability\n\\[\n\\begin{align}\n&P_{ni} = \\int L_{ni}(\\beta) \\; f(\\beta\\;|\\;\\boldsymbol{\\theta}) \\; d\\beta\\\\\n&\\mbox{where} \\; L_{ni}(\\beta) = \\frac {e^{V_{ni}(\\beta)}}{\\sum_{j=1}^J e^{V_{ni}(\\beta)}}\n\\end{align}\n\\]\n\n\\(\\boldsymbol{\\theta}\\) is a vector of distributional parameters (e.g.Â \\(\\mu, \\sigma^2\\) for a Normal Distribution)\n\nThe \\(\\theta\\) parameters are what get estimated by the model in order to calculate a distribution of \\(\\beta\\)s (i.e.Â Bayesian posterior)\n\nIntegrates over a density so there is not a closed form solution. Therefore, numerical simulation must be used.\n\nElasticities donâ€™t have a closed form solution either (Assume Marginal Effects are the same way)(See lecture videos for the math)\n\nThink of it as a weighted average of logit choice probabilities\n\nThe logit choice probabilities are evaluated a different values of \\(\\beta\\), and each logit choice probability is weighted by the density \\(f(\\beta\\;|\\;\\boldsymbol{\\theta})\\)\n\ni.e.Â Based on the density, more likely \\(\\beta\\)s will be more heavily weighted.\n\n\nIn statistical language, itâ€™s a mixed function of logit choice probabilities, \\(L_{ni}(\\beta)\\), and the mixing distribution, \\(f(\\beta\\;|\\;\\boldsymbol{\\theta})\\)\n\nRandom Coefficients\n\\[\nU_{nj} = \\hat{\\alpha}\\boldsymbol{x}_{nj} + \\hat{\\mu}\\boldsymbol{z}_{nj} + \\epsilon_{nj}\n\\]\n\n\\(x_{nj}\\), \\(z_{nj}\\) - Data for alternative j and decision maker n\n\\(\\hat{\\alpha}\\) - Vector of fixed coefficients (i.e.Â same for all decision makers)\n\\(\\hat{\\mu}_n\\) - Vector of random coefficients (i.e.Â a coefficient for each decision maker)\n\\(\\epsilon_{nj}\\) - Residual from an extreme value distribution (e.g.Â Gumbel)\nCorrelated Random Utility\n\nLet \\(\\nu_{nj} = \\boldsymbol{\\hat{\\mu}}_n \\boldsymbol{z}_{nj} + \\epsilon_{nj}\\) be the random (unobserved) component of utility and \\(\\mbox{Cov}(\\nu_{ni}, \\nu_{nj}) = \\boldsymbol{z}_{ni} \\Sigma \\boldsymbol{z}_{nj}\\) the covariance between random utilities of different alternatives where \\(\\Sigma\\) is the variance/covariance matrix for \\(\\boldsymbol{\\hat{\\mu}}\\)\nThis is a structure to model the correlation between alternatives\n\n\nPanel Data\n\nData where each decision maker makes multiple choices over time periods\nModel allows for unobserved preference variation through random coefficients, which yields correlations in utility over time for the same decision maker\nDecision maker, n, chooses from a vector of alternatives over T time periods\nUtility\n\\[\nU_{njt} = \\hat{\\beta}_n x_{njt} + \\epsilon_{njt}\n\\]\n\nThe utility, U, for decision maker, n, in choosing alternative, j, at time period, t.\n\\(\\hat{\\beta}\\) varies for each decision maker but is constant across time.\n\nLag or lead predictors can be included\nLagged responses can be included\n\nUse cases:\n\nHabit Formation\nVariety-Seeking Behavior\nSwitching Costs\nBrand Loyalty\n\n\nOnly models a sequence of static choices\n\nLagged responses account for past choices affecting current choices, but not how current choices affect future choices\nA fully dynamic discrete choice model models how every choice affects subsequent choices.\n\n\nIndividual-level Coefficients\n\nConditional Distribution of Coefficients: \\(h(\\beta \\;|\\; \\boldsymbol{y}_n, \\boldsymbol{x}, \\boldsymbol{\\theta})\\)\n\nDistribution of \\(\\beta\\) among the group, from a popuation with an unconditional distribution defined by \\(\\boldsymbol{\\theta}\\), who choose a sequence of alternatives \\(y_n\\) when faced with choice setting, \\(\\boldsymbol{x}\\)\n\\(y_n\\) is the sequence of choices made by decision maker, n (i.e.Â panel data)\n\\(\\boldsymbol{\\theta}\\) is the vector of \\(\\beta\\) distribution parameters\n\\(\\boldsymbol{x}\\) is the vector of choice attributes\nProportional to the product (i.e.Â joint distribution) of\n\\[\nP(\\boldsymbol{y}_n \\;|\\; \\boldsymbol{x}_n, \\beta) \\;\\times\\; f(\\beta \\;|\\; \\boldsymbol{\\theta})\n\\]\n\n(Left) The probability that an individual with coefficients \\(\\beta\\) would choose \\(y_n\\)\n(Right) The likelihood of observing \\(\\beta\\) in the population\n\n\nMean of the Conditional Distribution (aka Conditional Mean Coefficients)\n\nMore practical to calculate than the conditional distribution itself.\nItâ€™s the weighted average of vectors of \\(\\beta\\) drawn randomly from the \\(\\beta\\) distribution, \\(\\text f(\\beta|\\boldsymbol{\\theta})\\), \\(R\\) times.\n\\[\n\\begin {align}\n\\breve{\\beta}_n &= \\sum_{r=1}^R w_r \\beta_r \\\\\n\\mbox{where} \\;\\; w_r &= \\frac{P(\\boldsymbol{y}_n|x_n, \\boldsymbol{\\beta}_r)}{\\sum_{r=1}^R P(\\boldsymbol{y}_n|x_n, \\boldsymbol{\\beta}_r)}\n\\end {align}\n\\]\n\n\\(w\\) is the weight which is the proportion choice probability for draw, \\(r\\)\n\nA decision maker must make many choices for the conditional mean coefficient to approach the true individual-level coefficient.\n\nA Monte Carlo simulation in the textbook for these lectures said even after 50 choices there was substantial difference between the two values. Although, in the era of big data, it is conceivable to see a person make hundreds of choices.\n\n\nFuture Choice Probabilities\n\nUse past choices to define a conditional distribution of coefficients for the decision-maker\nThen use this conditional distribution, instead of the unconditional distribution, to calculate the mixed logit choice probabilities.\n\nNo closed form solution so must use simulation. See lecture video for the procedure, but itâ€™s similar to the procedure of the conditional mean coefficient above."
  },
  {
    "objectID": "qmd/econometrics-discrete-choice-models.html#sec-reg-multin-ddcm",
    "href": "qmd/econometrics-discrete-choice-models.html#sec-reg-multin-ddcm",
    "title": "Discrete Choice Models",
    "section": "Dynamic Discrete Choice Models",
    "text": "Dynamic Discrete Choice Models\n\nUnlike the previous static models , this model will explicitly represent how 1 choice affects future choice sets and therefore future utilities.\n\nStatic models assume that you make a choice based on the utility of that choice alone at that current time and not on some future utility associated with that first choice.\n\nUtility\n\nTwo Examples for â€œGo to college or get a jobâ€\n\nTotal Utility for:\n\nTwo Periods: The utility gained during 4 years of college or work + the utility gain by choosing a job from a job set that depends on the completion of college or working for 4yrs\nThree Periods: Same as two but add in utility gained after choosing a retirement plan from a set of plans that were determined by the job you chose in period 2.\n\nDecision maker attends college \\(\\; iff \\;\\; TU_C &gt; TU_W\\)\n2 Period Setting: During College/Work and Available Jobs After College/Work\n\\[\n\\begin {align}\nTU_C &= U_{1C} + \\lambda \\max_j (U_{2j}^C) \\\\\nTU_W &= U_{1W} + \\lambda \\max_j (U_{2j}^W)\n\\end {align}\n\\]\n\nCollege (\\(C\\)) or Work (\\(W\\)) for four years\n\\(U_1C\\) - Utility in period 1 from four years of college\n\\[\nU_{1C} = V_{1C} + \\epsilon_{1C}\n\\]\n\\(U_1W\\) - Utility in period 1 from four years of working (Similar equation to \\(U_{1C}\\) )\n\\(U_{2j}^C\\) - Utility in period 2 from job \\(j\\) after attending college\n\\[\nU_{2j}^C = V_{2j}^C + \\epsilon_{2j}^C\n\\]\n\\(U_{2j}^W\\) - Utility in period 2 from job \\(j\\) after working (Similar equation to \\(U_{2j}^C\\) )\n\\(\\max\\) says the person will take whichever job that gives the most utility\n\nJob choice set is different for decision makers who went to college than those that chose to work in the 1st period (even though both indexed by js).\n\n\\(\\lambda\\) reflects relative weighting of the two periods. Factor that\nâ€œdiscountsâ€ a future utility. Typically people assign more utility for current things than things in the future, so in that case, this factor will lessen utility.\n\n3 Period Setting: Before College, After College, and Start of Retirement\n\\[\n\\begin {align}\nTU_C &= U_{1C} + \\lambda \\max_j [U_{2j}^C + \\theta \\max_s (U_{3s}^{Cj})] \\\\\nTU_W &= U_{1W} + \\lambda \\max_j [U_{2j}^W + \\theta \\max_s (U_{3s}^{Wj})]\n\\end {align}\n\\]\n\n\\(J\\) possible jobs for a career over many future years\n\\(S\\) possible retirement plans\n\n\\(U_{3s}^{Cj}\\) - Utility in period 3 from retirement plan \\(s\\) after attending college in period 1 and working job \\(j\\) in period 2\n\nSimilar utility model equations as \\(U_{1C}\\) and \\(U_{2j}^C\\)\n\n\\(U_{3s}^{Wj}\\) - Utility in period 3 from retirement plan \\(s\\) after working in period 1 and working job \\(j\\) in period 2\n\n\\(\\max\\) is maximizing both job choice (period 2) and its associated retirement plan (period 3)\n\\(\\theta\\) - Reflects relative weighting of three periods. Discount factor thatâ€™s the similar to \\(\\lambda\\).\n\n\n\nNotation\n\n\\(\\{i_1, i_2, \\ldots, i_t\\}\\) - Sequence of choices up to and including period \\(t\\)\n\\(U_{tj}(i_1, i_2, \\ldots, i_{t-1})\\) - Utility obtained in period \\(t\\) from alternative \\(j\\), which depends on all previous choices\n\\(TU_{tj}(i_1, i_2, \\ldots, i_{t-1})\\) - Total Utility (current and all future time periods) obtained from choosing alternative \\(j\\) in period \\(t\\), assuming the optimal choice is made in all future periods. (aka Conditional Value Function) (i.e.Â conditional on the alternative)\n\nAll possible values of \\(TU_{tj}(i_1, i_2, \\ldots, i_{t-1})\\) need to be calculated in order to express the optimal choice in each time period\n\n\\(TU_t(i_1, i_2, \\ldots, i_{t-1})\\) - Total Utility obtained from the optimal choice in period \\(t\\), assuming the optimal choice is made in all future periods (aka Value Function or Valuation Function at time \\(t\\) )\n\n\\(TU_t(i_1, i_2, \\ldots, i_{t-1}) = \\max_j TU_{tj}(i_1, i_2, \\ldots, i_{t-1})\\)\n\n\nBellman Equation\n\nIncorporates Discounted Utility (See Terms) into the Value Functions\n\nGeneralization of the 2 â€œcollege or workâ€ examples above and gives a procedure for how one would go about calculating the utilities for each period.\n\nValue Functions\n\nConditional Value Function\n\\[\nTU_{tj}(i_1, \\ldots, i_{t-1}) = U_{tj}(i_1, \\ldots, i_{t-1}) + \\delta\\max_k[TU_{t+1,k}(i_1, \\ldots, i_t = j)]\n\\]\n\nRegarding \\(TU_{t+1}\\), since \\((i_1, \\ldots, i_{t-1})\\) says â€œgiven the sequence of choices prior to time, \\(t\\),â€ the appending of \\(i_t = j\\) is saying â€œgiven all the sequence of choices prior to time \\(t\\) and includeing the current period, \\(t\\), where choice \\(j\\) is made.â€\nNot sure what \\(k\\) is and he didnâ€™t say in the video.\n\nValue Function\n\\[\nTU_{t}(i_1, \\ldots, i_{t-1}) = \\max_j[U_{tj}(i_1, \\ldots, i_{t-1}) + \\delta\\;TU_{t+1}(i_1, \\ldots, i_t = j)]\n\\]\n\nSays that the total utility for time period \\(t\\) i\n\\(\\delta\\) - discount rate on utility in future periods (See Terms)\n\n\nProcedure: Work backwards from the last period to the first period to calculate the overall Total Utility. At each period (except the last period), you are using the utility calculated for the future period directly after it.\nAssumptions\n\nPerfect information about:\n\nUtility of each alternative in each future time period\nHow every possible sequence of choices affects this future utility\n\n\nWith J alternatives in each of T time periods, you have to calculate \\(J^T \\times T\\) utilities.\n\nTherefore, only model only broad time periods (e.g.Â [college, graduation, retirement]) in order to reduce the computational burden of calculating all these utilities."
  },
  {
    "objectID": "qmd/econometrics-discrete-choice-models.html#sec-econ-dcm-endo",
    "href": "qmd/econometrics-discrete-choice-models.html#sec-econ-dcm-endo",
    "title": "Discrete Choice Models",
    "section": "Endogeneity",
    "text": "Endogeneity\n\nNeed exogenous variation in our explanatory variables in order to give our parameter estimates a â€œcausalâ€ inerpretation\n\nIf data are endogenous, our parameters can be interpreted as a kind of correlation between the data and choices\nTrue exogenous variable are tough to come by\n\nExamples of Endogeneity\n\nHousing choice and commute choice are correlated\n\nPeople who like public transit live closer to transit stations which makes their travel time lower\nTherefore, the coefficient on travel time will be biased upwards\n\nPrice and unobserved quality are correlated\n\nProducts with higher unobserved quality cost more and are preferrred by customers\nTherefore, the coefficient on price with be biased downward and may even have the wrong sign\n\nPrice and unobserved marketing are correlated\n\nLarge marketing campaigns may be accompanied by sales or increased prices (increased demand \\(\\rightarrow\\) decreased supply \\(\\rightarrow\\) increased price)\nTherefore, the coefficient on price will be biased, but the direction is uncertain.\n\n\nSolutions\n\nBLP estimation\nControl Function estimation\n\nWhen to use Control Functions instead of BLP\n\nIf any market shares are zero\n\nConstant terms in BLP are not identified for zero market shares\n\nIf you need to control for individual-specific endogeneity rather than market-level endogeneity (not feasible in BLP)\nIf you donâ€™t want to use the Contraction Mapping algorithm.\n\nMore complex to code, computationally intensive\n\n\n\nBoth methods use instruments\n\nGood Instruments:\n\nCorrelated with explanatory variables\nExogenous (i.e.Â uncorrelated with residuals aka random utility aka unobserved utility)\n\nInstruments are very context specific. So in order to find good instruments for your context, you need to know the details about the process youâ€™re trying to model.\n\n\n\n\nBLP (Berry-Levinsoln-Pakes)\n\nUses instruments to isolate exogenous variation in explanatory variables\nMixed logit model of demand for a differentiated product using market-level data.\n\ni.e.Â Estimate how the attributes of a product affect consumer demand\n\nPrice is one of the most important attributes, but is almost certainly correlated with other unobserved attributes\nModel includes instruments in a nonlinear model\nUtility Model\n\nIf consumers also get utility from unobserved attributes, then Price is correlated with the composite error term (\\(\\xi_{jm} + \\epsilon_{njm}\\))\nTherefore observed utility, \\(V\\), gets separated into 2 componets, Consumer and Market.\n\\[\n\\begin {align}\nU_{njm} &= \\delta_{jm} + \\tilde V (p_{jm}, \\boldsymbol{x}_{jm}, \\boldsymbol{s}_n, \\boldsymbol{\\tilde \\beta}_n) + \\epsilon_{njm} \\\\\n\\mbox where \\;\\; \\delta_{njm} &= \\bar V (p_{jm}, \\boldsymbol{x}_{jm}, \\boldsymbol{\\bar \\beta}_n) + \\xi_{jm} = \\bar{\\hat{\\boldsymbol{\\beta}}}(p_{jm}, \\boldsymbol{x}_{jm}) + \\xi_{jm}\n\\end {align}\n\\]\n\n\\(\\tilde V (p_{jm}, \\boldsymbol{x}_{jm}, \\boldsymbol{s}_n, \\boldsymbol{\\tilde \\beta}_n)\\) - Component that varies by consumer\n\\(\\bar V (p_{jm}, \\boldsymbol{x}_{jm}, \\boldsymbol{\\bar \\beta}_n)\\) - Component that varies by products and markets\n\\(\\delta_{jm}\\) - Effectively becomes a product-market constant term (i.e.Â intercept that varies by product and market for a Mixed Logit model) that represents the average utility obtained by product \\(j\\) in market \\(m\\)\n\nThis term gets estimated in the top equation. Then, this \\(\\hat{\\delta}\\) is used as the LHS to estimate \\(\\bar{\\hat{\\boldsymbol{\\beta}}}\\)\n\n\\(p_{jm}\\) - Price of product \\(j\\) in market \\(m\\)\n\n1 product can be the â€œoutside productâ€ or purchase nothing category (See Terms) which could be a vector of 0s\n\n\\(\\boldsymbol{x}_{jm}\\) - Vector of non-price attribtuies of product \\(j\\) in market \\(m\\)\n\\(\\boldsymbol{s}_n\\) - Vector of demographice characteristics of consumer \\(n\\)\n\\(\\boldsymbol{\\beta}_n\\) - Vector of coefficients for consumer \\(n\\)\n\\(\\xi_{jm}\\) - Utility (average) of unobserved attributes of product \\(j\\) in market \\(m\\)\n\\(\\epsilon_{njm}\\) - idiosyncratic (i.e.Â different per consumer) unobserved utility\nThe Choice Probability equation for this model looks similar to the Mixed Logit choice probability equation (See lecture video for the math)\n\n\nProcedure\n\nEstimate the average utility for product \\(j\\) in market \\(m\\), including observable and unobservable attributes (top equation)\nRegress this average utilitiy value, \\(\\hat{\\delta}\\), on price, \\(p_{jm}\\), and other observable attributes, \\(x_{jm}\\), while instrumenting for price. (i.e.Â IV model)\n\nIssue\n\n\\(\\hat{\\delta}_{jm}\\) must be estimated for each product in each market. This can result in 100s or even 1000s of terms to estimate.\n\nBLP insights\n\nA set of unique \\(\\delta_{jm}\\) terms equates predicted market shares with observed market shares for a given set of \\(\\boldsymbol{\\theta}\\) parameters\n\nHigher \\(\\delta_{jm}\\) means there will be higher market shares for product \\(j\\) in market \\(m\\)\n\nTheir â€œContraction Mappingâ€ algorithm effectively finds these unique \\(\\delta_{jm}\\) terms\n\nContraction Mapping Algorithm\n\nStart with initial value, \\(\\delta^0\\)\nPredict the market share for the current constant value, \\(\\hat{S}_{jm}(\\boldsymbol{\\delta}^s)\\), for each product and each market\n\n\\(\\hat{S}_{jm}(\\boldsymbol{\\delta})\\) is the predicted market share (which is a function of delta)\n\nI think these should be the predicted values for the Utility model above except the choice probabilities are now market shares since weâ€™re predicting with market-level explanatory variables\n\nThe \\(s\\) in the \\(\\delta^s\\) term seems to be a counter for which iteration of this algorithm that youâ€™re on.\n\nAdjust each product-market constant term by comparing predicted and observed market share\n\\[\n\\delta_{jm}^{s+1} = \\delta_{jm}^s + \\ln \\left(\\frac{S_{jm}}{\\hat{S}_{jm}(\\boldsymbol{\\delta}^s)}\\right)\n\\]\n\ni.e.Â Weâ€™re adjusting our \\(\\delta\\) by the difference of log predicted and log observed market shares\n\nRepeat steps 2 and 3 until the algorithm converges to the unique set of \\(\\hat\\delta\\) (product-market) constants\n\nSince this is a unique set, I think algorithm has converged with the observed market shares equal the predicted market shares exactly.\n\n\nOverall Procedure\n\nOuter-Loop: Search over \\(\\boldsymbol{\\theta}\\) to optimize the estimation objective function\n\nInner-Loop: Use contraction mapping to find \\(\\boldsymbol{\\delta(\\theta)}\\), the vector of product-market constant terms conditional on \\(\\theta\\)\nUse \\(\\boldsymbol{\\theta}\\) and \\(\\boldsymbol{\\delta(\\theta)}\\) to simulate choice probabilities, \\(P_{njm}(\\boldsymbol{\\delta(\\theta)}, \\boldsymbol{\\theta})\\)\nUse the choice probabilities to calculate the estimation objective function\n\nEstimate \\(\\boldsymbol{\\bar\\beta}\\) by regressing \\(\\boldsymbol{\\delta_{jm}}\\) on \\((p_{jm}, \\boldsymbol{x}_{jm})\\) with price instruments, \\(\\boldsymbol{z}_{jm}\\)\n\nEstimator Options (See Week 13, video 5 lecture for details)\n\nMaximized Simulated Likelihood (MSL) for step 1 and Two-Stage Least Squares (2SLS) for step 2\nMSM - Some kind of method of moments algorithm\n\n\n\n\nControl Function\n\nUse instruments to control for endogeneity in explanatory variables\nUtility Model\n\\[\n\\begin {align}\nU_{nj} &= V(y_{nj}, \\boldsymbol{x}_{nj}, \\boldsymbol{\\beta}_n) + \\epsilon_{nj} \\\\\n\\\\\n\\mbox{where} \\;\\; y_{nj} &= W(\\boldsymbol{z}_{nj}, \\boldsymbol{\\gamma}) + \\mu_{nj} \\\\\n\\epsilon_{nj} &= CF(\\mu_{nj}, \\boldsymbol{\\lambda}) + \\tilde \\epsilon_{nj} \\\\\nCF(\\mu_{nj}, \\boldsymbol{\\lambda}) &= \\mathbb{E}[\\epsilon_{nj} \\;|\\; \\mu_{nj}]\n\\end {align}\n\\]\n\n\\(y_{nj}\\) - Endogenous explanatory variable (e.g price) for consumer \\(n\\) and product \\(j\\)\n\\(\\boldsymbol{x}_{nj}\\) - Vector of non-price attributes for consumer \\(n\\) and product \\(j\\)\n\\(\\boldsymbol{\\beta}_n\\) - Vector of coefficients for consumer \\(n\\)\n\nHas density, \\(f(\\boldsymbol{\\beta}_n,\\boldsymbol{\\theta})\\)\n\n\\(\\epsilon_{nj}\\) - Unobserved utility for consumer \\(n\\) and product \\(j\\) (i.e.Â residuals)\n\\(\\tilde \\epsilon_{nj}\\) - The leftover unobserved utility that is not correlated with \\(\\mu_{nj}\\)\n\nHas the conditional density \\(g(\\tilde \\epsilon_n, \\boldsymbol{\\mu_n})\\)\nThis term controls for the source of endogeneity and therefore \\(y_{nj}\\) is no longer endogenous.\n\n\\(\\boldsymbol{z}_{nj}\\) - Vector of exogenous instruments for \\(y_{nj}\\)\n\\(\\boldsymbol{\\gamma}\\) - Parameters that relate \\(y_{nj}\\) to \\(\\boldsymbol{z}_{nj}\\)\n\\(\\mu_{nj}\\) - Unobserved factors that affect \\(y_{nj}\\) (i.e.Â residuals)\n\\(CF(\\mu_{nj}, \\boldsymbol{\\lambda})\\) - Control Function that contains all the endogenous variation between \\(\\epsilon_{nj}\\) and \\(\\mu_{nj}\\)\n\nTypically modeled as linear, \\(CF(\\mu_{nj}, \\boldsymbol{\\lambda}) = \\lambda \\mu_{nj}\\), but itâ€™s important the this term is specified correctly else \\(y_{nj}\\) will remain endogenous.\n\n\nAssumptions\n\n\\(\\epsilon_{nj}\\) and \\(\\mu_{n}\\) are correlated. Therefore,\n\n\\(y_{nj}\\) and \\(\\epsilon_{nj}\\), so \\(y_{nj}\\) is endogenous\n\\(\\mathbb{E}[\\epsilon_{nj} \\;|\\; \\mu_{nj}] \\neq 0\\) hence the use of the control function\n\n\\(\\epsilon_{nj}\\) and \\(\\mu_{n}\\) are independent of \\(\\boldsymbol{z}_{nj}\\). Therefore,\n\n\\(\\boldsymbol{z}_{nj}\\) are good instruments for \\(y_{nj}\\)\n\n\nProcedure\n\nEstimate \\(\\hat\\mu_{nj}\\) by regressing \\(y_{nj}\\) on \\(\\boldsymbol{z}_{nj}\\)\n\nTherefore,\\(\\hat\\mu_{nj}\\) will be the residuals of this regression\n\nEstimate \\((\\boldsymbol{\\hat\\theta}, \\boldsymbol{\\hat\\lambda})\\) by Maximized Simulated Likelihood (MSL) using the simulated choice probabilities to construct a simulated log-likelihood function"
  },
  {
    "objectID": "qmd/econometrics-discrete-choice-models.html#multinomial-probit",
    "href": "qmd/econometrics-discrete-choice-models.html#multinomial-probit",
    "title": "Discrete Choice Models",
    "section": "Multinomial Probit",
    "text": "Multinomial Probit\n\nAssumes a Normal distribution of errors which can deal with heteroscedasticity and correlation of the errors."
  },
  {
    "objectID": "qmd/econometrics-fixed-effects.html#sec-econ-fe-misc",
    "href": "qmd/econometrics-fixed-effects.html#sec-econ-fe-misc",
    "title": "Fixed Effects",
    "section": "Misc",
    "text": "Misc\n\n\nModel with independent intercepts for each time point and/or case, which are called â€œfixed effectsâ€\n\nThe effects the omitted variables have on the subject at one time, they will also have the same effect at a later time; hence their effects will be constant, or â€œfixed.â€\nA â€œfixed effectâ€ in statistics is a non-random regression term, while a â€œfixed effectâ€ in econometrics means that the coefficients in a regression model are time-invariant\n\nNotes from\n\nhttps://www.econometrics-with-r.org/10-rwpd.html\nhttps://www.robertkubinec.com/post/fixed_effects/\n\nPackages\n\n{plm}\n\nFunctions for model estimation, testing, robust covariance matrix estimation, panel data manipulation and information.\n\n{fixest}\n\nFast estimation, has parallel option, glm option and many other features\n\n{estimatr}\n\nProviding a range of commonly-used linear estimators, designed for speed and for ease-of-use. Users can easily recover robust, cluster-robust, and other design appropriate estimates.\nUsers can choose an estimator to reflect cluster-randomized, block-randomized, and block-and-cluster-randomized designs.\n\n\nIf you used {plm} + {coeftest} and want stata errors, then vcov = vcovCL"
  },
  {
    "objectID": "qmd/econometrics-fixed-effects.html#sec-econ-fe-terms",
    "href": "qmd/econometrics-fixed-effects.html#sec-econ-fe-terms",
    "title": "Fixed Effects",
    "section": "Terms",
    "text": "Terms\n\nCoarse Clustering - Grouping the data into larger clusters or units. Each cluster represents a broader and more aggregated subset of observations (as compared to Fine Clustering).\n\nCan lead to lower variance in the estimated standard errors because it captures less of the within-cluster variation.\nMay be used when there is less within-cluster heteroscedasticity or correlation, or when computational efficiency is a concern.\n\nFine Clustering - Grouping the data into small clusters or units. Each cluster represents a relatively small and specific subset of observations in the dataset.\n\nCan lead to higher variance in the estimated standard errors because it captures more of the within-cluster variation.\nAppropriate when there is a substantial degree of heteroscedasticity or correlation within these small clusters.\n\nFixed Panel - When the same set of units/people/cases is tracked throughout the study\nHomogeneous (or Pooled) - Panel data models that assume the model parameters are common across individuals.\nHeterogeneous - Panel models allow for any or all of the model parameters to vary across individuals.\n\nFixed effects and random effects models are both examples of heterogeneous panel data models.\n\nRotating Panel - When the units/people/cases change during the study"
  },
  {
    "objectID": "qmd/econometrics-fixed-effects.html#sec-econ-fe-consid",
    "href": "qmd/econometrics-fixed-effects.html#sec-econ-fe-consid",
    "title": "Fixed Effects",
    "section": "Considerations",
    "text": "Considerations\n\nFixed Effects or Random Effects (aka mixed effects model)?\n\nIf thereâ€™s likely correlation between unobserved group/cases variables (e.g.Â individual talent) and treatment variable (i.e.Â E(Î±|x) != 0) AND thereâ€™s substantial variance between group units, then FE is a better choice (see 1-way assumptions or Econometrics, Mixed Effects, Frequentist &gt;&gt; Assumptions for more details)\nIf cases units change little, or not at all, across time, a fixed effects model may not work very well or even at all (SEs for a FE model will be large)\n\nThe FE model is for analyzing within-units variance\n\nDo we wish to estimate the effects of variables whose values do not change across time, or do we merely wish to control for them?\n\nFE: these effects arenâ€™t estimated but adjusted for by explicitly including a separate intercept term for each individual (Î±i) in the regression equation\nRE: estimates these effects (might be biased if RE assumptions violated)\nThe RE model is for analyzing between-units variance\n\nThe amount of within-unit variation relative to between-unit variation has important implications for these two approaches\n\nArticle with simulated data showed that within variation around sd &lt; 0.5 didnâ€™t detect the effect of explanatory variable but ymmv (depends on # of units, observations per unit, N)\n\nDurbinâ€“Wuâ€“Hausman test (plm::phtest)\n\nIf H0 is not rejected, then both FE and RE are consistent but only RE is efficient. â€“&gt; use RE but if you have a lot of data, then FE is also fine.\nIf H0 is rejected, then only FE is consistent â€“&gt; use FE\n\n\nValid research questions for using a fixed effect for:\n\nCases/Units (e.g.Â State, school, individuals, stores) - â€œHow much does a case unit change relative to other case units?â€\nTime (e.g.Â Year) - â€œHow much does a case change in relation to itself over time?â€\n\nHow much each case varies around its average. The larger this coefficient the more cases fluctuate in their outcomes\nExample: Survey data with individual incomes over time\n\nHow the measure is different in a particular year compared to the individual average (e.g., do they have a lower or higher income compared to their normal income).\n\n\nExamples\n\nWhether obtaining more education leads to higher earnings.\nWhether wealthier countries tend to be more democratic than poorer countries\n\n\nFixed Effects or First Difference Estimator (FD)?\n\nTaking the first difference is an alternative to the demeaning step in the FE model\nIf the error terms are homoskedastic with no serial correlation, the fixed effects estimator is more efficient than the first difference estimator.\nIf the error follows a random walk, however, the first difference estimator is more efficient. If T=2, then they are numerically equivalent, and for T &gt; 2, they are not.\n\nIs the panel data balanced?\n\nplm::is.pbalanced(&lt;data&gt;, index = c(\"&lt;id_var&gt;\", \"&lt;time_var&gt;\"))\nBalanced - Has the same number of observations for all groups/units at each time point\nUnbalanced - At least one group/unit is not observed every period\n\ne.g.Â Have missing values at some time observations for some of the groups/units.\n\nCertain panel data models are only valid for balanced datasets.\n\nFor such models, data will need to be condensed to include only the consecutive periods for which there are observations for all individuals in the cross section.\n\n\nOmitted variable bias\n\nMultiple regression can correct for observable omitted variable bias, however, this cannot account for omitted unobservable factors that differ (e.g.Â from state to state)Â \n\nThis refers to doing two multivariable regression models - one for each time period\n\nFE models control for any omitted variables that are constant over time but vary between individuals by explicitly including a separate intercept term for each individual (\\(\\alpha_i\\)) in the regression equation\nYou can difference the outcome and difference predictor variables from period 1 to period 2 in order to remove the effects of unobserved omitted variables that are constant between the time periods\nFrom Kubinec differs regarding omitted variables\n\nAny statistical model should have, as its first requirement, that it match the researcherâ€™s question. Problems of omitted variables are important, but necessarily secondary.\nFixed effects models do not control for omitted variables. What fixed effect models do is isolate one dimension of variance in the model. As a result, any variables that donâ€™t vary on that dimension are by definition removed from the model. This side-effect is trumpeted as the great inferential benefit of fixed effect models, but it has nothing to do with inference. Fixed effects (or their cousin, random effects/hierarchical models) are simply about selecting which part of the panel dataset is most germane to the analysis."
  },
  {
    "objectID": "qmd/econometrics-fixed-effects.html#sec-econ-fe-pit",
    "href": "qmd/econometrics-fixed-effects.html#sec-econ-fe-pit",
    "title": "Fixed Effects",
    "section": "Pitfalls",
    "text": "Pitfalls\n\nBickel: If you performed matching on your sample, donâ€™t condition on any of the matching variables\n\nCan result in collider bias and opening up a previously closed backdoor\nMe: Matching makes sense because FE model has â€œCommon Trendsâ€ assumption\n\nKubinec says\n\n2-way fixed models have big problems\n\nSlope Interpretation\n\nCases and time points are nested and we end up making comparisons across both dimensions simultaneously. There is no clear research question that matches this model.\nThe one known use of the model is for difference-in-difference estimation, but only with two time points. Says to read his paper for more details.\n\nIs this what the eor book is describing for unobserved omitted variables? (see above)\n\n\nSlope Value Unreliable\n\nOnly identifiable if thereâ€™s a different effect ofÂ xÂ onÂ yÂ for each time point/case\n\nI think heâ€™s saying if there is no variation in one of your fixed effects and you fit a two-way model anyways, the calculated effect is unreliable. He says the data looks normal and you wouldnâ€™t recognize what happened necessarily.\n\nWhen this model is unidentifiable, R fixes the problem by deleting the last dummy variable (created by factor(fixed_effect_var)) and spits out the estimate.\n\nThe coefficient estimate for the removed dummy variable shows-up as an NA in the summary\n\n\n\nItâ€™s best to choose whether within-case or between-case effect is more important and fit the 1-way model.\n\ni.e.Â It is important to think about which dimension is more relevant, and then go with that dimension.\nAssumptions for a model with just an cases fixed effect\n\nResiduals have mean = 0 (i.e.Â errors uncorrelated with X)\n\nif violated, then omitted variable bias\n\nX (variable of interest) is i.i.d\n\nwithin-cases, autocorrelation is allowed (e.g.Â states)\n\nlarge outliers unlikely\nno perfect multicollinearity between variables\n\n\n\nPotential danger of biased effects when treatment is assigned during different periods for each group\n\nExample: group 1 is untreated at periods 1 and 2 and treated at period 3, while group 2 is untreated at period 1 and treated both at periods 2 and 3\nWhen the treatment effect is constant across groups and over time, FE regressions estimate that effect under the standard â€œcommon trendsâ€ assumption.\n\nRequires that the expectation of the outcome without treatment follow the same evolution over time in every group\n\nEstimates can be severely biased â€“ and may even be incorrectly signed â€“ when treatment effects change over time within treated units (aka hetergeneous treatment effects)\nFundamentally, the main reason TWFE estimates get weird and biased with differently-timed treatments is because of issues with weightsâ€”in TWFE settings, treated observations often get negative weights and vice versa\nAccording to Jakiela (2021, 5), negative weights in treated observations are more likely in (1) early adopter countries, since the country-level treatment mean is high, and (2) later years, since the year-level treatment mean is higher.\n\n\nSo, in general, the bias comes from entity variable categories that received the treatment early and the biased weight estimates occur on observations with later time values. This is because of the extreme treatment imbalance during these ranges/intervals, and its effect on the outcome variable.\n\nHaving negative weights on treated observations isnâ€™t necessarily bad! Itâ€™s often just a mathematical artefact, and if you have (1) enough never-treated observations and (2) enough pre-treatment data, and if (3) the treatment effects are homogenous across all countries, it wonâ€™t be a problem. But if you donâ€™t have enough data, your results will be biased and distorted for later years and for early adopters.\nDiagnostics\n\nDo any treated units get negative weight when calculatingÂ Î²TWFE? Check this by looking at the weights\nCan we reject the hypothesis that the treatment effects are homogenous? Check this by looking at the relationship betweenÂ Yit andÂ Dit. The slope shouldnâ€™t be different.\n\ntreatment effect homogeneity implies a linear relationship between residualized outcomes and residualized treatment after removing the fixed effects\n\n\nComments\n\nShe states that sheâ€™s only looking for linearity between the two sets of residuals, but actually breaks it down further by checking whether the relationship varies by treatment. This whole procedure is computing a partial correlation except instead of the last step of measuring the correlation between the two sets of residuals (e.g.Â cor.test(treatment_resid, out_resid) and getting the p-value, she looks at an interaction.\nI donâ€™t understand the homogeneity check in 3.2 though. She says that if the linearity relationship varies by treatment then this breaks assumptions for TWFE models. Iâ€™ve only looked at her paper and the Chaisemartin paper, and the only assumptions I saw for TWFE models in general was the â€œcommon trendsâ€ and the â€œstrong exogeneityâ€ assumption. I think this is more likely to be about the â€œcommon trendsâ€ assumption, and my understanding of that one is that it pertains to the effect across time for a particular group. Iâ€™m guessing thereâ€™s a connection between those two concepts, but Iâ€™m not seeing it."
  },
  {
    "objectID": "qmd/econometrics-fixed-effects.html#sec-econ-fe-clus",
    "href": "qmd/econometrics-fixed-effects.html#sec-econ-fe-clus",
    "title": "Fixed Effects",
    "section": "Clusters",
    "text": "Clusters\n\nMisc\n\nNotes from Cluster-robust inference: A guide to empirical practice (Paper)\n\nSections: 4.2 (level of clustering), 4.3 (level of clustering), 4.3.2 (influential clusters), 8.1 (infl clusters), 8.2 (placebo regression)\n\nAlso see Econometrics, General &gt;&gt; Standard Errors &gt;&gt; HC and HAC vcov estimators\n\nCluster-Robust Variance Estimators (CRVE)\n\nâ€œRandom-effects model is the only model within the class of factor models for which including cluster fixed effects can remove all intra-cluster dependenceâ€\n\nThink this says that HC or HAC ({sandwich}) should be used for 2FE but not RE models\n\nâ€œEven very small intra-cluster correlations can have a large effect on standard errors when the clusters are largeâ€\nâ€œIt has become quite standard in modern empirical practice both to include cluster fixed effects (and perhaps other fixed effects as well) and also to employ cluster-robust inference.â€\n\nLevel of Clustering\n\nâ€œone or more fine clusters nested within each of the coarse clustersâ€\nâ€œClustering at too fine a level generally leads to serious over-rejection, which becomes worse as the sample size increases with the numbers of clusters at all levels held constantâ€\nâ€œClustering at too coarse a level also leads to both some over-rejection and some loss of power, especially when the number of clusters is small.â€\nIssues for Certain Rules of Thumb\n\nJust cluster at the coarsest feasible level\n\nMay be attractive when the number of coarse clusters G is reasonably large, but it can be dangerous when G is small, or when the clusters are heterogeneous in size or other features\n\nCluster at whatever level yields the largest standard error(s) for the coefficient(s) of interest\n\nWill often lead to the same outcome as the first one, but not always. When the number of clusters, G, is small, cluster-robust standard errors tend to be too small, sometimes much too small. Hence, the second rule of thumb is considerably less likely to lead to severe over-rejection than the first one. However, because it is conservative, it can lead to loss of power (or, equivalently, confidence intervals that are unnecessarily long).\n\n\nRecommended: Cluster at the treatment level\n\ne.g.Â If the treatment is assigned by classroom then cluster by classroom\nBut if thereâ€™s concern of significant spillover effects, then cluster at a coarser level than the treatment level (e.g.Â schools)\n\n\nDiagnostics\n\nStatistical testing for the correct level of clustering\n\nHard to tell but I donâ€™t think any of the tests were recommended in the paper\n\nChecking for influential clusters\n\nInfluential Cluster - Estimates change a lot when itâ€™s deleted.\nâ€œIn a few extreme cases, there may be a cluster \\(h\\) for which it is impossible to compute \\(Î²_j^{(h)}\\). If so, then the original estimates should probably not be believed.\n\nThis will happen, for example, when cluster \\(h\\) is the only treated one. Inference is extremely unreliable in that case.â€\n\n\nPlacebo Regressions\n\nProcess\n\nAdd a random dummy variable to the model\nfit model check if dummy variable is significant\nrepeat many times\n\nBecause a placebo regressor is artificial, we would expect valid significance tests at level Î± to reject the null close to Î±% of the time when the experiment is repeated many times.\nExample:\n\nClustering at levels below state-level leads to rejection rates far greater than Î±\nUsing a state-level CRVE is important for survey data that samples individuals from multiple states. If we fail to do so, we will find, with probability much higher than Î±, that nonsense regressors apparently belong in the model.\n\ni.e.Â placebo regressors are significant &gt; 5% of the time\n\n\nA placebo-regressor experiment should lead to over-rejection whenever both the regressor and the residuals display intra-cluster correlation at a coarser level than the one at which the standard errors are clustered. (e.g.Â &lt; 5%)\nIf the placebo regressor is clustered at the coarse level, we would expect significance tests based on heteroskedasticity-robust standard errors to over-reject whenever the residuals are clustered at either level. Similarly, we would expect significance tests based on finely-clustered standard errors to over-reject whenever the residuals are clustered at the coarse level. Table 4 in Section 8.2 displays both of these phenomena."
  },
  {
    "objectID": "qmd/econometrics-fixed-effects.html#sec-econ-fe-owfe",
    "href": "qmd/econometrics-fixed-effects.html#sec-econ-fe-owfe",
    "title": "Fixed Effects",
    "section": "One-Way Fixed Effects",
    "text": "One-Way Fixed Effects\n\nOnly compares different periods within the same cases category and discards the between-cases variance Steps\nSteps\n\nRemove endogeneity (resulting from omitted variable bias)\n\nFirst, the error is broken into 2 parts\n\\[\n\\begin{align}\n&y_{it} = \\beta x_{it}+ \\nu_{it} \\\\\n&\\text{where}\\: \\nu_{it} = \\alpha_i + \\epsilon_{it} = 0\n\\end{align}\n\\]\n\n\\(\\alpha\\) is the cases/units-specific or between part of the error\n\nunit-specific heterogeneity, the error component that is constant over time\nItâ€™s the unit fixed effect, the unit-specific intercept.\n\n\\(\\epsilon\\) is time-varying or within part of the error\n\nIdiosyncratic, varying both over units and over time\n\n\nThen, each group (aka cases) is centered by each groupâ€™s mean\n\\[\n\\begin{align}\ny_{it}-\\bar y_i &= \\beta(x_{it} - \\bar x_i) + (\\alpha_i - \\alpha_i) + (\\epsilon_{it} - \\bar \\epsilon_i) \\\\\n\\tilde y_{it} &= \\beta \\tilde x_{it} + \\tilde \\epsilon_{it}\n\\end{align}\n\\]\n\nThe centering eliminates all between-group variance, including the person-specific part of the error term (\\(\\alpha_i\\)), and leaves only the within-group variability to analyze\n\n\\(\\alpha_i\\) is a constant so itâ€™s mean is equal to itself\n\n\n\nOLS is performed after the endogeneity is removed.\n\nAssumptions\n\nFunctional Form\n\nAdditive fixed effect\nConstant and contemporaneous treatment effect (aka homogeneous treatment effects)\nLinearity in covariates\n\nTime-constant unobserved heterogeneity is allowed (not the case for Mixed Effects models)\n\ni.e.Â \\(\\mathbb{E}(\\alpha|x) \\neq 0\\) or correlation between unobserved unit variables that are constant across time and \\(x\\) is allowed\n\nThis correlation is seen in the figure at the top of section\n\nEach groupâ€™s \\(x\\) values get larger from left to right as each groupâ€™s \\(\\alpha\\) (aka \\(y\\)-intercepts) for each unit get larger time-constant, unobserved variablesexplain variation between cases units\n\n\n\nStrong (strict) Exogeneity\n\n\\(\\mathbb{E}(\\epsilon|x,\\alpha)=0\\)\nTime-varying unobserved heterogeneity biases the estimator\nAlso see Pitfalls &gt;&gt; Kubinec\n\n\nExample\ne2 &lt;- plm(wage ~ marriage, data = df2,\nÂ  Â  Â  Â  Â  index = c(\"id\", \"wave\"),\nÂ  Â  Â  Â  Â  effect = \"individual\", model = \"within\")\n\nWhere marriage is the variable of interest, id is the cases variable and wave is the time variable\nUsing effect = â€œindividualâ€, model = â€œwithinâ€ specifies a one-way fixed effects model"
  },
  {
    "objectID": "qmd/econometrics-fixed-effects.html#sec-econ-fe-twfe",
    "href": "qmd/econometrics-fixed-effects.html#sec-econ-fe-twfe",
    "title": "Fixed Effects",
    "section": "Two-Way Fixed Effects (TWFE)",
    "text": "Two-Way Fixed Effects (TWFE)\n\nAdds a time variable that is constant (fixed effect) across cases but varies over time\n\\[\ny_{it} = \\beta x_{it} + \\alpha_i + \\zeta_t + \\epsilon_{it}\n\\]\n\nWhere \\(\\zeta\\) is the time fixed effect\n\nSteps\n\nRemove endogeneity (resulting from omitted variable bias)\n\nFirst, the error is broken into 2 parts: \\(\\nu_{it} = \\alpha_i + \\epsilon_{it} = 0\\)\n\nWhere \\(\\alpha\\) is the cases-specific or between part of the error and \\(\\epsilon\\) is time-varying or within part of the error\n\nThen,\n\\[\n(y_{it} - \\bar y_i -\\bar y_t + \\bar y) = \\beta(x_{it} - \\bar x_i - \\bar x_t + \\bar x) +(\\epsilon_{it} = \\bar \\epsilon_i - \\bar \\epsilon_t + \\bar \\epsilon)\n\\]\n\nFor each group/case, variables are centered by that groupâ€™s mean\nFor each period, variables are centered by that time periodâ€™s mean\nThe grand mean is added back\n\n\nOLS is performed after the endogeneity is removed.\n\nAssumptions\n\nTime-constant unobserved heterogeneity is allowed (See 1-way FE assumptions)\nFunctional Form\n\nAdditive fixed effect\nConstant and contemporaneous treatment effect\nLinearity in covariates\n\nStrong (strict) Exogeneity (also see 1-way FE assumptions)\n\n\\(Îµ \\perp D_{is}, X_{i}, \\alpha_i, \\zeta_t\\)\n\nThis implies the below statement\n\nTreatment assignment, \\(D_i\\), for a given unit, \\(i\\), in time, \\(s\\), is independent of the potential outcomes for that unit in that time period\n\\[\n{Y_{it}(0), Y_{it}(1)} \\perp D_{is}\\;|\\; \\boldsymbol{X}_i^{1:T}, \\alpha_i, \\boldsymbol{f}^{1:T} \\quad \\quad \\forall\\; i, t, s\n\\]\n\ne.g A policy (i.e.Â treatment) doesnâ€™t get enacted in region because it experiences negative economic shocks and weâ€™re measuring some economic metric\nAs a result, if we only had observed outcomes (which of course is all we have), we can substitute either \\(Y_{it}(0)\\) or \\(Y_{it}(1)\\) depending on whether we observe \\(D_{is}= 1\\) or \\(D_{is}= 0\\) and we can still, at least theoretically, get an unbiased estimate of the treatment effect.\n\n\\(D\\) is the treatment variable so itâ€™s \\(x_{it}\\) in the other equations above and here, \\(X\\) is probably other adjustment variables\n\\(f\\) is the time fixed effect\n\n\nImplies treatment status is assigned randomly or at one shot, not sequentially\n\n\nCommon Trends\n\nSee Fixed Effects with Individual Slopes (FEIS) section for models that relax this assumption\nFor \\(t \\geq 2, \\mathbb{E}(Y_{g,t}(0) âˆ’ Y_{g,tâˆ’1}(0))\\) does not vary across group, \\(g\\)\n\n\\(Y_{g,t}(0)\\) denotes average potential outcomes without treatment in group \\(g\\) at period \\(t\\).\n\\(Y_{g,t}(1)\\) would denote average potential outcomes with treatment in group \\(g\\) at period \\(t\\).\ni.e.Â For each period after the first period, the expected change in outcome doesnâ€™t vary across group \\(g\\)\n\nExample\n\n\nBefore treatment (getting married), wages for the treatment group (top 2 lines) were growing at a substantially faster rate than the control group (bottom two lines). This violates the Common Trends assumption\n\n\n\nExample:\nfe3 &lt;- \n  plm(wage ~ marriage, data = df2,\nÂ  Â  Â  index = c(\"id\", \"wave\"),\nÂ  Â  Â  effect = \"twoways\", \nÂ  Â  Â  model = \"within\")\n\nWhere marriage is the variable of interest, id is the cases variable and wave is the time variable\nUsing effect = â€œtwowaysâ€, model = â€œwithinâ€ specifies a two-way effects model\n\nExample:\n\nModel\n\\[\n\\begin{align}\nY_{it} &= \\beta_0 + \\beta_1 X_{it} + \\gamma_2 D2_i + \\cdots + \\gamma_n DT_i + \\delta_2B2_t + \\cdots + \\delta_n BT_t + u_{it} \\\\\n\\text{FatilityRate}_{it} &= \\beta_1 \\text{BeerTax}_{it} + \\text{StateEffects} + \\text{TimeFixedEffect} + u_{it}\n\\end{align}\n\\]\n\nIncluding the intercept would allow for a change in the mean fatality rate in the time between the years 1982 and 1988 in the absence of a change in the beer tax.\nThe variable of interest is Beer Tax and itâ€™s effect on Fatality Rate\n\nBeer Tax is a continuous variable with a value for each unit and for each year\n\nThe state and time fixed effects are the dummy variables in the formal equation\nTheir coefficients start at 2 because the intercept coefficient is considered the first coefficient\n\nCode\n# Two ways to fit the model\n\nlm(fatal_rate ~ beertax + state + year - 1, data = Fatalities)\n\nfatal_tefe_mod &lt;- \n  plm::plm(fatal_rate ~ beertax,\nÂ  Â  Â  Â  Â  Â data = Fatalities,\nÂ  Â  Â  Â  Â  Â index = c(\"state\", \"year\"),\nÂ  Â  Â  Â  Â  Â # fixed effects estimator is also called the 'within' estimator\nÂ  Â  Â  Â  Â  Â model = \"within\",\nÂ  Â  Â  Â  Â  Â effect = \"twoways\") # twoways required for \"entities\" and \"time\" fixed effects\n\n# only calcs for variable of interest\n# if needed, dof = nrow(dat) - 1\ncoeftest(fatal_tefe_mod, vcov = vcovHC, type = \"HC1\")\n#&gt; t test of coefficients:\n#&gt;Â \n#&gt;Â  Â  Â  Â  Estimate Std. Error t value Pr(&gt;|t|)Â \n#&gt; beertax -0.63998Â  Â  0.35015 -1.8277Â  0.06865 .\n\n# moar adjustment vars\nfatalities_mod6 &lt;- \n  plm::plm(fatal_rate ~ beertax + year + drinkage\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  + punish + miles + unemp + log(income),\nÂ  Â  Â  Â  Â  Â index = c(\"state\", \"year\"),\nÂ  Â  Â  Â  Â  Â model = \"within\",\nÂ  Â  Â  Â  Â  Â effect = \"twoways\",\nÂ  Â  Â  Â  Â  Â data = Fatalities)\n\nstate and year variables need to be factors\nIntercept removed because it has no meaning in this context\n\n\nExample: {estimatr}\nmodel_lm_robust &lt;- \n    estimatr::lm_robust(primary ~ treatment,\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  fixed_effects = ~ country + year,\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  data = fpe_primary,\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  clusters = country, se_type = \"stata\")\n\ntidy(model_lm_robust)\n##Â  Â  Â  Â  term estimate std.error statistic p.value conf.low conf.high df outcome\n## 1 treatmentÂ  Â  20.4Â  Â  Â  9.12Â  Â  Â  2.24Â  0.0418Â  Â  0.867Â  Â  Â  Â  40 14 primary\n\nglance(model_lm_robust)\n##Â  r.squared adj.r.squared statistic p.value df.residual nobs se_type\n## 1Â  Â  0.768Â  Â  Â  Â  0.742Â  Â  Â  Â  NAÂ  Â  Â  NAÂ  Â  Â  Â  Â  14Â  490Â  stata\nExample: {fixest}\nmodel_feols &lt;- \n  fixest::feols(primary ~ treatment | country + year,\nÂ  Â  Â  Â  Â  Â  Â  Â  data = fpe_primary,\nÂ  Â  Â  Â  Â  Â  Â  Â  cluster = ~ country,\nÂ  Â  Â  Â  Â  Â  Â  Â  dof = dof(fixef.K = \"full\"))\n\ntidy(model_feols)\n## # A tibble: 1 Ã— 5\n##Â  termÂ  Â  Â  estimate std.error statistic p.value\n##Â  &lt;chr&gt;Â  Â  Â  Â  &lt;dbl&gt;Â  Â  &lt;dbl&gt;Â  Â  &lt;dbl&gt;Â  &lt;dbl&gt;\n## 1 treatmentÂ  Â  20.4Â  Â  Â  9.12Â  Â  Â  2.24Â  0.0418\n\nglance(model_feols)\n## # A tibble: 1 Ã— 9\n##Â  r.squared adj.r.squared within.r.squared pseudo.r.squared sigmaÂ  nobsÂ  AICÂ  BIC logLik\n##Â  Â  Â  &lt;dbl&gt;Â  Â  Â  Â  &lt;dbl&gt;Â  Â  Â  Â  Â  Â  &lt;dbl&gt;Â  Â  Â  Â  Â  Â  &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;Â  &lt;dbl&gt;\n## 1Â  Â  0.768Â  Â  Â  Â  0.742Â  Â  Â  Â  Â  Â  0.111Â  Â  Â  Â  Â  Â  Â  NAÂ  14.7Â  490 4071. 4280. -1985.\n\n# Standard print,summary output from a fixest model (from vignette)\nprint(fixest_pois_mod)\n#&gt; Poisson estimation, Dep. Var.: Euros\n#&gt; Observations: 38,325Â \n#&gt; Fixed-effects: Origin: 15,Â  Destination: 15,Â  Product: 20,Â  Year: 10\n#&gt; Standard-errors: Clustered (Origin)Â \n#&gt;Â  Â  Â  Â  Â  Â  Â  Estimate Std. Error t valueÂ  Pr(&gt;|t|)Â  Â \n#&gt; log(dist_km) -1.52787Â  0.115678 -13.208 &lt; 2.2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:Â  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; Log-Likelihood: -7.025e+11Â  Adj. Pseudo R2: 0.764032\n#&gt;Â  Â  Â  Â  Â  Â  BIC:Â  1.405e+12Â  Â  Squared Cor.: 0.612021\n\n# With clustered SEs\nsummary(fixest_pois_mod, vcov = \"twoway\")\n#&gt; Poisson estimation, Dep. Var.: Euros\n#&gt; Observations: 38,325Â \n#&gt; Fixed-effects: Origin: 15,Â  Destination: 15,Â  Product: 20,Â  Year: 10\n#&gt; Standard-errors: Clustered (Origin & Destination)Â \n#&gt;Â  Â  Â  Â  Â  Â  Â  Estimate Std. ErrorÂ  t valueÂ  Pr(&gt;|t|)Â  Â \n#&gt; log(dist_km) -1.52787Â  0.130734 -11.6869 &lt; 2.2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:Â  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; Log-Likelihood: -7.025e+11Â  Adj. Pseudo R2: 0.764032\n#&gt;Â  Â  Â  Â  Â  Â  BIC:Â  1.405e+12Â  Â  Squared Cor.: 0.612021"
  },
  {
    "objectID": "qmd/econometrics-fixed-effects.html#sec-econ-fe-feis",
    "href": "qmd/econometrics-fixed-effects.html#sec-econ-fe-feis",
    "title": "Fixed Effects",
    "section": "Fixed Effects with Individual Slopes (FEIS)",
    "text": "Fixed Effects with Individual Slopes (FEIS)\n\nFixed effects model that relaxes the Common Trends assumption (see 2-way FE assumptions above)\n\nGives each case (e.g.Â State, school, individual, store) itâ€™s own intercept and slope\nData are not cases â€œdemeanedâ€ like with a FE estimator, but â€œdetrendedâ€ by the predicted individual slope of each cases unit\n\nMisc\n\nNotes from https://ruettenauer.github.io/Panel-Data-Analysis/Panel_part2.html#Fixed_Effects_Individual_Slopes\n{feisr}\n** Each additional slope variable requires more observations per cases category **\n\nEach cases unit needs at least q+1 observations to contribute to the model. If not, they are dropped.\n\nWhere q number of slope parameters (including a constant)\n\nMost likely this refers to the number of slope variables + constant\nExample: Slope variables are exp + I(exp^2)\n\nq = number_of_slope_vars + constant = 2 + 1 = 3 observations for each unit are required.\n\n\n(Probably not this) Example (Based on the feisr vignette): Slope variables are exp + I(exp^2)\n\nq = number_of_cases_units * (number_of_slope_vars + constant)\nq = number_of_ids * 3\nThis is the actual number of slope parameters estimated but this could be huge, so I doubt itâ€™s this.\n\n\n\n\nModel Equation: \\(y_i = \\beta X_i + \\alpha_i W_i + \\epsilon_i\\)\n\n\\(W\\) is a matrix of slope variables\n\\(\\alpha\\) is a vector of estimated parameters for the slope variables\n\nProcess\n\nItâ€™s equivalent to a typical lm model except with dummies of your cases variable (e.g.Â â€œidâ€ below) and 2-way interaction terms for all combinations of dummies \\(\\times\\) each slope variable\nActual process (more efficient) (see article for more mathematical detail)\n\nEstimate the individual-specific predicted values for the dependent variable and each covariate based on an individual intercept and the additional slope variables of \\(W_i\\)\nDetrend the original data by these individual-specific predicted values\nRun an OLS model on the residual (â€˜detrendedâ€™) data\n\n\nExample: Does marrying increase (log) wages\nwages.feis &lt;- \n  feis(lnw ~ marry + enrol + yeduc + as.factor(yeargr)\nÂ  Â  Â  Â | exp + I(exp^2), \nÂ  Â  Â  Â data = mwp, \nÂ  Â  Â  Â id = \"id\",\nÂ  Â  Â  Â robust = TRUE)\n\nsummary(wages.feis)\n## Coefficients:\n##Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Estimate Std. Error t-valueÂ  Pr(&gt;|t|)Â  Â \n## marryÂ  Â  Â  Â  Â  Â  Â  0.0134582Â  0.0292771Â  0.4597Â  0.64579Â  Â \n## enrolÂ  Â  Â  Â  Â  Â  Â  -0.1181725Â  0.0235003 -5.0286 5.325e-07 ***\n## yeducÂ  Â  Â  Â  Â  Â  Â  -0.0020607Â  0.0175059 -0.1177Â  0.90630Â  Â \n## as.factor(yeargr)2 -0.0464504Â  0.0378675 -1.2267Â  0.22008Â  Â \n## as.factor(yeargr)3 -0.0189333Â  0.0524265 -0.3611Â  0.71803Â  Â \n## as.factor(yeargr)4 -0.1361305Â  0.0615033 -2.2134Â  0.02697 *Â \n## as.factor(yeargr)5 -0.1868589Â  0.0742904 -2.5152Â  0.01196 *Â \n## ---\n## Signif. codes:Â  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n##Â \n## Cluster robust standard errors\n## Slope parameters:Â  exp, I(exp^2)\n\nExperience (exp) is used for the slope variables.\nTo estimate the slope parameters, the relationship with wage (lnw) is assumed to be non-linear (exp + I(exp^2))\nInterpretation: Marrying doesnâ€™t reliably affect wages (p-value = 0.64579)"
  },
  {
    "objectID": "qmd/econometrics-general.html#sec-econ-gen-misc",
    "href": "qmd/econometrics-general.html#sec-econ-gen-misc",
    "title": "General",
    "section": "Misc",
    "text": "Misc\n\nPackages\n\nCRAN Task View\n\nATE or LATE?\n\nIf proposed policy is to give everyone the treatment, then ATE\nIf proposed policy only affects a subset, then maybe LATE is more appropriate"
  },
  {
    "objectID": "qmd/econometrics-general.html#sec-econ-gen-terms",
    "href": "qmd/econometrics-general.html#sec-econ-gen-terms",
    "title": "General",
    "section": "Terms",
    "text": "Terms\n\nDisturbances - the error term in an econometric models (aka residuals in a regression)\nEconomic Shock - refers to any change to fundamental macroeconomic variables or relationships that has a substantial effect on macroeconomic outcomes and measures of economic performance,\n\nExamples: unemployment, consumption, and inflation.\n\nEndogenous Variable - variables that are correlated with the population error term. â€œDetermined inside the modelâ€.\n\nAn observed endogenous variable is affected by other variables in the system (it is roughly equivalent to a dependent variable in an experiment). Itâ€™s the variable that show differences we wish to explain.\nWhen the causality between X and Y goes both directions, both are endogenous.\nAlso see Glossary: DS terms\n\nExogenous Variable - Variables that are NOT correlated with the population error term. â€œDetermined outside the modelâ€\n\nAn observed exogenous variable is not controlled by other variables in the system (it is roughly equivalent to an independent variable in an experiment). Itâ€™s the variable used to explain the differences in the endogenous variable.\n\nLimited Dependent Variable (LDV) - a variable whose range of possible values is â€œrestricted in some important way.â€\n\ni.e.Â censoring, truncating, discrete\ne.g.Â probabilities, or is constrained to be positive, as in the case of wages or hours worked"
  },
  {
    "objectID": "qmd/econometrics-general.html#sec-econ-gen-se",
    "href": "qmd/econometrics-general.html#sec-econ-gen-se",
    "title": "General",
    "section": "Standard Errors",
    "text": "Standard Errors\n\nHeteroskedastic Robust Standard Errors\n\nDiscussed by Zeileis in his econometrics book and in Whiteâ€™s paper\nTypes\n\nH0 is the original\nH1 follows H0 but corrects for degrees of freedom. Only unbiased when experiment is balanced.\n\nBalance example: data for vehicle accidents has state and year variables. Balanced is each state has accidents data for each year\nExample\nlmtest::coeftest(model, vcov. = vcovHC, type = \"HC1\") # vcovHC is part of the sandwich package\n\nH2 is unbiased when errors are homeoskedastic\n\ni.e.Â Donâ€™t just blindly use without checking for homeoskedastity\n\nH3 is derived from jackknife procedure\nAlso HC4, HC5, and modified HC4m\n\nHC4 corrects for high-leverage points\nCribari-Neto F., Da Silva W.B. (2011). â€œA New Heteroskedasticity-Consistent Covariance Matrix Estimator for the Linear Regression Model.â€ Advances in Statistical Analysis, 95(2), 129â€“146\n\n\nGuidelines\n\ntl;dr - Use HC3 (default method for vcovHC) for small to moderately sized data sets and jackknife, vcovBS(..., type = \"jackknife\") or vcovJK, for large datasets (HC3 computation will fail).\nIf sample size is small and heteroskedasticity present, then H0, H1,Â or H2 shouldnâ€™t be used.Â H3 isnâ€™t quite as reliable as regular OLS standard errors.\nIf heteroskedasticity present, then H3 is superior, otherwise H2 better than H1 which is better than H0\nNo real guidance on HC4, HC5, and modified HC4m. See paper (above)\n\n\n\n\nHeteroskedastic and Autocorrelation (HAC) Consistent\n\nDiscussed in Hanck book\nSimilar as for heteroskedasticity, autocorrelation invalidates the usual standard error formulas as well as heteroskedasticity-robust standard errors since these are derived under the assumption that there is no autocorrelation.\nR2 and F test not affected (Wald test preferred when heterskadasticity and autocorrelation present)\n\nnot sure where I got this. Wasnâ€™t from the Hanck book\n\nNewey-West is suboptimal; the QS kernel is optimal\n\n2021 tweetÂ  or paper\nuse sandwich::kernHAC(kernel=\"Quadratic Spectral\") for vcov arg in lmtest::coeftest()\n\nfyi Newey-West is a special case of these kernel estimators\n\nI donâ€™t think these are for clustered data\n\n\n\n\nClustered Standard Errors\n\nBelong to HAC type of standard errors. They allow for heteroskedasticity and autocorrelated errors within an entity but not correlation across entities.\nFrom https://datascience.blog.wzb.eu/2021/05/18/clustered-standard-errors-with-r/\nIn ordinary least squares (OLS) regression, we assume that the regression model errors are independent. This is not the case here: Each subject may be surveyed several times so within each subjectâ€™s repeated measures, the errors will be correlated. Although that is not a problem for our regression estimates (they are still unbiased [Roberts 2013], it is a problem for for the precision of our estimates â€” the precision will typically be overestimated, i.e.Â the standard errors (SEs) will be lower than they should be [Cameron and Miller 2013]. The intuition behind this regarding our example is that within our clusters we usually have lower variance since the answers come from the same subject and are correlated. This lowers our estimatesâ€™ SEs.\nvcovCL() will give STATA clustered standard errors.\nvcovCL()may be biased downwards\n\nCircumstances where it may be biased downwards (i.e.Â CIs too small)\n\nImprecise calculations arise when there is a low number of clusters (e.g.Â classrooms, schools)\n\nless than 50 clusters\n\nMulti-way (i.e.Â more than 1 fixed effect in panel data)\nIf the cluster sizes are wildly different.\nIf the intra-cluster correlations varies across clusters.\n\nSolutions:\n\nvcovJK -Not downward biased and yield better coverage rates for confidence intervals compared to other â€œrobustâ€ covariance estimates\n\nBased on leave-one-out estimates of the coefficients/parameters of a model. This means that the model is reestimated after dropping each observational unit once, i.e., each individual observation in independent observations or each cluster in dependent data\nHC3 seems to be an estimate of the Jackknife. To obtain HC3 covariances that exactly match the jackknife covariances, the jackknife has to be centered with the full-sample estimates (arg center = â€œestimateâ€) and the right finite-sample adjustment (?) has to be selected for the HC3.\n\nSatterthwaite corrected cluster robust sandwich estimator (?)\nWild Cluster Bootstrap {clubSandwich} {fwildclusterboot}\n\nComputationally expensive\n\nfwildclusterboot is VERY fast though\n\n\nFor small cluster sizes, choose wild cluster bootstrap over Satterthwaite corrected cluster robust sandwich estimator when: (article)\n\nExtreme treatment proportions (e.g.Â 80% obs treated, 10% control)\nExtreme differences in cluster sizes (i.e.Â extreme imbalance)\n\n{ceser} - Cluster-estimated standard errors\n\nMore conservative than the CRSE method, sandwich::vcovCL\nLess sensitive to the number of clusters and to the heterogeneity of the clusters, which can be a problem for both CRSE and bootstrap methods\nAlso has heteroskedacity corrections: HC0, HC1, HC2, HC3, or HC4"
  },
  {
    "objectID": "qmd/econometrics-general.html#sec-econ-gen-iv",
    "href": "qmd/econometrics-general.html#sec-econ-gen-iv",
    "title": "General",
    "section": "Instrumental Variable (IV)",
    "text": "Instrumental Variable (IV)\n\nModel\n\\[\n\\begin{align}\nY_i &= \\beta_0 + \\beta_1 X_i + \\beta_2 W_i + u_i \\\\\nX_i &= \\pi_0 + \\pi_1 Z_i + \\pi_2 W_i + v_i \\quad \\text{where}\\; i = 1, \\ldots, n\n\\end{align}\n\\]\n\n\\(u\\) and \\(v\\) are error terms, \\(X\\) is an endogenous variable, \\(Z\\) is an exogenous, instrumental variable, \\(W\\) is another predictor of the outcome variable except itâ€™s exogenous.\n\nMisc\n\nResources\n\nVideo series covering all the concepts\n\nIf X and u are correlated (endogenity) then OLS is inconsistent. So IV modeling uses the Z to isolate the part of X that isnâ€™t correlated with u. Potential causes for this correlation between X and u are:\n\nUnobservable omitted variable(s)\n\nUsing an IV allows us to use part of X than isnâ€™t associated with the omitted variable (i.e.Â confounder) but is still associated with Y\n\nMeasurement error\nsimultaneous causality\n\n\nTerms\n\nEndogenous Variable - Variables that are correlated with u, the population error term. â€œDetermined inside the modelâ€.\n\nWhen the causality between X and Y goes both directions, both are endogenous.\n\nExogenous Variable - Variables that are NOT correlated with u. â€œDetermined outside the modelâ€\n\nConditions for Valid Instruments\n\nInstrument Relevance: \\(Corr (X , Z) \\neqÂ 0\\) (Predictive of X)\n\nChecks (1st stage)\n\nInstrument should have a significant p-val\nF-Test stat &gt; 10; t-test stat &gt; 3.16 (rules of thumb)\n\n\nInstrument Exogeneity: \\(Corr (Z, u) = 0\\), \\(Corr(Z, v) = 0\\)\n\nCheck: â€œbalancing test (t-test); results should be insignificantâ€ (?)\n\nWouldâ€™ve thought you could do some kind of check on the residuals\n\n\nExclusion restiction: no impact on the dependent variable directly. It only impacts the dependent variable through its impact on the treatment variable\n\nCheck\n\nCorrelation? or a Partial Correlation?\n\n\n\nGood Instruments\n\nMay not have a strong causal relationship with x and therefore overlooked in the subject matter literature. Domain â€œfield workâ€ into the data generating process can help identify new instruments.\nThe effect of the instrument on the population is somewhat random (Instruments perform a quasi-randomization)\n\ne.g.Â A policy that may or may not have an effect on a population should make it exogenous. Something outside the control of the individual that influences her likelihood of participating in a program, but is otherwise not associated with her characteristics.\n\nExamples\n\nOutcome: log(wage),Â  predictor: womanâ€™s education, instrument: motherâ€™s education\n\nThis might not follow condition #2. If the daughterâ€™s â€œabilityâ€ is an omitted variable which is in the error term (u), and â€œmotherâ€™s educationâ€ are correlated, then #2 is violated.\n\nOutcome: #_of_kids (fertility), predictor: years_of_education, instrument: pre/post government policy that increases mandatory years of education\n\n\nSteps\n\nRegress X on Z where the Ï€-terms and Z are the parts uncorrelated with u.\nDrop the error term, v, which is the part of X thatâ€™s correlated with u\nRegress the Y on the modified X to estimate the Î²s\n\\[\n\\begin{align}\nY_i &= \\beta_0 + \\beta_1 \\tilde X_i + \\beta_2 W_i + u_i \\\\\n\\tilde X_i &= \\pi_0 + \\pi_1 Z_i + \\pi_2 W_i\n\\end{align}\n\\]\n\nWhen OLS is used to calculate the modified X, this process is called Two-Stage Least Squares (2SLS)\nEffects (Also see LATE in Effects, Calculating LATE and Compliance in Experiments, Analysis)\n\nUsing an instrumental variable allows us to identify the impact of the treatment on compliers. This is known as the local average treatment effect or LATE.\n\nThe LATE is the impact that the treatment has on the people that comply with the instrument.\n\n\\(\\hat\\beta_{IV}\\) only captures the causal effect of X on Y for compliers whose X vary by Z\n\\(\\hat\\beta_{IV}\\) is a weighted average of the treatment effect for compliers, with more weight given to more compliant groups\n\nExample: it is the impact of additional years of schooling (treatment) on fertility of women (outcome) affected by the school reform policy (instrument) only because they live in municipalities that had implemented it.\n**Requires an extra restriction on the instrumental variable**\n\nMonotonocity (no defiers): There is no one in the sample that does not receive the treatment because they received the instrument. This is usually a reasonable assumption to make but it can only be made based on intuition.\n\n(Mathematically) itâ€™s the number of people assigned and received treatment is always greater than or equal to the number of people not assigned yet received treatment.\n\n\nLATE = ATE if any of the following is true\n\nNo heterogeneity in treatment effects\n\n\\(\\beta_{1,i} = \\beta_1 \\quad \\forall i\\)\n\nNo heterogeneity in first-stage responses to the instrument Z\n\n\\(\\pi_{1,i} = \\pi_1 \\quad \\forall i\\)\n\nNo correlation between response to instrument Z and response to treatment X\n\n\\(\\text{Cov}(\\beta_{1,i} , \\pi_{1,i}) = 0\\)\n\n\n\nAlso see Complier Average Causal Effects (CACE) https://www.rdatagen.net/post/cace-explored/\n\nCaveats\n\nThe IV model is not an unbiased estimator, and in small samples its bias may be substantial\nA weak correlation between the instrument and endogenous variable may provide misleading inferences about parameter estimates and standard errors.\n\\(\\beta_1\\), the average treatment effect, assumes that all subgroups experience the roughly the same effect. If there are different subgroups of the population that are substantially affected differently, then a â€œweighted average of subsetsâ€ approach can be used.\n\nExample: Y = lung cancer, X = cigarettes, Z = cigarette tax. Perhaps people whose smoking behavior is sensitive to a tax may have a different Î²1 than other people"
  },
  {
    "objectID": "qmd/econometrics-general.html#sec-econ-gen-did",
    "href": "qmd/econometrics-general.html#sec-econ-gen-did",
    "title": "General",
    "section": "Difference-in-Differences Estimator",
    "text": "Difference-in-Differences Estimator\n\n\nWithout random samples as data, the selection into one of the two groups is by choice, thus introducing a selection bias\nSome treatments we wish to apply cannot be applied at the individual level but necessarily effect entire groups. Instead of comparing treatment and control groups within the same population at the same time, we can compare the relative change across treatment and control populations across time.\nWhen you have group-level treatments or data available, use random variation across populations to compare their overall trends over time\nPackages\n\n{did}\n\nManually\ndf_did &lt;- df %&gt;%\nÂ  mutate(after = year &gt;= 2014) %&gt;%\nÂ  mutate(treatafter = after*treat)\nreg &lt;- lm(murder ~ treat + treatafter + after, data = DiD)\nBasically a standard lm with an interaction between the treatment indicator and time period indicator demarking before/after treatment.\n\nPredictions\n\n\nThe DiD effect works out to be the interaction effect, \\(\\beta_3\\) = (2nd - 1st eq) - (4th - 3rd eq)\n\nExample\n\nWe want to estimate the effect of a store remodel on visits.\nA remodel affects all potential customers, so this â€œtreatmentâ€ cannot be applied at the individual level; in theory, it could be randomized to individual stores, but we do not have the budget for or interest in randomly remodel many stores before there is evidence of a positive effect.\n\nApproach\n\nIn two separate populations, one receives the treatment and one does not. We believe but-for the treatment the two populations would have similar trends in outcome\nWe can estimate the treatment effect by taking the difference between the (post-treatment difference between populations)(solid lines after treatment) and (the pre-treatment difference between populations) (solid lines before treatment)\n\nDiDâ€™s control (dotted line) is an extrapolation of the treatment case that must be parallel to the mean post-treatment outcome (green line post-treatment) of the non-treated case\nThe effect is the difference between DiDâ€™s control (blue dotted line) and the post-treatment outcome (blue line post-treatment) of the treated case.\n\nIn effect, this is the same as extrapolating the counterfactual for the treated population in the post-treatment period if it had not received treatment (the dashed line in the image above)\nTechnically, this is implemented as a fixed-effects regression model\n\nKey Assumptions\n\nThe decision to treat the treatment group was not influenced by the outcome (no anticipation to treat)\n\ne.g.Â poverty rate spikes and community expects a policy to be enacted soon, so it acts (spends money, etc.) in anticipation of that help coming\n\nIf not for the treatment, the two groups being compared would have parallel trends in the outcome. Note that groups are allowed to have different levels but must have similar trends over time\n\nPretesting parallel trends assumption:\n\n{did} vignette\n{HonestDiD} vignette\nThese arenâ€™t particularly liked. Tests are considered low-powered\n\nOptions if this assumption is violated\n\nUse pre-treatment variables to filter data to create similar groups (Treatment/Control) so they are more likely to have similar trends (pre-cursor to Synthetic Controls method)\n\nEestimate the propensity score based on observed covariates; compute the fitted value\nRun a weighted DiD model\n\nExtrapolate the difference in pre-treatment trends to post-treatment (paper) (Also see {HonestDiD})\nUse a â€œdifferential trendsâ€ method (explainer, says code available on request)\n\nincludes each post-intervention time period as a dummy variable in your model, and average these to obtain an average treatment effect\n\nCombination of DiD and IV (paper)\n\n\nThere is no spill-over effect such that treating the treatment group has an effect on the control group\n\nApplication\n\nWe can estimate the effect of a store remodel on visits by comparing store traffic before and after the remodel with traffic at a store that did not remodel.\nNote how sensitive this method is to our assumptions:\n\nIf the remodel is an expansion and caused by a foreseen increase in traffic, our first assumption is violated and our effect will be overestimated\nIf the control we chose is another nearby store in the same town, we could experience spillover effects where more people who would have otherwise gone to the control store decide to go to the treatment store instead. This again would overestimate the effect\nAnother counter-example that violates the assumption would be measuring the effect of placing a certain product brand near a storeâ€™s check-out on sales and using sales of a different brand of the same product as the control. Why? Since these products are substitutes, the product placement of the treatment group could â€œspilloverâ€ to negatively effect sales of the control\n\n\nRelated Methods\n\nVariants exist that relax different assumptions. For example, we may consider cases in which different units receive the treatment at different times, different units have different (heterogenous) treatment effects, the parallel trend assumption only holds after conditioning on covariates, and many more scenarios\nSynthetic control methods can be thought of as an extension of difference-in-differences where the control is a weighted average of a number of different possible controls\nBayesian structural time-series methods relax the â€œparallel trendsâ€ asumptions of difference-in-differences by modeling the relationship between time series (including trend and seasonal components)\n\nAbadie, Alberto (2005). â€œSemiparametric Difference-in-Differences Estimators,â€ Review of Economic Studies (2005) 72, 1â€“19\n\nAssumption: non-parallel outcome dynamics between treated and controls caused by observed characteristics\nTwo-step strategy:\n\nEstimate the propensity score based on observed covariates; compute the fitted value\nRun a weighted DiD model\n\nThe idea of using pre-treatment variables to adjust trends is a precursor to synthetic control\n\nStrezhnev (2018) extends this approach to incorporate pre-treatment outcomes\nOther considerations (article)\n\nLevels are important.\n\nAlways look at differences in levels between treatment and control, and not just trends. If there are large differences, than think about why they are so different.\nCould these differences affect future trends in our outcome of differences?\n\nFunctional forms matter.\n\nWhen comparing our treatment and control trends, do we think that they evolve similarly in terms of absolute or relative terms? Do we want to use levels or logs?\n\nPre-treatment parallel tests are problematic.\n\nOnly because we reject an unequal parallel trend does not mean that we confirmed its validity, and often, these rejection tests are underpowered.\n\n\nStepped Design (Athey)\n\nAssumptions\n\nAdoption date is conditional on the potential outcomes and possibly pretreatment variables. Guaranteed by design\n\nYou can relax (troublesome) random assignment assumption by requiring only that the adoption date is completely random within subpopulations with the same values for the pre-treatment variables (e.g.Â units are clusters of individuals like states)\n\nPotential outcomes which rules out the presence of certain treatment effects\n\nNo anticipation - outcome at present is not affected by anticipation of a future treatment date.\nInvariance to history - duration of treatment prior to a given period doesnâ€™t affect the outcome variable value for that period\n\nMore plausible when units are clusters of individuals (e.g.Â states)\n\n\n\nâ€œAuxillaryâ€ Assumptions (i.e.Â Sometimes needed for particular analyses)\n\nConstant treatment effect across time\nConstant treatment effect across units"
  },
  {
    "objectID": "qmd/econometrics-general.html#sec-econ-gen-scm",
    "href": "qmd/econometrics-general.html#sec-econ-gen-scm",
    "title": "General",
    "section": "Synthetic Control Method (SCM)",
    "text": "Synthetic Control Method (SCM)\n\nCreates a synthetic control based on the pre-treatment features of the treatment unit and non-treated units. A control thatâ€™s based on comparison units (i.e.Â non-treatment units) often provides a better control than a control solely based on the treated unit (like in DiD). After treatment, you take the difference between this synthetic control and your treatment unit to estimate the effect of the treatment. Similar to DiD, except on how the control is formulated.\nMisc\n\nNotes from: Using Synthetic Controls: Feasibility, Data Requirements, and Methodological Aspects\nExtensions\n\nGeneralized synthetic control by Xu (2017)\nSynthetic difference-in-differences by Doudchenko and Imbens (2017)\nPenalized synthetic control of Abadie e Lâ€™Hour (2020)\nMatrix completion methods of Athey et al.Â (2021)\n\n\nTerms\n\nDonor Pool or Donors: The group of units that are untreated which are used to calculate the synthetic control\nUnit, Cell, Case: Interchangeable names for the population level youâ€™re testing in the experiement (e.g.Â herd, school, store, city, state, precinct)\n\nRecommended Use Cases\n\nWhen events take place at the aggregated level, e.g.Â county, state, province.\nYou only have one treated unit and a few control units.\n\nAdvantages\n\nBetter apples to apples comparison that DiD since the control should be a better estimate.\nThe weights (which sum to 1) from the calculation of the synthetic control add to the interpretability of the method by giving us information about the â€œimportanceâ€ of each non-treated unit in the formulation of the synthetic control\n\nThe donor weights are sparse due to the optimization process. Only a few donors contribute to the synthetic control.\n\nSCM provides transparency about how different the non-treatment units are from the treatment unit. This difference can be calculated.\nThe choice of a synthetic control does not rely on the post-intervention outcomes, which makes it impossible to cherrypick the study design that may affect the conclusions.\n\nChoosing Units for the Donor Pool\n\nThe risk of over-fitting may also increase with the size of the donor pool, especially when T0 (pre-treatment period) is small\nEach of the units in the donor pool have to be chosen judiciously to provide a reasonable control for the treated unit. Including in the donor pool units that are regarded by the analyst to be unsuitable controls (because of large discrepancies in the values of their observed attributes Zj or because of suspected large differences in the values of the unobserved attributes Î¼j relative to the treated unit) is a recipe for bias.\nDonor units with similar values of the observed predictors as the treated unit should be chosen. If itâ€™s believed that a unit has a large unobserved difference with the treated unit, it shouldnâ€™t be included.\nAs a rule of thumb, Abadie, Diamond, and Hainmueller (2010) suggest excluding units for which the prediction MSE is larger than twice the MSE of the treated unit.\n\nPredictors\n\nPredictors are often time series reported by government agencies, multilateral organizations, and private entities (e.g.Â GDP, crime statistics, cigarette usage, census survey micro-data\nThe predictors of the outcome variable, which are used to calculate the synthetic control, are not affected by the treatment\n\nData\n\nThe larger the pre-treatment period the smaller the bias of the synthetic control estimator (assuming the synthetic control closely tracks the outcome variable during the pre-treatment period.\n\nA trade-off of obtaining more pre-treatment data may be that the predictors are better short term than long term.\n\nIf this is the case, adding weights that favor more recent predictor data can help\n\nIf the amount of pretreatment data is relatively small, then you need very good predictors of the post-treatment outcome such that residual variance will be small which will reduce the chance of overfitting.\n\n\nRobustness Checks\n\nin-time placebo test (backdating): move the treatment date backwards in the data. If the synthetic control still closely tracks the outcome variable until the actual treatment date, then this is evidence of a reliable synthetic control\n\n\nActual date of the treatment (i.e.Â German reunification) is 1990. Here the re-calculated synthetic control (dashed line) using 1980 as the treatment still tracks GDP until the actual treatment date then they split. Therefore this is evidence of a credible synthetic control.\n\nRobustness with alternate design\n\nMethods\n\nRemove a donor from the donor pool and refit the model and see if the results hold. Repeat with each donor.\n\n\nAll synthetic checks closely track pretreatment GDP and are centered around the synthetic control that used all the donors. Effect for all the synthetic checks are still negative. Evidence of robustness.\nIf the exclusion of a unit from the donor pool has a large effect on results without a discernible change in pre-intervention fit, this may warrant investigating if the change in the magnitude of the estimate is caused by the effects of other interventions or by particularly large idiosyncratic shocks on the outcome of the excluded untreated unit (see Potential Issues and Solutions below)\n\nThe choice of predictors of the outcome variable (no example given)\n\n\nPre-Post Error Ratio\n\\[\n\\lambda = \\frac{\\text{MSE}_{\\text{post}}}{\\text{MSE}_{\\text{pre}}} = \\frac{\\frac{1}{n}\\sum_{t\\in \\text{post}}(Y_t - \\hat Y_t)^2}{\\frac{1}{n}\\sum_{t\\in \\text{pre}}(Y_t - \\hat Y_t)^2}\n\\]\n\nAbadie, Diamond, and Hainmueller (2010) suggest to perform a randomization test is the ratio between pre-treatment MSE and post-treatment MSE.\nP-Value (article)\nlambdas = {}\nfor city in cities:\nÂ  Â  mse_pre = synth_predict(df, SyntheticControl(), city, treatment_year).mse\nÂ  Â  mse_tot = np.mean((df[f'Synthetic [{city}]{style='color: #990000'}'] - df[city])**2)\nÂ  Â  lambdas[city] = (mse_tot - mse_pre) / mse_pre\n\nprint(f\"p-value: {np.mean(np.fromiter(lambdas.values(), dtype='float') &gt; lambdas[treated_city]):.4}\")\n\n\nPotential Issues and Solutions\n\nVolatility of the outcome variable is low. Small or even large effects are difficult to detect if the outcome experiences a lot of shocks that are larger or comparable to the size of the effect.\n\nIn units where substantial volatility is present in the outcome of interest it is advisable to remove it via filtering, in both the treatment unit as well as in the non-treatment units, before applying synthetic control techniques\n\nIdiosyncratic shocks in donor units\n\nimportant to eliminate from the donor pool any units that may have suffered large idiosyncratic shocks to the outcome variable during the treatment period, if it is judged that such shocks would not have affected the outcome of the treatment unit in the absence of the intervention.\n\nI guess the shocks indicate a substantial difference between the treatment unit and the donor\n\n\nAnticipation: if any agents jumped the gun in anticipation of a policy/treatment and engaged in behavior that affects a predictor or outcome variable, the SCM results may be biased.\n\nIf this happens, the treatment date in the dataset should be moved back to just before the agent began itâ€™s behavior or the change in the variable occurred in reaction to agentâ€™s behavior.\n\nSpillover: Donor units experience effects of the treatment even though they werenâ€™t treated. Common if donor units are in close geographical proximity to the treatment unit.\n\nDonor units affected by spillover should be removed from the dataset.\nIf you do include the donor, make note of the direction of the bias. Then, if the bias has a â€œnegativeâ€ effect on the treatment effect, you can say the synthetic control estimate provides a lower bound on the magnitude of the causal effect of the treatment\n\nExtreme values in the treatment unit\n\nIf the extreme values are in a predictor variable, but the synthetic control tracks the observed outcome in the pretreatment period, then all is well.\nIf the synthetic control doesnâ€™t track, then the outcome variable should be transformed to differences or growth rates\n\nLong time horizons: Some treatments effects take a long time to emerge.\n\nIn these cases, you either have to just continue to wait, use surrogate outcomes, or use leading indicators\n\nI think â€œsurrogate outcomesâ€ means indirect or proxy measures of the outcome of interest\nAnd leading indicators isnâ€™t referring to normal usage as a predictor but to use as the outcome."
  },
  {
    "objectID": "qmd/econometrics-general.html#sec-econ-gen-its",
    "href": "qmd/econometrics-general.html#sec-econ-gen-its",
    "title": "General",
    "section": "Interrupted Time Series (ITS)",
    "text": "Interrupted Time Series (ITS)\n\n\nAnalysis of a single time-series data before and after the intervention\n\nExamine whether the outcome variable returns to the baseline after taking away the treatment condition\n\nDoing this multiple times increases data and adds power to the analysis (see Netflix articles in bkmks)\n\nUses Segmented Regression to examine the effects of the intervention\n\nEach segment has its own slope and intercept, and we compare the two segmented regression models to derive the effects\n\n\nMisc\n\nNotes from: A Practitionerâ€™s Guide To Interrupted Time Series\n\nStrengths\n\nTo control for long-term time trends in the data. ITS presents a long-term analytical framework with more extended periods, which better explain any data trends.\nTo account for individual-level bias and to evaluate the outcome variable at the population level. Individual-level data may introduce bias, but not with population data. Honestly, this is both a blessing and a curse. We will elaborate more on the latter aspect in the following part.\nTo evaluate both intended and unintended consequences of interventions. We can easily enlarge analysis and incorporate more outcome variables with minimum or no adaptations.\nTo conduct stratified analyses of subpopulations of individuals and to derive different causal effects. This is critical. We can divide the total population into different sub-groups according to various criteria and examine how each sub-group may behave differently. Social groups are different, and grouping them together may dilute or hide critical information, as positive and negative effects mix together and cancel out (see Harper and Bruckner for examples).\nTo provide clear and interpretable visual results. Visual inspections are always welcome and should be treated seriously (See my other post for more explanations).\n\nLimitations\n\nMultiple rounds of data entries. A minimum of 8 periods before and 8 after an intervention to evaluate the changes. So, we need a total of 16 data entries, which may not be possible all the time. I think Penfold and Zhang (2013) are being cautious about the number of data entries. Itâ€™s still possible to apply ITS with few rounds of data entry. Just the causal power may not as robust as the one with multiple rounds.\nTime lag. It takes some unknown time for a program to achieve intended results, which makes it difficult to pinpoint the causal effects of several events that coincide. Letâ€™s say the transportation department in the U.S. adopt three policies within a two-year timespan to curb highway speeding. Playing God, we somehow know it would take 1 yr for Policy A to have any effect, 1.5 ys for Policy B, and 3 yrs for Policy C. In the meantime, it becomes impossible to separate the intertwined effects using ITS.\nInference Level. Itâ€™s population-level data, so we canâ€™t make inferences about each individual.\n\nPower and Sample Size Considerations\n\nNumber of time points in each before- and after- segment\n\nRecommendations range from 3 time points per segment to 50 time points per segment\n\nAverage sample size per time point\nFrequency of time points (e.g.Â weekly, monthly, yearly, etc.)\nLocation of intervention (e.g.Â midway, 1/3, 2/3, etc.)\n\nAs long as there are sufficient time points per segment and each time point is supported by a large enough sample size, there is not much difference in the study power of an early or late intervention\n\nExpected effect size\n\nSlope change: a gradual change in gradient (or slope) of trend\nLevel change: an instant change in level (i.e.Â mean)"
  },
  {
    "objectID": "qmd/econometrics-general.html#sec-econ-gen-rdd",
    "href": "qmd/econometrics-general.html#sec-econ-gen-rdd",
    "title": "General",
    "section": "Regression Discontinuity Design (RDD)",
    "text": "Regression Discontinuity Design (RDD)\n\n\nRDDs generate asymptotically unbiased estimates of the effect of an intervention if:\n\nThe relationship between the outcome and running variable is modeled appropriately\n\nDonâ€™t use a particular curve to justify the discontinuity. Gelman prefers reasonable nonlinear curves but donâ€™t go crazy with it so that it maximizes the effect.\n\nThe forcing variable was not manipulated (either behaviorally or mechanically) to influence assignment to the intervention group.\n\ne.g.Â if the running variable is a test score and the threshold is a particular test score, is there evidence of some sort of cheating to where assignment of students around the threshold isnâ€™t random? (see bullet under step 1 below)\n\n\nMisc\n\nGelman: The big mistakes seem to come from:\n\nUnregularized regression on the forcing variable which randomly give you wild jumpy curves that pollute the estimate of the discontinuity\nNot adjusting for other important pre-treatment predictors\nTaking statistically significant estimates and treating them as meaningful, without looking at the model thatâ€™s been fit.\n\nUsecases\n\nLee study of the incumbency effect\n\nWe want to know if a party holding a House seat gives that party an advantage in the next election. But candidates who win (the incumbent) tend to better than challengers from the same party. To overcome this, Lee used an RDD with the Democratic share of the two-party vote in the last election as the forcing variable for Democratic incumbency in the current election. Thee key idea is that, in close elections, seats where a Democratic candidate won will have similar characteristics to districts where a Democratic candidate lost.\n\n\nLakeland recommends using Bayesian estimation and Chebyshev Polynomials\n\nTypes\n\nSharp RDD:\n\nThe threshold separates the treatment and control group exactly\n\nFuzzy RDD:\n\nThe threshold influences the probability of being treated\nThis is in fact an instrumental variable approach (estimating a LATE)\n\n\nTerms\n\nForcing or Assignment or Running Variable:\n\nUsed to assign units to the intervention group and comparison group on either side of a fixed threshold (Cutoff Score).\nMay or may not be related to the potential outcomes but we assume that relationship is smooth, so that changes in the outcome around the threshold can be interpreted as a causal effect.\n\nBandwidth - The number of points selected on each side of the cutoff\n\nShould be wide enough to include a sufficient number of observations and obtain precise estimates. It should also be narrow enough to compare similar units and reduce selection bias.\nCurrent best practice for defining the â€œneighborhoodâ€ of the threshold is to use weights based on a triangular kernel and an â€œoptimalâ€ bandwidth proposed by Imbens and Kalyanaraman (2012). The optimal bandwidth is derived for the simple RDD model with no covariates, though the authors comment that inclusion of additional covariates should not greatly affect the result unless the covariates are strongly correlated with the outcome, conditional on the running variable.\n\n\nSteps\n\nFind and include adjustment variables for differences between the treatment and control groups. Avoid only adjusting for one pre-treatment variable.\n\nThose individuals on both sides of the cut-off point, should be very similar (i.e.Â itâ€™s more or less random that theyâ€™re on one side of the cutoff and not the other). Therefore have something close to a random allocation into treatment and control group\n\ni.e if the cutoff is a test score of 71, then, characteristically, students scoring a 70 should be very similar to students scoring a 72.\n\nIncluding covariates shouldnâ€™t affect the LATE very much but should help lower the std errors some.\n\nIf there is a large effect then the function is probably creating interaction terms with treatment and the covariates. (see bkmk)\n\n\nFit a regression line (or curve) for the intervention group and similarly for the comparison group,\nThe difference in these regression lines at the threshold value of the forcing variable is the estimate of the effect of the intervention (i.e.Â Local Average Treatment Effect (LATE)).\n\nExample: Corruption\n\nFrom\n\nArticle: Quasi-Experimental Design: Regression Discontinuity Design\nPaper: Businesspeople in Elected Office: Identifying Private Benefits from Firm-Level Returns\n\nDo businesspeople who win elected office in Russia use their positions to help their former firms?\nVariables:\n\nOutcome:\n\nLog total revenue of the candidateâ€™s former firm\nProfit margin: net profit/total revenue during last year of term if member won election or if the member lost, the last year of the hypothetical term if they had won.\n\nTreatment: electoral victory or not (1/0)\nRunning: Vote margin (difference between former firm member/current candidate and their opponent\n\nnegative if former firm member lost, positive if they won\nCutoff = 0\n\n\nAssumptions\n\nCheck for manipulation of the running variable\n\nExamine the balance along a range of covariates between winning and losing candidates in close elections (i.e.Â around the threshold).\n\nIf no significant imbalance is detected, then thereâ€™s no evidence that electoral manipulation favors a specific type of candidate or firm\nIs there any coordination among firms and their candidates?\n\nIf so, weâ€™d expect to see a sharing of the spoils after the election. Therefore, some conspicuous number of firmâ€™s revenue or profit margin should increase even though they lost.\n\n\nThe splits looks pretty even around the cutoff between treated (winners) and control (losers)\n\n\n\nDensity Test: assess the validity of the assumption of continuity around the threshold.\nlibrary(rddensity)\nsummary(rddensity(X = cons$margin, vce=\"jackknife\"))\n\n#&gt; Manipulation testing using local polynomial density estimation.\n#&gt; Number of obs =Â  Â  Â   2806\n#&gt; Model =Â  Â  Â  Â  Â  Â  Â   unrestricted\n#&gt; Kernel =Â  Â  Â  Â  Â  Â  Â  triangular\n#&gt; BW method =Â  Â  Â  Â  Â   estimated\n#&gt; VCE method =Â  Â  Â  Â  Â  jackknife\n#&gt; c = 0Â  Â  Â  Â  Â  Â  Â  Â   Left of cÂ  Â  Â  Â  Â  Right of cÂ  Â  Â  Â  Â \n#&gt; Number of obsÂ  Â  Â  Â   1332Â  Â  Â  Â  Â  Â  Â  Â 1474Â  Â  Â  Â  Â  Â  Â  Â \n#&gt; Eff. Number of obsÂ  Â  448Â  Â  Â  Â  Â  Â  Â  Â  409Â  Â  Â  Â  Â  Â  Â  Â  Â \n#&gt; Order est. (p)Â  Â  Â  Â  2Â  Â  Â  Â  Â  Â  Â  Â  Â  2Â  Â  Â  Â  Â  Â  Â  Â  Â  Â \n#&gt; Order bias (q)Â  Â  Â  Â  3Â  Â  Â  Â  Â  Â  Â  Â  Â  3Â  Â  Â  Â  Â  Â  Â  Â  Â  Â \n#&gt; BW est. (h)Â  Â  Â  Â  Â   0.157Â  Â  Â  Â  Â  Â  Â  0.172Â  Â  Â  Â  Â  Â  Â  Â \n#&gt; MethodÂ  Â  Â  Â  Â  Â  Â  Â  TÂ  Â  Â  Â  Â  Â  Â  Â  Â  P &gt; |T|Â  Â  Â  Â  Â  Â  Â \n#&gt; RobustÂ  Â  Â  Â  Â  Â  Â  Â  -1.7975Â  Â  Â  Â  Â  Â  0.0723\n\nHave to check out docs + referenced papers to get a detailed idea of whats happening, but the p-value is whatâ€™s important\nX is the running variable\npval &gt; 0.05 says not enough evidence to reject null where H0: thereâ€™s continuity around the cutoff (i.e.Â no manipulation)\n\n\n\nFit the RDD\nlibrary(rdrobust)\nfit &lt;- rdrobust(cons$fullturnover.e.l.d, cons$margin, c = 0, all=TRUE)\nsummary(fit)\n\nBW est. (h)Â  Â  Â  Â  Â  Â  Â  Â  Â  0.138Â  Â  Â  0.138\nBW bias (b)Â  Â  Â  Â  Â  Â  Â  Â  Â  0.260Â  Â  Â  0.260\n=============================================================================\nÂ  Â  Â  Â  MethodÂ  Â  Coef. Std. Err.Â  Â  Â  Â  zÂ  Â  P&gt;|z|Â  Â  Â  [ 95% C.I. ]Â  Â  Â  Â \n=============================================================================\nÂ  ConventionalÂ  Â  0.548Â  Â  0.197Â  Â  2.777Â  Â  0.005Â  Â  [0.161 , 0.934]Â  Â  Â \nBias-CorrectedÂ  Â  0.619Â  Â  0.197Â  Â  3.136Â  Â  0.002Â  Â  [0.232 , 1.005]Â  Â  Â \nÂ  Â  Â  Â  RobustÂ  Â  0.619Â  Â  0.225Â  Â  2.746Â  Â  0.006Â  Â  [0.177 , 1.060]Â  Â  Â \n=============================================================================\n\nOutcome: cons$fullturnover.e.l.d\nRunning: cons$margin\nc is the cutoff (default = 0)\nall = TRUE says to report three different methods for std.errors\n\nConventional RD estimates with conventional standard errors.\nBias-corrected estimates with conventional standard errors.\nBias-corrected estimates with robust standard errors.\n\nBW est (for Conventional estimate), BW bias (for bias-corrected estimate) are the bandwidths used\n\nrdbwselect can be used to calculate diffferent bandwidths and then specified in rdrobust with h and b args.\n\nIf 2 numbers are provided for an arg, then it specifies different bandwidths for before and after the cutoff\nThere are a quite a few different methods available (see manual for details)\ndefault â€œmserdâ€: one common MSE-optimal bandwidth selector for the RD treatment effect estimator\n\n\np and q args specify the order of polynomial to be used to fit the Conventional model and Bias-corrected model respectively (default = 2 , quadratic)\nInterpretation\n\nThe LATE is 0.548 with a pval = 0.005.\nThere is enough evidence to reject the claim that when a businessperson from a company barely wins an election to a state legislature, there is no effect to the firmâ€™s revenue.\nThe revenue of the firm in the next year will be 0.548 larger than if the businessperson didnâ€™t win the election\n\n\nPotential covariates in this dataset: dummy for foreign ownership, a dummy for state ownership, and logged total fixed assets in the year prior to taking office (baseline feature), categorical financial sector of the firm.\nSensitivity Checks\n\nAdjust the bandwidth and polynomial orders\nif your effect is no longer significant or looks substantially different, then your result is too sensitive and not very credible.\n\nRobustness Checks\n\nTest other values of the cutoff variable.\nThere shouldnâ€™t be a significant effect or one that is similar in strength to the effect when the original cutoff was used.\n\n\nExample: Sometimes a rdd isnâ€™t the answer\n\nMight be worth following this precedure and use the results as a check on the rdd or as a alternative after an rdd doesnâ€™t show convincing results\nFrom Gelman critique, â€œAir Filters, Pollution, and Student Achievementâ€:\nDescription:\n\nAliso Canyon gas leak leads many schools to install air filters. RDD study shows test scores went up after the filters were installed. What follows is how Gelman would have conducted the study.\n\n\nSteps\n\nCompare outcomes in schools in the area with and without air filters\n\nFit a regression\n\ndata has one row per school\noutcome being average post-test score per school\npredictors: average pre-test score per school\nindicator: air filters installed\n\n\nMake a scatterplot of post-test vs.Â pre-test with one point per school, displaying treated schools as open circles and control schools as dots.\nMake a separate estimate and graph for each grade level if youâ€™d like, but Iâ€™m guessing that averages will give you all the information you need.\nMake plots of pre-test scores, post-test scores, and regression residuals on a map, using color intensities. I donâ€™t know that this will reveal much either, but who knows. Iâ€™d also include the schools in the neighborhood that were not part of the agreement\n(Optional) fit a multilevel model using data from individual students (random effect)â€”why not, itâ€™s easy enough to doâ€”but I donâ€™t think it will really get you much of anything beyond the analysis of school-level averages."
  },
  {
    "objectID": "qmd/econometrics-general.html#sec-econ-gen-psm",
    "href": "qmd/econometrics-general.html#sec-econ-gen-psm",
    "title": "General",
    "section": "Propensity Score Matching",
    "text": "Propensity Score Matching\n\nA Propensity Score is the probability of being assigned to a certain treatment, conditional on pre-treatment (or baseline) characteristics\n\n\nMisc\n\nAlso see Survey, Analysis &gt;&gt; Weights &gt;&gt; Types &gt;&gt; Inverse Probability Weights\nPackages\n\n{MatchIt}: propensity score methods\n\nAlso non-parametric: nearest neighbor matching, optimal pair matching, optimal full matching, genetic matching, exact matching, coarsened exact matching, cardinality matching, and subclassification\n\n{MatchThem}\n\nProvides essential tools for the pre-processing techniques of matching and weighting multiply imputed datasets.\nVignette\n\nOthers\n\n{twang}, {Matching}, {optmatch}, {CBPS}, {ebal}, {WeightIt}, {designmatch}, {sbw}, and {cem}\n\nViz\n\n{cobalt}: balance tables and plots using output from above packages\n\n\nNotes from\n\nTwitter thread\n\nblog post\n\nslack::kris used â€œcoarsened exact matchingâ€ in his project. No idea what this is. Need to check it out.\nPaper: Choosing the Estimand When Matching or Weighting in Observational Studies\n\nHow to choose an estimand based on your question (and how that maps to particular weighting / matching choices)\n\n\n\n\n\n\nBBR Chapter 17.2 to 17.8\n\nBiostatistics for Biomedical Research (Harrell) Ch.17.2 - 17.8: Modeling for Observational Treatment Comparisons\nAdjusting for Confounders\n\nUse of the Propensity Score (PS) allows one to aggressively adjust for measured potential confounders\nDoing an adjusted analysis where the adjustment variable is the PS simultaneously adjusts for all the variables in the score insofar as confounding is concerned (but not with regard to outcome heterogeneity)\nStratifying for PS does not remove all the measured confounding\n\nBut adjusting only for PS is inadequate.\n\nTo get proper conditioning so that the treatment effect can generalize to a population with a different covariate mix, one must condition on important prognostic factors\nNon-collapsibility of hazard and odds ratios is not addressed by PS adjustment\nAdjusting only for PS can hide interactions with treatment\n\nPS is not necessary if the effective sample size (e.g.Â number of outcome events) &gt; 5p where p is the number of measured covariates\nWhen judging covariate balance (as after PS matching) it is not sufficient to examine the mean covariate value in the treatment groups\nTreatment Effects\n\nEliminate units in intervals of PS where there is no overlap between treatment A and treatment B, or include an interaction between treatment and a baseline characteristic\n\nExample: Including an interaction between age and treatment and there were no units greater than 70 years old receiving treatment B\n\nThen, the B:A difference for age greater than 70 would have an extremely wide confidence interval as it depends on extrapolation. So the estimates that are based on extrapolation are not misleading; they are just not informative.\n\n\n\nTypes\n\nPairs Matching\n\nThrows away data â€“&gt; low power\n\nUnits get discarded that have characteristics which are the same as another unit and has already been matched (i.e.Â units that have the same information)\n\n\nInverse Probability Weighting\n\na high variance/low power approach like matching\nAlso see Survey, Analysis &gt;&gt; Weights &gt;&gt; Types &gt;&gt; Inverse Probability Weights\n\n\nModeling\n\\[\n\\begin{align}\nY =\\:\\: &\\operatorname{treat} + \\log \\frac{\\text{PS}}{1-\\text{PS}} \\\\\n&+ \\text{nonlinear functions of}\\: \\log \\frac{\\text{PS}}{1-\\text{PS}} \\\\\n&+ \\text{important prognostic variables}\n\\end{align}\n\\]\n\nIn biostatistics, a prognostic factor or variable is a patient characteristic that can predict that patientâ€™s eventual response to an intervention\nPrognostic variables need to be in model even though they are also in the PS, to account for subject outcome heterogeneity (susceptibility bias)\nIf outcome is binary and you can afford to ignore prognostic variables, use nonparametric regression, Y ~ PS, and fit a model to each treatment groupâ€™s data\n\nNonparametric Regression - does not assume linearity; only assumes smoothness, Y ~ X where X is continuous\n\ne.g.Â moving avg, loess, other smoothers, etc.\nSee BBR Ch 8.7 for details, examples (no binary outcome examples)\nChecking functional form in logistic regression using loess plots\n\nShows a binary outcome used in a loess model\n\n\nPlotting these two curves with PS on x-axis and looking at vertical distances between curves is an excellent way to adjust for PS continuously without assuming a model\n\nGuessing the average distance between the curves is the treatment effect (?)\n\n\n\n\n\n\nPsuedo-Distributions After Weighting According to the Type of Estimand\n\n{cobalt} (not on CRAN) can be used to produce the balance plots below using output from various propensity scoring packages (see above)\nShows how weights derived from propensity scores makes treatment and control groups comparable\nLight green and light blue show psuedo-counts that are added to the groups after applying weights\nNo Estimand\n\nMirrored histogram of propensity scores for treatment (top) and control (bottom) groups\nNo groups are upweighted (or equivalently, for both groups, weights = 1)\n\n\nx-axis is the propensity score\ny-axis is the count of people with that score\nMore mass on the *right* in the treatment group (top) means that more people in that group had a higher probability of receiving treatment (duh)\nMore mass in the treatment group than the control group means more people received the treatment than control\n\n\nAverage Treatment Effect (ATE)\n\nTarget: whole population\nTreated and Control groups are upweighted\n\n\nLight green and light blue show psuedo-counts that are added to the groups after applying weights\nBoth groups now similar (i.e.Â comparable)\nindividual \\(\\text{unit}_i\\) weights\n\n\\(\\text{treatment\\_weight}_i = \\frac{1}{\\text{propensity\\_score}_i}\\)\n\\(\\text{control\\_weight}_i = \\frac{1}{1 - \\text{propensity\\_score}_i}\\)\n\n\nPotential Issues\n\nWeights are unbounded\n\nReally small propensity scores for the Treatment group (or really large ones for control) could have an oversized effect on the analysis.\nCan lead to finite sample bias // variance issues\n\n\n\nAverage Treatment Effect on the Treated (ATT)\n\nTarget: treatment group\nControl group is upweighted\n\n\nIndividual \\(\\text{unit}_i\\) weights\n\n\\(\\text{treatment\\_weight}_i = 1\\)\n\\({\\text{control\\_weight}_i} = \\frac{\\text{propensity\\_score}_i}{1 - \\text{propensity\\_score}_i}\\)\n\n\nPotential Issues\n\nExtremely unbalanced groups\n\nIn this example, there are much more treated units than control units \\(\\rightarrow\\) control group must be substantially upweighted to become comparable\nCan lead to instability\n\n\n\nAverage Treatment Among Overlap Population (ATO)\n\nTarget: Clinical equipoise\n\nThe assumption that there is not one â€˜betterâ€™ intervention present (for either the control or experimental group) during the design of a randomized controlled trial (RCT). A true state of equipoise exists when one has no good basis for a choice between two or more care options.\nSee Notes from &gt;&gt; Paper for more details\n\nTreated is downweighted\n\n\nIndividual \\(\\text{unit}_i\\) weights\n\n\\(\\text{treatment\\_weight}_i = 1 - \\text{propensity\\_score}_i\\)\n\\(\\text{control\\_weight}_i = \\text{propensity\\_score}_i\\)\n\n\nWeights are bounded by 0 and 1, so they have nice variance properties"
  },
  {
    "objectID": "qmd/econometrics-mixed-effects-frequentist.html#sec-econ-me-freq-misc",
    "href": "qmd/econometrics-mixed-effects-frequentist.html#sec-econ-me-freq-misc",
    "title": "Mixed Effects",
    "section": "Misc",
    "text": "Misc\n\nRandom Effects model = Multi-level model with random intercepts = Hierarchical model\nBayesian\n\nResources\n\nBayesian Generalized Linear Mixed Effects Models for Deception Detection Analyses\n\nPaper thatâ€™s an in-depth tutorial. Uses {brms}, {emmeans}, {parameters}\n\n\n\nAdvantages of a mixed model (y ~ x + (x | g)) vs a linear model with an interaction (y ~ x * g)\n\nFrom T.J. Mahr tweet\nConceptual: Assumes participant means are drawn from the same latent population\nComputational: partial pooling / smoothing\nBoth will have very similar parameter estimates when the data is balanced with few to no outliers\n\nlinear least squares regression can overstate precision, producing t-statistics for each fixed effect that tend to be larger than they should be; the number of significant results in LLSR are then too great and not reflective of the true structure of the data\nStandard errors are underestimated in the interaction model though.\n\nDoesnâ€™t account for dependence in the repeated measure for each subject\n\nFor unbalanced data w/some group categories having few data points or with outliers, the mixed effects model regularizes/shrinks/smooths the estimates to the overall group (i.e.Â population) mean\n\nExample of model formula interpretation\n\n\nPackages\n\n{lme4} - linear and generalized linear mixed-effects models; implemented using the â€˜Eigenâ€™ C++ library for numerical linear algebra and â€˜RcppEigenâ€™ â€œglueâ€.\n\nUsing {lmerTest} will produce a summary of lme4 models with pvals for coefficients\n\n{multilevelmod} - tidymodels wrapper for many mixed model packages.\n{plm} - linear models for panel data; including within/fixed effects, random effects, between, first-difference, nested random effects\n{glmmTMB} - for fitting generalized linear mixed models (GLMMs) and extensions\n\nWide range of statistical distributions (Gaussian, Poisson, binomial, negative binomial, Beta â€¦) and zero-inflation.\nFixed and random effects models can be specified for the conditional and zero-inflated components of the model, as well as fixed effects for the dispersion parameter.\n\n{spaMM} - Inference based on models with or without spatially-correlated random effects, multivariate responses, or non-Gaussian random effects (e.g., Beta).\n\nMixed Effects and repeated measures (aka longitudinal)\n\nMixed Effects Models = Fixed Effects and Random Effects\n\ni.e.Â variation within the unit and between the units\n\nHarrell (article)\n\nSays that Mixed Models models can capture within-subject correlation of repeated measures over very short time intervals but not over extended time intervals where autocorrelation comes into play\n\nExample of a short interval is a series of tests on a subject over minutes when the subject does not fatigue\nExample of a long interval is a typical longitudinal clinical trial where patient responses are assessed weekly or monthly\n\nHe recommends a Markov model for longitudinal RCT data (see bkmks)\n\nBartlett\n\nMixed model repeated measures (MMRM) in Stata, SAS and R\n\nTutorial; uses {nlme::gls}\n\n\n\n{tidymodels} workflows (optional outputs: lmer, glmer, stan_glmer objects)"
  },
  {
    "objectID": "qmd/econometrics-mixed-effects-frequentist.html#sec-econ-me-freq-consid",
    "href": "qmd/econometrics-mixed-effects-frequentist.html#sec-econ-me-freq-consid",
    "title": "Mixed Effects",
    "section": "Considerations",
    "text": "Considerations\n\nMotivation for using a Random Effects model\n\nYou think the little subgroups are part of some bigger group with a common mean effect\n\ne.g.Â multiple observations of a single person, or multiple people in a school, or multiple schools in a district, or multiple varieties of a single kind of fruit, or multiple kinds of vegetable from the same harvest, or multiple harvests of the same kind of vegetable, etc.\n\nThese subgroup means significantly deviate from the big group mean.\nThese deviations follow a distribution, typically Gaussian.\n\nThatâ€™s where the â€œrandomâ€ in random effects comes in: weâ€™re assuming the deviations of subgroups from a parent follow the distribution of a random (Gaussian) variable\n\nThe variation between subgroups is assumed to have a normal distribution with a 0 mean and constant variance (variance estimated by the model).\n\nGEEs are a semi-parametric option for panel data (See Regression, Other &gt;&gt; Generalized Estimating Equations (GEE))\n\n\nFixed Effects or Random Effects?\n\nIf thereâ€™s likely correlation between unobserved group/cases variables (e.g.Â individual talent) and treatment variable (i.e.Â E(Î±|x) != 0) AND thereâ€™s substantial variance between group units, then FE is a better choice (See Econometrics, Fixed Effects &gt;&gt; One-Way Fixed Effects &gt;&gt; Assumptions)\nIf cases units change little, or not at all, across time, a fixed effects model may not work very well or even at all (SEs for a FE model will be large)\n\nThe FE model is for analyzing within-units variance\n\nDo we wish to estimate the effects of variables whose values do not change across time, or do we merely wish to control for them?\n\nFE: these effects arenâ€™t estimated but adjusted for by explicitly including a separate intercept term for each individual (Î±i) in the regression equation\nRE: estimates these effects (might be biased if RE assumptions violated)\n\nThe RE model is for analyzing between-units variance\n\n\nThe amount of within-unit variation relative to between-unit variation has important implications for these two approaches\n\nArticle with simulated data showed that within variation around sd &lt; 0.5 didnâ€™t detect the effect of explanatory variable but ymmv (depends on # of units, observations per unit, N)\n\nDurbinâ€“Wuâ€“Hausman test ({plm::phtest})\n\nIf H0 is not rejected, then both FE and RE are consistent but only RE is efficient. \\(\\rightarrow\\) use RE but if you have a lot of data, then FE is also fine.\nIf H0 is rejected, then only FE is consistent \\(\\rightarrow\\) use FE\n\nICC &gt; 0.1 is generally accepted as the minimal threshold for justifying the use of Mixed Effects Model (See Diagnostics &gt;&gt; ICC)\n\nPooling\n\nComplete pooling - Each unit is assumed to have the same effect\n\nExample: County is the grouping variable and radon level is the outcome\n\nAll counties are alike.\n\ni.e.Â all characteristics of counties that affect radon levels in houses have the statistically same effect across counties. Therefore, the variable has no information.\n\nRun a single regression to estimate the average radon level in the whole state.\n\nlm(radon_level ~ predictors)\nNote that â€œcountyâ€ is NOT a predictor in this model\n\n\n\nNo pooling - All units are assumed to have independent effects\n\nExample: County is the grouping variable (although not a predictor in this case) and radon level is the outcome\n\nAll counties are different from each other.\n\ni.e.Â there are no common characteristics of counties that affect radon levels in houses. Any characteristic a county has that affects radon levels is unique to that county.\n\nRun a regression for each county to estimate the average radon level for each county.\n\nlm(radon_level ~ 0 + county + predictors\nUsing the â€œ0 +â€ formula removes the common intercept which means each county will get itâ€™s own intercept\n\n\n\nPartial pooling - Each unit is assumed to have a different effect, but the data for all of the observed units informs the estimates for each unit\n\nExample: County is the grouping variable (random effect) and radon level is the outcome\n\nAll counties are similar each other.\n\ni.e.Â all charcteristics of counties that affect radon levels in house have statistically varying effects sizes depending on the particular county\n\nRun a multi-level regression to share information across counties.\n\nlmer(radon_level ~ predictors + (1 + predictor | county))\n\n\nThis can be a nice compromise between estimating an effect by completely pooling all groups, which masks group-level variation, and estimating an effect for all groups completely separately, which could give poor estimates for low-sample groups.\nIf you have few data points in a group, the groupâ€™s effect estimate will be based partially on the more abundant data from other groups. (2 min video)\nPartial pooling is typically accomplished through hierarchical models. Hierarchical models directly model the population of units. From a population model perspective, no pooling corresponds to infinite population variance, whereas complete pooling corresponds to zero population variance.\n\n\nVariable Assignment\n\nQuestions (article has examples)\n\nCan the groups we see be considered the full population of possible groups we could see, or are they more like a random sample of a larger set of groups?\n\nFull Population: Fixed\nRandom Sample: Random\n\nDo we want to estimate a coefficient for this grouping, or do we just want to account for the anticipated structure that the groups may impose on our observations?\n\nY: Fixed\nN: Random\n\n\nFixed Effects provide estimates of mean-differences or slopes.\n\nâ€œFixedâ€ because they are effects that are constant for each subject/unit\nLevel One: variables measured at the most frequently occurring observational unit\n\ni.e.Â vary for each repeated measure of a subject and vary between subjects\n\nIn the dataset, these variables that (for the most part) have different values for each row\nTime-dependent if you have longitudinal data\n\nFor a RE model, these are usually the adjustment variables\n\ne.g.Â conditioning on a confounder\n\n\nLevel Two: variables measured at the observational unit level\n\ni.e.Â constant for each repeated measure of a subject but vary between each subject\nFor a RE model, these are usually the treatment variables or variables of interest\n\nThey should contain the information about the between-subject variation\n\nIf a factor variable, it has levels which would not change in replications of the study\n\n\nRandom Effects estimate of variation between and within subgroups\n\nClustering variable\nQuantifies how much of the overall variation can be attributed to that particular variable.\n\nExample: the variation in beetle DAMAGE was attributable to the FARM at which the damage took place, so youâ€™d cluster by FARM (1|FARM)\n\nIf you want slopes to vary according to a variable, the variation of slopes between-units will be a random effect\n\nUsually a level 2 fixed effect variable\n\nIf you want intercepts to vary according to a variable, the variation of intercepts between-units will be a random effect\n\nThis will the unit/subject variable (e.g student id, store id) that has the repeated observations\n\nTypically categorical variables that we are not interested in measuring their effects on the outcome variable, but we do want to adjust for. This variation might capture effects of latent variables.\n\nThis factor variable has levels which can be thought of as a sample from a larger population of factor levels (e.g.Â hockey players)\nExample: 2 hockey players both averaged around 20 minutes per game last year (fixed variable). Predictions of the amount of points scored by just accounting for this fixed variable would produce similar results. But using PLAYER as a random variable will capture the impact of persistent characteristics that might not be observable elsewhere in the explanatory data. PLAYER can be thought of as a proxy for â€œoffensive talentâ€ in a way.\n\nIf the values of the variable were chosen at random, then you should cluster by that variable (i.e.Â choose as the random variable)\n\nExample: If you can rerun the study using different specific farms (i.e .different values of the FARM factor, see above) and still be able to draw the same conclusions, then FARM should be a random effect.\n\nHowever, if you had wanted to compare or control for these particular farms, then Farm would be â€œfixed.â€\nSay that there is nothing about comparing these specific fields that is of interest to the researcher. Rather, the researcher wants to generalize the results of this experiment to all fields. Then, FIELD would be â€œrandom.â€\n\n\nIf the random effects are correlated with variables of interest (fixed effects), leaving them out could lead to biased fixed effects. Including them can help more reliably isolate the influence of the fixed effects of interest and more accurately model a clustered system.\nTo see individual random effects: lme4::ranef(lme_mod) or lme4::ranef(tidy_mod$fit)"
  },
  {
    "objectID": "qmd/econometrics-mixed-effects-frequentist.html#sec-econ-me-freq-assum",
    "href": "qmd/econometrics-mixed-effects-frequentist.html#sec-econ-me-freq-assum",
    "title": "Mixed Effects",
    "section": "Assumptions",
    "text": "Assumptions\n\nNo Time-Constant Unobserved Heterogeneity: \\(\\mathbb{E}(\\alpha_i\\;|\\;x_{it}) = 0\\)\n\ni.e.Â No correlation between time-invariant valued, unobserved variables (aka random effect variable) and the explanatory variable of interest (e.g.Â treatment)\n\nTime-Invariant Valued, Unobserved Variables: Variables that are constant across time for each unit and explain variation between-units\n\ne.g.Â If random effect variable (aka clustering variable) is the individual, then the â€œtime-invariant, unobservedâ€ variable could be something latent like talent or something measureable like intelligence or socio-economic status. Whatever is likely to explain the variance between-units in relation to the response.\n\n\nThe effect that these variables have must also be time-invariant\n\ne.g.Â The effect of gender on the outcome at time 1 is the same as the effect of gender at time 5\nIf effects are time-varying, then an interaction of gender with time could be included\n\nItâ€™s not reasonable for this to be exactly zero in order to use a mixed effected model, but for situations where thereâ€™s high correlation, this model should be avoided\nExample: Violation\n\n\nEach groupâ€™s x values get larger from left to right as each groupâ€™s Î± (aka y-intercepts) for each unit get larger\n\ni.e.Â Mixed-Effects models fail in cases where thereâ€™s very high correlation between group intercepts and x, together with large between-group variability compared to the within-group variability\n\n**FE model would be better in this case\n\nGelman has a paper that describes how a mixed effect model can be fit in this situation though\n\n\n\nNo Time-Varying Unobserved Heterogeneity: \\(\\mathbb{E}(\\epsilon_{it}|x_{it}) = 0\\)\n\ni.e No endogeneity (no correlation between the residuals and explanatory variable of interest)\nIf violated, there needs to be explicit measurements of omitted time-invariant variables (see 1st assumption for definition) if they are thought to interact with other variables in the model."
  },
  {
    "objectID": "qmd/econometrics-mixed-effects-frequentist.html#sec-econ-me-freq-specnot",
    "href": "qmd/econometrics-mixed-effects-frequentist.html#sec-econ-me-freq-specnot",
    "title": "Mixed Effects",
    "section": "Specifications and Notation",
    "text": "Specifications and Notation\n\nVarying Intercepts: (1 | v1)\n\nAKA Random Intercepts, where each level of the random variable (our random effect) had an adjustment to the overall intercept\nExample: Each department (random variable) has a different starting (intercepts) salary (outcome variable) for their faculty members (observations), while the annual rate (slope) at which salaries increase is consistent across the university (i.e.Â effect is constant between-departments)\n\n\\[\n\\widehat{\\text{salary}}_i = \\beta_{0 j[i]} + \\beta_1 \\cdot \\text{experience}_i\n\\]\n\nWhere j[i] is the index for department\nThis strategy allows us to capture variation in the starting salary (intercepts) of our faculty\n\n\nVarying Slopes: (0 + v2| v1)\n\nAKA Random Slopes, which would allow the effect of the v2 to vary by v1\n\ni.e.Â v2 is a predictor whose effect varies according to the level of the grouping variable, v1.\n\ni.e.Â A slope for each level of the random effects variable\nExample: faculty salary (outcome) increase at different rates (slopes) depending on the department (random variable).\n\n\\[\n\\widehat{\\text{salary}}_i = \\beta_0 + \\beta_{1 j[i]} \\cdot \\text{experience}_i\n\\]\n\nWhere j[i] is the index for department\nThis strategy allows us to capture variation in the change (slopes) in salary\n\n\nVarying Slopes and Intercepts: (1 + v2 | v1) or just (v2 | v1) (See examples)\n\nSee above for descriptions of each\nExample: Each department (random variable) has a different starting (intercepts) salary (outcome variable) for their faculty members (observations), while the annual rate (slope) at which salaries increase varies depending on department\n\n\\[\n\\widehat{\\text{salary}}_i = \\beta_{0 j[i]} + \\beta_{1 j[i]} \\cdot \\text{experience}_i\n\\]\n\nWhere j[i] is the index for department\nSee above for descriptions of each type of variation this strategy captures\n\n\nNested effects\n\ne.g.Â Studying test scores (outcome variable) within schools (random variable) that are within districts (random variable)\nNotation: (1 | v1 / v2) says intercepts varying among v1 and v2 within v1.\n\ne.g.Â schools is v2 and districts is v1\n\nUnit IDs may be repeated within groups.\n\nExample: lme4::lmer(score ~ time  + (time | school/student_id))\n\nThe random effect at this level is the combination of school and student_id, which is unique.\nYou can verify this by calling ranef on the fitted model. Itâ€™ll show you the unique combinations of student_ids within schools used in the model.\n\n\nIf you take the fixed effects values shown in summary(model) then add the ranef(model) values to them, thatâ€™s what coef(model) gives."
  },
  {
    "objectID": "qmd/econometrics-mixed-effects-frequentist.html#sec-econ-me-freq-strat",
    "href": "qmd/econometrics-mixed-effects-frequentist.html#sec-econ-me-freq-strat",
    "title": "Mixed Effects",
    "section": "Strategy",
    "text": "Strategy\n\nMisc\n\nAlso see\n\nBMLR Ch 8 &gt;&gt; Model Building Workflow: Simple to Complex\nModel Building, Concepts &gt;&gt; Misc\n\nBegins with a saturated fixed effects model, determines variance components based on that, and then simplifies the fixed part of the model after fixing the random part.\n\nOverall:\n\nEDA\nFit some simple, preliminary models, in part to establish a baseline for evaluating larger models.\nThen, build toward a final model for description and inference by attempting to add important covariates, centering certain variables, and checking model assumptions.\n\nProcess\n\nEDA at each level (See EDA, Multilevel, Longitudinal)\nExamine models with no predictors to assess variability at each level\nCreate Level One models: starting simple and adding terms as necessary (See Considerations &gt;&gt; Variable Assignment &gt;&gt; Fixed Effects )\nCreate Level Two models: starting simple and adding terms as necessary (See Considerations &gt;&gt; Variable Assignment &gt;&gt; Fixed Effects)\n\nBeginning with the equation for the intercept term.\n\nExamine the random effects and variance components (See Considerations &gt;&gt; Variable Assignment &gt;&gt; Random Effects)\n\nBeginning with a full set of error terms and then removing covariance terms and variance terms where advisable\n\ne.g.Â When parameter estimates are failing to converge or producing impossible or unlikely values\n\nSee Specifications and Notation for different RE modeling strategies."
  },
  {
    "objectID": "qmd/econometrics-mixed-effects-frequentist.html#sec-econ-me-freq-diag",
    "href": "qmd/econometrics-mixed-effects-frequentist.html#sec-econ-me-freq-diag",
    "title": "Mixed Effects",
    "section": "Diagnostics",
    "text": "Diagnostics\n\nAlso see\n\nDiagnostics, Mixed Effects\nBMLR Ch.8 &gt;&gt; Model Building Workflow &gt;&gt; Unconditional Means for an ICC example\nExamples &gt;&gt; Random Intercept-Only model for an ICC example\n\nInterClass Coefficient (ICC): The proportion of variation that is between-cases\n\n\\[\n\\rho = \\frac{\\sigma_0}{\\sigma_0 + \\sigma_\\epsilon}\n\\]\n\nWhere \\(\\sigma_0\\) is the between-case variance and \\(\\sigma_\\epsilon\\) is the within-case variance.\n1-ICC is the proportion of variation within cases\nStatistical power is a function of ICC (article)\n\n\nBoth higher ICCs and cluster size variability lead to reduced power\nThe dispersion parameter is a parameter used in the data simulation\n\nGuideline: ICC &gt; 0.1 is generally accepted as the minimal threshold for justifying the use of Mixed Effects Model\nExample: {sjPlot}\nlibrary(sjPlot)\ntab_model(lme_fit)\n\nMight need {lmerTest} loaded to get coefficient pvals\nAlso calculates two R2 values\n\nMarginal: proportion of variance explained , by the fixed effects only\nConditional: proportion of variance explained by the fixed effects and random effects"
  },
  {
    "objectID": "qmd/econometrics-mixed-effects-frequentist.html#sec-econ-me-freq-examp",
    "href": "qmd/econometrics-mixed-effects-frequentist.html#sec-econ-me-freq-examp",
    "title": "Mixed Effects",
    "section": "Examples",
    "text": "Examples\n\nExample: One-Way Random Effects\nre1 &lt;- plm(happiness ~ age, data = df,\nÂ  Â  Â  Â  Â  index = c(\"id\", \"wave\"),\nÂ  Â  Â  Â  Â  effect = \"individual\", model = \"random\")\n\nsummary(re1)\n## Effects:\n##Â  Â  Â  Â  Â  Â  Â  Â  Â  var std.dev share\n## idiosyncratic 0.03641 0.19081 0.096\n## individualÂ  Â  0.34396 0.58648 0.904\n## theta: 0.8394\n## Coefficients:\n##Â  Â  Â  Â  Â  Â  Â  Estimate Std. Error z-valueÂ  Pr(&gt;|z|)Â  Â \n## (Intercept)Â  6.933947Â  1.534054Â  4.5200 6.183e-06 ***\n## ageÂ  Â  Â  Â   -0.015621Â  0.031277 -0.4994Â  Â  0.6175\n\nIn Fixed effects, â€œidâ€ is the cases variable and â€œwaveâ€ is the time variable, but Iâ€™m not sure if thatâ€™s how theyâ€™re referred to in a Mixed Effects model\n\nMaybe indexes are always required for panel data even if it isnâ€™t a FE model\n\neffect = â€œindividualâ€, model = â€œrandomâ€ specifies the model as a mixed effects model\nidiosyncratic is the time-variant variance (within-cases, Îµ) variance\nindividual is the time-constant (between-cases, Î±) variance\n\nExample: {lme4}\n\nFrom https://www.alexcernat.com/etimating-multilevel-models-for-change-in-r\nusl is UK sociological survey data\n\nlogincome is a logged income variable\npidp is the personâ€™s id\nwave0 is the time variable thatâ€™s indexed at 0\n\n\n\nRandom InterceptsRandom InterceptsRandom Slopes and Intercepts\n\n\nlibrary(lme4)\n# unconditional means model (a.k.a. random effects model)\nm0 &lt;- lmer(data = usl, logincome ~ 1 + (1 | pidp))\n\n# check results\nsummary(m0)\n## Linear mixed model fit by REML ['lmerMod']\n## Formula: logincome ~ 1 + (1 | pidp)\n##Â  Â  Data: usl\n##Â \n## REML criterion at convergence: 101669.8\n##Â \n## Scaled residuals:Â \n##Â  Â  MinÂ  Â  Â  1QÂ  MedianÂ  Â  Â  3QÂ  Â  MaxÂ \n## -7.2616 -0.2546Â  0.0627Â  0.3681Â  5.8845Â \n##Â \n## Random effects:\n##Â  GroupsÂ  NameÂ  Â  Â  Â  Variance Std.Dev.\n##Â  pidpÂ  Â  (Intercept) 0.5203Â  0.7213Â \n##Â  ResidualÂ  Â  Â  Â  Â  Â  0.2655Â  0.5152Â \n## Number of obs: 52512, groups:Â  pidp, 8752\n##Â \n## Fixed effects:\n##Â  Â  Â  Â  Â  Â  Estimate Std. Error t value\n## (Intercept) 7.162798Â  0.008031Â  891.8\n\n0 covariates\nInterpretation\n\nFixed effects:\n\n(Intercept) = 7.162798, which is the grand mean\n\nSays that over all the time points and individuals, the estimated average log income is 7.162798\n\n\nRandom effects:\n\nEverything that varies by pidp\nBetween-case (or person, group, cluster, etc.) variance: (Intercept) = 0.5203\nWithin-case (or person, group, cluster, etc.) variance: Residual = 0.2655\n\n\nICC\n\n\\(\\frac{0.520}{0.520 + 0.265} = 0.662\\)\nSays about 66% of the variation in log income comes between people while the remaining (~ 34%) is within people.\n\n\n\n\nlibrary(lme4)\n# unconditional change model (a.k.a. MLMC)\nm1 &lt;- lmer(data = usl, logincome ~ 1 + wave0 + (1 | pidp))\n\nsummary(m1)\n## Linear mixed model fit by REML ['lmerMod']\n## Formula: logincome ~ 1 + wave0 + (1 | pidp)\n##Â  Â  Data: usl\n##Â \n## REML criterion at convergence: 100654.8\n##Â \n## Scaled residuals:Â \n##Â  Â  MinÂ  Â  Â  1QÂ  MedianÂ  Â  Â  3QÂ  Â  MaxÂ \n## -7.1469 -0.2463Â  0.0556Â  0.3602Â  5.7533Â \n##Â \n## Random effects:\n##Â  GroupsÂ  NameÂ  Â  Â  Â  Variance Std.Dev.\n##Â  pidpÂ  Â  (Intercept) 0.5213Â  0.7220Â \n##Â  ResidualÂ  Â  Â  Â  Â  Â  0.2593Â  0.5092Â \n## Number of obs: 52512, groups:Â  pidp, 8752\n##Â \n## Fixed effects:\n##Â  Â  Â  Â  Â  Â  Estimate Std. Error t value\n## (Intercept) 7.057963Â  0.008665Â  814.51\n## wave0Â  Â  Â   0.041934Â  0.001301Â  32.23\n##Â \n## Correlation of Fixed Effects:\n##Â  Â  Â  (Intr)\n## wave0 -0.375\n\n1 covariate\nRandom Effect: Cases; Fixed Effect: Time\nInterpretation\n\nFixed effects:\n\n(Intercept) = 7.057; the expected log income at the beginning of the study (wave 0).\nwave0 = 0.0419\n\nThe average rate of change with the passing of a wave.\n\nI donâ€™t think this is a percentage. Itâ€™s the same as a standard OLS regression interpretation.\n\nSo after each wave, individual log income is expected to slowly increase by 0.041 on average.\n\n\nRandom Effects\n\nEverything that varies by pidp\nBetween-Case (or person, group, cluster, etc.) variance in log income: (Intercept) = 0.5213\nWithin-Case (or person, group, cluster, etc.) variance in log income: Residual = 0.2593\nSo this stuff is still very similar numbers as the random intercept-only model\n\n\nVisualize Effect\n\nusl$pred_m1 &lt;- predict(m1)\nusl %&gt;%Â \nÂ  filter(pidp %in% 1:5) %&gt;% # select just five individuals\nÂ  ggplot(aes(wave, pred_m1, color = pidp)) +\nÂ  geom_point(aes(wave, logincome)) + # points for observed log income\nÂ  geom_smooth(method = lm, se = FALSE) + # linear line showing wave0 slope\nÂ  theme_bw() +\nÂ  labs(x = \"Wave\", y = \"Logincome\") +Â \nÂ  theme(legend.position = \"none\")\n\nwave was indexed to 0 for the model but now wave starts at 1. He mightâ€™ve reverted wave to have a starting value of 1 for graphing purposes\nLines show the small, positive, fixed effect slope for wave0\nParallel lines means we assume the change in log income over time is the same for all the individuals\n\ni.e.Â We assume there is no between-case variation in the rate of change.\n\n\n\n\n\nlibrary(lme4)\n# unconditional change model (a.k.a. MLMC) with re for change\nm2 &lt;- lmer(data = usl, logincome ~ 1 + wave0 + (1 + wave0 | pidp))\n\nsummary(m2)\n## Linear mixed model fit by REML ['lmerMod']\n## Formula: logincome ~ 1 + wave0 + (1 + wave0 | pidp)\n##Â  Â  Data: usl\n##Â \n## REML criterion at convergence: 98116.1\n##Â \n## Scaled residuals:Â \n##Â  Â  MinÂ  Â  Â  1QÂ  MedianÂ  Â  Â  3QÂ  Â  MaxÂ \n## -7.8825 -0.2306Â  0.0464Â  0.3206Â  5.8611Â \n##Â \n## Random effects:\n##Â  GroupsÂ  NameÂ  Â  Â  Â  Variance Std.Dev. CorrÂ \n##Â  pidpÂ  Â  (Intercept) 0.69590Â  0.8342Â  Â  Â  Â \n##Â  Â  Â  Â  Â  wave0Â  Â  Â   0.01394Â  0.1181Â  -0.51\n##Â  ResidualÂ  Â  Â  Â  Â  Â  0.21052Â  0.4588Â  Â  Â  Â \n## Number of obs: 52512, groups:Â  pidp, 8752\n##Â \n## Fixed effects:\n##Â  Â  Â  Â  Â  Â  Estimate Std. Error t value\n## (Intercept) 7.057963Â  0.009598Â  735.39\n## wave0Â  Â  Â   0.041934Â  0.001723Â  24.34\n##Â \n## Correlation of Fixed Effects:\n##Â  Â  Â  (Intr)\n## wave0 -0.558\n\nRandom Effect: outcome (i.e.Â intercept) by cases\nTime Effect by cases\nFixed Effect: time\n(1 + wave0 | pidp) - Says let both the intercept (â€œ1â€) and wave0 vary by pidp - Which means that average log income varies by person and the rate of change (slope) in log income over time (wave0 fixed effect) varies by person.\nInterpretation\n\nFixed effects:\n\n(Intercept) = 7.057963: The expected log income at the beginning of the study (wave 0).\nwave0 = 0.0419\n\nThe average rate of change with the passing of a wave.\n\nI donâ€™t think this is a percentage. Itâ€™s the same as a standard OLS regression interpretation.\n\nSo after each wave, individual log income is expected to slowly increase by 0.041 on average.\n\n\nRandom effects:\n\nEverything that varies by pidp\nBetween-Case (or person, group, cluster, etc.) variance of log income:\n\n(Intercept) = 0.69590\nHow much average log income varies by person at the start of the study (wave = 0)\n\nWithin-Case (or person, group, cluster, etc.) variance of log income:\n\nResidual = 0.21052\nHow much each personâ€™s log income varies in relation to her average log income\n\nBetween-Case variance in the rates of change of log income over time\n\nwave0 = 0.01394\ni.e.Â How much the fixed effect, wave0, slope varies by person\n\n\n\nVisualize Effect\n\nusl$pred_m2 &lt;- predict(m2) \nusl %&gt;%  \n  filter(pidp %in% 1:5) %&gt;% # select just two individuals \n  ggplot(aes(wave, pred_m2, color = pidp)) + \n  geom_point(aes(wave, logincome)) + # points for observed logincome \n  geom_smooth(method = lm, se = FALSE) + # linear line based on prediction \n  theme_bw() + # nice theme \n  labs(x = \"Wave\", y = \"Logincome\") + # nice labels \n  theme(legend.position = \"none\")\n\nDifferent slopes for each person\n\n\n\n\n\nExample: Varying Slopes and Intercepts\nm_slope &lt;- lmer(pp60 ~ position + toi + \n                (1 + age | player),\n                data = df)\n\nWe use this model if we wanted to incorporate an age variable into our model and we wanted the influence of that variable to vary by player, it would be incorporated like so, before the | player:\n\nExample: {ggeffects} Error Bar Plot\n\nlibrary(ggeffects)\nlibrary(ggplot2)\n# create plot dataframe\n# Has 95% CIs for fixed effects and lists random effects\nplot_data &lt;- ggpredict(fit, terms = c(\"Season\"))\n\n#create plot\nplot_data %&gt;%\nÂ  #reorder factor levels for plotting\nÂ  mutate(x = ordered(x, levels = c(\"Preseason\", \"Inseason\", \"Postseason\"))) %&gt;%\nÂ  #use plot function with ggpredict objects\nÂ  plot() +Â \nÂ  #add ggplot2 as needed\nÂ  theme_blank() + ylim(c(3000,7000)) + ggtitle(\"Session Distance by Season Phase\")\n\nDescription:\n\nOutcome: Distance\nFixed Effect: Season\nRandom Effect: Athlete\n\n\nExample: {tidymodels} Varying Intercepts\nlmer_spec &lt;-Â \nÂ  linear_reg() %&gt;%Â \nÂ  set_engine(\"lmer\")\n\ntidy_mod &lt;-Â \nÂ  lmer_spec %&gt;%Â \nÂ  fit(pp60 ~ position + toi + (1 | player),\nÂ  Â  Â  data = df)\n\ntidy_mod\n## parsnip model object\n##Â \n## Linear mixed model fit by REML ['lmerMod']\n## Formula: pp60 ~ position + toi + (1 | player)\n##Â  Â  Data: data\n## REML criterion at convergence: 115.8825\n## Random effects:\n##Â  GroupsÂ  NameÂ  Â  Â  Â  Std.Dev.\n##Â  playerÂ  (Intercept) 0.6423Â \n##Â  ResidualÂ  Â  Â  Â  Â  Â  0.3452Â \n## Number of obs: 80, groups:Â  player, 20\n## Fixed Effects:\n## (Intercept)Â  Â  positionFÂ  Â  Â  Â  Â  toiÂ \n##Â  Â  -0.16546Â  Â  Â  1.48931Â  Â  Â  0.06254"
  },
  {
    "objectID": "qmd/econometrics-mixed-effects-frequentist.html#sec-econ-me-freq-bmlrch8",
    "href": "qmd/econometrics-mixed-effects-frequentist.html#sec-econ-me-freq-bmlrch8",
    "title": "Mixed Effects",
    "section": "BMLR Chapter 8: Multilevel Models",
    "text": "BMLR Chapter 8: Multilevel Models\n\nBeyond Multiple Linear Regression, Chapter 8: Introduction to Multilevel Models\nVariable Types\n\nLevel One: variables measured at the most frequently occurring observational unit\n\ni.e.Â Variables that (for the most part) have different values for each row\ni.e.Â Vary for each repeated measure of a subject and vary between subjects\n\nLevel Two: variables measured on larger observational units\n\ni.e.Â Constant for each repeated measure of a subject but vary between each subject\n\n\nData Description in the Examples\n\nna (outcome) - Negative Affect (i.e.Â Performance Anxiety)\nlarge - binary\n\nlarge = 1 means the performance type is a Large Ensemble\nlarge = 0 means the performance type is either a Solo or Small Ensemble\n\norch - binary\n\norch = 1 means the instrument is Orchestral\norch = 0 means the instrument is either a Keyboard or itâ€™s Vocal\n\n\nTwo-Stage Model\n\nThe 2-stage model shows a clearer depiction of how a multi-level model is fit in principle\n\n2-stage model uses Ordinary Least Squares to fit a system of regression equations in stages\nThe multi-level model fits a composite of that system of equations using Maximum Likelihood Estimation\n\nProcess\n\nLevel 1 models are fitted for each subject/unit\nUsing the estimated level 1 intercepts and slopes as outcome variables, Level 2 intercept and slope models respectively are fit\nThe errors from the Level 2 models are the random effects\n\nIssues\n\nWeights every subject the same regardless of the number of repeated observations\nResponds to missing individual slopes (i.e.Â subjects never exposed to treatment) by simply dropping those subjects\nDoes not share strength effectively across subjects\n\nSpecification\n\nLevel 1\n\\[\nY_{ij} = a_i + b_i \\text{large}_{ij} + \\epsilon_{ij}\n\\]\n\n\\(b_i\\) is the fixed effect for subject/unit \\(i\\) and observation \\(j\\) (i.e.Â each subject has repeated observations)\n\nLevel 2\n\\[\na_i = \\alpha_0 + \\alpha_1 \\text{orch}_i + u_i \\\\\nb_i = \\beta_0 + \\beta_1 \\text{orch}_i + v_i\n\\]\n\n\\(u_i\\) and \\(v_i\\) are the random effects for subject/unit \\(i\\)\n\\(a_i\\) is the true (i.e.Â not estimated since no hat) mean of performance anxiety (outcome) when subject plays solos or small ensembles (large = 0)\n\\(b_i\\) is the true mean difference in performance anxiety (outcome) for subjecti between large ensembles and other performance types (large contrast)\n\n\n\n\n\nMulti-Level\n\nProcess\n\nLevel 1 and Level 2 equations have been combined through substitution then reduced into a composite model\nParameters estimated through Maximum Likelihood Estimation\n\nMisc\n\nError Distribution\n\nCorrelation between parameters is accounted for using a multivariate normal distribution estimated through a variance-covariance matrix of the error terms (aka random effects) of each Level (see link for details)\n\nAdding Level 1 variables to a model formula should change within-person variability (\\(\\hat\\sigma^2\\) and \\(\\hat\\sigma\\))\n\nEven without adding a Level 1 variable, small changes could occur due to numerical estimation procedures used in likelihood-based parameter estimates\n\nOptimization methods\n\nUsually very little difference between ML and REML parameter estimates\nRestricted Maximum Likelihood (REML) is preferable when the number of parameters is large or the primary interest is obtaining estimates of model parameters, either fixed effects or variance components associated with random effects\nMaximum Likelihood (MLE) should be used if nested fixed effects models are being compared using a likelihood ratio test, although REML is fine for nested models of random effects that have the same fixed effects.\n\nP-Values\n\nCanâ€™t be calculated because the exact distribution of the test statistics under the null hypothesis (no fixed effect) is unknown, primarily because the exact degrees of freedom is not known\nRule of Thumb: t-values (ratios of parameter estimates to estimated standard errors) with absolute value above 2 indicate significant evidence that a particular model parameter is different than 0\nPackages that do report p-values of fixed effects typically using conservative assumptions, large-sample results, or approximate degrees of freedom for a t-distribution\n\nUsing {lmerTest} will produce a summary of {lme4} with pvals for coefficients\n\nParametric Bootstrap can be used to approximate the distribution of the likelihood test statistic and produce more accurate p-values by simulating data under the null hypothesis\n\nModel Comparison\n\nPseudo-R2\n\nSee below in Simple to Complex Model Building Workflow &gt;&gt;\n\nRandom Slopes and Intercepts Model (with 1 covariate)\nRandom Slopes and Intercepts Model (with 2 covariates)\n\n\nAIC, BIC\n\nSee Random Intercepts (with 2 covariates)\n\nLikelihood Ratio Test (LR-Test)\n\nSee Random Slopes and Intercepts Model (with 3 covariates)\n\n\n\nComposite Specification\n\\[\nY_{ij} = [\\alpha_0 + \\alpha_1 \\text{orch}_i + \\beta_0 \\text{large}_{ij} + \\beta_1 \\text{orch}_i \\text{large}_{ij}] + [u_i + v_i \\text{large}_{ij} + \\epsilon_{ij}]\n\\]\n\nComposite model of level 1 and level 2 equations\nRandom Slopes and Intercepts Model (with 2 covariates)\nFixed Effects: \\(\\alpha_0\\), \\(\\alpha_1\\), \\(\\beta_0\\) and \\(\\beta_1\\)\n\\(u_i + v_i \\cdot \\text{large}_{ij} + \\epsilon_{ij}\\) is the interesting part.\n\nThis part has all the error terms and any variables in the Level 2 equations\nThe first part is a typical regression with interaction terms.\n\n\nInterpretation\n\nSee Two-stage model for definitions for \\(a_i\\) and \\(b_i\\)\n\nKeyboardists and Vocalists (orchi = 0)\n\\[\n\\begin{align}\na_i &= \\alpha_0 + u_i \\\\\nb_i &= \\beta_0 + v_i\n\\end{align}\n\\]\nOrchestral Instrumentalists (orchi = 1)\n\\[\n\\begin{align}\na_i = (\\alpha_0 + \\alpha_1) + u_i \\\\\nb_i = (\\beta_0 + \\beta_1) + v_i\n\\end{align}\n\\]\n\nMean Performance Anxiety (outcome) when:\n\nKeyboardists or Vocalists (orch = 0) play solos or small ensembles (large = 0): \\(\\alpha_0\\)\nKeyboardists or vocalists (orch = 0) play large ensembles (large = 1): \\(\\alpha_0 + \\beta_0\\)\nOrchestral instrumentalists (orch = 1) play solos or small ensembles (large = 0): \\(\\alpha_0 + \\alpha_1\\alpha_0 + \\alpha_1\\)\nOrchestral instrumentalists (orch = 1) play large ensembles (large = 1): \\(\\alpha_0 + \\alpha_1 + \\beta_0 + \\beta_1\\)\n\n\nModel Summary (using lmer4::lmer())\n#&gt; Â  Â  Linear mixed model fit by REML ['lmerMod']Â \n#&gt; A)Â  Formula: na ~ orch + large + orch:large + (large | id)Â \n#&gt; Â  Â  Â  Â  Data: musicÂ \n#&gt; B)Â  REML criterion at convergence: 2987Â \n\n#&gt; B2)Â  Â  Â  AICÂ  Â  Â  BICÂ  logLik deviance df.residÂ \n#&gt; Â  Â  Â  Â  3007Â  Â  3041Â  Â  -1496Â  Â  2991Â  Â  Â  489Â \n\n#&gt; Â  Â  Random effects:Â \n#&gt; Â  Â  Â  GroupsÂ  NameÂ  Â  Â  Â  Variance Std.Dev. CorrÂ \n#&gt; C)Â  idÂ  Â  Â  (Intercept)Â    5.655Â   2.378Â  Â  Â  Â  Â \n#&gt; D)Â  Â  Â  Â  Â  Â  largeÂ  Â  Â  Â  0.452Â   0.672Â  Â -0.63Â \n#&gt; E)Â  ResidualÂ  Â  Â  Â  Â  Â    21.807Â   4.670Â  Â  Â  Â  Â \n#&gt; F)Â  Number of obs: 497, groups:Â  id, 37Â \n\n#&gt; Â  Â  Fixed effects:Â \n#&gt; Â  Â  Â  Â  Â  Â  Â  Â  Estimate Std. Error t valueÂ \n#&gt; G)Â  (Intercept)Â  15.930Â  Â  Â  0.641Â  24.83Â \n#&gt; H)Â  orchÂ  Â  Â  Â  Â  1.693Â  Â  Â  0.945Â  Â 1.79Â \n#&gt; I)Â  largeÂ  Â  Â  Â  -0.911Â  Â  Â  0.845Â  -1.08Â \n#&gt; J)Â  orch:largeÂ  Â -1.424Â  Â  Â  1.099Â  -1.30\n\nDefinitions\n\nA: How our multilevel model is written in R, based on the composite model formulation.\nB: Measures of model performance. Since this model was fit using REML, this line only contains the REML criterion.\nB2: If the model is fit with ML instead of REML, the measures of performance will contain AIC, BIC, deviance, and the log-likelihood.\nC: Estimated variance components (\\(\\hat \\sigma^2_u\\) and \\(\\hat \\sigma_u\\)) associated with the intercept equation in Level Two. (between-unit variability)\nD: Estimated variance components (\\(\\hat \\sigma^2_v\\) and \\(\\hat \\sigma_v\\)) associated with the large ensemble (large = 1) effect equation in Level Two. (Also between-unit variability but just for the slope)\n\nAlso, in the â€œCorrâ€ column, the estimated correlation (\\(\\hat \\rho_{uv}\\)) between the two Level Two error terms.\n\nE: Estimated variance components (\\(\\hat \\sigma^2\\) and \\(\\hat \\sigma\\) associated with the Level One equation. (within-unit variability)\nF: Total number of performances where data was collected (Level One observations = 497) and total number of subjects (Level Two observations = 37).\nG: Estimated fixed effect (\\(\\hat \\alpha_0\\)) for the intercept term, along with its standard error and t-value (which is the ratio of the estimated coefficient to its standard error).\nH: Estimated fixed effect (\\(\\hat \\alpha_1\\)) for the orchestral instrument (orch = 1) effect, along with its standard error and t-value.\nI: Estimated fixed effect (\\(\\hat \\beta_0\\)) for the large ensemble (large = 1) effect, along with its standard error and t-value.\nJ: Estimated fixed effect (\\(\\hat \\beta_1\\)) for the interaction between orchestral instruments (orch = 1) and large ensembles (large = 1), along with its standard error and t-value.\n\nInterpretations\n\nFixed effects:\n\n\\(\\hat \\alpha_0 = 15.9\\) â€” The estimated mean performance anxiety (outcome) for solos and small ensembles (large = 0) for keyboard players and vocalists (orch = 0) is 15.9.\n\\(\\hat \\alpha_1 = 1.7\\) â€” Orchestral instrumentalists (orch = 1) have an estimated mean performance anxiety (outcome) for solos and small ensembles (large = 0) which is 1.7 points higher than keyboard players and vocalists (orch = 0).\n\\(\\hat \\beta_0 = âˆ’0.9\\) â€” Keyboard players and vocalists (orch = 0) have an estimated mean decrease in performance anxiety (outcome) of 0.9 points when playing in large ensembles (large = 1) instead of solos or small ensembles (large = 0).\n\\(\\hat \\beta_1 = âˆ’1.4\\) â€” Orchestral instrumentalists (orch = 1) have an estimated mean decrease in performance anxiety (outcome) of 2.3 points when playing in large ensembles (large = 1) instead of solos or small ensembles (large = 0), 1.4 points greater than the mean decrease among keyboard players and vocalists (orch = 0).\n\nHeâ€™s calculating simple slope/marginal effect (See Regression, Interactions) and choosing to use that as the interpretation for the interaction term\n\nFrom Ch. 1 (link): â€œWe interpret the coefficient for the interaction term by comparing slopes under fast and non-fast conditions; this produces a much more understandable interpretation for a reader than attempting to interpret the -0.011 (interaction coef) directlyâ€\n\nFast and non-fast are levels of a main effect and a term in the interaction.\n\n\nCalculation: The interaction is the difference in slopes for one interaction variable (e.g.Â large) at different values of the other interaction variable (e.g.Â orch)\n\nRegression eq (ignoring the random effects part):\n\n\\(\\hat Y_i = 15.93 + 1.693\\text{orch}_i - 0.911\\text{large} âˆ’ 1.424\\text{orch}_i \\times \\text{large}_i\\)\n\nExamining the large slope so only dealing with terms that contain â€œlargeâ€\n\nlarge = 1 is reference category in the interaction that the coefficient is associated with so it remains constant\n\nWhen orch = 0 and large = 1, the slope for large is -0.911 = \\(\\hat \\beta_0\\)\n\n\\(- 0.911\\text{large} âˆ’ 1.424\\text{orch}_i \\times \\text{large}_i = -0.911 \\times 1 - 1.424 \\times \\boldsymbol{0} \\times 1 = -0.911 = \\hat \\beta_0\\)\n\nWhen orch = 1 and large = 1, the slope for large is -2.335 (uses this one for his interpretation)\n\n\\(- 0.911\\text{large} âˆ’ 1.424\\text{orch}_i \\times \\text{large}_i = -0.911 \\times 1 - 1.424 \\times \\boldsymbol{1} \\times 1 = -0.911 - 1.424 = -2.335\\) (i.e.Â â€œdecreaseâ€¦ of 2.3 pointsâ€)\n\nThe difference in these slopes is the interaction coefficient: \\(-2.335 - (-0.911) = -1.424\\)\n\n\n\nVariance components\n\n\\(\\hat \\sigma_u = 2.4\\) â€” The estimated standard deviation of performance anxiety (outcome) for solos and small ensembles (large = 0) is 2.4 points, after controlling for instrument (orch) played.\n\\(\\hat \\sigma_v = 0.7\\) â€” The estimated standard deviation of differences in performance anxiety (outcome) between large ensembles (large = 1) and other performance types (large = 0) is 0.7 points, after controlling for instrument (orch) played.\n\\(\\hat \\rho_{uv} = âˆ’0.64\\) â€” The estimated correlation between performance anxiety scores (outcome) for solos and small ensembles (large = 0) and increases in performance anxiety (outcome) for large ensembles (large = 1) is -0.64, after controlling for instrument (orch) played.\n\nThose subjects with higher performance anxiety scores for solos and small ensembles tend to have greater decreases in performance anxiety for large ensemble performances.\n\n\\(\\hat \\sigma = 4.7\\) â€” The estimated standard deviation in residuals for the individual regression models is 4.7 points.\n\n\n\n\n\n\nModel Building Workflow: Simple to Complex\n\nWorkflow should include:\n\nFixed effects that allow one to address primary research questions\nFixed effects that control for important covariates at all levels\nInvestigation of potential interactions\nVariables that are centered where interpretations can be enhanced\nImportant variance components\nRemoval of unnecessary terms\nA final model that tells a â€œpersuasive story parsimoniouslyâ€\n\nModel Comparison (Also see multi-level &gt;&gt; misc &gt;&gt; p-values & model comparison)\n\nLR-Test for nested models\nAIC, BIC for unnested models\nConsider models that involve removing terms that have t-stats &lt; |2|\n\nModels\n\nUnconditional Means (aka Random Intercepts)\nRandom Slopes and Intercepts Model (with 1 covariate)\nRandom Slopes and Intercepts Model (with 2 covariates)\nRandom Intercepts (with 2 covariates)\nRandom Slopes and Intercepts Model (with 3 covariates)\nFinal Model\n\n\n\nUnconditional Means (aka Random Intercepts)\n\nSpecification\n\nLevel 1: \\(Y_{ij} = a_i + \\epsilon_{ij} \\quad \\text{where}\\: \\epsilon \\sim \\mathcal{N}(0, \\sigma^2)\\)\n\n\\(a_i\\) is the true mean response of all observations for subjecti\n\nLevel 2: \\(a_i = \\alpha_0 + u_i \\quad \\text{where} \\: u_i \\sim \\mathcal{N}(0, \\sigma^2_u)\\)\n\nEach subjectâ€™s intercept is assumed to be a random value from a normal distribution centered at \\(\\alpha_0\\) with variance \\(\\sigma^2_u\\).\n\nComposite: \\(Y_{ij} = \\alpha_0 + u_i + \\epsilon_{ij}\\)\n\nModel\n#Model A (Unconditional means model)\nmodel.a &lt;- lmer(na ~ 1 + (1 | id), REML = T, data = music)\n##Â  GroupsÂ  NameÂ  Â  Â  Â  Variance Std.Dev.\n##Â  idÂ  Â  Â  (Intercept)Â  4.95Â  Â  2.22Â  Â \n##Â  ResidualÂ  Â  Â  Â  Â  Â  22.46Â  Â  4.74\n##Â  Number of Level Two groups =Â  37\n##Â  Â  Â  Â  Â  Â  Estimate Std. Error t value\n## (Intercept)Â  Â  16.24Â  Â  0.4279Â  37.94\nInterpretation\n\n\\(\\hat \\alpha_0 = 16.2\\) â€” The estimated mean performance anxiety score across all performances and all subjects.\n\\(\\hat \\sigma^2 = 22.5\\) â€” The estimated variance in within-person deviations.\n\\(\\hat \\sigma^2_u = 5.0\\) (rounded from 4.95) â€” The estimated variance in between-person deviations.\nICC\n\\[\n\\hat \\rho = \\frac{\\text{between-person variability}}{\\text{total variability}} = \\frac{\\hat \\sigma^2_u}{\\hat \\sigma^2_u + \\hat \\sigma ^2} = \\frac{5.0}{5.0 + 22.5} = 0.182\n\\]\n\n18.2% of the total variability in performance anxiety scores are attributable to differences among subjects.\n\n*For plain random intercepts models only*, you can also say this same number says that the average correlation for any pair of responses from the same individual is a moderately low 0.182.\n\nAs \\(\\rho\\) approaches 0: responses from an individual are essentially independent and accounting for the multilevel structure of the data becomes less crucial.\nAs \\(\\rho\\) approaches 1: repeated observations from the same individual essentially provide no additional information and accounting for the multilevel structure becomes very important.\nEffective Sample Size (ESS): The number of independent pieces of information we have for modeling\n\nWith \\(\\rho\\) near 0: ESS approaches the total number of observations.\nWith \\(\\rho\\) near 1: ESS approaches the number of subjects in the study.\n\n\n\n\n\n\nRandom Slopes and Intercepts Model (with 1 covariate)\n\n1 - Level 1\nSpecification\n\nLevel 1: \\(Y_{ij} = a_i + b_i \\text{large}_{ij} + \\epsilon_{ij}\\)\nLevel 2\n\\[\na_i = \\alpha_0 + u_i \\\\\nb_i = \\beta_0 +v_i\n\\]\nComposite\n\\[\n\\begin{aligned}\n&Y_{ij} = [\\alpha_0 + \\beta_0 \\text{large}_{ij}] + [u_i + v_i \\text{large}_{ij} + \\epsilon_{ij}] \\\\\n&\\begin{aligned}\n\\text{where}\\quad \\epsilon &\\sim \\mathcal{N}(0, \\sigma^2) \\: \\text{and}\\\\\n\\left[ \\begin{array}{cc} u_i \\\\ v_i \\end{array} \\right] &\\sim \\mathcal{N} \\left(\\left[\\begin{array}{cc} 0\\\\0 \\end{array}\\right], \\left[\\begin{array}{cc} \\sigma^2_u\\\\\\rho\\sigma_u \\sigma_v \\quad \\sigma^2_v \\end{array}\\right]\\right)\n\\end{aligned}\n\\end{aligned}\n\\]\n\nModel\nmodel.b &lt;- lmer(na ~ large + (large | id), data = music)\n##Â  GroupsÂ  NameÂ  Â  Â  Â  Variance Std.Dev. CorrÂ \n##Â  idÂ  Â  Â  (Intercept)Â  6.333Â  2.517Â  Â  Â  Â  Â \n##Â  Â  Â  Â  Â  largeÂ  Â  Â  Â  0.743Â  0.862Â  Â  -0.76\n##Â  ResidualÂ  Â  Â  Â  Â  Â  21.771Â  4.666\n##Â  Number of Level Two groups =Â  37\n##Â  Â  Â  Â  Â  Â  Estimate Std. Error t value\n## (Intercept)Â  16.730Â  Â  0.4908Â  34.09\n## largeÂ  Â  Â  Â  -1.676Â  Â  0.5425Â  -3.09\nInterpretation\n\n\\(\\hat \\alpha_0 = 16.7\\) â€” The mean performance anxiety level (outcome) before solos and small ensemble performances (large = 0).\n\\(\\hat \\beta_0 = âˆ’1.7\\) â€” The mean decrease in performance anxiety (outcome) before large ensemble performances (large = 1).\n\nSubjects had a performance anxiety level of 16.7 before solos and small ensembles, and their anxiety levels were 1.7 points lower, on average, before large ensembles, producing an average performance anxiety level before large ensembles of 15.0\nStatistically significant since |t-value| &gt; 2\n\n\\(\\hat \\sigma^2 = 21.8\\) â€” The variance in within-person deviations.\n\\(\\hat \\sigma^2_u = 6.3\\) â€” The variance in between-person deviations in performance anxiety scores (outcome) before solos and small ensembles (large = 0).\n\\(\\hat \\sigma^2_v = 0.7\\) â€” The variance in between-person deviations in increases (or decreases) (slope) in performance anxiety scores (outcome) before large ensembles (large = 1).\n\\(\\hat \\rho_{uv} = âˆ’0.76\\) (Corr column)\n\nSlopes and intercepts are negatively correlated\nA strong negative relationship between a subjectâ€™s performance anxiety (outcome) before solos and small ensembles (large = 0) (Intercept) and their (typical) decrease in performance anxiety (Slope) before large ensembles (large = 1) .\n\n\nPseudo-R2 (Not always a reliable performance measure)\n\nUnconditional Means vs Random Intercepts and Slopes\n\\[\n\\text{Psuedo R}^2_{L1} = \\frac{\\hat \\sigma^2 (\\text{Model A}) - \\hat\\sigma^2(\\text{Model B})}{\\hat \\sigma^2(\\text{Model A})} = \\frac{22.5 - 21.8}{22.5} = 0.031\n\\]\n\nWhere Model A: Unconditional Means; Model B: Random intercepts and slopes\nFYI\n\n\\(\\text{percent decrease} = \\frac{\\text{original value} - \\text{new value}}{\\text{orginal value}}\\)\n\npseudo-R2 uses this one\nIf negative, itâ€™s a percent increase.\n\n\\(\\text{percent increase} = \\frac{\\text{new value} - \\text{original value}}{\\text{original value}}\\)\n\nIf negative, itâ€™s a percent decrease.\n\n\n\nPositive value means Model B is an improvement over Model A\n\nMeans the variance in Model Bâ€™s error terms is smaller than Model Aâ€™s, and therefore better fits the data. (I think)\n\nThe estimated within-person variance \\(\\hat \\sigma^2\\) decreased by 3.1% (from 22.5 to 21.8) from the unconditional means model\nImplies that only 3.1% of within-person variability in performance anxiety scores can be explained by performance type\nValues of \\(\\hat \\sigma^2_u\\) and \\(\\hat \\sigma^2_v\\) from Model B cannot be compared to between-person variability from Model A using pseudo-R2, since the inclusion of performance type has changed the interpretation of these values\nIssues\n\nâ€œBecause of the complexity of estimating fixed effects and variance components at various levels of a multilevel model, it is not unusual to encounter situations in which covariates in a Level Two equation for the intercept (for example) remain constant (while other aspects of the model change), yet the associated pseudo R-squared values differ or are negative.â€ (?)\n\n\n\n\n\nRandom Slopes and Intercepts Model (with 2 covariates)\n\n1 - Level 1; 1 - Level 2\nSpecification, Model, Interpretation (see Multi-Level section above)\n\nSays the large effect (slope) varies across subjects/units\n\nPseudo-R2\n\\[\n\\begin{align}\n\\text{Psuedo R}^2_{L2_u} &= \\frac{\\hat \\sigma^2_u (\\text{Model B}) - \\hat\\sigma^2_u(\\text{Model C})}{\\hat \\sigma^2_u(\\text{Model B})} = \\frac{6.33 - 5.66}{6.33} = 0.106 \\\\\n\\text{Psuedo R}^2_{L2_v} &= \\frac{\\hat \\sigma^2_v (\\text{Model A}) - \\hat\\sigma^2_v(\\text{Model B})}{\\hat \\sigma^2_v(\\text{Model A})} = \\frac{0.74 - 0.45}{0.74} = 0.392\n\\end{align}\n\\]\n\nModel B is Random Slopes and Intercepts Model (with 1 covariate)\nModel C is Random Slopes and Intercepts Model (with 2 covariates)\nThe addition of Level 2 variable, orch, in Model C:\n\n(Top) Decreased the between-person variability in mean (intercept) performance anxiety (outcome) before solos and small ensembles (large = 0) by 10.6%\n(Bottom) Decreased the between-person variability in the effect (slope) of large ensembles (large = 1) on performance anxiety (outcome) by 39.2%\n\n\n\n\n\nRandom Intercepts (with 2 covariates)\n\n1 - Level 1; 1 - Level 2\nInstead of assuming that the large ensemble (large) effects, after accounting for instrument played (orch), vary by individual, we are assuming that large ensemble effect is fixed across subjects.\nSpecification\n\nLevel 1: \\(Y_{ij} = a_i + b_i \\text{large}_{ij} + \\epsilon_{ij}\\)\nLevel 2\n\\[\n\\begin{align}\n&a_i = \\alpha_0 + \\alpha_1 \\text{orch}_i + u_i\\\\\n&b_i = \\beta_0 + \\beta_1 \\text{orch}_i \\\\\n&\\text{where}\\: \\epsilon \\sim \\mathcal{N}(0, \\sigma^2)\\: \\text{and} \\: u_i \\sim \\mathcal{N}(0, \\sigma^2_u)\n\\end{align}\n\\]\n\nOnly difference is the random effect/error term, \\(v_i\\), isnâ€™t specified\n\nComposite\n\nNot given, but probably very similar to the previous model just without \\(v_i\\)\n\n\nModel\nmodel.c2 &lt;- lmer(na ~ orch + large + orch:large + (1|id), data = music)\n##Â  GroupsÂ  NameÂ  Â  Â  Â  Variance Std.Dev.\n##Â  idÂ  Â  Â  (Intercept)Â  5.13Â  Â  2.27Â  Â \n##Â  ResidualÂ  Â  Â  Â  Â  Â  21.88Â  Â  4.68\n##Â  Number of Level Two groups =Â  37\n##Â  Â  Â  Â  Â  Â  Estimate Std. Error t value\n## (Intercept)Â  15.9026Â  Â  0.6187Â  25.703\n## orchÂ  Â  Â  Â  Â  1.7100Â  Â  0.9131Â   1.873\n## largeÂ  Â  Â  Â  -0.8918Â  Â  0.8415Â  -1.060\n## orch:largeÂ   -1.4650Â  Â  1.0880Â  -1.347\nInterpretation\n\nSame as previous model except thereâ€™s no between-units variation estimate for the slope (see previous model and multi-level model section)\nEstimates are similar to the previous modelâ€™s. Largest difference seems to be the between-unit variation for the Intercept (5.66 vs 5.13)\n\nComparison\nÂ  Â  Â  Â  dfÂ  AIC\nmodel.cÂ  8 3003\nmodel.c2Â 6 2999\nÂ  Â  Â  Â  dfÂ  BIC\nmodel.cÂ  8 3037\nmodel.c2Â 6 3025\n\nModel C is the previous model and Model C2 is this model\nBoth criterion pick this model to predict best\nThe more complex model (e.g.Â varying intercepts and varying slopes) doesnâ€™t always perform best\n\n\n\n\nRandom Slopes and Intercepts Model (with 3 covariates)\n\n1 - Level 1; 2 - Level 2\nMPQnem has been centered (mean = 31.63)\n\nOtherwise some interpretations would involve MPQnem = 0 when the minimum score is 11 which would make the interpretation nonsensical\n\nSpecification\n\nLevel 1: \\(Y_{ij} = a_i + b_i \\text{large}_{ij} + \\epsilon_{ij}\\)\nLevel 2\n\\[\n\\begin{align}\na_i = \\alpha_0 + \\alpha_1 \\text{orch}_i + \\alpha_2 \\text{MPQnem}_i + u_i \\\\\nb_i = \\beta_0 + \\beta_1 \\text{orch}_i + \\beta_2 \\text{MPQnem}_i + v_i\n\\end{align}\n\\]\nComposite\n\\[\nY_{ij} = [\\alpha_0 + \\alpha_1 \\text{orch}_i + \\alpha_2 \\text{mpqnem}_i + \\beta_0 \\text{large}_{ij} + \\beta_1 \\text{orch}_i \\text{large}_{ij} + \\beta_2 \\text{mpqnem}_i \\text{large}_{ij}] + [u_i + v_i \\text{large}_{ij} + \\epsilon_{ij}]\n\\]\n\nModel\nmodel.d &lt;- lmer(na ~ orch + mpqnem + large +\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  orch:large + mpqnem:large +\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  (large | id),\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  data = music)\n##Â  GroupsÂ  NameÂ  Â  Â  Â  Variance Std.Dev. CorrÂ \n##Â  idÂ  Â  Â  (Intercept)Â  3.286Â  1.813Â  Â  Â  Â  Â \n##Â  Â  Â  Â  Â  largeÂ  Â  Â  Â  0.557Â  0.746Â  Â  -0.38\n##Â  ResidualÂ  Â  Â  Â  Â  Â  21.811Â  4.670\n##Â  Number of Level Two groups =Â  37\n##Â  Â  Â  Â  Â  Â  Â   Estimate Std. Error t value\n## (Intercept)Â   16.25679Â  Â  0.54756 29.6893\n## orchÂ  Â  Â  Â  Â   1.00069Â  Â  0.81713Â  1.2246\n## cmpqnemÂ  Â  Â  Â  0.14823Â  Â  0.03808Â  3.8925\n## largeÂ  Â  Â  Â   -1.23484Â  Â  0.84320 -1.4645\n## orch:largeÂ  Â  -0.94927Â  Â  1.10620 -0.8581\n## cmpqnem:large -0.03018Â  Â  0.05246 -0.5753\nInterpretation\n\nCompared to Random Slopes and Intercepts Model (with 2 covariates):\n\nDirections of the effects of instrument (orch) and performance type (large) are consistent\nEffect sizes and levels of significance are reduced because of the relative importance of the negative emotionality (mpqnem) term\n\n\\(\\alpha_0 = 16.26\\) â€” The estimated mean performance anxiety (outcome) for solos and small ensembles (large = 0) is 16.26 for keyboard players and vocalists (orch=0) with an average level of negative emotionality at baseline (mpqnem = 31.63)\n\\(\\hat \\alpha_1 = 1.00\\) â€” Orchestral instrument (orch = 1) players have an estimated mean performance anxiety (outcome) level before solos and small ensembles (large = 0) which is 1.00 point higher than keyboardists and vocalists (orch = 0), controlling for the effects of baseline negative emotionality.\n\\(\\hat \\sigma^2 = 0.15\\) â€” A one point increase in baseline negative emotionality (mpqnem = 0) is associated with an estimated 0.15 mean increase in performance anxiety (outcome) levels before solos and small ensembles (large = 0), after controlling for instrument (orch).\n\\(\\hat \\beta_0 = âˆ’1.23\\) â€” Keyboard players and vocalists (orch = 0) with an average level of baseline negative emotionality levels (mpqnem = 31.63) have an estimated mean decrease in performance anxiety level (outcome) of 1.23 points before large ensemble performances (large = 1) compared to other performance types (large = 0).\n\nSee multi-level&gt;&gt; interpretation for explainer on the interaction slope interpretations\n\n\\(\\hat \\beta_1 = âˆ’0.95\\) â€” After accounting for baseline negative emotionality (mpqnem = 0), orchestral instrument players (orch = 1) have an estimated mean performance anxiety level (outcome) before solos and small ensembles (large = 0) which is 1.00 point higher than keyboardists and vocalists (orch = 0), while the mean performance anxiety (outcome) of orchestral players (orch = 1) is only .05 points higher before large ensembles (large = 1) (a difference of .95 points).\n\nSee multi-level &gt;&gt; interpretation for explainer on the interaction slope interpretations\n\n\\(\\hat \\beta_2 = âˆ’0.03\\) â€” After accounting for instrument, a one-point increase in baseline negative emotionality is associated with an estimated 0.15 mean increase in performance anxiety levels before solos and small ensembles, but only an estimated 0.12 increase before large ensembles (a difference of .03 points).\n\nLR-Test for comparing nested models\ndrop_in_dev &lt;- anova(model.d, model.c, test = \"Chisq\")\n#&gt; Â  Â  Â  Â  nparÂ  AICÂ  BIC logLikÂ  dev Chisq DfÂ  Â  Â  pval\n#&gt; model.cÂ  Â  8 3007 3041Â  -1496 2991Â  Â  NA NAÂ  Â  Â  Â  NA\n#&gt; model.dÂ   10 2996 3039Â  -1488 2976 14.73Â  2 0.0006319\n\nMLE must be used instead of REML to estimate the parameters of each model\nmodel.d is the current and more complex model, Random Slopes and Intercepts Model (with 3 covariates)\nmodel.c is the less complex model, Random Slopes and Intercepts Model (with 2 covariates)\nProcess\n\nThe likelihood is larger (and the log-likelihood is less negative) under the larger model (Model D);\n\\(\\text{Chisq} = 14.734 = -2 \\cdot (-1488 - (-1496))\\)\nUsing dof = 2, signifying the number of additional terms in Model D, we obtain a p-value of .0006.\n\nA p-value &lt; 0.05 indicates the difference in log-likelihoods is significantly different from 0, and therefore the more complex model, model.d, fits the data better.\nNote: dropping the mpqnem:large term which has a t-stat &lt; |2| (-0.5753) produces a better model than model.d according the LR-test\n\n\n\n\nFinal Model\n\nDescription\n\nOne of other valid final models\nPerformance type categorical is no longer collapsed into the binary â€œlargeâ€ ensemble/not large and is has now been collapsed into the binary, solo, not solo\nAudience categorical has been transformed to dummies: students, juried, public with instructor as the reference category.\nVarying/random slopes for previous, students, juried, public, and solo\nmpqnem is in the solo level 2 equation, so the combined model has an interaction between the two in the fixed effects.\n\nSpecification\n\nLevel 1: \\(Y_{ij} = a_i + b_i \\text{previous}_{ij} + c_i \\text{students}_{ij} + d_i \\text{juried}_{ij} + e_i \\text{public}_{ij} + f_i \\text{solo}_{ij} + \\epsilon_{ij}\\)\nLevel 2\n\\[\n\\begin{align}\na_i &= \\alpha_0 + \\alpha_1 \\text{mpqpem}_i + \\alpha_2 \\text{mpqab}_i + \\alpha_3 \\text{orch}_i + \\alpha_4 \\text{mpqnem}_i + u_i \\\\\nb_i &= \\beta_0 + v_i \\\\\nc_i &= \\gamma_0 + w_i \\\\\nd_i &= \\delta_0 + x_i \\\\\ne_i &= \\epsilon_0 + y_i \\\\\nf_i &= \\zeta_0 + \\zeta_1 \\text{mpqnem}_i + z_i\n\\end{align}\n\\]\n\nVariance-Covariance matrix\n\\[\n\\left[\\begin{array}{cc} u_i\\\\v_i\\\\w_i\\\\x_i\\\\y_i\\\\z_i \\end{array} \\right]\n\\sim \\mathcal{N} \\left(\n\\left[\\begin{array}{cc} 0\\\\0\\\\0\\\\0\\\\0\\\\0\\end{array} \\right],\n\\begin{bmatrix}\n\\sigma_u^2 \\\\\n\\sigma_{uv} & \\sigma_v^2 \\\\\n\\sigma_{uw} & \\sigma_{vw} & \\sigma^2_w \\\\\n\\sigma_{ux} & \\sigma_{vx} & \\sigma_{wx} & \\sigma_x^2 \\\\\n\\sigma_{uy} & \\sigma_{vy} & \\sigma_{wy} & \\sigma_{xy} & \\sigma_{y}^2 \\\\\n\\sigma_{uz} & \\sigma_{vz} & \\sigma_{wz} & \\sigma_{xz} & \\sigma_{yz} & \\sigma_z^2\n\\end{bmatrix}\n\\right)  \n\\]\n6 variance terms and 15 correlation terms at Level Two, along with 1 variance term at Level One.\n\nThe number of correlation terms is equal to the number of unique pairs among Level Two random effects\n\n\n\nModel\n# Model F (One - of many - reasonable final models)\nmodel.f &lt;- lmer(na ~ previous + students + juried +Â \nÂ  Â  public + solo + mpqpem + mpqab + orch + mpqnem +Â \nÂ  Â  mpqnem:solo + (previous + students + juried +Â \nÂ  Â  public + solo | id), REML = T, data = music)\n\n##Â  GroupsÂ  NameÂ  Â  Â  Â  Variance Std.Dev. CorrÂ  Â  Â  Â \n##Â  idÂ  Â  Â  (Intercept) 14.4802Â  3.805Â  Â  Â  Â  Â  Â  Â  Â \n##Â  Â  Â  Â  Â  previousÂ  Â   0.0707Â  0.266Â  Â  -0.65Â  Â  Â \n##Â  Â  Â  Â  Â  studentsÂ  Â   8.2151Â  2.866Â  Â  -0.63Â  0.00\n##Â  Â  Â  Â  Â  juriedÂ  Â  Â  18.3177Â  4.280Â  Â  -0.64 -0.12\n##Â  Â  Â  Â  Â  publicÂ  Â  Â  12.8094Â  3.579Â  Â  -0.83Â  0.33\n##Â  Â  Â  Â  Â  soloÂ  Â  Â  Â   0.7665Â  0.876Â  Â  -0.67Â  0.47\n##Â  ResidualÂ  Â  Â  Â  Â  Â  15.2844Â  3.910Â  Â  Â \n##Â  Â  Â  Â  Â  Â  Â \n##Â  0.84Â  Â  Â  Â  Â  Â \n##Â  0.66Â  0.58Â  Â  Â \n##Â  0.49Â  0.21Â  0.90\n##Â \n##Â  Number of Level Two groups =Â  37\n##Â  Â  Â  Â  Â  Â  Estimate Std. Error t value\n## (Intercept)Â  8.36883Â  Â  1.91369Â  4.3731\n## previousÂ  Â  -0.14303Â  Â  0.06247 -2.2895\n## studentsÂ  Â   3.61115Â  Â  0.76796Â  4.7022\n## juriedÂ  Â  Â   4.07332Â  Â  1.03130Â  3.9497\n## publicÂ  Â  Â   3.06453Â  Â  0.89274Â  3.4327\n## soloÂ  Â  Â  Â   0.51647Â  Â  1.39635Â  0.3699\n## mpqpemÂ  Â  Â  -0.08312Â  Â  0.02408 -3.4524\n## mpqabÂ  Â  Â  Â  0.20377Â  Â  0.04740Â  4.2986\n## orchÂ  Â  Â  Â   1.53138Â  Â  0.58384Â  2.6230\n## mpqnemÂ  Â  Â   0.11465Â  Â  0.03591Â  3.1930\n## solo:mpqnemÂ  0.08296Â  Â  0.04158Â  1.9951\nInterpretation\n\nIn general\n\nPerformance anxiety (outcome) is higher when a musician is performing in front of students, a jury, or the general public rather than their instructor\n\nstudents, juried, and public are indicator variables created from the audience categorical variable (so that â€œInstructorâ€ is the reference level in this model)\n\nPerformance anxiety (outcome) is lower for each additional diary the musician previously filled out\n\nprevious is the number of previous diary entries filled out by that individual\n\nMusicians with lower levels of positive emotionality (mpqpem) and higher levels of absorption (mpqab) tend to experience greater performance anxiety (outcome)\nThose who play orchestral instruments experience (orch = 1) more performance anxiety than those who play keyboards or sing (orch = 0)\n\nKey terms\n\n\\(\\hat \\alpha_4 = 0.11\\) (mpqnem, fixed) â€” A one-point increase in baseline level of negative emotionality (mpqnem) is associated with an estimated 0.11 mean increase in performance anxiety (outcome) for musicians performing in an ensemble group (solo=0), after controlling for previous diary entries (previous), audience (dummy vars), positive emotionality (mpqpem), absorption (mpqab), and instrument (orch).\n\\(\\hat \\zeta_1 = 0.08\\) (solo:mpqnem, fixed) â€” When musicians play solos (solo = 1), a one-point increase in baseline level of negative emotionality (mpqnem) is associated with an estimated 0.19 mean increase in performance anxiety (outcome), 0.08 points (73%) higher than musicians playing in ensemble groups (solo = 0), controlling for the effects of previous diary entries (previous), audience (dummy vars), positive emotionality (mpqpem), absorption (mpqab), and instrument (orch).\n\nSee multi-level &gt;&gt; interpretation for explainer on the interaction slope interpretations\n\n\nAddressing the researchersâ€™ primary hypothesis:\n\nAfter controlling for all these factors, we have significant evidence that musicians with higher levels of negative emotionality (mpqpem) experience higher levels of performance anxiety (outcome), and that this association is even more pronounced (interaction) when musicians are performing solos (solo = 1) rather than as part of an ensemble group (solo = 0)."
  },
  {
    "objectID": "qmd/econometrics-mixed-effects-frequentist.html#sec-econ-me-freq-bmlrch9",
    "href": "qmd/econometrics-mixed-effects-frequentist.html#sec-econ-me-freq-bmlrch9",
    "title": "Mixed Effects",
    "section": "BMLR Chapter 9: Longitudinal Data",
    "text": "BMLR Chapter 9: Longitudinal Data\n\nBeyond Multiple Linear Regression, Chapter 9: Two-Level Longitudinal Data\nResearch Questions\n\nWhich factors most influence a schoolâ€™s performance in Minnesota Comprehensive Assessment (MCA) testing?\nHow do the average math MCA-II scores for 6th graders enrolled in charter schools differ from scores for students who attend non-charter public schools? Do these differences persist after accounting for differences in student populations?\nAre there differences in yearly improvement between charter and non-charter public schools?\n\nData Answers\n\nWithin schoolâ€”changes over time\nBetween schoolsâ€”effects of school-specific covariates (charter or non-charter, urban or rural, percent free and reduced lunch, percent special education, and percent non-white) on 2008 math scores and rate of change between 2008 and 2010.\n\nMisc\n\nMissing data is a common phenomenon in longitudinal studies"
  },
  {
    "objectID": "qmd/eda-geospatial.html",
    "href": "qmd/eda-geospatial.html",
    "title": "23Â  EDA, Geospatial",
    "section": "",
    "text": "Numeric Outcome\n\n{ggmap} Dot map\n\nExample: Does Price vary by location?\nIn your data, find the min and max latitude and longitude to specify a bounding box\n\n\nlibrary(ggmap)\nbbox &lt;- c(left = &lt;min longitude&gt;, bottom = &lt;min latitude&gt;, right = &lt;max longitude&gt;, top = &lt;max latitude&gt;)\nmap_tiles &lt;- get_stamenmap(bbox, zoom = 13)\n\n(optional) Aggregate some of the data (i.e.Â dots)\n\nagg_dat &lt;- dat %&gt;%\nÂ  Â  group_by(latitude = round(latitude, 2),\nÂ  Â  Â  Â  Â  Â  longitude = round(longitude, 2)) %&gt;%\nÂ  Â  summarize(avg_outcome = mean(numeric_outcome),\nÂ  Â  Â  Â  Â  Â  Â  n = n())\n\nscale_size_continuous adjusts the range of dot sizes. This range makes them a little smaller.\n\ntrans and labels args are for the legend (I think).\n\n\nggmap(map_tiles) +\nÂ  Â  geom_point(aes(longitude, latitude, size = n, color = avg_outcome), data = agg_dat) +\nÂ  Â  scale_color_gradient2(low = \"blue\", high = \"red\", midpoint = &lt;midpoint value of numeric_outcome&gt;,\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  trans = \"log10\", labels = dollar) +\nÂ  Â  scale_size_continuous(range = c(0.5, 4)) +\nÂ  Â  theme_map() +\nÂ  Â  labs(color = \"&lt;avg_outcome&gt;\", size = \"&lt;n&gt;\""
  },
  {
    "objectID": "qmd/eda-general.html#sec-eda-gen-",
    "href": "qmd/eda-general.html#sec-eda-gen-",
    "title": "General",
    "section": "Misc",
    "text": "Misc\n\nFirst contact with an unfamiliar database\n\nselect * from limit 50\nLook for keys/fields to connect tables\nMake running list of Qâ€™s, try to answer them by poking around first\nFind team/code responsible for DB and ask for time to review questions â€“ communication can be a superpower here!\n\nUse domain knowledge to assess peculier relationships\n\nExample: Is there a nonlinear relationship between Driver hours and Incentive Level\n\n\nCommon sense says if we raise payment bonuses, we should see more drivers want to work more hours.\nReason behind the relationship shown in this chart is omitted variables: weather and holiday.\n\nIncentives stop having an effect on drivers because they hate going out in shitty weather and want to stay home with their family on the holidays."
  },
  {
    "objectID": "qmd/eda-general.html#sec-eda-gen-bascln",
    "href": "qmd/eda-general.html#sec-eda-gen-bascln",
    "title": "General",
    "section": "Basic Cleaning",
    "text": "Basic Cleaning\n\nTidy column names\nShrink long column names to something reasonable enough for an axis label\nMake sure continuous variables arenâ€™t initially coded as categoricals and vice versa\nMake note of columns with several values per cell and willÂ need to be separated into multiple columns (e.g.Â addresses)\nFind duplicate rows\n\nSee\n\nCode, Snippets &gt;&gt; Cleaning\nSQL &gt;&gt; Processing Expressions &gt;&gt; Duplicates\nPython, Pandas &gt;&gt; Distinct\n\nThese can cause data leakage if the same row is in the test and train sets.\n\nMake a note to remove columns that the target is a function of\n\ne.g.Â Donâ€™t use monthly salary to predict yearly salary\n\nRemove columns that occur after the target event\n\ne.g.Â Using info occurring in or after a trial to predict something pre-trial\n\nYou wonâ€™t have this info beforehand when you make your prediction\n\n\nOrdinal categorical\n\nReorder by a number in the text (parse_number)\nmutate(income_category = fct_reorder(income_category, parse_number(income_category)),\nÂ  Â  Â  # manually fix category that is still out of order\nÂ  Â  Â  # moves \"Less thatn $40K\" to first place in the levels\nÂ  Â  Â  income_category = fct_relevel(income_category, \"Less than $40K\"))"
  },
  {
    "objectID": "qmd/eda-general.html#sec-eda-gen-pkgs",
    "href": "qmd/eda-general.html#sec-eda-gen-pkgs",
    "title": "General",
    "section": "Packages",
    "text": "Packages\n\n{skimr::skim} - overall summary, check completion percentage for vars with too many NAs\n{dataexplorer}\ncreate_report(airquality)\ncreate_report(diamonds, y = \"price\") # specify response variable\n\nRuns multiple functions to analyze dataset\n\n{dataxray} - table with interactive distributions, summary stats, missingness, proportions. (dancho article/video)\n{trelliscopejs} - quick, interactive, facetted pairwise plots\n{visdat} has decent visualization for group comparison, missingness, correlation, etc.\n{Hmisc::describe}\nsparkline::sparkline(0)\ndes &lt;- describe(d)\nplot(des) # maybe for displaying in Viewer pane\nprint(des, 'both') # maybe just a console df of the numbers\nmaketabs(print(des, 'both'), wide=TRUE) # for Quarto\n\nâ€œbothâ€ says display â€œcontinuousâ€ and â€œcategoricalâ€\nâ€œcontinuousâ€\n\nâ€œcategoricalâ€\n\nColumns (from Hmisc Ref Manual)\n\nâ€œInfoâ€: Info which is a relative information measure using the relative efficiency of a proportional odds/Wilcoxon test on the variable relative to the same test on a variable that has no ties. Info is related to how continuous the variable is, and ties are less harmful the more untied values there are. The formula for Info is one minus the sum of the cubes of relative frequencies of values divided by one minus the square of the reciprocal of the sample size. The lowest information comes from a variable having only one distinct value following by a highly skewed binary variable. Info is reported to two decimal places.\nâ€œMeanâ€ and â€œSumâ€ (Binary): , the sum (number of 1â€™s) and mean (proportion of 1â€™s)"
  },
  {
    "objectID": "qmd/eda-general.html#sec-eda-gen-miss",
    "href": "qmd/eda-general.html#sec-eda-gen-miss",
    "title": "General",
    "section": "Missingness",
    "text": "Missingness\n\nAlso see\n\nMissingness\nModel Building, tidymodels &gt;&gt; Recipe &gt;&gt; Imputation\n\nPackages\n\n{naniar} - tidy ways to summarize, visualize, and manipulate missing data with minimal deviations from the workflows in ggplot2 and tidy data\n{qreport} - Harrell package\n\nA few of the charts arenâ€™t intuitive and donâ€™t have good documentation in terms of explaining how to interpret them.\nFits an ordinal logistic regression model to describe which types of subjects (based on variables with no NAs) tend to have more variables missing.\nHierarchically clusters variables that have similar observations missing\nSee naclus docs, RMS Ch.19.1, R Workflow Ch.2.7 (interprets the clustering), Ch.6 (interpretes the ordinal regression) (possibly more use cases in that ebook)\n\n\nQuestions\n\nWhich features contain missing values?\nWhat proportion of records for each feature comprises missing data?\nIs the missing data missing at random (MAR) or missing not at random (MNAR) (i.e.Â informative)?\nAre the features with missing values correlated with other features?\n\nCategoricals for binary classification\n\ntrain_raw %&gt;%\nÂ  select(\nÂ  Â  damaged, precipitation, visibility, engine_type,\nÂ  Â  flight_impact, flight_phase, species_quantity\nÂ  ) %&gt;%\nÂ  pivot_longer(precipitation:species_quantity) %&gt;%\nÂ  ggplot(aes(y = value, fill = damaged)) +\nÂ  geom_bar(position = \"fill\") +\nÂ  facet_wrap(vars(name), scales = \"free\", ncol = 2) +\nÂ  labs(x = NULL, y = NULL, fill = NULL)\n\nThe NAs (top row in each facet) arenâ€™t 50/50 between the two levels of the target. The target is imbalanced and the NAs seem to be predictive of â€œno damage,â€ so they arenâ€™t random.\nSince these NAs look predictive, you can turn them into a category by using step_unknown in the preprocessing recipe."
  },
  {
    "objectID": "qmd/eda-general.html#sec-eda-gen-wtp",
    "href": "qmd/eda-general.html#sec-eda-gen-wtp",
    "title": "General",
    "section": "What is the problem?",
    "text": "What is the problem?\n\nGeneral\n\nGeneral Questions\n\nâ€œWhat variables are relevant to the problem Iâ€™m trying to solve?â€\nâ€œWhat are the key components of this data set?â€\nâ€œCan this data be categorized?â€\nâ€œIs this analysis result out of the ordinary?â€\nâ€œWhat are the key relationships?â€\nâ€œIs this the best way this company could be carrying out this task?â€\nâ€œWhat will happen under new conditions?â€\nâ€œWhat factors are best used to determine or predict this eventuality?â€\n\nBreak down the problem into parts and focus on those during EDA\n\nAlso see Decison Intelligence &gt;&gt; Mental Models for details on methods to break down components\nExample: Why are sales down?\n\nHow are sales calculated?\n\ne.g.Â Total Sales = # of Orders * Average Order Value\n\nBreakdown # of orders and average order value\n\nnumber of orders = number of walk-ins * % conversion\n\nHas walk-ins or conversion declined?\n\nAverage Order Value\n\nBin avg order value by quantiles, plot and facet or group by binned groups. Is one group more responsible for the decline than others?\n\n\nIs there regional or store or brand variability? (grouping variables)\n\n\nDrill down into each component until the data doesnâ€™t allow you to go any farther.\nSegment data by groups\n\nColor or facet by cat vars\nPay attention to counts of each category (may need to collapse categories)\nCommon segments in product analytics\n\nFree vs Paid users\nDevice Type (desktop web vs mobile web vs native app)\nTraffic Source (people coming from search engines, paid marketing, people directly typing in your companyâ€™s URL into their browser, etc.)\nDay of the Week.\n\n\n\n\n\nTROPICS framework\n\nMisc\n\nFor analyzing changes in key performance metrics\nFrom https://towardsdatascience.com/answering-the-data-science-metric-change-interview-question-the-ultimate-guide-5e18d62d0dc6\nComponents: Time, Region, Other Internal Products, Platform, Industry and Competitors, Cannibalization, Segmentation\n\nTime\n\nWhat to explore\n\nHow has our performance been trending over the last few weeks (or months)?\n\nExample: If we saw a 10% increase in the last week, was the percentage change in the weeks before also 10%? In which case the 10% may actually be pretty normal? Or was the change lower? Higher?\n\nIs this change seasonal? Do we see the same spike around this time each year?\n\nExample: Does WhatsApp see a spike in messages sent during the holiday season?\n\nWas the change sudden or gradual? Did we see a sudden spike or drop overnight? Or has the metric gradually been moving in this direction over time?\n\nExample: If product usage jumps by 50% overnight could there be a bug in our logging systems?\n\nAre there specific times during the day or week where this change is more pronounced?\n\nSolution examples\n\nIf the change is seasonal then there may not necessarily be anything you need to â€˜solveâ€™ for. But, you can leverage this to your advantage.\n\nExample: Amazon sales may jump up on Black Friday so they would want to make sure they have the proper infrastructure in place so the site doesnâ€™t crash. They may also see if there are certain types of products that are popular purchases and increase their inventory accordingly.\n\nIf there is a sudden decline, there may be a bug in the logging or a new feature or update recently launched thatâ€™s creating problems that you may need to roll back.\nIf thereâ€™s a gradual decline, it may indicate a change in user behavior.\n\nExample: If the time spent listening to music is declining because people prefer to listen to podcasts then Spotify may want to focus more of their content inventory on podcasts.\n\n\n\nRegion\n\nWhat to explore\n\nIs this change concentrated in a specific region or do we see a similar change across the board?\n\nSolution examples\n\nThere may be newly enforced regulations in countries that are affecting your product metrics. You would need to do further research to assess the impacts of these regulations and potential workarounds.\n\nExample: Uber was temporarily banned in London in 2019 for repeated safety failures which resulted in a series of lawsuits and court cases.\n\nPopular local events may also be potential explanations. While these may not be areas to â€˜solveâ€™ for they can be opportunities to take advantage of.\n\nExample: Coachella season means a jump in the number of Airbnb bookings in Southern California that are capitalized on by surge pricing.\n\n\n\nOther Internal Products\n\nWhat to explore\n\nIs this change specific to one product or is it company-wide? How does this metric vary across our other product offerings?\n\nExample: If the Fundraising feature on Facebook is seeing increased usage, is the swipe up to donate feature on Instagram (which Facebook owns) also seeing a similar uptick?\n\nAre there other metrics that have also changed in addition to the one in question?\n\nExample: If the time spent on Uber is going down, is the number of cancellations by drivers also declining (implying people are spending less time on the app because theyâ€™re having a more reliable experience)?\n\n\nSolution examples\n\nIf there is a metric change across our other features and products, itâ€™s likely a larger problem we should address with multiple teams and may need a Public Relations consultant.\n\nExample: Elon + Twitter.\n\n\n\nPlatform\n\nWhat to explore\n\nMobile vs Desktop?\nMac vs Windows?\nAndroid vs iOS?\n\nSolution examples\n\nIf there was a positive change in our metric on a specific platform (e.g.Â iOS) and coincides with an (iOS) update we released, we would want to do a retrospective to determine what about that update was favorable so we can double down on it. Alternatively, if the metric change was negative, we may want to reconsider and even roll back the update.\nIf the change was due to a change in the platform experience (e.g.Â app store placement, ratings) we may want to seek advice from our marketing team since this is a top of the funnel problem\nIf users are showing astrong preference for a specific platform, we want to make sure that the experience of the preferred platform is up to par. We also need to make sure our platform-specific monetization strategies are switching to follow the trend.\n\nExample: Facebookâ€™s ad model was initially tied to the desktop app only and had to be expanded as mobile became the platform of preference.\n\n\n\nIndustry & Competitors\n\nWhat to explore\n\nWhen our decline began, was there a new competitor or category that emerged?\n\nExample: Did the number of users listening to Apple podcasts go down when Clubhouse came on to the scene?\n\nHave competitors changed their offering lately?\nIs the category as a whole declining?\n\nSolution examples\n\nIf the category is shifting as a whole, we should begin looking at larger-scale changes to the app.\n\nExample: What Kodak should have done.\n\nIf thereâ€™s a new competitor taking our market share, we can begin with reactivation campaigns on churned users. We may also want to conduct user research to understand the gap between our offering and those of our competitors\n\n\nCannibalization\n\nWhat to explore\n\nAre other products or features in our offering experiencing growth in the face of our decline or vice versa?\nHave we released a new feature that is drawing users away from our old features? If so, can we fully attribute the release of the new feature with the decline in the metric of our feature in question?\n\nExample: When Facebook released reactions, did the number of comments on a post go down because people found it easier to press a react button instead of writing a comment?\n\n\nSolution examples\n\nCannibalization may not necessarily be a bad thing. We need to determine whether this shift in user interest across our features is favorable by determining whether the new features align better with the goals of the business.\nCannibalization may also be an indication of but it is indicative of a change in user behavior. In which case we may want to consider if perhaps our core metrics need to change as user behaviors change.\n\nExample: If users care more about watching Instagram stories than engaging with the Instagram feed we may want to optimize for retention (because the ephemeral nature of stories is more likely to motivate users to keep coming back to the platform) instead of time spent on the app.\n\nWe can also look at ways to bridge the two features together to create a more unified platform.\n\n\nSegmentation\n\nWhat to explore\n\nHow does this metric vary byÂ user type:\n\nAge, sex, education\nPower users versus casual users * New users versus existing users\n\nHow does this metric vary by different attributes of the product:\n\nExample: If the time spent watching YouTube videos is going down, is it across longer videos or shorter clips? Is it only for DIY videos or interview tutorial content? Is the same number of people that started watching a video the same but a large chunk of them stop watching it halfway through?\n\n\nSolution examples\n\nIf the metric varies between new and existing users then maybe there is a overcrowding effect.\n\nExample: Reddit forums could hit a critical mass where new users feel lost and less likely to engage than existing users resulting in a drop in engagements per user\n\nIf users are dropping off at certain parts of the funnel then maybe the experience at that funnel step is broken.\n\nExample: While the same number of people are starting carts on Amazon there may be a drop in purchases if the payment verification system isnâ€™t working."
  },
  {
    "objectID": "qmd/eda-general.html#sec-eda-gen-out",
    "href": "qmd/eda-general.html#sec-eda-gen-out",
    "title": "General",
    "section": "Outliers",
    "text": "Outliers\n\nAlso see Outliers\nAbnormalities due to likely data entry errors\n\nExample: store == â€œopenâ€ and sales == 0 or store == â€œclosedâ€ and sales &gt; 0\n\nPotential solâ€™n: replace 0â€™s (open) with mean sales and sales &gt;0 (closed) with 0s\n\n\nExtreme counts in charts when grouping by a cat var\n\nWhy is one categoryâ€™s count so low or so high?\n\nMay need subject matter expert\n\nWhat can be done to increase or decrease that categoryâ€™s count?\n\nFor prediction, experiment with keeping or removing outliers while fitting baseline models"
  },
  {
    "objectID": "qmd/eda-general.html#sec-eda-gen-grpcal",
    "href": "qmd/eda-general.html#sec-eda-gen-grpcal",
    "title": "General",
    "section": "Group Calculations",
    "text": "Group Calculations\n\nAlso see Feature Engineering, General &gt;&gt; Domain Specific\nVariance of Value by Group\n\nExample: how sales vary between store types over a year\nimportant to standardize the value by group\n\ngroup_by(group), mutate(sales = scale(sales))\n\nWhich vary wildly and which are more stable\n\nRates by Group\n\nExample: sales($) per customer\n\ngroup_by(group), mutate(sales_per_cust = sum(sales)/sum(customers)\n\n\nAvg by Group(s)\ndat %&gt;%\nselect(cat1, cat2, num) %&gt;%\ngroup_by(cat1, cat2) %&gt;%\nsummarize(freq = n(),\nÂ  Â  Â  Â  Â  avg_cont = mean(num))"
  },
  {
    "objectID": "qmd/eda-general.html#sec-eda-gen-cont",
    "href": "qmd/eda-general.html#sec-eda-gen-cont",
    "title": "General",
    "section": "Continuous Variables",
    "text": "Continuous Variables\n\nDoes the variable have a wide range. (i.e.Â values across multiple magnitudes: 101 and 102 and â€¦ etc.)\n\nIf so, log the variable\n\nHistogram - Check shape of distribution\nggplot(aes(var)) +\nÂ  Â  geom_histogram()\n\nLooking at skew. Is it roughlyÂ normal?\nDoes filter(another_var &gt; certain_value (see below) help it look more normal?\nIs it multi-modal\n\nSee Regression, Other &gt;&gt; Multi-Modal(visuals, tests, modelling, etc.)\n{{gghdr}} - Visualization of Highest Density Regions in ggplot2\nInteractions &gt;&gt; Outcome: Categorical &gt;&gt; Binary Outcome (pct_event) vs Discrete by Discrete (or binary in this case)\n\nIs the variable highly skewed\n\nIf so, try:\n\nChanging units (min to hr),\nfilter(some_var &gt; some_value)\nsome combination of the above make more normal?\n\nNormality among predictors isnâ€™t necessary, but I think it improves fit or prediction somewhat\n\nlog transformation may help some if the skew isnâ€™t too extreme\n\n\n\nQ-Q plot to check fit against various distributions\n\n{ggplot}\nggplot(data)+\nÂ  Â  stat_qq(aes(sample = log_profit_rug_business))+\nÂ  Â  stat_qq_line(aes(sample = log_profit_rug_business))+\nÂ  Â  labs(title = 'log(profit) Normal QQ')\n\nA plot of the sample (or observed) quantiles of the given data against the theoretical (or expected) quantiles.\nSee article for the math and manual code\nstat_qq, stat_qq_line default distributions are Normal\nggplot::stat_qq docs have some good examples on how to use q-q plots to test your data against different distributions using MASS::fitdistr to get the distributional parameter estimates. Available distributions: â€œbetaâ€, â€œcauchyâ€, â€œchi-squaredâ€, â€œexponentialâ€, â€œgammaâ€, â€œgeometricâ€, â€œlog-normalâ€, â€œlognormalâ€, â€œlogisticâ€, â€œnegative binomialâ€, â€œnormalâ€, â€œPoissonâ€, â€œtâ€ and â€œweibullâ€\n\n{dataexplorer}\n## View quantile-quantile plot of all continuous variables\nplot_qq(diamonds)\n\n## View quantile-quantile plot of all continuous variables by feature `cut`\nplot_qq(diamonds, by = \"cut\") \nSkewed Variables\n\nx &lt;- list()\nn &lt;- 300\nx[[1]] &lt;- rnorm(n)\nx[[2]] &lt;- exp(rnorm(n))\nx[[3]] &lt;- -exp(rnorm(n))\n\npar(mfrow = c(2,3), bty = \"l\", family = \"Roboto\")\n\nqqnorm(x[[1]], main = \"Normal\")\nqqnorm(x[[2]], main = \"Right-skewed\")\nqqnorm(x[[3]], main = \"Left-skewed\")\nlapply(x, function(x){plot(density(x), main = \"\")})\nGood fits\n\nnormal distribution\n\nBad fits\n\nUniform data tested against a normal distibution\n\nUniform data tested against an exponential distribution\n\n\n\n\nIs the mean/median above or below any important threshold?\n\ne.g.Â CDC considers a BMI &gt; 30 as obese. Health Insurance charges rise sharply at this threshold\n\nIs there an important threshold value?\n\n1 value â€“&gt; split into a binary\nMultiple values â€“&gt; Multinomial\n\nExamples\n\nBinary\n\nWhether a user spent more than $50 or didnâ€™t (See Charts &gt;&gt; Categorical Predictors vs Outcome)\nIf user had activity on the weekend or not\n\nMultinomial\n\nTimestamp to morning/afternoon/ night,\nOrder values into buckets of $10â€“20, $20â€“30, $30+\n\n\n\nEmpirical Cumulative Density function (ecdf)\n\nggplot(aes(x = numeric_var, color = cat) +\nÂ  Â  stat_ecdf()\n\nShows the percentage of sample (y-axis) that are below a numeric_var value (x-axis)\n{sfsmisc::ecdf.ksCI} - plots the ecdf and 95% CIs (see Harrell for details of the CI calculation)\nCan view alongside a table of group means to see if the different percentiles differ from the story of just looking at the mean.\ndata %&gt;%\nÂ  Â  group_by(categorical_var) %&gt;%\nÂ  Â  summarize(mean(numeric_var))"
  },
  {
    "objectID": "qmd/eda-general.html#sec-eda-gen-cat",
    "href": "qmd/eda-general.html#sec-eda-gen-cat",
    "title": "General",
    "section": "Categorical/Discrete Variables",
    "text": "Categorical/Discrete Variables\n\nCount number of rows per category level (or use skimr or DataExplorer)\ntbl %&gt;% count(cat_var, sort = True)\nLooking for how skewed data might be (only a few categories have most of the obs)\nIf levels are imbalanced, consider: initial_split(data, strata = imbalanced_var)\nFor cat vars with levels with too few counts, consider lumping together\n\nLevels with too few data will have large uncertainties about the effect and the bloated std.devs can cause some models to throw errors\n\nCount NAs (or use skimr or DataExplorer)\ntbl %&gt;%\nÂ  map_df(~ sum(is.na(.))) %&gt;%\nÂ  gather(key = \"feature\", value = \"missing_count\") %&gt;%\nÂ  arrange(desc(missing_count))\nVars with too many NAs, may need to be dropped or imputed\n\nSome models donâ€™t handle NAs\n\nIf the number of NAs is within tolerance and you decide to impute, you need to find out what kind of â€œmissingnessâ€ you have before you choose the imputation method. Some cause issues with certain types of missingness. (e.g.Â mean and missing-not-at-random (MNAR))\nYear variable\ndata |&gt;\nÂ  Â  count(year) |&gt;\nÂ  Â  arrange(desc(year)) |&gt;\nÂ  Â  ggplot(aes(year, n)) +\nÂ  Â  geom_line()\n\nLooking for skew.\nIs data older orÂ more recent?\n\nFree Text Sometimes these columns are just metadata (a url, product description, etc.), but other times they could have valuable information (e.g.Â customer feedback). If a column seems like it contains valuable information for your prediction task, you generate features from it text length, appearance/frequency of certain keywords, etc.\n\nTokenize\n\nSee below code for â€œFacetted bar by variable with counts of the valuesâ€ and the use of separate_rows to manually tokenize more useful when the columns donâ€™t have stopwords\n\n\nVisualize value counts for multiple variables\n\nFacetted bar by variable with counts of the values\n\ncategorical_variables &lt;- board_games %&gt;%\nÂ  Â  Â  # select all cat vars\nÂ  Â  Â  select(game_id, name, family, category, artist, designer, mechanic) %&gt;%\nÂ  Â  Â  # \"type\" receives all colnames; \"value\" receives their values\nÂ  Â  Â  gather(type, value, -game_id, -name) %&gt;%\nÂ  Â  Â  filter(!is.na(value)) %&gt;%\nÂ  Â  Â  # Some values of vars are free text separated by commas; code makes each value into a separate row\nÂ  Â  Â  separate_rows(value, sep = \",\") %&gt;%\nÂ  Â  Â  arrange(game_id)\ncategorical_counts &lt;- categorical_variables %&gt;%\nÂ  Â  Â  count(type, value, sort = TRUE)\n\ncategorical_counts %&gt;%\nÂ  Â  Â  # type is gathered colnames of the variables\nÂ  Â  Â  group_by(type) %&gt;%\nÂ  Â  Â  # high cardinality variables, so only show top 10\nÂ  Â  Â  top_n(10, n) %&gt;%\nÂ  Â  Â  ungroup() %&gt;%\nÂ  Â  Â  mutate(value = fct_reorder(value, n)) %&gt;%\nÂ  Â  Â  ggplot(aes(value, n, fill = type)) +\nÂ  Â  Â  geom_col(show.legend = FALSE) +\nÂ  Â  Â  facet_wrap(~ type, scales = \"free_y\") +\nÂ  Â  Â  coord_flip() +\nÂ  Â  Â  labs(title = \"Most common categories\")\n\nâ€œtypeâ€ has the names of the variables, â€œvalueâ€ has the levels of the variable"
  },
  {
    "objectID": "qmd/eda-general.html#sec-eda-gen-corr",
    "href": "qmd/eda-general.html#sec-eda-gen-corr",
    "title": "General",
    "section": "Correlation/Association",
    "text": "Correlation/Association\n\nMisc\n\nAlso see\n\nAssociation, General\nNotebook &gt;&gt; Statistical Inference &gt;&gt; Correlation\nInteractions &gt;&gt; Outcome: numeric &gt;&gt; Correlation Heatmaps\n\n{correlationfunnel} - Danchoâ€™s package; bins numerics, then dummies all character and binned numerics, then runs a pearson correlation vs the outcome variable. Surprisingly itâ€™s useful to use Pearson correlations for binary variables as long as you have a mix of 1s and 0s in each variable. (Cross-Validated post)\nchurn_df %&gt;%\nÂ  Â  binarize() %&gt;%\nÂ  Â  correlate(&lt;outcome_var&gt;) %&gt;%\nÂ  Â  plot_correlation_funnel()\n\ncorrelate returns a sorted? tibble in case you donâ€™t want the plot\nThe funnel plot is a way of combining and ranking all the correlation plots into a less eye-taxing visual.\nUses stats::cor for calculation so you can pass args to it and but changing the method (e.g.Â method = c(\"pearson\", \"kendall\", \"spearman\") ) wonâ€™t matter, since pearson and spearman (and probably kendall) will be identical for binary variables.\n\nFor binary vs.Â binary, also see Association, General &gt;&gt; Discrete &gt;&gt; Binary Similarity Measures and Cramerâ€™s V\n\nPairwise plots for patterns\n\nOutcome vs Predictor\nPredictor vs Predictor\n\nInteractions\nMulticollinearity\n\nCorrelation/Association scores for linear relationships\nHistograms for variations between categories\nExample: {{ggforce}}\n\nggplot(palmerpenguins::penguins, aes(x = .panel_x, y = .panel_y)) +\n  geom_point(aes(color = species), alpha = .5) +\n  geom_smooth(aes(color = species), method = \"lm\") +\n  ggforce::geom_autodensity(aes(color = species, fill = after_scale(color)), alpha = .7) +\n  scale_color_brewer(palette = \"Set2\", name = NULL) +\n  ggforce::facet_matrix(vars(names), layer.lower = 2, layer.diag = 3)\n\nLinear\n\n{greybox} for testing correlation between different types of variables\n\nMulticollinearity\n\nVIF (performance::check_collinearity(fit) or greybox::determ or vif(fit))\nUse PCA â€” if only a few (depends on the number of variables) pc explain all or almost all of the variation, then you could have a multicollinearity problem\n\nNonlinear\n\nScatterplots for non-linear patterns,\nCorrelation metrics\nAlso see General Additive Models &gt;&gt; Diagnostics for a method of determining a nonlinear relationship for either continuous or categorical outcomes.\n\nCategorical\n\n2-level x 2-level: Cramerâ€™s V\n2-level or multi-level x multi-level\n\nChi-square or exact tests\n\nLevels vs Levels correlation\n\nMultiple Correspondence Analysis (MCA) (see bkmks &gt;&gt; Features &gt;&gt; Reduction)\n\nBinary outcome vs Numeric predictors\n# numeric vars should be in a long tbl. Use pivot longer to make two columns (e.g. metric (var names) value (value))\n\nnumeric_gathered %&gt;%\nÂ  group_by(metric) %&gt;%\nÂ  # rain_tomorrow is the outcome; event_level says which factor level is the event your measuring\nÂ  roc_auc(rain_tomorrow, value, event_level = \"second\") %&gt;%\nÂ  arrange(desc(.estimate)) %&gt;%\nÂ  mutate(metric = fct_reorder(metric, .estimate)) %&gt;%\nÂ  ggplot(aes(.estimate, metric)) +\nÂ  geom_point() +\nÂ  geom_vline(xintercept = .5) +\nÂ  labs(x = \"AUC in positive direction\",\nÂ  Â  Â  title = \"How predictive is each linear predictor by itself?\",\nÂ  Â  Â  subtitle = \".5 is not predictive at all; &lt;.5 means negatively associated with rain, &gt;.5 means positively associated\")\n\n.5 is not predictive at all; &lt;.5 means negatively associated with rain, &gt;.5 means positively associated\n\n\nOrdinal\n\nPolychoric"
  },
  {
    "objectID": "qmd/eda-general.html#sec-eda-gen-contout",
    "href": "qmd/eda-general.html#sec-eda-gen-contout",
    "title": "General",
    "section": "Continuous Predictor vs Outcome",
    "text": "Continuous Predictor vs Outcome\n\nMisc\n\nIf the numeric-numeric relation isnâ€™t linear, then the model will be misspecified: an influential variable may be overlooked or the assumption of linearity may produce a model that fails in important ways to represent the relationship.\nAlso see General Additive Models &gt;&gt; Diagnostics for a method of determining a nonlinear relationship for either continuous or categorical outcomes.\n\n\n\nContinuous Outcome\n\nScagnostics (paper) - metrics to examine numeric vs numeric relationships\n\n{scagnostics}\nScagnostics describe various measures of interest for pairs of variables, based on their appearance on a scatterplot. They are useful tool for discovering interesting or unusual scatterplots from a scatterplot matrix, without having to look at every individual plot\nMetrics: Outlying, Skewed, Clumpy, Sparse, Striated, Convex, Skinny, Stringy, Monotonic\n\nâ€œStraightâ€ (paper) seems to have been swapped for â€œSparseâ€ (package)\n\nPotential use cases\n\nFinding linear/nonlinear relationships\nClumping or clustered patterns could indicate an interaction with a categorical variable\n\nScore Guide\n\n\nHigh value: Red\nLow value: Blue\nCouldnâ€™t find the ranges of these metrics in the paper or the package docs\nShows how scatterplot patterns correspond to metric values\n\n\n\n\n\nCategorical Outcome\n\nFor binary outcome, look for variation between numeric variables and each outcome level\n\n# numeric vars should be in a long tbl.\n# Use pivot longer to make two columns (e.g. metric (var names) value (value)) with the binary outcome (e.g rain_tomorrow) as a separate column\nnumeric_gathered %&gt;%\nÂ  ggplot(aes(value, fill = rain_tomorrow)) +\nÂ  geom_density(alpha = 0.5) +\nÂ  facet_wrap(~ metric, scales = \"free\")\n# + scale_x_log10()\n\nSeparation between the two densities would indicate predictive value.\nIf one of colored density is further to the right than the other then the interpretation would be:\n\nHigher values of metric result in a greater probability of &lt;outcome category of the right-most density&gt;\n\nNormalize the x-axis with rank_percentile(value)\n\nnumeric_gathered %&gt;%\nÂ  Â  mutate(rank = percent_rank(value)) %&gt;%\nÂ  Â  ggplot(aes(rank, fill = churned)) +Â \nÂ  Â  Â  geom_density(alpha = 0.5) +Â \nÂ  Â  Â  facet_wrap(~ metric, scales = \"free\")\n\nNot sure why youâ€™d do this unless there was a reason to compare the separation of densities (i.e.Â strength of association with outcome) between the predictors.\n\n\nEstimated AUC for binary outcome ~ numeric predictor\nnumeric_gathered &lt;- train %&gt;%\nÂ  mutate(rainfall = log2(rainfall + 1)) %&gt;%\nÂ  gather(metric, value, min_temp, max_temp, rainfall, contains(\"speed\"), contains(\"humidity\"), contains(\"pressure\"), contains(\"cloud\"),Â  Â  Â  Â  contains(\"temp\"))\n\nnumeric_gathered %&gt;%\nÂ  group_by(metric) %&gt;%\nÂ  # \"rain_tomorrow\" is a binary factor var\nÂ  # \"second\" says the event we want the probability for is the second level of the binary factor variable\nÂ  yardstick::roc_auc(rain_tomorrow, value, event_level = \"second\") %&gt;%\nÂ  arrange(desc(.estimate)) %&gt;%\nÂ  mutate(metric = fct_reorder(metric, .estimate)) %&gt;%\nÂ  ggplot(aes(.estimate, metric)) +\nÂ  geom_point() +\nÂ  geom_vline(xintercept = .5) +\nÂ  labs(x = \"AUC in positive direction\",\nÂ  Â  Â  title = \"How predictive is each linear predictor by itself?\",\nÂ  Â  Â  subtitle = \".5 is not predictive at all; &lt;.5 means negatively associated with rain, &gt;.5 means positively associated\")"
  },
  {
    "objectID": "qmd/eda-general.html#sec-eda-gen-catout",
    "href": "qmd/eda-general.html#sec-eda-gen-catout",
    "title": "General",
    "section": "Categorical Predictor vs Outcome",
    "text": "Categorical Predictor vs Outcome\n\nContinuous Outcome\n\nBoxplot by Categorical\n\nfct_reorderÂ  says order cat_var by a num_var\n\nMake sure data is NOT grouped\n\ndata %&gt;%\nÂ  Â  mutate(cat_var = fct_reorder(cat_var, numeric_outcome)) %&gt;%\nÂ  Â  ggplot(aes(numeric_outcome, cat_var)) +\nÂ  Â  geom_boxplot()\n\nIf all the medians line up then no relationship. A slope or nonlinear shows relationship.\n\nfct_lumpÂ  can be used to create an â€œotherâ€ group.\n\ndata %&gt;%\nÂ  Â  mutate(cat_var = fct_lump(cat_var, 8),\nÂ  Â  Â  Â  Â  cat_var = fct_reorder(cat_var, numeric_outcome)) %&gt;%\nÂ  Â  ggplot(aes(numeric_outcome, cat_var)) +\nÂ  Â  geom_boxplot()\n\nUseful for cat_vars with too many levels which can muck-up a graph\nSays to keep the top 8 levels with the highest counts and put rest in â€œotherâ€.\n\nAlso takes proportions. Negative values says keep lowest.\n\n\n\nBoxplot by Categorical (Titanic5 dataset)\n\n\nY-Axis is the â€œClassâ€ categorical with 3 levels\nFor ticket price, only class 1 shows any variation\nFor Age, thereâ€™s a clear trend but also considerable overlap between classes\n\n\n\n\nCategorical Outcome\n\nHistograms of cat_vars split by response_varÂ \ndf %&gt;%\nÂ  Â  select(cat_vars) %&gt;%\nÂ  Â  pivot_longer(key, value = cat_vars, response_var) %&gt;%\nÂ  Â  ggplot(aes(value)) +\nÂ  Â  geom_bar(fill = response_var) +\nÂ  Â  facet_wrap( ~key, scales = \"free\")\n\nJust looking for variation in the levels of the cat_var given response var. More variation = more likely to be a better predictor\nEach facet will be a level of the response variable\n\nError Bar Plot\n# outcome variable is a binary for whether or not it rained on that day\ngroup_binary_prop &lt;- function(tbl) {\nÂ  Â  ret &lt;- tbl %&gt;%\nÂ  Â  Â  Â  # count of events for each category (successes)\nÂ  Â  Â  Â  summarize(n_rain = sum(rain_tomorrow == \"Rained\"),\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  # count of rows for each category (trials)\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  n = n()) %&gt;%\nÂ  Â  Â  Â  arrange(desc(n)) %&gt;%\nÂ  Â  Â  Â  ungroup() %&gt;%\nÂ  Â  Â  Â  # probability of event for each category\nÂ  Â  Â  Â  mutate(pct_rain = n_rain / n,\nÂ  Â  Â  Â  Â  Â  Â  # jeffreys interval\nÂ  Â  Â  Â  Â  Â  Â  # bayesian CI for binomial proportions\nÂ  Â  Â  Â  Â  Â  Â  low = qbeta(.025, n_rain + .5, n - n_rain + .5),\nÂ  Â  Â  Â  Â  Â  Â  high = qbeta(.975, n_rain + .5, n - n_rain + .5)) %&gt;%\nÂ  Â  Â  Â  # proportion of all events for each category\nÂ  Â  Â  Â  mutate(pct = n_rain / sum(n_rain))\nÂ  Â  Â  Â  # this was the original but this would just be proportion of the total data for each caategory\nÂ  Â  Â  Â  # mutate(pct = n / sum(n))\nÂ  Â  ret\n}\n\n# error bar plot\n# cat vs probability of event w/CIs\ntrain %&gt;%\nÂ  Â  # cat predictor\nÂ  Â  group_by(location = fct_lump(location, 50)) %&gt;%\nÂ  Â  # apply custom function\nÂ  Â  group_binary_prop() %&gt;%\nÂ  Â  mutate(location = fct_reorder(location, pct_rain)) %&gt;%\nÂ  Â  ggplot(aes(pct_rain, location)) +\nÂ  Â  geom_point(aes(size = pct)) +\nÂ  Â  geom_errorbarh(aes(xmin = low, xmax = high), height = .3) +\nÂ  Â  scale_size_continuous(labels = percent, guide = \"none\", range = c(.5, 4)) +\nÂ  Â  scale_x_continuous(labels = percent) +\nÂ  Â  labs(x = \"Probability of raining tomorrow\",\nÂ  Â  Â  y = \"\",\nÂ  Â  Â  title = \"What locations get the most/least rain?\",\nÂ  Â  Â  subtitle = \"Including 95% confidence intervals. Size of points is proportional to frequency\")\n\nBinary Outcome: Group by cat predictors and calculate proportion of event\nThis needs some tidyeval so it can generalize to other binary(?) outcome vars\n\nSimpler (uncommented) version\nsummarize_churn &lt;- function(tbl) {\nÂ  Â  tbl %&gt;%\nÂ  Â  Â  Â  summarize(n = n(),\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  n_churned = sum(churned == \"yes\"),\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  pct_churned = n_churned/n,\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  low = qbeta(.025, n_churned + .5, n - n_churned + .5),Â \nÂ  Â  Â  Â  Â  Â  Â  Â  Â  high = qbeta(.975, n_churned + .5, n - n_churned + .5)) %&gt;%\nÂ  Â  Â  Â  arrange(desc(n))\n}\n\nplot_categorical &lt;- function(tbl, categorical, ...) {\nÂ  Â  tbl %&gt;%Â  Â  Â  Â \nÂ  Â  Â  Â  ggplot(aes(pct_churned, cat_pred), ...) +\nÂ  Â  Â  Â  geom_col() +\nÂ  Â  Â  Â  geom_errorbar(aes(xmin = low, xmax = high), height = 0.2, color = red) +\nÂ  Â  Â  Â  scale_x_continuous(labels = percent) +\nÂ  Â  Â  Â  labs(x = \"% in category that churned\")\n}\n\ndata %&gt;%\nÂ  Â  group_by(cat_var) %&gt;%\nÂ  Â  summarize_churn() %&gt;%\nÂ  Â  plot_categorical(cat_var)\nBinary Outcome vs Two Binned Continuous\n\nsummarize_churn &lt;- function(tbl) {\nÂ  Â  tbl %&gt;%\nÂ  Â  Â  Â  summarize(n = n(),\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  n_churned = sum(churned == \"yes\"),\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  pct_churned = n_churned/n,\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  low = qbeta(.025, n_churned + .5, n - n_churned + .5),Â \nÂ  Â  Â  Â  Â  Â  Â  Â  Â  high = qbeta(.975, n_churned + .5, n - n_churned + .5)) %&gt;%\nÂ  Â  Â  Â  arrange(desc(n))\n}\n\ndata %&gt;%\nÂ  Â  mutate(avg_trans_amt = total_trans_amt / total_trans_ct,\nÂ  Â  Â  Â  Â  total_transactions = ifelse(total_trans_ct &gt;= 50,\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  \"&gt; 50 Transactions\",\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  \"&lt; 50 Transactions\"),\nÂ  Â  Â  Â  Â  avg_transaction = ifelse(avg_trans_amt &gt;= 50,\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  \"&gt; $50 Average\",\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  \"&lt; $50 Average\")\nÂ  Â  ) %&gt;%\nÂ  Â  group_by(total_transactions,avg_transaction) %&gt;%\nÂ  Â  summarize_churn() %&gt;%\nÂ  Â  ggplot(aes(total_transactions, avg_transaction)) +\nÂ  Â  geom_tile(aes(fill = pct_churned)) +\nÂ  Â  geom_text(aes(label = percent(pct_churned, 1))) +\nÂ  Â  scale_fill_gradient2(low = \"blue\", high = \"red\", midpoint = 0.3) +\nÂ  Â  labs(x = \"How many transactions did the customer do?\",\nÂ  Â  y = \"What was the average transaction size?\",\nÂ  Â  fill = \"% churned\",\nÂ  Â  title = \"Dividing customers into segments\")\n\nSegmentation chart\nEach customerâ€™s spend is averaged and binned (&gt; or &lt; $50)\nEach customerâ€™s transaction count is binned (&gt; or &lt; 50)\nThe df is grouped by both binned vars, so you get 4 subgroups\n\nProportions of each subgroup that falls into the event category of then binary variable (e.g.Â churn) are calculated\nLow and high quantiles for churn counts are calculated (typical calc of CIs for the proportions of binary variables)\n\nUsed to add context of whether these are high proportions, low proportions, etc."
  },
  {
    "objectID": "qmd/eda-general.html#sec-eda-gen-inter",
    "href": "qmd/eda-general.html#sec-eda-gen-inter",
    "title": "General",
    "section": "Interactions",
    "text": "Interactions\n\nMisc\n\nAlso see\n\nRegression, Interactions for details\nDiagnostics, Model Agnostic &gt;&gt; DALEX &gt;&gt; Instance Level &gt;&gt; Break-Down &gt;&gt; Example: Assume Interactions\n\nInterpretation\n\nSignificant Interactions - The lines of the graph cross or sometimes if they converge (if thereâ€™s enough data/power)\n\nThis pattern is a visual indication that the effects of one IV change as the second IV is varied.\n\nNon-Significant Interactions - Lines that are close to parallel.\n\nTypical Format: outcome_mean vs pred_var by pred_var\ndata %&gt;%Â \nÂ  group_by(pred1, pred2) %&gt;%Â \nÂ  summarize(out_mean = mean(outcome)) %&gt;%Â \nÂ  ggplot(aes(y = out_mean, x = pred1, color = pred2)+\nÂ  Â  geom_point() +\nÂ  Â  geom_line()\n\nMay also need a â€œgroup = pred2â€ in the aes function\n\n\n\n\nContinuous Outcome\n\nNumeric vs Numeric, Scatter with Smoother by a Categorical\n\nggplot(w, aes(x=age, y=price, color=factor(class))) +\nÂ  geom_point() +\nÂ  geom_smooth() +\nÂ  scale_y_continuous(trans='sqrt') +\nÂ  guides(color=guide_legend(title='Class')) +\nÂ  hlabs(age, price)\n\nContinuous outcome has been transformed so that the lower values can be more visible\nâ€œClassâ€ == 1 â¨¯ Age shows some variation but the other two classes do not seem to show much. Lookng at the scatter of red dots, Iâ€™m skeptical that variation being shown by the curve.\n\nAlthough the decent separation of the â€œClassâ€ groups may be what indicates an informative interaction\n\n\nContinuous vs Binary by Binary\n\n\nSignificant interaction effect (crossing)\n\nVariable A had no significant effect on participants in Condition B1 but caused a decline from A1 to A2 for those in Condition B2\n\n\nContinuous vs Continuous by Categorical\n\nplot_manufacturer &lt;- function(group) {\n\n  ## check if input is valid\n  if (!group %in% mpg$manufacturer) stop(\"Manufacturer not listed in the data set.\")\n\n  ggplot(mapping = aes(x = hwy, y = displ)) +\n    ## filter for manufacturer of interest\n    geom_point(data = filter(mpg, manufacturer %in% group), \n               color = \"#007cb1\", alpha = .5, size = 4) +\n    ## add shaded points for other data\n    geom_point(data = filter(mpg, !manufacturer %in% group), \n               shape = 1, color = \"grey45\", size = 2) +\n    scale_x_continuous(breaks = 2:8*5) +\n    ## add title automatically based on subset choice\n    labs(x = \"Highway gallons\", y = \"Displacement\", \n         title = group, color = NULL)\n}\n\ngroups &lt;- unique(mpg$manufacturer)\nmap(groups, ~plot_manufacturer(group = .x))\n\nThe grouping variable is the facet variable but also highlights the dots with color\nHighlighting plus using all the data in each chart helps add context with the other groups when you want to compare groups but in a low data situation.\n\nContinuous vs Continuous by Ordinal\n\nplot_scatter_lm &lt;- function(data, var1, var2, pointsize = 2, transparency = .5, color = \"\") {\n\n  ## check if inputs are valid\n  if (!is.data.frame(data)) stop(\"data needs to be a data frame.\")\n  if (!is.numeric(pull(data[var1]))) stop(\"Column var1 needs to be of type numeric, passed as string.\")\n  if (!is.numeric(pull(data[var2]))) stop(\"Column var2 needs to be of type numeric, passed as string.\")\n  if (!is.numeric(pointsize)) stop(\"pointsize needs to be of type numeric.\")\n  if (!is.numeric(transparency)) stop(\"transparency needs to be of type numeric.\")\n  if (color != \"\") { if (!color %in% names(data)) stop(\"Column color needs to be a column of data, passed as string.\") }\n\n  g &lt;- \n    ggplot(data, aes(x = !!sym(var1), y = !!sym(var2))) +\n    geom_point(aes(color = !!sym(color)), size = pointsize, alpha = transparency) +\n    geom_smooth(aes(color = !!sym(color), color = after_scale(prismatic::clr_darken(color, .3))), \n                method = \"lm\", se = FALSE) +\n    theme_minimal(base_family = \"Roboto Condensed\", base_size = 15) +\n    theme(panel.grid.minor = element_blank(),\n          legend.position = \"top\")\n\n  if (color != \"\") { \n    if (is.numeric(pull(data[color]))) {\n      g &lt;- g + scale_color_viridis_c(direction = -1, end = .85) +\n        guides(color = guide_colorbar(\n          barwidth = unit(12, \"lines\"), barheight = unit(.6, \"lines\"), title.position = \"top\"\n        ))\n    } else {\n      g &lt;- g + scale_color_brewer(palette = \"Set2\")\n    }\n  }\n\n  return(g)\n}\n\nmap2(\n  c(\"displ\", \"displ\", \"hwy\"), \n  c(\"hwy\", \"cty\", \"cty\"),\n  ~plot_scatter_lm(\n    data = mpg, var1 = .x, var2 = .y, \n    color = \"cyl\", pointsize = 3.5\n  )\n)\n\nA continuous color scale is used for the ordinal variable\nTrend shows relationship follows the ordinal variable values for the most part which might indicate that this interaction would be predictive\n\nInteresting values might be at dots where the colors are swapped â€” defying the order of the ordinal variable\n\n\nContinuous vs Continuous by Categorical by Categorical\n\nplot_manufacturer_marginal &lt;- function(group, save = FALSE) {\n\n  ## check if input is valid\n  if (!group %in% mpg$manufacturer) stop(\"Manufacturer not listed in the data set.\")\n  if (!is.logical(save)) stop(\"save should be either TRUE or FALSE.\")\n\n  ## filter data\n  data &lt;- filter(mpg, manufacturer %in% group)\n\n  ## set limits\n  lims_x &lt;- range(mpg$hwy) \n  lims_y &lt;- range(mpg$displ)\n\n  ## define colors\n  pal &lt;- RColorBrewer::brewer.pal(n = n_distinct(mpg$class), name = \"Dark2\")\n  names(pal) &lt;- unique(mpg$class)\n\n  ## scatter plot\n  main &lt;- ggplot(data, aes(x = hwy, y = displ, color = class)) +\n    geom_point(size = 3, alpha = .5) +\n    scale_x_continuous(limits = lims_x, breaks = 2:8*5) +\n    scale_y_continuous(limits = lims_y) +\n    scale_color_manual(values = pal, name = NULL) +\n    labs(x = \"Highway miles per gallon\", y = \"Displacement\") +\n    theme(legend.position = \"bottom\")\n\n  ## boxplots\n  right &lt;- ggplot(data, aes(x = manufacturer, y = displ)) +\n    geom_boxplot(linewidth = .7, color = \"grey45\") +\n    scale_y_continuous(limits = lims_y, guide = \"none\", name = NULL) +\n    scale_x_discrete(guide = \"none\", name = NULL) +\n    theme_void()\n\n  top &lt;- ggplot(data, aes(x = hwy, y = manufacturer)) +\n    geom_boxplot(linewidth = .7, color = \"grey45\") +\n    scale_x_continuous(limits = lims_x, guide = \"none\", name = NULL) +\n    scale_y_discrete(guide = \"none\", name = NULL) +\n    theme_void()\n\n  ## combine plots\n  p &lt;- top + plot_spacer() + main + right + \n    plot_annotation(title = group) + \n    plot_layout(widths = c(1, .05), heights = c(.1, 1))\n\n  ## save multi-panel plot\n  if (isTRUE(save)) {\n    ggsave(p, filename = paste0(group, \".pdf\"), \n           width = 6, height = 6, device = cairo_pdf)\n  }\n\n  return(p)\n}\n\nplot_manufacturer_marginal(\"Dodge\")\n\n{ggside} should be able to add these marginal plots with fewer lines of code.\nThis is one of a set of facetted charts by the categorical, â€œmanufacturerâ€\nDots are grouped by categorical, â€œclassâ€\nTop boxplot shows a minivan as an outlier in terms of hwy mpg.\nBox plots and the scatter plot are combined using {patchwork}\n\nCorrelation Heatmaps\n\nFilter data by different levels of a categorical, then note how correlations between numeric predictors and the numeric outcome change\nExample: PM 2.5 pollution (outcome) vs complete dataset and filtered for Wind Direction = NE\n\nComplete\n\nWind Direction = NE\n\nInterpretation\n\nTemperatureâ€™s correlation (potentially its predictive strength) would lessen if would be interacted with Wind Direction. So we do NOT want to interact wind direction and temperature\n\nArticle didnâ€™t show whether it increases with other directions\n\nWind Strengthâ€™s (cws) correlation with the outcome would increase if interacted with Wind Direction. So we do want to interacted wind direction and wind strength\n\nFor ML, I think youâ€™d dummy the wind direction, then multiply windspeed times each of the dummies.\n\n\n\n\nBoxplot by Discrete (Binned) Continuous\n\npmin can be similarily used as fct_lump (see below) but for discrete integer variables\n\nIf the distribution of the discrete numeric is skewed to the right, then pmin will bin all integers larger than some number\n\nMost of the distribution are small integers and the rest will be binned into a sort of â€œotherâ€ category (e.g.Â 14)\n\nIf the distribution is skewed to the left, pmax can be used similarily.\n\ndata %&gt;%\nÂ  Â mutate(integer_var = pmin(integer_var, 14) %&gt;%\nÂ  Â ggplot(aes(int_var, numeric_outcome, group = int_var)) +\nÂ  Â geom_boxplot()\n\nIf all the medians line up then no relationship. A slope or nonlinear pattern shows relationship.\n\n\n\n\n\n\nCategorical Outcome\n\nNumeric vs Numeric by Cat Outcome\n\nScatter with 45 degree line\nggplot(aes(num_predictor1, num_predictor2, color = cat_outcome_var)) +\nÂ  Â geom_point() +\nÂ  Â geom_abline(color = \"red\")\n\nLook for groupings or other patterns wrt to cat var.\nCat-var colored points above line skew more towards the higher y-var than x-var and vice versa for below the 45 degree line.\nLine also shows how linearly correlated the two num vars are.\nIf clustering present, could indicate a good interaction pair with the numeric : cat_var\n\nScatter with linear smooth (or loess)\n\nggplot(aes(num_predictor1, num_predictor2)) +\nÂ  Â geom_point(alpha = 0.25) +\nÂ  Â geom_smooth(aes(color = cat_outcome_var), method = \"lm\")\n\nProduces a lm line for each outcome var category\nLooking for differing trends for ranges of values on the x-axis. A pattern for one line that is substantially different from the other line\nExample: At around 28, the blue line trend rises while the red line continues to slope downwards, and they actually cross to where at some threshold of x, the relationship is the opposite. So an interaction is likely present\n\n\nBinary Outcome (pct_event) vs Discrete by Discrete (or binary in this case)\n\ndata %&gt;%\nÂ  Â  mutate(avg_trans_amt = total_trans_amt / total_trans_ct) %?%\nÂ  Â  group_by(total_trans_ct = cut(total_trans_ct, c(0,30, 40, 50, 60, 80, Inf)),\nÂ  Â  Â  Â  Â  Â  avg_trans_amt = ifelse(avg_trans_amt &gt;= 50, \"&gt; $50\", \"&lt; $50\") %&gt;%\nÂ  Â  Â  Â  Â  Â  # use to figure out best cut point(s) that keeps the ribbon width small-ish on all lines\nÂ  Â  Â  Â  Â  Â  # avg_trans_amt = cut(avg_trans_amt, c(0, 50, 100, 130, Inf)) %&gt;%Â  Â \nÂ  Â  summarize(n = n(),\nÂ  Â  Â  Â  Â  Â  Â  n_churned = sum(churned == \"yes\"),\nÂ  Â  Â  Â  Â  Â  Â  pct_churned = n_churned/n,\nÂ  Â  Â  Â  Â  Â  Â  low = qbeta(.025, n_churned + .5, n - n_churned + .5),Â \nÂ  Â  Â  Â  Â  Â  Â  high = qbeta(.975, n_churned + .5, n - n_churned + .5)) %&gt;%\nÂ  Â  Â  Â  arrange(desc(n)) %&gt;%\nÂ  Â  ggplot(aes(total_trans_ct, pct_churned, color = avg_trans_amt) +\nÂ  Â  geom_point() +\nÂ  Â  geom_line() +\nÂ  Â  geom_ribbon(aes(ymin = low, ymax = high))Â  Â  Â  Â  Â  Â \n\nInterpretation:\n\nClear alternating trend from about 0 to 40 on the x-axis says thereâ€™s probably an interaction (at least with the binned versions of these variables) between total_trans_ct and avg_trans_amt.\n\ni.e.Â The relationship between transaction count and churned (binary outcome) (pct_churned) depends on the average transaction amount\n\n\nExample: The cut points for avg_trans_amt were chosen from its distribution\n\nThe distribution was bi-modal and the 3 cutpoints were the 1st mode, point that splits both modal distributions, and the 2nd mode.\n{Upsetr} might be useful to examine bimodal structure and determine cutpoints based on categorical predictor values and not just outcome values\n{gghdr} - viz for multi-modal distribtutions\nAlso see Regression, Other &gt;&gt; Mult-Modal\n\nExample of likely no interaction\n\n\nBlue and red lines move in unison. Same trend directions.\n\nThere is separation, so the mean value of percent churn is different. Also, the slopes are different, so the rates of increase and decrease would be different. Iâ€™m not convinced. Iâ€™d like to see if an interaction term wouldnâ€™t be significant\nkaggle sliced s01e07 dataset - percent churn (y-axis), revolving balance bucketed (x-axis), color = total_transactions dicotomized. DRob video for the code.\n\n\n\nBinary Outcome (pct_event) vs Categorical by Categorical\n\nSliding Window Continuous vs Binary Outcome (Proportion of Event) by Categorical\n\nggplot(z, aes(x=price, y=`Moving Proportion`, col=factor(class))) +\nÂ  geom_line() + guides(color=guide_legend(title='Class')) +\nÂ  xlab(hlab(price)) + ylab('Survival')\n\nâ€œMoving Proportionâ€ is the mean of the binary outcome (probability of an event) over a sliding window of â€œTotal Priceâ€\nâ€œTotal Priceâ€ should be sorted in ascending order and grouped by â€œClassâ€ before the sliding window is applied\nHarrell uses a default window of 15 observations on either side of the target point, but says the results can be noisy. Recommends passing the results through a smoother\n\nSo, might want to add a geom_smooth to the code chunk\nI might like to see the data points to see how many points at the ends of lines there are. Smoothed lines can be misleading on the boundaries.\n\n\nSliding Window Continuous vs Binary Outcome (Proportion of Event) by 2 Categoricals\n\nggplot(d, aes(x=age, y=`Moving Proportion`, col=factor(class))) +\nÂ  geom_smooth() +\nÂ  facet_wrap(~ sex) +\nÂ  ylim(0, 1) + xlab(hlab(age)) + ylab('Survival') +\nÂ  guides(color=guide_legend(title='Class'))\n\nSimilar to above but grouped by 2 variables before the sliding window calculation.\n\nGrouped Bar\n\nsummarize_churn &lt;- function(tbl) {\nÂ  Â  tbl %&gt;%\nÂ  Â  Â  Â  summarize(n = n(),\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  n_churned = sum(churned == \"yes\"),\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  pct_churned = n_churned/n,\nÂ  Â  Â  Â  Â  Â  Â  Â  # Jeffrey's Interval (Bayesian CI)\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  low = qbeta(.025, n_churned + .5, n - n_churned + .5),Â \nÂ  Â  Â  Â  Â  Â  Â  Â  Â  high = qbeta(.975, n_churned + .5, n - n_churned + .5)) %&gt;%\nÂ  Â  Â  Â  arrange(desc(n))\n}\nplot_categorical &lt;- function(tbl, categorical, ...) {\nÂ  Â  tbl %&gt;%Â  Â  Â  Â \nÂ  Â  Â  Â  ggplot(aes(pct_churned, [{{categorical}}]{style='color: goldenrod'}), ...) +Â \nÂ  Â  Â  Â  geom_col(position = position_dodge()) +Â \nÂ  Â  Â  Â  geom_errorbar(aes(xmin = low, xmax = high),\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  height = 0.2, color = red,\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  position = position_dodge(width = 1) +\nÂ  Â  Â  Â  scale_x_continuous(labels = percent) +\nÂ  Â  Â  Â  labs(x = \"% in category that churned\")\n}\ndata %&gt;%\nÂ  Â  group_by(cat_var1, cat_var2) %&gt;%\nÂ  Â  summarize_churn() %&gt;%\nÂ  Â  plot_categorical(cat_var1, fill = cat_var2, group = cat_var2)\n\nInterpretation: Probably not an interaction variable. Pct Churned by education Level doesnâ€™t vary (much) byÂ  Gender especially if you take the error bars into account\n\nOnly for â€œcollegeâ€ do you see a flip in the relationship where females churn more than men, but itâ€™s still within the error bars.\n\n\n\nBinary Outcome vs Binary by Categorical\n\n\nNot certain but Iâ€™d think youâ€™d want your outcome on the x-axis. Although, if you swapped the x-axis variable with the grouping variable, youâ€™d probably come to the same conclusion. Therefore, it may not matter that much\nShows percent, and not counts"
  },
  {
    "objectID": "qmd/eda-multilevel-longitudinal.html#sec-eda-multlong-misc",
    "href": "qmd/eda-multilevel-longitudinal.html#sec-eda-multlong-misc",
    "title": "Multilevel, Longitudinal",
    "section": "Misc",
    "text": "Misc\n\nAlso see Econometrics, Mixed Effects &gt;&gt; Considerations &gt;&gt; Variable Assignment\nNeed to figure out if\n\nThereâ€™s significant within-unit variation. If so, then FE model will likely be the best model\n\nArticle with simulated data showed that within variation around sd &lt; 0.5 didnâ€™t detect the effect of explanatory variable but ymmv (depends on # of units, observations per unit, N)\n\nThereâ€™s significant between-unit variation. If so, then RE model will likely be the best model"
  },
  {
    "objectID": "qmd/eda-multilevel-longitudinal.html#sec-eda-multlong-mult",
    "href": "qmd/eda-multilevel-longitudinal.html#sec-eda-multlong-mult",
    "title": "Multilevel, Longitudinal",
    "section": "Multilevel",
    "text": "Multilevel\n\nMisc\n\nIs my data clustered?\nSeparate variables into levels\n\nLevel One: Variables measured at the most frequently occurring observational unit\n\ni.e.Â Variables that (for the most part) have different values for each row\ni.e.Â Vary for each repeated measure of a subject and vary between subjects\n\nLevel Two: Variables measured on larger observational units\n\ni.e.Â Constant for each repeated measure of a subject but vary between each subject\n\n\n\n\n\nUnivariate\n\nLevel 1 and Level 2\n\nGroup-level correlation or autocorrelation in variables can mislead or obscure patterns\n\nIf level 2 variable categories are pretty well balanced and thereâ€™s sufficient data, then plotting means can remove the correlation affect in the plot\n\nContinuous\n\nLooking at the skew, median/mean, bimodal or not\nExample:\n\n\n(Top) Each observation is plotted as if each observation is independent of the other\n\n* Ignores dependency (via repeated measures)\n\n(Bottom) Means for each subject or case or other level of a random variable\n\n* Removes dependency\n\nInterpretation: Right skew remains in both plots but plot 1â€™s decrease is smoother than plot 2â€™s\n\n\nCategorical\n\nCalculate proportions of each category and noting trends (ordinal variables) or severe imbalances\n\n\n\n\n\nBivariate\n\nQuestions\n\nIs there is a general trend suggesting that as the covariate increases the response either increases or decreases (trend)\nDo subjects at certain levels of the covariate tend to have similar mean values of the response (low variability)\nIs the variation in the response at different levels of the covariate (unequal variability)\n\nMe: Comparison between plots that take into account dependency and the same plot that doesnâ€™t\n\nTrend in plot that ignores dependency but no trend in plot that removes dependency\n\nMay indicate within-subject variation\n\nNo trend in plot that ignores dependency but trend in plot that removes dependency\n\nMay indicate between-subject variation\n\n\nBoxplots (Categorical)\n\n\nLevel 1 categorical covariates (y-axis) vs continuous outcome (x-axis)\n* Ignores dependency (via repeated measures)\nInterpretation\n\nLeft: ordinal covariate, medians are close and boxes pretty much contained within each other but there might be a trend\nRight: Looks like some decent variation between categories\n\nMean outcome (per subject) vs covariate\n\n\n* Removes dependency\nInterpretation: looks like some decent variation between categories\n\n\nScatter (Continuous)\n\n\nLevel 1 continuous covariate (x-axis) vs continuous outcome (y-axis)\n* Ignores dependency (via repeated measures)\nActually a discreteÂ  covariate being treated as continuous the fact that does seem to be a small trend is whatâ€™s important\nMean outcome (per subject) vs covariate\n\n\n* Removes dependency\nInterpretation: PEM not showing much of an correlation if any\n\n\nFacetting previous plots by subject\n\n\n\nLeft\n\nMostly downward trends but some upward trends\nUseful for prior formulation\nGives an idea about the uncertainty of the slope of this variable\n\nRight\n\nScarcity of points for some categories makes boxplots a bad idea\nDifficult to spot any trends\n\n* Removes dependency\n\n\n\n\nTrivariate\n\nScatter, color by random variable\n\n\nVariables\n\nâ€œpoints per 60 minâ€ (outcome)\nâ€œtime on iceâ€ (fixed effect)\nfacetted by â€œpositionâ€ (fixed effect)\ncolored by â€œplayerâ€ (potential random variable)\n\nInterpretation\n\nTheres does seem to be clustering by â€œplayerâ€ therefore a mixed effects model might be a good choice.\n\n\nNull Model (aka random intercept-only model)\nm0 &lt;- \n  lmer(pp60 ~ 1 + (1 | player),Â \n       data = df)\n\njtools::summ(m0)\nGROUPING VARIABLES\nGROUP # GROUPS ICC\nplayer Â  Â  Â  20Â  Â  Â  0.89\nICC &gt; 0.1 is generally accepted as the minimal threshold for justifying the use of Mixed Effects model (See ICC section)"
  },
  {
    "objectID": "qmd/eda-multilevel-longitudinal.html#sec-eda-multlong-long",
    "href": "qmd/eda-multilevel-longitudinal.html#sec-eda-multlong-long",
    "title": "Multilevel, Longitudinal",
    "section": "Longitudinal",
    "text": "Longitudinal\n\nMisc\n\nRepeated measures that have a sequential or time component\npackages: {brolgar}\n\n\n\nUnivariate\n\nContinuous Outcome vs.Â Time\n\nFacetted by Observational Unit (e.g.Â school)\n\n\n\nLinear Fit and Line Chart\n\nSpaghetti\n\n\n\n\n\nBivariate\n\nBold line is the overall fit with LOESS\nContinuous Outcome vs Time\n\nFacetted by Categorical\n\nFacetted by Binned Continuous\n\n\nTime Endpoints\n\n\nâ€œSchool Typeâ€ is a categorical, level 2 variable and â€œMath Scoreâ€ is the numeric outcome\nLooking for change from the initial measurement to the final measurement\n\n\n\n\nLinear parameters\n\nFit a linear regression for each subject/unit with its repeated measurements\n\nSee univariate &gt;&gt; numerical outcome vs.Â time &gt;&gt; Facetted by observational unit &gt;&gt; (left) linear fit\n\nAdvantages\n\nEach unitâ€™s/subjectâ€™s data points can be summarized with two summary statisticsâ€”an intercept and a slope\n\nBigger advantage when there are more observations over time per unit/subject\n\nSeems like a good way for using empirical bayes (i.e.Â use these distributions for prior specifications)\n\nDisadvantages\n\nSlopes cannot be estimated for those units/subjects with just a single observation\nR-squared values cannot be calculated for those units/subjects with no variability in test scores during the time period\nR-squared values must be 1 for those units/subjects with only two test scores.\n\nSummary Statistics\n\nMean and SD for intercepts and slopes\n\nUnivariate\n\n\\(y_t = \\beta_0 + \\beta_1 t + \\epsilon_t\\)\n\n\\(t\\) is the time variable (aka trend)\n\nParameter Distributions\n\nCorrelation\n\n\nLower intitial values (intercepts) show the greatest growth (slopes) over time\nCorrelation = -0.32\n\n\nBivariate\n\nProcess\n\nFilter data by Level 2 variable\nFor each category of the Level 2 variable, fit a regression, yt = Î²0 + Î²1t + Îµt, for each unit/subject.\nAggregate results\n\nParameter Distributions\n\n\nâ€œSchool Typeâ€ is a Level 2, binary variable"
  },
  {
    "objectID": "qmd/eda-text.html#sec-eda-text-misc",
    "href": "qmd/eda-text.html#sec-eda-text-misc",
    "title": "Text",
    "section": "Misc",
    "text": "Misc\n\nAlso see\n\nFeature Engineering, Tokenization\nFeature Engineering, Embeddings\nDiagnostics, NLP\nNLP, General\n\nCount most popular words\ndata %&gt;%\nÂ  Â  unnest_tokens(word, text_var) %&gt;%\nÂ  Â  count(word, sort = TRUE)\nAvg value of outcome variable that associated with words\n\ndata %&gt;%\nÂ  Â  unnest_tokens(word, text_var) %&gt;%\nÂ  Â  group_by(word) %&gt;%\nÂ  Â  summarize(avg_outcome = mean(outcome),\nÂ  Â  Â  Â  Â  Â  Â  n = n()) %&gt;%\nÂ  Â  arrange(desc(n)) %&gt;%\nÂ  Â  head(30) %&gt;%\nÂ  Â  mutate(word = fct_reorder(word, avg_outcome)) %&gt;%\nÂ  Â  ggplot(aes(avg_outcome, word, size = n)) +\nÂ  Â  geom_point()\n\nPattern in the example shows that words in an airbnb listing probably have predictive power on price (outcome variable)"
  },
  {
    "objectID": "qmd/eda-time-series.html#misc",
    "href": "qmd/eda-time-series.html#misc",
    "title": "24Â  Time Series",
    "section": "24.1 Misc",
    "text": "24.1 Misc\n\nPackages\n\n{{diaquiri}}\n\nAggregated values are automatically created for each data field (column) depending on its contents (e.g.Â min/max/mean values for numeric data, no. of distinct values for categorical data)\nOverviews for missing values, non-conformant values, and duplicated rows.\n\n\nBasic Steps\n\nCheck seasonality at all periods at and above data frequency\nInvestigate spikes (i.e.Â special days or weeks) Check lags of outcome variable\nCheck for NAs in outcome variable\nAre there a significant amount of zeros\n\nLook for NAs\n\nNAs affect the number of lags to be calculated for a variable\n\ne.g.Â exports only recorded quarterly but stock price has a monthly close price you want to predict. So if youâ€™re forecasting monthly oil price then creating a lagged variable for exports is difficult\n\nBizsci (lab 29), minimum_lag = length_of_sequence_of_tail_NAs +1Â \n\ntail(ts) are the most recent values\n\n\n\nEven if you donâ€™t want a lag for a predictor var and it has NAs, you need to recipe::step_lag(var, lag = #_of_tail_NAs). So var has no NAs.\nConsider seasonality of series when determining imputation method\n\nCheck the shape of the distribution of the outcome variable\n\nFor low volume data, a right-skewed distribution might be needed instead of a Gaussian.\n\nThin tails - Use a Gamma distribution\nHeavier tails - Use a Log Normal or the Inverse Gaussian\n\n\nAre there a significant amount of zeros\n\nSee notebook for tests on the number of zeros in Poisson section\nAlso might be tests in the intermittent forecasting packages, so see bkmks\nIf so, see Logistics &gt;&gt; Demand Forecasting &gt;&gt; Intermittent Data for modeling approaches Timestamp Columns\nAre there gaps in the time series (e.g.Â missing a whole day/multiple days, days of shockingly low volume)?\n\nSeasonality Tests (weekly, monthly, and yearly)\n\n{seastests} QS and Friedman (see bkmk in Time Series &gt;&gt; eda for example)\nQS testâ€™s null hypothesis is no positive autocorrelation in seasonal lags in the time series\nFriedman testâ€™s null hypothesis is no significant differences between the valuesâ€™ period-specific means present in the time series\nFor QS and Friedman, pval &gt; 0.05 indicates NO seasonality present\n\nLook at quantile values per frequency unit (by group and total)\n\n{timetk::plot_time_series_boxplot}\naverage closing price for each month, each day of the month, each day of the week\nwhen are dips and peaks?\nWhich groups are similar?\nWhat are the potential reasons behind these dips and peaks?\nExample (daily power consumption)\n\nmedian, the lower quartile, and the upper quartile for Saturdays and Sundays are below the remaining weekdays when inspecting daily power consumption\nsome outliers are present during the week, which could indicate lower power consumption due to moving holidays\n\nMoving holidays are holidays which occur each year, but where the exact timing shifts (e.g.Â Easter)\n\n\nExample (monthly power consumption)\n\nmedian, the lower quartile, and the upper quartile of power consumption are lower during the spring and summer than autumn and winter\n\nExample: Demand per Month and per Category\n\nStatistical Features vs Outcome\n\nSee Feature Engineering, Time Series\nFirst Autocorrelation Coefficient vs Categorical vs Binary Outcome\n\n\nThere does seem to be some variance. An interaction between autocorrelation and the cateogorical variable might be predictive of a heart murmur event.\n\n\nVariance of value by group\n\nExample: how sales vary between store types over a year\nimportant to standardize the value by group\n\ndf %&gt;% group_by(group) %&gt;% mutate(sales = scale(sales))\n\nWhich groups vary wildly and which are more stable\n\nrates by group\n\nExample: sales($) per customer\n\ndf %&gt;% group_by(group, month) %&gt;% mutate(sales_per_cust = sum(sales)/sum(customers)\n\n\nstl decomposition\n\nIf we are interested in short- or long-term movements of the time series, we do not care about the various seasonalities. We want to know the trend component of the time series and if it has reached a turning point\nis there strong seasonality or trend?\nIf thereâ€™s a pattern in the random/remainder component, this could indicate that there are other strong influences present.\nDaily Seasonal Adjustment (DSA)\n\nDaily data can have multiple seasonalities present\nCombines the seasonal trend decomposition procedure using Loess (STL) with a regression model with ARIMA errors\n{dsa}, example\n\nexample shows it outperforming Hyndmanâ€™s STR procedure\n\nProcedure\n\nSTL adjusts intra-weekly periodic patterns.\nRegARIMA estimates calendar effects, cross-seasonal effects, and outliers.\nSTL adjusts intra-monthly periodic effects.\nSTL adjusts intra-annual effects\n\n\n\nDoes the series have an additive or multiplicative structure\n\nDoes the amplitude of the seasonal or cyclical component increase over time?\n\nThe amplitude of the seasonal component increases over time so this series has a multiplicative structure\n\nAlso if thereâ€™s a changing seasonal amplitude for different times of the year\n\n\n** If you have a multiplicative structure and zeros in your data (i.e.Â intermittent data), then they must handled in some way. **\n\nSee Logistics &gt;&gt; Demand Forecasting &gt;&gt; Intermittent Data\n\n\nlag (scatter) plot\n\ncross-correlation\n\nSteps\n\nstandardize all variables\nlook at faceted plots for outcome ts vs lags of a predictor ts\nchose promising predictors and lags\nrun pearson correlation heatmap\n\nIssues/questions\n\nshould differences be done before doing this?\nare there better correlation metrics?\n\n\n\nacf, pcf plots\n\ncan also be done with heat maps which maybe more useful for presentation than eda\n\ntrend strength and seasonal strength plots\nPCA (Dynamic Factor modeling takes into account the time dimension â€” see )\nsubseries plots for multi-categorical series\n\nnumber of trips (measurement), region, state, purpose (keys)\n\nIs data recorded at irregular intervals. If so:\n\n{{BINCOR}}handles cross-correlation between 2 series with irregular intervals and series with regular but different intervals\nI think common forecasting algorithms require regularly spaced data, so look towards ML, Multilevel Model for Change (MLMC), or Latent Growth Models\nMay also try binning points in the series (like BINCOR does) or smoothing them to get a regular interval\n\nSeasonality\n\nfacet scatterplots by month;Â  with x = year, y = value\ntsfeatures has seasonality strength metric\n\nSteps:\n\nGet a sense of whether the relationships are linear or nonlinear (should difference series first I think)\n\nLag scatterplots within the target series (i.e.Â yt vs yt+h)\nLag scatterplots between target series and lags of predictor series (i.e.Â yt vs xt+h)\n\n\n\nastsa::lag1.plot(y, 12) # lags 1-12 of y\nastsa::lag2.plot(y, x, 8) # y vs lags 0-8 of x\n\nautocorrelation/cross-correlation values in upper right corner\n\nautocorrelations/cross-correlation values only valid if relationships are linear but maybe still useful in determining a positive or negative relationship\n\nloess smoothing line added\nsee ccf section below for interpreting lag.2 plot (e.g.Â lags and leads)\nnonlinear patterns can indicate that behavior between the two variables is different for high values and low values\nCheck ACF for all series\n\nif relationships were linear, confirm strongest relationships seen in scatterplot with spikes in ACF\n\nIs the series a trend-stationary or unit root process?\n\ntest all series of interest with ADF and KPSS tests\n\nDifference or detrend (according to results of ADF and KPSS tests) all series of interest\n\nshould all series be required to have the same transform?\n\nApply CCF to transformed series with linear and/or nonlinear correlation with function (interpretations of lag scatterplots to give a clue) to determine predictive lags.\nIs the time series additive or multiplicative, is the variation (mostly) constant or not constant over time?\nShannon Spectral Entropy\n\nfeasts::feat_spectralÂ  will compute the (Shannon) spectral entropy of a time series, which is a measure of how easy the series is to forecast.\nA series which has strong trend and seasonality (and so is easy to forecast) will have entropy close to 0.\nA series that is very noisy (and so is difficult to forecast) will have entropy close to 1.\n\nACF (autocorrelation function)\n\nFor a stationary time series,\n\nFor stationary data, the ACF will drop to zero relatively quickly, while the ACF of non-stationary data decreases slowly.\nfor non-stationary data, the value of r1 is often large and positive.\n\n\nThe ACF of the differenced Google stock price (right fig) looks just like that of a white noise series. There are no autocorrelations lying outside the 95% limits, and the Ljung-BoxÂ  Qâˆ— statistic has a p-value of 0.355 (forÂ  h = 10) which implies the ts is stationary. This suggests that the daily change in the Google stock price is essentially a random amount which is uncorrelated with that of previous days."
  },
  {
    "objectID": "qmd/experiments-a_b-testing.html#sec-exp-ab-misc",
    "href": "qmd/experiments-a_b-testing.html#sec-exp-ab-misc",
    "title": "25Â  A/B Testing",
    "section": "25.1 Misc",
    "text": "25.1 Misc\n\nResources\n\nPractical Guide to Controlled Experiments on the Web (Paper)\n\nGeneral non-parametric A/B testing based on the Random Forest (Paper)\n\nUses the sum of OOB errors for each group as a test statistic\n{hypoRF}\n\nGoogle Analytics switched in 2017 from exact user counts to estimates of user counts by default. Estimates have Stderr and have significant effects on A/B testing. Think this can be switched to exact users in settings, see link\n\nhttps://towardsdatascience.com/the-perils-of-using-google-analytics-user-counts-in-a-b-testing-e50b5dfc5f6c\nWith user estimates and greater than 12K users per test arm, accuracy begins to suffer. Occurrences of significant p-values becomes inflated\nMaybe even worse with Adobe Analytics which uses a similar algorithm to estimate their user counts\n\nRegardless of the test results, there may be business considerations to take into account when deciding whether to add a feature\n\nItâ€™s believed user experience will be improved\nOne group shows positive results while another doesnâ€™t, but in order to maintain parity you decide to add the feature\nYou were unable to perform the test during the optimal part of the year, and you have reason to believe the results wouldâ€™ve been positive if the test was conducted during that time period.\n\nOften a good idea to limit outcome measurement to events that happen (or donâ€™t happen) within a reasonable attribution window from exposure or assignment.\n\nBetter to use the first exposure as the attribution window start, as it should be unaffected by the experiment variants\nExample: a simple exposure of seeing â€œsign upâ€ for the control and â€œsign up today!â€ for the treatment\n\nThe attribution window should be a within hours of exposure as we donâ€™t expect the call-to-action text to have long-lasting effects.\n\n\nWhen running experiments on as few users as possible to reach a specified level of statistical significance, experiments may end up being statistically significant but with little precision in terms of the estimate\n\n\nAllows us to have conviction in rolling out the treatment to production\nNo certainty of the expected impact of the treatment\nIssues with this approach (see universal holdout section for solution)\n\nShort-term impact from a product feature may not equal its longer-term impact (e.g.Â novelty effect)\nOne algorithmâ€™s performance advantage over another may fade over time\nCertain metrics are lagging indicators where we cannot observe any effects unless we extend the time horizon\n\n\nOptimal Stopping\n\nAlso see multi-armed bandit section\nSequential Probability Ratio Test (article)\n\nperforms a rolling likelihood ratio test (LRT) and once the value passes a threshold, the experimentâ€™s p-value has a certain probability of correctly accepting/rejecting the null hypothesis\n\n\nUsed gated dial-up to minimize risk to business\n\nProcess\n\nAssign users that will be in the experiment and not in the experiment\n\nThis population can be just 1% of the population in the beginning, and increase from there.\n\nFor users in the experiment, divide them randomly with equal probability into control and treatment\n\n** Donâ€™t use gradual dial-up over the whole population\n\nExample: start with a 1% treatment, dial up to 5% after a week, then to 10%, 25%, and finally to 50%\nIssues:\n\nCannot use the data from the dial-up period itself in the A/B test because it may be biased by seasonal effects.\nPre-exposure effect: some of the users in the treatment group have already had exposure to the treatment before and this pre-exposure can change their actions during the measured test period.\n\nThe gated dial-up solves these issues because both groups in the experiment are of the same size at all times and no participant in the treatment has been pre-exposed"
  },
  {
    "objectID": "qmd/experiments-a_b-testing.html#sec-exp-ab-terms",
    "href": "qmd/experiments-a_b-testing.html#sec-exp-ab-terms",
    "title": "25Â  A/B Testing",
    "section": "25.2 Terms",
    "text": "25.2 Terms\n\nA/A testing - uses A/B testing to test two identical versions of a page against each other. Typically, this is done to check that the tool being used to run the experiment is statistically fair. In an A/A test, the tool should report no difference in conversions between the control and variation, if the test is implemented correctly. Useful for exposing experiments with high false positive rates. (link, article)(Also see Workflow section)\nExploitation - One exploits a single action without knowing the rewards of the other actions.\n\ne.g.Â A person finds a good restaurant and repeatedly goes to that restaurant instead of trying others\n\nExploration - Always exploring the other actions to acquire as much information as possible to finally choose the best action\n\ne.g.Â A person repeatedly goes to a new restaurant instead of always going to the same restaurant\n\nMinimum Detectable Effect (MDE) - The smallest improvement over the baseline we are willing to detect in a controlled experiment. How large the difference should be in order to generate statistically significant outcomes (generally provided by the A/B testing platform)\nSample Ratio Mismatch (SRM) - refers to the mismatch between the sample ratio set by the experimenter and the observed sample ratio (example)\n\nExample\n\nSuppose you have an A/B test where you expected a 50/50 split of users in each test group. Instead, the Control and Variation groups are off by around 7%."
  },
  {
    "objectID": "qmd/experiments-a_b-testing.html#sec-exp-ab-wkflw",
    "href": "qmd/experiments-a_b-testing.html#sec-exp-ab-wkflw",
    "title": "25Â  A/B Testing",
    "section": "25.3 Workflow",
    "text": "25.3 Workflow\n\nNotes from Typical 8-Step A/B Test Workflow for Data Scientists in 2022\nPre-Experiment: Think About the Product & Talk with the Product Manager (PM)\n\nExplore the product logic and user journey in detail\nFeasibility and Impact Questions\n\nWhat are the hypotheses?\nCan we find calculable metrics to test that?\nWhat are the risks for our product and users?\nIs there a network effect?\nIs it possible for users to demonstrate a strong novelty effect or change aversion\nIs it fair to keep some users from our new feature treatment?\nWhy do we need this feature?\nWhat is our expected gain from this feature, is it short-term or long-term?\n\n\nDesign (also see Design, Metrics sections)\n\nEvaluation Metrics: What are our north star metrics, directly-influenced metrics, guardrail metrics, and adoption metrics in the treatment group?\n\nApart from the business interpretation, it would be better for the evaluation metrics to have lower variance and be sensitive to changes.\nExample: Add Top 10 movies/shows feature to Netflix homepage\n\nHypothesis: â€œShowing members the Top 10 â€˜experienceâ€™ will help them find something to watch, increasing member joy and satisfaction.â€\nPrimary Metric: Engagement with the Netflix app\n\nMetric answers, â€œAre the ideas we are testing helping our members to choose Netflix as their entertainment destination on any given night?â€\nResearch shows that this metric is correlated, in the long term, with the probability that members will retain their subscriptions.\n\nSecondary Metrics:\n\nTitle-level viewing of those titles that appear in the Top 10 list\n\ni.e.Â viewing of titles that appear in the Top 10 list\n\nFraction of viewing that originates from that Top 10 row of movies vs other parts of the UI\n\nGuardrail Metric: Compare customer service contacts for the control and treatment groups\n\nAn increase may indicate member confusion or dissatisfaction\n\n\n\nArticulate a causal chain\n\nShow how user behavior will change in response to the new product experience (aka feature) to the change in our primary decision metric\nMonitor secondary metrics along this chain\n\nHelps build confidence that any movement in our primary metric is the result of the causal chain we are hypothesizing, and not the result of some unintended consequence of the new feature (or a false positive)\nConfident in positive effect of primary metric IF secondary metrics have also shown an increase\nSkeptical of positive effect of primary metric IF secondary metrics have shown a decrease.\n\n\nUnit of Diversion: Shall we randomize the test based on user_id or session_id. (see Assignment)\nSet the False Positive Rate (typically 5%) (see Experiments, General &gt;&gt; Misc)\nPostulate an effect size and calculate the sample size\n\nFor different evaluation metrics and various minimum detectable effects (MDE), what are the necessary sample sizes for the result to be statistically significant?\n\nVery often we have to retrieve historical data and calculate on our own when the metrics of interest are too specific and are not supported by the A/B test platforms.\n\nChoice of effect size that you want to be able to detect should be meaningful in terms of business value\n\nExperiment Duration & Traffic Portion: This usually depends on the required samples size and the eligible traffic size that could potentially see the new feature.\n\nAlso take into consideration the risks of the experiment.\n\nExperiment Layer & Experiment Type: Determine which experiment layer to deploy our A/B tests.\n\nCommon that most of the experiment layers have been populated with other tests.\n\nOptions: Choose another layer or consider orthogonal experiments, if our feature is not correlated with others.\n\n\n\nCheck A/A variation\n\nMost platforms have automated this process\nCheck the A/A variation output and make sure everything is as expected â€” the p-values out of A/A simulations are uniformly distributed (Also see links in Terms section)\n\nA/A variation is the difference in key metrics between the control group (group A) and another control group (~group A). Since there is no actual difference between the two, the A/A variation is not expected to deviate from 0 in a statistically significant (p-value &lt; 0.0005 recommended) manner. (i.e.Â any detections are false positives)\n\nPre-treatment A/A (e.g.Â 60 days): A statistically significant result from the pre-assignment test indicates bias in the A/B test\n\nLikely due to user assignment.\n\ne.g.Â (user characteristics) A pre-assignment test would uncover that treatment users already had a higher number of purchases even before the experiment.\n\n\nTest under different scenarios: e.g.Â multiple steps in the conversion funnel (e.g., product impression, click, adding to the shopping cart, and purchase), client-side and server-side experiments, logged-out and logged-in experiments, etc.\nKeep past A/A tests and use them as a baseline Should be performed after any improvement to your experimentation platform\n\nDuring Experiment\n\nCheck the Invariant Metrics & Experiment Set-up\n\nMake sure users in the treatment group see all the treated features as expected\n\nCan be very bad if they arenâ€™t and youâ€™re using the traffic bought by your advertisers for A/B testing but end up sabotaging the experiments due to wrong parameters\n\nCheck that the diversion of traffic is random (also see Assignment section &gt;&gt; SRM Check)\n\nTypically use the Chi-squared Test to check the population across control and treatment groups\n\nCheck that the distribution of user profiles (e.g.Â gender, age) is homogeneous across treatment/control groups\n\nMost platforms will compute these\n\n\nMonitor Key Evaluation Metrics\n\nIf the key metrics and guardrail metrics drop significantly and continuously, consider taking a step back and re-evaluating the experiment for the benefit of our product and users, as the negative significant results are likely to be just a matter of time\n\nStart doing checks as soon as the experiment launches\n\nCheck regularly for at least the first week for new experiments\n\nNew experiments should be treated like intensive care patients\n**Be aware that the more you test, the greater probability of getting a false positive**\n\nShould apply a multiple-testing correction to the p-value (e.g.Â bonferoni)\n\nSee Romano-Wolf\n\nSee Statistical Concepts &gt;&gt; Null Hypothesis Significance Testing (NHST) &gt;&gt; Romano and Wolfâ€™s correction\nSimilar to Westfall-Young but less restrictive\n\n\n\n\n\n\nPost-Experiment - Analyze the Results\n\nCollect the descriptive differences as well as the p-value between the treatment groups and control groups\n\nAlso see\n\nPost-Hoc Analysis, general\nExperiments, Analysis &gt;&gt; A/B Post-Hoc Analysis\nWilcoxon test in notebook and bkmks\n\n\nDo the results align with the hypothesis?\n\nIf they do, delve deeper to understand the reasons by breaking it down into key business dimensions (e.g.Â new/old users, channels, geographic regions).\n\nIf observed effect is large, itâ€™s more likely that itâ€™s a false positive or something went wrong with the execution of the experiment.\n\nInsignificant results can because of the sample size or high variance in the metrics being measured\n\nFor reducing variance, see Experiments, General &gt;&gt; Decreasing the sampling variance of the treatment effect\n\n\nLook at changes in these secondary metrics to assess if any changes in the primary metric follow the hypothesized causal chain\nIs there additional supporting or refuting evidence?\n\ne.g.Â consistent patterns across similar variants of the same feature\nIf you test 20 variants and only one yields a significant movement in the primary decision metric, you should be skeptical of that one variantâ€™s positive effect.\n\nWith that 5% false positive rate, we expect on average one significant (1 in 20) result from random chance alone\n\n\nCompare the empirical Minimum Detectable Effect (MDE) to the actual difference in key metrics\n\nOften reveals how volatile the metrics are.\nGreat reference when studying user behaviors or designing similar experiments in the future.\n\n\nIf results are mixed/suggestive but not conclusive, run a second A/B experiement based on learnings from the first test\n\nExample: Half of the feature variants had a positive effect, but the other half did not.\n\nRefine these most promising variants, and run a new test\n\nWith fewer variants to test, you can also increase the allocation size to gain more power.\n\nFormulate Product Recommendations\n\nSummarize the analysis of the experiment results\nDerive a product recommendation on whether we should gradually expose more traffic to the treatment or select a more conservative strategy.\n\nWrite the Report\n\nContents\n\nproduct backgrounds and feature hypotheses\nexperiment resign\nresult analysis\nproduct recommendations"
  },
  {
    "objectID": "qmd/experiments-a_b-testing.html#sec-exp-ab-dsn",
    "href": "qmd/experiments-a_b-testing.html#sec-exp-ab-dsn",
    "title": "25Â  A/B Testing",
    "section": "25.4 Design",
    "text": "25.4 Design\n\nSimple\n\nfind sample size\n\nStandard way: use equation with power, significance level, and effect size\nâ€œRule of Thumbâ€ way: sample size = (16* Ïƒ2) / Î´2\n\nÏƒ is variance of the data\nÎ´ is the effect size (stake holders or literature) (e.g.Â 1% increase of revenue would be practically significant)\n\n\nCalculate run time of the experiment\n\nDivide sample size the number of users per group (i.e.Â experiment and control groups)\nRound up to the next week to capture weekly seasonality, calculate weekly metrics, etc.\n\nSubmit recommendation based on the results of the experiment\n\nLink results goals and business impact (see step 1) (how much does a 1% increase in click rate relate to revenue?)\nDiscuss any conflicting results (e.g.Â rise in daily active users and bounce rate)\n\nTranslate how each affects the user and the company\n\nShort term vs long term impact\n\nDo the benefits (e.g.Â dau) outweigh the drawbacks (e.g.Â bounce back) over the long term?"
  },
  {
    "objectID": "qmd/experiments-a_b-testing.html#sec-exp-ab-ass",
    "href": "qmd/experiments-a_b-testing.html#sec-exp-ab-ass",
    "title": "25Â  A/B Testing",
    "section": "25.5 Assignment",
    "text": "25.5 Assignment\n\nMisc\n\nSpookyHash hash function meets all 4 characteristics of a good assignment algorithm + itâ€™s fast (see below for characteristics)\n\nFNV hash function fails #3\nMD5 hash function is ~50% slower than SpookyHash\n\nUse a population split (e.g.Â userid, sessionid, cookieid). Never do a time split because it violates the identity assumption (i.e.Â the statistical properties of the control and treatment groups are identical), and the test results are therefore inconclusive\nCookies have a limited life-time which makes it difficult to measure the long-term user impact\n\nSome browsers like Safariâ€™s Intelligent Tracking Prevention (ITP) delete some cookies after 7 days, so if a test runs for more than a week, then users will be re-assigned after each week.\nEUâ€™s General Data Protection Regulation (GDPR) states that website owners must receive explicit user consent before they use any cookies except those that are â€œstrictly necessaryâ€ (and itâ€™s hard to argue that A/B testing is strictly necessary to run a service)\n\n\nPotential Issues\n\nAssignment of ineligible users - These may be bots or users that already have an account.\n\nIf we include many ineligible users in our analysis, we may underestimate the effect size even if their distribution across groups is uniform.\n\nCrossovers - These are users that manage to experience both variants.\n\nExample:Â  Users may come across our site on mobile with the â€œsign up today!â€ (exposure) text, and then switch to desktop and see the â€œsign upâ€ (control) message.\nDepending on the instrumentation we have in place, we may not be able to detect such users, or we may only detect them if they sign up on one device and then log in on the other device.\n\nAssignment without exposure - Due to implementation constraints, we may not be guaranteed that assigned users are actually exposed to the treatment and control.\n\nExample: it may be that the assignment is done on the backend while exposure happens conditionally and asynchronously on the frontend â€“ some users may bounce in the gap between assignment and exposure, and never see the exposure (e.g.Â call-to-action text, sign up today!â€œ ).\n\nMultiple exposures - Once a user has been assigned, they may get exposed to the treatment and control multiple times (without crossing over).\n\nExample: Users may visit the landing page repeatedly and see the â€œsign upâ€ (control) or â€œsign up today!â€ (exposure) text multiple times before deciding to sign up.\n\n\nCharacteristics of a good assignment algorithm (Paper)\n\nUsers must be equally likely to see each variant of an experiment (assuming a 50â€“50 split). There should be no bias toward any particular variant. (i.e.Â uniform distribution)\n\nCan run a Chi-Square or KS test to compare the random numbers generated (see R in example) to the uniform distribution.\n\nRepeat assignments of a single user must be consistent; the user should be assigned to the same variant on each successive visit to the site.\nWhen multiple experiments are run concurrently, there must be no correlation between experiments. A userâ€™s assignment to a variant in one experiment must have no effect on the probability of being assigned to a variant in any other experiment.\nThe algorithm should support monotonic ramp-up, meaning that the percentage of users who see a Treatment can be slowly increased without changing the assignments of users who were already previously assigned to that Treatment.\n\nExample: Wish AI (article)\n\n\nSteps\n\nConcatenate salt and user ID to get string, S.\n\nSalt is just a string of letters & numbers (e.g.Â E1F53135E559C253) that gets concantenated to an object to provide an extra layer of security\nUser ID can also be cookie ID, session ID, etc.\n\nApply a Hash function to map the concatenated string, S, to a hash value H. Note, H follows a uniform distribution due to the uniformity property of hash values.\nAssuming the hash function is 64 bit, H is then divided by float(0xFFFFFFFFFFFFFFFF) and multiplied by 10,000 to get a uniform random number integer R ranging from 0 to 9,999.\nDivide R by 100. If R/100 &gt;= exposure_rate (e.g., 10%) times 100, we assign ignore to this user, and the user will be excluded in any calculations for this experiment.\nR modulo 100. Assuming there are two experiment buckets: control, treatment. If the remainder is &lt; control bucket percentage (e.g.Â 50%) time 100, assign control. Otherwise, assign treatment.\n\nOne random number R is generated. The first two digits of R are used for determining exposure, and the last two digits are used for assigning treatment/control buckets. The first two digits and the last two digits of R are independent\n\nSample Ratio Mismatch (SRM) Check\n\n\nMisc\n\nMake sure to use â€œusersâ€ and not â€œvisitsâ€ or â€œsessionsâ€ for the check\nDonâ€™t be concerned if early in the experiment, some mismatch occurs\n\nCheck for glaringly large or small ratios\n\ne.g.Â If you see 1,000 users in one group and 100 in the other, you know thereâ€™s a problem\n\nCalculate the sample ratios to check ratios closer to one that might still be a problem\n\ncontrol = users_in_control / total_users_in_test\ntreatment = users_in_treatment / total_users_in_test\n\nTest counts with Chi-Square Test\ngroup_counts &lt;- c(170471, 171662)\np &lt;- c(0.50, 0.50)\nchisq.test(x = group_counts, p = p)\n#&gt; Chi-squared test for given probabilities\n#&gt; data: group_counts\n#&gt; X-squared = 4.146, df = 1, p-value = 0.04173\n\nP-values &lt; 0.01 indicate a SRM\n\nFor this use case, using 0.05 results in too many false positives (see article)\nWe expect the type I error rate (FPR) to be less than 1%\n\n**Be aware that the more you test, the greater probability of getting a false positive**\n\nThe probability of obtaining a false positive using a Chi-squared test configured at the 0.05 level can increase up to 0.14 with as few as five usages\nShould apply a multiple-testing correction to the p-value (e.g.Â bonferoni)\n\nSee Romano-Wolf correction\n\nSee Statistical Concepts &gt;&gt; Null Hypothesis Significance Testing (NHST) &gt;&gt; Romano and Wolfâ€™s correction\nSimilar to Westfall-Young but less restrictive\n\n\n\nTest counts with the sequential SRM test\n\nBayesian method\nSee github for code, blog and paper links"
  },
  {
    "objectID": "qmd/experiments-a_b-testing.html#sec-exp-ab-pb",
    "href": "qmd/experiments-a_b-testing.html#sec-exp-ab-pb",
    "title": "25Â  A/B Testing",
    "section": "25.6 Potential Biases",
    "text": "25.6 Potential Biases\n\nAlso see\n\nExperiements, RCT &gt;&gt; Sources of Bias\nExperiments, Planning &gt;&gt; Misc\n\nThese biases can cause violation in assumptions for difference-in-means post-hoc tests (e.g.Â Wilcoxon signed-rank test, Welchâ€™s t-test)\n\nIndependence of sampling between groups (e.g.Â Interference between variants, see below)\n\nassigning samples to groups (e.g.Â treatment/control) at the user level, rather than the visit level, can ensure there is no cross-pollution of effects between groups if the user can see both versions.\n\nIndependence and identical distribution (iid) of sampling within groups (e.g.Â treatment/control)\n\nCondorcet Voting Paradox (article)\n\nWhen testing more than 1 treatment sequencially in binary testing scenarios, order of testing may determine the outcome\nCustomer segments or groups may not have identical transitive preferences\n\nExample: Customer Segment preferences for treatments X, Y, Z are different (all segments have equal sample sizes)\n\nSegment 1: X &gt; Y &gt; Z\nSegment 2: Y &gt; Z &gt; X\nSegment 3: Z &gt; X &gt; Y\n\nScenario 1: Test X against Y in an A/B test, then test the winner against Z\n\n1st rd A/B testing: Segments 1 and 3 prefer X to Y, so X wins the first round of testing.\n2nd rd A/B testing: Segments 2 and 3 prefer Z to X, so Z wins is the overall winner\n\nScenario 2: Test Y against Z in an A/B test, then test the winner against X\n\n1st rd A/B testing: Segments 1 and 2 prefer Y to Z\n2nd rd A/B testing: Segments 1 and 3 prefer X to Y, so X is the overall winner.\n\n\nSuppose the three segments represent 45%, 35%, and 20% of the market respectively. We can still have any option be the final winner, depending on the order of testing.\n\nBut now some tests are better than others. If we tested all three options at once in an A/B/C test, weâ€™d learn that a plurality of the market prefers X, and weâ€™d learn that there is no option that the market as a whole prefers.\n\n\nPrimacy or Novelty effect\n\nAfter a short time period after implementation, the effect measured in the A/B experiement sharply degrades or grows.\nPrimacy Effect: A new feature is implemented and subgroup of users are reluctant to change. This subgroup may initially use the product less but eventually return to normal usage or ramp up their usage or potentially churn\nNovelty Effect: A new feature is implemented and a subgroup of users who are excited by change begin to use the product more often. This subgroup may initially use the product more but eventually ramp down their usage.\nSince these affects shouldnâ€™t effect first time users, either:\n\nConduct the A/B experiment on first time users, or\nAfter the experiment, investigate by comparing effects of first time users to returning users\n\nA long-run holdout (keeping a small control group for a long period of time) is one way to help capture this over a longer period of time\n\nInterference between variants\n\nCan happen when multiple variants of a feature are being test at the same time\nAssumption of user independence is violated\nCase 1: Social Media\n\nUsers within the same social network influence each other\nIf one user posts in their social media about using a product, then that influences the usage of another user in that personâ€™s social network. If one person is in the experimental group and the other is in the control group, then the experiment is biased\nThis bias dampens the measured effect: actual effect &gt; measured effect\nSolutions:\n\nCreate network clusters and then randomize by cluster\nEgo-network randomization (linkedin, paper)\n\nmore scalable than randomizing by network clusters\nfocal node known as the Ego, and the nodes to whom ego is directly connected to, called Alters, with edges showing links between ego to altars or between altars\nA network effect is said to take place when a new feature not only impacts the people who receive it, but also other users of the platform, like their connections or the people who follow them\n\n\n\nCase 2: Two-sided markets\n\nCompany resources (e.g.Â people) are finite. Therefore if one variant is popular, then it will consume more company resources which steals resources away from other variant groups.\nUber has a finite amount of drivers. If a feature variant becomes popular before other variants, then more users in that variant group will request more rides which means more drivers will be occupied by that variant group. This reduces driver availability for other variant groups which inflates the effect for first variant to acquire an outsized proportion of the companyâ€™s resources.\nThis bias inflates the measured effect for some variants and dampens them in others.\nSolutions:\n\nGeo-based randomization\n\nCompany resources are usually allocated by region. So by splitting variant groups according to region, resources from one region canâ€™t be leached by another region and therefore one feature variant canâ€™t leach resources from another variant\n\nTime-based randomization\n\nSelect a day of the week for each variant to be ran. Since Tuesdayâ€™s resources canâ€™t be used on Wednesday, a variant canâ€™t leach resources from another variant\nBest used when treatment effect is short-lived (e.g.Â surge pricing, not referral programs)\n\n\n\n\nType I error (aka false positive): the models perform equally well, but the A/B test still produces a statistically significant result. As a consequence, you may roll out a new model that doesnâ€™t really perform better. Itâ€™s a false positive. You can control the prevalence of this type of error with the p-value threshold. If your p-value threshold is 0.05, then you can expect a Type I error in about 1 in 20 experiments, but if itâ€™s 0.01, then you only expect a Type I error in only about 1 in 100 experiments. The lower your p-value threshold, the fewer Type I errors you can expect.\nType II error (aka false negative): the new model is in fact better, but the A/B test result is not statistically significant. In statistical terms, your test is underpowered, and you should either collect more data, choose a more sensitive metric, or test on a population thatâ€™s more sensitive to the change.\nType S error (sign error): the A/B test shows that the new model is significantly better than the existing model, but in fact the new model is worse, and the test result is just a statistical fluke. This is the worst kind of error, as you may roll out a worse model into production which may hurt the business metrics.\nType M error: (magnitude error): the A/B test shows a much bigger performance boost from the new model than it can really provide, so youâ€™ll over-estimate the impact that your new model will have on your business metrics.\nRandom assignment may fail to distribute power users equally: If your user population contains a few users that create a large amount of user activity, then a random A/B assignment is not guaranteed to distribute these users equally. This may violate the identity assumption and make the test results mode difficult to interpret.\nTreatment self-selection: If users themselves can opt into a treatment group, the A/B test violates the identity assumption: the two groups are not identical, but instead the treatment group consists of a particular subset of users, namely those more willing to take part in experiments. In that case we will not be able to tell whether the difference between the groups is due to the treatment or due to the sample differences."
  },
  {
    "objectID": "qmd/experiments-a_b-testing.html#sec-exp-ab-met",
    "href": "qmd/experiments-a_b-testing.html#sec-exp-ab-met",
    "title": "25Â  A/B Testing",
    "section": "25.7 Metrics",
    "text": "25.7 Metrics\n\nAlso see\n\nGoogle, Analytics, Reports\nMarketing\nExperiments, Analysis &gt;&gt; A/B Post-Hoc Analysis\n\nMost A/B tests should be based on user-based metrics, ideally Average Revenue per User. Having user-based instead of session-based metrics means that:\n\nThe results are much less ambiguous in interpretation and can have a much greater impact on decision-making.\nUnlike metrics based on sessions or page views, user-based metrics are as close as possible to independent observations which is a crucial assumption in many statistical tests, thus promoting statistical validity.\n\nFocus on â€˜fast-twitchâ€™ metrics, such as those measuring conversion rates or user engagement: this might be a difference between the two groups you can meaningfully measure (and that you know in the long-run is correlated with driving more revenue or more users)\n\nrevenue or the number of users are likely long-run metrics and due to the experiment (e.g.Â changing the color scheme of your website), it may not possible to produce a measurable effect for one experiment using those metrics\n\nLook at the historical movements of any metric to understand how noisy it is, if it is impacted by seasonality etc.\nUtilize guard-rail metrics to make sure that if your experiment metric is positive that it isnâ€™t similtaneously driving down another important metric.\n\nExample: Recommendation module change cannabalizes usage of other tools\n\nA typical A/B test of a recommendation module commonly involves the change in the underlying machine learning algorithm, its user interface, or both.Â  The recommendation change significantly increased usersâ€™ clicks on the recommendation while significantly decreasing usersâ€™ clicks on organic search results.\n\n\nGross Merchandise Sales (GMS)\n\nThere is an intuitive explanation to the drop in search clicks: users might not need to search as much as usual because they could find what they were looking for through recommendations.Â  In other words, improved recommendations effectively diverted usersâ€™ attention away from search and thus cannibalized the user engagement in search.\n\n\nBe careful of divisor metrics\n\nExample: You might see the average daily engagements per active user go up in the variant group, but if the number of users who are actually active in the variant group drops over the testing period because they donâ€™t like the feature, you have a bias that you are only measuring the improvement from the users who continue to remain active Donâ€™t test too many metrics (multiple-testing problem), be selective\nUsing a correction (e.g.Â Bonferroni) also reduces the power of your experiment\n\nSee Romano-Wolf Correction\n\nSee Statistical Concepts &gt;&gt; Null Hypothesis Significance Testing (NHST) &gt;&gt; Romano and Wolfâ€™s correction\nSimilar to Westfall-Young but less restrictive\n\n\nThe correction method should be based on the False Discover Rate (article)"
  },
  {
    "objectID": "qmd/experiments-a_b-testing.html#sec-exp-ab-interlv",
    "href": "qmd/experiments-a_b-testing.html#sec-exp-ab-interlv",
    "title": "25Â  A/B Testing",
    "section": "25.8 Interleaving Experiments",
    "text": "25.8 Interleaving Experiments\n\nExperiments where you present to each user both the control and the treatment, and see which version they prefer\nBecause each user gets to directly select from the control and the treatment, we should get test results sooner compared to traditional A/B testing with two populations\n\nNetflix report that they need 100X fewer users to achieve 95% experimental power (the equivalent of recall in an A/B experiment) compared to traditional, population-based A/B testing.\n\nExample: Team-Draft Interleaving for recommender models\n\n\nThe recommendations shown to the user are a mix of the results from model A and model B:\n\nThe two models simply take turns contributing their highest ranked recommendatiion that is not yet in the interleaved list\nThe model that gets to pick first is selected by a coin flip"
  },
  {
    "objectID": "qmd/experiments-a_b-testing.html#sec-exp-ab-univhold",
    "href": "qmd/experiments-a_b-testing.html#sec-exp-ab-univhold",
    "title": "25Â  A/B Testing",
    "section": "25.9 Universal Hold-Out",
    "text": "25.9 Universal Hold-Out\n\nRandomly sampled group that isnâ€™t exposed to any treatments during a period where multiple experiments are taking place\nNotes from: Universal Holdout Groups at Disney Streaming\nBenefits\n\nDetermine accurate lifts of cumulative product change efforts (i.e.Â total treatment effect from all experiments)\nVerify if changes made have lasting impact, or are ephemeral (e.g.Â novelty effect).\nObserve potential long-term changes in metrics that the core product typically canâ€™t influence with a single change, such as retention.\nInnovate faster by running more experiments simultaneously on fewer users; leave it to the universal holdout to evaluate the lift of the winners.\n\nHow long should we run each holdout for?\n\nIf too long:\n\nIncreased engineering costs of maintaining two separate experiences for every experimental change\nPotential negative impact for non-exposed customers\n\nIf too short:\n\nCannot assess the long-term impact of product changes\n\nExample: Disney Streaming/Hulu resets the universal holdout group every 3 months (i.e.Â 1 quarter)\n\nPower analysis\n\nDecide on the size of the effect size you want to be able to detect\n\nExample: A 1% change is large enough to drive a financially meaningful ad revenue impact for Hulu.\n\n\nHoldout then evaluate\n\n\nSteps\n\nFor the first three months â€” the â€œenrollment periodâ€ â€” we actively sample a fixed percentage of visiting users into the experiment.\n\nâ€œinto the experimentâ€ - Is the experiment stage?\n\nFor the fourth month â€” the â€œevaluation periodâ€ â€” we stop sampling users into the experiment, and assess the impact of changes over the course of one month\n\nRun a standard A/B experiment for 1â€“3 weeks. (dunno if this belongs here or not)\nRelease the product change to all users who are not in the universal holdout group (if the results from stage one are positive).\n\n\nThis doesnâ€™t make sense to me (could be the chart)\n\nSooo, does this mean that they arenâ€™t actually collecting any data for the first 3 months? Itâ€™s just picking people?\nSupposedly Disney resets their holdout group every 3 months, but in the chart itâ€™s extended to 4 months\n\nThey would analyze treatment effects during the 4th month of the holdout vs treatment\nThis makes more sense to me, but I donâ€™t think this is whatâ€™s happening\n\nIf the chart is wrong and the holdout is for 3 months then reset, then is the chart saying thereâ€™s no control group for that particular experiment during the evaluation period?\nThe evaluation procedure would clear some things up."
  },
  {
    "objectID": "qmd/experiments-a_b-testing.html#sec-exp-ab-tandr",
    "href": "qmd/experiments-a_b-testing.html#sec-exp-ab-tandr",
    "title": "25Â  A/B Testing",
    "section": "25.10 Test and Roll",
    "text": "25.10 Test and Roll\n\nFrom Test & Roll: Profit-Maximizing A/B Tests\nBayesian, small sample test design\n\nThink this is an extension/improvement upon Thompson Sampling (see below, Multi-Armed Bandit Algorithms &gt;&gt; Thompson Sampling)\n\nWhile these tests have traditionally been analyzed using hypothesis testing, we re-frame them as an explicit trade-off between the opportunity cost of the test (where some customers receive a sub-optimal treatment) and the potential losses associated with deploying a sub-optimal treatment to the remainder of the population.\na closed-form expression is derived for the profit-maximizing test size and show that it is substantially smaller than typically recommended for a hypothesis test, particularly when the response is noisy or when the total population is small.\nThe common practice of using small holdout groups can be rationalized by asymmetric priors. The proposed test design achieves nearly the same expected regret as the flexible, yet harder-to-implement multi-armed bandit under a wide range of conditions.\nWe [Feit and Berman] demonstrate the benefits of the method in three different marketing contextsâ€”website design, display advertising and catalog testsâ€”in which we estimate priors from past data. In all three cases, the optimal sample sizes are substantially smaller than for a traditional hypothesis test, resulting in higher profit."
  },
  {
    "objectID": "qmd/experiments-a_b-testing.html#sec-exp-ab-mbandit",
    "href": "qmd/experiments-a_b-testing.html#sec-exp-ab-mbandit",
    "title": "25Â  A/B Testing",
    "section": "25.11 Multi-Armed Bandit (MAB) Algorithms",
    "text": "25.11 Multi-Armed Bandit (MAB) Algorithms\n\nCompeting ad designs are viewed as different slot machines each with its own rate of success (conversion). We want to find the slot machine with the best rate and then keep pulling its arm.\nMisc\n\nNotes from\n\nhttps://www.inwt-statistics.com/read-blog/multi-armed-bandits-as-an-a-b-testing-solution.html\n\nPackages\n\n{contextual}\n\n\nA/B Experiment Issues\n\nLarge experimentation costs. Because all the competing treatments in the A/B test are guaranteed a fixed portion of the sample size, even a â€œbadâ€ treatment can be exposed to a significant amount of users and it could be hurtful to the user experience. A longer experiment means even larger experimentation costs.\nProne to erroneous decisions if not analyzed correctly. The A/B tests are designed to be analyzed only when the targeted sample size is reached. But inexperienced and impatient experimenters are often inclined to peek at results and make decisions before the experiments, which could lead to erroneous conclusions.\n\nMAB Benefits A MAB algorithm will provide a principled way to iteratively adjust the assignment ratio throughout the experiment until the best treatment receives the majority of the sample.\n\nMAB has the advantage of reducing the opportunity costs from the experimentation and is immune to peeking.\n\nEpsilon Greedy (Îµ-greedy)\n\nAfter an initial number of purely exploratory trials, a random lever is pulled a fraction Îµ of the time. The rest of the time (1 - Îµ), the lever with the highest known payoff is pulled.\n\nExample: if we set Îµ to 0.10, 10% of the time the algorithm will explore random alternatives, and 90% of the time it will exploit the variant that is performing the best.\n\nWe can expand on the Îµ-greedy algorithm by having it reduce the value of Îµ over time, thus limiting the possibility of continuing to explore once weâ€™re aware of each leverâ€™s payoff. This is referred to as Decayed Epsilon Greedy.\nThe disadvantage of exploring alternatives randomly is it is possible to run into bad actions that you have already run into before. (solution: UCB)\n\nUpper Confidence Bounds (UCB)\n\nAssumes that the unknown payoff of each lever is as high as the observable data indicate is possible. UCB initially explores randomly to try to get an understanding of the true probability of a lever being successful. Then, the algorithm adds an â€œexploration bonusâ€ to levers it is unsure about, prioritizing those until it becomes more sure about the leverâ€™s performance. This exploration bonus decreases over each round, similar to the Decayed Epsilon Greedy in that respect.\n\nExample: A person using this algorithm would likely choose one of the restaurants where his reviews have been very different between his different visits. If he has already been to a restaurant four times and the ratings have always been relatively the same non-optimal result, it is very unlikely to be chosen since going to this restaurant for a fifth time would likely produce the same results.\n\nEquations\n\\[\n\\begin{align}\n&a_t^{\\text{UCB}} = \\text{argmax}_{a \\in A} \\hat Q_t(a) + U_t(a)\\\\\n&\\text{where}\\;\\; U_t(a) = \\sqrt{\\frac{2\\log t}{N_t(a)}}\n\\end{align}\n\\]\n\n\\(a\\) - Action\n\\(N_t(a)\\) - The number of times a has been performed at time, t\n\\(\\hat Q_t(a)\\) - ?\n\\(U_t(a)\\) - Exploration bonus at time, t?\nThe more the same action is performed, the more \\(N_t(a)\\) increases and therefore \\(U_t(a)\\) decreases which results in at being less likely to be chosen.\n\n\nThompson Sampling\n\nIt is also possible that a winning lever may initially appear weak, and thus not be explored by a â€œgreedyâ€ algorithm sufficiently enough to determine its true payoff. Thompson Sampling is a Bayesian, non-greedy alternative. In this algorithm, a probability distribution of the true success rate is built for each variant based on results that have already been observed. For each new trial, the algorithm chooses the variant based on a sample of its posterior probability of having the best payout. The algorithm learns from this, ultimately bringing the sampled success rates closer to the true rate."
  },
  {
    "objectID": "qmd/experiments-analysis.html#sec-exp-anal-misc",
    "href": "qmd/experiments-analysis.html#sec-exp-anal-misc",
    "title": "26Â  Analysis",
    "section": "26.1 Misc",
    "text": "26.1 Misc\n\nAlso see Post-Hoc Analysis, ANOVA &gt;&gt; ANCOVA\nRecommended metrics to be reported for medical studies (Harrell). Some of this is perhaps generalizable to any RCT with a binary outcome.\n\nThe distribution of Risk Difference (RD)\ncovariate-adjusted OR\nadjusted marginal RD (mean personalized predicted risk as if all patients were on treatment A minus mean predicted risk as if all patients were on treatment B) (emmeans?)\nmedian RD\n\nAnalysis of a two armed trial comparing a treatment to placebo:\n\nQuestions\n\nWas there a treatment effect in this trial?\nWhat was the ATE of this trial?\nWas the treatment effect identical for all patients in the trial?\nWhat was the treatment effect for the different subgroups of the trial?\nWhat will the treatment effect be when used more generally (outside the trial)?\n\nStrategies\n\n(Predictive) adjustment variables (e.g.Â Age) are good to help answer Q1 & Q2\nInteractions (group_cat â¨¯ treatment) could help answer Q3 and Q4.\nQ5 is about external validity (see Diagnostics, Classification &gt;&gt; Terms)\n\n\nAnalysis shouldnâ€™t only include Intent-to-Treat effects\n\nTo adequately guide decision making by all stakeholders, report estimates of both the intention-to-treat effect and the per-protocol effect, as well as methods and key conditions underlying the estimation procedures.\nâ€œIntent-to-treat analysis makes sense from a public health point of view if it closely reflects the actual medical practice. But from a patient point of view of making a decision regarding treatment, the actual treatment is more meaningful than intent-to-treat. So, when the two estimates differ considerably, it seems to me that they should both be reported â€“ or, at least, the data should be provided that would allow both analyses to be done.â€ Lehman from Gelman blog post\nâ€œThe correct model models the 4 groups and the conditional probability to be in the 4 groups as a function of pre-treatment covariates, including both psychological covariates, cultural covariates, as well as symptomatic and disease progression knowledge.â€ Lakeland\n\nAnalysis of substudies must be separate\n\nExample of Study Effects: 2 Studies with the same treatment (Lumiracoxib) but different controls (ibuprofen, naproxen) (article)\n\nCells are p-values\nBottom table shows when a â€œSub-studyâ€ indicator is used as a model term (â€œTreatment given Sub-studyâ€) vs not used (â€œTreatmentâ€), the p-values for all the â€œDemographic Characteristicâ€ variables lose significance."
  },
  {
    "objectID": "qmd/experiments-analysis.html#sec-exp-anal-comp",
    "href": "qmd/experiments-analysis.html#sec-exp-anal-comp",
    "title": "26Â  Analysis",
    "section": "26.2 Compliance",
    "text": "26.2 Compliance\n\nRefers to how observations respond to treatment.\nFull compliance with treatment means that all units to whom a program (i.e.Â treatment) has been offered actually enroll, and none of the control (aka comparison) units receive the program\nExclusion of some randomized subjects can be justified (such as patients who were deemed ineligible after randomization or certain patients who never started treatment) when analyzing the results of an experiment.\nTypes\n\nAlways-Takers:Â A person who receives the treatment regardless of the assignment (or instrument in IV).\nNever-Takers: A person who never receives the treatment regardless of the assignment (or instrument in IV).\nCompliers: A person who receives the treatment only when assigned to the treatment group (or when the instrument compels them to).\nDefiers: A person who receives the treatment only when they are NOT assigned to the treament group (or the instrument does not compel them to). They always do the opposite of what the assignment mechanism instructs them to do (assholes). Usually a reasonable assumption to make that there are none in your (quasi-) experiment, but it can only be made based on intuition\n\nExample: receiving additional years of Education is the treatment; whether an educational Reform was implemented in their region is the instrument\n\nA person who decides to stay in school regardless of whether thereâ€™s a government policy to do so would be an Always-Taker (EDUCATION=1, regardless of REFORM )\nCompliance Modeling (Lakeland thread)\n\nModel how people make the decision to adhere to the randomization or not (â€œPerson makes decision about actual treatmentâ€ step)\n\nRealistic Model: Enroll person \\(\\rightarrow\\) Randomize to treatment group \\(\\rightarrow\\) Person makes decision about actual treatment \\(\\rightarrow\\) Actual treatment occurs \\(\\rightarrow\\) Outcomes observed\n\nITT effect by itself is misleading\n\nSuppose you did a mouse experiment and randomized mice to different surgeries and then told the surgeons to do the surgery they were randomized to unless they see conditions A,B,C in which case do a different surgery, and then analyzed via intention to treat. That would just be disingenuous. The fact that we donâ€™t know a precise rule for adherence doesnâ€™t mean adherence isnâ€™t part of the question. So build a probabilistic rule for adherence\n\nHighlights a source of uncertainty which should then increase the uncertainty in the final analysis, but would not be included in the ITT or any other frequentist analysis\nExample: Prostate Cancer\n\nPretreatment Survey\n\nYouâ€™re trying to understand the patientâ€™s state-of-mind.\nQuestions:\n\nWhat is your age\nWhat is your biological sex\nWhat race do you identify as (set of choices)\nWhat religious affiliation do you affiliate most with â€¦ (set of choices)\nOn the following scale rate the discomfort that your condition causesâ€¦ 0,1,2,3,4,5,6 with some text descriptors from â€œnoneâ€ to â€œsevere discomfortâ€\nWhat is your education level (from some grade school to PhD, and a check mark for whether itâ€™s a biology/medicine related degree)\nCheck all that apply: what are the primary motivations for entering the study (things like â€œto get free treatmentâ€ and â€œto improve the state of knowledge about the diseaseâ€ and various other things)\nWhat is your income level?\nDo you have insurance that would cover this treatment outside the study? What is the level of coverage?\nWhat is your level of concern about having surgery? (from zero to â€œI am very nervous about having surgeryâ€)\nWhat is your level of concern about potential side effects (similar scale)\nWhich side effects are you concerned about (check boxes)\nWhat are your existing thoughts about watchful waiting (0 I believe it is not right for me, up to 6 watchful waiting is my currently preferred treatment)\nWhat is your current belief about the severity of your condition: similar\nWhat is your current belief about the aggressiveness of your condition: similar\nWhat is your current belief about metastatic tumors: 0â€¦ I have no reason to believeâ€¦ 6 â€¦ I have medical biopsy proving metastatic\n\n\nBayesian Procedure\n\nNow, hypothesize at least the *direction* that each of these affects the probability of compliance with surgery assignment, and with watchful waiting assignment.\nPlace a prior over coefficients of each one with bias towards the direction hypothesized in a logistic regression with nonlinear response on any variables that may seem appropriate.\nNow, hypothesize the direction with which some subset of these predicts actual aggressiveness of the underlying condition, for example using the patients own beliefs, using the reported level of symptoms, using info about metastatic condition, etc.\nPlace prior over coefficients in a hidden variable model for severityâ€¦ again with nonlinear response if necessary.\nCollect dat\nPosterior distribution of parametersâ€¦\n\nModel: adherence ~ Binomial(inv_logit(k * symptom_severity * assigned_monitoring + random_individual_factor))\n\nWith random_individual_factor having a group level prior distribution gets an estimate of the probability of adherence\nSo you can fit probability of good outcome based on adherence with symptom_severity informing, telling you something about whether outcomes are due to selection or severity."
  },
  {
    "objectID": "qmd/experiments-analysis.html#sec-exp-anal-effs",
    "href": "qmd/experiments-analysis.html#sec-exp-anal-effs",
    "title": "26Â  Analysis",
    "section": "26.3 Effects",
    "text": "26.3 Effects\n\nComparison\n\n\nATE is for the whole population (he shouldâ€™ve had the arrow coming out of the word, population, or the edge of the circle)\nCircle is split into half: treatment (upper left, yellow), control/comparison (bottom right, pink)\n\nATT is for treated (treated compliers, always takers, treated defiers)\nATU is for control/comparison (non-treated compliers, never takers, non-treated defiers)\n\nLATE is for treated compliers\nCATE is for a subset of the population (e.g.Â men)\n\nHeterogeneous Treatment Effect (HTE) - Also called differential treatment effect, includes difference of means, odds ratios, and Hazard ratios for time-to-event outcome vars\n\nAscertaining subpopulations for which a treatment is most beneficial (or harmful) is an important goal of many clinical trials.\nOutcome heterogeneity is due to wide distributions of baseline prognostic factors. When strong risk factors exist, there is hetergeneity in the outcome variable.\n\nSolution: add baseline predictors to your model that account for these strong risk factors.\n\nHeterogeneity of Treatment Effects - The degree to which different treatments have differential causal effects on each unit.\nExamples\n\nThe effect of attending college on earnings differs across students\nThe effect of a state-wide smoking ban on smoking rates varies across states\n\n\nIntention-to-Treat (ITT) - estimates the difference in outcomes between the units assigned to the treatment group and the units assigned to the control (aka comparison group in quasi-experiments) group, irrespective of whether the units assigned to the treatment group actually receive the treatment. An intention-to-treat analysis is not feasible if trial participants are lost to follow-up\n\nPotential solution: weighted average of the outcomes of participants and non-participants in the treatment group compared with the average outcome of the control group\nExample: Doctor tells everyone in a treatment group to go home and exercise for an hour per day and tell the control group nothing.\n\nAfter a month, if you just compare the difference in mean blood pressures between the two groups, you get the intention to treat estimator\nDoesnâ€™t tell you the causal effect of exercise on blood pressure, but the causal effect of telling people to exercise on blood pressure.\n\nThis estimate would be smaller than the treatment effect of exercise per se, as only a (small!) fraction of people in the treatment group would completely follow the treatment\n\n\n\nModified Intention-to-Treat (mITT) - No ineligible users. This applies to cases where we detect the ineligibility after assignment, but the eligibility criteria are based on factors that could have been known before the experiment. Hence, it should be safe to exclude the ineligible users after the fact\n\ne.g.Â bots and existing users should increase the observed effect size, but not change the preferred variant.\n\nModified Intention-to-Treat No Crossovers (mITTnc). If we have a mechanism to detect some crossovers, excluding them and comparing the results to the intention-to-treat analysis may uncover implementation bugs.\n\nCrossovers are users that experience both the treatment and control exposures or (unintentionally) more than one treatment\nItâ€™s worth noting that crossovers shouldnâ€™t occur in cases where we can uniquely identify users at all stages of the experiment â€“ it is a problem that is more likely to occur when dealing with anonymous users.\nAs such, and given the inability to detect all crossovers, A/B experiments should be avoided when users are highly motivated to cross over.\n\nExample: displaying different price levels based on anonymous and transient identifiers like cookies is often a bad idea.\n\n\nAverage Treatement Effect (ATE) - expected causal effect of the treatment across all individuals in the population\n\nOLS estimate, Yi = Î²0 + Î²1Xi + ui\n\nÎ²1 = ATE = E[Y |X = 1] âˆ’ E[Y |X = 0] = E[Î²1,i ] = Average effect of a unit change in X\n\nConditional Average Treatment Effect (CATE) - ATE for a subgroup\n\nCoefficient for an interaction (e.g.Â explanatory*treatment)\n\nAlso see Generalized Additive Models (GAM) &gt;&gt; Interactions\n\n\nAverage Treatment Effect on the Treated (ATT) - expected causal effect of the treatment for individuals in the treatment group ATT = E[Î´ | D = 1] = E[Y1 âˆ’ Y0 | D = 1] = E[Y1 | D = 1] âˆ’ E[Y0 | D = 1]\n\nwhere Î´: individual-level causal effect of the treatment and D is the treatment\nIn the ideal scenario of a randomized control trial (RCT) (commonly violated in observational studies), ATE equals ATT because we assume that:\n\nthe baseline of the treatment group equals the baseline of the control group (layman terms: people in the treatment group would do as bad as the control group if they were not treated) and\nthe treatment effect on the treated group equals the treatment effect on the control group (layman terms: people in the control group would do as good as the treatment group if they were treated).\n\nATT should be used instead of ATE when thereâ€™s extreme imbalance between covariate criteria of treated vs control/comparison groups (e.g.Â quasi-experiment)\n\n\nAlso see Econometrics, General &gt;&gt; Propensity Scoring\nOverlap plot or balance plot from video\n\n{cobalt} may provide a way generate these\ny-axis: count, x-axis: covariate, color: treatment\n\nThe range of x covered by blue (treatment) is much smaller than the range of x covered by red (control), therefore ATT might be a better choice of estimated effect\n\n\n\nLocal Average Treatment Effect (LATE) - applies when there is noncompliance in the treatment group, comparison group, or both simultaneously.\n\nIf there is noncompliance in both the treatment and comparison group, then the LATE estimate is valid only for those in the treatment group (who enrolled in the program; i.e.Â treated) and (who would have not enrolled had they been assigned to the control/comparison group).\nâ€œwho would have not enrolled had they been assigned to the comparison groupâ€ is a weird counterfactual\nâ€œLocalâ€ indicates that LATE is the average effect for the group known as compliers\nTreatment and Instrument are binary variables\n\nIV models still valid for treatments and instruments with more than 2 levels, but effect calculation is more complicated\n\nCalculation (always-takers and defiers are assumed not to exist)\n\nLATE = (avg potential outcome of compliers who do receive treatment) - (avg potential outcome of compliers who donâ€™t receive treatment)\nThe (avg potential outcome of compliers who donâ€™t receive treatment) has to be solved for.\n\nGiven that we know the proportions and outcomes for the compliers and never-takers in our treatment group, you can solve a simple equation for this quantity.\nSee video for details\n\nThink this is the primary estimate of an IV model as well (see Econometrics, General &gt;&gt; Instrumental Variables)\n\nTreatment-on-the-treated (ToT) is simply a LATE in the more specific case when there is noncompliance only in the treatment group. Estimates the difference in outcomes between the units that actually receive the treatment and the comparison group (Seems similar to ATT)\n\nPer-Protocol Effect (PPE) - the effect of receiving the assigned treatment strategies throughout the follow-up as specified in the study protocol\n\ni.e.Â the effect that would have been observed if all patients had adhered to the protocol of the RCT\nAlternative to the intention-to-treat effect that is not affected by the study-specific adherence to treatment\nValid estimation of the per-protocol effect in the presence of imperfect adherence generally requires untestable assumptions\nApproaches below are generally invalid to estimate the per-protocol effect. (G-estimation and instrumental variable methods can sometimes be used to estimate some form of per-protocol effects even in the presence of unmeasured confounders)\n\n(biased) Approaches:\n\nAs-Treated: Compare the outcomes of those who took treatment (A=1) and didnâ€™t take the the treatment (A=0) regardless of their assignment\n\nPr[Y=1|A=1] âˆ’ Pr[Y=1|A=0]\n\nPer-Protocol: Compare the outcomes of those who took treatment (A=1) among those assigned to Treatment (Z=1) to those who didnâ€™t take the treatment (A=0) among those assigned to Control (Z=0)\n\nPr[Y=1|A=1, Z=1] âˆ’ Pr[Y=1|A=0, Z=0].)"
  },
  {
    "objectID": "qmd/experiments-analysis.html#sec-exp-anal-late",
    "href": "qmd/experiments-analysis.html#sec-exp-anal-late",
    "title": "26Â  Analysis",
    "section": "26.4 Calculating LATE",
    "text": "26.4 Calculating LATE\n\nMisc\n\nNotes from https://chris-said.io/2021/03/28/youre-measuring-wrong/\n\nExperiment\n\n\nz is the treatment availability assignment\ncp is shorthand for complier, referring to people who complied with the instructions.\nYcp,z = 1 therefore represents the average outcome of the group that actually received treatment, since they were compliers (cp) who were randomly assigned to treatment availability (z=1).\nYz = 0 represents the average outcome of the control group (z=0).\n\nLocal Average Treatment Effect (LATE)\n\n\nThe LATE tells you how much the treatment affects the people who actually got treated\nÎ´cp = Ycp,z = 1 - Y0cp,z=1\nÎ´cp is the Local Average Treatment Effect (LATE), since it reflects the impact of the treatment on a particular subpopulation (subpopulation being the compliers who were treated)\nYcp,z = 1 is the average outcome for compliers who were treated\nY0cp,z=1 is the average outcome for compliers if they hypothetically werenâ€™t treated (counterfactual)\n\nUsing substitution and some algebra (see article above for details), the counterfactual part can be avoided and this equation becomes\n\n\nÏ€cp is the fraction of compliers\n\nBias within complier group\n\nGroupâ€™s counterfactual outcomes might be different from other groups.\n\nLATE accounts for that by correctly reporting the impact of the treatment relative to the counterfactual.\n\nThe treatment might be more effective in the complier group than in the never-taker group.\n\nThat bias is unescapable and is known as a heterogenous treatment effect. The way to deal with this bias is to acknowledge it transparently.\n\n\nIf treatment involves more than a single dose, and people can withdraw midway through the program.\n\nReport the Intention To Treat (ITT) metric, which is the impact of being assigned to treatment (Yz=1 âˆ’ Yz=0) rather than the impact of being treated."
  },
  {
    "objectID": "qmd/experiments-analysis.html#sec-exp-anal-csm",
    "href": "qmd/experiments-analysis.html#sec-exp-anal-csm",
    "title": "26Â  Analysis",
    "section": "26.5 Change Score Models",
    "text": "26.5 Change Score Models\n\nChange Scores - subtract the baseline value of the outcome from the value measured at the end of the study and use that difference for your statistical tests or models.\nMisc\n\nSee tutorial, https://github.com/CRFCSDAU/EH6126_data_analysis_tutorials/blob/master/Unit_1_Review/Change_scores/Change_scores.md\nReason: Randomization of participants will result in groups (e.g.Â treated/control) that are comparable â€œon averageâ€ over many hypothetical trials, at the end of the day, we just have the one trial that we actually ran. And for that one trial there really could be important differences between the groups at baseline that could lead to errors of inference (e.g.Â concluding the treatment is beneficial when it isnâ€™t).\nExample: a trial for a blood pressure medication that we hope will lower patientsâ€™ SBP values. So we set up the trial, recruit some patients and randomize them into two groups. Then we give one group the new medication we are testing, and the other gets standard-of-care. At the end of the study we compare the mean blood pressure of the two groups and find that the active group had a SBP that was 3 mmHg lower, on average, than the values seen in the control group. We might thus conclude that the treatment worked. However, what if it just so happened that the active group also had a similarly lower mean blood pressure (vs the other group) measured at baseline, before the intervention?\n\nExample: Change Score Model\n\nw1 &lt;- \n  glm(data = dw,\n      family = gaussian,\n      (post - pre) ~ 1 + tx)\nSpecification\n\nWhere\n\npost, pre are pre-treatment, post-treatment measurements of the outcome variable\ntx is the treatment indicator variable\nÎ²0: population mean for the change in the control group\n\neasier to interpret if â€œpreâ€ is mean centered\n\nÎ²1: parameter is the population level difference in pre/post change in the treatment group, compared to the control group.\n\nAlso a causal estimate for the average treatment effect (ATE) in the population, Ï„"
  },
  {
    "objectID": "qmd/experiments-analysis.html#sec-exp-anal-exmpls",
    "href": "qmd/experiments-analysis.html#sec-exp-anal-exmpls",
    "title": "26Â  Analysis",
    "section": "26.6 Examples",
    "text": "26.6 Examples\n\nExample: pretest-posttest between-person factorial design to compare competing theories on depression and suicidal behaviors. (link)\nExample: Gelman (general approach)\n\nNotes from: Article\nEDA\n\n\nVariables\n\nOutcome:\n\n(top) Absolute EEG (brainwaves) power; X-axis is frequency\n(bottom) log(Absolute EEG (brainwaves) power); X-axis is frequency\n\nBlue: Each control group patient\nRed: Each treatment group patient\n\nInterpretations\n\nRaw, z-scores, Relative to mean:\n\nNot logged: substantial overlap between Control and Treatment groups\nlog: Control and Treatment groups almost completely overlap\n\nGroup averages relative to the mean at each frequency:\n\nSo grouped by group, frequencyâ€“&gt; mutate(mean)?\nSubstantial differences\n\n\nCompare sample data with random-chance data\n\nKeep the same observations but randomly permute the treatment assignment variable and see what happens\nRepeat (e.g.Â 9 times)\n\n\n\n\nGroup averages: patterns in these random permutations donâ€™t look so different, either qualitatively or quantitatively, from what we saw from the actual comparison\n\nThe red line is on top most of the time and substantially separated from the blue line 3 out fo the 9 times.\n\n\n\n\nlog response if:\n\nall the measurements are positive\nSeems reasonable to start with proportional effect\n\ninclude pre-test brain activity as a predictor (baseline)\nfit y ~ z + x\n\ny = outcome (log brain activity),\nz = treatment indicator\nx = pre-treatment measure\n\nTry including interaction of x and z\nPlot y vs.Â x with blue dots for the controls (z=0) and red dots for the treated kids (z=1).\n\nExample: Soloman, RCT, 2 groups, unequal treatment schedule\n\n\nNotes from: Thread\nfirst three time points are baseline, mid-treatment, and immediately post-treatment. The last two are follow-ups\nNonlinear means indicate a GAM would be a good option\nSolution\n\nGAM: mgcv::gam(data, y ~ 1 + group + s(weeks, by = group, k = 4) + s(week, subject, bs = \"fs\", k = 4))\n\nrandom smooth for subjects not just a random intercept\nâ€œsubjectâ€ is a factor variable\nâ€œfsâ€ is a factor smooth spline\n\npredict(fit, exclude = \"s(weeks,subject)\")\n\ncomputes the population fitted lines\ngratia::smooths(model) will return the labels for all smooths in the model so you know what you need for the exclude call without having to call summary()\n\n\n\nExample: Randomized Complete Block Design (RCBD)\n\nnotes from https://www.r-bloggers.com/2020/12/accounting-for-the-experimental-design-in-linear-nonlinear-regression-analyses/\nData\nObs block trt\n1   2     B\n2   2     C\n3   2     A\n4   2     D\n5   2     E\n6   2     F\n7   1     B\n8   1     C\n9   1     E\n10  1     A\n11  1     F\n12  1     D\n13  3     D\n14  3     A\n15  3     C\n16  3     F\n17  3     B\n18  3     E\n19  4     A\n20  4     F\n21  4     B\n22  4     C\n23  4     D\n24  4     E\nFitting a linear model (eda: check scatterplot of outcome vs treatment)\ndo not model with block as a fixed effect\n\n\nmod.reg &lt;- lm(yield ~ block + density, data=dataset)\n\nassumes that the blocks produce an effect only on the intercept of the regression line, while the slope is unaffected\ndo model with block as a random effect (i.e.Â block effect may produce random fluctuations for both model parameters, intercept and slope)\n\nmodMix.1 &lt;- lme(yield ~ density, random = ~ density|block, data=dataset)\n# or equivalently\nmodMix.1 &lt;- lme(yield ~ density, random = list(block = pdSymm(~density)), data=dataset)\n## Linear mixed-effects model fit by REML\n##Â  Data: datasetÂ \n##Â  Â  Â  Â  AICÂ  Â  Â  BICÂ  Â  logLik\n##Â  340.9166 355.0569 -164.4583\n##Â \n## Random effects:\n##Â  Formula: ~density | block\n##Â  Structure: General positive-definite, Log-Cholesky parametrization\n##Â  Â  Â  Â  Â  Â  StdDevÂ  Â  CorrÂ \n## (Intercept) 3.16871858 (Intr)\n## densityÂ  Â  0.02255249 0.09Â \n## ResidualÂ  Â  1.38891957Â  Â  Â  Â \n##Â \n## Fixed effects: yield ~ densityÂ \n##Â  Â  Â  Â  Â  Â  Â  Â  Value Std.Error DFÂ  t-value p-value\n## (Intercept) 31.78987 1.0370844 69Â  30.65311Â  Â  Â  0\n## densityÂ  Â  -0.26744 0.0096629 69 -27.67704Â  Â  Â  0\n##Â  Correlation:Â \n##Â  Â  Â  Â  (Intr)\n## density -0.078\n##Â \n## Standardized Within-Group Residuals:\n##Â  Â  Â  Â  MinÂ  Â  Â  Â  Q1Â  Â  Â  Â  MedÂ  Â  Â  Â  Q3Â  Â  Â  Â  MaxÂ \n## -1.9923722 -0.5657555 -0.1997103Â  0.4961675Â  2.6699060Â \n##Â \n## Number of Observations: 80\n## Number of Groups: 10\n\nIf there is NOT a strong correlation between the slope (e.g.Â listed above as corr = 0.09 for density) and intercept (i.e.Â correlated random effects) in the Random Effects section of summary(modMix.1), try modeling with the random effects as independent\n\nmodMix.2 &lt;- lme(yield ~ density, random = list(block = pdDiag(~density)), data=dataset)\n\nâ€˜pdDiagâ€™ specifies a var-covar diagonal matrix, where covariances (off-diagonal terms) are constrained to 0\ncheck if the change made a significant difference (i.e.Â pval &lt; 0.05)\n\nanova(modMix.1, modMix.2)\n\nOther options include: either random intercept or random slope\n\n# Model with only random intercept\nmodMix.3 &lt;- lme(yield ~ density, random = list(block = ~1), data=dataset)\n# Alternative notation\n# random = ~ 1|block\n\n# Model with only random slope\nmodMix.4 &lt;- lme(yield ~ density, random = list(block = ~ density - 1), data=dataset)\n# Alternative notation\n# random = ~density - 1 | block\n\nFitting a nonlinear model\n\nlibrary(aomisc)\ndatasetG &lt;- groupedData(yieldLoss ~ 1|block, dataset)\nnlin.mix &lt;- nlme(yieldLoss ~ NLS.YL(density, i, A), data=datasetG,Â \nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  fixed = list(i ~ 1, A ~ 1),\nÂ  Â  Â  Â  Â  Â  random = i + A ~ 1|block)\n# or equivalently\nnlin.mix2 &lt;- nlme(yieldLoss ~ NLS.YL(density, i, A), data=datasetG,Â \nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  fixed = list(i ~ 1, A ~ 1),\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  random = pdSymm(list(i ~ 1, A ~ 1)))\n\n## Nonlinear mixed-effects model fit by maximum likelihood\n##Â  Model: yieldLoss ~ NLS.YL(density, i, A)Â \n##Â  Data: datasetGÂ \n##Â  Â  Â  Â  AICÂ  Â  Â  BICÂ  Â  logLik\n##Â  474.8225 491.5475 -231.4113\n##Â \n## Random effects:\n##Â  Formula: list(i ~ 1, A ~ 1)\n##Â  Level: block\n##Â  Structure: General positive-definite\n##Â  Â  Â  Â  Â  StdDevÂ  Â  CorrÂ \n## iÂ  Â  Â  Â  0.1112839 iÂ  Â \n## AÂ  Â  Â  Â  4.0466971 0.194\n## Residual 1.4142009Â  Â  Â \n##Â \n## Fixed effects: list(i ~ 1, A ~ 1)Â \n##Â  Â  Â  Value Std.ErrorÂ  DFÂ  t-value p-value\n## iÂ  1.23242Â  0.038225 104 32.24107Â  Â  Â  0\n## A 68.52068Â  1.945173 104 35.22600Â  Â  Â  0\n##Â  Correlation:Â \n##Â  iÂ  Â  Â \n## A -0.409\n##Â \n## Standardized Within-Group Residuals:\n##Â  Â  Â  Â  MinÂ  Â  Â  Â  Q1Â  Â  Â  Â  MedÂ  Â  Â  Â  Q3Â  Â  Â  Â  MaxÂ \n## -2.4414051 -0.7049356 -0.1805322Â  0.3385275Â  2.8787362Â \n##Â \n## Number of Observations: 120\n## Number of Groups: 15\n\nExclude correlation between random effects (0.194 above) if not substantial for a simpler model\n\nnlin.mix3 &lt;- nlme(yieldLoss ~ NLS.YL(density, i, A), data=datasetG,Â \nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  fixed = list(i ~ 1, A ~ 1),\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  random = pdDiag(list(i ~ 1, A ~ 1)))"
  },
  {
    "objectID": "qmd/experiments-designs.html#sec-exp-des-misc",
    "href": "qmd/experiments-designs.html#sec-exp-des-misc",
    "title": "27Â  Designs",
    "section": "27.1 Misc",
    "text": "27.1 Misc\n\nCausal Hierarchy\n\n\nPreference of experiment in measuring causality where RCT is the most desirable"
  },
  {
    "objectID": "qmd/experiments-designs.html#sec-exp-des-types",
    "href": "qmd/experiments-designs.html#sec-exp-des-types",
    "title": "27Â  Designs",
    "section": "27.2 Types",
    "text": "27.2 Types\n\nRCT\n\nCannot return valid causal estimates of the treatment effect at the participant level, but it can return a valid causal estimate of the average treatment effect (ATE), in the population\n\nApproaches for estimatingÂ  the ATE\n\nChange-Score model (see Experiments, RCT &gt;&gt; Change Score Model)\nANCOVA model (see ANOVA &gt;&gt; ANCOVA)\n\n\nTypical procedure\n\nrecruit participants from the target population,\nmeasure the outcome variable during a pre-treatment assessment,\nrandomly assign participants into\n\na control condition or\nan experimental treatment condition,\n\ntreat the participants in the treatment condition, and\nmeasure the outcome variable again at the conclusion of treatment.\n\nReasons for not running a RCT\n\nItâ€™s just not technically feasible to have individual-level randomization of users as we would in a classical A/B test\n\ne.g.Â randomizing which individuals see a billboard ad is not possible\n\nWe can randomize but expect interference between users assigned to different experiences, either through word-of-mouth, mass media, or even our own ranking systems; in short, the stable unit treatment value assumption (SUTVA) would be violated, biasing the results\n\n\nQuasi-Experiemental\n\nDue to the lack of a random assignment, the treatment and control groups are not equivalent before the intervention. So, any differences from these two groups could be caused by the pre-existing differences.\nExample\n\nRandomly choose some cities within which to show billboards and other cities to leave without.\nWe can look for changes in the test regions at-specific-times as compared to the control regions at-specific-times.\nSince random changes happen all the time, we need to look historically to figure out what kinds of changes are normal so we can identify the impact of our test.\nBecause groups of individuals are assigned based on location rather than assigning each individual at random, and without the individual randomization there is a much larger chance for imbalance due to skewness and heterogeneous differences.\n\nTypes\n\nDifference-in-Differences, Regression Discontinuity Design, Synthetic Control Method, Interrupted Time Series\n\n\nObservational\n\nTypes\n\nMatching, Propensity Score Matching, Propensity Score Stratification, Inverse Probability of Treatment Weighting, and Covariate Adjustment"
  },
  {
    "objectID": "qmd/experiments-designs.html#sec-exp-des-factdes",
    "href": "qmd/experiments-designs.html#sec-exp-des-factdes",
    "title": "27Â  Designs",
    "section": "27.3 Factorial Designs (aka Multifactorial Designs)",
    "text": "27.3 Factorial Designs (aka Multifactorial Designs)\n\nTwo or more independent variables that are qualitatively different\n\nEach has two or more levels\n\nNotation\n\nDescribed in terms of number of IVs and number of levels of each IV\nExample 2 X 2 X 3\n\n3 IVs\n\n2 with 2 levels and 1 with 3 levels\n\nResults in 12 conditions\n\n\nFlavors\n\nBetween-subjects: different subjects participating in each cell of the matrix\nWithin-subjects: the same subjects participating in each cell of the matrix\nMixed: a combination where one (or more) factor(s) is manipulated between subjects and another factor(s) is manipulated within subjects\n\nCombined/Expericorr\n\n\nIn this example both depressed and non-depressed categories are between-subjects & non-experimental\n\nI think experimental/non-experimental terminology is the same as manipulated/measured\n\nBelieve the no\nAn experimental design that includes one or more manipulated independent variables and one or more preexisting participant variables that are measured rather than manupulated\nSometimes participant continuous variables are dicotomized to keep a strict factorial design but this may bias the results by missing effects that are actually present or obtaining effects that are statistical artifacts. (Should just use multivariable regression instead)\n\nMedian-split procedure â€“ participants who score below the median on the participant variable are classified as low, and participants scoring above the median are classified as high\nExtreme groups procedure â€“ use only participants who score very high or low on the participant variable (such as lowest and highest 25%)\n\nUse cases\n\nDetermine whether effects of the independent variable generalize only to participants with particular characteristics\nExamine how personal characteristics relate to behavior under different experimental conditions\nReduce error variance by accounting for individual differences among participants"
  },
  {
    "objectID": "qmd/experiments-designs.html#sec-exp-des-obs",
    "href": "qmd/experiments-designs.html#sec-exp-des-obs",
    "title": "27Â  Designs",
    "section": "27.4 Observational",
    "text": "27.4 Observational\n\nMatching and Propensity Score Matching\nPropensity Score Stratification\nInverse Probability of Treatment Weighting\nCovariate Adjustment"
  },
  {
    "objectID": "qmd/experiments-designs.html#sec-exp-des-quasexp",
    "href": "qmd/experiments-designs.html#sec-exp-des-quasexp",
    "title": "27Â  Designs",
    "section": "27.5 Quasi-Experimental",
    "text": "27.5 Quasi-Experimental\n\nTypical Preconditions\n\nThe treated group looks like the control group (similarity for comparability);\nA sufficiently large number of observations within each group (a large n)\n\nRandomizing at the lowest level possible Notes from: Key Challenges with Quasi Experiments at Netflix\n\nDescription: RCTs require you to randomize similar units (e.g.Â individual people) into treatment and control groups. If this isnâ€™t possible at the individual level, then randomizing at the lowest level possible is the closest, next best thing.\nExample\n\nNetflix: Measure the impact of TV or billboard advertising on member engagement. It is impossible to have identical treatment and control groups at the member level as we cannot hold back individuals from such forms of advertising. Randomize our member base at the smallest possible level. For instance, TV advertising can be bought at TV media market level only in most countries. This usually involves groups of cities in closer geographic proximity.\n\nProblems\n\nsmall sample sizes\n\ne.g.Â If randomizing by geographical units, there are probably not too many of these\n\nhigh variation and uneven distributions in treatment and control groups due to heterogeneity across units\n\ne.g.Â London with its high population is randomly assigned to the treatment cell, and people in London love sci-fi much more than other cities. Londonâ€™s love for sci-fi would result in an overestimated effect.\n\n\nSolutions\n\nrepeated randomizations (aka re-randomization)\n\nkeep randomizing until we find a randomization that gives us the maximum desired level of balance on key variables across treatment cells\nSome problems still remain\n\nCan only simultaneously balance on a limited number of observed variables, and it is very difficult to find identical geographic units on all dimensions\nCan still face noisy results with large confidence intervals due to small sample size\n\n\nImplement designs involving multiple interventions in each treatment cell over an extended period of time whenever possible (i.e.Â instead of a typical experiment with single intervention period).\n\nThis can help us gather enough evidence to run a well-powered experiment even with a very small sample size. Large amounts of data per treatment cell increases the power of the experiment.\n\nUse a Bayesian Dynamic Linear Model (DLM) to estimate the treatment effect\n\nuses a multivariate structure to analyze more than a single point-in-time intervention in a single region.\ndlm PKG (see bkmks)"
  },
  {
    "objectID": "qmd/experiments-designs.html#sec-exp-des-rcbd",
    "href": "qmd/experiments-designs.html#sec-exp-des-rcbd",
    "title": "27Â  Designs",
    "section": "27.6 Randomized Complete Block Design (RCBD)",
    "text": "27.6 Randomized Complete Block Design (RCBD)\n\nNotes from https://www.r-bloggers.com/2020/12/accounting-for-the-experimental-design-in-linear-nonlinear-regression-analyses/\nAlso see\n\nExperiments, Analysis &gt;&gt; Examples for an analysis example of RCBD\n\nThe defining feature is that each block sees each treatment exactly once\nRunning a linear regression analysis without taking into account the correlation within blocks\n\nAny block-to-block variability goes into the residual error term, which is, therefore, inflated.\nTaking the mea\n\nAdvantages\n\nGenerally more precise than the completely randomized design (CRD).\nNo restriction on the number of treatments or replicates.\nSome treatments may be replicated more times than others.\nMissing plots are easily estimated.\n\nDisadvantages\n\nError degrees of freedom is smaller than that for the CRD (problem with a small number of treatments).\nLarge variation between experimental units within a block may result in a large error term\nIf there are missing data, a RCBD experiment may be less efficient than a CRD\n\nSteps\n\nChoose the number of blocks (minimum 2) â€“ e.g.Â 4\n\nThe number of blocks is the number of â€œreplicationsâ€\n\nChoose treatments (assign numbers or letters for each) â€“ e.g.Â 6 trt â€“ A,B, C, D, E, F\n\nTreatments are assigned at random within blocks of adjacent subjects, each treatment once per block.\nAny treatment can be adjacent to any other treatment, but not to the same treatment within the block\n\nRandomize the treatments and blocks\n\nExample\nObs block trt\n1   2     B\n2   2     C\n3   2     A\n4   2     D\n5   2     E\n6   2     F\n7   1     B\n8   1     C\n9   1     E\n10  1     A\n11  1     F\n12  1     D\n13  3     D\n14  3     A\n15  3     C\n16  3     F\n17  3     B\n18  3     E\n19  4     A\n20  4     F\n21  4     B\n22  4     C\n23  4     D\n24  4     E"
  },
  {
    "objectID": "qmd/experiments-designs.html#conjoint-analysis",
    "href": "qmd/experiments-designs.html#conjoint-analysis",
    "title": "27Â  Designs",
    "section": "27.7 Conjoint Analysis",
    "text": "27.7 Conjoint Analysis\n\nConjoint experiments are a special kind of randomized experiment where study participants are asked questions that have experimental manipulations. (wiki)\n\nThe objective of conjoint analysis is to determine what combination of a limited number of attributes is most influential on respondent choice or decision making.\nUnlike a standard randomized experiment where one feature of interest is manipulated (like in an A/B test), conjoint experiments are choose-your-own-adventure randomized experiments.\nParticipants are presented with 2+ possible options that have a variety of features with different levels in those features, and then theyâ€™re asked to choose one (for a binary outcome) or rate them on some sort of scale (for a continuous outcome).\n\nMisc\n\nNotes from\n\nThe ultimate practical guide to conjoint analysis with R\n\n\nExample: What effect do different political candidate characteristics have on the probability that a respondent would select that candidate (or on candidate favorability)?\n\nSurvey Question\n\n\n\n\nCandidate 1\nCandidate 2\n\n\n\n\nMilitary service\nDid not serve\nServed\n\n\nReligion\nNone\nMormon\n\n\nCollege\nState university\nIvy League university\n\n\nProfession\nLawyer\nBusiness owner\n\n\nGender\nFemale\nFemale\n\n\nIncome\n$54,000\n$92,000\n\n\nRace/Ethnicity\nWhite\nAsian American\n\n\nAge\n45\n68\n\n\n\nIf you had to choose between them, which of these two candidates would you vote for?\n\nCandidate 1\nCandidate 2\n\nChoice Attributes\n\n\n\n\n\n\n\nFeatures/Attributes\nLevels\n\n\n\n\nMilitary service\nServed, Did not serve\n\n\nReligion\nNone, Jewish, Catholic, Mainline protestant, Evangelical protestant, Mormon\n\n\nCollege\nNo BA, Baptist college, Community college, State university, Small college, Ivy League university\n\n\nProfession\nBusiness owner, Lawyer, Doctor, High school teacher, Farmer, Car dealer\n\n\nGender\nMale, Female\n\n\nIncome\n$32,000; $54,000; $65,000; $92,000; $210,000; $5,100,000\n\n\nRace/Ethnicity\nWhite, Native American, Black, Hispanic, Caucasian, Asian American\n\n\nAge\n36, 45, 52, 60, 68, 75\n\n\n\n\nEach of the eight attributes had different levels within them that respondents could possibly see.\n2 Ã— 6 Ã— 6 Ã— 6 Ã— 2 Ã— 6 Ã— 6 Ã— 6, or 186,624 different attribute combinations that very likely wonâ€™t all be used in a one setting."
  },
  {
    "objectID": "qmd/experiments-planning.html#sec-exp-plan-misc",
    "href": "qmd/experiments-planning.html#sec-exp-plan-misc",
    "title": "28Â  Planning",
    "section": "28.1 Misc",
    "text": "28.1 Misc\n\nIf youâ€™re going to analyzing the results of a test, ask to be involved in the planning stages. This well help insure that the test has usable results.\nSources of Bias\n\nAlso see\n\nExperiments, A/B Testing &gt;&gt; Potential Biases\nExperiements, RCT &gt;&gt; Sources of Bias\n\nSampling Bias - The probability distribution in the collected dataset deviates from its true natural distribution one would actually observe in the wilderness.\nSpectrum Bias - Whenever a distribution which a model has been trained with changes, e.g.Â due to spatial or temporal effects, the validity of this model expires. (model drift?)\n\nCheck randomization procedure by testing for pairwise associations between the treatment variable and the adjustment variables. If independence is rejected (pval &lt; 0.05), then randomization failed. (also see Experiments, A/B Testing &gt;&gt; Terms &gt;&gt; A/A Testing)\n\ntreatment vs continuous - 2 sample t-tests\ntreatment vs categorical - chisq test\n\n\nError\n\n\nThe false positive rate is closely associated with the â€œstatistical significanceâ€ of the observed difference in metric values between the treatment and control groups, which we measure using the p-value.\n\nFPR typically set to 5% (i.e.Â falsely conclude that there is a â€œstatistically significantâ€ difference 5% of the time)\n\nFalse negatives are closely related to the statistical concept of power, which gives the probability of a true positive given the experimental design and a true effect of a specific size\n\nPower = 1 - FNR"
  },
  {
    "objectID": "qmd/experiments-planning.html#sec-exp-plan-cons",
    "href": "qmd/experiments-planning.html#sec-exp-plan-cons",
    "title": "28Â  Planning",
    "section": "28.2 Considerations",
    "text": "28.2 Considerations\n\nMetrics\n\nIf using multiple metrics/KPIs, make sure that you and the product manager agree on which metric/KPI should be primary and which should be secondary.\n\nWhere do users get randomized? Can depend on the KPI youâ€™re measuring.\n\nApp or website login - appropriate for product purchasing\nA click on the first screen of the signup flow - appropriate for app subscriptions\n\nWill you only be testing a subset of your customers?\n\nExample: testing changes in one country or platform and apply the learnings from the test before releasing them to our remaining users\nMay affect the baseline KPI used to calculate the sample size\n\nExample: if a new feature is only going to be tested for English users on iOS the conversion rate may be different than the rate for all users on iOS. This also affects the number of users expected to enter the test because more users logged into iOS versus just English users.\n\n\nCalculate sample size\n\nMay take months to reach the sample size needed to determine statistical significance of a measured effect\n(approx) Sample Size\n\nSee Sample Size/Power/MDE\n\nIssues\n\ngetting more samples or running an experiment for a longer time to increase the sample size might not always be easy or feasible\n\nIf your sample size is large and therefore test duration is too long, you may need to change the metric/KPI youâ€™re measuring\n\nExample\n\nKPI: test whether new feature increased the percentage of new users that returned to the app 30 days after signup.\nThis meant the test needed to run an additional 30 days to ensure new users in the control didnâ€™t get exposed to the new feature within the 30-day engagement window we wanted to measure.\n\n\n\nDoes the time of year matter?\n\nIs there a seasonality aspect to your KPI, customer engagement, etc.?\n\nIf so, the treatment effect may differ depending on when the test is conducted\n\n\nMonitoring\n\nConfirm group/cohort proportions\n\nExample: If you have 3 treatments (aka variants) and 1 control, make sure each group has 25% of the test participants\nUnbalanced groups can result in violations of assumptions for the statistical tests used on the results\n\nTrack KPIs\n\nVery bad treatments could substantially affect KPIs negatively. So you need to pull the plug if your business starts to tank."
  },
  {
    "objectID": "qmd/experiments-planning.html#sec-exp-plan-smppow",
    "href": "qmd/experiments-planning.html#sec-exp-plan-smppow",
    "title": "28Â  Planning",
    "section": "28.3 Sample Size/Power/MDE",
    "text": "28.3 Sample Size/Power/MDE\n\n28.3.1 MIsc\n\nUnderpowered Experiments\n\nâ€œIn particular, if your data are noisy relative to the size of the effects you can reasonably expect to find, then itâ€™s a big mistake to use any sort of certainty thresholding (whether that be p-values, confidence intervals, posterior intervals, Bayes factors, or whatever) in your summary and reporting. That would be a disasterâ€”type M and S errors will kill you.\nSo, if you expect ahead of time that the study will be summarized by statistical significance or some similar thresholding, then I think itâ€™s a bad idea to do the underpowered study. But if you expect ahead of time that the raw data will be reported and that any summaries will be presented without selection, then the underpowered study is fine.â€ Gelman\n\n\n\n\n28.3.2 Approximate Sample Size\n\n80% Power\n\nn = 8 / (effect size^2)\n\nYou can substitute correlation (?) for effect size\n\nDifference between means of two groups\n\nn = 32 / (effect size^2)\n\nUsing variance\n\nn = (16* Ïƒ2) / Î´2\n\nÏƒ is variance of the data (outcome?)\nÎ´ is the effect size\n\n\n\n90% Power\n\nn = 11 / (effect size^2)\n\nBayesian\n\nFrom https://www.rdatagen.net/post/2021-06-01-bayesian-power-analysis/\nBayesian inference is agnostic to any pre-specified sample size and is not really affected by how frequently you look at the data along the way\nA bayesian power analysis to calculate a desired sample size entails using the posterior distribution probability threshold (or another criteria such as the variance of the posterior distribution or the length of the 95% credible interval)\n\nMinimum Detectable Effect (MDE) is proportional to 1/sqrt(sample_size)\nExample: Gelman (Confirming sample size of 126 has 80% power)\n\nAssumption: drug (binary treatment) increased survival rate by 25 percentage points (i.e.Â treatment effect)\n\nEvidently for a survival model, but Gelman uses standard z-test gaussian power calculation. So, I guess the survival model part doesnâ€™t matter.\n\nâ€œWith 126 people divided evenly in two groups, the standard error of the difference in proportions is bounded above by âˆš(0.5*0.5/63 + 0.5*0.5/63) = 0.089, so an effect of 0.25 is at least 2.8 standard errors from zero, which is the condition for 80% power for the z-test.â€\n\nSE for the difference in 2 proportions\n\n\nIn the example, the experiment is balanced so both the treatment and control groups have an equal number of participants (i.e.Â 63 in each group which is a 0.5 proportion of the total sample size)\n\n0.25 / 0.089 = 2.8 s.d. from 0\n\nGelmanâ€™s Explanation: â€œIf you have 80% power, then the underlying effect size for the main effect is 2.8 standard errors from zero. That is, the z-score has a mean of 2.8 and standard deviation of 1, and thereâ€™s an 80% chance that the z-score exceeds 1.96 (in R, pnorm(2.8, 1.96, 1, lower.tail = F) = 0.8).â€\n\nExplanation of the Explanation: â€œA two-tail hypothesis with a significance level of 0.05 are assumed. The right-tail critical value is 1.96. The power is the mass of the sampling distribution under the alternative to the right of this decision boundary. Then we want to find a Gaussian with a standard deviation of 1 so that 80% of its mass is to the right of 1.96. Then a mean of 2.8 gives the desired outcome.â€\nAlso see Notebook pg 95\n\n\n\n\n\n28.3.3 Increasing Power\n\nIncrease the expected magnitude of the effect size by:\n\nBeing bold vs incremental with the hypotheses you test.\nTesting in new areas of the product\n\nLikely more room for larger improvements in member satisfaction\n\n\nIncrease sample size\n\nAllocate more members (or other units) to the test\nReduce the number of test groups\n\nthere is a tradeoff between the sample size in each test and the number of non-overlapping tests that can be run at the same time.\n\n\nTest in groups where the effect is homogenous\n\nincreases power by effectively lowering the variability of the effect in the test population\nNetflix paper\nExample: Testing a feature that improves latency\n\ne.g.Â the delay between a member pressing play and video playback commencing\nLatency effects are likely to substantially differ across devices and types of internet connections\nSolution: run the test on a set of members that used similar devices with similar web connections\n\n\n\n\n\n28.3.4 {PUMP}\n\nFrequentist Multilevel Model Power/Sample Size/MDE Calculation\nMisc\n\ngithub, paper\n\nalso has vignettes and shiny app\n\nNotes from\n\nVideo useR conference 2022\n\nAssumes multi-test correction procedure (MTP) will occur\nBayesian calculation for this specification would be different\n\nFactors affecting power\n\nWith at least 1 outcome:\n\ndesign of the study; assumed model (type of regression)\nnbar, J, K: number of levels (e.g.Â students, schools)\n\nUnless block size differences are extreme, these should not affect power that much\n\nT: proportion of units treated\nnumber of covariates\n\nand R2, the proportion of variance that they explain\n\nICC: ratio of variance at a particular level (e.g.Â student, school) to overall variance\n\nUnique to multiple outcomes\n\nDefinitions of power\n\nChoose depends on how we define success\nTypes\n\nIndividual: probability of rejecting a particular H0\n\nthe one you learn in stats classes\n\n1-Minimal: probability of rejecting at least 1 H0\nD-Minimal: probability of rejecting at least D H0s\nComplete (Strictest): probability of rejecting all H0s\n\nNote: in the video, the presenter wasnâ€™t aware of any guidelines (e.g.Â 80% for Individual) for the different types of power definitions\n\nM: number of outcomes, tests\nrho: correlation between test statistics\nproportion of outcomes for which there truly are effects\nMultiple Testing Procedure (MTP)\n\n\nUses a simulation approach\n\nCalculate test statistics under alternative hypothesis\nUse these test stats to calculate p-values\nCalculate power using the distribution of p-values\n\nPUMP::pump_power\n\noptions\n\nExperiment\n\nLevels: 1, 2, or 3\nRandomization level: 1st , 2nd, or 3rd\n\nModel\n\nIntercepts: fixed or random\nTreatment Effects: constant, fixed, or random\n\nMTP\n\nBonferroni: simple, conservative\nHolm: less conservative for larger p-values than Bonferroni\nBenjamini-Hochberg: controls for the false discovery rate (less conservative)\nWestfall-Young\n\npermutation-based approach\ntakes into account correlation structure of outcomes\ncomputationally intensive\nNot overly conservative\n\nRomano-Wolf\n\nSee Statistical Concepts &gt;&gt; Null Hypothesis Significance Testing (NHST) &gt;&gt; Romano and Wolfâ€™s correction\nSimilar to Westfall-Young but less restrictive\n\n\n\nExample\n\nDescription\n\nOutcome: 3 level categorical\n2-level Block Design\n\nâ€œ2-levelâ€:Â  students within schools\nâ€œBlock Designâ€: treatment/control randomization of students occurs within each school\n\n\nPower calculation\n\n\nd_m is the code for the experimental design (assume these are listed in the documentation)\nMDES is a vector of the treatment effects for each of the 3 levels of the outcome\nSee â€œFactors affecting powerâ€ (above) for descriptions of some of these args.\n\nResults\n\n\nSee above for descriptions of the types of power (Factors affecting power &gt;&gt; Unique to multiple outcomes &gt;&gt; Definitions of Power)\nNone: w/o multi-test correction: 81% power\nBF: w/ Bonferroni (multiply p-values by number of outcomes): 67%\nD 1,2,3 are individual power for each of the 3 levels of the outcome\nmin 1, 2 = at least 1, 2 levelsÂ  of the outcome\ncomplete is for all 3 levels of the outcome (will always be lowest)\n\n\npump_mdes() calculates minimal detectable effect size (MDES)\npump_sample() calculates the sample size given target power (e.g.Â 0.80) and MDES\n\nSample Size Types\n\nK: number of level 3 units (e.g.Â school districts)\nJ: number of level 2 units (e.g.Â schools)\nnbar: number of level 1 units (e.g.Â students)\n\nExample\n\n\nResults\n\n\n\nObserve the sensitivity of power for different design parameter values\n\nExample\npgrid &lt;- update_grid(\nÂ  Â  pow,\nÂ  Â  # vary parameter values\nÂ  Â  rho = seq(0, 0.9, by = 0.1)\nÂ  Â  # compare multiple MTPs\nÂ  Â  MTP = c(\"BF\", \"HO\", \"WY-SS\", \"BH\")\n)\nplot(pgrid, var.vary = \"rho\")\n\n\nOutputs facetted multi-line plots with\n\ny = rho, y = power\nmultiple lines by MTP\nfacetted by power definition"
  },
  {
    "objectID": "qmd/experiments-planning.html#sec-exp-plan-olctn",
    "href": "qmd/experiments-planning.html#sec-exp-plan-olctn",
    "title": "28Â  Planning",
    "section": "28.4 Collection",
    "text": "28.4 Collection\n\nRecord data; donâ€™t calculate or transform it\n\nIf possible, store data as text or in text compatible format. (i.e.Â .csv, .tsv, or some other delimited file)\n\nSome other formats add trailing spaces, etc.\n\n\nBack up data\n\nMultiple places is recommended\n\nCurate Data Organization\n\nClean data with simple organization fosters its use and a shared understanding of procedures and analysis.\nObservations, cases, or units, etc. appear in rows\nvariables appear in columns\nvalues for observations on variables appear in the matrix of cells between them\nNesting structure (i.e.Â grouping variables) should appear in columns, not rows.\nBeware complicated row, column, or value labels.\n\nRow, column, or value labels with case sensitive characters, special characters, or whitespace cause problems in analytical software beyond the spreadsheet (they can be a problem within the spreadsheet as well)\nUse lower cases that fully denote the observation, variable, or label, unless data is used as-is.\nAvoid spaces.\nUse underscores rather than periods to indicate white space.\nAvoid special characters â€” â€œpercentâ€ or â€œpctâ€ is better than â€œ%.â€\n\n\nAll calculations should occur outside the data repository\n\n** keep an original, un-adulterated copy of the data in a separate sheet or file **\nCarrying calculations, summaries, and analysis within the data structure gets in the way of efficient updating.\nUpdating an analysis means merely updating the data set (again in the native form) called by the procedure if scripts and functions are well-documented.\nAutomating reporting and analysis is a big deal in both the public and private sectors.\n\nDo not summarize data during collection (unless the need is pressing)"
  },
  {
    "objectID": "qmd/experiments-planning.html#sec-exp-plan-peec",
    "href": "qmd/experiments-planning.html#sec-exp-plan-peec",
    "title": "28Â  Planning",
    "section": "28.5 Post-Experiment Evaluation Checklist",
    "text": "28.5 Post-Experiment Evaluation Checklist\n\nDid the test run long enough so that the sample size reached?\nAre treatment variants proportioned correctly?\nDid users get exposed to multiple treatment variants and how many?"
  },
  {
    "objectID": "qmd/experiments-planning.html#sec-exp-plan-dsvte",
    "href": "qmd/experiments-planning.html#sec-exp-plan-dsvte",
    "title": "28Â  Planning",
    "section": "28.6 Decreasing the Sampling Variance of the Treatment Effect",
    "text": "28.6 Decreasing the Sampling Variance of the Treatment Effect\n\n28.6.1 Misc\n\nNotes from Online Experiments Tricks â€” Variance Reduction\nAlternative to increasing power\nWinsorize ({DescTools::Winsorize}), dichotomizing, etc. metrics will help decrease the variance significantly, but introduce more bias\nCUPED is widely used and productionalized in tech companies and ML-based methods are often used to incorporate multiple covariates. (see below)\n\n\n\n28.6.2 Stratified Sampling\n\n\nSee Surveys, Sampling Methods &gt;&gt; Probabilistic Sampling Methods &gt;&gt; Stratified Sampling\nPro - Provides an unbiased estimate of the treatment effect and effectively removes the between-strata variance\nCon - Very hard to implement stratified sampling before experiments\n\n\n\n28.6.3 Post-Stratification\n\n\nPost-stratification randomly samples the population first and then places individuals into strata.\nThe Effect is measured as a difference in means between treated and untreated\nSteps\n\nRandomly sample population then allocate individuals into strata\nRandomly assign treatment to all individuals all together\n\nShe didnâ€™t do the assignment per strata which Iâ€™m not sure is correct.Â  You could get a long run of 1s for one strata and a long run of zeros for another strata.\n\nRun experiment\nFor each strata\n\nCalculate mean outcome for treated and mean outcome for untreated\nCalculate the difference in mean outcomes\n\nTake the mean of the differences for the average treatment effect (ATE)\n\nDenominator is the number of strata\n\n\nIn the example, the procedure was simulated multiple times to get an ATE distribution\n\nI guess you could bootstrap or use {emmeans} to CIs, pvals, etc.\n\n\n\n\n28.6.4 CUPED\n\n\nControlled-Experiment Using Pre-Experiment Data\nY is the outcome variable\nX is pre-experiment values of the outcome variable\n\nSo, youâ€™d need as many pre-experiment values as observed values during the experiment.\n\nâ€¦and potentially the same individuals? Probably not necessary but desirable.\n\nWhen no pre-experiment values of the outcome variable exist, a variable highly correlated to the outcome variable thatâ€™s NOT RELATED TO THE EXPERIMENT can be used.\n\nLike an instrument from an IV model.\nCan use ML to construct the control variate. (see CUPAC below)\n\nThis blog post goes through the algebra extending CUPED from one covariate, X,Â  to multiple covariates.\n\nAlso see Understanding CUPED\nSteps\n\nRandomly assign treatment to individuals\nPerform experiment\nCalculate Î¸ (eq.3)\nCalculate Ycuped (eq.1)\nCalculate the effect size by taking the difference between the treated Ycuped mean and the untreated Ycuped mean\n\n\n\n\n28.6.5 Variance-Weighted Estimators\n\nVariance is reduced by calculating a weighted variance based on the variance of an individualâ€™s pre-experiment data\n\n\nY is the outcome variable\nZ is the treatment indicator\nÎ´ is the treatment effect\nÏƒi2 is the pre-experiment variance of individual iâ€™s data\n\nalternative ways of estimating the variance include ML models and using Empirical Bayes estimators (Paper)\n\n\nSteps\n\nCalculate individual variances, Ïƒi2\nBucket individuals into k strata based on their variances\nCalculate the mean of each strataâ€™s variance, stratak_mean_variance\nRandomly assign treatment to individuals\nPerform experiment\nFor each strata\n\nCalculate the effect for each strata by taking the difference between the treated mean Y and untreated mean Y\nCalculate strata weight, wk = 1 / stratak_mean_variance\nCalculate weighted effect for strata k, Î´w,k = Î´k x wk\n\nCalculate variance weighted treatment effect by adding all the weighted effects and dividing it by the sum of the weights\n\nÎ´w = sum(Î´w,k) / sum(wk)\n\n\nPros and Cons\n\nThe variance-weighted estimator models individual pre-experiment variance as weight and it can be used as a nice extension to other methods such as CUPED.\n\nI guess you just calculate k Ycuped and then do the weighting procedure. Î¸ and X shouldnâ€™t be affected â€” just some grouped calculations.\n\nIt works well when there is a highly skewed variance between users and when the pre-treatment variance is a good indicator of the post-treatment variance.\n\nNot sure what exactly is meant by â€œhighly skewed variance between users.â€ Most users have high or most users have low variance for the pre-experiment data?\n\nWhen the variance of the pre-treatment variance is low or when the pre- and post-experiment variances are not consistent, the variance-weighted estimator might not work.\nThe variance-weighted estimator is not unbiased. Managing bias is important for this method.\n\n\n\n\n28.6.6 CUPAC\n\nControl Using Predictions As Covariates\nML extension of CUPED (Paper)\nAssuming we have pre-experiment metrics, X1, X2, X3, and X4. Essentially, what this method does is to use some machine learning model to predict Y using X1, X2, X3, and X4. And then, we can use the predicted value as the control covariate in CUPED.\n\n\n\n28.6.7 MLRATE\n\nMachine Learning Regression-Adjusted Treatment Effect Estimator\nAlso see\n\nPaper\nUpgrade Variance Reduction Beyond CUPED: Introducing MLRATE\nDeep dive into MLRATE - machine learning regression-adjusted treatment effect estimator and comparing it to other methods\n\nDoes the same thing as CUPAC to get the control covariate, but instead using the CUPED equation with Î¸ to get Ycuped, it estimates Ycuped using OLS regression.\n\nSee Introducing MLRATE article for more details"
  },
  {
    "objectID": "qmd/experiments-rct.html#sec-exp-rct-misc",
    "href": "qmd/experiments-rct.html#sec-exp-rct-misc",
    "title": "29Â  RCT",
    "section": "29.1 Misc",
    "text": "29.1 Misc\n\nReasons for not running a RCT\n\nItâ€™s just not technically feasible to have individual-level randomization of users as we would in a classical A/B test\n\ne.g.Â randomizing which individuals see a billboard ad is not possible\n\nWe can randomize but expect interference between users assigned to different experiences, either through word-of-mouth, mass media, or even our own ranking systems; in short, the stable unit treatment value assumption (SUTVA) would be violated, biasing the results\n\nA gold-standard design is a 6-period 2-treatment randomized crossover study; the patient actually receives both treatments and her responses can be compared (Harrell)\nATE for RCT:\n\nNon-theoretical ATE (i.e.Â calculated from actual data) is sample-averaged; population sampling weights are unavailable for RCT subject groups. So this ATE applies to a replication of the study with similar sampling patterns. ATE does not apply to the population and in fact may apply to no one due to lack of conditioning on patient characteristics. The ATE used in 99% of papers has nothing to do with population but uses only convenience sample weighting.Â  Some papers even blatantly call it population-averaged.â€\nâ€œThey test causal hypotheses about a group of patients with symptoms & other â€˜diagnosticâ€™ findings that form entry criteria for the RCT & may only be available in sufficient numbers in specialist centres.â€\nGelman\n\nâ€œthe drug works on some people and not othersâ€”or in some comorbidity scenarios and not othersâ€”we realize thatâ€the treatment effectâ€ in any given study will depend entirely on the patient mix. There is no underlying number representing the effect of the drug. Ideally one would like to know what sorts of patients the treatment would help, but in a clinical trial it is enough to show that there is some clear average effect. My point is that if we consider the treatment effect in the context of variation between patients, this can be the first step in a more grounded understanding of effect size.\n\nGelman regarding a 0.1 ATE for a treatment in an education study\n\nâ€œActually, though, an effect of 0.1 GPA is a lot. One way to think about this is that itâ€™s equivalent to a treatment that raises GPA by 1 point for 10% of people and has no effect on the other 90%. Thatâ€™s a bit of an oversimplification, but the point is that this sort of intervention might well have little or no effect on most people. In education and other fields, we try lots of things to try to help students, with the understanding that any particular thing we try will not make a difference most of the time. If mindset intervention can make a difference for 10% of students, thatâ€™s a big deal. It would be naive to think that it would make a difference for everybody: after all, many students have a growth mindset already and wonâ€™t need to be told about it.\nâ€œMaybe in some fields of medicine this is cleaner because you can really isolate the group of patients who will be helped by a particular treatment. But in social science this seems much harder.â€\n\nMe: So, a 0.1 effect wouldnâ€™t be large if there was no variation (i.e.Â same size effect for everyone), but thatâ€™s very unlikely to be the case.\n\n\n\nCalculation of standard-errors is different depending on the RCT type in order that variation within arms could be validly used to estimate variation between.\nRandom Sampling vs Random Treatment Allocation (source)\n\nRandom Sampling: licenses the use of measures of uncertaintyÂ  for (sub)groups of sampled patients.\nRandom Treatment Allocation: licenses the use of measures of uncertainty for the differences between the allocated groups.\n\nRe RCTs:\n\nlicenses the use of measures of uncertainty for hazard ratios, odds ratios, risk ratios, median/mean survival difference, absolute risk reduction etc that measure differences between groups.\nBecause there is no random sampling, measures of uncertainty are not licensed by the randomization procedure for cohort-specific estimates such as the median survival observed in each treatment cohort.\n\nFor those, we can use descriptive measures such as standard deviation (SD), interquartile range etc. Measures of uncertainty will require further assumptions to be considered valid. Further discussion hereâ€"
  },
  {
    "objectID": "qmd/experiments-rct.html#sec-exp-rct-feats",
    "href": "qmd/experiments-rct.html#sec-exp-rct-feats",
    "title": "29Â  RCT",
    "section": "29.2 RCT Features",
    "text": "29.2 RCT Features\n\nThree valuable design features of clinical trials are concurrent control, randomization and blinding.\n\nBlinding is weak at best without Randomization\nRandomization is impossible without Concurrent Control,\nConcurrent Control is necessary for the other two, so it can be regarded as the most important of the three.\n\nBlinding or Masking - patients are unaware of the treatment they are receiving and treating physicians are unaware of the treatment they are administering\n\nPrevents differential care during follow-up, accounts for nonspecific effects associated with receiving an intervention (placebo effects), may facilitate blinding of outcome assessors, and may improve adherence.\n\nConcurrent Control - the effect of a treatment should be assessed by comparing the results of subjects treated with the intervention being studied with the results of subjects treated concurrently (i.e.Â at the â€˜sameâ€™ places at the â€˜sameâ€™ times) with a control treatment, for example, placebo.\n\nIn reality\n\nRe â€˜same timeâ€™: the idea behind concurrent control is that the times at which they are recruited will vary randomly within treatment arms in the same way as between, so that variation in outcomes arising as a result of the former can be used to judge variation in outcomes as a result of the latter.\n\nTiming matters: The time at which a patient is recruited into the trial matters, but should not be biasing if patients are randomized throughout the trial to intervention and control. It will tend to increase the variance of the treatment effect, and rightly so, but it is a component that may be possible to eliminate (partially) by modelling a trend effect.\n\nExample: 1990s AIDS studies found survival of patients who were recruited later into trials tended to be better than those recruited earlier\n\n\nRe â€˜same placeâ€™: The vast majority of randomised clinical trials are run in many centers. The variations in design around this many-centers aspect is the primary difference between various types of RCTs (see below). All the types will be regarded as employing concurrent control, but have their standard errors calculated differently.\n\nConsequence of violations\n\nIf variation from center to center is ignored and patients have not been randomized concurrently, then Fisherâ€™s exact test, Pearsonâ€™s chi-square and Studentâ€™s t will underestimate the variation\n\n\nRandomized assignment means that eligible units are randomly assigned to a treatment or comparison group. Each eligible unit has an equal chance of being selected. This tends to generate internally valid impact estimates under the weakest assumptions.\n\nRandomization also allows us to achieve statistical independence, which eliminates omitted variable bias. Statistical independence implies that the treatment variable is not correlated with the other variables. The key assumption is that randomization effectively produces two groups that are statistically identical with respect to observed and unobserved characteristics. In other words, the treatment group is the same as the control group on average.\n\ni.e.Â randomization process renders the experimental groups largely comparable. Thus, we can attribute any differences in the final metrics between the experimental groups to the intervention.\n\nIn the absence of randomization, we might fall victim to this omitted variable bias because our treatment variable will probably be endogenous. That is, it will be probably correlated with other variables excluded from the model (omitted variable bias)."
  },
  {
    "objectID": "qmd/experiments-rct.html#sec-exp-rct-assopts",
    "href": "qmd/experiments-rct.html#sec-exp-rct-assopts",
    "title": "29Â  RCT",
    "section": "29.3 Assignment Options",
    "text": "29.3 Assignment Options\n\nMisc\n\nIn-Trial Follow-Up (ITFU)\n\nWithout ITFU, the unbiased ascertainment of outcomes may be compromised and statistical power considerably reduced\nStrategies\n\nFace-to-face follow-up is widely used during the initial â€œin-trialâ€ period, but is costly if employed longer term.\nTelephone-based approaches are more practical, with the ability to contact many participants coordinated by a central trial office\nPostal follow-up has been shown to be effective.\nWeb-based techniques may become more widespread as technological advances develop.\n\n\nPost-Trial Follow-Up (PTFU)\n\nRCTs are costly and usually involve a relatively brief treatment period with limited follow-up. A treatment response restricted to this brief â€œin-trialâ€ period can potentially underestimate the long-term benefits of treatment and also may fail to detect delayed hazards.\nStrategies\n\nSee ITFU strategies\nUse of routine health records can provide detailed information relatively inexpensively, but the availability of such data and rules governing access to it varies across countries.\n\n\n\nRandomization\n\nIndividual Randomization - the individual or patient that is allocated to an intervention (may be more than one intervention group) or control group, and simple statistical analyses on participant outcomes are used to evaluate if the intervention was effective.\n\nThese analyses assume that all participants are completely independent (ie. unlike each other, do not influence each other, and any outcomes measured on them are influenced by the intervention or usual care in the same way)\n\nCluster Randomization - is one in which intact social units, or clusters of individuals rather than individuals themselves, are randomized to different intervention groups\n\nAll participants recruited from the practice, school or workplace are allocated to either the intervention or the control group\nThe outcomes of the intervention are still measured at the individual level, but the level at which the comparison is made is the practice, school or workplace\nAdvantages\n\nMembers from intervention and control groups are less likely to have direct contact with each other and are less likely to pass on components of the intervention to the control group. (i.e.Â contamination)\nThere may also be increased compliance due to group participation.\nClusters typically consistent in their management.\n\n\n\nTreatment Strategy\n\nParallel Group - subjects are randomized to one or more study arms (aka treatment groups) and each study arm will be allocated a different intervention. After randomization each participant will stay in their assigned treatment arm for the duration of the study\n\nThink this is just typical randomization into treatment/control groups but can be extended to include multiple treatment arms.\nâ€œchange from baselineâ€ (aka change scores) should never be the outcome variable\nCentral Question: For two patients with the same pre measurement value of x, one given treatment A and the other treatment B, will the patients tend to have different post-treatment values of y?\n\nCrossover Group - subjects are randomly allocated to study arms where each arm consists of a sequence of two or more treatments given consecutively.\n\ni.e.Â each subject receives more than one treatment and each treatment occurs sequentially over the duration of the study.\nExample: AB/BA study - Subjects allocated to the AB study arm receive treatment A first, followed by treatment B, and vice versa in the BA arm.\nAllows the response of a subject to treatment A to be contrasted with the same subjectâ€™s response to treatment B.\n\nRemoving patient variation in this way makes crossover trials potentially more efficient than similar sized, parallel group trials in which each subject is exposed to only one treatment.\n\nIn theory treatment effects can be estimated with greater precision given the same number of subjects.\nMisc\n\nBest practice is to avoid this design if there is a reasonable chance of Carry Over\nAlso see\n\nSenn SJ. Cross-over trials in clinical research. Chichester: John Wiley; 1993.\n\nâ€œreadable approach to the problems of designing and analysing crossover trialsâ€\n\n\n\nIssue: Carry Over\n\nEffects of one treatment may â€œcarry overâ€ and alter the response to subsequent treatments.\n(Pre-experiment) Solution: introduce a washout (no treatment) period between consecutive treatments which is long enough to allow the effects of a treatment to wear off.\n\nA variation is to restrict outcome measurement to the latter part of each treatment period. Investigators then need to understand the likely duration of action of a given treatment and its potential for interaction with other treatments.\n\nTesting for Carry Over\n\nIf carry over is present the outcome on a given treatment will vary according to its position in the sequence of treatments.\nExample: Concluding that there was no carry over when an analysis of variance found no statistically significant interaction between treatment sequence and outcome.1\nHowever such tests have limited power and cannot rule out a type II error (wrongly concluding there is no carry over effect).\n\n(Post-experiment) Solution: If Carry Over is detected:\n\nOption 1: Treat the study as though it were a parallel group trial and confine analysis to the first period alone.\n\nThe advantages of the crossover are lost, with the wasted expense of discarding the data from the second period.\nMore importantly, the significance test comparing the first periods may be invalid\n\nOption 2 (applicable only to studies with at least three treatment periods, e.g.Â ABB/BAA)\n\nModel the carry over effect and use it to adjust the treatment estimate.\nSuch approaches, while statistically elegant, are based on assumptions which can rarely be justified in practice.\nSee Senn paper above\n\n\n\n\nBlocked Randomization - at any given point in the trial we restrict the degree of â€œimbalanceâ€ between the groups to half of the block size\n\nImbalance leads to a loss of power\nIn an open-label study, a small fixed block size would make it possible for the study staff to predict what the next allocation is.\n\nSolution: Use variable block sizes, which retain some of the protection of balanced group size while making it harder for the study staff to tell where they are in a given block or where one block ends and another begins\n\nTwitter thread presenting an example of the procedure for a block randomized RCT\n\nBalance - balanced allocations are more efficient in that they lead to lower variances\n\nVariance of the Mean difference (e.g between treatment and control groups) for unbalanced design\n\n\nRandomized designs are classified as completely randomized design, complete block design, randomized block design, Latin square design, split pot design, cross over design, family block design, stepped-wedge cluster design, etc.\n\nCompletely Randomized Parallel Group trial - any given center will have some patients randomly allocated to intervention and some to control. Randomization includes centers (i.e.Â a patient is randomly selected either treatment/control and which center they will receive the treatment)\n\nParallel Group Blocked by Center - Randomization happens within each center (i.e.Â each center handles their own randomization). Treatment/Control ratio is the same for each center.\n\nâ€œCenterâ€ should be included as a variable in the model.\n\n\nCluster-Randomized trial - randomly allocate some centers to dispense the intervention and some the control\n\nFundamental unit of inference becomes the center and patients are regarded as repeated measures on it\n\n\nExamples\n\nThe effects of a leading mindfulness meditation app (Headspace) on mental health, productivity, and decision-making (Paper)\n\nRCT with 2,384 US adults recruited via social media ads.\nFour-week experiment\n\nfirst group is given free access to the app (worth $13)\nsecond group receives, in addition, a $10 incentive to use the app at least four or ten separate days during the first two weeks\nthird group serves as a (waitlist) control group"
  },
  {
    "objectID": "qmd/experiments-rct.html#sec-exp-rct-sbias",
    "href": "qmd/experiments-rct.html#sec-exp-rct-sbias",
    "title": "29Â  RCT",
    "section": "29.4 Sources of Bias",
    "text": "29.4 Sources of Bias\n\nMisc\n\nNotes from\n\nBiases in randomized trials: a conversation between trialists and epidemiologists\nAlso see\n\nExperiments, A/B Testing &gt;&gt; Potential Biases\nExperiments, Planning &gt;&gt; Misc\n\n\n\nSelection Bias - Occurs when there are systematic differences between baseline characteristics of groups.\n\nIf the assignment that was not properly randomized or the randomized assignment was not sufficiently concealed (i.e.Â allocation concealment), and so the person enrolling participants was aware of allocation sequence and influenced which patients were assigned to each group based on their prognostic factors\nExample: if groups are not comparable on key demographic factors, then between-group differences in treatment outcomes cannot necessarily be attributed solely to the study intervention.\nExample: The assignment of patients to a group is influenced by knowledge of which treatment they will receive\nSolutions:\n\nRandomized Assignment - RCTs attempt to address selection bias by randomly assigning participants to groups â€“ but it is still important to assess whether randomization was done well enough to eliminate the influence of confounding variables.\nBlinding - participants and investigators should remain unaware of which group participants are assigned to.\n\n\nPerformance Bias - Refers to systematic differences between groups that occur during the study. Leads to overestimated treatment effects, because of the physical component of interventions\n\nExample: if participants know that they are in the active treatment rather than the control condition, this could create positive expectations that have an impact on treatment outcome beyond that of the intervention itself.\nSolution: Blinding - participants and investigators should remain unaware of which group participants are assigned to.\n\nMore easily achieved in medication trials than in surgical trials\n\n\nDetection Bias - Refers to systematic differences in the way outcomes are determined.\n\nExample: if providers in a psychotherapy trial are aware of the investigatorsâ€™ hypotheses, this knowledge could unconsciously influence the way they rate participantsâ€™ progress.\nSolution: Attention to conflicts of interest and Blinding (also see Performance Bias) - RCTs address this by utilizing independent outcome assessors who are blind to participantsâ€™ assigned treatment groups and investigatorsâ€™ expectations.\n\nAttrition Bias - occurs when there are systematic differences between groups in withdrawals from a study.\n\nItâ€™s common for participants to drop out of a trial before or in the middle of treatment, and researchers who only include those who completed the protocol in their final analyses are not presenting the full picture.\nSolution: Intention to Treat analysis - Analyses should include all participants who were randomized into the study, and not only participants who completed some or all of the intervention.\n\nReporting Bias - Refers to systematic differences between reported and unreported data.\n\nExample: publication bias - occurs because studies with positive results are more likely to be published, and tend to be published more quickly, than studies with findings supporting the null hypothesis.\nExample: outcome reporting bias - occurs when researchers only write about study outcomes that were in line with their hypotheses.\nSolution: Requirements that RCT protocols be published in journals or on trial registry websites, which allows for confirmation that all primary outcomes are reported in study publications.\n\nOther Bias - A catch-all category that includes specific situations not covered by the above domains.\n\nIncludes bias that can occur when study interventions are not delivered with fidelity, or when there is â€œcontaminationâ€ between experimental and control interventions within a study (for example, participants in different treatment conditions discussing the interventions they are receiving with each other)."
  },
  {
    "objectID": "qmd/experiments-stepped-wedge-cluster-design.html",
    "href": "qmd/experiments-stepped-wedge-cluster-design.html",
    "title": "30Â  Experiments, Stepped Wedge Cluster Design",
    "section": "",
    "text": "Stepped Wedge(https://www.rdatagen.net/post/alternatives-to-stepped-wedge-designs/) [bkmk in design] \n[](./_resources/Experiments,_Stepped_Wedge_Cluster_Design.resources/unknown_filename.1.png|812x0]]\n\nIn theÂ example,Â the study lasts 24 weeks and is conducted using 50 total sites (geographical locations). Each site will include six patients per week [the â€œper weekâ€ just means each site will have 6 total subjects participating each week as part of control or later in the treatment]. That means if we are collecting data for all sites over the entire study period, we will have 24Ã—6Ã—50=7200 outcome measurements.\nIn the stepped-wedge design, all clusters [I think the clusters are the â€œwavesâ€] in a trial will receive the intervention at some point, but the start of the intervention will be staggered. The amount of time in each state (control or intervention) will differ for each site (or group of sites if there are waves of more than one site starting up at the same time).\nIn this design (and in the others as well) time is divided into discrete data collection/phase-in periods. In the schematic figure, the light blue sections are periods during which the sites are in a control state, and the darker blue are periods during which the sites are in the intervention state. Each period in this case is 4 weeks long.\nFollowing the Thompson et al.Â paper, the periods can be characterized as pre-rollout (where no intervention occurs), rollout (where the intervention is introduced over time), and post-rollout (where the all clusters are under intervention). Here, the rollout period includes periods two through five.\nStepped Wedge with Rollout-Only\n\n\n\nThe Thompson et al.Â paper argued that if we limit the study to the rollout period only (periods 2 through 5 in the example above) but increase the length of the periods (here, from 4 to 6 weeks), we can actually increase power. In this case, there will be one wave of 10 sites that never receives the intervention.\nThe data generation process is exactly the same as above, except the statement defining the length of periods (6 weeks instead of 4 weeks) and starting point (week 0 vs.Â week 4) is slightly changed\nStepped wedge with rollout maintains power best as the inter-class correlation (ICC) increases between sites\n[](./_resources/Experiments,_Stepped_Wedge_Cluster_Design.resources/unknown_filename.2.png|812x0]]\nStepped Wedge vs.Â Cluster Randomized Trial (CRT)\n\n\nCRT is a traditionalÂ parallel design unlike the stepped design thatâ€™s been discussed\nThe simulations confirm findings that the CRT is more efficient than stepped-wedge designs when the ICC is close to zero, but pales in comparison even with ICCs as low as 0.01.\n\n\nIntra-cluster correlation (ICC) across time periods(https://www.rdatagen.net/post/intra-cluster-correlations-over-time/) [bkmk in design]\n\nMoving beyond the parallel design (CRT design above) to the stepped-wedge design, time starts to play a very important role. It is important to ensure that we do not confound treatment and time effects; we have to be careful that we do not attribute the_general_changes over time to the intervention. This is accomplished by introducing a time trend into the model. (Actually, it seems more common to include a time-specific effect [dummy vars for each period I assume] so that each time period has its own effect. However, for simulation purposes, I will assume a linear trend.)\nIn the stepped-wedge design, we are essentially estimating within-cluster treatment effects by comparing the cluster with itself pre- and post-intervention. To estimate sample size and precision (or power), it is no longer sufficient to consider a single ICC, because there are now multiple ICCâ€™s - the within-period ICC and the between-period ICCâ€™s. The within-period ICC is what we defined in the parallel design [standard ICC definition in my notebook](since we effectively treated all observations as occurring in the same period.) Now we also need to consider the expected correlation of two individuals in the same cluster in different time periods.\nIf we do not properly account for within-period ICC and the between-period ICCâ€™s in either the planning or analysis stages, we run the risk of generating biased estimates.\nSide note: according to the standard ICC equation, we can say thatÂ correlation between any two subjects in a cluster increases as the variation between clusters increases\n\nModeling\n\nTraditional parallel clustered design(e.g.Â CRT above) mixed model\n\n(https://www.rdatagen.net/post/intra-cluster-correlations-over-time/)\n\n, whereÂ Â y_icÂ is a continuous outcome for subject i in cluster c, andÂ X_cÂ is a treatment indicator for cluster c (either 0 or 1). The underlying structural parameters are Î¼, the grand mean, andÎ²1, the treatment effect. The unobserved random effects areÂ b_câˆ¼ N(0,Ïƒ_b^2), the normally distributed group level effect, andÂ e_icÂ âˆ¼ N(0,Â Ïƒ_e^2), the normally distributed individual-level effect. (This is often referred to as the â€œerrorâ€ term, but that doesnâ€™t adequately describe what is really unmeasured individual variation.)\n\n\nConstant (and equal) ICCs over time\n\nAssumes that the within-period ICC and between-period ICCâ€™s are equal and constant throughout the study\n\nThe key differences between this model compared to the parallel design is the time trend and time-dependent treatment indicator. The time trend accounts for the fact that the outcome may change over time regardless of the intervention. And since the cluster will be in both the control and intervention states we need to have an time-dependent intervention indicator.\nWithin- and between- period ICC is\n\nBetween-period ICC means we are estimating the expected correlation between any two subjects i and j in cluster c, one in time period t and the other in time periodÂ tâ€™ whereÂ (tâ‰ tâ€²). So correlations are being calculated between all pair-wise combinations of the periods. Not just between t and t+1 but also t and t+2, etc.\n(https://www.rdatagen.net/post/estimating-treatment-effects-and-iccs-for-stepped-wedge-designs/)\n\n\n\n- library(lme4)\n- lmerfitÂ &lt;-Â lmer(Y ~ period + rx + (1|Â cluster)Â , data = dx)\n- varsÂ &lt;-as.data.frame(VarCorr(lmerfit))$vcov\n- iccestÂ &lt;-Â round(vars[1]/(sum(vars)), 3)\n\nDifferent within- and between-period ICCs\n\n(https://www.rdatagen.net/post/varying-intra-cluster-correlations-over-time/)\nInstead of a single cluster level effect, b_c, we have a vector of correlated cluster/time specific effects, b_ct.The vectorÂ b_c has a multivariate normal distributionÂ N(0,(Ïƒ_b^2)*R).Â These cluster-specific random effects,Â (b_c1,Â b_c2, â€¦,Â b_cT) [I think T is the number of periods]Â replaceÂ b_c, and the slightly modified data generating model is\n\nHow we specifyÂ r_0 and rÂ reflects different assumptions about the between-period intra-cluster correlations.\n Â (see b_ct distribution specification above)\nWithin-period ICC is still the same\n\n, but the between-period ICC is\n\n\n\nCase 1:Â In this first case, the correlation between individuals in the same cluster but different time periods is less than the correlation between individuals in the same cluster and same time period. In other words,Â ICC_ttÂ != ICC_ttâ€™ [whatâ€™s described is actually ICC_tt &gt; ICC_ttâ€™ but that still follows them being â€œnot equal.â€]. However the between-period correlation isconstant,Â  or in other words,Â ICC_ttâ€²Â are constant for all t andÂ tâ€². [So the correlation between individuals within-period is different than between-period but the between-period correlation is the equal for each pair of periods]. We have these correlations whenÂ r_0 =Â ÏÂ and r = 1, such that\n\n\n\nÂ  Â  Â library(lme4)\nÂ  Â  Â Â Â Â Â lmerfitÂ &lt;- lmer(Y ~ period + rx + (1|Â cluster/period)Â , data = dcs)Â \nÂ Â  Â Â Â  Â Â Â The cluster-level period-specific effects are specified in the model as â€œcluster/periodâ€, which indicates that the Â Â  Â Â Â  Â Â Â  Â Â  Â period effects are nested within the cluster.\n\n(https://www.rdatagen.net/post/bayes-model-to-estimate-stepped-wedge-trial-with-non-trivial-icc-structure/)\nExtracting period:cluster variance (Ïƒ_w^2),Â the cluster variance (Ïƒ_v^2),Â and the residual (individual level) variance (Ïƒ_e^2) from the model fit allows us to estimate cluster level effects (Ï),Â the within-period ICC_tt, and the between-period ICC_ttâ€™. Donâ€™t confuseÂ \\rho_Ï_Â with the ICC.Â \\rho_Ï_Â is the correlation between the cluster-level period-specific random effects.Â \n\n\n- vsÂ &lt;-Â as.data.table(VarCorr(lmerfit))$vcov\n- rhoÂ &lt;-Â vs[2]/sum(vs[1:2])\n\nThe within-period ICC is the ratio of total cluster variance relative to total variance\n\n\nÂ  Â  - iccwÂ &lt;-Â sum(vs[1:2])/sum(vs\n)\n\nThe between-period ICC_ttâ€™Â is really just the within-period ICC_ttÂ adjusted byÂ Ï\n Â (the result of ICC_tt *Â Ï)\n\n- iccbÂ &lt;-Â vs[2]/sum(vs)\n\nCase 2: The correlation between individuals in the same cluster degrades over time [the change from case 1 istime-varying, between-period correlation. ICC_tt remains not equal to ICC_ttâ€™]. Here, the correlation between two individuals in adjacent time periods is stronger than the correlation between individuals in periods further apart. That isÂ ICC_ttâ€™ &gt; ICC_ttâ€™â€˜Â ifÂ |tâ€™ - t| &lt; |tâ€™â€™ - t|. This structure can be created by settingÂ r_0 = 1Â and r = Ï,\n\n\n\nLast article linked also models this case using {rstan}"
  },
  {
    "objectID": "qmd/extreme-value-theory-(evt).html#sec-evt-misc",
    "href": "qmd/extreme-value-theory-(evt).html#sec-evt-misc",
    "title": "31Â  Extreme Value Theory",
    "section": "31.1 Misc",
    "text": "31.1 Misc\n\nPackages\n\nCRAN Task View\n{erf} - able to extrapolate extimates beyond the training data since erf is based on EVT and is also flexible since it uses a RF\n\nvideo: from the 33min mark to 55:19\nQ(Ï„) is the desired quantile you want to estimate\n\nQ(Ï„0) is an intermediate quantile (e.g.Â 0.80) that can be estimated using a quantile RF (package uses {grf})\n\nDepends on thickness of tail (i.e whether the shape parameter is negative, 0, or positive)\n0.80 tends to work reasonable well\nThe higher the threshold you use, the less variance but higher bias\n\n\nÎ¾(x) and Ïƒ(x) says the shape and scale parameters depend on the predictors. Theyâ€™re estimated by minimizing a probability distributionâ€™s log-likelihood which are multiplied by weights extracted from quantile RF.\ntune minimum node size, penalty term on the variability of the shape parameter\ncv using deviance metric for model selection\n\n{gbex} - no docs, only paper, gradient boosting for extreme quantile regression; able to extrapolate since theyâ€™re based on EVT\n{evgam} - Extreme Value GAM; able to extrapolate since theyâ€™re based on EVT\n\n{erf} and {gbex} peform better than regular quantile rf model types for quantiles &gt; 0.80 (video: from the 33min mark to 55:19, results towards the end)\n\nNon-ML methods like {evgam} perform poorly for data with highh dim\n\nWhy using Random Forest models that do NOT incorporate EVT usually donâ€™t produce good results.\n\nTypical RF weighs every data point equally while a grf (see Regression, Quantile), depending on the quantile estimate, will weigh data points closer to the quantile more heavily\nQuantile Regression Forests work fine on moderate quantiles (e.g.Â 0.80) but even those like grfs struggle with more extreme quantiles because no matter how large the quantile you choose, the predicted quantile will be no larger than the most extreme data point. They use empirical methods and have no way to extrapolate."
  },
  {
    "objectID": "qmd/extreme-value-theory-(evt).html#sec-evt-distrtail",
    "href": "qmd/extreme-value-theory-(evt).html#sec-evt-distrtail",
    "title": "31Â  Extreme Value Theory",
    "section": "31.2 Distribution Tail Classification",
    "text": "31.2 Distribution Tail Classification\n\nMisc\n\nNotes from quantitative risk management lectures QRM 4-3, 4-4,Â https://www.youtube.com/watch?v=O0fdBwBRGU4\n\nDifference between tail events and outliers:\n\nOutliers tend to be extreme values that occur very infrequently. Typically they are less than 1% of the data.\nTail events are less extreme values compared to outliers but occur with greater frequency.\n\nTail events can be difficult to predict because\n\nAlthough not as rare as outliers, itâ€™s still difficult to get enough to data to model these events with any sufficient precision.\nDifficult to obtain leading indicators which are correlated with the likelihood of a tail event occurring\n\nPrediction tips\n\nConsider binning numerics to help the model learn sparse patterns.\nUse realtime features\n\nExample: Predicting delivery time tail events\n\nunexpected rainstorm (weather data)\nroad construction (traffic data)\n\n\nUtilize a quadratic or L2 loss function.\n\nMean Squared Error (MSE) is perhaps the most commonly used example. Because the loss function is calculated based on the squared errors, it is more sensitive to the larger deviations associated with tail events\n\n\nHeavy tails\n\nYour random variable distribution is heavy tailed if:\n\n\n\nwhere the exponential survival function,Â  \nSays if you take the ratio of your most extreme positive values (i.e.Â your survival function) at the tail (i.e.Â supremum)(numerator) and those of the positive tail of exponential survival function (denominator), then that ratio will go to positive infinity as x goes to infinity\nOr in other words, the probability mass of the pdf of your random variable in the tail is greater than the probability mass that of the exponential pdf\nAlso means that the moment generating function is equal to infinity which means that it canâ€™t be used to calculate distribution parameters\n\n\nSubsets of heavy tails\n\nAlong with survival function ratio (see above), these tails have additional conditionsÂ \nlong tails\n\ncommon in finance\nYour random variable distribution is long tailed if:\n\nit follows the explosion principle\n\nIf an extreme event manifests itself, then the probability of an even more extreme event approaches 1\n\nno time prediction on the next more extreme event but extreme value theory + timeseries + conditions say extreme events tend to cluster\n\n\nSays, for example, if you take a huge loss in your portfolio, itâ€™s a mistake to think that that value is an upper bound on losses or that the probability of an even larger loss is negligible\n\nNot practical to determine from data\n\nsubexponential tails\n\nsubset of long tail\nYour random variable distribution is subexponential tailed if:\n\nit follows the one-shot aka catastrophe principle aka â€œwinner takes allâ€\n\n\n\nwhere Sn is a partial sum of values of your random variable; Mn is a partial maximum; x is a large value\nsays at some point the partial sum, SnÂ , will be dominated by one large value, Mn\nExample: if your portfolio follows this principle, then your total loss can be mostly attributed to one large loss\n\n\ntools available to practically test\n\nExamples:\n\nlog-normal\n\ncan get normal parameters from lognormal parameters by formula that involves exponentiation (see notebook) or vice versa with logs\nall statistical moments always exist\n\n\nfat tails\n\n\n\nL(x) is just characterized as slowly varying function that gets dominated by the decaying inverse power law element, x-Î±. as x goes to infinity\nÎ± is a shape parameter, aka â€œtail indexâ€ aka â€œPareto indexâ€\n\nExamples\n\npareto\n\npareto has similar relationship with the exponential distribution as lognormal does with normal\n\n\nxm is the (positive) minimum of the randomly distributed pareto variable, X that has iindex Î±\nYexp is exponentially distributed with rate Î±\n\nsome theoretical statistical moments may not exist\n\nIf the theoretical moments do not exist, then calculating the sample moments is useless\nExample: Pareto (Î± = 1.5) has a finite mean and an infinite variance\n\nNeed Î± &gt; 2 for a finite variance\nNeed Î± &gt; 1 for a finite mean\nIn general you need Î± &gt; p for the pth moment to exist\nIf the nth moment is not finite, then the (n+1)th moment is not finite.\n\n\n\n\n\n\n\n\n\nLight tails\n\nOpposite of heavy\nInstead of larger than pdf or survival function of the exponential version, itâ€™s equal to or smaller than.\n\ni.e.Â your function decays as fast or faster as x goes to infinity as an exponential\n\nExamples\n\nexponential, normal\n\n\nBoth\n\nclass depends on parameter values\nExamples\n\nWeibull\n\n\n\n\n\nTests\n\nNotes\n\nAll the plots below should be used and considered when diagnosing tails\nCan use the zipf and me plots to find the thresholds in the data where it would be useful to start modeling the data as pareto or lognormal\n\nAsk these questions\n\nDoes the subject matter youâ€™re modeling lead you to expect a certain type of tail?\n\nExample: Does the explosion principle hold or not?\n\nIs there an upper bound to your data (theoretical or actual)?\n\nExample: Is the upper bound due to the quality of the data\n\nDo I have over 10,000 observations?\n\nIn the various plots below, it can be difficult to distinguish between Pareto (fat tail) and Lognormal (long tail) distributions. As a rule-of-thumb, usually takes 10K observations to really be able to tell the two apart in order to get enough data points in the tail.\nUsually get at least 10K observations in a market risk portfolio, but not in credit risk or operational risk portfolios\n\n\nQ-Q plot\n\nexponential quartiles on the y-axis and ordered data on the x-axis\n\nSee EDA &gt;&gt; Numeric &gt;&gt; Q-Q plot for code\n\nif data hugs the diagonal line â€“&gt; exponential â€“&gt; light tails\nif data is concave â€“&gt; potentially heavy tails\nif data is convex â€“&gt; potentially tails that are lighter than an exponential\n\nZipf plot\n\nlog-log plot of the empirical survival function of the data\n\nlog of the pareto survival function makes it linear where the slope of the line is -Î±\n\nindicates if thereâ€™s a power law decay in the tails of the data (i.e.Â fat tails)\n\nresults of this plot is â€œnecessaryâ€ but not â€œsufficientâ€ for confirmation of fat tails (pareto)\nIt is sufficient to say itâ€™s not a pareto if thereâ€™s curvature\n\n\n\n\nThe real data shows linearity at the very end, so even though itâ€™s not linear from the beginning, it is still potentially fat tailed\n\nReal data often show mixed, complex behaviors.\n\nAlso not that even in the simulated dataset, the data points at the end have some randomness to them and donâ€™t fall directly on the line.\n\nthe randomness is called small sample bias; usually not much data in the tails\n\n\n\n\nlog-normal can look like a pareto if its sigma parameter is large (small data). It will look linear and curve down at the very end.\nExample above shows lognormal with sd = 1, so sd doesnâ€™t have to be very large to be tricky to discern from a Pareto.\nIf the data has a smallish range (x-axis), then that is a signal to wary about deeming the distribution to having fat tails\n\nThis one goes from 0 to 100 while the one above it goes from 0 to a million\nâ€œLargeâ€ or â€œsmallâ€ depends on the type of data your looking at though. In another subject matter, maybe 100 is considered large, so context matters\n\n\n\n\nCompare slopes between your original data (red) and aggregations of your data in a zipf plot; If you have fat tails, the line will be shifted because of aggregation but the slope, Î±, will remain the same\n\nExamples of aggregation methods (halves the sample size)\n\nOrder data from largest to smallest; add a1 + an, a2 + an-1, â€¦; plot alongside original data (green)\nOrder data from largest to smallest; add a1 + a2, a3 + a4, â€¦; plot alongside original data (blue)\n\n\n\n\nMean Excess (ME) plot\n\nCalculating the empirical mean excess variable - Order the data, calc mean2, remove the 1 data point, calc mean2, remove data points 1 and 2, calc mean3, and so on. Then plot the means\n\n\nlognormal is similar to pareto in this plot as well. The more data you have the easier it will be to distinguish the two.\nThe left equation is for the lognormal curve (with Normal parameters) and the right equation is the pareto\n\nNeed Î± &gt; 1, so that the mean is finite\n\n\n\n\nDisregard last few points (small sample bias)\nPoints in green circles (only a few points in tails, so difficult to be confident about)\n\nleft: shows a straight line\nright: concave down\n\nRight plot: curvature at the beginning common in the wild, since youâ€™re not likely dealing with pure distributions but some kind of noisy mixture\n\n\nMaximum to sum plotÂ (MS Plot)\n\n\nS is the partial sum, M is the partial maximum, p is the order of the moment that you want to see if it exists or not\n\nFor lognormal, all moments always exist\nFor pareto, you usually only need to check up to p = 4 or p = 5\n\nFor higher levels of p (and hence Î±) the pareto distribution begins to act like a normal\nUsually in credit, market, or operational risk markets youâ€™re dealing with pareto 0 &lt; Î± &lt;= 3\n\n\nProcedure\n\nchoose a p that you want to check\nfor each n, calculate the sum, maximum, and ratio\ny-axis is the ratio, x-axis is the n value\n\n\n\nA lognormal will always converge to 0 for every p you check (black line)\nWhen a moment doesnâ€™t exist (i.e.Â infinite), it just oscillates and never converges (orange line)\nMS plots always start at 1\nPotentially with fewer than 100 observations, you could start to see a convergence if one is going to happen. Of course hundreds of observations is better. Point is that it doesnâ€™t take thousands.\n\nLeft - credit data (real estate losses), Right -Â operational loss data\n\n\n\nLeft\n\np = 1 definitely exists; p = 2 is iffy; p = 3,4 donâ€™t exist\nInterpretation: either Î± is between 1 and 2 or there arenâ€™t enough observations to show a convergence\n\nAlthough n is pretty large in this case\n\n\nRight\n\np = 1 is iffy, the rest donâ€™t exist\nInterpretation: Î± might be less than 1\n\n\n\n\nConcentration Profile\n\nRequirements\n\ndata &gt;= 0 and mean is finite\n\nSimilar to the Mean Excess plot, except the gini index is computed instead of the mean\n\n\nIn the wild you can expect mixtures, so there will likely be noisy behavior in the beginning and when the fat tail is reached, a flat line is formed"
  },
  {
    "objectID": "qmd/feature-engineering-embeddings.html#sec-feat-eng-emb-misc",
    "href": "qmd/feature-engineering-embeddings.html#sec-feat-eng-emb-misc",
    "title": "Embeddings",
    "section": "Misc",
    "text": "Misc\n\nAlso see\n\nFeature Engineering, Tokenization\nDatabases, Vector Databases\nDiagnostics, NLP\nEDA, Text\nNLP, General\n\nEmbeddings can be aggregated (e.g.Â averaged) to make larger groupings\n\nExample: Averaging the embeddings of food items to create an embedding for a meal\n\nJina AI model, jina-embeddings-v2, has 8K (8192 tokens) context length, which puts it on par with OpenAIâ€™s proprietary model, text-embedding-ada-002, in terms of both capabilities and performance\n\nWillison llm plugin for this model"
  },
  {
    "objectID": "qmd/feature-engineering-embeddings.html#sec-feat-eng-emb-conc",
    "href": "qmd/feature-engineering-embeddings.html#sec-feat-eng-emb-conc",
    "title": "Embeddings",
    "section": "Concepts",
    "text": "Concepts\n\nEmbeddings are numerical representations of words or sentences\nMisc\n\nNotes from a video\n\nCapture analogies well\n\nHorizontally: Puppy is to Dog as Calf is to Cow\n\nX-Axis could represent a latent variable like Age\n\nVertically: Pupy is to Calf as Dog is to Cow\n\nY-Axis could represent a latent variable like Size\n\n\nEach latent variable is a dimension in the embedding\nWord and Sentence embedding matrices can have lengths in the 1000s\n\n\nIssue with one-hot encoding is that it does not place similar entities closer to one another in vector space.\nThe embeddings form the parameters â€” weights â€” of the network which are adjusted to minimize loss on the task.\nWhich categories get placed closer to each other in the embedding depends on the outcome variable during the training\n\nFiguring out how to create the supervised task to produce relevant representations is the toughest part of making embeddings.\nExample: Categorical to be embedded is book titles\n\nâ€œWhether or not a book was written by Leo Tolstoyâ€Â  as the outcome variable will result in embeddings would place books written by Tolstoy closer to each other.\n\n\nThe categorical variable with 100s of levels can be reduced to something like 50 vectors (node weights in the embedding layer of the network)\n\n\nSale Price is the outcome (observed values represented by â€œSale Priceâ€ in orange box)\nSparse Vector Encoding (I think this is one-hot) for the categorical levels you want embedded\nOther features are included in the embedding model but they only connect to other hidden layers (pink) We can use the same group of predictors and outcome variable in the embedding DL model that we want to use in the tree (or whatever) algorithm"
  },
  {
    "objectID": "qmd/feature-engineering-embeddings.html#sec-feat-eng-emb-eng",
    "href": "qmd/feature-engineering-embeddings.html#sec-feat-eng-emb-eng",
    "title": "Embeddings",
    "section": "Engineering",
    "text": "Engineering\n\nHyperparameter: Dimension of the embedding layer\n\nHigher dimension embeddings are a more accurate representation of the relationship\n\nDownside is a greater risk of overfitting and longer training times\n\nShould be tuned\n\nStarting point: dimensions â‰ˆ (possible values)0.25\n\nâ€œpossible valuesâ€ would be the vocabulary for a text variable embedding\n\n\n\n{text}\n\nprovides access to hugginface transformers\n\n{embed}\n\nExample\nembed::step_embed(cat_var,\nÂ  Â  num_terms = 4, hidden_units = 16, outcome = vars(binary_outcome_var),\nÂ  Â  options = embed_control(loss = \"binary_crossentropy\",\n    Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  epochs = 10))\n\nnum_terms is the dimension of the embedding layer (i.e.Â number of output variables)\nhidden_units is the layer between the embedding layer and output layer\nShould probably try totune() num_terms at least.\n\nExample: Likelihood or Effect Encoding\nmuseum_rec &lt;-Â \nÂ  recipe(Accreditation ~ ., data = museum_train) %&gt;%\nÂ  update_role(museum_id, new_role = \"id\") %&gt;%\nÂ  step_lencode_glm(Subject_Matter, outcome = vars(Accreditation)) %&gt;%\nÂ  step_dummy(all_nominal_predictors())\n\nâ€œSubject_Matterâ€ is the high cardinality cat var\nstep_lencode_glm fits a glm for each? level of the cat var uses its estimated effect as the encoded value\nmixed linear model and _bayes_ian model are also available instead of a glm\nThese type of embeddings use the average estimated effect as a value for any new levels that show-up in future data\ntidy(grants_glm, number = 1) %&gt;%\nÂ  dplyr::filter(level == \"..new\") %&gt;%\nÂ  select(-id)\n\nView embedding values\nprep(museum_rec) %&gt;%\nÂ  tidy(number = 1)\n\nNot sure if â€œnumber = 1â€ is the step in the recipe or what"
  },
  {
    "objectID": "qmd/feature-engineering-embeddings.html#sec-feat-eng-emb-aug",
    "href": "qmd/feature-engineering-embeddings.html#sec-feat-eng-emb-aug",
    "title": "Embeddings",
    "section": "Augmentation",
    "text": "Augmentation\n\nUseful for imbalanced outcomes with text predictors\n\nBetter performance than subsampling\n\nWords are randomly swapped, deleted, as well as replaced or inserted with synonyms using pretrained word embeddings\n\nAdversarial Text Attack\n\nDiagnostic to test how a model performs on a test set where data augmentation techniques have been applied\n{{textattack}}, article, ipynb"
  },
  {
    "objectID": "qmd/feature-engineering-embeddings.html#sec-feat-eng-emb-mon",
    "href": "qmd/feature-engineering-embeddings.html#sec-feat-eng-emb-mon",
    "title": "Embeddings",
    "section": "Monitoring",
    "text": "Monitoring\n\nTracking embeddings drift\nMisc\n\nNotes from\n\nMeasuring Embedding Drift\n\nRecommended Euclidean\n\nHow to Measure Drift in ML Embeddings\n\n\nRecommended Model-based first and Share of Drifted Components second\nAlso has overview and link to paper for Maximum Mean Discrepancy (MMD) which is a more complex method I didnâ€™t want to get into at the time.\nCode available; Used {{evidently}}\n\n\n\nReasons for drift\n\nChanging your modelâ€™s architecture can change the dimensionality of the embedding vectors. If the layers become larger/smaller, your vectors will too.\n\nUse another extraction method: In the event of the model not changing, you can still try several embedding extraction methods and compare between them.\n\nRetraining your model: Once you retrain the model from which you extract the embeddings, the parameters that define it will change. Hence, the values of the components of the vectors will change as well.\nVision\n\nUnique situations, events, people or objects that are observed in production data but are missing from the training set\n\nText\n\nWhen a word, category or language that does not exist in the training data emerges in production\nAny changes in terminology in the data or changes to the context or meaning of words or phrases over time can contribute to drift.\n\nLow-resource languages and cultural gaps in speech can also compound these difficulties\n\nExample: a sentiment classification model trained on apparel product reviews in English but in production, encounters reviews in Spanish for the first time\n\nOr is asked to predict the sentiment of reviews of specialized medical devices.\n\n\n\nExtraction Methods\n\nExtract embeddings from the current model in production.\n\nExtracting the last fully connected layer before the classification to create an image embedding is advisable.\n\nThe latent structure in this layer will contain information about structure in the image such as objects and actions, in addition to general quality information relative to images in the training set\n\nIn the case of a vision transformer (ViT), it is recommended that you extract the embedding that the multilayer perceptron (MLP) is acting on to make an image-level decision\nExample on how to extract embeddings from a well-known Hugging Face model (article)\n\nUsing a model to extract the embedding\n\ne.g.Â a foundational model like BERT\nAdvantages\n\nNo modification is needed on the production model\nEasy option for testing and running on internal data.\n\nDisadvantage: the model is only looking at the data itself and is not looking at the internal model decisions.\n\n\nChoose a reference vector or dataset\n\nFormulate an expectation on how stable or volatile your data is and choose the reference data that adequately captures what you expect to be a â€œtypicalâ€ distribution of the input data and model responses.\nExamples\n\nValidation data\nPast period that you consider representative\n\ne.g.Â this weekâ€™s data to the previous week and move the reference as you go.\n\n\n\nMetrics\n\nWhichever metric you choose, it will need to be tuned. To tune it, you can model your drift detection framework using historical data or, alternatively, start with some sensible default and then adjust it on the go.\nEuclidean distance\n\nSmaller the distance, the more similar the embeddings\n\nRange: 0 to âˆ\n\nMore stable, sensitive and scalable compared to hyperbox IOU, euclidean distance, cosine distance, and clustering-based group purity scores.\nAn absolute measure which makes setting a specific drift alert threshold harder: the definition of â€œfarâ€ will vary based on the use case and the embedding model used. You need to tune the threshold individually for different models you monitor.\nCalculate centroid for embedding vectors in the production model (and baseline model)\n\nNote: vertical dots are missing in the vectors\nThe centroid is calculated by taking the row-wise averages\n\nCompare production centroid to baseline centroid\n\nCosine Distance (1 - cosine similarity)\n\n\nSmaller the distance, the more similar the embeddings\nIf the two distributions are the same, the Cosine similarity will be 1, and the Cosine distance will be 0. The distance can take values from 0 to 2.\nThe threshold might be not very intuitive to tune, since it can take values as low as 0.001 for a change that you already want to detect.\nAnother downside is that it does not work if you apply dimensionality reduction methods like PCA, leading to unpredictable results.\n\nModel-based drift detection\n\nTrain a classification model that tries to identify to which distribution each embedding belongs and use the AUROC as the drift score.\nIf the model can confidently predict from which distribution the specific embedding comes, it is likely that the two datasets are sufficiently different.\n\ni.e.Â An AUROC score above 0.5 shows at least some predictive power, and an AUROC score of 1 corresponds to â€œabsolute driftâ€ when the model can always identify to which distribution the data belongs.\n\nWorks consistently for different datasets and embedding models we tested, both with and without PCA. It also has an intuitive threshold\n\nShare of drifted components\n\n\nThe individual components of each embedding are treated as â€œcolumnsâ€ in a structured dataset.\nCompute the drift in each component.\n\ne.g.Â Wasserstein (Earth-Mover) distanceÂ with the 0.1 threshold. The intuition behind this metric is that when you set the threshold to 0.1, you will notice changes in the size of the â€œ0.1 standard deviationsâ€ of a given value.\n\nMeasure the overall share of drifting components.\n\ne.g.Â if your vector length is 400, you can set the threshold to 20%. If over 80 components drift, you will get a drift detection alert.\n\nAllows you to reuse familiar techniques that you might be using to detect drift on tabular data.\nMore tuning parameters than other methods: underlying drift detection method, its threshold, and the share of drifting components to react to.\n\n\nUse the 2-sample Kolmogorovâ€“Smirnov test (KS)\n\nâ€œMultiple samples from the embedding set can be taken calculating the euclidean distance metric for each sample set separately, and the KS test can be used to determine if drift has or hasnâ€™t occurred.â€\nSee Distributions &gt;&gt; Tests for more details"
  },
  {
    "objectID": "qmd/feature-engineering-general.html#sec-feat-eng-gen-misc",
    "href": "qmd/feature-engineering-general.html#sec-feat-eng-gen-misc",
    "title": "General",
    "section": "Misc",
    "text": "Misc\n\nTree-based Models\n\nFrom Uber, â€œTree-based models are performing piecewise linear functional approximation, which is not good at capturing complex, non-linear interaction effects.â€\n\nWith regression models, you have to be careful about encoding categoricals as ordinal (i.e.Â integers) which means one-hot encoding is better.\n\nFor example, the raw numerical encoding (0-24) of the â€œhourâ€ feature prevents the linear model from recognizing that an increase of hour in the morning from 6 to 8 should have a strong positive impact on the number of bike rentals while a increase of similar magnitude in the evening from 18 to 20 should have a strong negative impact on the predicted number of bike rentals.\n\nModels with large numbers (100s) of features increases the opportunity for feature drift\nZero-Inflated Predictors/Features\n\nFor ML, transformations probably not necessary\nFor regression\n\nlog(x + 0.05)\n\nlarger effect on skew than sqrt\n\narcsinh(x) (see Continuous &gt;&gt; Transformations &gt;&gt; Logging)\n\napproximates a log but handles 0s\n\nsqrt maybe Yeo-Johnson (?)\n\n&gt;60% of values = 0, consider binning or binary"
  },
  {
    "objectID": "qmd/feature-engineering-general.html#sec-feat-eng-gen-cont",
    "href": "qmd/feature-engineering-general.html#sec-feat-eng-gen-cont",
    "title": "General",
    "section": "Continuous",
    "text": "Continuous\n\nBinning\n\nBenefits\n\nReduces Noise\n\nContinuous variables tend to store information with minute fluctuations that provide no added value for the machine learning task of interest\n\nMakes the feature more intuitive\n\nIs there an important threshold value?\n\n1 value â€“&gt; split into a binary\nmultiple values â€“&gt; multinomial\n\n\nMinimizes outlier influence\n\nBin and Embed\n\nSteps\n\nFind bin ranges\n\nIf sufficient data, calculate quantiles of the numeric vector to find the bin ranges\nsklearn.preprocessing.KBinsDiscretizer has a few different methods\nUse some other method to find the number/ranges of bins (see R packages)\n\nUse the indices of the bins (i.e.Â leftmost bin is 1, 2nd leftmost bin is 2) to discretize each value of the numeric\n\nMight need to be one-hot coded\n\nCreate an embedding of the discretized vector and use the embedding as features.\n\n\nDichotomizing is bad (post, list of papers)\n\nTypical arguments for splitting (even when thereâ€™s no underlying reason to do so) include: simplifies the statistical analysis and leads to easy interpretation and presentation of results\n\nExample: splitting at the medianâ€”leads to a comparison of groups of individuals with high or low values of the measurement, leading in the simplest case to a t test or Ï‡2 test and an estimate of the difference between the groups (with its confidence interval) on another variable.\n\nUsing multiple categories (to create an â€œordinalâ€ variable) is generally preferable , and using four or five groups the loss of information can be quite small\nIssues:\n\nInformation is lost, so the statistical power to detect a relation between the variable and patient outcome is reduced.\n\nDichotomising a variable at the median reduces power by the same amount as would discarding a third of the data\n\nMay increase the risk of a positive result being a false positive\nMay seriously underestimate the extent of variation in outcome between groups, such as the risk of some event, and considerable variability may be subsumed within each group.\nIndividuals close to but on opposite sides of the cutpoint are characterised as being very different rather than very similar.\nConceals any non-linearity in the relation between the variable and outcome\nUsing a stat like median for a cutpoint means studies will have different cutpoints, therefore results cannot easily be compared, seriously hampering meta-analysis of observational studies\nAn â€œoptimalâ€ cutpoint (usually that giving the minimum P value) runs a high risk of a spuriously significant result. Effect will be overestimated and the CI too narrow\nAdjusting for the effect of a confounding variable, dichotomisation will run the risk that a substantial part of the confounding remains\n\n\nHarrell\n\nThink most of these issues are related to inference models like types of logistic regression\nA better approach that maximizes power and that only assumes a smooth relationship is to use a restricted cubic spline (regression spline; piecewise cubic polynomial) function for predictors that are not known to predict linearly. Use of flexible parametric approaches such as this allows standard inference techniques (P -values, confidence limits) to be used (See Feature Engineering, Splines)\nIssues with binning continuous variables\n\nIf cutpoints are chosen by trial and error in a way that utilizes the response, even informally, ordinary P -values will be too small and confidence intervals will not have the claimed coverage probabilities.\n\nThe correct Monte-Carlo simulations must take into account both multiple tests and uncertainty in the choice of cutpoints.\n\nUsing the â€œminimum p-value approachâ€ often results in multiple cutpoints so Â¯\\_(ãƒ„)_/Â¯ plus multiple testing p-value adjustments need to be used.\n\nThis approach involves testing multiple cutpoints and choosing one that minimizes the p-value below a threshold.\n\nOptimal cutpoints often change from sample to sample\nThe optimal cutpoint for a predictor would necessarily be a function of the continuous values of all the other predictors\nYouâ€™re losing variation (information) which causes a loss of power and precision\nAssumes that the relationship between the predictor and the response is flat within each interval\n\nthis assumption is far less reasonable than a linearity assumption in most cases\n\nPercentiles\n\nUsually estimated from the data at hand, are estimated with sampling error, and do not relate to percentiles of the same variable in a population\nValue of binned variable potentially takes on a different relationship with the outcome\n\ne.g.Â Body Mass Index has a smooth relationship with every outcome studied, and relates to outcome according to anatomy and physiology. Binning may change that relationship to being how many subjects have a similar BMI.\n\n\nMany bins usually required to make it worth it. Therefore, many dummy variables will end up being created resulting in a loss of power and precision. (i.e.Â more bins = more variables = more dof used)\nPoor predictive performance with Cox regression models\n\n\nThey might help with prediction using ML or DL models though\n\nâ€œInstead of directly using marketplace health as a continuous feature, we decided to use a form of target-encoding by splitting up the metric into buckets and taking the average historical delivery duration within that bucket as the new feature. With this approach, we directly helped the model learn that very supply-constrained market conditions are correlated with very high delivery times â€” rather than relying on the model to learn those patterns from the relatively sparse data available.â€\n\nImproving ETA Prediction Accuracy for Long Tail Events\nHelps to â€œrepresent features in a way that makes it easy for the model to learn sparse patterns.â€\n\nThis article was about modeling tail events, so maybe this is most useful for features that have an association with the tail values in the outcome variable\n\n\nXGBoost seems to like numerics much more than dummies\n\nTrees may prefer larger cardinalities. So if you do bin, youâ€™d probably want quite a few bins\nNever really seen a binned age variable do well, so guessing more than 10 at least. Though maybe Age just wasnâ€™t important enough.\n\n\nExamples\n\nBinary\n\nWhether a user spent more than $50 or didnâ€™t\nIf user had activity on the weekend or not\n\nMultinomial\n\nTimestamp to morning/afternoon/ night,\nOrder values into buckets of $10â€“20, $20â€“30, $30+\nHeight, age\n\n\n\n\n\nTransformations\n\nMisc\n\nAlso see:\n\nRegression, Linear &gt;&gt; Transformations\nFeature Engineering, Splines\n\nGuide for choosing a scaling method for classification modeling\n\nNotes from The Mystery of Feature Scaling is Finally Solved (narrator: it wasnâ€™t)\n\nOnly used a SVM model for experimentation so who knows if this carries over to other classifiers\n\ntldr\n\nGot time and compute resources? â€“&gt; Ensemble different standardization methods using averaging\nNo time and limited compute resources â€“&gt; standardization\n\nModels that are distribution independent or distance sensitive (e.g.Â SVM, kNN, ANNs) should use standardization\n\nModels that are distribution dependent (e.g.Â regularized linear regression, regularized logistic regression, or linear discriminant analysis) werenâ€™t tested\n\nNo evidence that data-centric rules (e.g.Â normal or non-normal distributed variables, outliers present)\nFeature scaling that is aligned with the data or model can be responsible for overfitting\nEnsembling by averaging (instead of using a model to ensemble) different standarization methods\n\nExperiment used robust scaler (see below) and z-score standardization\n\nWhen they added a 3rd method it created more biased results\n\nRequires predictions to be probabilities\n\nFor ML models, this takes longer because an extra CV has to be run\n\n\n\n\n\n\nStandardization\n\nThe standard method transforms feature to have mean = 0, and standard deviation = 1\n\nNot robust to outliers\n\nFeature will be skewed\n\n\nUsing the median to center and the MAD to scale makes the transformation robust to outliers\nScaling by 2 sd/MAD instead of 1 sd/MAD can be useful to obtain model coefficients of continuous parameters comparable to coefficients related to binary predictors, when applied to the predictors (not the outcome)\nNotes from\n\nWhen conducting multiple regression, when should you center your predictor variables & when should you standardize them?\n\nReasons to standardize\n\nMost ML/DL models require it\n\nMany elements used in the objective function of a learning algorithm (such as the RBF kernel of Support Vector Machines or the l1 and l2 regularizers of linear models) assume that all features are centered around zero and have variance in the same order. If a feature has a variance that is orders of magnitude larger than others, it might dominate the objective function and make the estimator unable to learn from other features correctly as expected.\n\nMost Clustering methods require it\nPCA can only be interpreted as the singular value decomposition of a data matrix when the columns have centered\nInterpreting the intercept as the mean of the outcome when all predictors are held at their means\nPredictors with large values (country populations) can have really small regression coefficients. Standardization makes the coefficients have a more managable scale.\nSome types of models are more numerically stable with the predictors have been standardized\nEasier to set priors in Bayesian modeling\nCentering fixes collinearity issues when creating powers and interaction terms\n\nCollinearity between the created terms and the main effects\n\n\nOther Reasons why you might want to:\n\nCreating a composite score\n\nWhen youâ€™re trying to sum or average variables that are on different scales, perhaps to create a composite score of some kind. Without scaling, it may be the case that one variable has a larger impact on the sum due purely to its scale, which may be undesirable.\nOther Examples:\n\nResearch into childrenâ€™s behavioral disorders - researchers might get ratings from both parents & teachers, & then want to combine them into a single measure of maladjustment.\nStudy on the activity level at a nursing home w/ self-ratings by residents & the number of signatures on sign-up sheets for activities\n\n\nTo simplify calculations and notation.\n\nA sample covariance matrix of values that has been centered by their sample means is simply Xâ€²X (correlation matrix)\nIf a univariate random variable, X, has been mean centered, then var(X)=E(X2) and the variance can be estimated from a sample by looking at the sample mean of the squares of the observed values.\n\n\nReasons NOT to standardize\n\nWe donâ€™t want to standardize when the value of 0 is meaningful.\n\nscale(var or matrix)\n\nDefault args: center = T, scale = T\nStandardizes each column of a matrix separately\nFYI scale(var) == scale(scale(var))\n\n{datawizard::standardize} - Can center by median and scale by MAD (robust), can scale by 2sd (Gelman)\n{{sklearn::RobustScaler}}\n\nStandardize by median and IQR instead of mean and sd\n\n(value âˆ’ median) / IQR\n\nThe resulting variable has a zero mean and median and a standard deviation of 1, although not skewed by outliers and the outliers are still present with the same relative relationships to other values.\nstep_normalize has means, sd args, so it might be able to do this\n\nHarrell recommends substituting the gini mean difference for the standard deviation\n\nGiniâ€™s mean difference - the mean absolute difference between any two distinct elements of a vector.\n\n\nHmisc::GiniMd(x, na.rm = F) (doc)\nsjstats::gmd(x or df, ...) (doc)\n\nIf â€œdfâ€ then it will compute gmd for all vectors in the df\nâ€œâ€¦â€ allows for use of tidy selectors\n\nManual\ngmd &lt;- function(x) {\nÂ  n &lt;- length(x)\nÂ  sum(outer(x, x, function(a, b) abs(a - b))) / n / (n - 1)\nÂ  }\n\n\n\n\n\nRescaling/Normalization\n\nMisc\n\nIf the values of the feature get rescaled between 0 and 1, i.e.Â [0,1], then itâ€™s called normalization\nExcept in min/max, all values of the scaling variable should be &gt; 0 since you canâ€™t divide by 0\n{datawizard::rescale} - Scales variable to a specified range\n\nMin/Max\n\nRange: [0, 1]\n\n\nMake sure the min max value are NOT outliers. If they are outliers, then the range of your data will be more constricted that it needs to be.\n\ne.g.Â if values are in between 100 and 500 with an exceptional value of 25000, then 25000 is scaled as 1 and all the other values become very close to the lower bound of zero\n\nExample: Age is the predictor and Happiness is the outcome. Imagine a very strong relationship between age and happiness, such that happiness is at its maximum at age 18 and its minimum at age 65. Itâ€™ll be easier if we rescale age so that the range from 18 to 65 is one unit. Now this new variable A ranges from 0 to 1, where 0 is age 18 and 1 is age 65. (from Statistical Rethinking section 6.3.1 pg 182)\nd2 &lt;- d[ d$age&gt;17 , ] # only adults\nd2$A &lt;- ( d2$age - 18 ) / ( 65 - 18 )\n\nRange: [a, b]\n\nAlso see notebook for code to transform more than 1 variable at a time.\n\nBy max\nscaled_var = var/max(var)\n\nExample: From Statistical Rethinking, pg 246\n\nâ€œâ€¦ zero ruggedness is meaningful. So instead terrain ruggedness is divided by the maximum value observed. This means it ends up scaled from totally flat (zero) to the maximum in the sample at 1 (Lesotho, a very rugged and beautiful place).â€\n\nExample: From Statistical Rethinking, pg 258\n\nâ€œIâ€™ve scaled blooms by its maximum observed value, for three reasons. First, the large values on the raw scale will make optimization difficult. Second, it will be easier to assign a reasonable prior this way. Third, we donâ€™t want to standardize blooms, because zero is a meaningful boundary we want to preserve.â€\n\nblooms is bloom size. So there canâ€™t be a negative but zero makes sense.\nblooms is 2 magnitudes larger than both its predictors.\n\n\n\nBy mean\nscaled_var = var/mean(var)\n\nExample: From Statistical Rethinking, pg 246\n\nâ€œlog GDP is divided by the average value. So it is rescaled as a proportion of the international average. 1 means average, 0.8 means 80% of the average, and 1.1 means 10% more than average.â€\n\n\n\n\n\nLogging\n\nUseful for skewed variables\nIf you have zeros, then its common to add 1 to the predictor values\n\nTo backtransform: exp(logged_predictor) - 1\narcsinh(x): approximates a log (at large values of x) but handles 0s:\n\nBacktransform: log(x + sqrt(1+x^2))\n\n* Donâ€™t use these for outcome variables (See Regression, Other &gt;&gt; Zero-Inflated/Truncated &gt;&gt; Continuous for methods, Thread for discussion and link to a paper on the alternatives)\n\nThe scale of the outcome matters. The thread links to a discussion of a paper on log transforms.\nProposals in the paper are in Section 4.1. One of the recommendations is log(E[Y(0)] + Y) where (I think) E[Y(0)] is the average value of Y when Treatment = 0 but Iâ€™m not sure. Need to read the paper.\n\n\nRemember to back-transform predictions if you transformed the target variable\n# log 10 transformed target variable\npreds_intervals &lt;- predict(\nÂ  workflows::pull_workflow_fit(lm_wf),\nÂ  workflows::pull_workflow_prepped_recipe(lm_wf) %&gt;% bake(ames_holdout),\nÂ  type = \"pred_int\",\nÂ  level = 0.90\n) %&gt;%Â \nÂ  mutate(across(contains(\".pred\"), ~10^.x))\nCombos\n\nLog + scale by mean\n\nExample From Statistical Rethinking Ch8 pg 246\n\nâ€œRaw magnitudes of GDP arenâ€™t meaningful to humans. Since wealth generates wealth, it tends to be exponentially related to anything that increases it (earlier in chapter). This is like saying that the absolute distances in wealth grow increasingly large, as nations become wealthier. So when we work with logarithms instead, we can work on a more evenly spaced scale of magnitudesâ€\nâ€œLog GDP is divided by the average value. So it is rescaled as a proportion of the international average. 1 means average, 0.8 means 80% of the average, and 1.1 means 10% more than average.â€"
  },
  {
    "objectID": "qmd/feature-engineering-general.html#sec-feat-eng-gen-disc",
    "href": "qmd/feature-engineering-general.html#sec-feat-eng-gen-disc",
    "title": "General",
    "section": "Discrete",
    "text": "Discrete\n\nQuantitative variables that are countable with no in-between the values. (e.g.Â integer value variables)\n\ne.g.Â Age, Height (depending on your scale), Year of Birth, Counts of things\n\nMany variables can be either discrete or continuous depending on whether they are â€œexactâ€ or have been rounded (i.e.Â their scale).\n\nTime since event, distance from location\nA zip code would not be a discrete variable since it is not quantitative (i.e.Â donâ€™t represent amounts of anything). The values just represent geographical locations and could just as easily be names instead of numbers. There is no inherent meaning to arithmetic operations performed on them (e.g.Â zip_code1 - 5 has no obvious meaning)\n\nBinning\n\nSee Binning\n\nRange to Average\n\nSo numerical range variables like Age can have greater predictive power in ML/DL algorithms by just using the average value of the range\ne.g.Â Age == 21 to 30 â€“&gt; (21+30)/2 = 25.5\n\nRates/Ratios\n\nSee Domain Specific\n\nMin/Max Rescaling\n\nSee Continuous &gt;&gt; Transformations &gt;&gt; Rescaling/Normalization"
  },
  {
    "objectID": "qmd/feature-engineering-general.html#sec-feats-general-cats",
    "href": "qmd/feature-engineering-general.html#sec-feats-general-cats",
    "title": "General",
    "section": "Categoricals",
    "text": "Categoricals\n\nMisc\n\nSee Feature Engineering, Embeddings &gt;&gt; Engineering\nOne-Hot Encode Issues:\n\nWith high cardinality, the feature space explodes â€“&gt;\n\nLess power\nLikely to encounter memory problems\n\nUsing a sparse matrix is memory efficient which might make the one-hot encode feasible\n\nSparse data sets donâ€™t work well with highly efficient tree-based algorithms like Random Forest or Gradient Boosting.\n\nModel canâ€™t determine similarity between categories (embedding does)\nEvery kind of encoding and embedding outperforms it by a lot, especially in tree models\n\n\n\n\nCombine/Lump/Collapse\n\nCollapse categories with similar characteristics to reduce dimensionality\n\nstates to regions (midwest, northwest, etc.)\n\nLump\n\nCat vars with levels with too few counts â€“&gt; lump together into an â€œOtherâ€ category\nstep_other(cat_var, threshold = 0.01) # see\n\nFor details see Model Building, tidymodels &gt;&gt; Recipe\nLevels with too few data will have large uncertainties about the effect and the bloated std.devs can cause some models to throw errors\n\n\nCombine\n\nThe feature reduction can help when data size is a concern\n\nThink this is equivalent to a cat-cat interaction.Â  ML models usually algorithmically create interactions but I guess this way you get the interaction but with fewer features.\nAlso might be useful to use the same considerations that you use to choose interactions to choose which cat variables to combine.\n\nSteps\n\nCombine var1 and var2 (e.g.Â â€œdogâ€, â€œminnesotaâ€) to create a new feature called var3 (â€œdog_minnesotaâ€).\nRemove individual features (var1 and var2) from the dataset.\nencode (one-hot, dummy, etc.) var 3\n\n\n\n\n\nEncode/Hashing\n\nCat vars with high numbers of levels need encoded\nCanâ€™t dummy var because it creates too many additional variables â€“&gt; reduces power\nNumeric: as.numeric(as.factor(char_var))\nTarget Encoding\n\n{collinear}\n\ntl;dr; I donâ€™t see a method that stands out as theoretically better or worse than the others. The rnorm method would probably produce the most variance within the predictor.\ntarget_encoding_lab takes a df and encodes all categoricals using all or some of the methods\nRank (target_encoding_rank): Returns the rank of the group as a integer, starting with 1 as the rank of the group with the lower mean of the response variable\n\nwhite_noise argument might be able to used.\n\nMean (target_encoding_mean): Replaces each value of the categorical variable with the mean of the response across the category the given value belongs to.\n\nThe argument, white_noise, limits potential overfitting. Must be a value betwee 0 and 1. The value added depends on the magnitude of the response. If response is within 0 and 1, a white_noise of 0.25 will add to every value of the encoded variable a random number selected from a normal distribution between -0.25 and 0.25\n\nrnorm (target_encoding_rnorm): Computes the mean and standard deviation of the response for each group of the categorical variable, and uses rnorm() to generate random values from a normal distribution with these parameters.\n\nThe argument rnorm_sd_multiplier is used as a multiplier of the standard deviation to control the range of values produced by rnorm() for each group of the categorical predictor. Values smaller than 1 reduce the spread in the results, while values larger than 1 have the opposite effect.\n\nLOO (target_encoding_loo): Replaces each categorical value with the mean of the response variable across the other cases within the same group.\n\nThe argument, white_noise, limits potential overfitting.\n\n\n{{category_encoders}}\npip install category_encoders\nimport category_encoders as ce\ntarget_encoder = ce.TargetEncoder(cols=['cat_col_1', 'cat_col_2'])\ntarget_encoder.fit(X, y)\nX_transformed = target_encoder.transform(X_pre_encoded)\n\nCatboost Encoder\npip install category_encoders\nimport category_encoders as ce\ntarget_encoder = ce.CatBoostEncoder(cols=['cat_col_1', 'cat_col_2'])\ntarget_encoder.fit(X, y)\nX_transformed = target_encoder.transform(X_pre_encoded)\nBinary Encoding\n\nBenchmarks for decision trees:\n\nNumeric best (&lt; 1000 categories)\nBinary best (&gt; 1000 categories)\n\nStore N cardinalities using ceil(log(N+1)/log(2)) features\n\n\nHashing\n\nBeyond security and fast look-ups, hashing is used for similarity search.\n\ne.g.Â Different pictures of the same thing should have similar hashes\nSo, if these hashes are being binned, youâ€™d want something a hashing algorithm thinks is similar to actually be similar in order for this to be most effective.\n\nzip codes, postal codes, lat + long would be good\nNot countries or counties since Iâ€™d think the hashing similarity would be related to how similar they are alphabetically or maybe phonetically\nMaybe something like latin species names since those have similar roots, etc. would work. (e.g.Â dogs are canis-whatever)\n\n\nCanâ€™t be reversed to the original values\n\nAlthough since you have the original, it seems like you could see which cat levels are in a particular hash and maybe glean some latent variable\n\nCreates dummies for each cat but fewer of them.\n\nIt is likely that multiple levels of the column will map to the same hashed columns (even with small data sets). Similarly, it is likely that some columns will have all zeros.\n\nA zero-variance filter (via recipes::step_zv) is recommended for any recipe that uses hashed columns\n\n\nembed::step_feature_hash - Dimension Reduction. It hashes the categorical level string and then bins them somehow\n\nDocs donâ€™t say which algorithm this uses, so I donâ€™t know if itâ€™s data independent hashing or not. If it is, then this probably doesnâ€™t help.\n\n\nLikelihood Encodings\n\nEstimate the effect of each of the factor levels on the outcome and these estimates are used as the new encoding. The estimates are estimated by a generalized linear model. This step can be executed without pooling (via glm) or with partial pooling (stan_glm or lmer). Currently implemented for numeric and two-class outcomes.\n{embed}\n\nstep_lencode_glm, step_lencode_bayes , and step_lencode_mixed\n\n\n\n\n\nOrdinal\n\nMisc\n\nIf there are NAs or Unknowns, etc.,\n\nAfter coercing into a numeric/integer, you can convert Unknowns to NA and then impute the variable\n\nAll these encodings will produce the same results for a tree model, since tree-based models rely on variable ranks rather than exact values.\n0 = â€œ0 Childrenâ€\n1 = â€œ1 Childâ€\n2 = â€œ2 Childrenâ€\n3 = â€œ3 Childrenâ€\n4 = â€œ4 or more Childrenâ€\n\n1 = â€œ0 Childrenâ€\n2 = â€œ1 Childâ€\n3 = â€œ2 Childrenâ€\n4 = â€œ3 Childrenâ€\n5 = â€œ4 or more Childrenâ€\n\n-100 = â€œ0 Childrenâ€\n-85Â  = â€œ1 Childâ€\n0Â  Â  = â€œ2 Childrenâ€\n10Â  = â€œ3 Childrenâ€\n44Â  = â€œ4 or more Childrenâ€\n\nVia {tidymodels}\nstep_mutate(ordinal_factor_var = as_integer(ordinal_factor_var))\n# think this uses as_numeric\nstep_ordinalscore(ordinal_factor_var)\nPolynomial Contrasts\n\nSee the section Kuhnâ€™s book\n\nRainbow Method (article)\n\nMIsc\n\nCreates an artifical ordinal variable from a nominal variable (i.e.Â ordering colors according the rainbow, roy.g.biv)\nAt worst, it maintains the signal of a one-hot encode, but with tree models, it results in less splits and therefore a simpler, more efficient, and less overfit model.\nTest psuedo ordinal method by constructing a simple bayesian model with response ~ 0 + ordinal. Then, you extract the posterior for each constructed ordinal level. Pass these posteriors through a constraint that labels draws for that level that are less (or not) than the draws of the previous level. Lastly calculate the proportion of those that were less than in order to get a probability that the predictor is ordered (article &gt;&gt; â€œThe Modelâ€ section)\n\nCode\ngrid &lt;- data.frame(\n  Layer = c(\"B\", \"C\", \"E\", \"G\", \"I\"),\n  error = 0\n)\n\ngrid_with_mu &lt;- tidybayes::add_linpred_rvars(grid, simple_mod, value = \".mu\")\n\nis_stratified &lt;- with(grid_with_mu, {\n  .mu[Layer == \"B\"] &gt; .mu[Layer == \"C\"] &\n  .mu[Layer == \"C\"] &gt; .mu[Layer == \"E\"] &\n  .mu[Layer == \"E\"] &gt; .mu[Layer == \"G\"] &\n  .mu[Layer == \"G\"] &gt; .mu[Layer == \"I\"]\n})\n\nPr(is_stratified)\n#&gt; [1] 0.78725\nâ€œLayerâ€ is the ordinal variable being tested\nadd_linpred_rvars extracts the mean response posteriors for each level of the variable\nResults strongly suggest that the levels of the variable (â€œLayerâ€) are ordered, with a 0.79 posterior probability.\n\n\nMethods:\n\nDomain Knowledge\nVariable Attribute (see examples)\nOthers - Best to compute these on a hold out set, so as not cause data leakage\n\nAssociation with the target variable where the value of association is used to rank the categories\nProportion of the event for a binary target variable where the value of the proportion is used to rank the categories\n\n\nIf itâ€™s possible, use domain knowledge according the projectâ€™s context to help choose the ranking of the categories.\nThere are always multiple ways to rank the categories, so it may be worthwhile to try multiple versions of the artificial ordinal variable\n\nNot recommended to use more than logâ‚‚(K) versions, so as to not surpass the number of variables creating using One-hot (where k is the number of categories)\n\nExample: Vehicle Type\n\nCategories\n\nC: â€œCompact Carâ€\nF: â€œFull-size Carâ€\nL: â€œLuxury Carâ€\nM: â€œMid-Size Carâ€\nP: â€œPickup Truckâ€\nS: â€œSports Carâ€\nU: â€œSUVâ€\nV: â€œVanâ€\n\nPotential attributes to order by: vehicle size, capacity, price category, average speed, fuel economy, costs of ownership, motor features, etc.\n\nExample: Occupation\n\nCategories\n\n1: â€œProfessional/Technicalâ€\n2: â€œAdministration/Managerialâ€\n3: â€œSales/Serviceâ€\n4: â€œClerical/White Collarâ€\n5: â€œCraftsman/Blue Collarâ€\n6: â€œStudentâ€\n7: â€œHomemakerâ€\n8: â€œRetiredâ€\n9: â€œFarmerâ€\nA: â€œMilitaryâ€\nB: â€œReligiousâ€\nC: â€œSelf Employedâ€\nD: â€œOtherâ€\n\nPotential attributes to order by: average annual salary, by their prevalence in the geographic area of interest, or variables in a Census dataset or some other data source\n\n\n\n\n\nWeight of Evidence\n\nembed::step_woe"
  },
  {
    "objectID": "qmd/feature-engineering-general.html#sec-feat-eng-gen-inter",
    "href": "qmd/feature-engineering-general.html#sec-feat-eng-gen-inter",
    "title": "General",
    "section": "Interactions",
    "text": "Interactions\n\nManually\n\nNumeric â¨¯ Cat\n\nDummy the cat, then multiply the numeric times each of the dummies."
  },
  {
    "objectID": "qmd/feature-engineering-general.html#sec-feat-eng-gen-date",
    "href": "qmd/feature-engineering-general.html#sec-feat-eng-gen-date",
    "title": "General",
    "section": "Date",
    "text": "Date\n\nDuration\n\nDays since last purchase per customer\n\nExample: (max(invoice_date) - max_date_overall) / lubridate::ddays(1)\n\nThink ddays converts this value to a numeric\n\n\nCustomer Tenure\n\nExample: (min(invoice_date) - max_date_overall) / lubridate::ddays(1)"
  },
  {
    "objectID": "qmd/feature-engineering-general.html#sec-feat-eng-gen-dom",
    "href": "qmd/feature-engineering-general.html#sec-feat-eng-gen-dom",
    "title": "General",
    "section": "Domain Specific",
    "text": "Domain Specific\n\nRates/Ratios\n\nPurchase per Customer\n\nTotal Spent\n\nExample: sum(total_per_invoice, na.rm = TRUE)\n\nAverage Spent\n\nExample: mean(total_per_invoice, na.rm = TRUE)\n\n\nLet the effect of Cost vary by the personâ€™s income\n\nmutate(cost_income = cost_of_product/persons_income)\nIntuition being that the more money you have the less effect cost will have on whether purchase something.\nDividing the feature by income is equivalent to dividing the \\(\\beta\\) by income.\n\n\nPre-Treatment Baseline\n\nExample: From Modeling Treatment Effects and Nonlinearities in A/B Tests with GAMS\n\noutcome = log(profit), treatment = exposure to internation markets, group = store\nBaseline variable is log(profit) before experiment is conducted\n\nShould center this variable"
  },
  {
    "objectID": "qmd/feature-engineering-geospatial.html#sec-feat-eng-geo-misc",
    "href": "qmd/feature-engineering-geospatial.html#sec-feat-eng-geo-misc",
    "title": "Geospatial",
    "section": "Misc",
    "text": "Misc\n\nAlso see Real Estate &gt;&gt; Features\nSafegraph data\n\nMisc\n\n$200 free credits\n{SafeGraphR}\narticle\n\nMeasure foot traffic\nLocate where you customers come from"
  },
  {
    "objectID": "qmd/feature-engineering-geospatial.html#sec-feat-eng-geo-latlongtran",
    "href": "qmd/feature-engineering-geospatial.html#sec-feat-eng-geo-latlongtran",
    "title": "Geospatial",
    "section": "Latitude and Longitude Transformations",
    "text": "Latitude and Longitude Transformations\n\nAlso see Engineered &gt;&gt; Bin Latitude and Longitude\nDecimal to Degrees, Minutes, Secs\n\nExample: 123.875\n\nDegrees: 123\nMinutes: .875 \\(\\times\\) 60 = 52.5 \\(\\rightarrow\\) 52 minutes\nSeconds: 0.5 (from minutes calculation) \\(\\times\\) 60 = 30 seconds\n\n\nRound\n\nRound to longitude and latitude to 3 or 4 decimal places\n\nRadians\n\nConvert the latitude and longitude to radians\n\nGeocode\n\nGeocode Addresses to latitude and longitude\n\n{tidygeocoder}\nnew_tbl &lt;- old_tbl %&gt;%\nÂ  Â  geocode(\nÂ  Â  Â  Â  address = &lt;address_var&gt;, #e.g. \"city, state\"\nÂ  Â  Â  Â  method = \"arcgis\"\nÂ  Â  )\n\n\nReverse Geocode\n\nlatitude and longitude to Addresses\nCreate categoricals like city, state, country\nIf the coordinates are in the same city, for example, then extracting the street address would be necessary\n{tidygeocoder}\nnew_tbl &lt;- old_tbl %&gt;%\nÂ  Â  reverse_geocode(\nÂ  Â  Â  Â  lat = \"&lt;latitude_var&gt;\",\nÂ  Â  Â  Â  long = \"&lt;longitude_var&gt;\",\nÂ  Â  Â  Â  address = \"estimated_address\", # disired column name for result\nÂ  Â  Â  Â  method = \"arcgis\"\nÂ  Â  )\n\nOutputs street address, city, state, zip, country\n\n\nCosine and Sine\n\nCyclical variable (0Â° - 360Â°) (e.g.Â slope aspect)\nnew_var &lt;- cos(old_var) * pi / 180)\nnew_var &lt;- sin(old_var) * pi / 180)\n\nIf thereâ€™s a specific azimuth direction (or point in the cycle) where you expect the maximum or minimum, you can shift the aspect variable before calculating the (co)sine.\n\nLatitude and Longitude\nx = cos(lat) * cos(lon)\ny = cos(lat) * sin(lon)\nz = sin(lat)\n\nLatitude and Longitude are 2-D coordinates representing a 3-D system. This transforms them to 3-D.\n\n\nTo Decimal\n\nfunction"
  },
  {
    "objectID": "qmd/feature-engineering-geospatial.html#sec-feat-eng-geo-zip",
    "href": "qmd/feature-engineering-geospatial.html#sec-feat-eng-geo-zip",
    "title": "Geospatial",
    "section": "Zip Codes",
    "text": "Zip Codes\n\nIssues\n\nThere are 10K Point ZIP codes out of ~42K US ZIP codes and only ~32K ZIP codes have a physical boundary\nEvery country has its own ZIP code system\nArenâ€™t necessarily a location\n\ne.g.Â US Navy, which has its own ZIP code, but no permanent location\n\n\nTypes - PO Box, Unique (individual addresses), Military, and Standard\nMight be easier to work with Zip Code Tabulation Areas (ZCTAs) (see bkmks Geospatial &gt;&gt; Resources &gt;&gt; Shapefiles)\nSee zip_code_database.csv in R &gt;&gt; Data for different attributes of all the US zipcodes (lat, long, cities, population, etc.)\n\nFrom https://www.unitedstateszipcodes.org/zip-code-database/\n\nHashing might be a good approach (See Feature Engineering, General &gt;&gt; Categorical &gt;&gt; Encoding/Hashing)"
  },
  {
    "objectID": "qmd/feature-engineering-geospatial.html#sec-feat-eng-geo-eng",
    "href": "qmd/feature-engineering-geospatial.html#sec-feat-eng-geo-eng",
    "title": "Geospatial",
    "section": "Engineered",
    "text": "Engineered\n\nDistance from\n\nDistance from a school, supermarket, movie theater, interstate, highway, industrial park, etc.\nDistance from the nearest city, Distance to the nearest big city\nThe Harvsine Formula takes into account the Earthâ€™s curvature for a more accurate distance\n\nNearest Location\n\nExamples: Nearest city name, Nearest big city, Nearest city population\n\nCluster Lat/Long\n\nCluster longitude and latitude and then one-hot encode the cluster labels\nDBSCAN and Hierarchical is supposed to work best with geospatial features\n\nBinning and Crossing\n\nâ€œCrossingâ€ is multiplying 1 boolean column times another or more boolean columns\nBinning prevents a change in latitude producing the same result as a change in longitude (?)\nYou can also bin latitude and longitude to different granularities such as by neighborhood or city block to provide info about those areas\nIf the column is continuous, then it needs to be binned and one-hot encoded before being crossed\nBinning and crossing latitude and longitude with a 3rd binned feature provides area info about that 3rd predictor to the model\n\nExample binned_latitude x binned_longitude x binned_traffic_stops where latitude and longitude are binned by city block\n\nProvides the model with info about traffic stops at the city block granularity\n\n\nPseudo code\nbinned_latitude(lat) = [\nÂ  0Â  &lt; lat &lt;= 10\nÂ  10 &lt; lat &lt;= 20\nÂ  20 &lt; lat &lt;= 30\n]\n\nbinned_longitude(lon) = [\nÂ  0Â  &lt; lon &lt;= 15\nÂ  15 &lt; lon &lt;= 30\n]\n\n# crossing the binned features\nbinned_latitude_X_longitude(lat, lon) = [\nÂ  0Â  &lt; lat &lt;= 10 AND 0Â  &lt; lon &lt;= 15\nÂ  0Â  &lt; lat &lt;= 10 AND 15 &lt; lon &lt;= 30\nÂ  10 &lt; lat &lt;= 20 AND 0Â  &lt; lon &lt;= 15\nÂ  10 &lt; lat &lt;= 20 AND 15 &lt; lon &lt;= 30\nÂ  20 &lt; lat &lt;= 30 AND 0Â  &lt; lon &lt;= 15\nÂ  20 &lt; lat &lt;= 30 AND 15 &lt; lon &lt;= 30\n]\n\ni.e.Â both conditions true â€“&gt; 1, either condition false â€“&gt; 0\n\nCyclic var\nbin_cycl_var &lt;- function(x) {\nÂ  cut((x + 22.5) %% 360, breaks = seq(0,360,by=45),\nÂ  Â  Â  labels = c(\"N\",\"NE\",\"E\",\"SE\",\"S\",\"SW\",\"W\",\"NW\"))\n}\n\nSine/Cosine transform or cyclic smoothing spline will probably produce better results\n\n\nBin Lat/Long Then Transform\n\nEmbed: use categorical embedding\n\nMight be better to bin, cross, then embed\nOr depending on the number of unique pairs, maybe just cross and embed\n\nExact Indexing: maps each grid cell to a dedicated embedding. This takes up the most space.\nFeature Hashing: maps each grid cell into a compact range of bins using a hash function. The number of bins is much smaller than with exact indexing.Â \n\nâ€œWhile feature hashing saved space compared to exact indexing, the accuracy was the same or slightly worse depending on the grid resolution. This is likely due to hash collisions that cause some information to be lostâ€ (from Uber ETA model article, also see Deep Learning, Tabular &gt;&gt; Architectures &gt;&gt; DeepETA: Uberâ€™s ETA model)\n\nMultiple Feature Hashing: extends feature hashing by mapping each grid cell to multiple compact ranges of bins using independent hash functions\n\n\nâ€œProvided the best accuracy and latency while still saving space compared to exact indexing. This implies that the network is able to combine the information from multiple independent hash buckets to undo the negative effects of single-bucket collisions.â€ (from Uber ETA model article, also see Deep Learning, Tabular &gt;&gt; Architectures &gt;&gt; DeepETA: Uberâ€™s ETA model)\nI think this means they binned longitude and latitude at 3 different grid resolutions, then each cell is hashed twice with 2 independent hashing functions. This results in 6 features(?)\n\n\nCyclic Smoothing Spline\n\nmgcv::s(cyclic_var, bs = \"cc\", k = k)Â \n\nk lets you optionally control the e.d.f. to avoid overfitting\n\nexample used k = 10\nI have no idea what edf is lol"
  },
  {
    "objectID": "qmd/feature-engineering-time-series.html#sec-feat-eng-ts-misc",
    "href": "qmd/feature-engineering-time-series.html#sec-feat-eng-ts-misc",
    "title": "Time Series",
    "section": "Misc",
    "text": "Misc\n\nPackages\n\n{feasts}\n\nThe package works with tidy temporal data provided by the tsibble package to produce time series features, decompositions, statistical summaries and convenient visualisations. These features are useful in understanding the behaviour of time series data, and closely integrates with the tidy forecasting workflow used in the fable package\n\n{theft}\n\nProvides a single point of access to &gt; 1200 time-series features from a range of existing R and Python packages. The packages which theft â€˜stealsâ€™ features from currently are:\n\ncatch22 (R; see Rcatch22 for the native implementation on CRAN)\nfeasts (R)\ntsfeatures (R)\nKats (Python)\ntsfresh (Python)\nTSFEL (Python)\n\n\n{timetk}\n\nIncorporates tsfeatures package, timetk::tk_tsfeatures\nExample: Take weekly dataset and compute tsfeatures for each quarter\nnew_dat &lt;- dat %&gt;%\nÂ  Â  mutate(date_rounded = lubridate::round_date(date, \"quarter\")) %&gt;%\nÂ  Â  group_by(date_rounded) %&gt;%\nÂ  Â  timetk::tk_tsfeatures(\nÂ  Â  Â  Â  .date_var = date,\nÂ  Â  Â  Â  .value = price,\nÂ  Â  Â  Â  .features = c(\"median\", \"frequency\", \"stl_features\", \"entropy\", \"acf_features\"),\nÂ  Â  Â  Â  .prefix = \"tsfeat_\"\nÂ  Â  ) %&gt;%\nÂ  Â  ungroup()\n\n.features is for specifying the names of the features from tsfeatures that you want to include\n.prefix is the prefix for the newly created column names\ndate_rounded is a column that has the date for each quarter\n\n\n{fractaldim} (paper) - The fractal dimension of a series measures its roughness or smoothness.\n{tsfeatures}\n\nExample\nlibrary(tsfeatures)\nts_fts &lt;- \n  tsfeatures(\n    ts_data,\n    features = c(\n        \"acf_features\", \"outlierinclude_mdrmd\",\n        \"arch_stat\", \"max_level_shift\",\n        \"max_var_shift\", \"entropy\",\n        \"pacf_features\", \"firstmin_ac\",\n        \"std1st_der\", \"stability\",\n        \"firstzero_ac\", \"hurst\",\n        \"lumpiness\", \"motiftwo_entro3\"\n      )\n  )\n\n{{kats}}\n\nTime series analysis, including detection, forecasting, feature extraction/embedding, multivariate analysis, etc. Kats is released by Facebookâ€™s Infrastructure Data Science team.\n\nhctsa - a Matlab software package for running highly comparative time-series analysis. It extracts thousands of time-series features from a collection of univariate time series and includes a range of tools for visualizing and analyzing the resulting time-series feature matrix. Can be ran through CLI. Calculates like 7000 features.\n\ncatch22 (paper) extracts only 22 canonical features (so much faster) used in hctsa and can be used in R {Rcatch22}, Python {{pycatch22}}, or Julia Catch22.jl.\n\n\nIssue: Features that arenâ€™t available during the forecast horizon arenâ€™t useful.\n\nSolutions:\n\nBut you can use a lagged value of predictor or an aggregated lagged value e.g.Â averages, rolling averages, tiled/windowed averages\n\nExample: average daily customers for the previous week.\n\nSome features are one value per series, but the functions could be fed a lagged window (length of horizon?) of the whole series and generate a value for each window.\n\n\nDifference features that are really linear or have little variation. Change in value can be more informative\nForecasting shocks is difficult for an algorithm\n\nIt can better to smooth out (expected) shocks (Christmas) in the training data and then add an adjustment to the predictions during the dates of the shocks.\nThe smoothed out data will help the algorithm produce more accurate predictions for days when there isnâ€™t an expected shock.\nExamples of shocks that may need training data to have manual adjustments and not be smoothed by an algorithm\n\none-time spikes due to abnormal weather conditions\none-off promotions\na sustained marketing campaign that is indistinguishable from organic growth.\n\n\nModels with large numbers (100s) of features increases the opportunity for feature drift\n\nPackage feature set comparison\n\nFrom paper, An Empirical Evaluation of Time Series Feature Sets\nMisc\n\nI think feasts âŠ† tsfeatures âŠ† catch22 âŠ† hctsa\nkats is a facebook python library\ntheft package integrates all these packages\n\nfeature redundancy\n\n\nSays catch22 features have fewer things in common with each other that the other packages\n\nComputation time\n\n\nNo surprise hctsa takes the most time. Itâ€™s like 7K features or something stupid\ntsfeatures,feasts are pretty disappointing\ncatch22 is excellent"
  },
  {
    "objectID": "qmd/feature-engineering-time-series.html#sec-feat-eng-ts-tran",
    "href": "qmd/feature-engineering-time-series.html#sec-feat-eng-ts-tran",
    "title": "Time Series",
    "section": "Tranformations",
    "text": "Tranformations\n\nLogging a feature can create more compact ranges, which then enables more efficient neural network training\nLog before differencing (SO post)\nstep_normalize(all_predictors)\n\nActually standardizes the variables\nIf youâ€™re using predictors on different scales\n\nlm (and AR) are scale invariant, so not really necessary for those models\n\n\nSmoothing\n\nLOESS (LOcally WEighted Scatter-plot Smoother)\n\nWeights are applied to the neighborhood of each point which depend on the distance from the point\nA polynomial regression is fit at each data point with points in the neighborhood as explanatory variables\nSome robustness to outliers (by downweighting large residuals and refitting)\nspan: the distance from each data that considered the neighborhood is controlled by this argument\n\nDefault: 0.75\n&lt; 1: the value represents the proportion of the data that is considered to be neighbouring x, and the weighting that is used is proportional to 1-(distance/maximum distance)3)3, which is known as tricubic\nChoosing a value thatâ€™s too small will result in insufficient data near x for an accurate fit, resulting in a large variance\nChoosing a value thatâ€™s too large will result in over-smoothing and a loss of information, hence a large bias.\n\ndegree: degree of the polynomial regression used to fit the neighborhood data points\n\nDefault: 2 (quadratic)\nHigh degree: provides a better approximation of the population mean, so less bias, but there are more factors to consider in the model, resulting in greater variance.\n\nHigher than 2 typically doesnâ€™t improve the fit very much.\n\nLower degree: (i.e.Â 1, linear) has more bias but pulls back variance at the boundaries.\n\nExample: {ggplot}\nggplot(data, aes(x = time, y = price)) +\nÂ  Â  geom_line(alpha = 0.55, color = \"black\") +Â \nÂ  Â  geom_smooth(aes(color = \"loess\"), formula = y ~ x, method = \"loess\", se = FALSE, span = 0.70) +\nÂ  Â  scale_color_manual(name = \"smoothers\", values = c(\"ma\" = \"red\", \"loess\" = \"blue\"))\n\n\nExample: base r\nloess_mod &lt;- stats::loess(price ~ time, data = dat, span = 0.05, degree = 4)\ndat$loess_price &lt;- fitted(loess_mod)\nCubic Regression Splines\n\nExample: {mgcv}: mgcv::gam(price ~ s(time, bs = \"cs\"), data = data, method = \"REML\")$fitted.values"
  },
  {
    "objectID": "qmd/feature-engineering-time-series.html#sec-feat-eng-ts-eng",
    "href": "qmd/feature-engineering-time-series.html#sec-feat-eng-ts-eng",
    "title": "Time Series",
    "section": "Engineered",
    "text": "Engineered\n\nMisc\n\nTidymodels\n\ndate variable needs to be role = ID for ML models\n\n\n\n\nDecimal Date\n\nrecipe::step_date(year_month_var, features = c(\"decimal\"))\n\n\n\nCalendar Features\n\nmodeltime::step_timeseries_signature creates a set of calendar features\nCalandar Variables\n\nday of the month, day of the year, week of the month, week of the year, month, and year\nhour of the week (168 hours/week)\nminute, hour\nmorning/afternoon/ night\n\nDaylight Savings - At one point in the year, we have 23 hours in a day, and in another time, we have 25 hours in a day\n\nIf using a smooth::adam model, then it shifts seasonal indices, when the time change happens. All you need to do for this mechanism to work is to provide an object with timestamps to the function (for example, zoo).\n\nLeap Year\n\nBecomes less important when we model week of year seasonality instead of the day of year or hour of year\n\nAs splines\n\nExample: {tidymodels}\nstep_mutate(release_year = year(release_date),\nÂ  Â  Â  Â  Â  Â  release_week = week(release_date)) %&gt;%\nstep_ns(release_year, deg_free = tune(\"deg_free_year\")) %&gt;%\nstep_ns(releas_week, deg_free = tune(\"deg_free_week\"))\n\nMay need lubridate loaded for the mutate part\nCan also use a basis spline (step_bs)\n\n\nExample: {mgcv}\n\nctamm &lt;- \n  gamm(temp ~ s(day.of.year, bs = \"cc\", k=20) + s(time, bs = \"cr\"),\nÂ  Â  Â   data = cairo,\nÂ  Â  Â  Â correlation = corAR1(form = ~1|year))\n\nFrom pg 371, â€œGeneralized Additive Models: An Introduction with R, 2nd Edâ€ (See R/Documents/Regression)\nHighly seasonal so uses a cyclic penalized cubic regression spline for â€œday.of.yearâ€\n10 peaks and 10 valleys probably explains â€œk = 20â€\n\nWith regression models, you have to be careful about encoding categoricals/discretes as ordinal (i.e.Â integers). Linear regression does not model non-monotonic relationships between the input features and the target while tree models do.\n\nFor example, the raw numerical encoding (0-24) of the â€œhourâ€ feature prevents the linear model from recognizing that an increase of hour in the morning from 6 to 8 should have a strong positive impact on the number of bike rentals while a increase of similar magnitude in the evening from 18 to 20 should have a strong negative impact on the predicted number of bike rentals.\nOptions\n\nOne-hot encoding for small cardinality (e.g.Â hours) and binning for large cardinality features (e.g.Â minutes)\nSpline tranformation\n\nReduces number of features with comparable performance to one-hot\nPeriod isnâ€™t used, just here (examples) for reference to the number of splines chosen\nnum_knots = num_splines + 1\nDegree 3 was used in the sklearn tutorial\n\nSKLearn also has an extrapolation=â€œperiodicâ€ arg\n\nâ€œPeriodic splines with a periodicity equal to the distance between the first and last knot are used. Periodic splines enforce equal function values and derivatives at the first and last knot. For example, this makes it possible to avoid introducing an arbitrary jump between Dec 31st and Jan 1st in spline features derived from a naturally periodicâ€day-of-yearâ€ input feature. In this case it is recommended to manually set the knot values to control the period.â€\nDonâ€™t see this arg in step_bs or step_ns\n\n\nExamples\n\nHour feature (period = 24, num_splines = 12)\nWeekday (day of the week, numeric) feature (period=7, num_splines=3)\nMonth (period=12, num_splines=6)\n\n\nSpline transform + step_kpca_poly or step_kpca_rbf\n\nProduced best results in sklearn tutorial\nKernel function smooths out the spline\nAdd kpca_poly allows a regression model to capture non-linear (spline) interactions (kpca_poly)\n\nBoosted trees naturally capture these features\n\nExample in sklearn tutorial used n_components=300 in elasticnet regression which seems crazy, but their point was that if you were to create non-linear interaction features manually itâ€™d be in the thousands\nUsing one-hot features instead of splines would require 3 or 4 times the number of components to reach the same performance which substantially increases training time.\n\n\n\n\n\n\nClustering\n\nSee Clustering, Time Series for details\n\n\n\nLags\n\nIf there is a gap period between the training and the validation (or test) set, all lags should be larger than this period\n\nSometimes predictions will have to be made with data that isnâ€™t up-to-date. So your model training should mimic this.\n\n{recipe}\nrecipe::step_lag(var, lag: 4:7)\n\nCreates 4 lagged variables with lags 4:7\n\n{slider} - more sophisticated way without data leakage\nSCORE_recent &lt;- \n  slider::slide_index_dbl(SCORE,\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  date,\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  mean,\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  na.rm = TRUE,\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  .before = lubridate::days(365*3),\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  .after = -lubridate::days(1),\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  .complete = FALSE)\n\nafter is -lubridate::days(1) says donâ€™t include current day\n\nâ€œPrevents data leakage by ensuring that this feature does not include information from the current day in its calculationâ€\n\nNot sure exactly why including the current day would cause data leakage\n\n\n\n{dplyr} + {purrr}\ncalculate_lags &lt;- function(df, var, lags){\nÂ  map_lag &lt;- lags %&gt;% map(~partial(lag, n = .x))\nÂ  return(df %&gt;% mutate(across(.cols = [{{var}}]{style='color: goldenrod'}, .fns = map_lag, .names = \"{.col}_lag[{lags}]{style='color: #990000'}\")))\n}\ntsla %&gt;%\nÂ  calculate_lags(close, 1:3)\n\n\n\nRolling\n\n{feasts} functions\n\nTiling (non-overlappping) features\nSliding window (overlapping) features\n\nExponentially weighted moving average on lags (more recent values get more weight)\n\nHelps smooth out you lags if your data is noisy\nMight be easier to just smooth the outcome\nH2Oâ€™s weight parameter, alpha,Â  has a range between 0.9 and 0.99\n\nExample: smoothed over 2 days\n\nsales, 1 day ago = 3; 2 days ago = 4.5; and alpha = 0.95\nsmoothed sales = [3.0*(0.95^1) + 4.5*(0.95^2)] / [(0.95^1) + (0.95^2)] = 3.73\n\n\n\nMoving Averages\ndt_ma = data.table::frollmean(data[, 2], n = window, align = \"right\", fill = NA, algo = \"fast\")\nrcpp_ma = RcppRoll::roll_mean(data[, 2], n = window, align = \"right\", fill = NA\n\n{data.table} looks to be slightly faster than {RcppRoll} in a benchmark\n\nBoth are substantially faster than {TTR} and base R. {zoo}â€™s was slowest\n\n{TTR} produced different values (floating point precision) than the other packages\n\n\n\nDescriptive statistics\n\nExample: (weekly) units sold\n\nMetrics: mean, standard deviation, minimum, maximum, 25th percentile, 50th percentile, and 75th percentile\nRolling time windows: 1, 2, 3, 4, 12, 26, 52 weeks prior\n\nExample: rolling average sales from the previous year\n\ne.g.Â 2-week average sales of the previous year\n\n\n\n\n\nInteractions\n\nExamples\n\nInteractions between lags\nBetween workday (indicator) and hour\n\nDistinguishes between 8am on a monday and 8am on a sunday\nSpline transform hour and create interaction with workday\nMain effects(x + y) and interactions (xy) only are included, not variables of the form, x2\n\n\n\n\n\nâ€œDays Sinceâ€ a Specific Date\n\nCheck scatter (e.g.Â price vs last review date) and see if variance is heteroskedastic but should probably be log transformed\n# think this data only has year and month variables (numeric?)\nstep_mutate(days_since = lubridate::today() - lubridate::ymd(year, month, \"01\"))\n\n# last review is a date var\nstep_mutate(last_review = as.integer(Sys.Date() - last_review))\nExample: {lubridate}\nlength_of_stay &lt;- start_date %--% end_date / ddays(1) \nis_los_g90 &lt;- start_date %--% current_date &gt;= ddays(90)\n** consider using the inverse: 1 / number of days since **\n\nThis keeps the value between 0 and 1\nHave to watch out for 0s in the denominator and replace those values with 0 or replace the count with a really small fraction\n\nExamples\n\nLog days since the brand or product first appeared on the market\nThe number of weeks since the product was last sold\n\n\n\n\nInterval Groups\n\nCreating group indicators for different intervals in the series\nProbably involves some sort of clustering of the series or maybe this is what â€œwaveletsâ€ are.\n\n\n\nFourier Transform (sine, cosine)\n\nDecision trees based algorithms (Random Forest, Gradient Boosted Trees, XGBoost) build their split rules according to one feature at a time. This means that they will fail to process these two features simultaneously whereas the cos/sin values are expected to be considered as one single coordinates system.\nHandled differently in different articles so Â¯\\_(ãƒ„)_/Â¯\nFrom https://towardsdatascience.com/how-to-handle-cyclical-data-in-machine-learning-3e0336f7f97c\n\\[\n\\begin{align}\n\\text{Hour}_{\\sin} &= \\sin \\left(\\frac{2\\pi\\cdot\\text{Hour}}{\\max (\\text{Hour})}\\right) \\\\\n\\text{Hour}_{\\cos} &= \\cos \\left(\\frac{2\\pi\\cdot\\text{Hour}}{\\max (\\text{Hour})}\\right)\n\\end{align}\n\\]\n\nExample: Tranforming an hour variable instead one-hot encoding\nThe argument is that since time features are cyclical, their transform should be a cyclical function. This way the difference between transformed 1pm and 2pm values are more closely related than if they were one-hot encoded where the difference between 1pm and 2pm is the same as 1pm and 8pm.\n\nFrom sklearn article\nperiod &lt;- 24 # hours variable\nvar &lt;- dat$hourÂ  Â  Â  Â  Â  Â \nsin_encode &lt;- function (x, period) {sin(x / period * 2 * pi)}\ncos_encode &lt;- function (x, period) {cos(x / period * 2 * pi)}\nsin_encode(x = var, period = period)\nstep_harmonic\nsunspots_rec &lt;-Â \nÂ  recipe(n_sunspot ~ year, data = sun_train) %&gt;%\nÂ    step_harmonic(year, frequency = 1 / 11, cycle_size = 1, # sunspots happen every 11 yrs\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  role = \"predictor\",\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  keep_original_cols = FALSE)\n\n\n\nChange Point Detection\n\nPersistent Homology Notes from Topological Change Point Detection\n\nTopological method\nClassifies the state of a set of features over a sliding window where the states are normal (incoherent), onset (partially coherent), synchronized (fully coherent)\nPython has a library that does this stuff, {geotto-tda}\n\nFor each window a 2-D pearson dissimilarity matrix is computed and a Vietoris-Rips persistence score is calculated â€œup to homology 2.â€ Repeat for each window.\nThe amplitude, persistent entropy and number of diagram points per homology dimension are calculated for each resulting persistence diagram resulting in a feature vector (or maybe its a matrix)\nTuning parameters\n\nWindow Size\nStride (how many steps the window slides)\nCoherence threshold (classification threshold for â€œsynchronizedâ€)\n\nExample: 0.8\n\nOnset threshold (classification threshold for â€œonsetâ€)\n\nExample: 0.5\n\n\n\nUse the trained model to predict classification categories that can be used as a feature.\nThis state is supposed to be predictive of whether a change point is about to happen in the time series\n\nBayesian Change Point Indicator Regression\n\n\n\nDomain Specific\n\nNet calculation: recipe::step_mutate(monthly_net = monthly_export - monthly_import)\nAmount of the last sale\nTotal volume of units sold for each product up to that date\n\nIndicates of long-term historical sales performance\n\nCustomer Age - How long a customer has been with the company\ndat_prep_tbl &lt;- \n  dat_raw |&gt; \n    mutate(dt_customer = dmy(dt_customer),\n           dt_customer_age = -1 * (dt_customer - min(dt_customer)) / ddays(1)) |&gt;\n    select(-dt_customer)\n\nSubtracts the minimum of the customer-first-bought-from-the-company date variable from each customerâ€™s first-bought date.\ndt_customer is a date the customer first bought from the company but in the raw dataset was a character type, so lubridate::dmy coerces it to a Date type\nWhy multiply by -1 instead of reversing the objects being substracted? Why divide by ddays(1)? I dunno. The resultant object is a dbl type, so maybe itâ€™s a formatting thing.\n\n\n\n\nMissing Values\n\nWhen aggregating values (e.g.Â daily to weekly), information about missing values is lost\nCreate a variable that is the sum of missing values over the aggregated period\nMissing rows in the original dataset (due to a lack of sales on those particular dates) can also be counted and added to the dataset"
  },
  {
    "objectID": "qmd/feature-engineering-splines.html#sec-feat-eng-spl-misc",
    "href": "qmd/feature-engineering-splines.html#sec-feat-eng-spl-misc",
    "title": "Splines",
    "section": "Misc",
    "text": "Misc\n\nKnots are placed at several places within the data range with (usually) low-order polynomials are chosen to fit the data between two consecutive knots.\n\nChoices\n\nNumber of knots\nTheir positions\nDegree of polynomial to be used between the knots (a straight line is a polynomial of degree 1)\n\nThe type of polynomial and the number and placement of knots is what defines the type of spline.\n\ne.g.Â cubic splines are created by using a cubic polynomial in an interval between two successive knots.\n\nIncreasing the number of knots may overfit the data and increase the variance, whilst decreasing the number of knots may result in a rigid and restrictive function that has more bias.\n\nNotes from A review of spline function procedures in R (paper)\nAlso see:\n\nFeature Engineering, General &gt;&gt; Continuous &gt;&gt; Binning &gt;&gt; Harrell on the benefits of using splines vs binning\nFeature Engineering, Time Series &gt;&gt; Engineering &gt;&gt; Calendar features\nStatistical Rethinking &gt;&gt; (end of ) Ch 4\nFeature Engineering, Geospatial &gt;&gt; Cyclic Smoothing Spline\nHarrellâ€™s RMS\nModel Building, tidymodels &gt;&gt; Recipe &gt;&gt; Transformations &gt;&gt; Splines\n\nCommon variables: trend, calendar features, age, cardinal directions (N, S, E, W, etc.)\nPackage Comparison\n\nDefault types: {mgcv} uses thin plate splines (see smoothing splines) as a default for itâ€™s s() which makes itâ€™s spline more flexible (i.e.Â curvy) than the default splines for {gam}, {VGAM}, and {gamlss} which use cubic smoothing splines.\n\n{gamlss} doesnâ€™t use s but instead has specific functions for specific types of splines\n\nP-Splines: {mgcv} and {gamlss} are very similar, and the differences can be attributed to the different way that two packages optimize the penalty weight, Î».\n\n{mgcv}: option, â€œpsâ€ within s will create a cubic p-spline basis on a default of 10 knots, with a third order difference penalty.\n\nThe penalty weight, Î», is optimized with generalized cross validation.\n\n{gamlss}: pb defines cubic p-splines functions with 20 interior knots and a second order difference penalty.\n\nThe smoothing parameter is estimated using local maximum likelihood method, but there are also other options based on likelihood methods, AIC, generalized cross validation and more.\nMultiple other functions available for p-splines with various attributes.\n\n\nDependencies: {mgcv} creates its own spline functions while {gam}, {VGAM}, and {gamlss} use the base R package, {splines}.\n\n{gam} and {VGAM} call the base R function smooth.spline (smoothing spline) with four degrees of freedom as default and give identical results"
  },
  {
    "objectID": "qmd/feature-engineering-splines.html#sec-feat-eng-spl-terms",
    "href": "qmd/feature-engineering-splines.html#sec-feat-eng-spl-terms",
    "title": "Splines",
    "section": "Terms",
    "text": "Terms\n\nSmoothly Joined -Â  Means that for polynomials of degree n, both the spline function and its first n-1 derivatives are continuous at the knots."
  },
  {
    "objectID": "qmd/feature-engineering-splines.html#sec-feat-eng-spl-tune",
    "href": "qmd/feature-engineering-splines.html#sec-feat-eng-spl-tune",
    "title": "Splines",
    "section": "Tuning Parameters",
    "text": "Tuning Parameters\n\nB: Basis functions (e.g.Â B-Spline)\nd: The degree of the underlying polynomials in the basis\n\nTypically d = 3 (cubic) is used (&gt;3 usuallly indistinguishable)\n\nK: Number of knots for Regression Splines\n\nUsually k = 3, 4, 5. Often k = 4\n\nHarrell (uses natural splines): â€œFor many datasets, k = 4 offers an adequate fit of the model and is a good compromise between flexibility and loss of precision caused by overfittingâ€\n\nIf the sample size is small, three knots should be used in order to have enough observations in between the knots to be able to fit each polynomial.\nIf the sample size is large and if there is reason to believe that the relationship being studied changes quickly, more than five knots can be used.\n\n\nThere should be at least 10â€“20 events per degree of freedom (Harrell, RMS)\nVariables that are thought to be more influential on the outcome or more likely to have non-linear associations are assigned more degrees of freedom (i.e.Â more knots)\nFlexibility of fit vs.Â n and variance\n\nLarge n (e.g.Â n â‰¥ 100): k = 5\nSmall n (e.g.Â n &lt; 30): k = 3\n\nCan use Akaikeâ€™s information criterion (AIC) to choose k\n\nThis chooses k to maximize model likelihood ratio of Ï‡2 âˆ’ 2k.\nCross-Validation is also valid\n\nAlso option for knot positions\n\nLocations not important in most situations\nPlace knots where data exist e.g.Â fixed quantiles of predictorâ€™s marginal distribution (See Regression Splines &gt;&gt; B-Splines for examples)\n\nFrom Harrellâ€™s RMS\n\n\n\n\nÎ»: Penalty weight for Smoothing Splines\n\nCalculated by generalized cross-validation in {mgcv} which is an approximation of LOO-CV\n\nSee article or Woodâ€™s GAM book or Elements of Statistical Learning (~pg 244) for details"
  },
  {
    "objectID": "qmd/feature-engineering-splines.html#sec-feat-eng-spl-interp",
    "href": "qmd/feature-engineering-splines.html#sec-feat-eng-spl-interp",
    "title": "Splines",
    "section": "Interpretation",
    "text": "Interpretation\n\nA regression fit will result in estimated coefficients for each parameter used in the splines.\nOther than including them in technical appendices, in almost all cases, one does not present these estimated coefficients â€“ their interpretation is essentially meaningless.\nVisual interpretations of the predicted response vs the splined variable are useful in discovering trends or patterns.\nPredicted responses given representative values, outlier values, or any values of interest of the splined variable are useful in calculating various contrasts.\nEffective Coefficient\n\nIt shows how the effect of the variable on the response varies over its range\nThink this is only possible for a natural spline\nExample: Age on Survival in Titanic dataset (link)\n\nmodel_02 &lt;- \n  glm(Survived ~ SibSp + ns(Age, df = 3) + Pclass + Parch + Fare,\n      data = titanic,\n      family = binomial)\n#\n# Create a data frame for prediction: only `Age` will vary.\n#\nN &lt;- 101\nx &lt;- titanic[which.max(complete.cases(titanic)), ]\ndf &lt;- do.call(rbind, lapply(1:N, function(i) x))\ndf$Age &lt;- with(titanic, seq(min(Age, na.rm=TRUE), max(Age, na.rm=TRUE), length.out=N))\n#\n# Predict and plot.\n#\ndf$Survived.hat &lt;- predict(model_02, newdata=df) # The predicted *link,* by default\nwith(df, plot(Age, Survived.hat, type=\"l\", lwd=2, ylab=\"\", main=\"Relative spline term\"))\nmtext(\"Spline contribution\\nto the link function\", side=2, line=2)\n#\n# Plot numerical derivatives.\n#\ndAge &lt;- diff(df$Age[1:2])\ndelta &lt;- diff(df$Survived.hat)/dAge\nage &lt;- (df$Age[-N] + df$Age[-1]) / 2\nplot(age, delta, type=\"l\", lwd=2, ylab=\"Change per year\", xlab=\"Age\",\n     main=\"Spline Slope (Effective Coefficient)\")\n\nThe varying coefficient is computed by calculating the first derivatives numerically: divide the successive differences in predicted values by the successive differences in age.\nAt Age near 35 the effective slope is nearly zero, meaning small changes of Age in this range have no effect on the predicted response. Near ages of zero the effective slope is near âˆ’0.15, indicating each additional year of Age reduces the value of the link function by about 0.15. At the oldest ages the effective slopes are settling down to a value near âˆ’0.09, indicating each additional year of age in this age group decreases the link function by âˆ’0.09."
  },
  {
    "objectID": "qmd/feature-engineering-splines.html#sec-feat-eng-spl-reg",
    "href": "qmd/feature-engineering-splines.html#sec-feat-eng-spl-reg",
    "title": "Splines",
    "section": "Regression Splines",
    "text": "Regression Splines\n\nNo penalty function added\n\nSplined variable is just added to the regression model like any other predictor\n\nTypes\n\nTruncated Power Basis\n\nIssue: Basis functions are not supported locally but over the whole range of the data\n\nCould lead to high correlations between some basis splines, implying numerical instabilities in spline estimation\n\nExample: d = 3 (cubic) with 5 equidistant knots\n\nExample: d = 3 with 3 knots (Ï„1, Ï„2, Ï„3)\n\n\\[\nf(X) = \\beta_0 + \\beta_1 X + \\beta_2 X^2 + \\beta_3 X^3 + \\beta_4 (X - \\tau_1)^3 + \\beta_5 (X - \\tau_2)^3 + \\beta_5 (X - \\tau_3)^3\n\\]\n\n7 dof\n\n\nB-splines\n\n\nBased on a special parameterization of a cubic spline\nSee Statistical Rethinking Notebook &gt;&gt; (end of) Chapter 4\nBasis functions supported locally which leads to high numerical stability, and also in an efficient algorithm for the construction of the basis functions.\nIssue: can be erratic at the boundaries of the data (boundary knots)\nDegrees of freedom (dof) = d + K\nbs(x) will create a cubic B-spline basis with two boundary knots and one interior knot placed at the median of the observed data values\n\nBounded by the range of the data\nlm(y ~ bs(x))\n\nExample: bs(x, degree=2, knots=c(0,.5,1))\n\ndegree specifies d\nknots specifies the number of knots and their locations\n\nExample: bs(x, knots = median(x))\n\n1 interior knot created at the median\n4 dof since d + K = 3 + 1\n\nd = 3 (default)\n\n\nExample: bs(x, knots = c(min(x), median(x), max(x)))\n\n1 interior knot specified at the median and 2 boundary knots at the min and max.\n6 dof since d + K = 3 + 3\n\nd = 3 (default)\n\n\n\nNatural Cubic and Cardinal Splines\n\n\nStable at boundaries of data because of additional constraints that they are linear in the tails of the boundary knots\nDegrees of freedom (dof) = K + 1\nns(x) returns a straight line within the boundary knots\n\nlm(y ~ ns(x))\n\nExample: ns(x,df=3)\n\nâ€œdfâ€ specifies degrees of freedom\nâ€œknotsâ€: alternatively to specifying df, you can specify the knots (# and positions) like in bs\n\nCardinal splines\n\nHave an additional constraint that leads to the interpretation that each coefficient \\(\\beta_k\\) is equal to the value of the spline function at the knot \\(\\tau_k\\)"
  },
  {
    "objectID": "qmd/feature-engineering-splines.html#sec-feat-eng-spl-smth",
    "href": "qmd/feature-engineering-splines.html#sec-feat-eng-spl-smth",
    "title": "Splines",
    "section": "Smoothing Splines (aka Penalized Splines)",
    "text": "Smoothing Splines (aka Penalized Splines)\n\nAutomatically handles the number of knots and knot positions by using a large number of knots and letting Î» control the amount of smoothness\n\nDifferent packages usually produce similar results. Penalties are very powerful in controlling the fit, given that enough knots are supplied into the function\n\nRequires modification of the fitting routine in order to accommodate it\n\nProbably need a GAM package to use.\n\nA special case of the more general class of thin plate splines\nFunction\n\\[\n\\hat{\\beta} = \\arg\\max_{\\beta} [l_\\beta (x_1, y_1, \\ldots, x_n, y_n) - \\lambda J_\\beta]\n\\]\n\nThe maximization of this function implies a trade-off between smoothness and model fit that is controlled by the tuning parameter Î»\nTerms\n\nlÎ² is the likelihood\nJÎ² (penalty function) is the roughness penalty (expresses the smoothness of the spline function)\n\nFor a gaussian regression this is the integrated second derivative of the spline function (see paper for more details)\n\nExample:\n\\[\n||y-f||^2 + \\lambda \\int \\left(\\frac {\\partial^2 f(\\text{log[baseline profit]})}{\\partial \\; \\text{log[baseline profit]}^2}\\right)^2 \\partial x\n\\]\n\n\nÎ» is a tuning parameter thatâ€™s â‰¥0\n\n\nB-Spline basis is typically used\nNot easy to specify the degrees of freedom, since they will vary depending on the size of the penalty\n\nUsually can be restricted to a maximum number of degrees of freedom or desired degrees of freedom\n\nPenalized Regression Splines\n\nApproximation of a smoothing spline\nBest used when n is large and the variable range is covered densely by the observed data\nP-Spline\n\nBased on the cubic B-spline basis and on a â€˜largeâ€™ set of equidistant knots (usually, 10â€“40)\nSimplifies the calculation of JÎ² (see paper for more details)\nPackages: {mgcv}, {gamlss} (See above, Misc &gt;&gt; Package Comparison)"
  },
  {
    "objectID": "qmd/feature-engineering-splines.html#sec-feat-eng-inter",
    "href": "qmd/feature-engineering-splines.html#sec-feat-eng-inter",
    "title": "Splines",
    "section": "Interactions",
    "text": "Interactions\n\nNumeric spline varying by indicator\ns(log_profit_rug_business_b, by = treatment)\n\nCoefficient is a conditional average treatment effect (CATE)\nCreates the main effect and the interaction"
  },
  {
    "objectID": "qmd/feature-engineering-tokenization.html#sec-feat-eng-tok-misc",
    "href": "qmd/feature-engineering-tokenization.html#sec-feat-eng-tok-misc",
    "title": "Tokenization",
    "section": "Misc",
    "text": "Misc\n\nAlso see\n\nFeature Engineering, Embeddings\nDiagnostics, NLP\nEDA, Text\nNLP, General\n\nText augmentation\n\nUseful for imbalanced outcomes with text predictors\n\nBetter performance than subsampling\n\nWords are randomly swapped, deleted, as well as replaced or inserted with synonyms using pretrained word embeddings\n\n{{textattack}}, article, ipynb\nAdversarial Text Attack\n\nDiagnostic to test how a model performs on a test set where data augmentation techniques have been applied\n{{textattack}}, article, ipynb"
  },
  {
    "objectID": "qmd/feature-engineering-tokenization.html#sec-feat-eng-tok-terms",
    "href": "qmd/feature-engineering-tokenization.html#sec-feat-eng-tok-terms",
    "title": "Tokenization",
    "section": "Terms",
    "text": "Terms\n\nOut-of-Vocabulary (OOV) - Sometimes vocabularies a trimmed to the only the most common tokens. Words that arenâ€™t in the vocabulary get assigned OOV tokens.\nTokenization - process of splitting a phrase, sentence, paragraph, one or multiple text documents into smaller units.Â Different algorithms follow different processes in performing tokenization\nToken - output from tokenization â€” a word, a subword (e.g.Â prefix, suffix, or root), or even a character.\nVocabulary - a set of unique tokens in a corpus (a dataset in NLP) that is then converted into numbers (IDs) which helps us in modeling"
  },
  {
    "objectID": "qmd/feature-engineering-tokenization.html#sec-feat-eng-tok-preproc",
    "href": "qmd/feature-engineering-tokenization.html#sec-feat-eng-tok-preproc",
    "title": "Tokenization",
    "section": "Preprocessing",
    "text": "Preprocessing\n\nMisc\n\nPrimer on Cleaning Text Data - py code for cleaning\nThe Ultimate Preprocessing Pipeline for Your NLP Models - py code for cleaning\n\nClusters embeddings to remove â€œboilerplate languageâ€ (i.e.Â noise) from data\nParts of Speech (POS) tagging\n\n** Cleaning needs to happen before any clustering, POS tagging, Lemmatization or Stemming takes place. **\n\nSimilarly, clustering should happen before POS tagging and lemmatizing, since using the entire text will make the compute extremely costly and and less effective.\n\n\nReplace NA values with empty spaces\nLowercase all text\nReplace digits with words for the numbers\nRemove punctuation\n\n#$%&\\â€™()*+,-./:;?@[\\\\]^_{|}`~\n\nRemove emojis\n\nIf you are trying to sentiment analysis, trying to transform emojis into some text format instead of outright removing them may be beneficial\n\nSpell out contractions\nStrip HTML Tags\nRemove stopwords\nRemove accented characters\nRemove URLs, Mentions (@), hastags (#) and special characters\nRemove whitespace\nRemove boilerplate language (see article)\n\n\nText embeddings are created. The embeddings are clustered to find repeatedly occurring sentences and words and removes them, with an assumption that something that is repeated more than a threshold number of times, is probably â€œnoiseâ€.\nThe resultant text is a cleaner, more meaningful, summarized form of the input text. By removing noise, we are pointing our algorithm to concentrate on the important stuff only.\nArticle reduces size of text by 88%\n\nParts of Speech (POS) tagging\n\nLabels each word in a sentence as a noun, verb, adjective, pronoun, preposition, adverb, conjunction, or interjection.\nContext can largely affect the natural language understanding (NLU) processes of algorithms.\n\nLemmatization and Stemming\n\nStemming removes suffixes from words to bring them to their base form.\nLemmatization uses a vocabulary and a form of morphological analysis to bring the words to their base form.\nBoth reduce the dimensionality of the input feature space.\nLemmatization is generally more accurate than stemming but is computationally expensive\nLemmatization preserves the semantics of the input text.\n\nAlgorithms that are meant to work on sentiment analysis, might work well if the tense of words is needed for the model. Something that has happened in the past might have a different sentiment than the same thing happening in the present.\n\nStemming is fast, but less accurate.\n\nIn instances where you are trying to achieve text classification, where there are thousands of words that need to be put into categories, stemming might work better than lemmatization purely because of the speed.\n\nSome deep-learning models have the ability to automatically learn word representations which makes using either of these techniques, moot."
  },
  {
    "objectID": "qmd/feature-engineering-tokenization.html#sec-feat-eng-tok-texrec",
    "href": "qmd/feature-engineering-tokenization.html#sec-feat-eng-tok-texrec",
    "title": "Tokenization",
    "section": "Text Recipes",
    "text": "Text Recipes\n\nMisc\n\nEvery text preprocessing sequence should probably start with step_tokenize and end withstep_tf or step_tfidfÂ \n\nExample sequence of steps: step_tokenize, step_stopwords, step_ngrams, step_tokenfilter, step_tfidf\n\nModels with large numbers (100s) of features increases the opportunity for feature drift (i.e.Â use step_tokenfilter)\nIf you have a number of text columns, itâ€™ll be easier to create a char var and use that in the recipe\ntext_cols &lt;- c(\"text_col1, \"text_col2\", \"text_col3\")\nstep_whatever(all_of(text_cols))\n\nExamine Tokens from recipe\ntoken_list_tbl &lt;- \n  text_recipe %&gt;% \n    prep() %&gt;% \n    juice()\n\n# look at tokens created from 1st row of text column (e.g. text_col)\ntoken_list_tbl$text_col[1] %&gt;%\nÂ  Â  attr(\"unique_tokens\")\n\nCan use this after every text recipe step to examine the results\n\nDebugging\n\nâ€œlength of â€˜dimnamesâ€™ [2] not equal to array extentâ€\n\nExample: from DRobâ€™s Prediction box office performance\n\nUsing the min_times arg with step_tokenfilter and step_stopwords with one of the tokenized variables resulted in this error.\n\n\n\nstep_tokenize\ntextrecipes::step_tokenize(genres, production_countries,\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â token = 'regex',\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â options = list(pattern = \";\"))\n\nTake delimited text and create tokens\nGenres has values like â€œaction;horror;comedyâ€ per row\nCreated using function that extract values from json columns\n\nSee script Code &gt;&gt; JSON &gt;&gt; extract-values-from-json-column.R\n\n\nstep_stopwords\ntextrecipes::step_stopwords(tokenized_var1, tokenizedvar2)\n\nRemove words like â€œtheâ€ and â€œatâ€\ncustom_stopword_source: Provide a vector of stopwords\nkeep: Provide a vector of words you donâ€™t want filtered out\n\nstep_ngram\n\nCreates ngrams (i.e.Â combines tokens into n number of words to create phrases)\nstep_ngram(\nÂ  Â  tokenized_var1,\nÂ  Â  num_tokens = 3,\nÂ  Â  min_num_tokens = 1\n)\n\nExample creates uni-gram (aka tokens), bi-grams, and tri-grams (n = 1, 2, and 3)\nTokens combined (with underscores) sequentially as they occur in the original string\n\ne.g.Â Bigram: 1st word combined with 2nd word, then 2nd word combined with 3rd word, etc.\n\n\n\nstep_tokenfilter\n\nUseful for reducing features\n\nThis step can cause the number of features to explode into the thousands, so if your sample size is in the 100s, youâ€™ll need to reduce the number of features\n\nRemove tokens if they have a count below a specified number\ntextrecipes::step_tokenfilter(genres, production_countries\nmin_times = 50\n\nSee above for example of genre variable\nSo if â€œactionâ€ doesnâ€™t appear in at least 50 rows of the dataset, a token wonâ€™t be created for it\nIf you set arg, percentage=T, then min_times can be a percentage\nmax_times also available\n\n\nOnly keep tokens with a count in the top_n\ntextrecipes::step_tokenfilter(genres, production_countries\nmax_tokens = 50\n\nCan use with min/max_times but max_tokens gets applied after min/max times\nIn the example, there are 2 text columns (genres, production countries) and max_tokens = 50, therefore 50*2 = 100 columns get created.\nDefault = 100\n\nstep_tf\ntextrecipes::step_tf(genres, production countries)\n\nTerm Frequency Columns; Creates indicator variables for your tokens\nFormat: â€œtf_variableName_tokenNameâ€\nExample: â€œtf_genres_actionâ€\n\nstep_tfidf\n\n\nMixture of tf and idf\n\nTerm frequency (tf) measures how many times each token appears in each observation\n\ni.e.Â Number of times a word appears in a document, divided by the total number of words in that document\n\nInverse document frequency (idf) is a measure of how informative a word is, e.g., how common or rare the word is across all the observations.\n\ni.e.Â Logarithm of the number of documents in the corpus divided by the number of documents where the specific term appears\n\nWeighs down the frequent words and scaling up the rare ones\n\n\n\nThe â€œmixtureâ€ seems to be the product of these two values\nstep_tfidf(var_thats_been_tokenized_and_filtered)\n\n**Strongly advised to use step_tokenfilter before using step_tfidf to limit the number of variables created; otherwise you may run into memory issues**\n\n\nOther\n\nNot sure what this weight scheme does and how it helps\nstep_tokenize(market_category) %&gt;%\nstep_tokenfilter(market_category, min_times = 0.05, max_times = 1, percentage = TRUE) %&gt;%\nstep_tf(market_category, weight_scheme = \"binary\")"
  },
  {
    "objectID": "qmd/feature-engineering-tokenization.html#sec-feat-eng-tok-tokalgs",
    "href": "qmd/feature-engineering-tokenization.html#sec-feat-eng-tok-tokalgs",
    "title": "Tokenization",
    "section": "Tokenization Algorithms",
    "text": "Tokenization Algorithms\n\nWord-Based\n\nSplits a piece of text into words based on a delimiter\nAdvantages: meaningful tokens\nIssues:\n\nVocabularies can be very large\n\nAssigns different IDs to the words like â€œboyâ€ and â€œboysâ€\nCausing the model to be heavier and requiring more computational resources\nSolution: Restrict size of the vocabulary to only most common tokens. Tradeoff is that youâ€™re losing information\n\nMisspelled words in the corpus get assigned an OOV token\nAll semantic uses for a word are combined into one representation.\n\nExample, the word â€œplayâ€ in â€œIâ€™m going to see a playâ€ and â€œI want to playâ€ will have the same embedding, without the ability to distinguish context\n\n\nTypes: space and punctuation, rule-based\nNLP models that use this type:\n\nTransformer XL (vocabulary = 267,735)\nWord2Vec (?)\n\n\nCharacter-Based\n\nSplits the text into individual characters\nLanguage has many different words but has a fixed number of characters. This results in a very small vocabulary.\n\nEnglish Language\n\n256 different characters (letters, numbers, special characters)\n170,000 words in its vocabulary\n\n\nAdvantages:\n\nCan create a representation of the unknown words (words not seen during training) using the representation for each character\nMisspelled words can be spelled correctly rather can marking them as OOV tokens and losing information (?)\nFast and requires less compute resources\n\nIssues:\n\nLess meaningful tokens for some languages:\n\nCharacters have meaning in some languages (e.g.Â Chinese) but not others (e.g.Â English).\n\nTokenized sequence is much longer than the initial raw text (e.g.Â â€œknowledgeâ€ will have 9 different tokens)\n\n\nSubword-Based\n\nSplits based on rules:\n\nDo not split the frequently used words into smaller subwords.\nSplit the rare words into smaller meaningful subwords.\n\nUses a special symbol to indicate which word is the start of the token and which word is the completion of the start of the token.\n\nâ€œtokenizationâ€ can be split into â€œtokenâ€ and â€œ##izationâ€ which indicates that â€œtokenâ€ is the start of the word and â€œ##izationâ€ is the completion of the word.\nDifferent models use different special symbols (Wordpiece uses ##)\n\nAdvantages:\n\nMeaningful tokens but with a more managable vocabulary size\nPossible for a model to process a word which it has never seen before as the decomposition can lead to known subwords\n\nTypes\n\nWordPiece used by BERT and DistilBERT\nUnigram by XLNet and ALBERT\nBye-Pair Encoding by GPT-2 and RoBERTa"
  },
  {
    "objectID": "qmd/feature-engineering-tokenization.html#sec-feat-eng-tok-eng",
    "href": "qmd/feature-engineering-tokenization.html#sec-feat-eng-tok-eng",
    "title": "Tokenization",
    "section": "Engineering",
    "text": "Engineering\n\ne.g.Â Comments on a social media platform\nSentiment\n\nCategorize each comment as positive, negative, or neutral\n\nNumber of new comments\nCreate consumer profiles\n\nClustering based on consumer characteristics and use a feature\nSocial media characteristics\n\nLocation of comments\n\nLocation tags\n\nLanguage spoken\nBiographical data in profile\n\nShared links"
  },
  {
    "objectID": "qmd/feature-reduction.html#sec-feat-red-misc",
    "href": "qmd/feature-reduction.html#sec-feat-red-misc",
    "title": "Feature Reduction",
    "section": "Misc",
    "text": "Misc\n\nCurse of Dimensionality\n\nIts when there are more variables than observations.\nCauses the least squares coefficient estimates to lose uniqueness.\nCauses overfitting in ML algorithms\n\nPackages\n\n{intRinsic} - Likelihood-Based Intrinsic Dimension Estimators; implements the â€˜TWO-NNâ€™ and â€˜Grideâ€™ estimators and the â€˜Hidalgoâ€™ Bayesian mixture model\n\nProvides a clustering function for the Hidalgo model\nGraphical outputs built using ggplot2 so they are customizable\nSee section 5 (Summary and discussion) of the vignette for the recommended workflow and examples\n\n{Rdimtools} - feature selection, manifold learning, and intrinsic dimension estimation (IDE) methods\n\nCurrent version delivers 145 Dimension Reduction (DR) algorithms and 17 Intrinsic Dimension Estimator (IDE) methods.\n\n{RDRToolbox} - nonlinear dimension reduction with Isomap and LLE\n\nFor Time Series, see\n\nForecasting, Multivariate &gt;&gt; Dynamic Factor Models\n(Below) Dynamic Mode Decomposition"
  },
  {
    "objectID": "qmd/feature-reduction.html#sec-feat-red-terms",
    "href": "qmd/feature-reduction.html#sec-feat-red-terms",
    "title": "Feature Reduction",
    "section": "Terms",
    "text": "Terms\n\nIntrinsic Dimension (ID) - the minimal number of parameters needed to represent all the information contained in the data without significant information loss. A necessary piece of information to have before attempting to perform any dimensionality reduction, manifold learning, or visualization tasks. An indicator of the complexity of the features of a dataset.\nIsomap (IM) - nonlinear dimension reduction technique presented by Tenenbaum, Silva and Langford in 2000 [3, 4]. In contrast to LLE, it preserves global properties of the data. That means, that geodesic distances between all samples are captured best in the low dimensional embedding\nLocally Linear Embedding (LLE) - introduced in 2000 by Roweis, Saul and Lawrence. It preserves local properties of the data by representing each sample in the data by a linear combination of its k nearest neighbors with each neighbor weighted independently. LLE finally chooses the low dimensional representation that best preserves the weights in the target space.\nProjection Methods - maps the original data to a lower-dimensional space. The projection function can be linear, as in the case of PCA or or nonlinear, as in the case of locally linear embedding, Isomap, and tSNE.\nGeometric Methods - rely on the topology of a dataset, exploiting the properties of the distances between data points"
  },
  {
    "objectID": "qmd/feature-reduction.html#sec-feat-red-pca",
    "href": "qmd/feature-reduction.html#sec-feat-red-pca",
    "title": "Feature Reduction",
    "section": "PCA",
    "text": "PCA\n\nDescription\n\nCreates a subset of variables that maximises the covariance with the initial variable set, in order to store as much information as possible in a lower dimension.\nCompute an orthogonal basis of the space created by the original set of variables. The vectors creating this basis are the eigenvectors of the variance-covariance matrix. Reducing the dimension is then easily done by selecting the eigenvectors that are most representative of the initial data: those that contain most of the covariance. The amount of covariance stored by the vectors is quantified by the eigenvalues: the larger the eigenvalue, the more interesting its associated vectors.\nProjects variables orthogonally which removes correlation between predictor variables. The projection is in the direction of maximum variation such that the variation is distributed unequally among the transformed vectors. ThisÂ allows the user to reduce the feature space while still being able to capture most of variance in the data.\nThe principal components are equal to linear combinations of the correlated variables and these components are orthogonal to each other.\nWhy? When multiple variables are highly correlated to each other it causes the math used calculate regression models to break down. High dimension datasets also require large amount computational resources. Too many columns compared to the number of rows.\n\n\n\nMisc\n\nAs a multicollinearity detector?\n\nâ€œuse principal component analysis, and examine the screeplot, or proportion of variation explained by a subset of principal components. If all (or almost all) of the variation is explained with a small subset of all the variables, it means you have a multicollinearity problem. You will need to drop some variables or do some other dimension reduction to fix it before choosing your final model.â€\nI mean what if you have 20 variables and 3 are collinear, would this be detectable with PCA? I donâ€™t think so. Seem more likely that it would take a large portion of your variables being collinear for it to be detectable in this fashion.\n\n\n\n\nPreprocessing\n\nNotes from thread\nCenter variables\n\ncenter = T is default in prcomp( )\nIf variables are NOT on similar scales, then the data need to be scaled, also.\n\nProbably safer to always scale.\n\n\nSqrt any count variables\nLog any variable with a heavy tail\nIf you have too many features, use a sparse matrix to speed the process.\n\n\n\nDiagnostics\n\nTest for localization (repo with R code/docs)\n\nBad: if you make a histogram of a component (or loading) vector and it has really big outliers (aka localization)\n\nMeans this vector is mostly noise\n\nSolution: Regularized spectral clustering (links to resources)\nD_r = Diagonal(1/ sqrt(rs + mean(rs))\nD_c = Diagonal(1/ sqrt(cs + mean(cs))\n# Do SVD on\nD_r %*% A %*% D_c\n\nA is your matrix\nrs is a vector containing the row sums of the matrix\ncs is a vector containing the column sums of the matrix\n\n\n\n\n\nSteps\n\nCenter data in design matrix, A (n x p)\n\nIf data are centered and scaled then the computation in step 2 will result in the correlation matrix instead of the covariance matrix.\n\nCompute \\(n \\times n\\) Covariance Matrix,\n\\[\nC_x = \\frac{1}{n-1}AA^T\n\\]\n\nAlso seen \\(A^T A\\) but I donâ€™t think it matters. The upper triangle and the lower triangle of this product are just reverse covariances of each other and thus equal and I suspect the order just switches flips the triangles. The eigenvectors/eigenvalues get reordered later on anyways.\nThe diagonal of this matrix is the variable variances.\n\nCalculate eigenvectors and eigenvalues: \\(C_x V = D_\\lambda V\\) shows the covariance matrix as a transformation matrix. \\(D\\) is a diagonal matrix (\\(p\\times p\\)) with eigenvalues along the diagonal. \\(V\\) is a matrix (\\(p \\times p\\)) of eigenvectors\n\\[\nD_\\lambda = VC_x V^{-1}\n\\]\nOrder eigenvalues from largest to smallest\nOrder the eigenvectors according to the order of their corresponding eigenvalues\nEquation for the ith value of the PC1 vector: \\(\\text{PC1}_i = V_{(,1)} \\cdot A_{(i,)}\\)\n\n\\(\\text{PC2}\\) is similar except \\(V_{(,2)}\\) is used\nWhere all the variables in \\(A\\) have been standardized and \\(V\\) contains the loadings (see below)\n\n\n\n\nNotes\n\n\\(AA^T\\) is positive definite\n\nWhich means itâ€™s symmetric\nWhich means it has real eigenvaluesÂ and orthogonal eigenvectors\nWhich means the eigenvectors have covariances = 0\nWhich means the eigenvectors arenâ€™t correlated.\n\nThe eigenvalues are eigenvectorâ€™s standard deviations which determines how much variance is explained by that PC.\nThe variance of a variable is the dot-product of itself and itâ€™s transpose, \\(x_i \\cdot x^t_i\\)\nThe covariance between two variables, \\(x_i \\cdot x^t_j\\)\nIn step 3 equation, eigenvalues give the magnitude (length of vector) and eigenvectors the direction after being transformed by the covariance matrix.\nElements in a PC vector are called scores and elements in the V eigenvector are called loadings.\n\nThe loadings are the coefficients in the linear combination of variables that equals the PC vector\nLoadings range from -1 to 1\nVariables with high loadings (usually defined as .4 in absolute value or higher because this suggests at least 16% of the measured variable variance overlaps with the variance of the component) are most representative of the component\nThe sign of a loading (+ or -) indicates whether a variable and a principal component are positively or negatively correlated.\n\nScaling your design matrix variables just means your using a correlation matrix instead of a covariance matrix.\nPCA is sensitive to outliers. Variance explained will be inflated in the direction of the outlier\n\nGuessing this means components strongly influenced by variables with outlier values will have their variance-explained value inflated\n\nRow order of data matters as to which interpretation (latent) of component is valid from Principle Components and Penguins\n\nUsed data from palmerpenguins to create a â€œpenguin sizeâ€ variable from performing PCA on the data.\nIn one row order, high values of pc1 were associated with high body mass, but after scrambling the rows, high values of pc1 were associated with low body mass.\nHave to be careful when adding new data to the PCA-created feature. It might arbitrarily change the sign of the component and change the meaning of the feature.\n\nPCA doesnâ€™t take the response variable into account (unsupervised). Therefore, the directions (eigenvectors) obtained may be well-suited for the predictor variables, but not necessarily optimal for predicting the response. It does often produce pretty good results though.\n\nAn alternative would be Partial Least Squares (PLS) which does take the response into account (supervised).\n\nIn practice, PLS reduces bias while potentially increasing the variance so the benefit vs PCA regression is usually a wash.\nCapable of handling multivariate regression\nPopular in chemometrics for analyzing spectra.\n\n\n\n\n\nPlots\n\nMisc\n\nNotes from: https://support.minitab.com/en-us/minitab/18/help-and-how-to/modeling-statistics/multivariate/how-to/principal-components/interpret-the-results/all-statistics-and-graphs/\n(See bkmk) use broom::augment to add original data to pca output. Coloring the points by categorical variables can help with interpreting the components\nAlso see pkgs in notebook for visualization options\n\nScore\n\n\nClusters\n\nIf data follow a multivariate normal distribution then scores should be randomly distributed around zero\nIf there are clusters, then there may be multiple distributions present\n\nExtreme points (e.g.Â point in bottom right) might be outliers and it might be worthwhile to investigate them further\n\nLoadings\n\n\nNeed to imagine an axis at (0,0). Donâ€™t know why they donâ€™t plot them with those axes.\nFinding the largest variable influences on a PC can used to interpret itâ€™s meaning (think latent variable)\nArrows\n\nA (near) horizontal arrow (along the x-axis) describes that the feature contributes strongly toward PC1.\nA (near) vertical arrow (along the y-axis) describes that a feature contributes strongly towards PC2.\n\nValues\n\nLoadings range from -1 to 1\nThe termination coordinate of the line gives the loading values for that variable for both PCs\nLoadings (absolute magnitude) close to 0 indicate that the variable has little influence on that PC\nLarger the absolute value of the loading the greater the influence on that PC\nNegative values have negative influence on the latent variable that the PC represents (vice versa for positive values)\n\nAngle\n\nacute angles represent a positive correlation between those variables\nobtuse angles represent a negative correlation between those variables\n90 degree angles represent independence between those variables\n\nExample\n\nAge, Residence, and Employ have large influences on PC1 (interpretation: financial stability)\nCredit cards, Debt, and Education have large influences on PC2 (interpretation: credit history)\nSays, â€œAs the number credit cards increases, credit history (PC2 interpretation) becomes more negative.â€\n\n\nBi-Plot\n\n\nCombination plot of the score and loading plot\nCan augment pca output (see top of section) with original data and color the scores by different categorical variables\n\nIf a categorical variable level is clustered around Education, you could say as Education rises, the more likely that that person is &lt;categorical level&gt;.\nIn turn, that categorical level would be either positively or negatively (depending on the loading sign) associated with that PC.\n\n\nInterpretation\n\nExample: Bluejays\n\nLoadings\n\n\nPC2 represents the difference between bill size and skull size\n\nLoadings together with components plot\n\n\nMale birds larger than female birds\n\nIf you look at the loadings plot, negative pc1 corresponds to larger size and the components plot shows males with negative PC1 values\n\nBoth sexes have large and short bills relative to their overall size\n\nMales and females both show values above and below 0 in PC2\nLarger bills but smaller bodies (+PC2) and larger bodies but smaller bills (-PC2)\n\n\nVariance Explained\n\n\nOverall bird size explains &gt; 50% of the variation in measurements\n\n\n\nExample: How much variation in a principal component can be explained by a categorical variable\n# Penguins dataset\n# pca_values is a prcomp() object\npca_points &lt;-Â \nÂ  # first convert the pca results to a tibble\nÂ  as_tibble(pca_values$x) %&gt;%Â \nÂ  # now we'll add the penguins data\nÂ  bind_cols(penguins)\n## # A tibble: 6 x 12\n##Â  Â  PC1Â  Â  PC2Â  Â  PC3Â  Â  PC4 species island bill_length_mm bill_depth_mm\n##Â  &lt;dbl&gt;Â  &lt;dbl&gt;Â  &lt;dbl&gt;Â  &lt;dbl&gt; &lt;fct&gt;Â  &lt;fct&gt;Â  Â  Â  Â  Â  &lt;dbl&gt;Â  Â  Â  Â  &lt;dbl&gt;\n## 1 -1.85 -0.0320Â  0.235Â  0.528 AdelieÂ  Torgeâ€¦Â  Â  Â  Â  Â  39.1Â  Â  Â  Â  Â  18.7\n## 2 -1.31Â  0.443Â  0.0274Â  0.401 AdelieÂ  Torgeâ€¦Â  Â  Â  Â  Â  39.5Â  Â  Â  Â  Â  17.4\n## 3 -1.37Â  0.161Â  -0.189Â  -0.528 AdelieÂ  Torgeâ€¦Â  Â  Â  Â  Â  40.3Â  Â  Â  Â  Â  18Â \n## 4 -1.88Â  0.0123Â  0.628Â  -0.472 AdelieÂ  Torgeâ€¦Â  Â  Â  Â  Â  36.7Â  Â  Â  Â  Â  19.3\n## 5 -1.92 -0.816Â  0.700Â  -0.196 AdelieÂ  Torgeâ€¦Â  Â  Â  Â  Â  39.3Â  Â  Â  Â  Â  20.6\n## 6 -1.77Â  0.366Â  -0.0284Â  0.505 AdelieÂ  Torgeâ€¦Â  Â  Â  Â  Â  38.9Â  Â  Â  Â  Â  17.8\n## # â€¦ with 4 more variables: flipper_length_mm &lt;int&gt;, body_mass_g &lt;int&gt;,\n## #Â  sex &lt;fct&gt;, year &lt;int&gt;\n\npc1_mod &lt;-Â \nÂ  lm(PC1 ~ species, pca_points)\nsummary(pc1_mod)\n## Call:\n## lm(formula = PC1 ~ species, data = pca_points)\n##Â \n## Residuals:\n##Â  Â  MinÂ  Â  Â  1QÂ  MedianÂ  Â  Â  3QÂ  Â  MaxÂ \n## -1.3011 -0.4011 -0.1096Â  0.4624Â  1.7714Â \n##Â \n## Coefficients:\n##Â  Â  Â  Â  Â  Â  Â  Â  Â  Estimate Std. Error t value Pr(&gt;|t|)Â  Â \n## (Intercept)Â  Â  Â  -1.45753Â  Â  0.04785Â  -30.46Â  &lt;2e-16 ***\n## speciesChinstrapÂ  1.06951Â  Â  0.08488Â  12.60Â  &lt;2e-16 ***\n## speciesGentooÂ  Â  3.46748Â  Â  0.07140Â  48.56Â  &lt;2e-16 ***\n## ---\n## Signif. codes:Â  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n##Â \n## Residual standard error: 0.5782 on 330 degrees of freedom\n## Multiple R-squared:Â  0.879,Â  Adjusted R-squared:Â  0.8782Â \n## F-statistic:Â  1198 on 2 and 330 DF,Â  p-value: &lt; 2.2e-16\n\nFrom https://bayesbaes.github.io/2021/01/28/PCA-tutorial.html\nAdjusted R-squared: 0.8782\n\n\nCan be seen visually in this chart by looking at the points in relation to the x-axis where species is segregated pretty nicely.\n\n\n\n\n\nOutliers\n\nMahalanobis Distance (MD)\n\nThis method might be problematic. Supposedly outliers affect the covariance matrix which affects the PCA, which affects the scores, which affects theMahalanobis distance (MD). So the MD might be biased and not be accurate in determining outliers\n\nRobust forms of PCA (see section below) would be recommended if you suspect outliers in your data.\n\nDisplays the Mahalanobis distance (MD) for each observation and a reference line to identify outliers. The Mahalanobis distance is the distance between each data point and the centroid of multivariate space (the overall mean).\n\nOutliers determined by whether the Mahalanobis Distance is greater than the square root of the Chi-Square statistic where m is the number of variables and Î± = 0.05\n\nNo outliers in the chart above as all MDs lower than the threshold at 4.4\n\n\nLeverage Points and Orthogonal Outliers\n\nNotes from https://towardsdatascience.com/multivariate-outlier-detection-in-high-dimensional-spectral-data-45878fd0ccb8\nTypes\n\nLeverage Points\n\ncharacterized by a high score distance\ngood leverage points also have short orthogonal distance and bad leverage points have long orthogonal distances\ngood leverage points have a positive effect\n\nOrthogonal Outliers\n\ncharacterized by a high orthogonal distance\n\n\nType determined by (see article for the math)\n\nScore DIstance (SD) - the distance an observation is from center of K-dimensional PCA subspace\nOrthogonal Distance (OD) -Â the deviation â€” i.e.Â lack of fit â€” of an observation from the k-dimensional PCA subspace\nOutliers are determined by Chi-Square test very similar to theÂ Mahalanobis Distance method (see above).\n\n\n\nIn the example shown, the red dots are data known to be measurement errors. Most of the red dots are captured in the orthogonal and bad sections but quite a few normal observation (blue) points too. So this method needs to be used as a guide and followed up upon when it flags points.\n\n\n\nHotellingâ€™s T2 and SPE/DmodX (Complementary Tests)\n\n{{pca}}\nHotellingâ€™s T2 works by computing the chi-square tests across the top n_components for which the p-values are returned that describe the likeliness of an outlier. This allows for ranking the outliers from strongest to weak.\nSPE/DmodX (distance to model) based on the mean and covariance of the first 2 PCs\n\n\n\n\nExtensions\n\nRobust PCA\n\nData with outliers and high dimensional data (p &gt;&gt; n) are not suitable for regular PCA where p is the number of variables.\nLow Dim methods (only valid when n &gt; 2p) that find robust (against outliers) estimates of the covariance matrix\n\nS-estimator, MM-estimator, (Fast)MCD-estimator, re-weighted MCD- (RMCD) estimator\n\nHigh Dim Methods\n\nRobust PCA by projection-pursuit (PP-PCA)\n\nfinds directions for eigenvectors that maximize a â€œprojection indexâ€ instead of directions that maximize variance\n\nMAD or Qn-estimator is used a projection index\n\n\nSpherical PCA (SPCA)\n\nhandles outliers by projecting points onto a sphere instead of a line or plane\n\nalso uses MAD or Qn-estimator\n\n\nRobust PCA (ROBPCA)\n\ncombines projection index approach with low dim robust covariance estimation methods somehow\n\nRobust Sparse PCA (ROSPCA)\n\nsame but uses sparse pca\napplicable to both symmetrically distributed data and skewed data\n\n\n\nKernel PCA\n\nPackages: {kernlab}\nNonlinear data (notebook)\nPCA in a hypothetical (kernel trick), higher dimensional space\nWith more dimensions, data points become more separable.\nResults depend on type of kernel\n\nGaussian Kernel\n\nTuning parameter: sigma\n\n\n\n\n\n\nTidymodels\n\nRecipe step\n# if only using dummy vars, no sure if normalization is necessary\n# step_normalize(&lt;pca variables&gt;)\nstep_pca(starts_with(\"tf_\"), num_comp = tune())\n# don't forget to include num_comp in your tuning grid\nTaking a tidymodelâ€™s recipe object and performing PCA\ntf_mat &lt;- recipe_obj %&gt;%\nÂ  Â  # normalizing tokenized indicators (?)\nÂ  Â  # since these are all dummy vars, not sure if a normalization step is necessary)\nÂ  Â  step_normalize(starts_with(\"tf_\")) %&gt;%\nÂ  Â  prep() %&gt;%\nÂ  Â  bake() %&gt;%\nÂ  Â  # only want to pca text features\nÂ  Â  select(starts_with(\"tf_\") %&gt;%\nÂ  Â  as.matrix()\n\ns &lt;- svd(tf_mat)\n# scree plot\ntidy(s, matrix = \"d\") %&gt;%\nÂ  Â  filter(PC &lt;= 50) %&gt;%\nÂ  Â  ggplot(aes(x = PC, y = percent)) +\nÂ  Â  geom_point()\n\nmatrix (tidy arg):\n\nâ€œuâ€, â€œsamplesâ€, â€œscoresâ€, or â€œxâ€: Returns info about the map from the original space to the pc space\nâ€œvâ€, â€œrotationâ€, â€œloadingsâ€, or â€œvariablesâ€: Returns information about the map from the pc space to the original space\nâ€œdâ€, â€œeigenvaluesâ€, or â€œpcsâ€: Returns information about the eigenvalues\n\n\nExample\nlibrary(tidymodels)Â \nlibrary(workflowsets)Â \nlibrary(tidyposterior)Â \ndata(meats, package= \"modeldata\")Â \n# Keep only the water outcomeÂ \nmeats &lt;- select(meats, -fat, -protein)Â \nset.seed(1)Â \nmeat_split &lt;- initial_split(meats)Â \nmeat_train &lt;- training(meat_split)Â \nmeat_test &lt;- testing(meat_split)Â \nset.seed(2)Â \nmeat_folds &lt;- vfold_cv(meat_train, repeats = 3)\nbase_recipe &lt;-Â \nÂ  recipe(water ~ ., data = meat_train) %&gt;%Â \nÂ  step_zv(all_predictors()) %&gt;%Â \nÂ  step_YeoJohnson(all_predictors()) %&gt;%Â \nÂ  step_normalize(all_predictors())Â \npca_recipe &lt;-Â \nÂ  base_recipe %&gt;%Â \nÂ  step_pca(all_predictors(), num_comp = tune())Â \npca_kernel_recipe &lt;-Â \nÂ  base_recipe %&gt;%Â \nÂ  step_kpca_rbf(all_predictors(), num_comp = tune(), sigma = tune())"
  },
  {
    "objectID": "qmd/feature-reduction.html#sec-feat-red-efa",
    "href": "qmd/feature-reduction.html#sec-feat-red-efa",
    "title": "Feature Reduction",
    "section": "Exploratory Factor Analysis (EFA)",
    "text": "Exploratory Factor Analysis (EFA)\n\nIdentifies a number of latent factors that explain correlations between observed variables\n\nFrequently employed in social sciences where the main interest lies in measuring and relating unobserved constructs such as emotions, attitudes, beliefs and behaviour.\nLatent variables, referred to also as factors, account for the dependencies among the observed variables, referred to also as items or indicators, in the sense that if the factors are held fixed, the observed variables would be independent.\nIn exploratory factor analysis the goal is the following: for a given set of observed variables x1, . . . , xp one wants to find a set of latent factors Î¾1, . . . , Î¾k, fewer in number than the observed variables (k &lt; p), that contain essentially the same information.\nIn confirmatory factor analysis, the objective is to verify a social theory. Hence, a factor model is specifed in advance and its fit to the empirical data is tested.\n\nMisc\n\nPackages\n\n{psych} - factor analysis, item response theory, reliability analysis\n{factominer} - Multiple Factor Analysis (MFA}\n{fspe} - Model selection method for choosing number of factors\n\nUses the connection between model-implied correlation matrices and standardized regression coefficients to do model selection based on out-of-sample prediction errors\n\n\nTwo main approaches for analysing ordinal variables with factor models:\n\nUnderlying Response Variable (URV)\n\nThe ordinal variables are generated by underlying continuous variables partially observed through their ordinal counterparts. (also see Regression, Ordinal &gt;&gt; Cumulative Link Models (CLM))\n\nItem Response Theory (IRT)\n\nOrdinal indicators are treated as they are.\n\n\n\nMethods for selecting the right number of factors\n\nMisc\n\nIssue: more factors always improve the fit of the model\n\nParallel Analysis: analyze the patterns of eigenvalues of the correlation matrix\nModel Selection: likelihood ratio tests or information criteria\n\nComparison with PCA\n\nPCA is a technique for reducing the dimensionality of oneâ€™s data, whereas EFA is a technique for identifying and measuring variables that cannot be measured directly (i.e.Â latent factor)\nWhen variables donâ€™t have anything in common, EFA wonâ€™t find a well-defined underlying factor, but PCA will find a well-defined principal component that explains the maximal amount of variance in the data.\nDifferences in the results between PCA and EFA donâ€™t tend to be obvious in practice. As the number of variables (&gt;40 variables) involved in the analysis grows, results from PCA and EFA become more and more similar.\nSimilarly calculated method to PCA, but FA is an analysis on a reduced correlation matrix, for which the ones in the diagonal have been replaced by squared multiple correlations (SMC)\n\nA SMC is the estimate of the variance that the underlying factor(s) explains in a given variable (aka communality).\n\nThe variability in measured variables in PCA causes the variance in the principal component. This is in contrast to EFA, where the latent factor is seen as causing the variability and pattern of correlations among measured variables\nAn eigenvalue decomposition of the full correlation matrix is done in PCA, yet for EFA, the eigenvalue decomposition is done on the reduced correlation matrix\nFactor Analysis is a latent variable measurement model\n\nThe causal relationship is flipped in FA as compared to PCA.\n\n\nF is the latent variable (instead of component in PCA), b is a weight (like loadings in PCA), Y is a predictor variable, and u is an error\n\nHere, b estimates how much F contributes to Y"
  },
  {
    "objectID": "qmd/feature-reduction.html#sec-feat-red-autoenc",
    "href": "qmd/feature-reduction.html#sec-feat-red-autoenc",
    "title": "Feature Reduction",
    "section": "Autoencoders",
    "text": "Autoencoders\n\nUnsupervised neural networks that learn efficient coding from the input unlabelled data. They try to reconstruct the input data by minimizing the reconstruction loss\nMisc\n\nUndercomplete Autoencoder (AE) â€” the most basic and widely used type, frequently referred to as an Autoencoder\nSparse Autoencoder (SAE) â€” uses sparsity to create an information bottleneck\nDenoising Autoencoder (DAE) â€” designed to remove noise from data or images\nVariational Autoencoder (VAE) â€” encodes information onto a distribution, enabling us to use it for new data generation\n\nLayers\n\nEncoder: Mapping from Input space to lower dimension space\nDecoder: Reconstructing from lower dimension space to Output space\n\nProcess\n\n\nEncodes the input data (X) into another dimension (Z), and then reconstructs the output data (Xâ€™) using a decoder network\nThe encoded embedding (Z) is preferably lower in dimension compared to the input layer and contains all the efficient coding of the input layer\nOnce the reconstruction loss is minimized, the learned weights or embeddings, in the Encoder layer can be used as features in ML models and the Encoder layer can be used to generate embeddings on future data.\n\nSparse Autoencoder (SE)\n\n\nUses regularization\nDimension reduction in the center is achieved through deactivating neurons\nExample\n\nThe model consists of 5 layers: one input, three hidden and one output.\nInput and output layers contain 784 neurons each (the shape of our data, i.e number of columns), with the size of hidden layers reduced to 16 neurons each.\nWe will train the model over 50 epochs and plot a loss chart (see below).\nWe will separate the encoder part of the model and save it to our project directory. Note, if you are not planning to reuse the same model afterwards, you donâ€™t need to keep a copy of it."
  },
  {
    "objectID": "qmd/feature-reduction.html#sec-feat-red-dmd",
    "href": "qmd/feature-reduction.html#sec-feat-red-dmd",
    "title": "Feature Reduction",
    "section": "Dynamic Mode Decomposition (DMD)",
    "text": "Dynamic Mode Decomposition (DMD)\n\nCombines PCA and fourier transform\nSupposed to handle time series better than PCA\n{{pydmd}}"
  },
  {
    "objectID": "qmd/feature-selection.html",
    "href": "qmd/feature-selection.html",
    "title": "32Â  Feature Selection",
    "section": "",
    "text": "TOC\n\nWhy?\nBasic\nComplex\nMultidimensional Feature Selection (MFDS)\n\nWhy?\n\nReduces chances of overfitting\nMulticollinearity among predictors blows up std.errors in regression\nLowers computer resources requirements and increase speed\n\nBasic\n\nHarrell: Use the full model unless p &gt; m/15 â†’ number_of_columns &gt; number_of_rows / 15\n\nIf p &gt; m/15, then use penalyzed regression (see below, Shrinkage Methods)\n\nVIF - eliminate low variance predictorsÂ \nRemove noisy features - compare correlation between a var in train set and same var in test set\nBest subset\n\n{lmSubsets}: for linear regression; computes best set of predictors by fitting all subsets; chooses based on metric (AIC, BIC, etc.)\n{leaps::regsubsets} performs best subset selection for regression models using RSS. summary(obj) gives best subset of variables for different sized subsets. nvmax arg can be used set the max size of the subsets.\nLikelihood Ratio Test (LR Test) - For a pair of nested models, the difference in âˆ’2ln L values has a Ï‡2 distribution, with degrees of freedom equal to the difference in number of parameters estimated in the models being compared.\n\n-2 * Log Likelihood is called the residual deviance of the model\nExample:\n\nÏ‡2 = (-2)*log(model1_likelihood) - (-2)*log(model2_likelihood) = 4239.49 â€“ 4234.02 = 5.47\n\n-2*log can probably be factored out\n\ndegrees of freedom = model1_dof - model2_dof = 12 â€“ 8 = 4\npval &gt; 0.05 therefore the likelihoods of these models are not signficantly different and the variable isnâ€™t worth adding.\n\n\n\nReduction methods (if multicollinearity is a problem) - PCA\nShrinkage methodsÂ (if multicollinearity is a problem) - lasso, ridge, elastic net,Â Least Angle Regression (LAR)\n\nComplex\n\n{projpred} (Vehtari): projection predictive variable selection for various regression models, and also allows for valid post-selection inference\n\nPapers and other articles in bkmks &gt;&gt; Features &gt;&gt; Selection &gt;&gt; projpred\nCurrently requires {rstanarm} or {brms} models\nFamilies: gaussian, binomial, poisson. Also categorical and ordinal\nTerms: linear main effects, interactions, multilevel, other additive terms\nComponents\n\nSearch: determines the solution path, i.e., the best submodel for each submodel size (number of predictor terms).\nEvaluation: determines the predictive performance of the submodels along the solution path\n\nAfter model selection, a matrix of projected posterior draws is produced using the reference model and the selected number of features. Then, typical posterior stats for variable effects can be calculated, visualized via {posterior}, {bayesplot}, etc.\n\nprojected posterior distribution - the distribution arising from the deterministic projection of the reference modelâ€™s posterior distribution onto the parameter space of the final submodel\n\nAlso see Horseshoe model\n\nRegularized Horseshoe prior\nGelman - link\nPaper\n\n\n{MLGL} - approach combines variables aggregation and selection in order to improve interpretability and performance\n\nGoal is to remove redundant variables from a high dim dataset.\nSteps\n\nFirst, hierarchical clustering procedure provides at each level a partition of the variables into groups.\nThen, the set of groups of variables from the different levels of the hierarchy is given as input to group-Lasso, with weights adapted to the structure of the hierarchy.\n\nAt this step, group-Lasso outputs sets of candidate groups of variables for each value of the regularization parameter\n\n\n\n{Rdimtools} - has many algorithms for feature selection\nEnsemble Ranking: apply multiple feature selection methods then create an ensemble ranking\n\npaper, article\n\nCompared 12 individual feature selection methods and the 6 ensemble methods described above in 8 datasets for a classification task\n\nBest performance is Ensemble Reciprocal Ranking\n\nwhere r(f) is the final rank of feature f\n\nj is the the index for the feature selection methods\n\nEquivalent to the harmonic mean rank\naka Inverse Rank Position\nLower is better (I think)\n\nBest performance by single method is SHAP\n\nBoruta - built around random forest algorithm,Â can identify variables involved in non-linear interactions, slow if dealing with thousands of variables\nMultivariable Fractional Polynomials\n\nfits polynomial transformations of features with exponents in a range of numbers including fractions\nincludes a pretty intensive statistical model testing procedure\n{mfp}, explainer\n\nbounceR pkg - ?\nnimble pkg - Reversible Jump MCMC (RJMCMC) is a general framework for MCMC simulation in which the dimension of the parameter space (i.e., the number of parameters) can vary between iterations of the Markov chain. (article)\nRandom forest with shallow trees - see overview kdnuggets 7 methods article and notebook, pg 153 for proper var importance flavor\n\nFuzzy Forests: Extending Random Forest Feature Selection for Correlated, High-Dimensional Data\nGain Penalyzed Forests: Uses gain penalization to regularize the random forest algorithm\n\nThink this also works for numeric target variables\nNotes from\n\nFeature Selection via Gain Penalization in Random Forests\n\nIncludes coded example\n\nPaper\n\nWhen determining the next child node to be added to a decision tree, the gain (or the error reduction) of each feature is multiplied by a penalization parameter\n\nU is the set of indices of the features previously used in the tree\nXi is the candidate feature\nt is the candidate splitting point and Î»âˆˆ(0,1]\nSo at each split point, variables that have NOT been chosen at prior split points receive a penalty\n\nPenalty for each variable (the Î» shown in the Gain equation above)\n\nÎ»i âˆˆ [0,1)\nÎ»0 âˆˆ [0,1) is interpreted as the baseline regularization\nÎ³ âˆˆ [0,1) is the mixture parameter\ng(xi) is a function of the ith feature\n\nshould represent relevant information about the feature, based on some characteristic of interest (correlation to the target, for example)\nintroduces â€œprior knowledgeâ€ regarding the importance of each feature into the model\nthe data will tell us how strong our assumptions about the penalization are, since even if we try to penalize a truly important feature, its gain will be high enough to overcome the penalization and the feature will get selected by the algorithm\nExamples\n\nThe Mutual Information between each feature and the target variable y\n\nnormalized to be between 0 and 1\n\nThe variable importance values obtained from a previously run standard Random Forest, which is what I call a Boosted g(xi)\n\nnormalized to be between 0 and 1\n\nother options, see the paper\n\n\n\nSteps\n\nWe run a bunch of penalized random forests models with different hyperparameters and record their accuracies and final set of features\n\nÎ³, Î»0 and mtry are hyperparameters that should be tuned\n\nFor each training dataset, select the top-n (e.g.Â n = 3) fitted models in terms of the accuracies, and run a â€œnewâ€ random forest for each of the feature sets used by them. This is done using all of the training sets so we can evaluate how these features perform in slightly different scenarios\nFinally, get the top-m set of models (here m = 30) from these new ones, check which features were the most used between them and run a final random forest model with this feature set.\n\ne.g.Â select only the 15 most used features from the top 30 models, but both numbers can be changed depending on the situation\n\n\n\ndistance correlation algorithm (also see Regression, Regularized &gt;&gt; Misc\n\nâ€œthe distance correlation algorithm for variable selection (DC.VS) of Febrero-Bande et al.Â (2019). This makes use of the correlation distance (SzÃ©kely et al., 2007; Szekely & Rizzo, 2017) to implement an iterative procedure (forward) deciding in each step which covariate enters the regression model.â€\nStarting from the null model, the distance correlation function, dcor.xy,Â  in {fda.usc} is used to choose the next covariate\n\nguessing you want large distances between an already chosen variable and the next variable\nMaybe for the first explanatory variable you want a short distance between the it and the outcome variable\ndunno what the stopping criteria is\n\nalgorithm discussed in this paper, Variable selection in Functional Additive Regression Models\n\n\nMultidimensional Feature Selection (MFDS)\n\nCompares tuples of variables using entropy/information gain (see below for details on the algorithm)\nA filter method which means it identifies informative variables before modelling is performed.\n\nIn general, filter methods are usually simplistic and therefore fast, but simplicity can sometimes lead to errors\n\nAs of 10-13-2019, implementation only for use with binary outcomes\n\nFor multi-categorical outcomes, vignette suggests either performing the analyses with all pairs of outcome categories, then any variable deemed relevant for one analysis is considered relevant. Instead of doing all-pairs, you can dichotomize variable into pairs of category-other which would be less expensive\nContinuous outcomes have to be discretizedÂ \n\nthe algorithm explores all k-tuples of variables, Â , for k-dimensional analysis., whereÂ   Â is one of the explanatory variables\n\n*** max k is 5\n\nFor larger values than 5, it becomes computationally expensive and detecting significant differences in conditional entropy becomes less likely.\n\nFor example, 2-dimensional analysis would explore the set of all possible pairs (2-tuples) of variables.\n\nFor each variable,  , check whether adding it to the set with another variable,Â   ,Â adds information to the system. If there exists such a  , then we declareÂ   as 2-weakly relevant.\n\n\nSteps\n\nDiscretize all variables\n\nChoose the number of classes, c\n\nAll variables are coerced into having the same number of classes (even binary? So if you have one binary, then all variables are coerced into binaries?)\nUnless thereâ€™s domain knowledge, multiple cs should be tried.\n\nRandomly sample (c-1) integers from a uniform distribution on the interval, (2, N) where N is the number of observations.\n\nThe integers will be indexes where the variable is split into c classes\nAt first glance it looks like the interval starts at 2 so that each class has minimum of 2 or 3 values (depending if the split happens before or after the integer), but unless thereâ€™s some kind of check, two consecutive integers, like 19,20, could still be sampled and then you have a problem. With a large data set, it seems unlikely to happen, but itâ€™d still be possible. So I donâ€™t know why it starts at 2. The vignette isnâ€™t clear about this IMO.\n\nSort the variable\n\nI guess smallest value to largest? Probably doesnâ€™t matter as long as itâ€™s consistent for all variables)\n\nSplit the sorted variable at the indexes given by the randomly sampled integers\nrepeat for each variable\n\noutcome variable might have to be done manually by the user prior to starting analysis/\n\n\nCompute conditional entropy for Y given the k-dimension variable set that includes a specific variable and the conditional entropy for the (k-1) set that excludes that specific variable.\n\nConditional Entropy forÂ   Â  is\n\n\n\nd is for the outcome variable category which much be binary\ni_1 is the ith category of the 1st variable\ni_k is the ith category of the kth variable\nThis sequence of summations amounts to taking every permutation of outcome category and every category of every explanatory variable and summing them together.\nSee the decision tree section in the â€œAlgorithm descriptionsâ€ note for further discussion\n\nThe conditional entropy for the (k-1) set of explanatory variable is calculated similarily\n\nIf k is less than total number of variables, then this process could include many permutations\n\nCompute the mutual information gain between these two entropies. Then out of all the mutual information values from all the different permutations of the k-tuple, choose the maximum value\n\nmutual information is the difference between two conditional entropies. Very similar to the information gain used in trees\n\n\nm is mth permutation of the k-tuple\nN is the number of observations. The reason for it being there is that multiplying the max-IG by N makes it a distribution parameter that can be hypothesis tested.\n\n\nHypothesis test this difference (IG_max) to see if itâ€™s significant\n\nIf the difference is significant, then that variable is â€œk-weakly relevantâ€\nTest statistic for a specific Î±-level is from an empirically calculated distribution\nÂ Adjust p-values to control FWER or FDR\n\nIf performing a more complicated model selection process, take the top n variables whose IG_max was significant and repeat steps\n\nGuessing this is backwards elimination\nThis is a method thatâ€™s built for dealing with thousands of variables, so this taking top-n-then-repeat seems to be a way of controlling the amount of time needed to complete the analysis."
  },
  {
    "objectID": "qmd/forecasting-decomposition.html#sec-decomp-misc",
    "href": "qmd/forecasting-decomposition.html#sec-decomp-misc",
    "title": "Decomposition",
    "section": "Misc",
    "text": "Misc\n\nThere are two approaches to noise reduction: filtering algorithms and smoothing algorithms. In filtering algorithms, signal points are fed sequentially, therefore only the current and the previous points are used to get rid of noise at the current point. Smoothing algorithms assume that the entire signal has been received, and all signal points, both previous and subsequent, are used to get rid of noise at the current point.\nLow Pass FIlter\n\nDampens higher frequencies in the data and allows lower frequencies to â€œpassâ€ through.\n\n\nA smoother version of the original data\n\n\nHigh Pass FIlter\n\nDampens low frequencies and allows high frequencies to pass\n\n\nLooks like a series of residuals with the trend removed\n\n\nMatching Filter\n\nOriginal series with extreme changepoints\n\nMatching filter indicates the changepoints with peaks in the filtered series"
  },
  {
    "objectID": "qmd/forecasting-decomposition.html#sec-decomp-saf",
    "href": "qmd/forecasting-decomposition.html#sec-decomp-saf",
    "title": "Decomposition",
    "section": "Seasonal Adjusted Forecasting",
    "text": "Seasonal Adjusted Forecasting\n\nThe seasonality is extracted from the time series using STL. The extracted seasonal time series and the seasonally adjusted time series are forecast separately. Both forecasts are then added back together to produce the final forecast.\n\nA seasonal naive method is commonly used to forecast the seasonal component.\nNot sure how you would combine the uncertainty (i.e PIs)\n\nExample\nfrom statsmodels.tsa.api import STL\nfrom sktime.forecasting.naive import NaiveForecaster\n\n# fitting the seasonal decomposition method\nseries_decomp = STL(yt, period=period).fit()\n\n# adjusting the data\nseas_adj = yt - series_decomp.seasonal\n\n# forecasting the non-seasonal part\nforecaster = make_reduction(estimator=RidgeCV(),\n                            strategy='recursive',\n                            window_length=3)\n\nforecaster.fit(seas_adj)\n\nseas_adj_pred = forecaster.predict(fh=list(range(1, 13)))\n\n# forecasting the seasonal part\nseas_forecaster = NaiveForecaster(strategy='last', sp=12)\nseas_forecaster.fit(series_decomp.seasonal)\nseas_preds = seas_forecaster.predict(fh=list(range(1, 13)))\n\n# combining the forecasts\npreds = seas_adj_pred + seas_preds"
  },
  {
    "objectID": "qmd/forecasting-decomposition.html#sec-decomp-stl",
    "href": "qmd/forecasting-decomposition.html#sec-decomp-stl",
    "title": "Decomposition",
    "section": "Seasonal Trend Decomposition using LOESS (STL)",
    "text": "Seasonal Trend Decomposition using LOESS (STL)\n\nTime series get decomposed into trend-cycle (T), seasonal (S), and remainder (R) components\n\nYt = Tt + St + Rt, where t = 1,2,â€¦,N\n\nLOESS smoothes a time series by:\n\nWeights are applied to the neighborhood of each point which depend on the distance from the point\nA polynomial regression is fit at each data point with points in the neighborhood as explanatory variables\n\nComponents are additive\nEntails two recursive procedures: inner loop and outer loop\n\nEach iteration updates the trend-cycle and seasonal components\nInner Loop is iterated until thereâ€™s a robust estimate of the trend and seasonal component\nThe outer loop is only iterated if outliers exist among the data points\nInner Loop Procedure\n\nDetrending\n\nInitially occurs by subtracting an initial trend component (?) from the original series\n\nSubseries smoothing\n\n12 monthly subseries are separated and collected\nEach is smoothed with LOESS\nRe-combined to create the initial seasonal component\n\nLow-Pass filtering of smoothed seasonal component\n\nSeasonal component passed through a 3x12x12 moving average.\nResult is again smoothed by LOESS (length = 13) in order to detect any trend-cycle in it.\n\nDetrending of smoothed subseries\n\nResult of low-pass filtering is subtracted from seasonal component in step 2 to get the final seasonal component\n\nDe-seasonalization\n\nFinal seasonal component is subtracted from the original series\n\nTrend smoothing\n\nLOESS is applied to deseasonalized series to get the final trend component\n\n\nOuter Loop procedure\n\nFinal trend and seasonal components are subtracted from the original series to get the remainder/residual series\nThe final trend and seasonal components are tested for outlier points\nA weight is calculated and used in the next iteration of the Inner Loop to down-weight the outlier points."
  },
  {
    "objectID": "qmd/forecasting-dl.html#sec-fcast-dl-misc",
    "href": "qmd/forecasting-dl.html#sec-fcast-dl-misc",
    "title": "Deep Learning",
    "section": "Misc",
    "text": "Misc\n\nA callback is a function that performs some action during the training process\n\ne.g.Â saving a model after each training epoch; early stopping when a threshold is reached\nList of Keras callbacks"
  },
  {
    "objectID": "qmd/forecasting-dl.html#sec-fcast-dl-preproc",
    "href": "qmd/forecasting-dl.html#sec-fcast-dl-preproc",
    "title": "Deep Learning",
    "section": "Preprocessing",
    "text": "Preprocessing\n\nMisc\n\nNotes from\n\nEXAMPLE py, global, keras LSTM, preprocessing - Deep Learning for Forecasting: Preprocessing and Training | by Vitor Cerqueira | Mar, 2023 | Towards Data Science\n\n\nScale by the mean\n\nFor global forecasting, this brings all series into a common value range. Therefore, the scale of the values wonâ€™t be a factor in model training.\nExample: global forecasting\nfrom sklearn.model_selection import train_test_split\n\n# leaving last 20% of observations for testing\ntrain, test = train_test_split(data, test_size=0.2, shuffle=False)\n\n# computing the average of each series in the training set\nmean_by_series = train.mean()\n\n# mean-scaling: dividing each series by its mean value\ntrain_scaled = train / mean_by_series\ntest_scaled = test / mean_by_series\n\nLogging\n\nLog series after scaling transformation\nHandles heterskedacity\nCan create more compact ranges, which then enables more efficient neural network training\nHelps avoid saturation areas of the neural network.\n\nSaturation occurs when the neural network becomes insensitive to different inputs. This hampers the learning process, leading to a poor model.\n\nExample\nimport numpy as np\n\nclass LogTransformation:\n\nÂ  Â  @staticmethod\nÂ  Â  def transform(x):\nÂ  Â  Â  Â  xt = np.sign(x) * np.log(np.abs(x) + 1)\n\nÂ  Â  Â  Â  return xt\n\nÂ  Â  @staticmethod\nÂ  Â  def inverse_transform(xt):\nÂ  Â  Â  Â  x = np.sign(xt) * (np.exp(np.abs(xt)) - 1)\n\nÂ  Â  Â  Â  return x\n\n# log transformation\ntrain_scaled_log = LogTransformation.transform(train_scaled)\ntest_scaled_log = LogTransformation.transform(test_scaled)\n\nCreate a matrix with lags and leads\n\nExample: Univariate\n\n# src module here: https://github.com/vcerqueira/blog/tree/main/src\nfrom src.tde import time_delay_embedding\n\n\n# using 3 lags as explanatory variables\nN_LAGS = 3\n# forecasting the next 2 values\nHORIZON = 2\n\n# using a sliding window method called time delay embedding\nX, Y = time_delay_embedding(series, n_lags=N_LAGS, horizon=HORIZON, return_Xy=True)\n\nâ€œTarget variablesâ€ are lead variables\n\nExample: Global\n\n# src module here: https://github.com/vcerqueira/blog/tree/main/src\nfrom src.tde import time_delay_embedding\n\n\nN_FEATURES = 1 # time series is univariate\nN_LAGS = 3 # number of lags\nHORIZON = 2 # forecasting horizon\n\n\n# transforming time series for supervised learning\ntrain_by_series, test_by_series = {}, {}\n\n# iterating over each time series\nfor col in data:\nÂ  Â  train_series = train_scaled_log[col]\nÂ  Â  test_series = test_scaled_log[col]\n\nÂ  Â  train_series.name = 'Series'\nÂ  Â  test_series.name = 'Series'\n\nÂ  Â  # creating observations using a sliding window method\nÂ  Â  train_df = time_delay_embedding(train_series, n_lags=N_LAGS, horizon=HORIZON)\nÂ  Â  test_df = time_delay_embedding(test_series, n_lags=N_LAGS, horizon=HORIZON)\n\nÂ  Â  train_by_series[col] = train_df\nÂ  Â  test_by_series[col] = test_df\n\ntrain_df = pd.concat(train_by_series, axis=0) # combine data row-wise"
  },
  {
    "objectID": "qmd/forecasting-dl.html#sec-fcast-dl-ff",
    "href": "qmd/forecasting-dl.html#sec-fcast-dl-ff",
    "title": "Deep Learning",
    "section": "Feed-Forward",
    "text": "Feed-Forward\n\nFrom Hyndman paper on Local vs Global modeling, Principles and Algorithms for Forecasting Groups of Time Series:Locality and Globality\n\nDeep Network Autoregressive (Keras):\n\nReLu MLP with 5 layers, each of 32 units width\nLinear activation in the final layer\nAdam optimizer with default learning rate.\nEarly stopping on a cross-validation set at 15% of the dataset\nBatch size is set to 1024 for speed\nLoss function is the mean absolute error (MAE)."
  },
  {
    "objectID": "qmd/forecasting-dl.html#sec-fcast-dl-lstm",
    "href": "qmd/forecasting-dl.html#sec-fcast-dl-lstm",
    "title": "Deep Learning",
    "section": "LSTM",
    "text": "LSTM\n\nExtensions\n\nCNN-LSTM - utilizes the CNN layers to improve the feature extraction before sequence prediction by the LSTM\nAutoregressive LSTM (AR-LSTM) -takes in n timesteps worth of data for a product and then makes a prediction for the n+1 week. The prediction for the n+1 week is then used to generate the features as input for the n+2 th weekâ€™s prediction GRU - bi-directional model - In NLP, it uses the preceding value and a succeeding value to predict the middle value. In forecasting, the preceding value is used as a substitute for the succeeding value \n\nExample: You have a sequence 15,20,22,24 and you want to predict the next value\n\nOne GRU which takes the input 15,20,22,24 often called the forward GRU.\n\nThis input sequence of the forward model is often called the forward context\n\nThen you use another representation of the same sequence in reverse order i.e.Â 24,22,20 and 15 which is used by another GRU called the backward GRU.\n\nThis input sequence of the forward model is often called the backward context\n\nThe final prediction is a function of the prediction of both the GRUs.\n\nGRU Extension: â€œbi-directional model of forecasting with truly bi-directional featuresâ€ (BD-BLSTM) (article)\n\nBasically adds seasonality to the GRU\n\nExample: Forecasting the value for May 24th 8:00am\n\nForward GRU: 07:00 AM, 07:15 AM, and 07:45 AM values from 24th May AND 07:00 AM, 07:15 AM, 07:45 AM values from 23rd May\nBackward GRU: 08:15 AM, 08:30 AM and 08:45 AM values from 23rd May"
  },
  {
    "objectID": "qmd/forecasting-dl.html#sec-fcast-dl-rnn",
    "href": "qmd/forecasting-dl.html#sec-fcast-dl-rnn",
    "title": "Deep Learning",
    "section": "RNN",
    "text": "RNN\n\nEssentially a bunch of neural nets stacked on top of each other\noverly simplistic in their assumptions about what should be passed to the next hidden layer\n\nLong Short Term Memory (LSTM) and Gate Recurring Units (GRU) layers provide filters for what information getâ€™s passed down the chain\n\nExample\n\n\nxâ€™s in blue are predictor variables\nhâ€™s in yellow are hidden layers\nyâ€™s in green are predicted values\noutput of the model at h1 feeds into the next model at h2, etc.\n\nNot sure if he output is y1 or some kind of embedding from h1 that feeds into h2\n\nI think each predictor variable"
  },
  {
    "objectID": "qmd/forecasting-dl.html#transformers",
    "href": "qmd/forecasting-dl.html#transformers",
    "title": "Deep Learning",
    "section": "Transformers",
    "text": "Transformers\n\nMisc\n\nI think most of the research is in using these as forecast models themselves and not necessarily as a categorical/discrete feature transformation\nNotes from\n\nTransformers in Time Series: A Survey\n\nAttention heads -Â  enable the Transformer to learn relationships between a time step and every other time step in the input sequence\n\nArchitecture Comparisons\n\nRNNs implement sequential processing: The input (letâ€™s say sentences) is processed word by word.\nTransformers use non-sequential processing: Sentences are processed as a whole, rather than word by word\nThe LSTM requires 8 time-steps to process the sentences, while BERT requires only 2\n\nBERT is better able to take advantage of parallelism, provided by modern GPU acceleration\n\nRNNs are forced to compress their learned representation of the input sequence into a single state vector before moving to future tokens.\nLSTMs solved the vanishing gradient issue that vanilla RNNs suffer from, but they are still prone to exploding gradients. Thus, they are struggling with longer dependencies\nTransformers, on the other hand, have much higher bandwidth. For example, in the Encoder-Decoder Transformer model, the Decoder can directly attend to every token in the input sequence, including the already decoded.\nTransformers use a special case called Self-Attention: This mechanism allows each word in the input to reference every other word in the input.\nTransformers can use large Attention windows (e.g.Â 512, 1048). Hence, they are very effective at capturing contextual information in sequential data over long ranges.\n\nIssues\n\nThe initial BERT model has a limit of 512 tokens. The naive approach to addressing this issue is to truncate the input sentences.\n\nAlternatively, we can create Transformer Models that surpass that limit, making it up to 4096 tokens. However, the cost of self-attention is quadratic with respect to the sentence length.\n\n\nRobustness and Model Size\n\n\nRobustness: As the time series gets longer (aka Input Len), Autoformer holds itâ€™s performance best\n\nThe rest start to crumble after the length gets past 192 time points\n\nModel Size: Autoformer is again the best and holds its performance pretty much up through 24 layers\nHyper-parameters:\n\nembedding dimension\nnumber of heads\nnumber of layers\n\nIn general, 3-6 layers yields the best performance\nMore layers typically adds more accuracy. In NLP and Computer Vision (CV), their models are usually 12 to 128 layers, so this will be an area of improvement.\n\n\nAutoformer Wu et al., 2021 has a moving average trend decomposition component that can be added to other transformer architectures to greatly enhance performance\n\n\nPatchTST (paper, article)\n\nMisc\n\nPackages\n\n{{neuralforecast}} - NIXLA collection of neural forecasting models\n\n\nPatched Attention: Their attention takes in large parts of the time series as tokens instead of a point-wise attention\n\nPrevious Architectures\n\nSelf-Attention treats each timestamp treated as a token\n\nIssues\n\nPermutation-Invariance â€” where the same attention values would be observed if you flipped the points around (?)\nEach timestamp doesnâ€™t have a lot of information in it and gets its importance from the timestamps around it. So treating each timestamp as a token is like tokenizing a character instead of a word.\n\nResults\n\nOverfitting: adding noise didnâ€™t significantly decrease transformer performance\nLonger lookback periods didnâ€™t increase accuracy. Meaning significant temporal patterns werenâ€™t recognized\n\n\n\nPatchTST\n\nSplit each input time series up into fixed-length â€œpatchesâ€ (i.e.Â windows).\nThese patches are then passed through dedicated channels as the input tokens to the main model (the length of the patch is the token size).\nThe model then adds a positional encoding to each patch and runs it through a vanilla transformer encoder.\nBenefits\n\nTakes advantage of local semantic information (i.e.Â window of timestamps)\nFewer input tokens needed allowing the model to capture info from longer sequences and dramatically reducing the memory required to train and predict\nMakes it viable to do Representational Learning (?)\n\n\n\nChannel Independence: different target series in a time series are processed independently of each other with different attention weights.\n\nPrevious Architectures\n\nAll target time series would be concatenated together into a matrix where each row of the matrix is a single series and the columns are the input tokens (one for each timestamp).\nThese input tokens would then be projected into the embedding space, and these embeddings were passed into a single attention layer\n\nPatchTST\n\nEach target series is passed independently into the transformer backbone\nTherefore, every series has its own set of attention weights, allowing the model to specialize better.\n\nSeems like the opposite rationalization of global model forecasting (See Forecasting, Hierarchical/Grouped &gt;&gt; Global\n\n\n\nBenchmarks\n\n\nTwo variants: 64 â€œpatchesâ€ and 42 â€œpatchesâ€\n\n42 patch variant has the same lookback window as the other models\nBoth variants have a patch length of 16 and a stride of 8 were used to construct the input tokens\n\nOn average, PatchTST/64 achieved a 21% reduction in MSE and a 16.7% reduction in MAE. PatchTST/42 achieved a 20.2% reduction in MSE and a 16.4% reduction in MAE."
  },
  {
    "objectID": "qmd/forecasting-ensembling.html",
    "href": "qmd/forecasting-ensembling.html",
    "title": "33Â  Forecasting, Ensembling",
    "section": "",
    "text": "Misc\n\nStatistical ensemble nearly as good as a DL ensemble and was much faster and cheaper. (Thread)\n\n4 models used in the ensemble\n\nAutoARIMA and Exponential Smoothing ({forecast})\nComplex Exponential Smoothing ({smooth})\nDynamically Optimized Theta method ({{StatsForecast}})\n\n\n\nDiagnostics\n\nresidual testing\n\nHyndman suggests taking average degrees of freedom (dof) of the models included in the ensemble to use for residual tests (?)\n\n\nAveraging (equal weighting)\n\nCDC ensemble: https://github.com/reichlab/covid19-forecast-hub#ensemble-model\n\neach model has a forecast distribution for each horizon\n\nâ€œ23 quantiles to be submitted for each of the one through four week ahead values for forecasts of deaths, and a full set of 7 quantiles for the one through four week ahead values for forecasts of cases (see technical README for details), and that the 10th quantile of the predictive distribution for a 1 week ahead forecast of cumulative deaths is not below the most recently observed data.â€\n\nused the median prediction across all eligible models at each quantile level\n\n\nAdaptive Ensembling\n\nAmaal Saadallah\n\nVideo http://whyr.pl//foundation/2021/amal/\nGithub https://github.com/AmalSd/DEMSC\nShe also has paper that uses Reinforcement Learning with actor-critic approach to compute weights, EADRL\n\n\n\nSteps\n\nDrift Detection\n\nDrift from stationarity detected which triggers a re-selection of base models and ensemble\nCalculating drift\n\nSelect a testing window with most recent data\nCalculate predictions using each base model from the ensemble for this testing window\nCalculate scaled root correlation (SRC) between each modelâ€™s predictions and the testing window observations \n\nÅ¶ is the predictions for model, Mi, and testing window W,t.\nY is testing window observations for testing window W,t\ncorr is the pearson correlation\n\nHoeffding Bound\n\nR is the range of the random variable\nW is the number of observations of the testing window used to calculate the SRC\nÎ´ is a user-defined hyperparameter\n\n\nPrune base models\n\nCV\nremove the awful ones\n\nCluster pruned set of models based on predictions\n\nBy selecting a model from each cluster, you get a diversified set of models for the ensemble\n\nBy reducing covariance between models in your ensemble, it reduces the variance in the ensembleâ€™s residuals (my notes)\n\nThe representative model from each cluster is selected for the ensemble\nSuggests using an Improper Maximum Likelihood Estimator Clustering (IMLEC) method\n\nsee paper [Coretto and Hennig, 2016]\nSays Euclidean distance is bad. This method uses covariance to form clusters which fits with trying to get a set of time series with maximum variance for the ensemble\notrimle PKG\n\nhyperparameters automatically tuned; outliers removed\nSeems to be a robust gaussian mixture clustering algorithm\n\nSince this is a GMM you could have models belonging to more than 1 cluster I assume.\n\nIf 1 model is chosen as representative for more than 1 cluster than probably wouldnâ€™t matter too much. Maybe less redundancy.\nif 1 model is in more than 1 cluster but not chosen as representative in both, it might be interesting to know that. Maybe means thereâ€™s some redundancy in your ensemble but not too much. Might want to score an ensemble with only 1 of cluster representatives and see if itâ€™s better.\n\n\nShe has a DTW option but seems to prefer this IMLEC method\n\nshowed some RMSE stats where the dtw method had a 50 RMSE and IMLEC had a 43\nused partitioning for a clustering method but no tuning was involved\n\n\n\nCombine models predictions\n\nsliding window average, averaging, stacking, metalearner"
  },
  {
    "objectID": "qmd/forecasting-functional.html",
    "href": "qmd/forecasting-functional.html",
    "title": "34Â  Forecasting, Functional",
    "section": "",
    "text": "The unit being estimated is the curve instead of the point"
  },
  {
    "objectID": "qmd/forecasting-hierarchical_grouped.html#sec-fcast-group-mdltm",
    "href": "qmd/forecasting-hierarchical_grouped.html#sec-fcast-group-mdltm",
    "title": "Hierarchical/Grouped",
    "section": "Modeltime",
    "text": "Modeltime\n\nMisc\n\nNotes from Learning Lab 50\nHierarchical\n\nData in long format\nAll hierarchical levels were forecasted at once\nFeature engineering\n\nSeparate column for hierarchical categoriesÂ  and they were dummied\nSeparate column for the levels of the hierarchicalÂ  categories and they were dummied\nNo real explicit sequencing variable like a date column used\n\nrow_id and date were designated as ids in recipe step\n\ne.g.Â update_role(row_id, date, new_roll = â€œidâ€)\n\n\n\ndate features were added\n\nrolling means\nmonths, days of week, etc\n\n\nGrouped (local modeling)\n\nA group variable is used to group_nest the datasets and modeling is done on each groupâ€™s dataset.\n\n\nExample: Local Modeling\n\nData preparation\nlibrary(modeltime)\nnested_data_tbl &lt;- data_tbl %&gt;%\n\n# 1. Extending: We'll predict 52 weeks into the future.\nextend_timeseries(\n    .id_varÂ  Â  Â  Â  = id,\n    .date_varÂ  Â  Â  = date,\n    .length_future = 52\n) %&gt;%\n\n# 2. Nesting: We'll group by id, and create a future dataset\n#Â  Â  that forecasts 52 weeks of extended data and\n#Â  Â  an actual dataset that contains 104 weeks (2-years of data)\nÂ  Â  nest_timeseries(\nÂ  Â  Â  Â  .id_varÂ  Â  Â  Â  = id,\nÂ  Â  Â  Â  .length_future = 52,\nÂ  Â  Â  Â  .length_actual = 52*2\nÂ  Â  ) %&gt;%\n\n# 3. Splitting: We'll take the actual data and create splits\nÂ  #Â  Â  for accuracy and confidence interval estimation of 52 weeks (test)\nÂ  #Â  Â  and the rest is training data\nÂ  Â  split_nested_timeseries(\nÂ  Â  Â  Â  .length_test = 52\nÂ  Â  )\n\nCreate NA values for 52 weeks into the future to later fill with forecasts\nCreate the nested (i.e.Â grouped) data structure\nCreate train/test splits of the nested (i.e.Â grouped) structure\n\nid - grouping var\n.actual_data - all data\n.future_data - NAs to filled with forecasts\n.splits - train/test splits\n\n\nSpecify workflow objects\n# prophet\nrec_prophet &lt;- recipe(value ~ date, extract_nested_train_split(nested_data_tbl, 1))Â \nwflw_prophet &lt;- workflow() %&gt;%\n  add_model(\n    prophet_reg(\"regression\", seasonality_yearly = TRUE) %&gt;%Â \n      set_engine(\"prophet\")\n  ) %&gt;%\n  add_recipe(rec_prophet)\n\n# xgbÂ  Â  Â  Â  Â  Â \nrec_xgb &lt;- recipe(value ~ ., extract_nested_train_split(nested_data_tbl, 1)) %&gt;%\nÂ  Â  step_timeseries_signature(date) %&gt;%\nÂ  Â  Â  Â  step_rm(date) %&gt;%\nÂ  Â  Â  Â  step_zv(all_predictors()) %&gt;%\nÂ  Â  Â  Â  step_dummy(all_nominal_predictors(), one_hot = TRUE)\nÂ  Â  wflw_xgb &lt;- workflow() %&gt;%\nÂ  Â  Â  Â  add_model(\nÂ  Â  Â  Â  Â  Â  boost_tree(\"regression\") %&gt;%\nÂ  Â  Â  Â  Â  Â  set_engine(\"xgboost\")\nÂ  Â  Â  Â  ) %&gt;%\nÂ  Â  Â  Â  add_recipe(rec_xgb)\n\nrecipe: notice .splits[[1]] (training split) is used in the recipe step\n\nTrain, Test, and Forecast\n\nTrain and test\nnested_modeltime_tbl &lt;- \n    modeltime_nested_fit(\n\n        # Nested dataÂ \n        nested_data = nested_data_tbl,Â  Â \n\n        # Add workflows\n        wflw_prophet,\n        wflw_xgb,\n        control = control_nested_fit(allow_par = TRUE)\n    )\nAssess performance\nnested_modeltime_tbl %&gt;%Â \n    extract_nested_test_accuracy() %&gt;%\n    table_modeltime_accuracy(.interactive = F)\nDisplays table of forecasting metrics\nnested_modeltime_tbl %&gt;%Â \n    extract_nested_test_forecast() %&gt;%\n  Â  group_by(id) %&gt;%\nÂ  Â  plot_modeltime_forecast(\nÂ  Â  Â  Â  .facet_ncolÂ  = 2,\nÂ  Â  Â  Â  .interactive = FALSE\nÂ  Â  Â  )\n\nfacetted line charts for each group showing forecasts vs the test set.\n\nShow any errors that occurred during modeling\nnested_modeltime_tbl %&gt;%Â \nÂ  Â  Â  extract_nested_error_report()\nSelect model\nbest_nested_modeltime_tbl &lt;- \n    nested_modeltime_tbl %&gt;%\n        modeltime_nested_select_best(\n            metricÂ  Â  Â  Â  Â  Â  Â  Â  = \"rmse\",Â \n      Â  Â  Â  minimizeÂ  Â  Â  Â  Â  Â  Â  = TRUE,Â \n      Â  Â  Â  filter_test_forecasts = TRUE\n      Â  )\n\nmetrics: rmse (default), mae, mape, mase, smape, rsq\nminimize: If TRUE, says select model with the lowest score\nfilter_test_forecasts: If TRUE, says only keep the best test forecasts for each group logged\n\nYou can access these test set forecasts using extract_nested_test_forecast()\n\n\nPerformance metrics of best models\nbest_nested_modeltime_tbl %&gt;%\n    extract_nested_best_model_report()\n\nmetrics for the best model of each groupâ€™s test set\n\nVisualize test set forecasts\nbest_nested_modeltime_tbl %&gt;%\n    extract_nested_test_forecast() %&gt;%\n        group_by(id) %&gt;%\n        plot_modeltime_forecast(\n        .facet_ncolÂ  = 2,\n        .interactive = FALSE\n        )\n\nfacetted line charts showing test set forecasts for each group\n\nForecast\nnested_modeltime_refit_tbl &lt;- \n    best_nested_modeltime_tbl %&gt;%\n        modeltime_nested_refit(\n            control = control_nested_refit(verbose = TRUE, allow_par = TRUE)\n        )\n\nFits selected models on each groupâ€™s whole dataset and forecasts\n\nAccess Forecasts and plot them\n\nnested_modeltime_refit_tbl %&gt;%\n    extract_nested_future_forecast() %&gt;%\n        group_by(id) %&gt;%\n        plot_modeltime_forecast(\n            .interactive = FALSE,\n            .facet_ncolÂ  = 2\n        )"
  },
  {
    "objectID": "qmd/forecasting-hierarchical_grouped.html#sec-fcast-group-global",
    "href": "qmd/forecasting-hierarchical_grouped.html#sec-fcast-group-global",
    "title": "Hierarchical/Grouped",
    "section": "Global Modeling",
    "text": "Global Modeling\n\nMisc\n\nfrom Why arenâ€™t you getting the most out of your Marketing AI\n\nâ€œConsider the consumer goods company whose data scientists proudly announced that theyâ€™d increased the accuracy of a new sales-volume forecasting system, reducing the error rate from 25% to 17%. Unfortunately, in improving the systemâ€™s overall accuracy, they increased its precision with low-margin products while reducing its accuracy with high-margin products. Because the cost of underestimating demand for the high-margin offerings substantially outweighed the value of correctly forecasting demand for the low-margin ones, profits fell when the company implemented the new,â€more accurateâ€ system.â€\nThis should be something to be aware of and monitored with global forecasting model. Does the optimization of single function unintentionally optimize the error of a series with less financial importance over a more financially important series in order to minimize global error? Where in classification models, error metrics and loss functions are chosen by the cost/benefit of TP, FP, TN, FN.Â  Can a loss function be developed that tunes a global model and weights errors by group series?\n\n\nFrom paper, Principles and Algorithms for Forecasting Groups of Time Series:Locality and Globality More detailed (raw) notes, Principles and Algorithms for Forecasting Groups of Time Series: Locality and Globality\n\nPaper tested global vs local methods on groups of series across many datasets (thousands of series).\n\nglobal models almost always had better generalization error. (ge = training set score - test set score)\n\nWhile local models may have better in-sample performance, the low generalization error of global models makes its CV score a truer representation of out-of-sample than with local methods. This should result in better production models since our algorithm selection will be better.\n\nglobal models almost always outperformed local methods on most homogeneous and heterogenous groups of series\n\nFinds that for global methods, the more complex the model (e.g.Â OLS â€“&gt; ML â€“&gt; DL), the better the forecasting\n\nUsing a global method â€“&gt; larger dataset â€“&gt; more degrees of freedom â€“&gt; suitable conditions for more complex models to perform well\n\nDancho, global example using modeltime, makes point that global models perform better on group dataset as a whole but not necessarily for every individual series in the dataset.\n\nmodeltime_accuracy(acc_by_id = T) will measure the accuracy per time series.\nOptions\n\nMight be better to remove worse performing series and just forecast them using a local method\nMore likely itâ€™s better to keep those series in the global method since they probably adds predictive value to the other series in the dataset and additionaly forecast those worse performing series with local method\n\n\nIf we want to estimate a complex pattern in a series and that series has sufficient sample size, then we are better off with local models.\nTerms\n\nlocal method - fitting each series separately with a different model\nglobal method - one model for the entire set of series\n\nAssumptions\n\nGroups of independent time series\n\nEach series in the group should evolve by similar patterns but independently of each other\n\nThinking this assumption can be somewhat relaxed and still achieve superior results vs local models. (see pedestrian data example below)\nPaper lists many popular datasets that can be tested for independence to see how strict this assumption needs to be.\n\nUnder heavy dependence global models loose their beneficial performance guarantees (though practicioners could still try them)\n\n\nExample\n\nEach time series in the group represents the hourly pedestrian count of a different location across Melbourneâ€™s CBD (city bar district: entertainment area)\n\nEach location in this district is different series.\nThe domain that allows for the â€œsame pattern of evolutionâ€ would be that each series is measuring pedestrian activity.\nSince theyâ€™re at different locations, the counts are independent of each other.\n\n(I guessâ€¦ if this is a large area and the same pedestrians are being counted. Maybe one group is goint to a concert, another is going to a show, and another is bar hopping, etc.)\n\n\n\nGlobal Method Steps:\n\nFix an AR order (i.e.Â number of lags)\nCreate lags according to that AR order for each series\nFor each series, embed the observed series and each lag into a matrix\nStack these matrices on top of each other to create one large matrix\nRun CV, using the large matrix, to determine the best lag order\nForecast using the large matrix.\n\nLags\n\nGlobal models should have large memories (i.e.Â large number of lags)\n\nLag order (i.e.Â max lag) should usually be determined by the length of the shortest time series in the dataset.\n\nOptimal lag may be in the 100s\n\nNotable exception to the pattern of â€œlonger memory, better accuracyâ€ is intermittent data where smaller lag orders work better for global models.\n\nMake sure the lag order is NOT a multiple of seasonal/cyclical frequency of each series\n\nPreferrably the lag order should be greater than the largest seasonal/cyclical frequency of the group as there is usually an improvement in error.\n\nThis effect is explained as an over-fitting phenomena related to the heterogeneity of seasonal strengths in the datasets. In those datasets that contain both strongly seasonal and non seasonal series (or different seasonal periods), a global model tends to favor the seasonality at memory levels that are multiples of the frequency because it produces good results in-sample. When the memory is over the seasonal period, the global model can use its coefficients to fit both seasonal and non seasonal patterns\n\n\nPolynomial Autoregression (AR) models exponentiate the lags. So the design matrix includes the lags and the exponentiated series.\n\nIf the polynomial is order 3, then order 2 is also included. So, now, the design matrix would be the lags, the square of each lag, and the cube of each lag\n\n\nPreprocessing\n\nProbably should standardize or at least scale\n\nFor ML/DL models, standardization of the series had a positive, minor effect (lm models are scale invariant)\nIf any of the series has a meaningful maximum or minimums at 1 or 0 respectively, then you maybe just want to scale.\nAlso possible to add a scale variable to the matrix so that even if you standardize or scale, that info is still available to the model.\nWithout standardization and depending on the error measure that you use, prediction errors on series that are on larger scales may have an outsized effect on the overall error calculation\nScaling can help deal with outlier series (e.g.Â one series has much larger scale than the rest, dominating the fitting) and in the case of deep networks, it makes them faster to train\n\nScaling by MASE and using MASE as the loss function is equivalent to to minimizing the MAE in the preprocessed time series.\n\nâ€œscaling done by the MASE can be applied to data as a preprocessing normalizing step, which we apply to each series. Then minimizing the MASE is equivalent to minimizing the mean absolute error in the preprocessed time series. Compared to SMAPE, MASE is easier to fit with off-the-shelf optimizers (it is convex), and it therefore isolates better the effect of the model class rather than the optimization procedure used to fit itâ€\n\n\n\nIndex variable not needed.\n\ntidymodels has a recipe step to where you can include an index variable but give it role = id. It wonâ€™t be used as a predictor but could help with separating out predictons later on.\n\n\nSeasonality\n\nDonâ€™t remove the seasonality from the data\nAutomatically discovered, even by a global AR model, and even though itâ€™s not part of the model specification.\nFor DL/ML models\n\nadding seasonality features may help but is not required for seasonal time series\nadding tsfeatures (ML/DL) and using larger input window sizes (DL) or sufficiently large AR orders (ML) should be enough to account for seasonality\n\n\nPartitioning heterogeneous time series\n\nheterogeneous: different domains (measured in different units) and with different frequencies (from hourly to yearly measurements), patterns, trends, etc.\nFor heterogeneous series, potentially more accuracy with partitioning the series by frequency and/or domains before fitting sub(?)-global models\n\nPartioning (or clustering) reduces sample size though. So something to keep an eye on.\n\nWithout partitioning, DL global models with tsfeatures + lags + frequency variables (fourier or splines?) outperformed partitioned models in the paper\n\nDeep Network Autoregressive (Keras):\n\nReLu MLP with 5 layers, each of 32 units width\nLinear activation in the final layer\nAdam optimizer with default learning rate.\nEarly stopping on a cross-validation set at 15% of the dataset\nBatch size is set to 1024 for speed\nLoss function is the mean absolute error (MAE).\n\n\nPartitioning and clustering\n\nBoth can potentially be beneficial\nUsing clustering adds a potentially expensive processing step, plus a sufficiently complex model (e.g.Â the DL model w/additional features above) should detect whatever additional information that clustering is likely to provide.\n\n\nPolynomial Regression\n\nBad to use for a local model but pretty good as a global model\n\nLocal Model: Recursive forecasting makes polynomials numerically unstable (e.g.Â an 18-step-ahead forecast for this dataset is equivalent to a 54 degree polynomial) and are not recommended in the literature for automatic time series forecasting.\nGlobal Model: 2nd order polynomial fit wouldâ€™ve placed 2nd in M4 (yearly ts) and the DL global model wouldâ€™ve placed 3rd for the Quarterly data.\n\nAccomplished by squaring the lags of the series and adding them as extra variables to the large matrix.\n\n\nPolynomial Autoregression (AR) models exponentiate the lags. So the design matrix includes the lags and the exponentiated series.\n\nIf the polynomial is order 3, then order 2 is also included. So, now, the design matrix would be the lags, the square of each lag, and the cube of each lag\nlibrary(dplyr); library(timetk)\n# tbl w/polynomial design matrix of order 3 (along with original series, group variable, and date variable)\n# value is the ts values (original series)\npoly_tbl &lt;- group_tbl %&gt;%Â \nÂ  tk_augment_lags(.value = value, .lags = 1:4) %&gt;%Â  Â \nÂ  mutate(across(contains(\"lag\"), .fns = list(~.x^2, ~.x^3), .names = \"{.col}_{ifelse(.fn==1, 'quad','cube')\"))\n\n.fnÂ  is the item number in the .fns list.\nsquared lag 2 will have the name â€œvalue_lag2_quadâ€\n\n\n\nMultivariate Time Series (see section in Forecasting, Multivariate)\n\nInterdependent series (think VAR models)\nExample: Energy consumption per household and income per household time series (i.e.Â energy consumption and income are interdependent) represent a multivariate process\n\nDataset has households sampled all around the planet. The pair of time series for each household does not necessary affect any of the other households, but we could assume that they follow the same pattern.\n\nâ€œSampled from around the planetâ€ allows us to reasonably assume these pairs of time series are independent."
  },
  {
    "objectID": "qmd/forecasting-ml.html#sec-fcast-ml-misc",
    "href": "qmd/forecasting-ml.html#sec-fcast-ml-misc",
    "title": "35Â  ML",
    "section": "35.1 Misc",
    "text": "35.1 Misc\n\nNotes from\n\nLearnings from Kaggle Forecasting Competitions: The Walmart Competition\n\nDeep networks suffer from the problem of instability for recursive forecasting, and itâ€™s recommended to use direct forecasting\nTree based models can only predict within the range of training data.\nModels seen used\n\nEnsemble combination: KNN, random forest and principal components regression together with ARIMA, Unobserved Components Model and linear regression\n\nConditions in which these models perform poorly\n\nunder 500 obs (need to reread the paper to get this more exact). Think at around this number, the ML models start to catch the statistical models\nadditional explanatory variables have poor predictive power\nts has high seasonality\n\nAlways fit a statistical model for comparison\nAlgorithm Specifications\n\nFrom Hyndman paper on Local vs Global modeling, Principles and Algorithms for Forecasting Groups of Time Series:Locality and Globality\n\nXGboost,\n\nsubsampling=0.5\ncol_sampling=0.5\nEarly stopping on a cross-validation set at 15% of the dataset.\nLoss function is RMSE\nValidation error is MAE\n\n\nLightGBM\n\narg: linear_tree fits a piecewise linear model for each leaf. Helps to extrapolate linear trends in forecasting\n\nSeems to act like a basic distribution forest"
  },
  {
    "objectID": "qmd/forecasting-ml.html#sec-fcast-ml-terms",
    "href": "qmd/forecasting-ml.html#sec-fcast-ml-terms",
    "title": "35Â  ML",
    "section": "35.2 Terms",
    "text": "35.2 Terms\n\nShort-Term Lags: Lags &lt; Forecast Horizon.\nDirect Forecasting: Modeling each horizon separately"
  },
  {
    "objectID": "qmd/forecasting-ml.html#sec-fcast-ml-preproc",
    "href": "qmd/forecasting-ml.html#sec-fcast-ml-preproc",
    "title": "35Â  ML",
    "section": "35.3 Preprocessing",
    "text": "35.3 Preprocessing\n\nDifference until stationary\n\nSince tree models canâ€™t predict outside the range of the their training data, trend must be removed.\nIn addition to the deterministic trend, this approach also removes stochastic trends.\nForecasts will need to be back-transformed\n\nforecast = differenced forecast + previous value\nThen recursively for the next forecasts in the horizon\n\n\n\nLog transform\n\nforecasts need to be back-transformed\n\nTarget encode cat feature then lag\nScale series\nRemove time stamp after creating date features\n\nDate features will help keep track of time\n\nThese will need to be one-hot encoded or some other discrete/categorical transformation"
  },
  {
    "objectID": "qmd/forecasting-ml.html#sec-fcast-ml-mstep",
    "href": "qmd/forecasting-ml.html#sec-fcast-ml-mstep",
    "title": "35Â  ML",
    "section": "35.4 Multi-Step Forecasting",
    "text": "35.4 Multi-Step Forecasting\n\nMisc\n\nNotes from: 6 Methods for Multi-step Forecasting\n\nExample dataset\n\n\nt-3 through t-0 are the predictors\nt+1 through t+4 are potential outcome variables\n\n\n\nRecursive (aka Iterative) - Training a single model for one-step ahead forecasting. Then, the modelâ€™s one-step ahead forecast is used as data to get the 2nd-step ahead forecast.\n\nThe one-step ahead forecast is NOT added to the training data and the model refitted. It replaces the 0th lag value in the training data, and the previous 0th lag data becomes the 1st lag data, etc. The final lag (e.g t-3 in the example data) that was used in training the model is discarded. Therefore, the same number of lags that was used in training the model remains the same. This updated dataset becomes â€œnew dataâ€ and along with the original model are used as input for predict\nThis process is iterated using previous forecasts to get predictions for multiple steps ahead\nMethod leads to propagation of errors\n\nDirect - Builds one model for each horizon\n\nEach model trains on a lead of the outcome variable that matches the horizon step\n\ne.g.Â the model forecasting the 2nd step-ahead value will train with an outcome variable that is t+2\n\nNo previous forecast is used to predict a forecast ahead of it.\nAssumes that each horizon is independent which is usually false\nExample\nfrom sklearn.multioutput import MultiOutputRegressor\ndirect = MultiOutputRegressor(LinearRegression())\ndirect.fit(X_tr, Y_tr)\ndirect.predict(X_ts)\n\nâ€œY_trâ€ contains lead variables for each step in the forecast horizon\n\n\nExample: Multi-step time series forecasting with XGBoost\n\nCodes a sliding-window (donâ€™t think itâ€™s a cv approach though) and forecasts each window with the model\nNot exactly sure how this works. No helpful figs in the article or the paper if follows, so would need to examine the code\n\nExample: Preprocessing training data for 1 then 2 steps ahead and forecasting (post,Â  Code &gt;&gt; Time Series &gt;&gt; direct-multistep-forecasting-regression.R)\n\nThe FIRST 9 rows and the LAST 4 rows of the training data before preprocessing are shown\nâ€œy_observedâ€ is the original outcome column\nâ€œy_fâ€ is y_observed but stepped forward\nâ€œx_0â€ is a copy of y_observed\nâ€œx_1â€ to â€œx_5â€ are lags of y_observed\n1-Step Ahead Training Data\n\nBefore Preprocessing\n\n\nâ€œy_fâ€ has been stepped forward 1-step\nâ€œy_observedâ€ and rows with NAs are deleted\nEven though 6 rows are deleted from the training data, all values are used for training\n\nAfter Preprocessing\n\n\nThe model is fit using this data\n\n\n2-Step Ahead Training Data\n\nBefore Preprocessing\n\n\nâ€œy_fâ€ has been stepped forward 2-steps\nâ€œy_observedâ€ and rows with NAs are deleted\nEven though 7 rows are deleted from the training data, all values are used for training\n\nAfter Preprocessing\n\n\nThe model is fit using this data\n\n\nForecast\n\n\nEach step-ahead model predicts on this same row of data which is the final row of the training data (row 110)\n\nNote that this row wasnâ€™t used to train the model.\n\n\n\nDirect-Recursive (aka Chaining) - Combo of Recursive and Direct\n\nRecursive - Previous forecasts are used as data to get the next step-ahead forecast\nDirect - The previous forecasts are added to the training data and the model is retrained for each horizon.\nMore computationally intensive since thereâ€™s more data and a model has to trained for each horizon step.\n\nCanâ€™t be parallelized since the process is sequential.\n\n\nData as Demonstrator (DaD) - Error correction method for the Recursive method\n\nUse the training set to correct errors that occurs during multi-step prediction to mitigate the propagation of errors that occurs in recursive forecasting.\nIteratively enriches the training set with these corrections after each recursive forecast.\ngithub\n\nDynamic Factor Machine Learning (DFML) - A feature reduction method is applied to a multivariate or multistep time series and the model trains on the components as outcome variables. Then, the forecasts are backtransformed.\n\nArticle uses the direct method\n\nSee article for code. Not sure how effective it is but the code is pretty simple\n\n\nMulti-Output - A single model which learns all the forecasting horizon jointly\n\nGoal is to capture the dependencies between forecasts better.\nOutput is a multi-step output all at once\n\nExample uses KNN but the article mentions regularized regression, rf, DL algorithms can be used\n\nThink this sounds like a multivariate approach and not a global approach\nPreprocessing should include removing the seasonality.\nSee article for paper reference\n\nHorizon as a Feature\n\nNotes from Forecast Multiple Horizons: an Example with Weather Data\nGlobal approach where design matrices for each horizon are stacked (i.e.Â bind_rows) on top of each other to create a global design matrix. A horizon feature variable is added (e.g.Â 1 for the 1-step, 2 for the 2-step, etc.) to index each sub-design matrix."
  },
  {
    "objectID": "qmd/forecasting-ml.html#sec-fcast-ml-feateng",
    "href": "qmd/forecasting-ml.html#sec-fcast-ml-feateng",
    "title": "35Â  ML",
    "section": "35.5 Feature Engineering",
    "text": "35.5 Feature Engineering\n\nAlso see Feature Engineering, Time Series\nDate time Features\n\nMinutes in a Day\nQuarter of the Year\nHour of Day\nWeek of the Year\nSeason of the Year\nWeekend or Not\nDaylight Savings or Not\nPublic Holiday or Not\nBefore or After Business Hours\nExample: py\nts_data['hour'] = [ts_data.index[i].hour for i in range(len(ts_data))]\nts_data['month'] = [ts_data.index[i].month for i in range(len(ts_data))]\nts_data['dayofweek'] = [ts_data.index[i].day for i in range(len(ts_data))]\n\nLags - Lags of the target variable\n\n{{pandas}} has a shift function\n\nWindow Statistics (See Python, Pandas &gt;&gt; Time Series &gt;&gt; Calculations &gt;&gt; Aggregation)\n\nSliding window features - at each time point, a summary of values over a fixed window of prior timesteps\nRolling window statistics - a range of values that includes the time point itself as well as some specified number of data points before and after (?) the time point used\n\n{{pandas}} has a rolling function\n\nExpanding window statistics\n\n{{pandas}} has an expanding function\nExample: minimum, mean, and maximum\n# create expanding window featuresÂ \nfrom pandas import concat\nload_val = ts_data[['load']] window = load_val.expanding()Â \nnew_dataframe = concat([window.min(),Â  window.mean(), window.max(), load_val. shift(-1)], axis=1) new_dataframe.columns = ['min', 'mean', 'max', 'load+1']\nprint(new_dataframe.head(10))\n\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  minÂ  Â  Â  meanÂ  Â  maxÂ  Â  Â  load+1Â \n2012-01-01 00:00:00 2,698.00 2,698.00 2,698.00 2,558.00Â \n2012-01-01 01:00:00 2,558.00 2,628.00 2,698.00 2,444.00Â \n2012-01-01 02:00:00 2,444.00 2,566.67 2,698.00 2,402.00Â \n2012-01-01 03:00:00 2,402.00 2,525.50 2,698.00 2,403.00Â \n2012-01-01 04:00:00 2,402.00 2,501.00 2,698.00 2,453.00Â \n2012-01-01 05:00:00 2,402.00 2,493.00 2,698.00 2,560.00Â \n2012-01-01 06:00:00 2,402.00 2,502.57 2,698.00 2,719.00Â \n2012-01-01 07:00:00 2,402.00 2,529.62 2,719.00 2,916.00Â \n2012-01-01 08:00:00 2,402.00 2,572.56 2,916.00 3,105.00Â \n2012-01-01 09:00:00 2,402.00 2,625.80 3,105.00 3,174.00\n\n\nUse of singular value decomposition to learn specific patterns across a grouping variable (e.g.Â department specific patterns across stores)\n\nPerform holiday adjustments to the data\nGet the training data for a specific department into a matrix with weeks as rows and stores as columns.\nUse SVD to produce a low-rank approximation of the sales matrix. The winner reduced the rank from 45 (# stores) to between 10-15.\nReconstruct the training matrix using the low-rank approximation.\nForecast the reconstructed matrix using an STL decomposition followed by exponential smoothing of the seasonality adjusted series. This was conducted in R using the stlf function of the forecast package.\n\nExternal Features\n\naverage weekly temperature by region,\nthe fuel price by region,\nthe consumer price index (CPI)\nunemployment rate.\n\nExample: From Hyndman local vs global models paper (See Algorithm Specifications section)\n\nUsed\n\n12 lags (length of shortest series in M4)\nfrequency features (fourier series?)\nother features (tsfeatures PKG)\n\ntsfeatures measure a bunch of ts characteristics.\nThese would be just 1 number per series. Iâ€™m not sure how you use them as a predictors\n\n\nData was a matrix stack of all the time series in the M4 competition\n\nmaybe since itâ€™s a ts stack, the tsfeatures stuff has enough variance to useful in prediction"
  },
  {
    "objectID": "qmd/forecasting-multivariate.html#sec-fcast-multivar-misc",
    "href": "qmd/forecasting-multivariate.html#sec-fcast-multivar-misc",
    "title": "Multivariate",
    "section": "Misc",
    "text": "Misc\n\nMultivariate time series analysis seeks to analyze several time series jointly. The rationale behind this is the possible presence of interdependences between the different time series.\nExamples\n\nUS GDP data, S&P 500 data, and oil prices\nMortgage applications, interest rate data, and unemployment rates\nOrder flow data, price levels, and volatilities\n\nModels\n\nVector Autoregression (VAR)\n\ntypes\n\nstructural: interdependent time series affect each other contemporaneously\nreduced: interdependent time series do not affect each other contemporaneously (method commonly used)\n\n\nExponentially Weighted Moving Average (EWMA)\nBEKK(p, q)\n\n(after Baba, Engle, Kraft, and Kroner) is a multivariate extension of the GARCH\n\nDynamic Conditional Correlation (DCC)\nMatrix Autoregression (MAR)\n\nNotes from Matrix Autoregressive Model for Multidimensional Time Series Forecasting\nSame as VAR but with some advantages\n\nCan maintain the original data representation in the form of matrix.\nReduces the amount of parameters in autoregressive models.\n\nFor example, if we use VAR to explore such data, we would have (mn)Â² parameters in the coefficient matrix. But using MAR, we only have _m_Â²+_n_Â². This can avoid the over-parameterization issue in VAR for handling high-dimensional data.\n\n\nLoss function\n\n\nMatrices A and B are coefficient matrices than need to be estimated\nThere are closed form solutions for A and B but the solution to A is dependent on the solution to B and vice versa\n\nThe Alternating Least Squares (ALS) Algorithm is used to solve this problem"
  },
  {
    "objectID": "qmd/forecasting-multivariate.html#sec-fcast-multivar-dynfact",
    "href": "qmd/forecasting-multivariate.html#sec-fcast-multivar-dynfact",
    "title": "Multivariate",
    "section": "Dynamic Factor Models (DFM)",
    "text": "Dynamic Factor Models (DFM)\n\nCommon dynamics of a large number of time series variables stem from a relatively small number of unobserved (or latent) factors, which in turn evolve over time.\nFor some macroeconomic applications it might be interesting to see whether a set of obserable variables depends on common drivers (unobserved factors). The estimation of such common factors can be done using so-called factor analytical models.\n\nFactor Analysis but for time series\nFor large sets of time series, which show, in most cases, strong correlation patterns\n\ntl;dr:\n\nRun PCA on bunch of time series that are relevant to the outcome variable you want to model or forecast.\nDetermine the number of factors (components) from the PCA\nUse VAR to determine the number of lags (or potentially leads) for the factors\nFit a model for the outcome variable using the factors and their lags as predictors.\n\nMisc\n\nNotes from\n\nHandbook of Macroeconomics, Ch.8\n\n111 pgs; SVARs, FAVARs, DFMs\n\n\npackages\n\n{dfms}:\n\nImports {collapse} and {Rcpp} so it should be very fast\n3 methods available\n\nA two-step estimator based on Kalman filtering\n\nfast, and forcasts should be similar to the other 2\n\nA quasi-maximum likelihood approach\nMaximum likelihood estimation of factor models on datasets with arbitrary pattern of missing data\n\n\n{nowcasting}- fit dynamic factor models specific to mixed-frequency nowcasting applications.\n\nThe latter two packages additionally support blocking of variables into different groups for which factors are to be estimated, and EM adjustments for variables at different frequencies\n\n{bvartools}\n\nUse Cases\n\nSensor Data\n\ne.g.Â IoT devices, telecommunication, industrial manufacturing, electric grid\n\nFinance\nMacroeconomics\n\nExample: (ebook ch8. introduction): a single-factor DFM fit using 58 quarterly US real activity variables (sectoral industrial production (IP), sectoral employment, sales, and National Income and Product Account (NIPA) series) is used to backcast four-quarter growth rates of four measures of aggregate economic activity (real Gross Domestic Product (GDP), total nonfarm, employment, IP, and manufacturing and trade sales)\n\n\nBlack solid line is the the observed; Red dotted line is the fitted value\nFitted line from lm(outcome ~ factor)\n\noutcome is total nonfarm, employment, IP, or manufacturing and trade sales\nfactor is from the dfm\n\nR2s of the fits range from 0.73 for GDP to 0.92 for employment.\n\nChicago Fed fits a factor model called the â€œcfnaiâ€ on these key economic indicators (Thread)\n\n\n\nâ€œStructural DFMs, FAVARs, and SVARs are a unified suite of tools with fundamentally similar structures that differ in whether the factors are treated as observed or unobserved. By using a large number of variables and treating the factors as unobserved, DFMsâ€average outâ€ the measurement error in individual time series, and thereby improve the ability to span the common macroeconomic structural shocks.â€\n\nFactor Analytical Model General Form (Static)\n\nxt = Î»*ft + ut where ut âˆ¼Â N(0,R)\n\nxt is an M-dimensional vector of observable variables\nft is an NÃ—1 vector of unobserved factors\nÎ» is an MÃ—N matrix of factor loadings\nut is an error term\nR is a MÃ—M measurement (observation) covariance matrix and assumed to be diagonal\n\nAlso known as the measurement equation, which describes the relationship between the observed variables and the factors\n\nAllowing ft to take autocorrelation into account makes it â€œdynamicâ€\n\nft = A1*ftâˆ’1 + â€¦ + Ap*ftâˆ’p + vt where vtâˆ¼Â N(0,Q)\n\nAi is the NÃ—N coefficient matrix correponding to the ith lag of factor, f\nvt is an error term\nQ is the NÃ—N state covariance matrix\n\nAlso known as the transition equation, which describes the intertemporal relationships between the factors\n\nPreprocessing\n\nAll series must be stationary with no cointegration\nAll series must be scaled and centered\nThe number of factors must be chosen.\nThen, the number of lags can chosen given the number of factors\nData at different frequencies\n\n{dfms} has a couple methods that approximate values\nDuplicate the longer frequency series multiple times. (see article)\n\ne.g.Â duplicate a quarterly series 3 times in a dataset with monthly time series.\nWhen these series get converted to shorter frequencies before fitting (e.g.Â quarterly to monthly), the missing values will just be NAs which will cause these series to have less weight in the estimation process. By duplicating them, it gives there values more weight.\nIf you decide to give these series more weight, you should be relatively certain theyâ€™re important to the forecast.\n\nEstimate different factor models for monthly and quarterly series, and combine them for the final prediction (e.g.Â by aggregating or blocking the monthly factor estimates (?)).\n{nowcasting} and {nowcastDFM} provide elaborate models for mixed-frequency nowcasting (but will be slower)\n\n\nForecasts\n\nFactors are dynamically forecasted using the transition equation.\nThe factor forecasts can then be fed to the observation equation to obtain forecasts for the input series\nExample: Introduction to dfms\n\n\n4 factors have been chosen and theyâ€™re highlighted\nInput series are in gray\nForecasts (after dotted vertical) converge the mean.\nquarterly variables, such as US GDP, to be forecast using a large set of monthly variables released with different lags\nreduce the information contained in dozens of monthly time series into only two dynamic factors. These two estimated factors, which are initially monthly, are then transformed into quarterly factors and used in a regression against GDP\n\nBackcasting refers to forecasting the value of a yet unpublished variable for a past period, while nowcasting will be with respect to the current period\nThe chosen number of factors, râˆ— will correspond the IC with the lowest value. The penalty in equation IC2 is highest when considering finite sample\nGiven a strategy to identify one or more structural shocks, a structural DFM can be used to estimate responses to these structural shocks.\ndescription of the error terms to the equations\n\nmean-zero idiosyncratic component et (measurement eq)\n\nidiosyncratic disturbance can be serially correlated. if so, then model is incompletely specified. Solution is to model it as an autoregression\nIf there is no autocorrelation in the error terms (*unreasonable assumption across most applications), then the model is an exact DFM, and the correlation of one series with another occurs only through the\nlatent factors\n\nÎ·t is the q x 1 vector of (serially uncorrelated) mean-zero innovations to the factors (transition eq)\n\nIf some of the variables are cointegrated, then transforming them to first differences loses potentially important information that would be present in the error correctionterms (that is, the residual from a cointegrating equation, possibly with cointegrating coefficients imposed). Here we discuss two different treatments of cointegrated variables, both of which are used in the empirical application of Sections 6 and 7.\n\nOption 1: Include the first difference of some of the variables and error correction terms for the others. This is appropriate if the error correction term potentially contains important information that would be useful in estimating one or more factors.\nOption 2: include all the variables in first differences and not to include any spreads. Appropriate if the first differences of the factors are informative for the common trend but the cointegrating residuals do not load on common factors\n\nDidnâ€™t understand it lol (pg 427). Think thereâ€™s an example in sect 7.2 that will illustrate it.\nExample: While there is empirical evidence that these oil prices, for example Brent and WTI, are cointegrated, there is no a priori reason to believe that the WTI-Brent spread is informative about broad macro factors, and rather that spread reflects details of oil markets, transient transportation and storage disruptions, and so forth.\n\nOption 3: specify the DFM in levels or log levels of some or all of the variables, then to estimate cointegrating relations and common stochastic trends as part of estimating the DFM. This approach goes beyond the coverage of this chapter, which assumes that variables have been transformed to be I(0) and trendless.\n\nBanerjee and Marcellino (2009)and Banerjee et al.Â (2014, 2016) develop a factor-augmented error correction model (FECM) in which the levels of a subset of the variables are expressed as cointegrated with the common factors. The discussion in this chapter about applications and identification extends to the FECM.\n\n\nThe error in estimation of the factors during PCA can be ignored when they are used as regressors"
  },
  {
    "objectID": "qmd/forecasting-nonlinear.html#sec-fcast-misc",
    "href": "qmd/forecasting-nonlinear.html#sec-fcast-misc",
    "title": "Nonlinear",
    "section": "Misc",
    "text": "Misc\n\nGaps in the time variable can be a problem if you are trying to interpolate between those gaps. (see bkmk, method = â€œremlâ€ + s(x, m = 1))\nPackages\n\nTime Series Task View â€œNonlinear Time Series Analysisâ€\n{nonlinearTseries}\n{probcast} which has function wrappers around gams, gamlss, and boosted gamlss models from {mgcv}, {mboost}, {gamlss}, etc. for use in forecasting. Supports high-dimensional dependency modeling based on Gaussian Copulas (paper, use case)\n\nCopulas\n\nAlso see Correlation, Association, and Distance &gt;&gt; Copulas\nTUTORIAL julia, copulas + ARMA model, example w/exonential distribution - ARMA Forecasting for Non-Gaussian Time-Series Data Using Copulas | by Sarem Seitz | Jun, 2022 | Towards Data Science\nIssues\n\nWhen the size of the observed time-series becomes very large.\n\nIn that case, the unconditional covariance matrix will scale poorly and the model fitting step will likely become impossible.\nPotential Solution: Implicit Copulas which define a Copula density through a chain of conditional densities\n\nMLE for distributions where the derivatives of their cdfs becomes complex\n\nExponental distributionâ€™s is simple (used in article)\nSee article for potential solutions"
  },
  {
    "objectID": "qmd/forecasting-nonlinear.html#sec-fcast-eda",
    "href": "qmd/forecasting-nonlinear.html#sec-fcast-eda",
    "title": "Nonlinear",
    "section": "EDA",
    "text": "EDA\n\nChaotic nature of the time series is obvious (e.g.Â frequent, unexplainable shocks that canâ€™t be explained by noise)\n\nCreate an artificial data set using a gaussian dgp and compare it to the observed data set\n\nFor details see R &gt;&gt; Documents &gt;&gt; Time Series &gt;&gt; nonlinear-time-series-analysis-2ed-kantz-schreiber (pg6 and Ch.4 sect 7.1)\n\nTakes the range of values, mean, and variance from the observed distribution and generates data\nThen data is filtered so that the power spectum is the same\nâ€œPhase Portraitsâ€ are used to compare the datasets."
  },
  {
    "objectID": "qmd/forecasting-nonlinear.html#sec-fcast-stlelm",
    "href": "qmd/forecasting-nonlinear.html#sec-fcast-stlelm",
    "title": "Nonlinear",
    "section": "STL-ELM",
    "text": "STL-ELM\n\nSeasonal Trend Decomposition using LOESS (STL) - Extreme Learning Machine (ELM)\nHybrid univariate forecasting model for complex series (non-stationary, non-linear)\nThe univariate series is decomposed into subseries using STL and each subseries is forecast using ELM, then those subseries forecasts are ensembled for the final forecast\n\nEach subseries is simpler and stationary"
  },
  {
    "objectID": "qmd/forecasting-nonlinear.html#sec-fcast-vmdtdnn",
    "href": "qmd/forecasting-nonlinear.html#sec-fcast-vmdtdnn",
    "title": "Nonlinear",
    "section": "VMD-TDNN",
    "text": "VMD-TDNN\n\nVariational Mode Decomposition (VMD) Based Time Delay Neural Network Model (TDNN)\nHybrid univariate forecasting model for complex series (non-stationary, non-linear)\nThe univariate series is decomposed into â€œmodesâ€ using VMD and each mode is forecast using TDNN, then those mode forecasts are recombined for the final forecast\n\nThe modes are generated by Intrinsic Mode Functions (IMFs)\n\nOrthogonal to each other, stationary, and non-linear\n\nThink the recombination method is simply to sum the forecasts\n\nMisc\n\n{vmdTDNN}\n\nuseR video (1st talk) (paper is paywalled)\n\n\nThe number of modes you choose is very important\nMethods for choosing the number of modes\n\nCentral Frequency Method (CFM)\nSignal Difference Average (SDA)"
  },
  {
    "objectID": "qmd/forecasting-nonlinear.html#sec-fcast-tknemb",
    "href": "qmd/forecasting-nonlinear.html#sec-fcast-tknemb",
    "title": "Nonlinear",
    "section": "Takenâ€™s Embedding",
    "text": "Takenâ€™s Embedding\n\nA Dynamic Systems Model that transforms the time series into space where the dimensions are determined by multiples of lags of the time series. This transformation removes the autocorrelation between the data points and allows it to be forecasted.\n\nKind of like a SVM\n\nMisc\n\n{nonlinearTseries}\n\nParameters\n\nd or Ï„ - Called the time delay, this will tell us how many time lags each axis of the phase space will represent\n# tau (time delay) estimation based on the average mutual information function\ntau.ami = timeLag(ts, technique = \"ami\",Â \nÂ  Â  Â  Â  Â  Â  Â  Â  Â  lag.max = 100, do.plot = T)\nm - Called the embedding dimension, this parameter will tell us the dimension of the phase space\n# m (embedding dimension) uses the tau estimation\nemb.dim = estimateEmbeddingDim(ts, time.lag = tau.ami,\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  max.embedding.dim = 15)\n\nEstimated using Caoâ€™s algorithm\n\n\nPhase Space Embedding Matrix\n\n\nWhere f(t) is the univariate time series\n\nBuild the Takenâ€™s model\ntak = buildTakens(ts,embedding.dim = emb.dim, time.lag = tau.ami)"
  },
  {
    "objectID": "qmd/forecasting-statistical.html",
    "href": "qmd/forecasting-statistical.html",
    "title": "36Â  Forecasting, Statistical",
    "section": "",
    "text": "TOC\n\nMisc\nTerms\nProcessing\nDiagnostics\nAlgorithms\n\nRegression (including ARIMA)\nRandom Walk\nProphet\nKalman Filter\nExponental Smoothing\nTBATS\n\nInterval Forecasting\n\nMisc\n\nFor intermittent data, see Logistics &gt;&gt; Demand Forecasting &gt;&gt; Intermittent Data\nLet the context of the decision making process determine the units of the forecast\n\ni.e.Â donâ€™t forecast on a hourly scale just because you can.\n\nWhat can be forecast depends on the predictability of the event:\n\nhow well we understand the factors that contribute to it;\n\nWe have a good idea of the contributing factors: electricity demand is driven largely by temperatures, with smaller effects for calendar variation such as holidays, and economic conditions.\n\nhow much data is available;\n\nThere is usually several years of data on electricity demand available, and many decades of data on weather conditions.\n\nhow similar the future is to the past;\n\nFor short-term forecasting (up to a few weeks), it is safe to assume that demand behaviour will be similar to what has been seen in the past.\n\nwhether the forecasts can affect the thing we are trying to forecast.\n\nFor most residential users, the price of electricity is not dependent on demand, and so the demand forecasts have little or no effect on consumer behaviour.\n\n\nStarting a project\n\nUnderstand the dgp through eda and talking to domain experts\n\nhow are sales generated? (e.g.Â online, brick and mortar,â€¦)\n\nWhat is the client currently using to forecast?\n\nmodel that you need to beat\nwhere does it fail?\n\nbiased? underfitting or overfitting somewhere\nmissing seasonality?\n\n\nWhat is the loss function?\n\ncarrying this many items in inventory results in this cost\nif weâ€™re out of stock and lose this many sales, how much does this cost\n\nWhat does the client really want?\n\nhow is success measured\n\n\nfable models produce different results with NAs in the time series\n\nin rolling cfr project, steinmetzâ€™s manually calcâ€™d rolling 7-day means and his lagged vars had NAs, models using data with and without NAs had different score\n\nit is helpful to keep track of and understand what our forecast bias has historically been.Â  Even where we are fortunate enough to show a history of bias in both directions.\nForecasting shocks is difficult for an algorithm\n\nIt can better to smooth out (expected) shocks (Christmas) in the training data and then add an adjustment to the predictions during the dates of the shocks.\nThe smoothed out data will help the algorithm produce more accurate predictions for days when there isnâ€™t an expected shock.\nExamples of shocks that may need training data to have manual adjustments and not be smoothed by an algorithm\n\none-time spikes due to abnormal weather conditions\none-off promotions\na sustained marketing campaign that is indistinguishable from organic growth.\n\n\nIntermittent(or sporadic) time series (lotsa zeros).\n\n{thief} has the latest methods while {tsintermittent} has older methods\n\nBenchmark models\n\nnaive\n28-day moving average (i.e.Â 4 week MA)\n\n\nTerms\n\nWeak stationarity (commonly referred to as just stationarity)(aka covariance stationary): implies that the mean and the variance of the time series are finite and do not change with time.\nCointegration: xt and yt are cointegrated if xt and yt are I(1) series and there exists a Î² such that zt = xt - Î²yt is an I(0) series (i.e.Â stationary).\n\nimportant for understanding stochastic or deterministic trends.\nthe differences in the means of the set of cointegrated series remain constant over time, without offering an indication of directionality\nmight have low correlation, and highly correlated series might not be cointegrated at all.\nCan use Error Correction Model (ECM) with differenced data and inserting a error correction term (residuals from a OLS regression)\n\nStochastic - no value of a variable is known with certainty. Some values may be more likely than others (probabilistic). Variable gets mapped onto a distribution.\n\nProcessing\n\nFilling in gaps\n\nBi-directional forecast method from AutoML for time series: advanced approaches with FEDOT framework\n\nSteps\n\nSmooth series prior to the gap\n\nThey used a â€œGaussian filter w/sigma = 2â€ (not sure what that is)\n\nCreate lagged features of the smoothed series\nForecast using ridge regression where h = length of gap\nRepeat in the opposite direction using the series after the gap\nUse the average of the forecasts to fill the gap in the series.\n\n\n\nLog before differencing (SO post)\nDetrending versus differencing to get a stationary series\n\n(The goal is to get a stationary series, so if one doesnâ€™t work try the other.)\nDifferencing (for unit root processes)(stochastic trend)\n\nif the process requires differencing to be made stationary, then it is called difference stationary and possesses one or more unit roots.\n\nSometimes see charts of roots and a unit circle. I read this in an article about VAR models â€œprocess is stationary if all the roots z1, . . . , zn of the determinant det(Ïˆ(z)), or det(I âˆ’ Bz) = 0, lie outside of the unit circle.â€\n\nOne advantage of differencing over detrending to remove trend is that no parameters are estimated in the differencing operation.\nOne disadvantage, however, is that differencing does not yield an estimate of the stationary process\nIf the goal is to coerce the data to stationarity, then differencing may be more appropriate.\nDifferencing is also a viable tool if the trend is fixed\n\nrandom walking looking series should be differenced and not detrended.\n\nBackshift operator notation:\n\nin general:Â  âˆ‡d = (1 âˆ’ B)d\n\nwhere d is the order of differencing\nFractional differencing is when 0 &lt; d &lt; 1\n\nwhen 0 &lt; d &lt; 0.5, the series is classified as a long term memory series (often used for environmental time series arising in hydrology)\n\nIf d is negative, then its called forward-shift differencing\n\nExamples\n\nB ytÂ  =Â  yt-1;Â  B2 ytÂ  =Â  yt-2\nSeasonal Difference: (1 - B)(1 - Bm) yt = (1 - B - Bm + Bm + 1)yt = yt - yt-1 - yt-m + yt-m-1\nARIMA : AR(p)(I) = MA(q)\n\n\n\nARIMA(1,1,1)(1,1,1)4 for quarterly data (m = 4)\n\n\n\n\n\n\nDetrending (for trend-stationary processes)(deterministic trend)\n\nIt is possible for a time series to be non-stationary, yet have no unit root and be trend-stationary\n\na trend-stationary process is a stochastic process from which an underlying trend (function solely of time) can be removed (detrended), leaving a stationary process.\n\nIf an estimate of the stationary process is essential, then detrending may be more appropriate.\nHow is this back-transformed after forecasting?\n\nmaybe look at â€œforecasting with STLâ€ section in fpp2\n\n\nIn both unit root and trend-stationary processes, the mean can be growing or decreasing over time; however, in the presence of a shock, trend-stationary processes are mean-reverting (i.e.Â transitory, the time series will converge again towards the growing mean, which was not affected by the shock) while unit-root processes have a permanent impact on the mean (i.e.Â no convergence over time).\nTesting\n\nKPSS test: H0 = trend-stationary, Ha =Â  unit root.\n\nurca::ur_kpss the H0 is stationarity\ntseries::kpss.test(res, null = â€œTrendâ€) where H0 is â€œtrend-stationarityâ€\n\nDickey-Fuller tests: H0 = unit root, Ha = stationary or trend-stationary depending on version\nKPSS-type tests are intended to complement unit root tests, such as the Dickeyâ€“Fuller tests. By testing both the unit root hypothesis and the stationarity hypothesis, one can distinguish series that appear to be stationary, series that appear to have a unit root, and series for which the data (or the tests) are not sufficiently informative to be sure whether they are stationary.\n\nSteps:\n\nADF:\n\nIf H0 rejected. The trend (if any) can be represented by a deterministic linear trend.\nIf H0 is not rejected then we apply the KPSS test.\n\nKPSS :\n\nIf H0 rejected then we conclude that there is a unit root and work with the first differences of the data.\n\nUpon the first differences of the series we can test the significance of other regressors or choose an ARMA model.\n\nIf H0 is not rejected then data doesnâ€™t contain enough information. In this case it may be safer to work with the first differences of the series.\n\n\nSteps when using an ARIMA:\n\nSuppose the series is not trending\n\nIf the ADF test (without trend) rejects, then apply model directly\nIf the ADF test (without trend) does not reject, then model after taking difference (maybe several times)\n\nSuppose the series is trending\n\nIf the ADF test (with trend) rejects, then apply model after detrending the series\nIf the ADF test (with trend) does not reject, then apply model after taking difference (maybe several times)\n\n\n\n\nDiagnostics\n\nTesting for significant difference between model forecasts\n\nNemenyi test\n\ntsutils::nemenyi( )\n\nMCB\n\ngreybox::rmcb( )\n\n\n\nAlgorithms\n\nMisc\n\nauto.arima, ets and theta are general-purpose methods particularly well-suited for monthly, quarterly and annual data\nTBATS and STL will also handle multiple seasonalities such as arise in daily and weekly data.\nWhen to try nonlinear models (see Forecasting, Nonlinear)\n\nLinear prediction methods (e.g.Â ARIMA) donâ€™t produce adequate predictions\nChaotic nature of the time series is obvious (e.g.Â frequent, unexplainable shocks that canâ€™t be explained by noise)\n\n\nRegression (including ARIMA)\n\nMisc\n\nDouble check auto_arima, for the parameters, (p, d, q), one should pick q to be at least p (link)\nSometimes the error terms are called â€œrandom shocks.â€\nIf using lm( ) and there are NAs, make sure to use na.action = NULL else they get removed and therefore dates between variables wonâ€™t match-up. See lm doc for further details on best practices.\nARIMA models make h-step out predictions by iterating 1-step forward predictions and feeding the intermediate predictions in as if they were actual observations (0 are used for the errors)\nPolynomial Autoregression (AR) models exponentiate the lags. So the design matrix includes the lags and the exponentiated series.\n\nIf the polynomial is order 3, then order 2 is also included. So, now, the design matrix would be the lags, the square of each lag, and the cube of each lag\n\n\n\n\nlibrary(dplyr); library(timetk)\n# tbl w/polynomial design matrix of order 3\n# value is the ts values\npoly_tbl &lt;- group_tbl %&gt;%Â \nÂ  tk_augment_lags(.value = value, .lags = 1:4) %&gt;%Â \nÂ  mutate(across(contains(\"lag\"), .fns = list(~.x^2, ~.x^3), .names = \"{.col}_{ifelse(.fn==1, 'quad','cube'){style='color: #990000'}[}]{style='color: #990000'}\"))\n\n.fnÂ  is the item number in the .fns list.\nsquared lag 2 will have the name â€œvalue_lag2_quadâ€\nTypes\n\nAR: single variable with autoregressive dependent variable terms\nARMA: same as AR but errors models as a moving average\nARIMA: same as ARMA but with differencing the timeseries\nSARIMA: same as ARIMA but also with seasonal P, D, Q terms\nARMAX: same as ARMA but with additional exogenous predictors\nDynamic Regression: OLS regression with modeled (usually arima) errors\n\nOLS vs ARIMA\n\nJohn Mount\n\nThe fear in using standard regression for time series problems is that the error terms are likely correlated.\n\nSo one can no longer appeal to the Gauss Markov Theorem (i.e.Â OLS is BLUE) to be assured of good out of sample performance (link)\n\n\nryer and chan regarding Dynamic Regression vs OLS\n\nâ€œregression (with arima errors) coefficient estimate on Price is similar to that from the OLS regression fit earlier, but the standard error of the estimate is about 10% lower than that from the simple OLS regression. This illustrates the general result that the simple OLS estimator is consistent but the associated standard error is generally not trustworthyâ€\n\nHyndman\n\nâ€œThe forecasts from a regression model with autocorrelated errors are still unbiased, and so are not â€œwrong,â€ but they will usually have larger prediction intervals than they need to. Therefore we should always look at an ACF plot of the residuals.â€\nThe estimated coefficientsÂ  are no longer the best estimates, as some information has been ignored in the calculation;\n\nmeaning modeling the errors to take into account the autocorrelation\n\nAny statistical tests associated with the model (e.g., t-tests on the coefficients) will be incorrect.\n\naffected by the bloated std errors\n\nThe AICc values of the fitted models are no longer a good guide as to which is the best model for forecasting.\nIn most cases, the p-values associated with the coefficients will be too small, and so some predictor variables will appear to be important when they are not. This is known as â€œspurious regression.â€\n\n\ntrend (Î²1 t) is modeled by setting the variableÂ  t to just an index variable (i.e.Â t = 1, â€¦., T). Modeling quadratic trend would be adding in t2 to the model formula.\n\nHyndman suggests that using splines is a better approach than using t2\nFrom Steinmitzâ€™s CFR article\n\nInstead of trend() (like in forecast::), heâ€™s using poly(date, 2) to include a quadratic trend\n\n\nR2 and adj R2\n\nAppropriate for time series (i.e.Â estimate of the population R2), as long as the data are stationary and weakly dependent\n\ni.e.Â the variances of both the errors and the dependent variable do not change over time.\ni.e.Â if yt has a unit root (integrated of order 1, I(1)) (need differenced)\n\n\nInterpretation of coefficients\n\n\n\nIfÂ  xÂ  increases by one unit today, the change inÂ  yÂ  will beÂ  Î²0+Î²1+â€¦+Î²sÂ  afterÂ  sÂ  periods; this quantity is called theÂ  s -period interim multiplier. The total multiplier is equal to the sum of allÂ  Î² s in the model.\n\n\nResiduals\n\ntypes\n\nâ€œregressionâ€ is for the main model\n\noriginal data minus the effect of the regression variables\n\nâ€œinnovationâ€ is for the error model\n\ndefault arg\nHyndman uses these for dynamic regression residual tests\n\n\nautocorrelation tests\n\nfailing the test does not necessarily mean that (a) the model produces poor forecasts; or (b) that the prediction intervals are inaccurate. It suggests that there is a little more information in the data than is captured in the model. But it might not matter much.\nBreusch-Godfrey test designed for pure regression or straight AR model\n\ndoes handle models with lagged dependent vars as predictors\nLM (lagrange multiplier) test\nforecast::checkresiduals can calc it and display it but you donâ€™t have access to the values programmatically\n\ndefaults for max lag is min(10,n/5) for nonseasonal and min(2m, n/5) for seasonal; freq is seasonality, m\n\n\n\n\n\nlag &lt;- ifelse(freq &gt; 1, 2 * freq, 10)\nlag &lt;- min(lag, round(length(residuals)/5))\nlag &lt;- max(df+3, lag)\n\nlmtest and DescTools (active) packages have the function that forecast uses but only takes lm objects\nDurbin-Watson designed for pure regression\n\nerror term canâ€™t be correlated with predictor to use this test\n\nso no lagged dependent variables can be used as predictors\nthere is an durbin alternate test mentioned in stata literature that can do lagged variables but I havenâ€™t seen a R version that specifies thatâ€™s the version it is.\n\nlmtest or DescTools takes a lm object and has a small sample size correction available\ncar::durbinWatsonTest takes a lm object or residual vector.\n\nOnly lm obj returns pval. Resid vector returns dw statistic\n\npvals &lt; 0.05 â€“&gt; autocorrelation present\ndw statistic guide (0 &lt; dw &lt; 4)\n\naround 2 â€“&gt; no autocorrelation\nsignifcantly &lt; 2 â€“&gt; positive correlation\n\nsaw values &lt; 1 have pvals = 0\n\nsignificantly &gt; 2 â€“&gt; negative correlation\n\n\nLjung-Box\n\nFor dyn regression, arima, ets, etc.\n\nThereâ€™s a SO post that shows this shouldnâ€™t be used for straight regression\n\nhttps://stats.stackexchange.com/questions/148004/testing-for-autocorrelation-ljung-box-versus-breusch-godfrey\nFor straight AR models, the comments show it should be fine as long as lags arg &gt; model df arg (see below)\n\n\ntest is whether a group of lagged residuals has significant autocorrelation, so an acf of the residuals might show individual spikes but the group as a whole may not have significant autocorrelation\n\nif you see a spike in the residuals, may be interesting to include that lag number in the group of lags and see if significance of the group changes\n\nfeasts::ljung_box\n\nrequires numeric residuals vector, model degrees of freedom, number of lags to check\n\nmodel df = number of variables used in the regression + intercept + p + q (of arima error model)\n\ne.g.Â model with predictors: trend + cases and an error model: arima (2,1,1) had df = 2 (predictors: trend, cases) + 1 (intercept) + 2 (p) + 1 (q) = 6 d.f.\ndof &lt;â€“ length(fit$coef)\n\nSee Breusch-Godfrey section for number of lags to use\n\n\npvals &lt; 0.05 â€“&gt; autocorrelation present\n\nSpectral analysis takes the approach of specifying a time series as a function of trigonometric components (i.e.Â regression with fourier terms)\n\nA smoothed version of the periodogram, called a spectral density, can also be constructed and is generally preferred to the periodogram.\n\nRandom Walk\n\nA process integrated to order 1, (an I(1) process) is one where its rate of change is stationary. Brownian motion is a canonical I(1) process because its rate of change is Gaussian white noise, which is stationary. But the random walk itself is not stationary. So the t+1 value of a random walk is just the value at t plus a number sampled from some bell curve.\nCharacteristics\n\nlong periods of apparent trends up or down\nsudden and unpredictable changes in direction\n\nExamples with and without drift\n\n\n\n\n\n\nProphet\n\nThe basic methodology is an iterative curve-matching routine, where Prophet will then train your data on a bigger period, then predict again and this will repeat until the end point is reached. The development team of Prophet claim that its strengths are:\n\nWorking with high-frequency data (hourly, daily, or weekly) with multi-seasonality, such as hour of day, day of week and time of year;\nspecial events and bank holidays that are not fixed in the year;\nallowing for the presence of a reasonable number of missing values or large outliers;\naccounting for changes in the historical trends and non-linear growth curves in a dataset.\n\nFurther advantages include the ability to train from a moderate sized dataset, without the need for specialist commercial software, and fast start up times for development.\nDisadvantages\n\nno autoregressive (i.e.Â lags of target series)Â  features since itâ€™s a curve-fitting algorithm Time series decomposition by prophet:\ng(t): Logistic or linear growth trend with optional linear splines (linear in the exponent for the logistic growth). The library calls the knots â€œchange points.â€\ns(t): Sine and cosine (i.e.Â Fourier series) for seasonal terms.\nh(t): Gaussian functions (bell curves) for holiday effects (instead of dummies, to make the effect smoother).\n\n\nKalman Filter Notes from How a Kalman filter works, in pictures\n\nIf a dynamic system is linear and with Gaussian noise (inaccurate measurements, etc.), the optimal estimator of the hidden states is the Kalman Filter\n\nFor nonlinear systems, we use the extended Kalman filter, which works by simply linearizing the predictions and measurements about their mean. (I may do a second write-up on the EKF in the future)\nGood for predictions where the measurements of the outcome variable over time can be noisy\n\nAssumptions\n\nGaussian noise\nMarkov property\n\nif you know xtâˆ’1, then knowledge of xtâˆ’2, . . . , x0 doesnâ€™t give any more information about xt (i.e.Â not much autocorrelation if at all)\n\n\ntl;dr\n\nA predicted value from a physically-determined autoregression-type equation with 1 lag that gets adjusted for measurement error\n\nAdvantages\n\nlight on memory (they donâ€™t need to keep any history other than the previous state)\nvery fast, making them well suited for real time problems and embedded systems\n\nUse cases\n\nengineering: common for reducing noise from sensor signals (i.e.Â smoothing out measurements)\ndetection-based object tracking (computer vision)\n\nFirst set of equations Notes This set of equations deals physical part of the system. Itâ€™s kinda how we typically forecast. The x^k equation is pretty much like a typical auto-regression plus explanatory variables except for the F matrix which may require knowledge of system dynamics\n  * Wiki shows a term, wk,Â  added to the end of the x^k equation. wk is the process noise and is assumed to be drawn from a zero mean multivariate normal distribution,\n  The **new best estimate** is a **prediction** made from **previous best estimate**, plus a **correction** for **known external influences**.\n      x^k is the step-ahead predicted \"state\"; x^k-1 is the current \"state\"\n\n      uk (\"control\" vector) is an explanatory variable(s)\n\n      Fk (\"prediction\" matrix) and Bk (\"control\" matrix) are transformation matrices\n          Fk was based on one of Galileo's equations of motion in the example so this might be very context specific\n              Might need to based on substantial knowledge of the system to create a system of linear equations (i.e. Fk matrix) that can be used to model the it.\n\n  And the **new uncertainty** is **predicted** from the **old uncertainty**, with some **additional uncertainty from the environment**.\n      Pk and Pk-1 are variance/covariance matrices for the step-ahead predicted state and current state respectively\n\n      Qk is the uncertainty term for the variance/covariance matrix of the predicted state distribution\nSecond Set of Equations Notes These equations refine the prediction of the first set of equations by taking into account various sources of measurement error in the observed outcome variable\n      The equations do this by finding the intersection, which is itself a distribution, of the transformed prediction distribution, Î¼0, and the measurement distribution, Î¼1.\n\n      ![](./_resources/Forecasting,_Statistical.resources/image.1.png)\n      * This mean, Î¼', of this intersection distribution is the predicted value that most likely to be the true value\n  Hk is a transformation matrix that maps the predicted state (result of the first set of equations), x^k , to the measurement space\n      where Hk \\* x^k is the expected measurement (pink area) (i.e. mean of the distribution of transformed prediction)\n\n  zâ†’k is the mean of the measurement distribution (green area)\n\n  x^'k is the intersection of the transformed prediction distribution and the measurement distribution\n      i.e. the predicted state thats most likely to true\n\n  Rk is the uncertainty term for variance/covariance matrix for the measurement distribution\n\n  K' is called the Kalman Gain\n      Didn't read anything interpretative about the value. Just seems to a mathematical construct that's part of the derivation.\n\n      In the derivation, it starts out as the ratio of the measurement covariance matrix to the sum of the measurement variance covariance matrix and the transformed prediction variance covariance matrix\n\n      ![](./_resources/Forecasting,_Statistical.resources/image.png)\nProcess \nHyperparameters\n\nQ is the process noise covariance\n\nControls how sensitive the model will be to process noise.\n\nR is the measurement noise variance\n\nControls how quickly the model adapts to changes in the hidden state.\n\nGuessing â€œstdâ€ is the default value?\n\n\n\n\nExponential Smoothing\n\nThe general idea is that future values are a weighted average of past values, with the weights decaying exponentially as we go back in time\nMethods\n\nsimple exponential smoothing\ndouble exponential smoothing or Holtâ€™s method (for time series with a trend)\ntriple exponential smoothing or Holt-Winterâ€™s method (for time series with a trend and sesaonality)\n\n\nTBATS\n\nTrigonometric seasonality, Box-Cox transformation, ARMA errors, Trend, and Seasonal components\nCan treat non-linear data, solve the autocorrelation problem in residuals since it uses an ARMA model, and it can take into account multiple seasonal periods\nRepresents each seasonal period as a trigonometric representation based on Fourier series. This allows the model to fit large seasonal periods and non-integer seasonal periods\n\n\nInterval Forecasting\n\nInterval data is commonly analyzed by modeling the range (difference between interval points)\n\nRange data doesnâ€™t provide information about the variation of the mean (aka level) over time.\nRange only provides information about the boundaries, where interval analysis provides information about the boundary and the interior of the interval.\n\nProvides more information than point forecasts.\nData examples:\n\ndaily temperature, stock prices: Each day a high and low values are recorded\nstock price volatility, bid-ask spread use hi-lo value differences\nIntra-house inequality: difference between wife and husband earnings\nurban-rural income gap\ninterval-valued output growth rate: China reports itâ€™s targeted growth rate as a range now.\ndiastolic and systolic blood pressure\n\nothers: blood lipid, white blood cell count, hemoglobin\n\n\nExamples where (generalized) intervals can be modeled instead of differences:\n\nStock volatility\n\nGARCH models often used to model volitility but Conditional Autoregressive Range (CARR) gives better forecasts\n\nBecause GARCH model is only based on the closing price but the CARR model uses the range (difference).\n\nDynamic Interval Modeling\n\nUse Autoregressive Interval (ARI) model to estimate the parameters using an interval time series (not the range)\nThen take the forecasted left and right values of the interval to forecast the volatility range in a CARR model\nThe extra information of the interval data over time (instead of a daily range) yields a more efficient estimation of the parameters\n\n\nCapital Asset Pricing Model (CAPM)\n\nStandard Equation\n\nRt - Rft = Î± + Î²(Rmt - Rft) + Îµt\n\nRt: return of certain portfolio\nRft: risk-free interest rate\nRmt: return of market portfolio\nRt - Rft: asset risk premium\n\n\nInterval-based version\n\nYt = (Î±0 + Î²0I0) + Î²Xt + ut\n\nI0 = [-0.5, 0.5]\nYt = [Rft, Rt]\nXt = [Rft, Rmt]\n\nThe Rt - Rft can then be calculated by taking the difference of the interval bounds of the interval-based predictions\n\n\n\nModel the center of the interval and the range in a bi-variate VAR model (doesnâ€™t use all points in the interval data)\n\nBi-variate Nonlinear Autoregressive Model for center and range\n\nHas an indicator variable that captures nonlinearity of interval data\n\nSpace-time autoregressive model\n\nAutoregressive Conditional Interval model\n\nThe interval version of an ARMA model\n\ndepends on lags and lagged residuals\n\nACI(p,q):\n\nÎ±0, Î²0, Î²j, Î³j are unknown scalar parameters\nI0 = [-1/2, 1/2] is a unit interval\nÎ±0 + Î²0I0 = [Î±0 - Î²0/2, Î±0 + Î²0/2] is a constant interval intercept\nut is the interval residuals that satisfies E(ut|It-1) = [0,0]\nYt is a random interval variable\n\nObjective function that gets minimized is called Dk distance\n\nD2K [ut(Î¸),0]\n\nut(Î¸) is the interval residuals\nK refers to some kind of kernel function\n\nitâ€™s a wacky quadratic with constants a,b,c\nmeasures the distance between all pairs of points\n\n\nThe minimization is a two-stage process\n\nFinds the optimal kernel, K, then uses it to minimize the residuals to estimate the parameters\n\n\n\nThreshold Autoregressive Interval (TARI)\n\nnonlinear ACI model and interval version of TAR(p) model (Â¯\\_(ãƒ„)_/Â¯)\n2-procedure model\n\nbasically 2 autoregressive equations with an i1ut or i2ut added on to the end.\nThe interval series, Yt ,follows one of the equations based on threshold variable qt is less than or equal to a threshold parameter, Î³ or great than.\n\nEstimation is similar to ACI model\nFor more details, need to research what a TAR model (Terasvirta, Tjostheim, and Granger 2010) is"
  },
  {
    "objectID": "qmd/generalized-additive-models-(gam).html#sec-gam-misc",
    "href": "qmd/generalized-additive-models-(gam).html#sec-gam-misc",
    "title": "Generalized Additive Models",
    "section": "Misc",
    "text": "Misc\n\nAlso see Feature Engineering, Splines\nLarge gaps in the values of the predictor variable can be a problem if you are trying to interpolate between those gaps. (See bkmks, method = \"reml\" + s(x, m = 1))"
  },
  {
    "objectID": "qmd/generalized-additive-models-(gam).html#sec-gam-diag",
    "href": "qmd/generalized-additive-models-(gam).html#sec-gam-diag",
    "title": "Generalized Additive Models",
    "section": "Diagnostics",
    "text": "Diagnostics\n\nâ€œDeviance explainedâ€ is the R2 value for GAMs\nmgcv::gam.check(gam_fit) \n## Method: GCVÂ  Optimizer: magic\n## Smoothing parameter selection converged after 19 iterations.\n## The RMS GCV score gradient at convergence was 5.938335e-08 .\n## The Hessian was positive definite.\n## Model rank =Â  21 / 22Â \n## Basis dimension (k) checking results. Low p-value (k-index&lt;1) may\n## indicate that k is too low, especially if edf is close to k'.\n## Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  k'Â  edf  k-index p-valueÂ  Â \n## s(id)Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â   1.00Â  0.35Â  Â  0.82Â  &lt;2e-16 ***\n## s(log_profit_rug_business_b)Â  Â  Â  Â  Â  Â  9.00Â  8.52Â  Â  1.01Â  Â  0.69Â  Â \n## s(log_profit_rug_business_b):treatment 10.00Â  1.50Â  Â  1.01Â  Â  0.62Â  Â \n## ---\n## Signif. codes:Â  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1\n\nCheck if the size of the basis expansion (k) for each smooth is sufficiently large\n\nk.check can also do this\nIf all your smoothing predictors are not sufficiently large, then this indicates that using a GAM is a bad fit for your data.\nSee SO post from Simpson\n\n\nFormal test for the necessity of a smooth\nm &lt;- \n  gam(y ~ x + s(x, m = c(2, 0), bs = \"tp\"),\n      data = foo,\n      method = \"REML\",\n      family = binomial())\n\nbs = \"tp\" is just the default thin plate basis function\nFit the predictor of interest as a linear term (x) plus a smooth function of x\nModify the basis for the smooth so that it no longer includes linear functions in the span of the basis with m = c(2, 0)\n\nIndicates we want the usual second order derivative penalty but with a 0 size null space (the span of functions that arenâ€™t affected by the penalty because they have 0 second derivative).\n\nsummary will give a test for the necessity of the wiggliness provided by the smooth over the linear effect estimated by the linear term.\n\nFrom Simpson SO post\nAlso see Woodâ€™s â€œGeneralized Additive Models: An Introduction with Râ€, 2nd Ed, section 6.12.3, â€œTesting a parametric term against a smooth alternativeâ€ p 312-313 (R &gt;&gt; Documents &gt;&gt; Regression &gt;&gt; gam)"
  },
  {
    "objectID": "qmd/geospatial-analysis.html#sec-geo-anal-misc",
    "href": "qmd/geospatial-analysis.html#sec-geo-anal-misc",
    "title": "Analysis",
    "section": "Misc",
    "text": "Misc\n\nAlso see\n\nDomain Knowledge &gt;&gt; Epidemiology &gt;&gt; Disease Mapping"
  },
  {
    "objectID": "qmd/geospatial-analysis.html#sec-geo-anal-terms",
    "href": "qmd/geospatial-analysis.html#sec-geo-anal-terms",
    "title": "Analysis",
    "section": "Terms",
    "text": "Terms\n\nBuffer - a zone around a geographic feature containing locations that are within a specified distance of that feature, the buffer zone. A buffer is likely the most commonly used tool within the proximity analysis methods. Buffers are usually used to delineate protected zones around features or to show areas of influence.\nCatchment - The area inside any given polygon is closer to that polygonâ€™s point than any other. Refers to the area of influence from which a retail location, such as a shopping center, or service, such as a hospital, is likely to draw its customers. (also see Retail &gt;&gt; Catchment)"
  },
  {
    "objectID": "qmd/geospatial-analysis.html#sec-geo-anal-proxanal",
    "href": "qmd/geospatial-analysis.html#sec-geo-anal-proxanal",
    "title": "Analysis",
    "section": "Proximity Analysis",
    "text": "Proximity Analysis\n\nExample: Basic Workflow\n\nData: Labels, Latitude, and Longitude\n\nCreate Simple Features (sf) Object\ncustomer_sf &lt;- \n  customer_table %&gt;%\n    sf::st_as_sf(coords = c(\"longitude\", \"latitude\"),\n                 crs = 4326)\n\nMerges the longitude and latitude columns into a geometry column and transforms the coordinates in that column according to projection (e.g.Â crs = 4326)\n\nView points on a map\n\nmapview::mapview(customer_sf)\nCreate Buffer Zones\n\ncustomer_buffers &lt;- \n  customer_sf %&gt;%\n    sf::st_transform(26914) %&gt;%\n    sf::st_buffer(5000)\n\nmapview::mapview(customer_buffers)\n\nMost of projections use meters, and based on the size of the circles as related to the size of Denton, TX, Iâ€™m guessing the radius of each circle is 5000m. Although, that still looks a little small.\n\nCreate Isochrones\n\ncustomer_drivetimes &lt;- \n  customer_sf %&gt;%\n    mapboxapi::mb_isochrone(time = 10, \n                            profile = \"driving\", \n                            id_column = \"name\")\n\nmapview::mapview(customer_drivetimes)\n\n10 minutes drive-time from each location\ntime (minutes): The maximum time supported is 60 minutes. Reflects traffic conditions for the date and time at which the function is called.\n\nIf reproducibility of isochrones is required, supply an argument to the depart_at argument.\n\n\nAdd Demographic Data\n\ndenton_income &lt;- \n  tidycensus::get_acs(\n    geography = \"tract\",\n    variables = \"B19013_001\",\n    state = \"TX\",\n    county = \"Denton\",\n    geometry = TRUE\n  ) %&gt;%\n    select(tract_income = estimate) %&gt;%\n    sf::st_transform(st_crs(customer_sf))\n\ncustomers_with_income &lt;- customer_sf %&gt;%\n  sf::st_join(denton_income)\n\ncustomers_with_income\n\nAdds median income estimate according to the census tract each person lives in.\nJoins on the geometry variable\n\n\nCircular Buffer Approach\n\nNotes from GIS-based Approaches to Catchment Area Analyses of Mass Transit\nThe simplest and most common used approach to make catchment areas of a location is to consider the Euclidean distance from the location.\nDue to limitations (See below), itâ€™s best suited for overall analyses of catchment areas.\nOften the level of detail in the method has been increased by dividing the catchment area into different rings depending on the distance to the station.\n\nExample: By applying weights for each ring it is possible to take into account that the expected share of potential travelers at a train station will drop when the distance to the stop is increased.\n\n\n\nLimitation: Does not take the geographical surroundings into account.\n\nExample: In most cases, the actual walking distance to/from a location is longer than the Euclidean distance since there are natural barriers like rivers, buildings, rail tracks etc.\n\nThis limitation is often coped with by applying a detour factor that reduces the buffer distance to compensate for the longer walking distance.\nHowever, in cases where the length of the detours varies considerably within the locationâ€™s surroundings, this solution is not very precise.\nFurthermore, areas that are separated completely from a location, e.g.Â by rivers, might still be considered as part of the locationâ€™s catchment area\n\n\nUse Case: Ascertain Travel Potential to Determine Potential Station Locations\n\nEvery 50m along the proposed transit line, calculate the travel potential for that buffer area\n\nUsing the travel demand data for that buffer area, calculate travel potential\n\nTravel Potential Graph\n\n\nLeft side represents the transit line.\nRight Side\n\nY-Axis are locations where buffer areas were created.\nX-Axis: Travel Potential\n\nNot sure if that is just smoothed line with a point estimate of Travel Potential at each location or how exactly those values are calculated.\n\n50m isnâ€™t a large distance so maybe all the locations arenâ€™t shown on the Y-Axis and the number of calculations produces an already, mostly, smooth line on itâ€™s own.\n\nPartitioning a buffer zone into rings or some kind of interpolation could provided more granular estimates around the central buffer location."
  },
  {
    "objectID": "qmd/geospatial-general.html#misc",
    "href": "qmd/geospatial-general.html#misc",
    "title": "37Â  General",
    "section": "37.1 Misc",
    "text": "37.1 Misc\n\nQGIS - free and open source\nArcGIS - expensive and industry-standard\nspatiotemporal data â€” data cubes with spatial and regular temporal dimensions â€” such as\n\ne.g.Â gridded temperature values (raster time series) and vector data with temporal records at regular temporal instances (e.g.Â election results in states).\n\n{stars} - regular intervals\n{sftime} - irregular intervals\n\n\nSpatial Resampling\n\nCreates cross-validation folds by k-means clustering coordinate variables\n\n\nlibrary(tidymodels)\nlibrary(spatialsample)\nset.seed(123)\nspatial_splits &lt;- spatial_clustering_cv(landslides, coords = c(\"x\", \"y\"), v = 5)\n\n# fit a logistic model\nglm_spec &lt;- logistic_reg()\nlsl_form &lt;- lslpts ~ slope + cplan + cprof + elev + log10_careaÂ \nlsl_wf &lt;- workflow(lsl_form, glm_spec)\ndoParallel::registerDoParallel()Â \nregular_rs &lt;- fit_resamples(lsl_wf, bad_folds)"
  },
  {
    "objectID": "qmd/geospatial-general.html#terms",
    "href": "qmd/geospatial-general.html#terms",
    "title": "37Â  General",
    "section": "37.2 Terms",
    "text": "37.2 Terms\n\nCensus Block Groups - ~600â€“3,000 population; the smallest geography reported; Wiki\nCensus Tract - ~4,000 average population; Docs\n\nAlso see Survey, Census Data &gt;&gt; Geographies\n\nGraticules - a network of lines on a map that delineate the geographic coordinates (degrees of latitude and longitude.)\n\nUse of graticules is not advised, unless the graphical output will be used for measurement or navigation, or the direction of North is important for the interpretation of the content, or the content is intended to display distortions and artifacts created by projection. Unnecessary use of graticules only adds visual clutter but little relevant information. Use of coastlines, administrative boundaries or place names permits most viewers of the output to orient themselves better than a graticule\n{sf::st_graticule}\n\nVRT - File format that allows a virtual GDAL dataset to be composed from other GDAL datasets with repositioning, and algorithms potentially applied as well as various kinds of metadata altered or added. VRT descriptions of datasets can be saved in an XML format normally given the extension .vrt.\n\nBasically a metadata XML file describing various properties of the actual raster file, like pixel dimensions, geolocation, etc.."
  },
  {
    "objectID": "qmd/geospatial-general.html#optimization",
    "href": "qmd/geospatial-general.html#optimization",
    "title": "37Â  General",
    "section": "37.3 Optimization",
    "text": "37.3 Optimization\nMisc\n    \n    \nVector Tiles\n* Misc\n    * Notes from [Push the limits of interactive mapping in R with vector tiles](https://www.milesmcbain.com/posts/vector-tiles/)\n        * McBain goes through a complete example with plenty of tips on simplification strategies and hosting mbtiles files\n    Issues (solution: Vector Tiles)\n        Limited number of features with DOM canvas\n            Thereâ€™s a limit to how many features leaflet maps can handle, because at some point the DOM gets too full and your browser stops being able to parse it.\n            \n        Limited number of maps on same webpage\n            Once you start rendering spatial data on WebGL canvasses instead of the DOM youâ€™ll find there is a low number of WebGL contexts that can co-exist on any one web page, typically limiting you to only around 8 maps.\n            \n        File sizes blow up to hundreds of MB\n            Trying to reuse WebGL maps by toggling on and off different layers of data for the user at opportune times. This is an improvement, but data for all those layers piles up, and your toolchain wants to embed this in your page as reams of base64 encoded text. Page file sizes are completely blowing out.\n            \n* Use Cases\n    * Simplification of geometry is not desirable, e.g. because of alignment issues\n        * e.g. The zoomed-in road network has to align with the road network on the basemap, so that viewers can see features that lie along sections of road.\n    * Simplification of geometry doesnâ€™t really help, you still have too many features\n    * Cumulatively your datasets are too large to handle.\n* **Vector Tiles**Â  - contain arrays of annotated spatial coordinate data which is combined with a separately transmitted stylesheet to produce the tile image.\n    * i.e. The edges of the roads, the boundaries of buildings etc. Not an image, but the building blocks for one\n    * Different stylesheets can use the same vector data to produce radically different looking maps that either highlight or omit data with certain attributes\n    * Mapbox Vector Tiles (MVT) - [specification](https://docs.mapbox.com/data/tilesets/guides/vector-tiles-standards/); the de-facto standard for vector tile files\n        * stored as a Google protocol buffer - a tightly packed binary format.\n* **MBTiles** - by Mapbox; describe a method of storing an entire MVT tileset inside a single file.\n    * Internally .mbtiles files are SQLlite databases containing two tables: _metadata_ and _tiles_.\n        * tiles table\n            * indexed by z,x,y\n            * contains a tile\\_data column for the vector tile protocol buffers, which are compressed using gzip\n    * SQLite format and gzip compression help with efficient retrieval and transmission\n* Using vector tiles we can have unlimited reference layers. Each one contributes nothing to the report file size since it is only streamed on demand when required.\n* Workflow to convert data to .tbtiles\n    * In R, read source data as an sf, and wrangle\n        * Tippecanoe expects by epsg 4326 by default\n    * Write data out to geojson\n    * On the command line, convert geojson to .mbtiles using the tippecanoe command line utility.\n        * Tippecanoe sources\n            * Mapbox version - [repo](https://github.com/mapbox/tippecanoe)\n                * McBain says, he uses this version and hasn't had any problems\n                * README has helpful cookbook section\n            * Actively maintained community forked version - [repo](https://github.com/protomaps/tippecanoe)\n            * May be a headache to get dependencies if using Windows\n        * Alternatively it can output a folder structure full of protocol buffer files.\n    * Example\ntippecanoe -zg \\\nÂ  Â  Â  Â  Â  -o abs_mesh_blocks.mbtiles \\\nÂ  Â  Â  Â  Â  --coalesce-densest-as-needed \\\nÂ  Â  Â  Â  Â  --extend-zooms-if-still-dropping \\\nÂ  Â  Â  Â  Â  mb_shapes.geojson\n\nMapping\n\nExample\n\n\nlibrary(mvtview)\nlibrary(rdeck)\n\n# Fire up the server\nserve_mvt(\"abs_mesh_blocks.mbtiles\", port = 8765)\n# Serving your tile data from http://0.0.0.0:8765/abs_mesh_blocks.json.\n# Run clean_mvt() to remove all server sessions.\n\nmesh_blocks &lt;- jsonlite::fromJSON(\"http://0.0.0.0:8765/abs_mesh_blocks.json\")\n\n# Map the data\nrdeck(\nÂ  Â  initial_bounds = structure(meshblocks$bounds, crs = 4326, class = \"bbox\") # set map limits using the tilejson\n) |&gt;\nÂ  add_mvt_layer(\nÂ  Â  data = rdeck::tile_json(\"http://0.0.0.0:8765/abs_mesh_blocks.json\"),\nÂ  Â  get_fill_color = scale_color_linear(\nÂ  Â  Â  random_attribute\nÂ  Â  ),\nÂ  Â  opacity = 0.6\nÂ  )\n\nSee McBain article for options on hosting .mbtiles files\nRegarding â€œabs_mesh_blocksâ€: {mvtview} provides a way to fetch the metadata table from .mbtiles as json by querying a json file with the same name as the .mbitles file.\nThe structure of â€˜tilejsonâ€™ is yet another specification created by Mapbox, and is supported in deck.gl (and therefore {rdeck}) to describe tile endpoints."
  },
  {
    "objectID": "qmd/geospatial-general.html#grid-systems",
    "href": "qmd/geospatial-general.html#grid-systems",
    "title": "37Â  General",
    "section": "37.4 Grid Systems",
    "text": "37.4 Grid Systems\n\nMisc\n\nExplainer: Why using hexbins to visualize Australian electoral map is better than a typical provincial map.\n\ntl;dr: Geographical size distorts what the value is trying to measure. The value is the party that wins the parliamentary seat\n\nThe bar graph shows the values the map is trying to visualize geographically. The hexabins better represent the close race by removing the distorting element which is the geographical size of the provinces.\nEach voting district (hexabin) is voting for 1 representative and has the same number of voters, but districts can have vastly different areas depending on population density.\n\nKeep unit at constant size (like hexabins) but alter hex shape to keep state shape.\n\nA better U.S. house election results map?\nResults\n\nstate size depends on number of districts which depends on population and therefore correctly conveys voting results visually across the whole country\nDistricts get distorted but the states retain their shape and so distortion of the overall visualization is minimized\n\n\n\n\nUberâ€™s H3 grid system -\n\nMisc\n\npackages: h3r, h3-r\ndocs\nAdd census data to H3 hexagons, calculate overlaps (article)\nFor large areas, you can reduce the number of hexagons by merging some hexagons into larger hexagons.\n\nReduces storage size\nIssue: leaves small gaps between hexagons\n\nmight not matter for your use case\n\nSolution: use Microsoftâ€™s Quadkeys approach (see article)\n\n\nEach hexagon has a series of smaller hexagons that sit (mostly) inside of another, which creates a hierarchy that can be used for consistent referencing and analysis, all the way down to lengths of 2 feet for the edges.\nâ€œHexagons were an important choice because people in a city are often in motion, and hexagons minimize theÂ quantization errorÂ introduced when users move through a city. Hexagons also allow us to approximate radiuses easily.â€\nRe other shapes: â€œWe could use postal code areas, but such areas have unusual shapes and sizes which are not helpful for analysis, and are subject to change for reasons entirely unrelated to what we would use them for. Zones could also be drawn by Uber operations teams based on their knowledge of the city, but such zones require frequent updating as cities change and often define the edges of areas arbitrarilyâ€\nGrid systems can have comparable shapes and sizes across the cities that Uber operates in and are not subject to arbitrary changes. While grid systems do not align to streets and neighborhoods in cities, they can be used to efficiently represent neighborhoods by clustering grid cells. Clustering can be done using objective functions, producing shapes much more useful for analysis. Determining membership of a cluster is as efficient as a set lookup operation.\n16 Resolutions\n\n0 - 15 (0 being coarsest and 15 being finest)\nEach finer resolution has cells with one seventh the area of the coarser resolution. Hexagons cannot be perfectly subdivided into seven hexagons, so the finer cells are only approximately contained within a parent cell.\nThe identifiers for these child cells can be easily truncated to find their ancestor cell at a coarser resolution, enabling efficient indexing. Because the children cells are only approximately contained, the truncation process produces a fixed amount of shape distortion. This distortion is only present when performing truncation of a cell identifier; when indexing locations at a specific resolution, the cell boundaries are exact.\nWant a resolution granular enough to introduce variability and wide enough to capture the effects of an area\nExample of resolution 6 in Iowa"
  },
  {
    "objectID": "qmd/geospatial-general.html#features",
    "href": "qmd/geospatial-general.html#features",
    "title": "37Â  General",
    "section": "37.5 Features",
    "text": "37.5 Features\n\nCarto Spatial Features dataset ($) - https://carto.com/spatial-data-catalog/browser/?country=usa&category=derived&provider=carto\n\nResolution: Quadgrid level 15 (with cells of approximately 1x1km) and Quadgrid level 18 (with cells of approximately 100x100m).\n\nGuessing if the areas youâ€™re interested in have high population density, then maybe 100 x 100 m cells would be more useful\n\nFeatures\n\nTotal population\nPopulation by gender\nPopulation by age and gender (e.g.Â female_0_to_19)\nPOIs by category\n\nretail stores\neducation\n\nNumber of education related POIs, incuding schools, universities, academies, etc.\n\nfinancial\n\nNumber of financial sector POIs, including ATMs and banks.\n\nfood, drink\n\nNumber of sustenance related POIs, including restaurants, bars, cafes and pubs.\n\nhealthcare\n\nNumber of healthcare related POIs, including hospitals\n\nleisure\n\nNumber of POIs related to leisure activities, such as theaters, stadiums and sport centers.\n\ntourism\n\nNumber of POIs related to tourism attractions\n\ntransportation\n\nNumber of transportation related POIs, including parking lots, car rentals, train stations and public transport stations.\n\n\n\n\nCarto Data Observatory ($) - https://carto.com/spatial-data-catalog/browser/dataset/mc_geographic__4a11e98c/\n\nFeatures\n\ngeo id\nregion id\nIndustry\nTotal Transactions Amount Index\nTransaction Count Index\nAccount Count Index\nAverage Ticket Size Index\nAverage Frequency of Transaction per Card Index\nAverage Spend Amount by Account Index"
  },
  {
    "objectID": "qmd/geospatial-general.html#interactions",
    "href": "qmd/geospatial-general.html#interactions",
    "title": "37Â  General",
    "section": "37.6 Interactions",
    "text": "37.6 Interactions\n\nSimilar to interpolation but keeps the original spatial units as interpretive framework. Hence, the map reader can still rely on a known territorial division to develop its analyses\n\nthey produce understandable maps by smoothing complex spatial patterns\nthey enrich variables with contextual spatial information.\n\nMisc\n\nResources\n\nGetting Started with Potential - nice little mathematical summary, some background\n\nPackages\n\n{potential}: spatial interaction modeling via Stewart Potentials. Also capable of interpolation\n\n\nThere are two main ways of modeling spatial interactions: the first one focuses on links between places (flows), the second one focuses on places and their influence at a distance (potentials).\nComparisons ({potential}article)\n\nGDP per capita (cloropleth)\n\nTypical cloropleth at the municipality level\nValues have been binned\n\nPotential GDP per Capita (interaction)\n\nStewart Potentials have smoothed the values\nMunicipality boundaries still intact, so you could perform an analysis based on these GDP regions\n\nSmoothed GDP per Capita (interpolation)\n\nSimilar results as the interaction model except there are no boundaries"
  },
  {
    "objectID": "qmd/geospatial-general.html#interpolation",
    "href": "qmd/geospatial-general.html#interpolation",
    "title": "37Â  General",
    "section": "37.7 Interpolation",
    "text": "37.7 Interpolation\n\nThe process of using points with known values to estimate values at other points. In GIS applications, spatial interpolation is typically applied to a raster with estimates made for all cells. Spatial interpolation is therefore a means of creating surface data from sample points.\nKriging\n\nMisc\n\n{gstat}\n\nUses a correlated Gaussian process to guess at values between data points\n\nUncorrelated - white noise\nCorrelated - smooth\n\nThe closer in distance two points are to each other, the more likely they are to have a similar value (i.e.Â geospatially correlated)\nExample: temperature\n\n\nFewer known points means greater unceretainty Inputs:\n\nthe measured values at the sampling points,\nthe geometric coordinates of the sampling points,\nthe geometric coordinates of the target points to interpolate,\nthe â€œcalibratedâ€ probabilistic model, with the spatial correlation obtained by data outputs:\nthe estimated values at the target points,\nthe estimated uncertainty (variance) at the target points."
  },
  {
    "objectID": "qmd/geospatial-remote-sensing.html#misc",
    "href": "qmd/geospatial-remote-sensing.html#misc",
    "title": "38Â  Remote Sensing",
    "section": "38.1 Misc",
    "text": "38.1 Misc\n\nResources\n\nGLCM Texture: A Tutorial\n\nTypes of Measures\n\nTexture - Descriptive statistic that measures spatial relationships\n\nValues cannot be transferred from one situation to another\n\ne.g.Â you canâ€™t say, â€œforests always have Contrast values between .5 and .7â€\n\nPrimarily useful in comparing one part of an image to another part\n\nFor multi-image comparison (e.g.Â mosaic):\n\nthe images analysed must be equivalent radiometrically, in regards to sun angle, and phenologically with regards to cyclically variable ground phenomena\n\n\n\nSpectral - Descriptive statistics that essentially measure chemical properties of the ground objects\n\nSpectral and spatial are very likely to be independent data and so complement one another\nGrey Level Co-occurrence Matrix (GCLM) - Used for texture measurements. A tabulation of how often different combinations of pixel brightness values (grey levels) occur in an image.\nPCA Issues\n\nEach new dataset requires recalculation of both, landscape metrics and principal components analysis (PCA)\nHighly correlated landscape metrics are used\nPCA results interpretation is not straightforward\n\nInformation Theory (IT) Based Metrics\n\nMarginal entropy [H(x)] - diversity (composition) of spatial categories - from monothematic patterns to multithematic patterns\nRelative mutual information [U] - clumpiness (configuration) of spatial categories from fragmented patterns to consolidated patterns)\nH(x) and U are uncorrelated\nIssues\n\nRelative mutual information is a result of dividing mutual information by entropy. What to do when the entropy is zero?\nHow to incorporate the meaning of categories into the analysis?"
  },
  {
    "objectID": "qmd/geospatial-remote-sensing.html#terms",
    "href": "qmd/geospatial-remote-sensing.html#terms",
    "title": "38Â  Remote Sensing",
    "section": "38.2 Terms",
    "text": "38.2 Terms\n\nNormalized Difference Vegetation Index (NDVI) - A widely-used metric for quantifying the health and density of vegetation using sensor data. It is calculated from spectrometric data at two specific bands: red and near-infrared. The spectrometric data is usually sourced from remote sensors, such as satellites.\n\nRange: -1 and 1\nInterpretation\n\n0: Area has nothing growing (e.g.Â Deserts)\n1: Arean has dense, healthy vegetation\n&lt;0: Suggest lack of dry land (e.g.Â oceans have NDVI = -1)\n\n\nSemantic Segmentation - The process of labelling pixels or regions of the image\n\nEssential in many applications including infrastructure planning, land cover, humanitarian crisis maps and environmental assessments."
  },
  {
    "objectID": "qmd/geospatial-processing.html#misc",
    "href": "qmd/geospatial-processing.html#misc",
    "title": "39Â  Processing",
    "section": "39.1 Misc",
    "text": "39.1 Misc\n\nD/L and Load a shapefile\n\nMay need API key from Census Bureau (see {tigris} docs)\nExample: Counties in California\n\n\ntbl &lt;- tigris::counties(state = \"CA\") %&gt;%\nÂ  Â  st_set_crs(4326)\n\nWrite data to geojson\n\ndata %&gt;%\nÂ  Â  st_write(\"mb_shapes.geojson\")\n\nBeware statistical computations of tibbles/sf_tibbles with geometry columns\n\nCould result in an expensive union operation over identical geometries and an R session crash\n\nExample with 100K rows crashed R.\n\nNotes from thread\nOption 1 (slower): Use summarizeâ€™s arg, do_union = FALSE\nOption 2 (faster): Perform calculation without geometries then join\n\n{tidycensus} has an arg to bypass d/ling the geometries, â€œgeometry = FALSEâ€ and a separate tracts function to get the census tract geometries"
  },
  {
    "objectID": "qmd/geospatial-processing.html#loading-and-reading-shape-files",
    "href": "qmd/geospatial-processing.html#loading-and-reading-shape-files",
    "title": "39Â  Processing",
    "section": "39.2 Loading and Reading Shape Files",
    "text": "39.2 Loading and Reading Shape Files\n\n{tigris} - US data\n\nlibrary(tigris)\n\nus_states &lt;- states(resolution = \"20m\", year = 2022, cb = TRUE)\n\nlower_48 &lt;- us_states %&gt;%\nÂ  filter(!(NAME %in% c(\"Alaska\", \"Hawaii\", \"Puerto Rico\")))\n\n{rnaturalearth} - World data\n\n# Via URL\n# Medium scale data, 1:50m Admin 0 - Countries\n# Download from https://www.naturalearthdata.com/downloads/50m-cultural-vectors/\nworld_map &lt;- read_sf(\"ne_50m_admin_0_countries/ne_50m_admin_0_countries.shp\") %&gt;%\nÂ  filter(iso_a3 != \"ATA\")Â  # Remove Antarctica\n\n# Via Package\nlibrary(rnaturalearth)\n\n# rerturnclass = \"sf\" makes it so the resulting dataframe has the special\n# sf-enabled geometry column\nworld_map &lt;- ne_countries(scale = 50, returnclass = \"sf\") %&gt;%\nÂ  filter(iso_a3 != \"ATA\")Â  # Remove Antarctica"
  },
  {
    "objectID": "qmd/geospatial-processing.html#projections",
    "href": "qmd/geospatial-processing.html#projections",
    "title": "39Â  Processing",
    "section": "39.3 Projections",
    "text": "39.3 Projections\n\nWGS 84\n\nGoogle â€œepsg codeâ€ + â€œyour region nameâ€ to find a reasonable projection code to use\n\nStandard projection is 4326 aka WGS84 (required by leaflet)\nTransform shapefile\n\n\n\nmb_shapes &lt;- read_sf(download_folder)\nmb_shapes %&gt;%\nÂ  st_transform(4326)\n\nTransform latitude and longitude then visualize\n\nnew_tbl &lt;- old_tbl # contains latitude and longitude variables\nÂ  Â  # convert to simple features object\nÂ  Â  sf::st_as_sf(\nÂ  Â  Â  Â  coords = c(\"&lt;longitude_var&gt;\", \"&lt;latitude_var&gt;\"), # order matters\nÂ  Â  Â  Â  crs = 4326 # standard crs\nÂ  Â  ) %&gt;%\nÂ  Â  mapviw::mapview()\n\nWGS 84 projection, which is what Google Maps (and all GPS systems) use\n\nus_states &lt;- us_states %&gt;% # df with geometries\nÂ  sf::st_transform(st_crs(\"EPSG:4326\"))Â  # WGS 84\n\nNAD83, Albers, Mercator, Robinson\n\nlibrary(patchwork)\n\np1 &lt;- ggplot() +\nÂ  geom_sf(data = lower_48, fill = \"#0074D9\", color = \"white\", linewidth = 0.25) +\nÂ  coord_sf(crs = st_crs(\"EPSG:4269\")) +Â  # NAD83\nÂ  labs(title = \"NAD83 projection\") +\nÂ  theme_void() +\nÂ  theme(plot.title = element_text(hjust = 0.5, family = \"Overpass Light\"))\n\np2 &lt;- ggplot() +\nÂ  geom_sf(data = lower_48, fill = \"#0074D9\", color = \"white\", linewidth = 0.25) +\nÂ  coord_sf(crs = st_crs(\"ESRI:102003\")) +Â  # Albers\nÂ  labs(title = \"Albers projection\") +\nÂ  theme_void() +\nÂ  theme(plot.title = element_text(hjust = 0.5, family = \"Overpass Light\"))\n\np3 &lt;- ggplot() +\nÂ  geom_sf(data = world_map, fill = \"#FF4136\", color = \"white\", linewidth = 0.1) +\nÂ  coord_sf(crs = st_crs(\"EPSG:3395\")) +Â  # Mercator\nÂ  labs(title = \"Mercator projection\") +\nÂ  theme_void() +\nÂ  theme(plot.title = element_text(hjust = 0.5, family = \"Overpass Light\"))\n\np4 &lt;- ggplot() +\nÂ  geom_sf(data = world_map, fill = \"#FF4136\", color = \"white\", linewidth = 0.1) +\nÂ  coord_sf(crs = st_crs(\"ESRI:54030\")) +Â  # Robinson\nÂ  labs(title = \"Robinson projection\") +\nÂ  theme_void() +\nÂ  theme(plot.title = element_text(hjust = 0.5, family = \"Overpass Light\"))\n\n(p1 | p2) / (p3 | p4)"
  },
  {
    "objectID": "qmd/git-general.html#misc",
    "href": "qmd/git-general.html#misc",
    "title": "40Â  General",
    "section": "40.1 Misc",
    "text": "40.1 Misc\n\nView HTML file in browser\n\nSyntax: â€œhttps://raw.githack.com/&lt;acct name&gt;/&lt;repo name&gt;/&lt;branch name&gt;/&lt;directory name&gt;/&lt;file name&gt;.htmlâ€\n\nInstalling from a git repo (From link)\n\nMake a fork of the repo and then clone it to your local machine.\nTo update, after setting an upstream remote (git remote add upstream git://github.com/benfulcher/hctsa.git) you can use git pull upstream main.\nTo update the submodule in the repo, git submodule update --init\n\nStart R project and Git repo in whichever order (I think)\n\nCreate R project in RStudio\n\nChoose â€œNew Directoryâ€ for all the templated projects (e.g.Â quarto book, shiny, etc.). None of the other choices have them.\n\nIf youâ€™ve already created a directory, it will NOT overwrite this directory or add to it. So youâ€™ll either have alter the name of your old directory or choose a new name.\n\n\nCreate repo on Github\n\nAdd license and readme\n\nDo work\nTools &gt;&gt; Version Control &gt;&gt; Project Set-up &gt;&gt; Version Control System &gt;&gt; Select Git\nOpen terminal and go to working directory of project\ngit checkout -B main\ngit pull origin main --allow-unrelated-histories\ngit add .\ngit commit -m \"initial commit\"\ngit push --set-upstream origin main \n\nTurn off â€œLF will be replaced by CRLF the next time Git touches itâ€\n\nMessage spams terminal when committing changes from a window machines. Has to do with line endings in windows vs unix.\nTurn off: git config core.autocrlf true\nSee SO post for more details\n\nURL format to download files from repositories\n\nhttps://raw.githubusercontent.com/user/repository/branch/filename\n\n# Or evidently this way works too\n# adds ?raw=true to the end of the url\nfeat_all_url &lt;- url(\"https://github.com/notast/hierarchical-forecasting/blob/main/3feat_all.RData?raw=true\")\nload(feat_all_url)\nclose(feat_all_url)\nGet filelist from repo and download to a directory\n\n** Directory urls change as commits are made **\n\nlibrary(httr)\n\n# example: get url for the data dir of covidcast repo\nreq &lt;- httr::GET(\"https://api.github.com/repos/ercbk/Indiana-COVIDcast-Dashboard/git/trees/master?recursive=1\") %&gt;%Â \nÂ  httr::content()\n# alphabetical order\ntrees &lt;- req$tree %&gt;%Â \nÂ  map(., ~pluck(.x, 1)) %&gt;%Â \nÂ  as.character()\n# returns 20 which is first instance, so 19 should the \"data\" folder\ndetect_index(trees, ~str_detect(., \"data/\"))\n# url for data dir\nreq$tree[[19]]$url\n\n# example\n# Get all the file paths from a repo\nreq &lt;- GET(\"https://api.github.com/repos/etiennebacher/tidytuesday/git/trees/master?recursive=1\")\n# any request errors get printed\nstop_for_status(req)\nfile_paths &lt;- unlist(lapply(content(req)$tree, \"[\", \"path\"), use.names = F)\n# file_path wanted &lt;- filter file path to file you want\n# gets the very last part of the path\nfile_wanted &lt;- basename(file_path_wanted)\norigin &lt;- paste0(\"https://raw.githubusercontent.com/etiennebacher/tidytuesday/master/\", file_wanted)\ndestination &lt;- \"output-path-with-filename-ext\"\n# if file doesn't already exist, download it from repo into destination\nif (!file.exists(destination)) {\nÂ      # if root dir doesn't exist create it\nÂ      if (!file.exists(\"_gallery/img\")) {\nÂ  Â      dir.create(\"_gallery/img\")\nÂ      }\nÂ      download.file(origin, destination)"
  },
  {
    "objectID": "qmd/git-general.html#optimizations",
    "href": "qmd/git-general.html#optimizations",
    "title": "40Â  General",
    "section": "40.2 Optimizations",
    "text": "40.2 Optimizations\n\nFor large repos, simple actions, like running git status or adding new commits can take many seconds. Cloning repos can take many hours.\nBenefits\n\nIt improves the overall performance of your development workflow, allowing you to work more efficiently. This is especially important when working with large organizations and open source projects, where multiple developers are constantly committing changes to the same repository. A faster repository means less time waiting for Git commands such as git clone or git push to finish. It helps to optimize the storage space, as large files are replaced by pointers which take up less space. This can help avoid storage issues, especially when working with remote servers.\n\nMisc\n\nSee How to Improve Performance in Git: The Complete Guide\n\nExplainer, config settings, advanced gc, checkout, and clone commands\n\nUse .gitignore\n\nGenerated files, like cache or build files\n\nThey will be modified at each different generation â€” and thereâ€™s no need to keep track of those changes.\n\nThird-party libraries\n\nInstead, aim for a list of the required dependencies (and the correct version) so that everyone can download and install them whenever the repo is cloned.\n\nFor example, with a package.json file for JavaScript projects you can (and should) exclude the /node_modules folder.\n.DS_Store files (which are automatically created by macOS) are another good candidate\n\n\n\nGit LFS\n\nDesigned specifically to handle large file versioning. LFS saves your local repositories from becoming unnecessarily big, preventing you from downloading unnessary data.\n\nGit LFS intercepts any large files and sends them to a separate server, leaving a smaller pointer file in the repository that links to the actual asset on the Git LFS server.\n\nThis is an extension to the standard Git feature set, so you will need to make sure that your code hosting provider supports it (all the popular ones do).\nAlso need to download and install the CLI extension on your machine before installing it in your repository.\nSet-Up\n$ git lfs install\n$ git lfs track \"*.wav\"\n$ git lfs track \"images/*.psd\"\n$ git lfs track \"videos\"\n$ git add .gitattributes\n\nTells Git LFS which file extensions it should manage.\n.gitattributes notes the file names and patterns in this text file and, just like any other change, it should be staged and committed to the repository.\nCan now add files and commit as normal\nList all file extensions being tracked: git lfs track\nList all files being managed: git lfs ls-files"
  },
  {
    "objectID": "qmd/git-general.html#troubleshooting",
    "href": "qmd/git-general.html#troubleshooting",
    "title": "40Â  General",
    "section": "40.3 Troubleshooting",
    "text": "40.3 Troubleshooting\n\nKeeps asking for username/password when pushing\n\nSolution: You (or if you used usethis::use_github/git) probably set-up a https connection when you need a ssh connection.\n\nsee https://docs.github.com/en/get-started/getting-started-with-git/managing-remote-repositories#changing-a-remote-repositorys-url to change from https to ssh.\n\n\nUndo a commit, but save changes made (e.g.Â you forgot to pull before you pushed)\n\nSteps\n\ngit log - Shows commit history. Copy the hash for your last commit\ngit diff &lt;last commit hash&gt; &gt; patch - save the diff of the latest commit to a file\ngit reset --hard HEAD^ to revert to the previous commit\n\n**After this, your changes will be lost locally **\n\ngit log - confirm that you are now at the previous commit\ngit pull - correct the mistake you made in first place\npatch -p1 &lt; patch - apply the changes you originally made\ngit diff - to confirm that the changes have been reapplied\nNow, you do the regular commit, push routine\n\n\nUndo uncommitted changes: git stash followed by git stash drop\n\nâ€œbut only use if you commit oftenâ€ - guessing this is not good if your commit is somehow large and/or involves multiple files\n\nSearch commits by string: git log --grep &lt;string&gt;"
  },
  {
    "objectID": "qmd/git-general.html#pulling",
    "href": "qmd/git-general.html#pulling",
    "title": "40Â  General",
    "section": "40.4 Pulling",
    "text": "40.4 Pulling\n\nSave your changes, pull in an update, apply your changes\ngit stash\ngit pull\ngit stash pop\n\ngit stash pop throws away the (topmost, by default) stash after applying it, whereas\ngit stash apply leaves it in the stash list for possible later reuse (or you can then git stash drop it).\n\nRe potential merge conflicts\n\nâ€œFor instance, say your stashed changes conflict with other changes that youâ€™ve made since you first created the stash. Both pop and apply will helpfully trigger merge conflict resolution mode, allowing you to nicely resolve such conflictsâ€¦ and neither will get rid of the stash, even though perhaps youâ€™re expecting pop too. Since a lot of people expect stashes to just be a simple stack, this often leads to them popping the same stash accidentally later because they thought it was gone.â€\n\nPulling is fetching + merging\n\nFetching just gets the info about the commits made to the remote repo\ngit fetch origin\nSome technical discussion for always using git pull â€“ff\n\nhttps://blog.sffc.xyz/post/185195398930/why-you-should-use-git-pull-ff-only-git-is-a\nhttps://megakemp.com/2019/03/20/the-case-for-pull-rebase/\nitâ€™s still confusing but pull rebase sounds fine to me\n--global tag says do it for all my repos\nnot sure what the true and only are for\n\ngit pull â€“help will open doc in browser\n\n\nPulling by rebase\n\nLocal: using this method as default\ngit config pull.rebase true\ngit pull\nRemote\ngit pull --rebase\n\nPulling by fast-forward\n\nLocal: using this method as default\ngit config --global pull.ff only\ngit pull\nRemote\ngit pull --ff"
  },
  {
    "objectID": "qmd/git-general.html#branching",
    "href": "qmd/git-general.html#branching",
    "title": "40Â  General",
    "section": "40.5 Branching",
    "text": "40.5 Branching\n\nMisc\n\nCreate a new branch for each ticket you are working on or each data model. It can get sloppy when you put all your code changes on one branch.\n\nCreate a branch (e.g.Â â€œtestingâ€)\ngit branch testing\nWork in a branch\ngit checkout testing\nthe files in your working directory change to the version saved in that branch\nIt adds, removes, and modifies files automatically to make sure your working copy is what the branch looked like on your last commit to it.\nCreate and work in a branch\n# new way\ngit switch -c testing\nor\ngit checkout -b testing\nor\ngit branch testing\ngit checkout testing\ncreates the branch and switches you to working in that branch\nIf you did a bunch of changes in a codebase, only to realize that youâ€™re working on `master`,Â  switch will bring those local changes with you to the new branch. So I guess they wonâ€™t affect master then.\n\nUnless If you already committed to main, then those changes are both in your new branch and in main. So you would still have to clean up the main branch.\n\nDeleting a branch\n\nlocal branch\ngit branch -d testing\n\nremote branch\ngit push &lt;remoteName&gt; --delete &lt;branchName&gt;\nSee existing branches\ngit branch\nSee what has been commited the remote repo branches\ngit fetch origin\ngit branch -vv\nâ€œoriginâ€ is the name of the remote\nresult\ntestingÂ  Â  7e424c3 [origin/testing: ahead 2, behind 1] change abcÂ \nmasterÂ  Â  Â  1ae2a45 [origin/master] Deploy index fix\n* issueÂ  Â  f8674d9 [origin/issue: behind 1] should do itÂ  Â  Â  Â  Â \ncartÂ  Â  Â  Â  5ea463a Try something new\nformat: branch, last commit sha-1, local branch status vs remote branch status, commit message\nthe star indicates the HEAD pointerâ€™s location (where youâ€™re at, i.e.Â checkout)\ntesting branch\n\nâ€œahead 2â€ meansÂ  I committed twice to the local testing branchÂ and this work has not been pushed to the remote testing branch repo yet.\nâ€œbehind 1â€ means someone has pushed a commit to the remote testing branch repo and we havenâ€™t merged this work to our local testing branch\n\nGet the last 10 branches that youâ€™ve committed to locally:\ngit branch --sort=-committerdate | head -n 10\nRename branch\n# change locally\ngit branch --move &lt;bad-branch-name&gt; &lt;corrected-branch-name&gt;\n# change remotely in repo\ngit push --set-upstream origin &lt;corrected-branch-name&gt;\n# confirm change\ngit branch --all\nHEAD determines to which branch new commits are added\n\nExample\n\nâ€œtestingâ€ branch is created (not shown in above picture)\n\nHEAD points at â€œmasterâ€ branch\nâ€œmasterâ€ branch and the new â€œtestingâ€ branch both point at commit, f30ab.\nf30ab commit points to previous commit 34ac2\n\nuser executes checkout to â€œtestingâ€ branch (not shown in picture)\n\nHEAD now points to testing branch\n\nuser commits 87ab2 (shown in pic)\n\n87ab2 is committed to the â€œtestingâ€ branch\nâ€œtestingâ€ branch is now ahead of the â€œmasterâ€ branch by 1 commit\n\n\nExample\n\nEverything above happens but now another user commits the master branch.\n\nBoth branches are in conflict. The testing branch is ahead and behind by 1 commit\n\n\n\nMerging\n\nNotes\n\nNEVER merge your branch locally on your machine with the master branch, ALWAYS merge online via pull request\n\nSteps\n\nPush final changes and use of a pull request\nSwitch to master branch locally and pull the merged changes\n\n\n\nUpdate branch with work thatâ€™s been done in master branch\n\nAfter updating your local branch, push to remote repo (no commit necessary)\n# while in branch\ngit merge master\n\n\nFast-Forward\n\nExample\n\nBefore the merge\n\nthe testing branch is 1 commit ahead of the master branch and the master branch doesnt have a new commit\n\nAfter the merge\n\nmaster is moved forward to the testing branch commit\n\n\nCode (merging work in branch with the master branch for production)\n# currently in test branch\ngit checkout master\ngit merge testing\n\nLines in file are marked\n# &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD:index.html\n# &lt;div id=\"footer\"&gt;contact : email.support@github.com&lt;/div&gt;\n# =======\n# &lt;div id=\"footer\"&gt;\n# please contact us at support@github.com\n# &lt;/div&gt;\n# &gt;&gt;&gt;&gt;&gt;&gt;&gt; iss53:index.html\nAbove ======= is the master branch version of the code and below is the iss53 branch version\nMake necessary changes and save the file\ngit add . or git add &lt;resolved file&gt;\n\nTells git that conflict is resolved\n\nCheck status to confirm everything has been resolved\ngit status\n\n    On branch master\n    All conflicts fixed but you are still merging.\n      (use \"git commit\" to conclude merge)\n    Changes to be committed:\n      modified:  index.html\ngit commit\n\nno message required (thereâ€™s a default message) but you can add one if you want\n\nExample\n\niss53 branch ahead of master by 2 commits (c3, c5) and behind 1 commit (c2)\nSame code as Fast-Forward merge but git handles the merge a bit differently\ngit checkout masterÂ \ngit merge iss53\n\n\n\nC6 (right pic) is called a â€œmerge commit.â€ Its created by git and points to two commits instead of one.\nNo need to merge with master (i.e.Â update local iss53 branch with c4 changes in master) before committing final changes\n\nIf there are changes in the same lines of code C4 and C5, then there will be a conflict (See below, Conflicts &gt;&gt; Example)\n\n\nConflicts\n\nExample\n\nChanged files in C4 (see above example) are in the same lines of the same files that you made changes to in C5\n\nRemember: youâ€™re now in the master branch since you did checkout master as part of the merge code\nSteps\n\nCheck status to which files are causing the conflict (e.g.Â index.html)\ngit status\nÂ  Unmerged paths:\nÂ  (use \"git add &lt;file&gt;...\" to mark resolution)Â \nÂ  Â  both modified:Â  Â  Â  index.html\n\n\n\n\nMoving between branches\n\nfrom master to testing\ngit checkout testing\n\nlocal files are deleted and replaced with branch versions\n\nalternative: worktree\n\nExample\n\nWhat happens when you move from branch-a to branch-b\nBRANCH-AÂ  Â  Â  Â  BRANCH-B\nalpha.txtÂ  Â  Â  alpha.txt\nbravo.txt\ncharlie.txtÂ  Â  charlie.txt\n                delta.txt\n\nbravo text is deleted from your local disc and delta.txt is added\nIf any changes to alpha.txt or charlie.txt have been made and no commit has been made, the checkout will be aborted\n\nSo either revert the changes or commit the changes\n\nUntracked files or newly created files\n\nIf you have branch-A checked out and you create a new file called echo.txt, Git will not touch this file when you checkout branch-B. This way, you can decide that you want to commit echo.txt against branch-B without having to go through the hassle of (1) move the file outside the repo, (2) checkout the correct branch, and (3) move the file back into the repo."
  },
  {
    "objectID": "qmd/git-general.html#collaboration",
    "href": "qmd/git-general.html#collaboration",
    "title": "40Â  General",
    "section": "40.6 Collaboration",
    "text": "40.6 Collaboration\n\nAdd collaborators to your repository\nOne person invites the others and provides them with read/write access (github docs)\n\nSteps\n\nGo to the settings for your repository\nmanage access &gt;&gt; â€œinvite a collaboratorâ€\n\nSearch for each collaborator by full name, acct name, or email\nClick â€œAdd &lt;name&gt; to &lt;repo&gt;â€\n\nEach collaborator will need to accept the invitation\n\nSent by email"
  },
  {
    "objectID": "qmd/git-github-actions.html",
    "href": "qmd/git-github-actions.html",
    "title": "41Â  Git, Github Actions",
    "section": "",
    "text": "TOC\n\nMisc\n\nMisc\n\n\n\nTriggering\n\non.schedule\n\nSyntax\n\nMinute [0,59]\nHour [0,23]\nDay of the month [1,31]\nMonth of the year [1,12]\nDay of the week ([0,6] with 0=Sunday)\n\nExample\n\n\non:\nÂ  schedule:\nÂ  Â  - cron: '30 5 * * 1,3'\nÂ  Â  - cron: '30 5 * * 2,4'\n\njobs:\nÂ  test_schedule:\nÂ  Â  runs-on: ubuntu-latest\nÂ  Â  steps:\nÂ  Â  Â  - name: Not on Monday or Wednesday\nÂ  Â  Â  Â  if: github.event.schedule != '30 5 * * 1,3'\nÂ  Â  Â  Â  run: echo \"This step will be skipped on Monday and Wednesday\"\nÂ  Â  Â  - name: Every time\nÂ  Â  Â  Â  run: echo \"This step will always run\"\n\nThis example triggers the workflow to run at 5:30 UTC every Monday-Thursday, but skips the Not on Monday or Wednesday step on Monday and Wednesday."
  },
  {
    "objectID": "qmd/glossary-ds-terms.html",
    "href": "qmd/glossary-ds-terms.html",
    "title": "Glossary: DS terms",
    "section": "",
    "text": "200 Status - An API serving an ML model returns a HTTP 200 OK success status response code indicates that the request has succeeded.\nAMI - amazon machine image. Thing that has R and the mainÂ packages you need to load onto the cloud server\nAnti-Patterns - certain patterns in software development that are considered bad programming practices.\n\nAs opposed to design patterns which are common approaches to common problems which have been formalized and are generally considered a good development practice, anti-patterns are the opposite and are undesirable.\n\nArm -Â a group of patients receiving a specific treatment (or no treatment). Trials involving several arms, or randomized trials, treat randomly-selected groups of patients with different therapies in order to compare their medical outcomes. Experimental arms, which receive an experimental drug, are compared with control arms.Â Single-arm or non-randomized trials, in which everyone enrolled in a trial receives the experimental therapy\nArtifacts - objects that are created as a result of a process. e.g.Â model objects, cleaned data sets, visuals, etc.\nAsynchronous Programming - code runs (or must run) after something else happens and also not sequentially (e.g.Â when a function calls a callback function in JS).\nAthena - amazon query service that works with S3. Best for analyses using kubernetes. ODBC drivers are best with interactive app\nB2C, B2B - business-to-consumer, business-to-business, describes a business thatâ€™s end-product is being sold to a consumer or a business.\nBalanced Design (aka orthogonal) has an equal number of observations for all possible level combinations. For example in an experiment where gender is an independent variable, an equal number of males receive the treatment as do females receive treatment. If the male/female counts were unequal, then the experiment is unbalanced.\n\nStat tests have greater power for balanced designs\nTest stat less susceptible to to small departures from the assumption of equal variances (homoscedasticity).\n\nBatch - collect a large number of data points, process them periodically and store results somewhere (contrasts with real-time in which a data input leads to an immediate prediction)\nBootstrapping (CS) - usually applies to a situation where a system depends on itself to start, sort of a chicken and egg problem. (e.g.Â How do you start an OS initialization process if you donâ€™t have the OS running yet?) Typically a simple file that starts a large process.\nBounce, Email - When an email cannot be delivered to an email server.\n\nHard Bounce - indicates a permanent reason an email cannot be delivered (e.g.Â Recipient email address doesnâ€™t exist; Recipient email server has completely blocked delivery)\nSoft Bounce - indicates a temporary delivery issue (for details on the reasons, see link)\n\nBounce Rate - the percentage of visitors to a particular website who navigate away from the site after viewing only one page. Low bounce rate can indicate the landing page needs improvement\nBPI - Business process improvement is a management exercise in which enterprise leaders use various methodologies to analyze their procedures to identify areas where they can improve accuracy, effectiveness and/or efficiency and then redesign those processes to realize the improvements.\nBLUE - best linear unbiased estimator, e.g.Â regression line\nCAC - customer acquisition cost - measures how much an organization spends to acquire new customers. The total cost of sales and marketing efforts, as well as property or equipment, needed to convince a customer to buy a product or service.\nCapEx - Capital Expenditure - 1 of 2 main forward budgeting mechanisms for a corporation (also see OpEx). Often used to undertake new projects or investments or large-scale asset acquisitions (buildings and vehicles)\nClinical Trial - research studies (e.g.Â RCT) performed in people that are aimed at evaluating a medical, surgical, or behavioral intervention\nCDI - Customer Data Infrastructure - built to collect behavioral data from primary or first-party data sources, but some solutions also support a handful of secondary data sources (third-party tools)\nCDP - Customer Data Platform - add-ons from CDI vendors; a layer on top of CDI that offers a set of capabilities to analyze data using a visual interface.\nCDN - content delivery network - a system of distributed servers (network) that deliver pages and other web content to a user, based on the geographic locations of the user, the origin of the webpage and the content delivery server.\nCLV/CLTV - Customer Lifetime Value - how much money a customer will bring your brand throughout their entire time as a paying customer.\nCOGS - Cost of goods sold (aka Cost of Sales) - refers to the direct costs of producing the goods sold by a company. This amount includes the cost of the materials and labor directly used to create the good. It excludes indirect expenses, such as distribution costs and sales force costs.\nComplete Factorial Design - a research study involving two or more independent variables in which every possible combination of the levels of each variable is represented. For instance, in a study of two drug treatments, one (A) having two dosages and the other (B) having three dosages, a complete factorial design would pair the dosages administered to different individuals or groups of participants as follows: A1 with B1, A1 with B2, A1 with B3, A2 with B1, A2 with B2, and A2 with B3.\nCPG - Consumer packaged goods are items used daily by average consumers that require routine replacement or replenishment, such as food, beverages, clothes, tobacco, makeup, and household products.\nCPC - Cost Per ClickÂ - refers to the cost an advertiser pays each time an online user clicks on his or her digital ad\nCRM - customer relationship management i.e.Â customer service. Salesforce tracks this data. Example: what features your salesperson promised, and when? How much revenue you have from each customer? Or which salesperson sold the most in the past year?\ncron- standard tool used on Unix and Unix-like systems to schedule the periodic execution in the background of a command or script (like a batch script)\nCrossed Factors - when every category of one factor co-occurs in the design with every category of the other factor. In other words, there is at least one observation in every combination of categories for the two factors. (in contrast to â€œnested factorsâ€). As a consequence, interaction terms involving these two factors is allowed.\nCrossover Study - A type of clinical trial in which the study participants receive each treatment in a random order. With this type of study, every patient serves as his or her own control. Crossover studies are often used when researchers feel it would be difficult to recruit participants willing to risk going without a promising new treatment.\nCross-Section Data - randomly sampled data from a population. Like a survey. Aka observational data. See experimental data for comparison.\n\nPooled - differs from panel data in that it is observations of different subjects (instead of the same subjects) in different time periods.\nRolling - both the presence of an individual in the sample and the time at which the individual is included in the sample are determined randomly.\n\nCross-Tabs - section of survey analysis where the aggregated results are broken down by demography, party affiliation, etc.\nCTA - marketing term, call-to-action.Â any device designed to prompt an immediate response or encourage an immediate sale; words or phrases that can be incorporated into sales scripts, advertising messages or web pages that encourage consumers to take prompt action\nCTR - click through rate: the ratio of users who click on a specific link to the number of total users who view a page, email, or advertisement. It is commonly used to measure the success of an online advertising campaign for a particular website as well as the effectiveness of email campaigns.\nCRM - Customer Relationship Management - acquiring new customers but especially about retaining existing ones\nDAU - daily active users, ex: daily avg # of registered users of the site over past 30 days\nDBA - Database Administrator is an admin role that understands the particular database technology and how to get the best out of it. This includes improving performance, backups and recovery.\nDDL - Data definition or description language - Subset of SQL. Used to:\n\nKeep a snapshot of the database structure\nSet up a test system where the database acts like the production system but contains no data\nProduce templates for new objects that you can create based on existing ones. For example, generate the DDL for the Customer table, then edit the DDL to create the table Customer_New with the same schema.\n\nDesparate Impact Analysis - Analysis of the result of the application of a standard, requirement, test or other screening tool used for selection thatâ€”though appearing neutralâ€”has an adverse effect on individuals who belong to a legally protected class Differential Dropout**]{style=â€˜color: #009499â€™} -Â Differing dropout rates between treatment arms\nDMA - Designated Market Area; a geographic region where Nielsen, the ratings company, analyzes and quantifies how television is viewed. Residents can receive the same local TV and radio stations\nDNS -Â Â Domain Name System**]{style=â€˜color: #009499â€™} -Â  translates domain names to IP addresses so browsers can load Internet resources.\nDSL - domain-specific language - a computer language specialized to a particular application domain\nEMR - Amazon version of a spark cluster used for big data processing and analysis.\nEndogenous - A model variable is correlated with other variables excluded from the model (omitted variable bias). Determined by measuring the correlation between the variable and residuals of the model. If a predictor variable hasnâ€™t been randomly assigned, itâ€™s likely to be endogenous.\nEquitability - concept that saysÂ a dependence measure should give equal importance to linear and nonlinear relationships. Consistent strength measurements across different variable relationships that have similar amounts of noise.\nERP - enterprise resource planning, sort of a catch-all for manufacturing, supply-chain, etc, see the wiki\nETL - extract, transfer, load - usually refers to transferring data from one location to another\nEndpoint (biostats) - Outcome variable measured in a medical study. e.g.Â Death, stroke, or quality of life are good endpoints. Blood tests and images on scans are not good endpoints.\n\nA composite endpoint is one that consists of two or more events\n\nExample: death due to cardiovascular causes or hospitalization due to heart failure\n\nSo the binary outcome would be a 1 if either of those events took place or a 0 if they did not. Or in a survival model, time until either of those events.\n\n\n\nEOF - End of file - Input from a terminal never really â€œendsâ€ (unless the device is disconnected), but it is useful to enter more than one â€œfileâ€ into a terminal, so a key sequence is reserved to indicate end of input.\nex ante - based on assumption and prediction and being essentially subjective and estimative\nex post - based on knowledge and retrospection and being essentially objective and factual\nExperimental Data - data from a RCE/RCT. Compare with observational data\nFaaS - Function as a service - type of cloud service for developing, running, and managing apps (e.g.Â AWS Lambda)\nFactorial Design - Experiment where youâ€™re interested in the effect of two or more independent variables.\nFraud Rules - fraud scores are calculated based on rules, which add or subtract points. The user action may be a transaction, signup or login. Rules look at data points such as an email address, IP address, or social media presence.\nFraud Score - assigned values to how risky a user action is. Scoring determined by fraud rules.\nFuzzy Design - See Sharp Design\nGMV - Gross merchandises value - the total value of merchandise sold over a given period of time through a customer-to-customer (C2C) exchange site\nGRP - Gross Rating Point. A standard measure in advertising, it measures advertising impact. You calculate it as a percent of the target market reached multiplied by the exposure frequency. Thus, if you get advertise to 30% of the target market and give them 4 exposures, you would have 120 GRP.\nHTE - Heterogeneous Treatment Effect - Also called differential treatment effect, includes difference of means, odds ratios, and Hazard ratios for time-to-event outcome vars\n\nAscertaining subpopulations for which a treatment is most beneficial (or harmful) is an important goal of many clinical trials.\nOutcome heterogeneity is due to wide distributions of baseline prognostic factors. When strong risk factors exist, there is hetergeneity in the outcome variable.\n\nSolution: add baseline predictors to your model that account for these strong risk factors.\n\nHeterogeneity of Treatment Effects - The degree to which different treatments have differential causal effects on each unit.\n\nHit Ratio - percent of records that were read in order to complete a query in a database. Cloud db providers often charge by the number of records searched\nHomogeneity of Treatment Effects - See Heterogeneity of Treatment Effects\nHPC - High Performance Computing\nHoneypot - data (for example, in a network site) that appears to be a legitimate part of the site, but is actually isolated and monitored, and that seems to contain information or a resource of value to attackers, who are then blocked.\nIaaS - infrastructure-as-a-service ( Hardware is provided by an external provider and managed for you)\nIAM - identity and access management, keys and passwords etc\nIRB - institutional review board, reviews studies ethical and moral issues\nITT - Intent-to-Treat analysisÂ includes all randomized patients in the groups to which they were randomly assigned, regardless of their adherence with the entry criteria, regardless of the treatment they actually received, and regardless of subsequent withdrawal from treatment or deviation from the protocol. Avoids overoptimistic estimates of the efficacy of an intervention resulting from the removal of non-compliers by accepting that noncompliance and protocol deviations are likely to occur in actual clinical practice. So mimics likely situation in the real world, but not good for estimating the causal effect of a treatment.\nKernels - (article) - system kernels - the interface between the operating system, i.e.Â the software, and the hardware components in a device. It is used in all devices with an operating system, for example, computers, laptops, smartphones, smartwatches, etc.\n\nWhen we use a program on a computer, such as Excel, we handle it on the so-called Graphical User Interface (GUI). The program converts every button click or other action into machine code and sends it to the operating system kernel. If we want to add a new column in an Excel table, this call goes to the system core. This in turn passes the call on to the computer processing unit (CPU), which executes the action.\nJupyter Kernels - an engine that executes notebook code and is specific to a particular programming language (e.g.Â python kernel)\nKaggle Kernels - a free platform from Kaggle to run Jupyter notebooks in the browser. Advantage is that you donâ€™t have to set-up an environment locally.\n\nKPI- key performance indicator\nKYC - Know-Your-Customer is info a company collects to verify your identity to combat fraud. Used by telecoms and financial services\nLazy Evaluation - â€ never pulls data into R unless you explicitly ask for it. It delays doing any work until the last possible moment. It collects together everything you want to do and then sends it to the database in one step.â€\nLikelihood - probability of seeing this data given a specific value for a distribution parameter (eg mean, sd). Goal is to search for parameter values until the likelihood is maximized.\nLOB - Line of Business is a general term which refers to a product or a set of related products that serve a particular customer transaction or business need. (i.e.Â product categories)\n\nExamples\n\nConsumer Banking: credit cards, line of credit or loan program, mortgages, and corporate, small business and personal bank accounts.\nFinancial services and brokerages: mergers and acquisitions or partnerships, real estate investments, and wealth management\nProperty and casualty insurance companies: property and casualty insurance (i.e., homeowners, car, boat, renters, etc.), life insurance, health insurance, and commercial business insurance.\n\nSub-lines of Business would be sub-categories within each LOB\n\nLongitudinal Data - see panel data\nLTV - see CLV/CLTV\nManual Review - A human is reviews the case to determine whether action is needed. In fraud, an model output may trigger a â€œmanual reviewâ€ to determine whether an event was indeed fraudulent.\nMLlib - Apache Spark machine learning library\nMVC - Minimum Viable Corpus - a data size threshold; such that below this threshold, the data simply isnâ€™t useful/valuable. Used in data products business.\nMVP - minimum viable project, agile term. Version of a new product which allows a team to collect the maximum amount of validated learning about customers with the least effort\nNamespace - allows you to use two functions with the same name but from different packages, e.g.Â dplyr::select or in general, package::function. https://stackoverflow.com/questions/3384204/what-are-namespaces/3384384#3384384\nNNH - Numbers Needed to Harm - a derived statistic that tells us how many patients must receive a particular treatment for 1 additional patient to experience a particular adverse outcome. Lower NNT and higher NNH values are associated with a more favorable treatment profile.\nNNT - Numbers Needed to Treat - a derived statistic that tells us how many patients must receive a particular treatment for 1 additional patient to experience a favorable outcome such as treatment response. Lower NNT and higher NNH values are associated with a more favorable treatment profile.\nNPS - Net Promoter Score - a measure of customer loyalty. Widely used market research metric that typically takes the form of a single survey question asking respondents to rate the likelihood that they would recommend a company, product, or a service to a friend or colleague.\nNRT - near real-time, aka streaming data\nObservational Data - see cross sectional data\nOEM - original equipment manufacturer\nOKR - Objectives and Key Results is a popular management strategy for goal setting within organizations. A framework for turning strategic intent into measurable outcomes for an organization.\nOn-Prem - on-premises â€” working with servers in the the building and not in the cloud.\nOOD - out-of-distribution - data which differ from the training data and on which a model might underperform\nOpen Cohort - subjects can leave or be added over time.\nOpEx - Operational Expenditures - 1 of 2 main forward budgeting mechanisms for a corporation (also see CapEx). Relates to day-to-day expenses (such as payroll and software subscriptions). Smaller payouts over time.\nOpportunity Sizing - Quantitative analysis to select a subset of ideas to which to devote resources in product development\nNested Factors - happens when all the levels of one factor only occur in combination with one level of another factor (in contrast to â€œcrossed factorsâ€). As a consequence, your model canâ€™t have an interaction term involving these two variables.\nP&L - Profit and Loss Statement Panel data - cross section data with a time element. Repeated measures of the same subject over time. Synonym for Longitudinal Data\nParcel - a land record that defines the boundary of a piece of land. These boundaries are the basic administrative unit of local government in regards to land and property. Managing ownership and tax records are the primary reason local governments generate these files. So these are boundaries differentiating ownership of properties.\nPEP8 - style guide for python\nPI - principal investigator\nPivot Table - Excel name for a group_by %\\&gt;% summarize calculation\n\ne.g.Â from a table of individual fruit sales: group_by(fruit_type, country) %\\&gt;% summarize(total_amt = sum(amount))\n\nPLG - Product-led growth is an end user-focused growth model that relies on the product itself as the primary driver of customer acquisition, conversion, and expansion. e.g.Â open source a product, let the customer go through the documentation and use and experiment with the product on their own time. In contrast to sales pitching a product to a customer and letting them use it for a trial basis.\nPM - product manager\nPoC - Proof of Concept\nPOS - point of sale, The point of sale or point of purchase is the time and place where a retail transaction is completed. It can be in a physical store, where POS terminals and systems are used to process card payments or a virtual sales point such as a computer or mobile electronic device.\nRCE - randomized controlled experiment, subjects randomly assigned to two groups, treatment and control. Double blind means the researcher doesnâ€™t know who is in which group.\nRCT - randomized clinical trial\nRDD - Regression discontinuity design\nRedis - REmote DIctionary Server - is an in-memory, key-value database, commonly referred to as a data structure server. Used when volume of read and write operations exceed the capabilities of traditional databases. With Redisâ€™s capability to easily persist the data to disk, it is a superior alternative to the traditional memcached solution for caching.\nRefactoring - updating or optimizing code\nRegression Testing - checks if changes made to a system negatively impacted or broke any of the existing features. It is often performed right after each update or commit to the code base to identify new bugs and ensure that your system works properly.\nRFI - Request for Information - Used to collect written information about the capabilities of various suppliers. Normally it follows a format that can be used for comparative purposes. An RFI is primarily used to gather information to help make a decision on what steps to take next. RFIs are therefore seldom the final stage and are instead often used in combination with request for proposal (RFP), request for tender (RFT), and request for quotation (RFQ).\nRFM - recency, frequency, monetary value - method of estimating customer value; common in retail\nRFP - Request for Proposal - A document that an organization, often a government agency or large enterprise, posts to elicit a response -- a formal bid -- from potential vendors for a desired solution. The RFP specifies what the customer is looking for and describes each evaluation criterion on which a vendorâ€™s proposal will be assessed.\n\nROAS - return on ad spend\nRUG - Regional User Group\nS3 - Amazon simple storage service, database\nSaaS - Software-as-a-service is a mechanism through which companies offer the functionality of their apps, which remain on their company servers, to other companies or customers.\nSCO - sales cycle optimization, active process of providing content on your site (and beyond) that speaks to each of the key phases\nSEO - Search engine optimization, generating high page rankings for key search terms\nSDK - software development kit\nSharp Design - Each individual or group receives the same â€œamountâ€ of treatment (e.g.Â a state law or medication dosage). Opposite being fuzzy design (?)\nSKU - Stock Keeping Unit**]{style=â€˜color: #009499â€™} - Usually a bar code that has all the information to distinguish it from another product. These attributes can include manufacturer, description, material, size, color, packaging, and warranty terms. When a business takes inventory of its stock, it counts the quantity it has of each SKU.\nSLA - service level agreement - a contract between a service provider and its internal or external customers that documents what services the provider will furnish and defines the service standards this provider is obligated to meet. service. Important for holding prediction latency of an app to a certain standard or maintaining data reliability with vendors. (see link for more details on SLA, SLO, and SLI)\nSLI - service level indicators - metrics that measure compliance with an SLO (see link for more details on SLA, SLO, and SLI)\nSLO - service level objectives - objectives your team must meet in order to meet the conditions of the SLA (see link for more details on SLA, SLO, and SLI)\nSME - Subject Matter Experts\nSPC - Statistical process control is a method of quality control which employs statistical methods to monitor and control a process\nSpill - missed opportunity metric, measures â€œlost trading daysâ€ on which flights or hotels filled too quickly (the result of pricing too low)\nSpoil - missed opportunity metric, measures empty seats or rooms (often the result of pricing too high)\nSSH - secure shell is a cryptographic Network protocol for operating Network Services securely over an unsecured Network. Typical applications include remote command line login in remote command execution\nstdout - standard output, which is the terminal by default\nTDD - Test-driven development is a style of programming where coding, testing, and design are tightly interwoven\nTF-IDF- stands for term frequency-inverse document frequency, and is often used in information retrieval and text mining.\nThroughput - the amount of material or items passing through a system or process.\ntx - treatment, seen as variable with different treatments as values\nURI - Uniform Resource Identifier - a string of characters that unambiguously identifies a particular resource. e.g.Â s3//bucket/path/to/folder or http://127.0.0.1:5000or c:\\Users\\me\\path\\to\\folder\nUTM - Urchin Traffic Monitor - used to identify marketing channels\n\ne.g.Â http://yourwebsite.com/your-post-title/?utm_source=google\n\nutm code = string after â€œ?â€\n\nThis person clicked a google ad to get to your site\n\nName comes from Urchin Tracker, a web analytics software that served as the base for Google Analytics.\n\nVPS - virtual private server\nWIP - Work-in-Progress\nWithin Person Study - multiple treatments on each person either all in the same period or different treatments in different periods\nYear-Over-Year - used to make comparisons between one time period and another that is one year earlier.\n\nFormula (percentage): (value_this_year / value_previous_year) - 1\nExample: (sales_Jul_2023 / sales_Jul_2022) - 1"
  },
  {
    "objectID": "qmd/gnu-make.html#sec-make-misc",
    "href": "qmd/gnu-make.html#sec-make-misc",
    "title": "GNU Make",
    "section": "Misc",
    "text": "Misc\n\nNotes from\n\narticle\nproject\n\nGood example of an advanced makefile for a practical data science project\n\nVideo\n\nGoes through the differect components of executing a python project with make (e.g.Â testing, clean-up, defining variables, setting up the virtual environment, etc.)\n\n\nResources\n\nNice little tutorial\nAnother tutorial\n\nHavenâ€™t read it, but looks pretty thorough\n\nDocs for variable types\nDocs for functions\n\nProject orchestration system that only builds steps that have changed\n\n{drake}/{targets} are based on this system\n\nAssuming that youâ€™ve named the file â€œmakefileâ€ or â€œMakefileâ€ or something like that, simply typing make at the command line while inside your projectâ€™s directory will execute the build process.\n\nmake -B --recon shows all the commands used to build the project (i.e.Â kind of like a DAG)\nmake -B rebuilds the entire project even if no targets have changed"
  },
  {
    "objectID": "qmd/gnu-make.html#sec-make-gen",
    "href": "qmd/gnu-make.html#sec-make-gen",
    "title": "GNU Make",
    "section": "General",
    "text": "General\n\nSyntax\ntargets: prerequisites\ncommand\ncommand\ncommand\nThe prerequisites are also file names, separated by spaces. These files need to exist before the commands for the target are run. These are also called dependencies.\n.PHONY - Helpful to avoid conflicts between target names and file names\n\nConsidered best practice to use\nExample\n.PHONY: install\ninstall:\n        python3.9 -m venv venv && source venv/bin/activat && pip install -r requirements-dev.txt\n\n.PHONY: dbsetup\ndbsetup:\n        source venv/bin/activate && python -m youtube.db\n\n.PHONY: lint\nlint:\n        flake8 emojisearcher tests"
  },
  {
    "objectID": "qmd/gnu-make.html#sec-make-rul",
    "href": "qmd/gnu-make.html#sec-make-rul",
    "title": "GNU Make",
    "section": "Rules",
    "text": "Rules\n\nMakefiles are made-up of rules. Each rule is a code chunk.\nExample\n# Inside Makefile\ndata/raw/NIPS_1987-2015.csv:\ncurl -o $@ https://archive.ics.uci.edu/ml/machine-learning-databases/00371/NIPS_1987-2015.csv\n\nDownloads a file\nâ€œdata/raw/NIPS_1987-2015.csvâ€ is the file path and target for this rule.\n$@Â  is a Make automatic variable that fills in the target name for the file name arg in the curl command.\nThere is no prerequisite required for this command. So, this syntax is just target : command."
  },
  {
    "objectID": "qmd/gnu-make.html#sec-make-tar",
    "href": "qmd/gnu-make.html#sec-make-tar",
    "title": "GNU Make",
    "section": "Targets",
    "text": "Targets\n\nThe targets are file names, separated by spaces. Typically, there is only one per rule.\nDummy Targets - A target with no commands directly associated with it (it is sort of a meta-target).\n\nUseful if you want to only rebuild part of the project\nExample: if you have a couple of scripts that involve data acquisition and cleaning, another few that involve data analysis, and a few that involve the presentation of results (paper, plot), then you might define a dummy for each of them.\nall: data model paper\ndata: raw.csv\nmodel: model.rds\npaper: plot.png paper.pdf\n\nExecuting make paper in the CLI and in the project directory will call the commands that built â€œplot.pngâ€ and â€œpaper.pdfâ€"
  },
  {
    "objectID": "qmd/gnu-make.html#sec-make-var",
    "href": "qmd/gnu-make.html#sec-make-var",
    "title": "GNU Make",
    "section": "Variables",
    "text": "Variables\n\nExpanded Variables\n\nValues are accessed using $(x) or ${x})\nâ€œRecursively Expandedâ€ Variables are defined using = operator\nx = hello\ny = $(x)\n# Both $(x) and $(y) will now yield \"hello\"\nx = world\n# Both $(x) and $(y) will now yield \"world\"\n\nAny functions referenced in the definition will be executed every time the variable is expanded\nCan cause infitinite loops\n\nâ€œSimply Expandedâ€ Variables are defined using the := or ::= operator\nx := hello\ny := $(x)\n# Both $(x) and $(y) will now yield \"hello\"\nx := world\n# $(x) will now yield \"world\", and $(y) will yield \"hello\"\n\nAutomatic Variables\n\n$@ is a Make variable that â€œexpandsâ€ into the (first?) target name\n$^ is a Make variable that â€œexpandsâ€ into all of the prerequisites\n$&lt; is a Make variable that â€œexpandsâ€ into the first prerequisite\n$? is a Make variable that â€œexpandsâ€ into any prerequisites which have a time stamp more recent than the target\n% is a wildcard; looks for any targets in the makefile that matches itâ€™s pattern or files in the project directory (also see abstraction section below)\nfoo%.o: %.c\n    $(CC) $(CFLAGS) -c $&lt; -o $@\n\nWill match target lib/foobar.o, with:\n\nStem ($*): lib/bar\nTarget name ($@): lib/foobar.o\nPrerequisites ($&lt;, $^): lib/foobar.c\n\n\n$*is a Make variable that â€œexpandsâ€ the â€œstemâ€ (i.e.Â value) of wildcard"
  },
  {
    "objectID": "qmd/gnu-make.html#sec-make-com",
    "href": "qmd/gnu-make.html#sec-make-com",
    "title": "GNU Make",
    "section": "Commands",
    "text": "Commands\n\nThe commands are a series of steps typically used to make the target(s). These need to start with a tab character, not spaces.\nSee command used to generate a target\n# CLI\n&gt;&gt; make --recon &lt;target&gt;\nUpdate a specific target\n# CLI\n&gt;&gt; make data/raw/NIPS_1987-2015.csv\n\nThis will re-run the rule that created the file. In this case, itâ€™s the curl command in the â€œDownload a fileâ€ section\nIf you run this command again, youâ€™ll receive this message: make:data/raw/NIPS_1987-2015.csvâ€™ is up to date.`"
  },
  {
    "objectID": "qmd/gnu-make.html#sec-make-exe",
    "href": "qmd/gnu-make.html#sec-make-exe",
    "title": "GNU Make",
    "section": "Execute a script",
    "text": "Execute a script\n\nExample\n\nMakefile\ndata/processed/NIPS_1987-2015.csv : src/data/transpose.py data/raw/NIPS_1987-2015.csv\n    $(PYTHON_INTERPRETER) $^ $@\n\nâ€œ$(PYTHON_INTERPRETER)â€ is an environment variable set in the Make file for python3 interpreter\nThe function in this example has 2 args: input file path and output file path\n$^ fills in the prerequisites which takes care of &lt;script&gt; &lt;arg1&gt;\n$@ fills in the target name for &lt;arg2&gt;\n\nCLI\n&gt;&gt; make --recon data/processed/NIPS_1987-2015.csv\npython3 src/data/transpose.py data/raw/NIPS_1987-2015.csv data/processed/NIPS_1987-2015.csv\n\nmake --recon shows us the translation of command line in the Make file\nBasic format for executing a python script in the cli is python3 &lt;script&gt; &lt;arg1&gt; &lt;arg2&gt; ... &lt;argn&gt;"
  },
  {
    "objectID": "qmd/gnu-make.html#sec-make-abs",
    "href": "qmd/gnu-make.html#sec-make-abs",
    "title": "GNU Make",
    "section": "Abstraction",
    "text": "Abstraction\n\nExample\nall: models/10_topics.png models/20_topics.png\n\nmodels/%_topics.png : src/models/fit_lda.py data/processed/NIPS_1987-2015.csv src/models/prodlda.py\n    $(PYTHON_INTERPRETER) $&lt; $(word 2, $^) $@ --topics $*\n\n% matches both targets in the dummy target, â€œallâ€ and takes the stem 10 and 20\n\nSo this rule runs twice: once with the value 10 then with the value 20.\n\n$&lt; is an autmatic variable that expands into â€œsrc/models/fit_lda.pyâ€\nBuilt-in Make text function, $(word n,text) , returns the nth word of text. (see Misc &gt;&gt; Resources for function docs)\n\nThe legitimate values of n start from 1. If n is bigger than the number of words in text, the value is empty\nIn this example, itâ€™s used to return the 2nd prerequisite to become the 1st argument of the fit_lda.py script\n\n1st arg is the input file path\n\n\n$@ is an automatic variable that expands into the target name which becomes the 2nd argument of the fit_lda.py script\n\n2nd arg is the output file path\n\n--topics is a function option for fit_ldy.py which is defined in the script using decorators from {{click}}\n\n$* is the stem of the wildcard which is a numeric in this case and provides the value the topics flag\n\n\nExample Clean text files in data directory\ndata/processed/%.txt: data/raw/%.txt\nsed 1,20d $^ &gt; $@\n\nTakes all text files in the raw directory, removes some rows (sed 1,20d), outputs (&gt;) the processed file into target with the same file name ($@)"
  },
  {
    "objectID": "qmd/google-analytics-analysis.html",
    "href": "qmd/google-analytics-analysis.html",
    "title": "42Â  Google, Analytics, Analysis",
    "section": "",
    "text": "TOC\n\nMisc\nAnalysis\n\nMisc\n\nPackages\n\n{googleAnalyticsR}\n\n\nAnalysis\n\nQuestions\n\nWhich cities represent the highest mobile traffic?\nWhich landing pages are most popular for mobile users?\nWhat is the average session duration for mobile users?\nDo converters spend more time on the site and consume more content than non-converters?\nWhich languages are converters most likely to speak?\nHow many Android sessions came from converters versus non-converters?\nhow is product quality affecting user retention? (Example 8)\n has moved in the negative direction, find the root cause (Example 9)\nHow do you measure the success or health of ? (Example 10)\nHow many users have installed your app? (Example 11)\nBy percentage, what are the top 10 countries that our app users are located in? (Example 12)\nBy percentage, which device do most users use our app on? (Example 13)\nHow many users are actively using the app on a daily basis? (Example 14)\nHow many users are actively using the app on a daily basis during the month of September? (Example 15)\nHow much are users in the United States spending on the app? (Example 16)\nHow many users are encountering app crashes? (Example 17)\nHow many users are uninstalling the app? (Example 18)\nAre crashes possibly affecting the user experience, causing them to uninstall? (Example 19)\nWhich webpages are viewed most often?? (Example 20)\nPage Transition Matrix (Example 21)\nSession Conversion Rate/User Conversion Rate Calculation (Example 22)\nSignal Searching: Find features that are associated with conversion (Example 23)\nPower User Analysis (Example 24)\nDiagnosing path lengths in user journeys (Example 25)\nSegment Users Purchases as New Customers or Returning Customers (Example 26)\n\nExample 1 (total events and unique events)\n\nPerson 1: clicks the â€œadd to cart buttonâ€ 4 times during 1 session\nPerson 2: clicks the â€œadd to cart buttonâ€ 2 times during 1 session\nGoogle Report\n\ntotal events: 6 (i.e.Â the button was pressed 6 times)\nunique events: 2 (i.e.Â the button was pressed in 2 different sessions)\nevent category: product card, event action: click, event label: add to cart\n\n\nExample 2 (unique dimension combinations; same)\n\nThe quick view event of â€œCap Blackâ€ has been fired 1045 times but only 121 of those unique events\nA view filtered by Category, Action, and Label has the same values for unique events, sessions, and unique dimension combinations\n\nIndex = Label\n\n\nExample 3 (unique dimension combinations; different)\n\nThink this is a breakdown of 1 session/user\n(Variables or top) The view is filter by Page and Category, Action, and Label\n\nUnique Events and Unique Dimension Combinations match\n\n(right or bottom) The view is filter by Page and only Category and Action\n\nUnique Events and Unique Dimension Combinations do NOT match\n\nInterpretation\n\nLeft or Top (index = page, event label)\n\nTotal Unique Events answers, â€œHow many total unique quickviews (event) were there per label per page?â€\n\nLabels can be repeatedly counted, just not on the same page\n\nTotal Unique Dimension Combinations answers, ditto\n\nRight or Bottom (index = page)\n\nTotal Unique Events still answers the same question as above as if event label were included in the view\nTotal Unique Dimension Combinations answers, â€œHow many total unique pages was a quickview (event) used?â€\n\n\n\nExample 4 (multiple users)\n\nSame basic scenario as example 3 (right or bottom pic) except itâ€™s for 2 people\n\nSame calculations for Unique Events and Unique Dimension Combinations but those calculations for each user/session are summed together\n\n\nExample 5 (Index = Device)\n\nQuestion: â€œHow many people interact with products in general by device?â€\nAnswer: 2480 (Unique Dimension Combinations)\n\nTotal Events: how many interactions overall per device?\nUnique Events: how many unique product and unique user interaction combinations are there per device?\nUnique Dimension Combinations: How many unique users are interacting with a product (i.e.Â products in general) per device?\n\n\nExample 6 (Interaction Rate, Avg. Clicks)\n\nFind the interaction rate and average clicks for the â€œAdd to Cartâ€ button on the Sock product page\nInteraction Rate\n\nSteps\n\nCreate a custom report with Page, Event Category, Event Action, Event Label\n\nFor Page = â€œSockâ€ there are 100 sessions (i.e.Â the number of people who saw the â€œAdd to Cartâ€ button)\n\nThis wasnâ€™t covered in detail, but in the examples, session always equaled unique dim combos. So Iâ€™m guessing this number and the one below arenâ€™t on the same lines of the report.\n\nFor Page = â€œSockâ€ and Event Action = â€œAdd to Cart Clickâ€ there are 80 Unique Dimension Combinations.\n\nCalculate IR = (80 * 100) / 100 = 80%\n\n\nAverage Click Rate\n\nCalculate avg_cr = total events / unique dimension combinations\n\n\nExample 7 (Funnel Test)\n\nOptimize a contact form on your website\nLeft Panel &gt;&gt; Explore &gt;&gt; Funnel Exploration\nHypothesized Funnel\n\nPeople enter your website through page X.\n\nYou suspect that after reading this page, they want to contact you so they visit the contact page\n\nPeople visit the contact page\nComplete contact form\n\nyouâ€™ve implemented an event inÂ Google Analytics that measure the form completion\n\n\nCompare segments in the Hypothesized Funnel where users did complete form and didnâ€™t complete the form\n\nThe differences should show how you can optimize so that more people complete a form.\n\n\nExample 8\n\nhow is product quality affecting user retention?\nLeft Panel &gt;&gt; Explore &gt;&gt; Cohort Exploration\n\nDefine a cohort that includes users who experienced an error and subsequently uninstalled your app.\n\nSteps:\n\nFor Cohort inclusion, under Others, choose a custom error event youâ€™ve defined.\nFor Return criteria, under Others, choose the app_remove event.\n\n\nExample 9\n\n has moved in the negative direction, find the root cause\nExamples\n\nCreation of facebook user groups is down 20%. What will you do?\nAvg ETA is up by 3 minutes. How do you investigate the problem?\n\nSolution framework\n\nClarify question/metrics\n\nExample: Avg ETA is up by 3 min\n\nHow is the start time and end time defined?\n\nIs the start time at the time of initial request by the customer or when the customer enters the vehicle\n\n\n\nExamine the time aspect of the problem\n\nDid the metric change suddenly or progressively?\nInternal issue? - bad data source, problem with data collection, bug in a processing script\nExternal issue? - seasonality, competitor new product or feature, industry trend, special event, natural disaster\n\nHave other related features/products in the company experienced the same change?\n\nWas there a recent change in the feature or product line?\n\nSegment by demographic, region, device, platform, language, etc.\nDecompose the metric and repeat steps 1-4 on each submetric\n\ne.g.Â daily active users (DAU) = existing users + resurrected users + new users - churned users\n\nSummarize approach\n\nList most probable causes and recommended solutions\n\n\n\nExample 10\n\nHow do you measure the success or health of ?\nExamples\n\nHow would measure the health of Mentions (Facebook celebrity app)? How can Facebook determine if itâ€™s worth keeping it?\nInstagram is launching a new feature (links to products). How do you tell if itâ€™s doing well?\nA job site wants to know how to measure the success of the job recommendations.\n\nSolution framework\n\nClarify the function or goal of the feature/product\n\n** Always do this one **\nWhat does it do? How is it used? Who is it for?\n\nDefine metrics (no more than 3)\n\nRecommended 2 success, 1 guardrail (metric that shouldnâ€™t degrade when a new feature is added)\n\nsuccess examples: daily active users, number of bookings, conversion rate, click-through-rate of recommendation results, percentage of users who ended up applying for a clicked-on job recommendation (sounds like a type of conversion rate)\nguardrail examples: cancellation rate, bounce rate\n\n\n\n\nExample 11\n\nHow many users have installed your app?\n\n\nSELECT COUNT(DISTINCT user_pseudo_id) as users_installed\nFROM `firebase-public-project.analytics_153293282.events_*`\nWHERE event_name = \"first_open\"\nÂ  Â  AND _TABLE_SUFFIX BETWEEN '20180927' and '20181003'\n\nBigQuery SQL + Firebase activate app\nâ€œ*â€ - tables come in an intraday format (one table for each day), you can combine several tables in one query using the wildcard symbol\n\nBest to combine with _TABLE_SUFFIX in order to include a date range, because if you have a lot of data, â€œ*â€ can be very expensive to execute without it\n\nDaily breakdown of the number of app installations\n\nSELECT\nÂ  FORMAT_DATE('%Y-%m-%d', PARSE_DATE('%Y%m%d', event_date)) AS date,\nÂ  COUNT(DISTINCT user_pseudo_id) as users_installed\nFROM `firebase-public-project.analytics_153293282.events_*`\nWHERE event_name = \"first_open\"\nÂ  AND _TABLE_SUFFIX BETWEEN '20180927' and '20181003'\nGROUP BY date\nORDER BY date\n\nOutputs: date and users_installed\nExample 12\n\nBy percentage, what are the top 10 countries that our app users are located in?\n\n\nWITH\n--Compute for the numerators\ncountry_counts AS (\nÂ  Â  SELECT\nÂ  Â  Â  geo.country,\nÂ  Â  Â  COUNT(DISTINCT user_pseudo_id) AS users\nÂ  Â  FROM `firebase-public-project.analytics_153293282.events_*`\nÂ  Â  WHERE event_name = \"first_open\"\nÂ  Â  Â  AND _TABLE_SUFFIX BETWEEN '20180927' and '20181003'\nÂ  Â  Â  AND geo.country &lt;&gt; \"\"\nÂ  Â  GROUP BY geo.country\n),\n--Compute for the denominators\nuser_counts AS (\nÂ  Â  SELECT\nÂ  Â  Â  COUNT(DISTINCT user_pseudo_id)\nÂ  Â  FROM `firebase-public-project.analytics_153293282.events_*`\nÂ  Â  WHERE event_name = \"first_open\"\nÂ  Â  Â  AND _TABLE_SUFFIX BETWEEN '20180927' and '20181003'\n),\n--Compute for the percentages\npercent AS (\nÂ  Â  SELECT\nÂ  Â  Â  country,\nÂ  Â  ROUND(users / (SELECT * FROM user_counts), 4) AS percent_users\nÂ  Â  FROM country_counts\n)\n\nSELECT * FROM percent\nORDER BY percent_users DESC\nLIMIT 10\n\nBigQuery SQL + Firebase activated App\nOutputs: country and percent_users\nâ€œ*â€ - tables come in an intraday format (one table for each day), you can combine several tables in one query using the wildcard symbol\n\nBest to combine with _TABLE_SUFFIX in order to include a date range, because if you have a lot of data, â€œ*â€ can be very expensive to execute without it\n\nExample 13\n\nBy percentage, which device do most users use our app on?\n\n\nWITH\ndevice_counts AS (\nÂ  Â  SELECT\nÂ  Â  Â  device.category,\nÂ  Â  Â  COUNT(DISTINCT user_pseudo_id) AS users\nÂ  Â  FROM `firebase-public-project.analytics_153293282.events_*`\nÂ  Â  WHERE event_name = \"first_open\"\nÂ  Â  Â  AND _TABLE_SUFFIX BETWEEN '20180927' and '20181003'\nÂ  Â  Â  AND device.category &lt;&gt; \"\"\nÂ  Â  GROUP BY device.category\n),\nuser_counts AS (\nÂ  Â  SELECT\nÂ  Â  Â  COUNT(DISTINCT user_pseudo_id)\nÂ  Â  FROM `firebase-public-project.analytics_153293282.events_*`\nÂ  Â  WHERE event_name = \"first_open\"\nÂ  Â  Â  AND _TABLE_SUFFIX BETWEEN '20180927' and '20181003'\n),\npercent AS (\nÂ  Â  SELECT\nÂ  Â  Â  category,\nÂ  Â  Â  ROUND(users / (SELECT * FROM user_counts), 4) AS percent_users\nÂ  Â  FROM device_counts\n)\n\nSELECT * FROM percent\nORDER BY percent_users DESC\n\nBigQuery SQL + Firebase activated App\nOutputs: category and percent_users\nâ€œ*â€ - tables come in an intraday format (one table for each day), you can combine several tables in one query using the wildcard symbol\n\nBest to combine with _TABLE_SUFFIX in order to include a date range, because if you have a lot of data, â€œ*â€ can be very expensive to execute without it\n\nExample 14\n\nHow many users are actively using the app on a daily basis?\n\n\nWITH\ndaily_user_count AS (\nSELECT\nÂ  FORMAT_DATE('%Y-%m-%d', PARSE_DATE('%Y%m%d', event_date)) AS date,\nÂ  COUNT(DISTINCT user_pseudo_id) AS active_users\nFROM `firebase-public-project.analytics_153293282.events_*`\nWHERE event_name = \"user_engagement\"\nÂ  AND _TABLE_SUFFIX BETWEEN '20180901' and '20180930'\nGROUP BY date\n)\n\nSELECT AVG(active_users) AS daily_active_users\nFROM daily_user_count\n\nBigQuery SQL + Firebase activated App\nOutputs: category and percent_users\nâ€œ*â€ - tables come in an intraday format (one table for each day), you can combine several tables in one query using the wildcard symbol\n\nBest to combine with _TABLE_SUFFIX in order to include a date range, because if you have a lot of data, â€œ*â€ can be very expensive to execute without it\n\nExample 15\n\nHow many users are actively using the app on a daily basisd during the month of September?\n\n\nWITH\ndaily_user_count AS (\nSELECT\nÂ  FORMAT_DATE('%Y-%m-%d', PARSE_DATE('%Y%m%d', event_date)) AS date,\nÂ  COUNT(DISTINCT user_pseudo_id) AS active_users\nFROM `firebase-public-project.analytics_153293282.events_*`\nWHERE event_name = \"user_engagement\"\nÂ  AND _TABLE_SUFFIX BETWEEN '20180901' and '20180930'\nGROUP BY date\n)\nSELECT AVG(active_users) AS daily_active_users\nFROM daily_user_count\n\nOutputs: daily_active_users\nAlso Reports &gt;&gt; Lifecycle &gt;&gt; Engagement\nDAU calculation depends on definition of Active User\n\nHere Active User = a user who did any â€œuser_engagementâ€ action in the day\nAlso see Example 9, Segments, Dimensions, and Metrics &gt;&gt; Metrics\n\nBigQuery SQL + Firebase activated App\nOutputs: category and percent_users\nâ€œ*â€ - tables come in an intraday format (one table for each day), you can combine several tables in one query using the wildcard symbol\n\nBest to combine with _TABLE_SUFFIX in order to include a date range, because if you have a lot of data, â€œ*â€ can be very expensive to execute without it\n\nExample 16\n\nHow much are users in the United States spending on the app?\n\n\nSELECT SUM(user_ltv.revenue) AS revenue\nFROM `firebase-public-project.analytics_153293282.events_*`\nWHERE event_name = \"in_app_purchase\"\nÂ  AND geo.country = \"United States\"\nÂ  AND _TABLE_SUFFIX BETWEEN '20180901' and '20180930'\n\nBigQuery SQL + Firebase activated App\nOutputs: revenue\nâ€œ*â€ - tables come in an intraday format (one table for each day), you can combine several tables in one query using the wildcard symbol\n\nBest to combine with _TABLE_SUFFIX in order to include a date range, because if you have a lot of data, â€œ*â€ can be very expensive to execute without it\n\nExample 17\n\nHow many users are encountering app crashes?\n\n\nSELECT COUNT(DISTINCT user_pseudo_id) AS users\nFROM `firebase-public-project.analytics_153293282.events_*`,\nUNNEST(event_params) e\nWHERE event_name = 'app_exception'\nÂ  AND _TABLE_SUFFIX BETWEEN '20180901' and '20180930'\nÂ  AND e.key = 'fatal' AND e.value.int_value = 1\n\nBigQuery SQL + Firebase activated App\nOutputs: users\nâ€œ*â€ - tables come in an intraday format (one table for each day), you can combine several tables in one query using the wildcard symbol\n\nBest to combine with _TABLE_SUFFIX in order to include a date range, because if you have a lot of data, â€œ*â€ can be very expensive to execute without it\n\nExample 18\n\nHow many users are uninstalling the app?\n\n\nWITH\n--List of users who installed in Sept\nsept_cohort AS (\nSELECT DISTINCT user_pseudo_id,\nFORMAT_DATE('%Y-%m-%d', PARSE_DATE('%Y%m%d', event_date)) AS date_first_open,\nFROM `firebase-public-project.analytics_153293282.events_*`\nWHERE event_name = 'first_open'\nAND _TABLE_SUFFIX BETWEEN '20180901' and '20180930'\n),\n--Get the list of users who uninstalled\nuninstallers AS (\nSELECT DISTINCT user_pseudo_id,\nFORMAT_DATE('%Y-%m-%d', PARSE_DATE('%Y%m%d', event_date)) AS date_app_remove,\nFROM `firebase-public-project.analytics_153293282.events_*`\nWHERE event_name = 'app_remove'\nAND _TABLE_SUFFIX BETWEEN '20180901' and '20181007'\n),\n--Join the 2 tables and compute for # of days to uninstall\njoined AS (\nSELECT a.*,\nb.date_app_remove,\nDATE_DIFF(DATE(b.date_app_remove), DATE(a.date_first_open), DAY) AS days_to_uninstall\nFROM sept_cohort a\nLEFT JOIN uninstallers b\nON a.user_pseudo_id = b.user_pseudo_id\n)\n--Compute for the percentage\nSELECT\nCOUNT(DISTINCT\nCASE WHEN days_to_uninstall &gt; 7 OR days_to_uninstall IS NULL THEN user_pseudo_id END) /\nCOUNT(DISTINCT user_pseudo_id)\nAS percent_users_7_days\nFROM joined\n\nBigQuery SQL + Firebase activated App\nOutputs: percent_users_7_days\n\nthe percentage of users who still have the app after a week, among the cohort who installed it in September\n\ni.e.Â 7-day retention rate among September installers\n\nGood or Bad?\n\nResearch industry benchmarks\nMonitoring how the trend progresses over time\n\n\nâ€œ*â€ - tables come in an intraday format (one table for each day), you can combine several tables in one query using the wildcard symbol\n\nBest to combine with _TABLE_SUFFIX in order to include a date range, because if you have a lot of data, â€œ*â€ can be very expensive to execute without it\n\nExample 19\n\nAre crashes possibly affecting the user experience, causing them to uninstall?\n\n\nWITH\n--List of users who installed in Sept\nsept_cohort AS (\nSELECT DISTINCT user_pseudo_id,\nFORMAT_DATE('%Y-%m-%d', PARSE_DATE('%Y%m%d', event_date)) AS date_first_open,\nFROM `firebase-public-project.analytics_153293282.events_*`\nWHERE event_name = 'first_open'\nAND _TABLE_SUFFIX BETWEEN '20180901' and '20180930'\n),\n--Get the list of users who uninstalled\nuninstallers AS (\nSELECT DISTINCT user_pseudo_id,\nFORMAT_DATE('%Y-%m-%d', PARSE_DATE('%Y%m%d', event_date)) AS date_app_remove,\nFROM `firebase-public-project.analytics_153293282.events_*`\nWHERE event_name = 'app_remove'\nAND _TABLE_SUFFIX BETWEEN '20180901' and '20181007'\n),\n--Get the list of users who experienced crashes\nusers_crashes AS (\nSELECT DISTINCT user_pseudo_id,\nFORMAT_DATE('%Y-%m-%d', PARSE_DATE('%Y%m%d', event_date)) AS date_crash,\nFROM `firebase-public-project.analytics_153293282.events_*`,\nUNNEST(event_params) e\nWHERE event_name = 'app_exception'\nAND _TABLE_SUFFIX BETWEEN '20180901' and '20181007'\nAND e.key = 'fatal' AND e.value.int_value = 1\n),\n--Join the 3 tables\njoined AS (\nSELECT a.*,\nb.date_app_remove,\nDATE_DIFF(DATE(b.date_app_remove), DATE(a.date_first_open), DAY) AS days_to_uninstall,\nc.date_crash\nFROM sept_cohort a\nLEFT JOIN uninstallers b\nON a.user_pseudo_id = b.user_pseudo_id\nLEFT JOIN users_crashes c\nON a.user_pseudo_id = c.user_pseudo_id\n)\n--Compute the percentage\nSELECT\nCOUNT(DISTINCT\nCASE WHEN days_to_uninstall &lt;= 7 AND date_crash IS NOT NULL\nTHEN user_pseudo_id END)\n/ COUNT(DISTINCT\nCASE WHEN days_to_uninstall &lt;= 7 THEN user_pseudo_id END)\nAS percent_users_crashes\nFROM joined\n\nBigQuery SQL + Firebase activated App\nOutputs: percent_users_crashes\n\nmay be useful to compare it versus non-installers to make a more reasonable conclusion\ncreate an app removal rate model to determine the predictors of app removal\n\nâ€œ*â€ - tables come in an intraday format (one table for each day), you can combine several tables in one query using the wildcard symbol\n\nBest to combine with _TABLE_SUFFIX in order to include a date range, because if you have a lot of data, â€œ*â€ can be very expensive to execute without it\n\nExample 20\n\nWhich webpages are viewed most often?\n\n\n-- pulling user page views from GA4 events\nWITH base_table AS (\n-- pulls relevant columns from relevant dates to decrease the size of data scanned\nÂ  SELECT\nÂ  Â  event_name,\nÂ  Â  event_date,\nÂ  Â  event_timestamp,\nÂ  Â  user_pseudo_id,\nÂ  Â  user_id,\nÂ  Â  device,\nÂ  Â  geo,\nÂ  Â  traffic_source,\nÂ  Â  event_params,\nÂ  Â  user_properties\nÂ  FROM\nÂ  Â  `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_*`\nÂ  WHERE\nÂ  Â  _table_suffix &gt;= '20210101'\nÂ  AND event_name IN ('page_view')\n)\n-- unnests event parameters to get to relevant keys and values\n, unnested_events AS (\nÂ  SELECT\nÂ  Â  event_date AS date,\nÂ  Â  event_timestamp AS event_timestamp_microseconds,\nÂ  Â  user_pseudo_id,\nÂ  Â  MAX(CASE WHEN c.key = 'ga_session_id' THEN c.value.int_value END) AS visitID,\nÂ  Â  MAX(CASE WHEN c.key = 'ga_session_number' THEN c.value.int_value END) AS visitNumber,\nÂ  Â  MAX(CASE WHEN c.key = 'page_title' THEN c.value.string_value END) AS page_title,\nÂ  Â  MAX(CASE WHEN c.key = 'page_location' THEN c.value.string_value END) AS page_location\nÂ  FROMÂ \nÂ  Â  base_table,\nÂ  Â  UNNEST (event_params) c\nÂ  GROUP BY 1,2,3\n)\nSELECT\nÂ  page_title,\nÂ  COUNT(*) as event_count,\nÂ  COUNT(DISTINCT user_pseudo_id) as users\nFROMÂ \nÂ  unnested_events\nGROUP BY 1\nORDER BY 2 DESC\n\nFrom Looking for Power User Journeys\nView of nesting schema for event_params\n\nEach event_param has a key (name of event) and a value\n\n\nExample 21\n\nPage Transition Matrix\n\n\n-- pulling user page views from GA4 events\nWITH base_table AS (\n-- pulls relevant columns from relevant dates to decrease the size of data scanned\nÂ  SELECT\nÂ  Â  event_name,\nÂ  Â  event_date,\nÂ  Â  event_timestamp,\nÂ  Â  user_pseudo_id,\nÂ  Â  user_id,\nÂ  Â  device,\nÂ  Â  geo,\nÂ  Â  traffic_source,\nÂ  Â  event_params,\nÂ  Â  user_properties\nÂ  FROM\nÂ  Â  `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_*`\nÂ  WHERE\nÂ  Â  _table_suffix &gt;= '20210101'\nÂ  AND event_name IN ('page_view')\n)\n, unnested_events AS (\n-- unnests event parameters to get to relevant keys and values\nÂ  SELECT\nÂ  Â  event_date AS date,\nÂ  Â  event_timestamp AS event_timestamp_microseconds,\nÂ  Â  user_pseudo_id,\nÂ  Â  MAX(CASE WHEN c.key = 'ga_session_id' THEN c.value.int_value END) AS visitID,\nÂ  Â  MAX(CASE WHEN c.key = 'ga_session_number' THEN c.value.int_value END) AS visitNumber,\nÂ  Â  MAX(CASE WHEN c.key = 'page_title' THEN c.value.string_value END) AS page_title,\nÂ  Â  MAX(CASE WHEN c.key = 'page_location' THEN c.value.string_value END) AS page_location\nÂ  FROMÂ \nÂ  Â  base_table,\nÂ  Â  UNNEST (event_params) c\nÂ  GROUP BY 1,2,3\n)\n, unnested_events_categorised AS (\n-- categorizing Page Titles into PDPs and PLPs\nÂ  SELECT\nÂ  *,\nÂ  CASE WHEN ARRAY_LENGTH(SPLIT(page_location, '/')) &gt;= 5Â \nÂ  Â  Â  Â  Â  Â  AND\nÂ  Â  Â  Â  Â  Â  CONTAINS_SUBSTR(ARRAY_REVERSE(SPLIT(page_location, '/'))[SAFE_OFFSET(0)], '+')\nÂ  Â  Â  Â  Â  Â  AND (LOWER(SPLIT(page_location, '/')[SAFE_OFFSET(4)]) INÂ \nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ('accessories','apparel','brands','campus+collection','drinkware',\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'electronics','google+redesign',\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'lifestyle','nest','new+2015+logo','notebooks+journals',\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'office','shop+by+brand','small+goods','stationery','wearables'\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  )\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  OR\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  LOWER(SPLIT(page_location, '/')[SAFE_OFFSET(3)]) INÂ \nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ('accessories','apparel','brands','campus+collection','drinkware',\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'electronics','google+redesign',\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'lifestyle','nest','new+2015+logo','notebooks+journals',\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'office','shop+by+brand','small+goods','stationery','wearables'\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  )\nÂ  Â  Â  Â  Â  Â  )\nÂ  Â  Â  Â  Â  Â  THEN 'PDP'\nÂ  Â  Â  Â  Â  Â  WHEN NOT(CONTAINS_SUBSTR(ARRAY_REVERSE(SPLIT(page_location, '/'))[SAFE_OFFSET(0)], '+'))\nÂ  Â  Â  Â  Â  Â  AND (LOWER(SPLIT(page_location, '/')[SAFE_OFFSET(4)]) INÂ \nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ('accessories','apparel','brands','campus+collection','drinkware',\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'electronics','google+redesign',\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'lifestyle','nest','new+2015+logo','notebooks+journals',\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'office','shop+by+brand','small+goods','stationery','wearables'\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  )\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  ORÂ \nÂ  Â  Â  Â  Â  Â  Â  Â  Â  LOWER(SPLIT(page_location, '/')[SAFE_OFFSET(3)]) INÂ \nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ('accessories','apparel','brands','campus+collection','drinkware',\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'electronics','google+redesign',\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'lifestyle','nest','new+2015+logo','notebooks+journals',\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'office','shop+by+brand','small+goods','stationery','wearables'\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  )\nÂ  Â  Â  Â  Â  Â  )\nÂ  Â  Â  Â  Â  Â  THEN 'PLP'\nÂ  Â  Â  Â  ELSE page_title\nÂ  Â  Â  Â  END AS page_title_adjustedÂ \nÂ  FROMÂ \nÂ  Â  unnested_events\n)\n, ranked_screens AS (\n-- prepares additional data points for analytics to understand transitions between the previous, current and following pages\nÂ  SELECT\nÂ  Â  *,\nÂ  Â  DENSE_RANK() OVER (PARTITION BY user_pseudo_id, visitID ORDER BY event_timestamp_microseconds ASC) page_rank,\nÂ  Â  LAG(page_title_adjusted,1) OVER (PARTITION BY user_pseudo_id, visitID ORDER BY event_timestamp_microseconds ASC) previous_page,\nÂ  Â  LEAD(page_title_adjusted,1) OVER (PARTITION BY user_pseudo_id, visitID ORDER BY event_timestamp_microseconds ASC)Â  next_page\nÂ  FROMÂ \nÂ  Â  unnested_events_categorised\n)\n, screen_summary AS (\n-- check the last page number viewed on a session\n-- aggregate all screens per session, which will be helpful in identifying power users\nÂ  SELECT\nÂ  Â  *,\nÂ  Â  MAX(page_rank) OVER (PARTITION BY user_pseudo_id, visitID) last_page_rank,\nÂ  Â  ARRAY_AGG(page_title_adjusted) OVER (PARTITION BY user_pseudo_id, visitID) pages_on_a_visit\nÂ  FROMÂ \nÂ  Â  ranked_screens\n)\n, screen_summary_agg AS (\n-- cleans up pages_on_a_visit field\nÂ  SELECT * EXCEPT(pages_on_a_visit),\nÂ  Â  ARRAY_TO_STRING(ARRAY(SELECT DISTINCT * FROM UNNEST(pages_on_a_visit) ORDER BY 1 ASC), '&gt;&gt;') AS screens_on_a_visit\nÂ  FROMÂ \nÂ  Â  screen_summary\n)\n\nSELECT\n-- first time a page is viewed in a session\nÂ  *,\nÂ  MIN(page_rank) OVER (PARTITION BYÂ  user_pseudo_id, visitID, page_title) AS page_rank_session_first_seen\nFROMÂ \nÂ  screen_summary_agg\n\nA Page Transition Matrix shows the number of transitions from top-n pages (row) to top-n pages (column)\nIn rows, transition matrix has the â€œcurrentâ€ page of reference, or page_title_adjusted as rows of a Pivot Table, and in columns â€” a page that follows, or next_page. The values are the number of transitions, record_count from row to column. (see right side panel) \n\nCTE column locations\n\npage_title_adjusted â€“&gt; unnested_events_categorised\nnext_page â€“&gt; ranked_screens\nrecord_count â€“&gt; not in query\n\nExample: if a user lands on, for example, a PLP or Store Search Result page, where would they likely go next.\n\nThere were 40,697 transitions from PLP to null page, which is an exit from the website\nA higher share of users browse from PLP to PLP 62,533/140,608=44%, and less â€” from PLP to PDP 21,633/140,608=15%, which means that users browse Product Listing Pages without showing a significant interest to click on a PLP item. (very bad if typical ecommerce site)\n\n\nMisc\n\nFrom Looking for Power User Journeys\nGrouping by user_pseudo_id, visitID constitutes a session\nIf you want percent of total transitions by starting page (i.e.Â row wise total), see article\n\nCTE\n\nbase_table\n\npulls relevant columns from relevant dates to decrease the size of data scanned\n\nunnested_events\n\nunnests event parameters to get to relevant keys and values\n\nSee previous example for schema of event_params\n\n\nunnested_events_categorized\n\ncategorizing Page Titles into PDPs and PLPs\nCreates a new categorical column, â€œpage_title_adjusted,â€ that has value, â€œPDP,â€ when a substring in â€œpage_locationâ€ is one of a set of words, and â€œPLPâ€ when itâ€™s not, and the value of page_title otherwise.\n\nPLP is the product listing page and PDP is the product details page (i.e.Â items on the PLP)\nSee SQL &gt;&gt; Strings for details on the various functions that are used\n\n\nranked_screens\n\nprepares additional data points for analytics to understand transitions between the previous, current and following pages\nWithin each user session\n\nwebpages are ranked by the event (page visit) timestamp\nvariables for previous and next webpage visit are created\n\n\nscreen_summary\n\nFinds last webpage viewed per session\nThink ARRAY_AGG puts all the webpage names in a list/array for each session\n\nscreen_summary_agg\n\nARRAY_TO_STRING coerces the list/array of webpage names to a string with a â€œ&gt;&gt;â€ delimiter between array values\n\n\nFinal query\n\nCreates column for the first time each page is viewed in each session\n\nNote that it also groups by â€œpage_titleâ€ instead of just the session variables, â€œuser_pseudo_idâ€ and visitID.\nIt should be a rank value. The rank indicates where itâ€™s first seen in the user journey.\n\ni.e.Â low rank = beginning of sesssion, high rank = end of session\n\nUsed in Example 25\n\n\nExample 22\n\nSession Conversion Rate/User Conversion Rate Calculation (Page View)\n\nFrom Looking for Power User Journeys\nis_checkout_session: CONTAINS_TEXT(screens_on_visit, 'Checkout Confirmation')\n\nIndicator variable that indicates users that have converted\nIf the checkout page isnâ€™t tracked, you can explore events outside of screen_view, such as ecommerce_purchase or purchase, checkout_progress, etc.\n\nSession Conversion Rate: COUNT_DISTINCT(IF(is_checkout_session, session_id, null))/COUNT_DISTINCT(_session_id)Â \nUser Conversion Rate: COUNT_DISTINCT(IF(is_checkout_session, user_pseudo_id, null))/COUNT_DISTINCT(user_pseudo_id)\n\n\nExample 23\n\nSignal Searching: Find features that are associated with conversion\nFrom Looking for Power User Journeys\nQuestions\n\nHow many users do it?\n\nYou can see whether or not enough users are engaging with the page or a feature and if thereâ€™s a potential for improvement\n\ni.e.Â if almost all users are doing it, then there wonâ€™t be much room for improvement.\n\nYouâ€™ll also need a decent sample size if youâ€™re measuring correlation. Since this is ecommerce, I doubt this will be a problem.\n\nHow their conversion looks like vs the users who donâ€™t do what you want them to do\n\nso you can hypothesize whether or not engaging more users with this part of a journey can have a positive impact on the business, or on conversion in this case.\n\n\nAnswer template\n\nâ€œUsers who do X tend to have a # times higher conversion rate compared to users who donâ€™t do X. That said, thereâ€™s only Y percent of users who do X. We can either analyze the behavior of those users and try to attract more similar users to use our product, or funnel more users to do X.â€\n\nProcess:\n\nFilter conversion metrics (See example 22) by page or feature\nExamine conversion range between those that visit the page/use the feature and those that donâ€™t\n\nExample: Over this period,\n\nBetween 3.97 â€“ 5.73% of users visit the â€œStore search resultsâ€ page (a proxy for search), but\n\ntheir user conversion rateÂ  is 0.9 - 7.3%\n\nUsers who donâ€™t visit that page have a user conversion rate between 0.5% - 2.4%\nMe: I think with this webpage/feature it would be safe to ignore the time aspect and perform difference-in-means test or a cohenâ€™s d standardized test to see if and how much these conversion rates differ.\n\nAlthough, viewing as line chart can indicate when various periods of seasonality for product are.\n\n\n\nWhen you find a sizable difference between these groups. Ask:\n\nWhat else makes those users different?\nWhat else they do on the app?\nWhere they come from?\nHow can you help them connect with the product they may be interested in?\n\nBeware: making the page visitation/feature usage mandatory (e.g.Â a user must visit the login page before purchasing) usually will decrease conversion\n\n\n\nCan also look at webpages/features that adversely affect conversion. (e.g.Â â€œPage Unavailableâ€)\n\nMight be useful to see how these events affect CLV, retention, etc.\n\n\nExample 24\n\nPower User Analysis\n\n% users vs views\n\nShows what percent of users view a page/feature vs number of views for a given time period\n\n\nFrom Looking for Power User Journeys\nAlso see Product Development &gt;&gt; Metrics &gt;&gt; Growth Metrics &gt;&gt; Power User Curve\nYou either want the mode of these charts to shift to the right or you want a smile pattern with modes on the far left and far right\n\nUsers in the far right mode are considered â€œpower users.â€\n\nIdentify features correlated with conversion (example 23) and moniter these featuresâ€™ power user curves to measure progress in producing more power users and therefore more conversions\n\nCan also be broken down be cohorts (see Algorithms, Product &gt;&gt; Cohort Analysis)\n\nProduct Details Pages (PDP)\n\nAround 50% of users in January 2021 viewed 2 PDPs\nSee example 21 &gt;&gt; CTE &gt;&gt; unnested_events_categorized for SQL code to create a PDP category. (collapses various product detail webpages into a PDP category)\nAn online business wants mode of this chart to move to the right (i.e.Â funnel users to PDP pages), since view of PDPs are positively correlated with conversion\n\n\nExample 25\n\nDiagnosing path lengths in user journeys\n\nLong user journeys may be because of active browsing or it may be a sign of a user getting lost\n\nFrom Looking for Power User Journeys\nWhen a user first visits a webpage, which is necessary for conversion (e.g.Â Product Listing Page), towards the end of a long journey, then something has likely gone wrong.\n\ni.e.Â theyâ€™re likely have trouble finding the PLP.\n\nSee Example 21 &gt;&gt; final query for code that ranks each webpage in the user journey according to when itâ€™s first visited. (most of the cleaning code in the CTE will also likely be required.)\nPLP: % session vs first seen rank during a session\n\nY axis: % of sessions\nX axis: â€œpage_rank_session_first_seenâ€\n\nPotential ranks for a webpage during a session\nLow ranks = the webpage is first viewed towards the beginning of the session\nHigh ranks = the webpage is first viewed towards the end of the session\n\n** Note that the ranks on the x-axis are ordered according to their y-axis values (highest to lowest) **\n\nSo donâ€™t interpret the chart as a distribution\n\nInterpretation: a large proportion of user sessions are viewing the PLP very early in the user journey, theyâ€™re likely landing on this page from paid search (e.g.Â Google)\n\ni.e.Â the ranks with the highest % session values are low ranks (Top 5 ranks are 1,3,4,5,6)\n\n\nStore Search Page: % session vs first seen rank during a session\n\nSame axis descriptions as in PLP example\nInterpretation: The search page is visited mostly as the 3rd, 4th, or 5th page visited, so itâ€™s early enought that users are likely not having a tough time finding it and may and prefer to engage with it rather than browse the navigation bar.\n\nCheckout page: % session vs first seen rank during a session\n\nSame axis descriptions as in PLP example\nInterpretation: There is large range (10 to 43) of relatively equally likely path lengths before conversion\n\n\nExample 26: Segment User Purchases as New Customers or Returning Customers\n\nCustomers are users that have purchased (i.e.Â converted)\nData is Google Merchandise Store\nBuild a base table with orders by customer and date\n\n\nWITH base_table AS (\n-- orders by customer and dateÂ  Â \nÂ  Â  SELECT\nÂ  Â  Â  user_pseudo_id AS user_id,\nÂ  Â  Â  ecommerce.transaction_id AS order_id,\nÂ  Â  Â  PARSE_DATE('%Y%m%d', event_date) AS order_date\nÂ  Â  FROM\nÂ  Â  Â  `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_*`\nÂ  Â  WHERE\nÂ  Â  Â  event_name = 'purchase'\nÂ  Â  Â  AND ecommerce.transaction_id &lt;&gt; '(not set)'\nÂ  Â  GROUP BY\nÂ  Â  Â  1,\nÂ  Â  Â  2,\nÂ  Â  Â  3\n)\n\nData is filtered to only include rows representing purchases with a valid transaction id.\n\nGoogle Analytics will store a default â€œ(not set)â€ value if the data isnâ€™t accessible, isnâ€™t being tracked properly, or for other reasons.\n\nResults are then grouped across all three dimensions so that each row represents a unique order per client per date\nLabel customers as â€˜newâ€™ or â€˜returningâ€™\n\nSELECT\nÂ  *,\nÂ  CASE\nÂ  Â  WHEN DENSE_RANK() OVER(PARTITION BY user_id ORDER BY order_date) = 1Â \nÂ  Â  THEN 'new' ELSE 'returning'\nEND\nÂ  AS customer_type\nFROM\nÂ  `datastic.new_existing.base_table`\n\nDENSE_RANK() allows us to assign the same rank to multiple orders made on the same day.\nOn September 12, 2020, the customer 13285520 makes the first order and was given the label â€˜newâ€™.\nThen, on December 12, 2020, this same customer makes a second order, which resulted in having a â€˜returningâ€™ type."
  },
  {
    "objectID": "qmd/google-analytics-reports.html",
    "href": "qmd/google-analytics-reports.html",
    "title": "43Â  Google, Analytics, Reports",
    "section": "",
    "text": "Notes from\n\nWhy use segments in Google Analytics?\nHow to Measure User Interactions in Google Analytics\n\nTOC\n\nMisc\nTerms\nSegments, Dimensions, and Metrics\nReports\n\nReports Snapshot\nRealtime\nLifecycle\n\nAcquisition\n\nuser, traffic\n\nEngagement\n\nevents, conversions, page and screen\n\nMonetization\n\nin-app purchases, publisher ads\n\nRetention\n\nUser\n\nDemographics\nTech Explore Free-form\n\nFunnel\nPath\nSegment overlap\nCohort\nUser lifetime\n\n\nMisc\n\nIn GA4, Google has deprioritized the concept of a session and tracking sessions in favour of user and event-based metrics\nVariables are divided into Segments (filters), Dimensions (group_bys), and Metrics (calculated values)\nSee Docs in Reports &gt;&gt; Lifecycle &gt;&gt; Monetization for details, links on how tag sites to get events data into GA4\nDifferences between Reports and Explorations\n\nReports are for monitoring key metrics. Explorations is for querying which allows you to drill down and answer questions. (i.e.Â EDA)\nSome dimensions and metrics available in reports arenâ€™t supported in explorations\nReports that use the aggregated data tables are always based on 100% of the available data. Explorations, on the other hand, may apply sampling when querying user and event level data if the size of your query exceeds 10 million events.\n\nShortening the date range of the explorations can reduce the size of the population to which sampling is applied, resulting in higher accuracy.\n\nMight have slightly different results when running queries for the past 48 hours, due to processing time differences in different Analytics systems.\n\nResources\n\nGA4 Dimensions and Metrics Docs\nGoogle API Dimensions and Metrics\nGoogle Analytics Glossary, Events Glossary, Metrics Glossary\nGoogle search for glossaries\nChannels, Sources, and Mediums Glossary\nMeasure eCommerce - set up ecommerce events to collect information about the shopping behavior of your users\nSet up a purchase event - set up a purchase event on your website so you can measure when someone makes a purchase\n\nNot every metric can be combined with every dimension.\n\nEach dimension and metric has a scope: user-level, session-level, or hit-level.\nIn most cases, it only makes sense to combine dimensions and metrics that share the same scope.\nReference for valid pairs\n\nGA4 retains 2 months of data by default. You can adjust this by going to Admin &gt;&gt; Data Settings &gt;&gt; Data Retention\nThe cross-channel last click attribution model is Googleâ€™s basic last-click attribution model with a slight twist: it excludes direct traffc (traffic that is unattributed) except when no other data is available.\nGoogle Analytics data in BigQuery (info for GA360 not GA4)\n\nGA4 Docs for exporting to BigQuery\nNote that the Big Query export table with Google Analytics data is a nested table at session level for GA360 and at the event level for GA4\n\nnested information contains actions and metrics for a specific customer within a session\n\nExample: query for session level features\nExample: unnested query for hit level features\n\n\nTerms\n\nCohort - group of users who share a common characteristic identified by an Analytics dimension\nDimension - categorical value; category to group_by in the calculation of a metric\n\ne.g.Â sesssions per country where country is the dimension\n\nEngagementÂ \n\nUser engagement is the length of time that your app is in the foreground or webpage is in focus so you can understand the time that users spend engaged with your content.\nEngaged sessions are sessions that last 10 seconds or longer, have one or more conversion events, or have two or more views\n\nâ€œFirst Userâ€ - naming convention for acquisition attributes of a user (aka a first time user). There is no â€œfirst userâ€ dimension. Itâ€™s a prefix that describes dimensions that measure how the website first acquires a user. These values donâ€™t change when a user returns to your app or website.\nInteraction - any touch point with an element on a website\nMedium: the general category of the source\n\nexamples: organic search (organic, i.e.Â unpaid search), cost-per-click paid search (cpc, i.e.Â paid search), web referral (referral),  (custom medium), none (direct traffic).\n\nppc: pay-per-click\nreferral: any traffic coming from another website thatâ€™s NOT Pay-Per-Click (PPC)/Cost-per-Thousand (CPM) (i.e.Â ads) or a search engine\n\ne.g.Â link in an article or video description on Facebook, TripAdvisor, etc.\n\n\n\nMetric - numeric value; usually a count or calculated value like a rate or average thatâ€™s aggregated per dimension value\n\ne.g.Â average conversions per device where average conversion is the metric\n\nNew Users - Users on your site or app for the first time\n\nanyone who doesnâ€™t have both a Google Analytics cookie and Client ID from your site or App Instance ID from your app\n\nâ€œ(not set)â€ (docs) - Google Analytics will store a default (not set) value if the data isnâ€™t accessible, isnâ€™t being tracked properly, or for other reasons.\nSegments -Â  website variables that you can filter the data in order to perform calculations on various user subgroups\n\nThink the term comes from â€œcustomer segmentationâ€\nDocs\n\nAlso shows how to build custom segments\n\nTypes\n\nSystem - provided segments from Google\nCustom - you can edit, copy/paste system segments to build segments or create one from scratch\n\nAlso see Analytics Solutions Gallery\n\na free marketplace where Analytics users share segments and other solutions theyâ€™ve developed\n\n\n\nScopes\n\nA user can be involved in multiple sessions and a session can contain multiple hits\nUsers - People interact with your property (e.g., your website or app)\n\nBehavior across all sessions within the date range youâ€™re using, up to 93 days\ne.g.Â all the goals users completed or all the revenue they generated (across all sessions) during the date range.\n\nSessions - Interactions by a single user are grouped into sessions.\n\nBehavior within a single session\ne.g.Â the goals that users completed during a session, or the amount of revenue they generated during a session.\n\nHits - Interactions during a session are referred to as hits. Hits include interactions like pageviews, events, and transactions.\n\nBehavior confined to a single action\ne.g.Â viewing a page or starting a video.\n\n\n\nSequenced Segment - sequence-based segmentT\n\nThink this is primarily used to create funnels pre-GA4, but can still be used in the â€œBlankâ€ or â€œFree Formâ€ template of Explore\n\nSee Segment Docs for pre-GA4\n\n\nSession - A single period of time a user is active on your site or app\n\n2 methods for how a session can be ended:\n\nTime-based expiration:\n\nAfter 30 minutes of inactivity if someone is inactive on your website for over 30 minutes, then a new session will be reported if they perform another interaction, for example, viewing another page * This explains how in a Path Exploration (see below) there are paths that go from session_start to page_view to session_start (i.e.Â the user left the tab open because they had to do something else, and after 30 minutes came back and interacted with the web page)\nAt midnight\n\nCampaign change:\n\nIf a user arrives via one campaign, leaves, and then comes back via a different campaign.\n\n\n\nSource -Â  the origin of your traffic, such as a search engine (for example, google) or a domain (example.com)\nUsers - Returning users who have been processed by Analytics. Sometimes itâ€™s Active Users.\nUTM - Urchin Traffic Monitor - used to identify marketing channels or results from ad campaigns\n\ne.g.Â http://yourwebsite.com/your-post-title/?utm_source=google\n\nutm code = string after â€œ?â€\n\nseparate each UTM parameter with the â€˜&â€™ sign.\n\nThis person clicked a google ad to get to your site\n\nName comes from Urchin Tracker, a web analytics software that served as the base for Google Analytics.\nGoogle URL builder tool\nSee article for me details, best practices, etc.\nParameter types\n\nutm_source - traffic source (e.g.Â google, facebook, twitter, etc.)\nutm_medium - type of traffic source (e.g.Â CPC, email, social, referral, display, etc.)\nutm_campaign - campaign name, track the performance of a specific campaign\nutm___content - In case you have multiple links pointing to the same URL (such as an email with two CTA buttons), this code will help you track which link was clicked (e.g utm_content=navlink )\nutm_term - track which keyword term a website visitor came from. This parameter is specifically used for paid search ads. (e.g.Â utm_term=growth+hacking+tactics)\n\n\n\nSegments, Dimensions, and Metrics\n\nSegments:\n\nMisc\n\nRecommended segments\n\neach geo-region (e.g.Â state or county), each product, each product-page, each source-medium (e.g.Â newsletter)\n\nUseful to compare segments\n\ne.g.Â compare the behavior differences between â€œvisitors that stayed longer than 5 minutesâ€ on your website and â€œvisitors that stayed less than 1 minute.â€\n\ncan apply up to four segments at a time to any report\nmaximum date range of 93 days to your reports\n\nSegments based on the Date of First Session option are limited to maximum range of 31 days\n\nreports are on only the first 1000 sessions for each user.\n\ncounts beyond 1000 in a 93-day period are usually an indication of non-human traffic\n\nDo not use segments with Multi-Channel Funnel reports. Use Conversion segments instead.\nGoogle Ads cost data is not compatible with segments. If you apply a segment to a Google Ads report that includes cost data, then the cost data all have zero values.\n\n\nDimensions:\n\nAny event: the user has at least 1 event within the exploration time period.\nAny transaction: the user has at least 1 transaction event within the exploration time period,\nAny conversion: the user has at least 1 conversion event within the exploration time period.\nEvent Category - something that describes the group of event actions and event labels\nEvent Action - e.g.Â button click, image hover, tap, etc.\nEvent Label - name of a product, navbar, sign-up form, etc.\nFirst Touch (acquisition date): the first time the user visited your app or website, as measured by this Google Analytics property.\nFirst user medium - The medium by which the user was first acquired, based on the Cross-channel last click attribution model (see Misc).\nFirst open - when a user opens the app for the first time\n\nFind out how many users have installed your app\n\nFirst Visit (app, web) - the first time a user visits a website\nMobile Traffic - users that use a mobile device to visit your site\n\nfind out if the â€˜time on pageâ€™ is what it should be as this might say something about the mobile friendliness of your site\n\nOrganic Traffic - all visitors that came from an organic search result to your site\n\nuseful for finding which landing pages these users visit\n\nSession Medium - The medium that initiated a session on your website or app (i.e.Â current â€œlast touchâ€ for that user; see First Touch for contrast)\n\nMetrics:\n\nActive Users - number of unique users who performed sessions on your website within the exploration time frame\n\nSeems to be a smaller number than â€œtotal usersâ€\na user who has an engaged session or when Analytics collects:\n\nthe first_visit event or engagement_time_msec parameter from a website\nthe first_open event or engagement_time_msec parameter from an Android app\nthe first_open or user_engagement event from an iOS app\n\n\nAverage Click Rate - average number of times an element is interacted with, within a session\n\nUsed to understand pain points\n\nIf an element that should only be clicked once or twice has a high number of avg. clicks this can be an indication of either the user becoming stuck or clicking out of frustration.\n\nAvg CR = Total Events / Unique Dimension Combinations (see Example 6)\n\nAverage Engagement Time - sum of user engagement durations divided by the number of active users\nBounce Rate - the percentage of visitors to a particular website who navigate away from the site after viewing only one page.\n\nHigh bounce rate can indicate the landing page needs improvement\nIf the bounces occur not just on the home page but all around the site, the high bounce rate may signal the need for a recommendation module.\n\nIf a viewer sees a recommendation on the product page, this may induce them to view more pages which lowers the bounce rat\n\n\nClick Through Rate (CTR) - can be defined multiple ways\n\nnumber of clicks / number of impressions\nnumber of unique users clicked / number of unique users that visited the page\nnumber of clicks of ad viewers / total number of ad viewer\n\nEngaged sessions: The number of sessions that lasted 10 seconds or longer, or had 1 or more conversion events or 2 or more page views.\nEngagement Rate: engaged sessions / sessions\nEvent Count - the number of events triggered across all users and all sessions in the exploration time frame\nInteraction Rate - the percentage of people that interact (click, hover, tap) with an element.\n\nHelps analysts find areas on websites or apps where the customer may be having trouble which may affect conversion rate\nIf a button, etc. breaks, the development team can understand how urgent getting this element fixed is\nIR = (Unique Dimension Combinations * 100) / Sessions (see Example 6)\n\nPower Users: people who love their product, are highly engaged, and contribute a ton of value to the network\n\nEngagement = DAU/MAU\n\nDAU: Daily Active Users\nMAU: Monthly Active Users\nGuideline: apps over 20% are said to be good, and 50%+ is world class\n\nUse: Product is used daily\n\nRecency of Events (inverse of a â€œdays sinceâ€ feature) - a customer that just visited your website is probably more keen to purchase than one that visited it 3 months ago\n\nRFM (recency, frequency monetary value) theory wiki\nRecency since last session = 1 / Number of days since last session\n\nHighest value = 1 (i.e.Â lower is worse)\n\nRecency since last session with at least one interaction with  = same formula except you change the denominator to this count of days since this event\n** There will be 0s in the denominator for some users, so those recency values need to be changed to zero or the 0 counts replaced with a really small fraction ** Return on Ad Spend (ROAS)\nROAS = Sales Revenue / Advertising Budget Spend\n\nSessions: The number of sessions that began on your site or app (the session_start event was triggered).\nTotal Events - total number of events with the same event category, event action, and event label across all sessions\nTotal Users - represents the number of unique users who viewed a screen or triggered an event in the exploration time frame\n\nIf a user repeats an interaction with the same node, then they only count as 1 user at that node.\n\nExample: when a user opens the home page, navigates to a product page, and then returns to the home page before navigating to another product page within the time frame selected, the path will show one home page user on the starting point and one product page user for each product in the first step\n\n\nUnique Dimension Combinations (UDC) - The unique number of sessions that have interacted with a combination of events.\n\nDiffers from unique events in that the number depends on which dimensions the view is grouped on. (see Example 6)\n**If in doubt on whether to use unique events or unique dimension combination, use Unique Dimension Combinations.**\n\nUDC probably answers more business questions, and it also matches the values for UE in some queries.\n\n\nUnique Events - total number of nonrepeated events with the same event category, event action, and event label across all sessions\nUnique User Scrolls - the number of unique users who scrolled down at least 90% of the page\nUser Engagement - sum of user engagement durations divided by the number of triggered events\nViews - the number of app screens or web pages your users saw.\n\nRepeated views of a screen or page are counted\nViews = screen_view + page_view\n\n\n\nReports\n\nMisc\n\nDocs\nReports are for monitoring key metrics. Explorations is for querying which allows you to drill down and answer questions. (i.e.Â EDA)\nEach Dimension category has itâ€™s own page. If you click one of the categories (e.g.Â first_visit) in a report, it takes you to a Overview-type page where those users are broken down into various comparisons.\nFirebase (apps that use firebase) and Games (gaming apps) reports also available\n\nSections\n\nReports Snapshot\nRealtime\nLife Cycle\n\nAcquisition\n\nUser, Traffic\n\nEngagement\n\nEvents, Conversions, and Pages and Screens tabs\n\nMonetization\n\nIn-app purchases, Publisher Ads\n\nRetention\n\nUser\n\nDemographics, Tech\n\n\nReports Snapshot\n\nTime Series\n\nUsers, Event Count, Conversions, Total Revenue\n\nUsers in last 30 min by country\nâ€œInsightsâ€ - Anomaly/change point detection\n\nGoogle forecasts various metrics and if any data points fall outside their 95% PI, theyâ€™re listed in Insights\n\nNew Users by first_user_medium\n\nCounts of users by the source category for how users are being acquired\n\n\nRealtime\n\nShows how users enter the conversion funnel, and how they behave once theyâ€™re in the funnel\nUse cases\n\nSee whether a one-day promotion is driving traffic to your site or app\nMonitor the immediate effects on traffic from a blog/social network post or tweet\nMonitor whether new and changed content on your site is affecting traffic\nVerify that the tracking code is working on your site or app\n\nMisc\n\nDocs\nRealtime Dimensions and Metrics\nHovering over the headers gives a description of the metric\n\nMap\n\nCan click on a bubble/location to drill down into a comparison for that particular area\n\nAdd Comparison\n\nevaluate subsets of your data side by side,\n\ne.g.Â how users 18-24 compare to your overall user base, or how two different campaigns compare to each other.\n\n\nView User Snapshot\n\nIncludes information about the userâ€™s device, app version, and location, along with the top events the user triggered, and the relevant user properties\nClick arrows to scroll through different user profiles\nProbably must have User-ID feature enabled to use (also see User Lifetime template)\n\nCards\n\nUsers in last 30 min\n\nAlso shows breakdown by 1 min intervals\nFor 30 min, a breakdown by desktop/mobile %s\nA metric for a Sales target during an marketing campaign might be # of users on the website every 30 min. Can monitor the campaign results in realtime\n\nUsers by source, medium, or campaign: where your users are coming from\nUsers or new users by audience: who your users are\nUsers by page title or screen name: which content they engage\nEvent count by event name: which events they trigger\nConversions by event name: which conversions they complete\n\n\nLifecycle\n\nTypes\n\nAcquisition: how you acquire traffic to your site or app\n\nUser, Traffic tabs\n\nEngagement: user engagement by events, pages, and screens\n\nEvents, Conversions, and Pages and Screens tabs\n\nMonetization: revenue by items, promotions, and coupons\n\nIn-app purchases, Publisher Ads\n\nRetention: retention by new and returning users, cohorts, and lifetime value\n\nAcquisition\n\nMisc\n\nDocs\nBoth User and Traffic acquisition tabs are similar except by the dimension the metrics are grouped by.\n\nUser â€“&gt; metrics about first time users\nTraffic â€“&gt; metrics about all(?) users\n\nSince many of the metrics are per session, it should include both first time and returning users.\nThe Users metric will be a count for only returning users though\n\n\n\nOverview tab\n\nSummarizes the User acquisition and Traffic acquisition tabs\nCards\n\nUsers and new users time series\nUsers in the last 30 min\n\nTop Countries\n\nNew Users by First Time Medium\nSessions by Session Medium\nSessions by Session Campaign\nLifetime value time series -Â  the average revenue from new users over their first 120 days\n\n\nUser Acquisition tab\n\nBar: New users by First user medium\nMulti Time Series: New users by First user medium\nTable\n\nDimension: First user medium (e.g.Â organic, referral, email, affiliate)\n\nOther dimensions available\n\nMetrics:\n\nNew Users\nEngaged Sessions, Engagement Rate, Engaged Sessions per User, Avg Engagement Time\nEvent Count\nConversions\nTotal Revenue\n\n\n\nTraffic Acquisitions tab\n\nMulti Time Series: Users by Session source/medium over time\nBar: Users by Session source/medium\nTable\n\nDimension: Session source/medium (e.g.Â google/organic, youtube.com/referral)\n\ncombinations of source and medium categories\nOnly top 10 shown (sortable by metric column)\nOther dimensions available\n\nMetrics:\n\nUsers\nSessions, Engaged Sessions, Engagement Rate, Engaged Sessions per User, Avg Engagement Time per Session\nEvents per Session, Event Count\nConversions\nTotal Revenue\n\n\n\n\nEngagement\n\nMisc\n\nDocs\nEvents tab enables you to measure when users purchase a product, submit a form, or scroll through an article\nConversions tab measure events that lead to purchases\nPage and Screens shows web pages and app screens that users visit the most frequently\n\nOverview\n\nSummarizes Events, Conversions, and Page Views tabs\nCards\n\nMulti Time Series\n\navg engagement time: sum of user engagement durations divided by the number of active users\nengaged sessions per user: engaged sessions / users\navg engagement time per session: sum of user engagement durations divided by the number of sessions\n\nUsers in last 30 min\n\ntop pages & screens\n\nViews and Event Count time series\nEvent count by Event name\nViews by Page title and screen class\nUser Activity time series\nUser stickiness\n\nRatios compare engagement by active users over a narrower time frame with engagement over a broader time frame.\n\nHigher ratios suggest good engagement and user retention.\n\nRatios\n\nDaily Active Users (DAU) / Monthly Active Users (MAU)\n\nAlso see Examples 9, 15 for different calculations of DAU\n\nDaily Active Users (DAU) / Weekly Active Users (WAU)\nWeekly Active Users (WAU) / Monthly Active Users (MAU)\n\nThe ratios consist of the following values:\n\nDAU: the number of active users in the last 24 hours\nWAU: the number of active users in the last 7 days\nMAU: the number of active users in the last 30 days\n\n\n\n\nEvents\n\nMulti Time Series: Event count by Event name\nScatter: Event count vs Total users; points labelled by Event name\nTable\n\nDimension: Event Name (e.g.Â view_item, scroll, view_cart)\nMetrics\n\nEvent Count, Total Users, Event Count per User, Total Revenue\n\n\n\nConversions\n\nMulti Time Series: Conversions by Event name\nScatter: Conversions vs Total users; points labelled by Event name\nTable\n\nDimension: Event Name (e.g.Â first_visit, begin_checkout,Â  purchase)\nMetrics\n\nConversions, Total Users, Event Revenue\n\n\n\nPages and Screens\n\nBar: Views by Page title and screen class\nScatter: Views vs Users; Points labelled by Page title and screen class\nTable\n\nDimension: Page Title and Screen Class (e.g.Â Sale | Google Merchandise Store, Apparel |Google Merchandise Store, Shopping Cart)\n\nOther dimensions available\n\nMetrics:\n\nViews, Views per User\nUsers, New Users\nAverage Engagement Time\nUnique User Scrolls\nEvent Count\nConversions, Total Revenue\n\n\n\n\nMonetization\n\nMisc\n\nDocs\n\nSee how to populate pages with tags that send events (e.g.Â Item views, Item ID, Item name, Item brand, Item category) to GA4\nSee how to set-up apps through firebase to send in_app_purchase and subscription events to GA4\nSee how to set-up ad measurement through firebase\n\nIn-App Purchases - revenue for digital content and features sold within a mobile app\nPublisher Ads - shows ad revenue by ad unit so you can identify the ads that generate the most revenue on your app (currently not for websites,Â  Dec 2021)\n\nOverview\n\nCards\n\nEcommerce purchases by Item list name:Â  number of purchase events by the item_list_name parameter\nEcommerce purchases by Item name:Â  number of purchase events by the item_name parameter\nEcommerce revenue by Order coupon:Â  revenue from purchase events by the coupon parameter\nItem views by Item promotion name:Â  number of view_item events by the name of the promotion\nProduct revenue by Product ID:Â  revenue by the product identifier for each in-app purchase\nPublisher ad impressions by Ad unit:Â  number of ad_impression events by ad unit\n\n\nIn-App Purchases\n\nMulti Time Series: Quantity by Product ID\nScatter: Quantity vs Product revenue; Points labelled by Product ID\nTable\n\nDimension: Product ID\nMetrics\n\nQuantity, Product Revenue, Avg Product Revenue\n\n\n\nPublisher Ads\n\nMulti Time Series: Publisher ad impressions by Ad unit\nScatter: Publisher ad impressions vs Ad unit exposure; Points labelled by Ad unit\nTable\n\nDimension: Ad unit\n\nOther dimensions available\n\nMetrics\n\nPublisher Ad Impressions, Ad Unit Exposure, Publisher Ad Clicks, Total Ad Revenue\n\n\n\n\nRetention\n\nShows engagement and revenue for returning users\nTime Series\n\nNew Users, Returning Users\nUser retention by cohort\n\npercentage of new users who return on their second and eighth day\nExample: 100 users visit your site on September 9. Ten of the users return on September 10 and two return on September 16. The Day 1 line shows 10% on September 10, while Day 7 shows 2% on September 16.\n\nlines not colored differently ann no legend. Iâ€™d guess the top line will be the Day 1 cohort since it should always have the larger percentage\nThere are multiple data points for each line and Iâ€™m not sure how to interpret each point. Are these days subsequent to day 1 or day 7 or is each point a different day 1 or day 7 cohort?\n\n\nUser engagement by cohort\n\naverage engagement time of new users who return to your site or app on their second and eighth day.\n\n(like the previous chart) Guessing the two lines represent 2nd day returning users and 8th day returning users\n\n\nUser Retention - percentage of users who return each day in their first 42 days\nUser Engagement - average engagement time of users who return in their first 42 days\nLifetime Value - average revenue from new users over their first 120 days\n\n\n\nUser\n\nDemographics: categorizes your users by age, location, interests, and more\n\nDocs\nMisc\n\nEnhanced by activating Google Signals. Adds Google account data\n\nOverview\n\nCards\n\nMap - Users, New Users, Returning Users by Country\n\nUsers is supposed to be Returning Users but not in this time I guess\n\ndoesnâ€™t equal New + Returning\nElsewhere itâ€™s Active Users\n\n\nUsers in Last 30 min (top countries)\n(Active) Users, New Users, Returning Users by City, Gender, Interests, Age, Language\n\n\nDetails\n\nBar: Users by Country\nScatter: Users vs New Users; Points labelled by Country\nTable\n\nDimension: Country\n\nCan be changed to any of the ones in Overview (affects charts too)\n\nMetrics: Users, New Users, Engaged Sessions, Engagement Rate, Engaged Sessions per User, Average Engagement Time, Event Count, Conversions, Total Revenue\n\n\n\nTech: user adoption of app releases and the technologies used to engage your content\n\nDocs\nOverview\n\nCards\n\nPie: Users by Platform\nUsers in the last 30 min (top Platforms)\n(Active) Users, New Users, Returning Users by\n\nOperating system\nPlatform / device category\nBrowser\nDevice Category\nScreen Resolution\nApp Version\nDevice Model\n\nApp stability overview\n\nPercent of Crash-Free Users\n\nLatest App Release Overview\n\nApp, Version, Status\n\n\n\nDetails\n\nMulti Time Series: Users by Browser\nScatter: Users vs New Users; Points labelled by Browser\nTable\n\nDimension: Browser\n\nCan be changed to any of the ones in Overview (affects charts too)\n\nMetrics\n\nUsers, New Users, Engaged Sessions, Engagement Rate, Engaged Sessions per User, Average Engagement Time, Event Count, Conversions, Total Revenue\n\n\n\n\n\n\nExplore\n\nReports are for monitoring key metrics. Explorations is for querying which allows you to drill down and answer questions. (i.e.Â EDA)\nMisc\n\nIf you right-click data in the visualization (maybe only table), thereâ€™s an option for View Users. It drills down from the aggregated data into the user-level data.\n\nMust have User-ID feature enabled to use (also see User Lifetime template)\n\n\nDocs\nTemplates\n\nTop home page; need to scroll through Template Gallery to see all the templates\nOnce you select a template, the UI loads which is a tableau-like set-up\n\nAdd variables (segments, dimensions, metrics) on the Variables panel\nThen drag (or double-click for default location) variables to rows, columns, etc.\nInteract with visual on the canvas\n\nTypes\n\nFree-form - Visualize data in crosstabs, bar charts, pie charts, line charts, scatter plots and geo maps.\nFunnel - Visualize the steps users take to complete tasks on your site or app, and see how you can optimize user experience and identify over- or under-performing audiences\nPath - Visualize the paths your users take as they interact with your website and app\nSegment overlap - See how different user segments relate to each other. Use this technique to identify new segments of users who meet complex criteria.\nCohort - Gain insights from the behavior and performance of groups of users related by common attributes.\nUser lifetime - Explore user behavior and value over their lifetime as a customer.\n\nMisc\n\nâ€œUndoâ€ is on the upper-right side of the canvas\nWhen an exploration is subject to either sampling or data thresholds, the icon in the right corner of the exploration changes from green to yellow\n\nexplorations may be based on sampled data if more than 10 million events are part of a particular exploration query\nIf your exploration includes demographic information or data provided by Google signals, the data may be filtered to remove data that might identify individual users\nThe yellow/orange iconâ€™s tooltip displays information about the data in the exploration\n\n\n\nFree Form\n\nAvailable visualizations: table, donut, line, scatter, bar, and map\n\nMultiple segments (Segments Comparison area) produce facetted visualizations (except table)\nGrouped by dimension (Breakdown area) but monochromatic (i.e.Â sequential)\n\nCouldnâ€™t find an option for using qualitative or diverging palettes\n\n\nTable\n\nDrag dimensions to the Rows fields (e.g.Â device category, screen resolution)\nDrag dimensions to the Columns fields (e.g.Â country)\nDrag metrics to the Value fields (e.g Total users, Total Revenue)\nIssues\n\nCouldnâ€™t find a way to arrange by column value\n\nIn the example, the table is locked into being arranged alphabetically by Country\nThereâ€™s a â€œsortâ€ option (supposedly) for some templates but not in this one.\n\nYou have to set the column number and row number in order to view them all\n\n\nLine\n\nAnomaly Detection option\n\nBayesian state space-time series model\nSettings\n\nTraining period - number of days before timeframe of the data that you want to use to train the model\nSensitivity - sets the p-value that will be used as a threshold for determining whether a point will be labelled an outlier\n\n\n\n\nFunnel\n\nMisc\n\nDocs\n\nVisuals\n\nStandard Funnel: bar chart thats ordered according the sequence of steps\nTrended Funnel: multi-line chart where each line is a step in the funnel\n\nDrill-down into a step by clicking the header at the top of chart with that stepâ€™s name\n\nTable (included in both funnel visuals)\n\nValues + Breakdown and Segment (see Options)\n\n\nOptions\n\nMake Open Funnel (Tab Settings panel, toggle)\n\nRecommmend reading the open/closed examples in the docs\nFor Both types:\n\nAfter entering, if a user skips a step, then they are only counted in steps prior to the skipped step\n\ne.g.Â User enters at step 2 but skips to step 4. That user is only counted in step 2.\n\n\nOpen funnel - users can enter the funnel in any step\nClosed funnel - users must enter the funnel in the first step\n\nSegment Comparisons\n\nDrag Segments from the Variable panel into the Segment Comparison area (** up to 4 **)\nFor a step, the data is filtered by each Segment and dimension group_by calculations are made\n\nBreakdown (Tab Settings panel)\n\nDimension group_by calculations\nDrop Dimensions into the Breakdown area\n\n\nEdit Funnel Steps\n\nIn the Tab Settings panel &gt;&gt; Steps, click the pencil icon to edit\n\nStep 1 - â€œOr first_visitâ€ conditional was added\nStep 2 - parameter â€œcontains organicâ€ was added\n\nIf itâ€™s a character parameter (e.g.Â Session_keyword), you then choose a tidy select verb (e.g.Â contains, matches, etc.) to filter the value of parameter\nIf itâ€™s a numeric parameter (e.g.Â discount_value), you choose a conditional like =, &gt;, etc.\n\nOther options\n\nChange the name; add a step\nChoose between indirectly or directly to control whether other potential steps are allowed to be in between the steps (top left of step)\n\nIf indirectly, then other steps can potentially be in between the prior step and the current step.\n\nChoose an allowable time window between steps using â€œWithinâ€ (top right of step)\n\n\n\n\n\nPath\n\nA path is a specific sequence of nodes occurring across one or more steps, within a specified time frame\nPaths can span one or more sessions, depending on the date range you choose.\n\nA new session begins if a user is inactive for 30 minutes.\nIf a path spans multiple sessions, the data for a node is an aggregation of all sessions.\n\nalluvial chart\nDimensions are nodes and Metrics are values\n\nnode variables: event name,Â  page title and screen name, or page title and screen class\n\nExcluding nodes only hides them from the chart (i.e.Â doesnâ€™t affect counts). 2 ways:\n\nClick pencil icon above each node variable to choose which node values are displayed (the rest are aggregated into â€œotherâ€)\nRight-click the node on the chart and select â€œExcludeâ€\n\n\nvalue variables:\n\nevent count - the number of events triggered across all users and all sessions in the exploration time frame\nactive users - number of unique users who performed sessions on your website within the exploration time frame\n\nSeems to be a smaller number than â€œtotal usersâ€\n\ntotal users - represents the number of unique users who viewed a screen or triggered an event in the exploration time frame\n\nIf a user repeats an interaction with the same node, then they only count as 1 user at that node.\n\nExample: when a user opens the home page, navigates to a product page, and then returns to the home page before navigating to another product page within the time frame selected, the path will show one home page user on the starting point and one product page user for each product in the first step\n\n\n\n\nClicking on a node value (e.g.Â first_visit)\n\ncreates another step (e.g.Â step+3) which will show all node values that branch from the clicked node value\n\nstep+2 is the second step in a user path after the starting point\n\nclicking a node again, removes the step(s?) in front of it\n\nOptions\n\nView unique nodes only (Tab Settings panel, toggle)\n\nwill display only one node for each even if the user performed many consecutive events on that page or screen.\nHowever, If the user then comes back to that page or screen after they visited another page or screen, it will show again in the path\n\nSegment and Breakdown (Tab Settings panel)\n\ndrag a Segment to the Segment area to filter the data\ndrag a Dimension to the Breakdown area to group_by a dimension (hover over a category in legend to get a subgroupâ€™s calculation)\n\nFilter (Tab Settings panel)\n\nAllows you to filter the data with a dimension category (normally a â€œSegmentâ€ functionality)\ndrag dimension to Filter area and enter the condition\n\nDimension filters are case sensitive\n\n\n\nBackwards-Path Analysis\n\nAllow you to work backwards from an event\n\nExample: tracing user steps backwards from â€œpurchaseâ€ or â€œnon-purchasersâ€\n\nSteps (After selecting the Path Template)\n\nIn the upper right, click Start over.\nClick in the ENDING POINT box to select the node that ends the path\n\n\n\nSegment Overlap\n\nSays that 13,608 Active Users (Metric) were on a non-purchasers on a mobile device while in the US.\nCompare up to 3 user segments to quickly see how those segments overlap and relate to each other.\nExplore audiences to create custom segments\n\nIsolate specific audiences based on complex conditions\nThen create new segments based on your findings\nApply new segments to other Explorations techniques and Google Analytics reports\n\nSteps\n\nDrag up to 3 segment to the Segments Comparison area\nDrag metrics into the Values area\n\nOptions\n\nBreakdown, Filter (see other templates for descriptions)\nVarious types of Segments available\n\n\nCohort\n\nInterpretation:\n\n18490 users were acquired between 0ct31 and Nov6.\n\n315 of the 18490 made a transaction that same week\nOf the 315, 80 made a transaction the following week.\n\nThe Return Criteria (see below) is â€œany eventâ€ so performing a transaction in week 0 would meet this criteria\n\n\n\nExplore the behavior of groups of users over time\n\nAlso see Example 8\nDocs\n\nDimensions characterize a cohort\n\ne.g.Â first_touch (i.e.Â acquisition date)\nBased on the userâ€™s device data only. User-ID is not considered when creating a cohort\n\nThe week 0 column shows the number of users that met the Cohort Inclusion dimension\nAfter week 0, each cell in the data table shows the number of users who subsequently met the Return Criterion dimension after the previous week\nCohorts are shown first column. Each row name is a cohort that met the Cohort Inclusion dimension during that timeframe.\n\nNote: The first column annotation of the number of users matches the Week 0 column\n\nSteps\n\nSet Cohort Inclusion\n\nthe condition that adds a user to a cohort\nDrop down menu with â€œpredefinedâ€ and â€œotherâ€ dimensions\n\npredefined: first touch, any event, any transaction, any conversion\nother: various dimensions\n\n\nSet Return Criteria\n\nthe condition these users meet to remain in your cohort\nDrop down menu with â€œpredefinedâ€ and â€œotherâ€ dimensions\n\npredefined: any event, any transaction, any conversion\nother: various dimensions\n\n\n\nOptions\n\nCohort Granularity\n\nDaily: from midnight to midnight in the property timezone.\nWeekly: from Sunday to Saturday included, not on a rolling 7 days.\nMonthly: from the beginning of the month to the end of the month\n\nCalculation Standard: Each cell includes all cohort users who meet the return criteria for that individual period, regardless of what they do in other periods. The metric displays the total value for that individual period.\n  Rolling: Each cell includes all cohort users who meet the return criteria for that period as well as all previous periods. The metric displays the total value for that individual period.\n\n  Cumulative: Each cell includes all cohort users who meet the return criteria in any period in the exploration. The metric displays the cumulative total value for each period.\nMetric Type sum: count\n  per user: percent\nBreakdown, Values See other templates for definitions\n  When you include a breakdown dimension, users are only attributed to the first instance of the breakdown value that applies to them.\n      Example: _User A_ first appears as a mobile user, then returns the same day as a desktop user. _User A_ only appears in the mobile breakdown for that cohort.\n\nMisc Demographic dimensions are subject to thresholding. If the number of users in the cohort is too small to protect their anonymity, those users wonâ€™t be included in the exploration\n\nUser Lifetime Docs\n  Table\n\n  Example Questions\n      The source/medium/campaign that drove users with the highest lifetime revenue, as compared to revenue only for the selected month.\n\n      The active campaigns that are acquiring users who are expected to be more valuable, with higher purchase probability and lower churn probability, as calculated by Google Analytics [predictions models](https://support.google.com/analytics/answer/9846734).\n\n      Unique user behavior insights, such as when your monthly active users last purchased a product from your site, or when they were last engaged with your app.\n\nProvides information about these usersâ€™ entire lifetime, including data from before the start of the specified range.\n\n** Some or all metrics may require enabling the User-ID feature that tracks a unique user id generated by your website or app though log-in, device id, etc. ** Available information for each user (available in the Metrics dropdown) Initial interactions: data associated with the first time the user was measured for a property. Example: their first visit or purchase date, or the campaign by which they were acquired as a user.\nMost recent interactions: data associated with the last time the user was measured for a property.\n\nExample: their last activity or purchase date.\n\nLifetime interactions: data aggregated over the lifetime of the user.\n\nExample: their lifetime revenue or engagement\n\nPredictive metrics: data generated through machine learning to predict user behavior:\n\nPurchase probability\nIn app purchase probability\nChurn probability Example (User-ID): Suppose a signed in user had multiple visits to your app last year, and conducted multiple transactions worth a total of $1,000. Suppose that this same user also had 4 transactions conducted as a guest (not signed in), each worth $50, and that each transaction was made on a different device. Finally, suppose this user visited your app while signed in at least once in the date range selected by your query. How this user appears in the user lifetime exploration depends on the reporting identity method used: With User-ID, Google signals, then device: this user is counted once, with revenue of $1000, as only her signed in data is applicable to the exploration. Her average lifetime revenue would be $1,000.\nWith device only: this user appears 5 times: one with revenue of $1000, and 4 times with revenue of $50. Her average lifetime revenue would be $240."
  },
  {
    "objectID": "qmd/google-bigquery.html#sec-goog-bigq-misc",
    "href": "qmd/google-bigquery.html#sec-goog-bigq-misc",
    "title": "BigQuery",
    "section": "Misc",
    "text": "Misc\n\nAlso see\n\nFinOps: Four Ways to Reduce Your BigQuery Storage Cost\n\n{bigrquery}\nI think query sizes under a 1TB are free\n\nif you go above that, then itâ€™s cheaper to look at flex spots\n\nBigQuery vs Cloud SQL\n\nCloud SQL is a service where relational databases can be managed and maintained in Google Cloud Platform. It allows its users to take advantage of the computing power of the Google Cloud Platform instead of setting up their own infrastructure. Cloud SQL supports specific versions of MySQL, PostgreSQL, and SQL Server.\nBigQuery is a cloud data warehouse solution provided by Google. It also comes with a built-in query engine. Bigquery has tools for data analytics and creating dashboards and generating reports. Cloud SQL does not have strong monitoring and metrics logging like BigQuery.\nBigQuery comes with applications within itself, Cloud SQL doesnâ€™t come with any applications.\nCloud SQL also has more database security options than BigQuery.\nThe storage space in Cloud SQL depends on the db engine being used, while that of Bigquery is equivalent to that of Google cloud storage.\nPricing\n\nBoth have free tiers\nCloudSQL has 2 types: Per Use and Packages\n\nIf usage over 450 hours monthly, then packages is a good option\n\n\n\nLeft pic is Per Use pricing; Right pic is Package pricing\n\n\nBigQuery based on Usage"
  },
  {
    "objectID": "qmd/google-bigquery.html#sec-goog-bigq-sqlfun",
    "href": "qmd/google-bigquery.html#sec-goog-bigq-sqlfun",
    "title": "BigQuery",
    "section": "SQL Functions",
    "text": "SQL Functions\n\nUNNEST - BigQuery - takes an ARRAY and returns a table with a row for each element in the ARRAY (docs)\n\nGoogle, Analytics, Analysis &gt;&gt; Example 17"
  },
  {
    "objectID": "qmd/google-bigquery.html#sec-goog-bigq-specexp",
    "href": "qmd/google-bigquery.html#sec-goog-bigq-specexp",
    "title": "BigQuery",
    "section": "BQ Specific Expressions",
    "text": "BQ Specific Expressions\n\nNotation Rules\n\nSquare brackets [ ] indicate optional clauses.\nParentheses ( ) indicate literal parentheses.\nThe vertical bar | indicates a logical OR.\nCurly braces { } enclose a set of options.\nA comma followed by an ellipsis within square brackets [, â€¦ ] indicates that the preceding item can repeat in a comma-separated list.\n\nUsing EXCEPT within SELECT\n\nPIVOT for pivot tables\n\n\n\nWith more than 1 aggregate\nselect * from (select No_of_Items, Item, City from sale)\nÂ  Â  pivot(sum(No_of_Items) Total_num, AVG(No_of_Items) Avg_num\nÂ  Â  for Item in ('Laptop', 'Mobile'))\n\nUNPIVOT\n\nselect * from sale\nunpivot(Sales_No for Items in (Laptop, TV, Mobile))\n\nItâ€™s a pivot_longer function that puts columns, Laptop, TV, and Mobile, into Items and their values into Sales_No\nCollapse columns into fewer categories\n\nselect * from sale\nÂ  Â  unpivot(\nÂ  Â  Â  Â  (Category1, Category2)\nÂ  Â  Â  Â  for Series\nÂ  Â  Â  Â  in ((Laptop, TV) as 'S1', (Tablet, Mobile) as 'S2')\nÂ  Â  )\n\nColumns have been collapsed into 2 categories, S1 and S2\n\n2 columns for each category\n\nValues for each category gets its own column\n\n\nGROUP BY + ROLLUP\n\n\ntotal sales (where quarter = null) and subtotals (by quarter) by year\n\nQUALIFY\n\n\nAllows you to apply it like a WHERE condition on a column created in your SELECT statement because itâ€™s evaluated after the GROUP BY, HAVING, and WINDOW statements\n\ni.e.Â a WHERE function that is executed towards the end of the order of operations instead of at the beginning\n\nUsing a WHERE instead of QUALIFY, the above query looks like this"
  },
  {
    "objectID": "qmd/google-bigquery.html#sec-goog-bigq-vars",
    "href": "qmd/google-bigquery.html#sec-goog-bigq-vars",
    "title": "BigQuery",
    "section": "Variables (aka Parameters)",
    "text": "Variables (aka Parameters)\n\nWays to create variables\n\nUsing a CTE\n\nBasically just using a CTE and calling it a variable or table of variables\n\nUsing BigQuery procedural language\n\n\n\nStatic values using CTE\n\n1 variable, 1 value\nÂ  -- Input your own value\nWITH\nÂ  variable AS (\nÂ  SELECT 250 AS product_threshold)\n\nÂ  -- Main Query\nSELECT\nÂ  *\nFROM\nÂ  `datastic.variables.base_table`,\nÂ  variable\nWHERE\nÂ  product_revenue &gt;= product_threshold\n\nCTE\n\nâ€œvariableâ€ is the name of the CTE that stores the variable\nâ€œproduct_thresholdâ€ isÂ  set to 250\n\nQuery\n\nThe CTE is called in the FROM statement, then the â€œproduct_thresholdâ€ can be used in the WHERE expression\n\n\n1 variable, multiple values\n-- Multiple values\nWITH\nÂ  variable AS (\nÂ  SELECT\nÂ  Â  *\nÂ  FROM\nÂ  Â  UNNEST([250,45,75]) AS product_threshold)\n\n-- Main Query\nSELECT\nÂ  *,\nÂ  'base_table_2' AS table_name\nFROM\nÂ  `datastic.variables.base_table_2`,\nÂ  variable\nWHERE\nÂ  product_revenue IN (product_threshold);\n\nCTE\n\nâ€œvariableâ€ is the name of the CTE that stores the variable\nUses SELECT, FROM syntax\nlist of values is unnested into the variable, â€œproduct_thresholdâ€\n\nSee SQL Functions for UNNEST def\nUNNEST essentially coerces the list into a 1 column vector\n\n\nQuery\n\nThe CTE is called in the FROM statement, then the â€œproduct_thresholdâ€ can be used in the WHERE expression\n\nNot sure why parentheses are around the variable in this case\n\nTable is filtered by values in the variable\n\n\nMultiple variables with multiple values\nÂ  -- Multiple variables as a table\nWITH\nÂ  variable AS (\nÂ  SELECT\nÂ  Â  product_threshold\nÂ  FROM\nÂ  Â  UNNEST([\nÂ  Â  Â  Â  STRUCT(250 AS price,'Satin Black Ballpoint Pen' AS name),Â \nÂ  Â  Â  Â  STRUCT(45 AS price,'Ballpoint Led Light Pen' AS name),Â \nÂ  Â  Â  Â  STRUCT(75 AS price,'Ballpoint Led Light Pen' AS name)]\nÂ  Â  Â  Â  ) AS product_threshold)\nÂ  -- Main Query\nSELECT\nÂ  *\nFROM\nÂ  `datastic.variables.base_table`,\nÂ  variable\nWHERE\nÂ  product_revenue = product_threshold.price\nÂ  AND product_name = product_threshold.name\n\nAlso see (Dynamic values using CTE &gt;&gt; Multiple variables, 1 valuej) where â€œtable.variableâ€ syntax isnâ€™t used\nCTE\n\nâ€œvariableâ€ is the name of the CTE that stores the variable\nInstead of SELECT *, SELECT  is used\n\nnot sure if thatâ€™s necessary or not\n\nUNNEST + STRUCT coerces the array into 2 column table\n\nThe â€œpriceâ€ and â€œnameâ€ variables each have multiple values\nEach STRUCT expression is a row in the table\n\n\nQuery\n\nThe CTE is called in the FROM statement, then the â€œproduct_thresholdâ€ can be used in the WHERE expression\nEach variable is accessed by â€œtable.variableâ€ syntax\nSurprised IN isnâ€™t used and that you can do this with â€œ=â€ operator\n\n\n\n\n\nDynamic values using CTE\n\nValue is likely to change when performing these queries with new data\n1 variable, 1 value\n\nExample: value is a statistic of a variable in a table\nÂ  -- Calculate twice the average product revenueÂ \nWITH\nÂ  variable AS (\nÂ  SELECT\nÂ  Â  AVG(product_revenue)*3 AS product_average\nÂ  FROM\nÂ  Â  `datastic.variables.base_table`)\n\n-- Main Query\nSELECT\nÂ  *\nFROM\nÂ  `datastic.variables.base_table`,\nÂ  variable\nWHERE\nÂ  product_revenue &gt;= product_average\n\nFor basic structure, see (Static values using CTE &gt;&gt; 1 variable, 1 value)\nValue is calculated in the SELECT statement and stored as â€œproduct_averageâ€\n\n\n1 variable, multiple values\n\nExample: current product names\nWITH\nÂ  variable AS (\nÂ  SELECT\nÂ  Â  product_name AS product_threshold\nÂ  FROM\nÂ  Â  `datastic.variables.base_table`\nÂ  WHERE\nÂ  Â  product_name LIKE '%Google%')\n\n-- Main Query\nSELECT\nÂ  *\nFROM\nÂ  `datastic.variables.base_table`,\nÂ  variable\nWHERE\nÂ  product_name IN (product_threshold)\n\nFor basic structure, see (Static values using CTE &gt;&gt; 1 variable, multiple values)\nCTE\n\nProduct names with â€œGoogleâ€ are stored in â€œproduct_thresholdâ€\n\n\n\nMultiple variables, 1 value\nWITH\nÂ  variable AS (\nÂ  SELECT\nÂ  Â  MIN(order_date) AS first_order,\nÂ  Â  MAX(order_date) AS last_order\nÂ  FROM\nÂ  Â  `datastic.variables.base_table_2`)\n\nÂ  -- Main Query\nSELECT\nÂ  a.*\nFROM\nÂ  `datastic.variables.base_table` a,\nÂ  variable\nWHERE\nÂ  order_date BETWEEN first_order\nÂ  AND last_order\n\nBasically the same as the 1 variable, 1 value example\nCTE\n\nvariable is the name of the CTE where â€œfirst_orderâ€ and â€œlast_orderâ€ are stored\n\nQuery\n\nNot idea why â€œa.*â€ is used here\n\n\n\n\n\nProcedural Language\n\nMisc\n\nDocs\nNotes from\n\nBigQuery SQL Procedural Language to Simplify Data Engineering\n\n\nDeclare/Set\n\nDECLARE statement initializes variables\nSET statement will set the value for the variable\nExample: Basic\n\nExample: SET within IF/THEN conditional\n\n\nChecks if a table had the latest data before running the remaining SQL\nProcedure\n\nchecks the row count of the prod_data table where the daily_date field is equal to 2022â€“11â€“18 and sets that value to the rowcnt variable\nusing IF-THEN conditional statements\n\nIf rowcnt is equal to 1, meaning if thereâ€™s data found for 2022â€“11â€“18, then the string FOUND LATEST DATA will be shown.\nElse the latest_date is set to the value of the max date in the prod_data table and DATA DELAYED is displayed along with the value of latest_date.\n\n\nResult: data wasnâ€™t found and the latest_date field shows 2022â€“11â€“15.\n\n\nLoop/Leave\n\nExample: Loops until a condition is met before running your SQL statements\n\n\nContinues from 2nd Declare/Set example\nProcedure\n\nA counter variable is added with default = -1\nSubtract days from 2022â€“11â€“18 using the date_sub function by the counter variable until the rowcnt variable equals 1.\nOnce rowcnt equals 1 the loop ends using the LEAVE statement\n\n\n\nTable Function\n\na user-defined function that returns a table\nDocs\nCan be used anywhere a table input is authorized\n\ne.g.Â subqueries, joins, select/from, etc.\n\nExample: Creating\nCREATE OR REPLACE TABLE FUNCTION mydataset.names_by_year(y INT64)\nAS\nÂ  SELECT year, name, SUM(number) AS total\nÂ  FROM `bigquery-public-data.usa_names.usa_1910_current`\nÂ  WHERE year = y\nÂ  GROUP BY year, name\n\ny is the variable and its type is INT64\n\nExample: Usage\nSELECT * FROM mydataset.names_by_year(1950)\nÂ  ORDER BY total DESC\nÂ  LIMIT 5\nExample: Delete\nDROP TABLE FUNCTION mydataset.names_by_year"
  },
  {
    "objectID": "qmd/google-bigquery.html#sec-goog-bigq-remote",
    "href": "qmd/google-bigquery.html#sec-goog-bigq-remote",
    "title": "BigQuery",
    "section": "Remote Functions",
    "text": "Remote Functions\n\nUser defined functions (UDF)\nNotes from\n\nRemote Functions in BigQuery\nBigQuery UDFs Complete Guide\n\nDocs\nUseful in situations where you need to run code outside of BigQuery, and situations where you want to run code written in other languages\nDonâ€™t want go overboard with remote functions because they have performance and cost disadvantages compared to native SQL UDFs\n\nyouâ€™ll be paying for both BigQuery and Cloud Functions.\n\nUse Cases\n\nInvoke a model on BigQuery data and create a new table with enriched data. This also works for pre-built Google models like Google Translate and Vertex Entity Extraction\n\nNon-ML enrichment use cases include geocoding and entity resolution.\nif your ML model is in TensorFlow, I recommend that you directly load it as a BigQuery ML model. That approach is more efficient than Remote Functions.\n\nLook up real-time information (e.g.Â stock prices, currency rates) as part of your SQL workflows.\n\nExample: a dashboard or trading application simply calls a SQL query that filters a set of securities and then looks up the real-time price information for stocks that meet the selection criteria\nWITH stocks AS (\nSELECT\nÂ  symbol\nWHERE\nÂ  ...\n)\nSELECT symbol, realtime_price(symbol) AS price\nFROM stocks\n\nWhere realtime_price is a remote function\n\n\nReplace Scheduled ETL with Dynamic ELT\n\nELT as need can result in a significant reduction in storage and compute costs\n\nImplement hybrid cloud workflows.\n\nMake sure that the service you are invoking can handle the concurrency\n\nInvoking legacy code from SQL"
  },
  {
    "objectID": "qmd/google-bigquery.html#sec-goog-bigq-flex",
    "href": "qmd/google-bigquery.html#sec-goog-bigq-flex",
    "title": "BigQuery",
    "section": "Flex Slots",
    "text": "Flex Slots\n\nDocs\nFlex slots are like spot instances on aws but for running queries\n\nDocs\nA BigQuery slot is a virtual CPU used by BigQuery to execute SQL queries.\nBigQuery automatically calculates how many slots are required by each query, depending on query size and complexity\n\nUsers on Flat Rate commitments no longer pay for queries by bytes scanned and instead pay for reserved compute resources (â€œslotsâ€ and time)\n\nWith on-demand pricing, you pay for the cost of the query and bytes scanned\n\nUsing Flex Slots commitments, users can now cancel the reservation anytime after 60 seconds.\n\nAt $20/500 slot-hours, billed per second, Flex Slots can offer significant cost savings for On-Demand customers whose query sizes exceed 1TiB.\nview reservation assignments on the Capacity Management part of the BigQuery console\n\nAn hourâ€™s worth of queries on a 500 slot reservation for the same price as a single 4TiB on-demand query (currently priced at $5/TiB)\nExperiment\n\n\n\nNot sure why there arenâ€™t lower count on-demand slots. Maybe you have to use 2000 slots for on-demand.\nX-axis is the duration of the query\n\n\nYouâ€™re charged by the minute (I think) with 1 minute being the minimum of Idle time."
  },
  {
    "objectID": "qmd/google-bigquery.html#sec-goog-bigq-opt",
    "href": "qmd/google-bigquery.html#sec-goog-bigq-opt",
    "title": "BigQuery",
    "section": "Optimization",
    "text": "Optimization\n\nMisc\n\nAlso see DB, Engineering &gt;&gt; Cost Optimization and SQL &gt;&gt; Best Practices\nNotes from 14 Ways to Optimize BigQuery SQL Performance\nSet-up Query Monitoring:\n\nGoals\n\nspot expensive/heavy queries executed by anyone from the organization. The data warehouse can be shared among the entire organization including people who donâ€™t necessarily understand SQL but still try to look for information. An alert is to warn them about the low-quality of the query and Data Engineers can help them with good SQL practices.\nspot expensive/heavy scheduled queries at the early stage. Itâ€™s going to be risky if a scheduled query is very expensive. Having the alerting in place can prevent a high bill at the end of the month.\nunderstand the resource utilization and do a better job on capacity planning.\n\nGuide\n\nâ€œBytes shuffledâ€ affects query time; â€œBytes processedâ€ affects query cost\n\nLIMIT speeds up performance, but doesnâ€™t reduce costs\n\nFor data exploration, consider using BigQueryâ€™s (free) table preview option instead.\nThe row restriction of LIMIT clause is applied after SQL databases scan the full range of data. Hereâ€™s the kicker â€” most distributed database (including BigQuery) charges based on the data scans but not the outputs, which is why LIMIT doesnâ€™t help save a dime.\nTable Preview\n\n\nallows you to navigate the table page by page, up to 200 rows at a time and itâ€™s completely free\n\n\nAvoid using SELECT * . Choose only the relevant columns that you need to avoid unnecessary, costly full table scans\n\nWith row-based dbs, all columns get read anyway, but with columnar dbs, like BigQuery, you donâ€™t have to read every column.\n\nUse EXISTS instead of COUNT when checking if a value is present\n\nIf you donâ€™t need the exact count, use EXISTS() because it exits the processing cycle as soon as the first matching row is found\n\nSELECT EXISTS (\nÂ  SELECT\nÂ  Â  number\nÂ  FROM\nÂ  Â  `bigquery-public-data.crypto_ethereum.blocks`\nÂ  WHERE\nÂ  Â  timestamp BETWEEN \"2018-12-01\" AND \"2019-12-31\"\nÂ  Â  AND number = 6857606\n)\nUse Approximate Aggregate Functions\n\nWhen you have a big dataset and you donâ€™t need the exact count, use approximate aggregate functions instead\nUnlike the usual brute-force approach, approximate aggregate functions use statistics to produce an approximate result instead of an exact result.\n\nExpects the error rate to be ~ 1 to 2%.\n\nAPPROX_COUNT_DISTINCT()\nAPPROX_QUANTILES()\nAPPROX_TOP_COUNT()\nAPPROX_TOP_SUM()\nHYPERLOGLOG++\n\nSELECT\nÂ  APPROX_COUNT_DISTINCT(miner)\nFROM\nÂ  `bigquery-public-data.crypto_ethereum.blocks`\nWHERE\nÂ  timestamp BETWEEN '2019-01-01' AND '2020-01-01'\nReplace Self-Join with Windows Function\n\nSelf-join are always inefficient and should only be used when absolutely necessary. In most cases, we can replace it with a window function.\nA self-join is when a table is joined with itself.\n\nThis is a common join operation when we need a table to reference its own data, usually in a parent-child relationship.\n\nExample\nWITH\nÂ  cte_table AS (\nÂ  SELECT\nÂ  Â  DATE(timestamp) AS date,\nÂ  Â  miner,\nÂ  Â  COUNT(DISTINCT number) AS block_count\nÂ  FROM\nÂ  Â  `bigquery-public-data.crypto_ethereum.blocks`\nÂ  WHERE\nÂ  Â  DATE(timestamp) BETWEEN \"2022-03-01\"\nÂ  Â  AND \"2022-03-31\"\nÂ  GROUP BY\nÂ  Â  1,2\nÂ  )\n\n/* self-join */\nSELECT\nÂ  a.miner,\nÂ  a.date AS today,\nÂ  a.block_count AS today_count,\nÂ  b.date AS tmr,\nÂ  b.block_count AS tmr_count,\nÂ  b.block_count - a.block_count AS diff\nFROM\nÂ  cte_table a\nLEFT JOIN\nÂ  cte_table b\nÂ  ON\nÂ  Â  DATE_ADD(a.date, INTERVAL 1 DAY) = b.date\nÂ  Â  AND a.miner = b.miner\nORDER BY\nÂ  a.miner,\nÂ  a.date\n\n/* optimized */\nSELECT\nÂ  miner,\nÂ  date AS today,\nÂ  block_count AS today_count,\nÂ  LEAD(date, 1) OVER (PARTITION BY miner ORDER BY date) AS tmr,\nÂ  LEAD(block_count, 1) OVER (PARTITION BY miner ORDER BY date) AS tmr_count,\nÂ  LEAD(block_count, 1) OVER (PARTITION BY miner ORDER BY date) - block_count AS diff\nFROM\nÂ  cte_table a\n\nORDER BY or JOIN on INT64 columns if you can\n\nWhen your use case supports it, always prioritize comparing INT64 because itâ€™s cheaper to evaluate INT64 data types than strings.\nIf the join keys belong to certain data types that are difficult to compare, then the query becomes slow and expensive.\ni.e.Â join on an int instead of a string\n\nInstead of NOT IN , use NOT EXISTS operator to write anti-joins because it triggers a more resource-friendly query execution plan\n\nanti-joinÂ  - a JOIN operator with an exclusion clause WHERE NOT IN , WHERE NOT EXISTS, etc) that removes rows if it has a match in the second table.\nSee article for an example\n\nIn any complex query, filter the data as early in the query as possible\n\nApply filtering functions early and often in your query to reduce data shuffling and wasting compute resources on irrelevant data that doesnâ€™t contribute to the final query result\ne.g.Â SELECT DISTINCT , INNER JOIN , WHERE , GROUP BY\n\nExpressions in your WHERE clauses should be ordered with the most selective expression first\n\nDoesnâ€™t matter except for edge cases (e.g.Â the example below didnâ€™t result in a faster query) such as:\n\nIf you have a large number of tables in your query (10 or more).\nIf you have several EXISTS, IN, NOT EXISTS, or NOT IN statements in your WHERE clause\nIf you are using nested CTE (common table expressions) or a large number of CTEs.\nIf you have a large number of sub-queries in your FROM clause.\n\nNot optimized\nWHERE\nÂ  miner LIKE '%a%'\nÂ  AND miner LIKE '%b%'\nÂ  AND miner = '0xc3348b43d3881151224b490e4aa39e03d2b1cdea'\n\nThe LIKE expressions are string searches which are expensive so they should be towards the end\nThe expression with the â€œ=â€ operator is the â€œmost selectiveâ€ expression since itâ€™s for a particular value of â€œminer,â€ so it should be near the beginning\n\nOptimized\nWHERE\nÂ  miner = '0xc3348b43d3881151224b490e4aa39e03d2b1cdea'\nÂ  AND miner LIKE '%a%'\nÂ  AND miner LIKE '%b%'\n\nUtilize PARTITIONS and/or CLUSTERS to significantly reduce amount of data thatâ€™s scanned - Clustering divides the table into even smaller chunks than partition - A Clustered Table sorts the data into blocks based on the column (or columns) that we choose and then keeps track of the data through a clustered index. - During a query, the clustered index points to the blocks that contain the data, therefore allowing BigQuery to skip through irrelevant ones. The process of skipping irrelevant blocks on scanning is known as block pruning.\n\nMisc\n\nAlso see\n\nDB, Engineering &gt;&gt; Cost Optimization &gt;&gt; Partitions and Indexes for CLUSTER\nSQL &gt;&gt; Partitions\nPartitioning Docs\nClustering Docs\n\nNotes from\n\noriginal optimization article\nHow to Use Partitions and Clusters in BigQuery Using SQL\n\nUse BOTH partitions and clusters on tables that are bigger than 1 GB to segment and order the data.\n\nFor big tables, itâ€™s beneficial to both partition and cluster.\n\nLimits\n\n4,000 partitions per table\n4 cluster columns per table\n\nInfo about partititoning and cluster located in Details tab of your table\n\n\nPartitioning\n\nMisc\n\nPartition columns should always be picked based on how you expect to use the data, and not depending on which column would evenly split the data based on size.\n\nExample: partition on county because your analysis or transformations will largely be done by county even though since some counties may be much larger than others and will cause the partitions to be substantially imbalanced.\n\n\nTypes of Partition Keys\n\nTime-unit column: Tables are partitioned based on a time value such as timestamps or dates.\nIngestion time: Tables are partitioned based on the timestamp when BigQuery ingests the data.\nInteger range: Tables are partitioned based on a number.\n\nExample: Partition by categorical\nCREATE TABLE database.zoo_partitioned\nPARTITION BY zoo_name AS\nÂ  (SELECT *\nÂ  FROM database.zoo)\nExample: Partition by date\nCREATE OR REPLACE TABLE\n  `datastic.stackoverflow.questions_partitioned`\nPARTITION BY\nDATE_TRUNC(creation_date,MONTH) AS (\n  SELECT\n  *\n  FROM\n  `datastic.stackoverflow.questions`)\n\nâ€œcreation_dateâ€ is truncated to a month which reduces the number of partitions needed for this table\n\nDays would exceed the 4000 partition limit\n\nPartition Options\n\npartition_expiration_days: BigQuery deletes the data in a partition when it expires. This means that data in partitions older than the number of days specified here will be deleted.\nrequire_partition_filter: Users canâ€™t query without filtering (WHERE clause) on your partition key.\n\nExample: Set options\nCREATE OR REPLACE TABLE\nÂ  `datastic.stackoverflow.questions_partitioned`\nPARTITION BY\nDATE_TRUNC(creation_date,MONTH) OPTIONS(partition_expiration_days=180,\nÂ  Â  require_partition_filter=TRUE) AS (\nÂ  SELECT\nÂ  Â  *\nÂ  FROM\nÂ  Â  `datastic.stackoverflow.questions`)\n\nALTER TABLE\nÂ  `datastic.stackoverflow.questions_partitioned`\nSET\nÂ  OPTIONS(require_partition_filter=FALSE,partition_expiration_days=10)\n\n\nClustering\n\nBest with values that have high cardinality, which means columns with various possible values such as emails, user ids, names, categories of a product, etcâ€¦\nAble cluster on multiple columns and you can cluster different data types (STRING, DATE, NUMERIC, etcâ€¦)\nExample: Cluster by categorical\nCREATE TABLE database.zoo_clustered\nCLUSTER BY animal_name AS\nÂ  (SELECT *\nÂ  FROM database.zoo)\nExample: Cluster by tag\nCREATE OR REPLACE TABLE\nÂ  `datastic.stackoverflow.questions_clustered`\nCLUSTER BY tags AS (Â \nÂ  SELECT\nÂ  Â  *\nÂ  FROM\nÂ  Â  `datastic.stackoverflow.questions`)\n\nPartition and Cluster\n\nExample\nCREATE OR REPLACE TABLE\nÂ  `datastic.stackoverflow.questions_partitioned_clustered`\nPARTITION BY\nÂ  DATE_TRUNC(creation_date,MONTH)\nCLUSTER BY\nÂ  tags AS (\nÂ  SELECT\nÂ  Â  *\nÂ  FROM\nÂ  Â  `datastic.stackoverflow.questions`)\n\n\nUse ORDER BY only in the outermost query or within window clauses (analytic functions)\n\nOrdering is a resource intensive operation that should be left until the end since tables tend to be larger at the beginning of the query.\nBigQueryâ€™s SQL Optimizer isnâ€™t affected by this because itâ€™s smart enough to recognize and run the order by clauses at the end no matter where theyâ€™re written.\n\nStill a good practice though.\n\n\nPush complex operations, such as regular expressions and mathematical functions to the end of the query\n\ne.g.Â REGEXP_SUBSTR()Â  and SUM()\n\nUse SEARCH() for nested data\n\nCan search for relevant keywords without having to understand the underlying data schema\n\nTokenizes text data, making it exceptionally easy to find data buried in unstructured text and semi-structured JSON data\n\nTraditionally when dealing with nested structures, we need to understand the table schema in advance, then appropriately flatten any nested data with UNNEST() before running a combination of WHERE and REGEXP clause to search for specific terms. These are all compute-intensive operators.\nExample\n-- old way\nSELECT\nÂ  `hash`,\nÂ  size,\nÂ  outputs\nFROM\nÂ  `bigquery-public-data.crypto_bitcoin.transactions`\nCROSS JOIN\nÂ  UNNEST(outputs)\nCROSS JOIN\nÂ  UNNEST(addresses) AS outputs_address\nWHERE\nÂ  block_timestamp_month BETWEEN \"2009-01-01\" AND \"2010-12-31\"\nÂ  AND REGEXP_CONTAINS(outputs_address, '1LzBzVqEeuQyjD2mRWHes3dgWrT9titxvq')\n\n-- with search()\nSELECT\nÂ  `hash`,\nÂ  size,\nÂ  outputs\nFROM\nÂ  `bigquery-public-data.crypto_bitcoin.transactions`\nWHERE\nÂ  block_timestamp_month BETWEEN \"2009-01-01\" AND \"2010-12-31\"\nÂ  AND SEARCH(outputs, â€˜`1LzBzVqEeuQyjD2mRWHes3dgWrT9titxvq`â€™)\nCreate a search index for the column to enable point-lookup text searches\n# To create the search index over existing BQ table\nCREATE SEARCH INDEX my_logs_index ON my_table (my_columns);\n\nCaching\n\nBigQuery has a cost-free, fully managed caching feature for our queries\nBigQuery automatically caches query results into a temporary table that lasts for up to 24 hours after the query has ran.\n\nCan toggle the feature through Query Settings on the Editor UI\n\nCan verify whether cached results are used by checking â€œJob Informationâ€ after running the query. The Bytes processed should display â€œ0 B (results cached)â€.\nNot all queries will be cached. Exceptions include: A query is not cached when it uses non-deterministic functions, such as CURRENT_TIMESTAMP(), because it will return a different value depending on when the query is executed.\n\nWhen the table referenced by the query received streaming inserts because any changes to the table will invalidate the cached results. If you are querying multiple tables using a wildcard."
  },
  {
    "objectID": "qmd/google-bigquery.html#sec-goog-bigq-mod",
    "href": "qmd/google-bigquery.html#sec-goog-bigq-mod",
    "title": "BigQuery",
    "section": "Modeling",
    "text": "Modeling\n\nMisc\n\nDocs\nmodel options\n\nTrain/Validation/Test split\n\nCreate or choose a unique column\n\nCreate\n\nUse a random number generator function such as RAND() or UUID()\nCreate a hash of a single already unique field or a hash of a combination of fields that creates a unique row identifier\n\nFARM_FINGERPRINT() is a common function\n\nAlways gives the same results for the same input\nReturns an INT64 value (essentially a number, rather than a combination of numbers and characters) that we can control with other mathematical functions such as MOD() to produce our split ratio.\nOthers donâ€™t have these qualities, e.g.Â MD5() or SHA()\n\n\n\n\n\nBigQueryML\n\nSyntax\nCREATE MODEL dataset.model_name\nÂ  OPTIONS(model_type=â€™linear_regâ€™, input_label_cols=[â€˜input_labelâ€™])\nAS SELECT * FROM input_table;\n\nMake predictions with ML.PREDICT\n\nExample: Logistic Regression\nCREATE MODEL `mydata.adults_log_reg`\nOPTIONS(model_type='logistic_reg') AS\nSELECT *,\nÂ  ad.income AS label\nFROM\nÂ  `mydata.adults_data` ad\n\nOutput\n\n\nModel appears in the sidebar alongside your data table. Click on your model to see an evaluation of the training performance."
  },
  {
    "objectID": "qmd/gtable-package.html",
    "href": "qmd/gtable-package.html",
    "title": "44Â  gtable package",
    "section": "",
    "text": "gtable\nTools to make it easier to work with â€œtablesâ€ of â€˜grobsâ€™. Undergirds ggplot2.\nNotes\n\nExample below is a typical ggplot scatterplot\nThe layout plot is jacked-up but you can still make out some coordinates\n\n1st coordinate: is up and down Above the center cell, (7,5) is (6,5) and below is (8,5)\n2nd coordinate is left and right. Left of the center cell is (7,4) and right is (7,6)\n\n\ngrob_obj &lt;- ggplotGrob() or tableGrob(), etc.\ngtable_show_layout(grob_obj)\n\n[](./_resources/gtable_package.resources/unknown_filename.3.png|1024x0]]\nlayout dataframe with t, l, b, r coordinates\n\nt = top, l = left, b = bottom, r = right\n\nseeÂ Add line plot grob, then table grobÂ section below for details on how this works\n\n\n\nlayout_df &lt;- grob_obj$layout\n\noutputs a data.frame of cell coordinates for each grob of the graph\nthe (7,5) cell above is called the â€œpanelâ€ grob (which is where the points of scatter plot go)\n\nIn layout_df, it has cell coordinates t = 7, l = 5, b = 7, r = 5\nExcept for the background grob, all grobs repeat the 2 coordinates\n\ne.g.Â y-axis (â€œaxis-lâ€) grob is 7,4,7,4\nI think if one pair either t and l or b and r have different coordinates then the grob overlaps cells\n\n\nBetter example: adding a table to a line plot\n\ncreate the grobs for the table and line plot\nImagine your grid layout\n\nI want the table a litte right of the y-axis and around the middle in terms of the height\nThe goal is to carve out a cell in the layout to place your table into\n\n3 Rows: above the table is one row, the table itself is the second row and below the table is a third row\n3 Columns: left of the table is a column, the table itself is the second column, and right of the table is the third column\n\n\nSpecify the proportions of each row and column\n\n\ng_tab &lt;- gtable(widths = unit(c(0.4, 0.6, 2), \"null\"), heights = unit(c(0.8, 0.4, 1), \"null\"))\n\nwidths: widths of the columns\n\nSpecifying 3 units for the total width\n0.4 will include the y-axis and a little more\n0.6 is the width of the cell for my table\n2 is for the rest of the graph\nâ€œnullâ€ just means no units like â€œcmâ€ or â€œinâ€ is used\n(from left to right) number describe columns from left to right\n\nheights: heights of rows\n\nsimilar explanations as for widths\n(from left to right) numbers describe rows from top to bottom\n\nAdd line plot grob, then table grob\n\ng_tab &lt;- gtable_add_grob(g_tab, plot_grob, t = 1, b = 3, l = 1, r = 3)\n\norder matters, so add the plot_grob first\n\nthereâ€™s also a â€œzâ€ arg that can be used to specify the order you want them layered\n\nthe first grob added spans the layout\n\nunless you adding complicated shit like in the scatterplot example above. Then youâ€™d add this to the â€œpanelâ€ cell\nSo we have 3 rows, therefore t(top) = 1, b (bottom) = 3 spans the height of the layout\nAnd we have 3 columns, so l (left) = 1, r (right) = 3 spans the width of the layout\n\n\ngtable_show_layout(g_tab)\n\nCheck out the layout and make sure which cell you want your table in\n\n[](./_resources/gtable_package.resources/unknown_filename.png|1024x0]]\nThen add the table grob to that cell\n\n\ng_tab &lt;- gtable_add_grob(g_tab, tab_grob, t = 2, l = 2)\n\nonly need t and l or b and r to specify the location.\n\nBoth sets are the same since youâ€™re specifying only one cell\n\ne.g.Â if you wanted to span the 2 middle left cells itâ€™d be b = 2, t = 2, l = 1, r = 2\n\n\nCheck it out\n\ngrid::grid.draw(g_tab) or plot(g_tabb)\n\n[](./_resources/gtable_package.resources/unknown_filename.1.png|1024x0]]\n\ntop-right annotation block is from ggforce using regular methods\n\nSave it with ggsave( )\n\nggsave(\"R/Projects/local-corona/plots/pos-plot-tbl-gtab.png\", plot = g_tab, dpi = \"print\", width = 33, height = 20, units = \"cm\")\n\npics may show something different than what in your view or plots pane. So may have to adjust\nMore complicated example (layout not exactly the same in the script. Moved subtitle row to create a caption. See Indiana COVID-19 Tracker project for details)\n\nadd 2 tables to 2 line graphs to cells (4,2) and (4,5) along with a title and subtitle\n[](./_resources/gtable_package.resources/unknown_filename.2.png|1024x0]]\n\n\ntitle_grob &lt;- grobTree(rectGrob(gp = gpar(fill = \"black\")), textGrob(\"Indiana COVID-19\", hjust = 3.75, gp = gpar(fontsize = 15, col = \"white\")))\nsubtitle_grob &lt;- grobTree(rectGrob(gp = gpar(fill = \"black\")), textGrob(glue(\"Last updated: {label_dat$date[[1]]}\"), hjust = 4.05, gp = gpar(fontsize = 10, col = \"white\")))\n\n# construct layout\ngtab &lt;- gtable::gtable(widths = unit(c(0.4, 0.6, 1, 0.4, 0.6, 1), \"null\"), heights = unit(c(0.12, 0.08, 0.54, 0.43, 0.83), \"null\"))\n# add plots\ngtab &lt;- gtable::gtable_add_grob(gtab, pos_plot_grob, t = 3, b = 5, l = 1, r = 3)\ngtab &lt;- gtable::gtable_add_grob(gtab, dea_plot_grob, t = 3, b = 5, l = 4, r = 6)\n# add titles\ngtab &lt;- gtable::gtable_add_grob(gtab, title_grob, t = 1, b = 1, l = 1, r = 6)\ngtab &lt;- gtable::gtable_add_grob(gtab, subtitle_grob, t = 2, b = 2, l = 1, r = 6)\n\n# find cell coordinates to place tables\n# gtable::gtable_show_layout(gtab)\n\n# add tables, only need two coord since we don't want to table to span more than one cell\ngtab &lt;- gtable::gtable_add_grob(gtab, pos_tab_grob, t = 4, l = 2)\ngtab &lt;- gtable::gtable_add_grob(gtab, dea_tab_grob, t = 4, l = 5)\n\n# grid.draw(gtab)\n# grid.newpage()\n\n\n[](./_resources/gtable_package.resources/2020-04-02.png|3897x0]]"
  },
  {
    "objectID": "qmd/html.html#sec-html-misc",
    "href": "qmd/html.html#sec-html-misc",
    "title": "HTML",
    "section": "Misc",
    "text": "Misc\n\nSome JS libraries use custom attributes in html tags that have hyphens in their name. R hates hypens, but you can just put the attribute name in quotes and it works (e.g.Â data-sub-html).\n# see data-sub-html\ntags$a(\n  href = paste0(\"images/gallery/large/\", l),\n  \"data-sub-html\" = \"&lt;h4&gt;Photo by - &lt;a href='https://unsplash.com/@entrycube' &gt;Diego GuzmÃ¡n &lt;/a&gt;\",\n  tags$img(src = paste0(\"images/gallery/thumbnails/\", t))\n  )\nglue(\"&lt;b style='background-color:{color}; font-family:Roboto; font-size:15px'&gt;{value}&lt;/b&gt;\")\n\ncolor and value are variables\n\nglue(\"&lt;b style= 'font-family:Roboto; font-size:15px'&gt;{name}&lt;/br&gt;Combined Indicator&lt;/b&gt;: {value_text}\")\n\nname and value_text are variables\n\nhtml coment - &lt;!-- comment --&gt;\nwithTags - Instead of needing to specify tags each time a tag function is used, as in tags\\(div() and tags\\)p(), code inside withTags is evaluated with tags searched first, so you can simply use div() and p().\ntagList - takes a list of tag objects and combines them into html code\nExample: From my website gallery\n\nhtml\n&lt;!---- withTags part ----&gt;\n&lt;div class=\"row\" id=\"lightgallery\"&gt;\n  &lt;!---- tagsList part ----&gt;\n  &lt;a href=\"_gallery/img/images/gallery/large/excess-death-col.png\"&gt;\n    &lt;img src=\"_gallery/img/images/gallery/thumbnails/thumb-excess-death-col.png\"/&gt;\n  &lt;/a&gt;\n  &lt;a href=\"_gallery/img/images/gallery/large/pos-policy-one-2021-06-20.png\"&gt;\n    &lt;img src=\"_gallery/img/images/gallery/thumbnails/thumb-pos-policy-one-2021-06-20.png\"/&gt;\n  &lt;/a&gt;\n&lt;/div&gt;\n{htmltools}\n\nCreate list of tags\n# images_thumb, images_full_size are paths to png files\nmoose &lt;- \n  purrr::map2(images$images_thumb, images$images_full_size, \n    function(t, l) {\n      tags$a(\n        href = paste0(\"_gallery/img/\", l),\n                      tags$img(src = paste0(\"_gallery/img/\", \n                      t))\n      )\n    })\n\n#&gt; [[1]]\n#&gt; &lt;a href=\"_gallery/img/images/gallery/large/excess-death-col.png\"&gt;\n#&gt; &lt;img src=\"_gallery/img/images/gallery/thumbnails/thumb-excess-death-col.png\"/&gt;\n#&gt; &lt;/a&gt;\n\n#&gt; [[2]]\n#&gt; &lt;a href=\"_gallery/img/images/gallery/large/pos-policy-one-2021-06-20.png\"&gt;\n#&gt; &lt;img src=\"_gallery/img/images/gallery/thumbnails/thumb-pos-policy-one-2021-06-20.png\"/&gt;\n#&gt; &lt;/a&gt;\nConvert list of tags to code with tagsList\nsquirrel &lt;- tagsList(moose)\n\n#&gt; &lt;a href=\"_gallery/img/images/gallery/large/excess-death-col.png\"&gt;\n#&gt; &lt;img src=\"_gallery/img/images/gallery/thumbnails/thumb-excess-death-col.png\"/&gt;\n#&gt; &lt;/a&gt;\n#&gt; &lt;a href=\"_gallery/img/images/gallery/large/pos-policy-one-2021-06-20.png\"&gt;\n#&gt; &lt;img src=\"_gallery/img/images/gallery/thumbnails/thumb-pos-policy-one-2021-06-20.png\"/&gt;\n#&gt; &lt;/a&gt;\nInsert into a div frame with withTags\nwithTags(\n  div(\n    class = \"row\",\n    id = \"lightgallery\",\n    squirrel\n  )\n)\n#&gt; &lt;div class=\"row\" id=\"lightgallery\"&gt;\n#&gt;    &lt;a href=\"_gallery/img/images/gallery/large/excess-death-col.png\"&gt;\n#&gt;        &lt;img src=\"_gallery/img/images/gallery/thumbnails/thumb-excess-death-col.png\"/&gt;\n#&gt;    &lt;/a&gt;\n#&gt;    &lt;a href=\"_gallery/img/images/gallery/large/pos-policy-one-2021-06-20.png\"&gt;\n#&gt;        &lt;img src=\"_gallery/img/images/gallery/thumbnails/thumb-pos-policy-one-2021-06-20.png\"/&gt;\n#&gt;    &lt;/a&gt;\n#&gt; &lt;/div&gt;"
  },
  {
    "objectID": "qmd/information-theory.html",
    "href": "qmd/information-theory.html",
    "title": "45Â  Information Theory",
    "section": "",
    "text": "TOC\n\nMisc\nKullback-Lieber Divergence\nMutual Information\n\nMisc\nKullback-Lieber Divergence\n\nMeasures the similarity between the joint probability density function and the product of the individual density functions\n\nIf theyâ€™re the same, then both variables are independent\n\nAlso see Statistical Rethinking &gt;&gt; Ch 7\nExample: Measuring Segregation from Did Residential Racial Segregation in the U.S. Really Increase? \n\np(g|u) is the proportion of a racial group, g, in a neighborhood, u\np(g) is the overall proportion of that racial group in the metropolitan area\nThis is a sum of scores across all racial groups of a neighborhood, u\n\n\nMutual Information\n\nMeasures how dependent two random variables are on one another\n\nAccounts for linear and non-linear dependence\n\nIf the mutual information is 0, the variables are independent, otherwise there is some dependence.\nExample: Measuring Segregation (see example in Kullback-Lieber Divergence section)\n\nLu see above\np(u) is described as the â€œsize of the neighborhoodâ€\n\nNot sure if this is a count or a proportion of the population of the neighborhood to the population of the metropolitan area. Both may end up in the same place.\n\nThis is a sum of scores across all neighborhoods in a metropolitan area\n\nSo the neighborhood scores are weighted by neighborhood population and summed for an overall metropolitan score\nL(u) is affected by the smallest racial proportion (see article) for that metropolitan area, so unless these are the same, you canâ€™t compare metropolitan areas with this number. But you can use these numbers to see how a metroâ€™s (or neighborhoodâ€™s) diversity has changed over time."
  },
  {
    "objectID": "qmd/inkscape.html#sec-ink-misc",
    "href": "qmd/inkscape.html#sec-ink-misc",
    "title": "Inkscape",
    "section": "Misc",
    "text": "Misc\n\nIn order to save something, it must be converted a â€œpathâ€\n\nPath &gt;&gt; Object to Path\n\nThe bottom row of buttons is determined by what tool is selected in the left panel\nRotate object\n\nOption 1\n\nClick select tool (&gt;&gt; select object if necessary)\nSelect object a second time to get the rotation arrows\nHold ctrl (in order to rotate in descrete degree units, e.g.Â 45, 90) or donâ€™t if want to rotate in continuous degrees\nClick, hold, and drag one of the arrows to rotate the object\n\nOption 2\n\nClick select tool (&gt;&gt; select object if necessary)\nHold ctrl+shift and rotate using mouse wheel\n\n\nConvert object to path\n\nNotes\n\nTo perform certain actions between 2 or greater objects, then need to be paths\n\ne.g.Â Creating 3d objects by Nickâ€™s union-method or by using Extensions &gt;&gt; generate from path &gt;&gt; Extrude\n\nBottom bar (middle, below color palette) shows what you the type of thing youâ€™ve selected. Should say path or group of paths is thatâ€™s what you got.\n\nSteps (object)\n\nSelect object with select tool (S key or left panel, top)\nPath &gt;&gt; Object to Path\nObject &gt;&gt; ungroup\nPath &gt;&gt; union\n\nSteps (stroke or outline)\n\nPath &gt;&gt; Stroke to Path\nPath &gt;&gt; break apart\nPath &gt;&gt; union\n\n\nRemove outline from an object\n\nHold shift and click the boxed X at the far left on the color bar (bottom, far left)"
  },
  {
    "objectID": "qmd/inkscape.html#sec-ink-keysh",
    "href": "qmd/inkscape.html#sec-ink-keysh",
    "title": "Inkscape",
    "section": "Keyboard shortcuts",
    "text": "Keyboard shortcuts\n\nGrab and object and move it to a different location\n\nClick on select tool then just click, hold, and drag object\n\nZoom-in/out\n\nHold ctrl and zoom using mouse-wheel\n\nâ€œ1â€ zooms out to 100%\nPan around canvas\n\nPress and hold mouse wheel and drag mouse\n\nScale up/down object\n\nSelect an object (may need to convert to path first)\nShift+ctrl and scale by dragging arrows around object\n\nSelect different layers\n\nUse the select tool to click the upper most layer of an object\nCheck out the fill color below the color bar (bottom) to tell you which layer you have\nHold alt and click the same spot again. This will select the next layer underneath.\nCheck out the fill color below the color bar (bottom) to tell you which layer you have\nRepeat until you have the desired layer selected.\n\nAlign objects\n\nctrl+shift+a"
  },
  {
    "objectID": "qmd/inkscape.html#sec-ink-setup",
    "href": "qmd/inkscape.html#sec-ink-setup",
    "title": "Inkscape",
    "section": "Set-up",
    "text": "Set-up\n\nRemove or set page border to desired size\n\nctrl+shift+d\nChoose page size or untick â€œShow page borderâ€\nCan also set the canvas measure to pixels\n\nSet to pixels\n\n3rd row &gt;&gt; towards end (usually set to mm)\n\nView set to custom\n\nView &gt;&gt; tick Custom at the bottom of the menu\n\nZoom to 1:1\n\nView &gt;&gt; Zoom &gt;&gt; Zoom 1:1\n\nOpen Align and Distribute objects\n\nSymbol: horizontal bar graph,\nEnd 2nd row\nSet â€œRelative toâ€ to Last selected\n\nOpen Edit Objects colors, gradients, etc\n\nSymbol: half triangle + diagonal paintbrush,\nEnd 2nd row"
  },
  {
    "objectID": "qmd/inkscape.html#sec-ink-img",
    "href": "qmd/inkscape.html#sec-ink-img",
    "title": "Inkscape",
    "section": "Images",
    "text": "Images\n\nConvert to a vector object (path/bitmap)\n\nProbably worthwhile to duplicate the image before breaking apart so you have a reference of how everything fits together\nSteps\n\nDrag png into Inkscape\nSelect image (if not already selected)\nPath &gt;&gt; Trace Bitmap\nChoose Single Scan (Black and White) or Multiple Scans (Color)\nChoose Algorithm\n\nAll do a little something different\nAutotrace seems good for B&W images\nMess with settings to see if it improves\nSome algorithms can be very computationally expensive, so monitor resources\n\nWhile selected, Path &gt;&gt; break apart\n\nDefaults for opitions\n\nSpeckles: 2\nSmooth corners: 1\nOptimize: 0.20\n\nAfter conversion and break apart, recommend selecting whole object and lowering opacity to get a sense of the different layers.\n\nClipping\n\nExample taking a square portrait and cutting into an oval\nSteps\n\nDrag image into Inkscape\nSelect the circle tool, (left-panel, top)\nClick and drag circle to an approximate shape\nChange fill color (right panel or bottom color bar) and reduce opacity (right panel)\nClick and drag the center of the circle (there should be an â€œxâ€) to the center of the area you want to preserve after clipping\nClick and drag edges of circle by nodes to finalize shape\nSwitch to select tool (left panel, top)\n(Circle should be already selected) Hold shift and click outer edge of photo to also select photo\nObject &gt;&gt; clip &gt;&gt; set"
  },
  {
    "objectID": "qmd/inkscape.html#sec-ink-txt",
    "href": "qmd/inkscape.html#sec-ink-txt",
    "title": "Inkscape",
    "section": "Text",
    "text": "Text\n\nNotes\n\nAfter conversion from object to path, text is no longer editable\n\nI think this means stylistically - like font family, bold, text size, etc.\n\n\nAdd text to canvas\n\nClick Text Icon (left panel, bottom)\nSymbol: AI\nLeft panel &gt;&gt; middle\n\nPaste text from outside source\n\nClick Text icon (left panel, bottom)\nDrag out a box big enough to hold the text\nctrl+v\n\nChange Font\n\nctrl + shift + T or Text menu &gt;&gt; Text and Font\n\nAdd outline color to text\n\nOption 1\n\nSelect tool (arrow, left panel) &gt;&gt; select object\nClick â€œstroke paintâ€ in right panel &gt;&gt; choose color\n\nHSL (Hue, Saturation, Luminosity) and A (alpha, aka opacity)\nEnter a hex color\n\nClick â€œstroke styleâ€ and adjust width\n\nMay want to use pixels\n\nIn â€œstroke styleâ€ window, you can select select a type of â€œjoinâ€ that give smooth edges or pointed, etc.\n\nOption 2\n\nSelect tool (arrow, left panel) &gt;&gt; select object\nHold shift and select color on the palette bar at the bottome of the screen\nAdjust hue, stroke width, join, etc. in the right panel (see option 1)\n\n\nBreak apart text into individual letters\n\nObject to path (see Misc section)\nUngroup\n\nText Portraits\n\nPNGs get a texture of words (or numbers)\nPNGs\n\nIf you have colors, convert png to greyscale (extensions &gt;&gt; color &gt;&gt; grayscale)\nIf pngs has layers, all layers need to have opacity = 1\nPNG background probably needs to be black to get maximal contrast\n\nFor numbers (useful as a logos for data packages), just knit a Rmd and copy/paste the numbers. Itâ€™s suprisingly hard to find a document online thatâ€™s full on numbers to copy and paste.\n---\ntitle: \"\"\noutput: html_document\n---\n\n```{r, echo=FALSE}\noptions(scipen=999)\n```\n`r stringr::str_remove_all(toString(sample.int(1e4, 1e4)),\",\")`\n\nTo paste text, select text tool (left panel, bottom), place cursor on canvas, ctrl+v\n\nDonâ€™t create a text box to paste text into or paste it, select it and reshape itÂ  (see below)\n\nText &gt;&gt; â€œflow into frameâ€ is required but itâ€™s picky as hell\n\nIf you use text that isnâ€™t generated by an extension inside Inkscape, you have to paste it directly onto the canvas and NOT into a text box or other shape. Otherwise, â€œflow into frameâ€ will not work. \n\nThe dashed line is actually a string of text but Iâ€™ve zoomed out so much that you canâ€™t tell. You have to work with the text in this form (e.g.Â font, color, spacing, et.) before you selecting it and using â€œflow into frameâ€ to place into some shape of container.\n\nIf you try to paste it into a text box and â€œflow into frameâ€ the text disappears or is placed outside the box like some funky glitch.\n\n\n\nSelecting the text and not the shape\n\nOnce youâ€™ve flowed the text into a shape, youâ€™ll need to center the alignment. First, you have to select the text before selecting the text tool and aligning\nSteps\n\nClick outside the shape to deselect all objects\nZoom into the box, so that the text is pretty large\nPlace selecting arrow on a letter or number of text and click\nCheck bottom status bar to confirm â€œText-in-shapeâ€ has been selected."
  },
  {
    "objectID": "qmd/inkscape.html#sec-ink-shp",
    "href": "qmd/inkscape.html#sec-ink-shp",
    "title": "Inkscape",
    "section": "Shapes",
    "text": "Shapes\n\nCircle\n\nSelect circle in left pane,\nMove cursor to location,\nHold ctrl+shift\nDrag mouse outward/inward until desired size\n\nLine\n\nSteps\n\nClick draw bezier curves (left pane, middle)\nClick snap-to-cusp-nodes (top row, middle) if not already selected\nClick location on canvas\nHold ctrl and drag mouse horizontal or vertical\nRelease mouse button then ctrl\nPress enter\n\nColor\n\nSelect object and convert to path using â€œstroke to pathâ€\nChoose color from bottom color bar or from right panel"
  },
  {
    "objectID": "qmd/inkscape.html#sec-ink-crbkgd",
    "href": "qmd/inkscape.html#sec-ink-crbkgd",
    "title": "Inkscape",
    "section": "Create a Background",
    "text": "Create a Background\n\nClick square shape from left panel\nFrom location on canvas, drag mouse outward to desired size (see size in width/heigth in 3rd row, middle)\n\nOr just drag until square is large enough to encase your object\n\nClick select on left panel and select square\nClick color on color bar (bottom)\n\nOr from right panel &gt;&gt; click fill &gt;&gt; adjust HSL or enter a hex color\n\nIf necessary, remove outline by holding shift and click â€œXâ€ on color bar (bottom, left-side)\nDrag square in front of object\nClick â€œlower selection to bottomâ€ (3rd row, left side) and adjust position fo square"
  },
  {
    "objectID": "qmd/inkscape.html#sec-ink-nodes",
    "href": "qmd/inkscape.html#sec-ink-nodes",
    "title": "Inkscape",
    "section": "Nodes",
    "text": "Nodes\n\nAdd nodes to segment\n\nClick â€œedit paths by nodesâ€ (left panel, top)\nDrag box around desired segment\nClick â€œinsert new nodes into selected segmentsâ€ (bottom row, far left)\n\nRepeat to insert desired number of nodes into segment(s)\n\n\nManipulate length or angle of segment by node\n\nDiscrete increments â€“&gt; hold ctrl and drag node\nContinuous increments â€“&gt; drag node"
  },
  {
    "objectID": "qmd/job-management-leadership.html",
    "href": "qmd/job-management-leadership.html",
    "title": "46Â  Job, Management / Leadership",
    "section": "",
    "text": "TOC\n\nMisc\nTerms\nGetting the promotion\nManagerial Hats\nPreparation for a managerial role Meetings\nTools for Servant Leadership\nNonviolent Communication\nWays to promote projects completed by your team\n\nMisc\n\nTry really hard not to send messages outside of work hours\nEmphasize unplugging during vacations\nProvide immediate feedback - positive and negative\nDedicate time to freeform exploration\n\nSometimes the rest of the business doesnâ€™t know what to ask of your data org. Thatâ€™s why you need to give your team time to explore.\nTeam members can come to new and exciting conclusions when theyâ€™re given time to explore the data for fun. They can apply their talents to looking for patterns that no one has requested, and have the space to uncover new discoveries. This freeform exploration can lead to game-changing innovations that no business stakeholder would have imagined were possible.\nHelps keep your most valuable team members engaged and satisfied in their work.\n\nWhen first starting, request documentation\n\nRelevant server locations & descriptions\nLocations of our documentation and dashboards\nA list of tools/software that are available to be used\nA list of relevant stakeholders/gatekeepers that Iâ€™d need to make contact with\n\n\nTerms\n\nFocus Time - uninterrupted time, usually refers to a period of time (e.g.Â 2 hrs) where people can work without any distractions\nReport (aka Individual Contributor (IC))- People who report to the manager\n\nGetting the Promotion\n\nYou donâ€™t get a promotion and THEN start to perform at the next level; you perform at the next level IN ORDER TO get a promotion.\n\nSo when you notice a gap somewhere, even if it doesnâ€™t necessarily fall into your current role description, donâ€™t be afraid to bring it up to your manager and discuss whether you can/should take initiative to help plug the gap.\nThe best way to notice gaps is to be a good listener and constantly communicate with your partners & stakeholders about their teamsâ€™ work and pain points.\n\nMentor a peer\n\nIf you have new members joining the team, offer to be an onboarding buddy to guide them through their first few weeks.\nbrainstorm with team members when then need help\n\nStep out of your immediate scope\nGet involved in team-level activities\n\nHelp out with things such as sprint planning, quarterly planning, etc.\n\nAllows you to gain knowledge about other team membersâ€™ work and other teamsâ€™ requests for your team\nGives you some exposure to the managerâ€™s plan and vision for the team\n\nVolunteering for culture initiatives is a great way to practice thinking about the team as a whole\nTake on projects that help the whole team\n\nproduct design doc for the data product\nSLA agreement with partner teams (?)\nCodify the best practices you use in your own work\n\n\nHave open, timely feedback conversations with your manager\n\nAsk for the leveling guide when you have the initial career development conversation with your manager.\n\nAnd make sure you mention your aspiration to be a manager as soon as possible (donâ€™t be shy) as well as your aspired timeline that you are working towards.\n\nAsk your manager for candid feedback with regards to their assessment of your readiness to become a manager, and any gaps that they think you need to address.\nIn followup career development check-ins, ask your manager to provide feedback for you against the leveling guide.\n\n\nManagerial Hats\nPeople manager **-** Learn what makes your direct reports (aka people that you manage) tick, identify their career aspirations, and point out opportunities for progress.\n\nResource manager - Determine what resources are needed and acquire them. Mostly this means recruiting, hiring, and onboarding, but it also means advocating for money for training and team activities.\n\nProject manager - Collect and triage projects and project requirements, set timetables and schedules, assigned tasks, and have the final say about when work was â€œdoneâ€.\n\nCommunications manager - Make sure the teamâ€™s work was being shared with the rest of the organization, and that everyone on the team knew what was going on outside.\n\nProcess manager - Help design the teamâ€™s processes to make sure we could identify, allot, do, and communicate work across the team.\n\nTechnical mentor and coach - A technical expert who reviews code, answers technical questions, and gives work feedback to my team.\nPreparation for a managerial role\n\nTake notes on the time needed to do difficult tasks, easy quick-wins, common roadblocks, and their solutions.\n\nThis will help estimate deadlines for new projects\n\nPractice verbal and written communication\nGather information on â€œbig pictureâ€ strategy of your company and that applies to data projects\nTake notes of every data team memberâ€™s strengths and weaknesses\nListen to your colleagues.\n\nComplaints on a day-to-day basis.\nPraise about the workplace in general.\nPay attention to pet projects of your teammates: these are the areas they actively pursue outside their usual work.\n\n\nMeetings\n\nMeetings are bad when they:\n\nResult in calendar fragmentation.\n\nTry to schedule sometime after a Focus Time\nLimit size and number of meetings\n\n1-1s (1 on 1), team-wide update, or decision-making meetings\nLarge (&gt; 4 ppl) brainstorming meetings donâ€™t work\n\nBetter to circulate a memo of come-up with options then debate those options during a meeting\n\n\n\nFeel useless to attendees\n\nKeep focus on the meetings agenda\n\nGroup meetings (managerâ€™s agenda)\n1-1s (1 on 1) (reportâ€™s agenda)\n\n\n\n\nTools for Servant Leadership\n\nTeaching - As a leader you often have more context and more experience than your team members.\n\nTeach the team which situations different models work in, how those models are perceived in your organization and the red-flags to watch out for during development.\n\nReflecting - Make time to think back to events within your team.\n\nWhat caused success? What led to failure? Are we setting expectations appropriately for our models and analyses?\n\nDebate - Encourage debate.\n\nThe team is trying to use data to understand the world, and as in any form of science, there will be competing hypotheses\nTake advantage of the diversity (all forms) within our teams to minimize the impact of those personal biases\n\nProcess - Leaders will have to deal with ambiguity, but for the wider team we need to ensure there are steps to follow that support consistency across the team and alignment on the teamâ€™s over-arching goals.\nFeedback - To maintain team membersâ€™ morale, the balance between negative and positive feedback has to tilt heavily towards the positive\n\nIf you canâ€™t find that balance, then you need to consider whether the team member should continue on your team. If you want them to remain, then you must figure out how to articulate their positives back to them, otherwise you can expect them to leave.\n\n\nNonviolent Communication (NVC)\n\nMisc\n\nNotes from How to deliver constructive feedback in difficult situations\nOther methods\n\nSBI (Situation-Behavior-Impact) Useful for giving better feedback by removing emotions from it and making it clear and concise\n  Steps\n      Situation - Start the feedback with a specific situation that occurred which serves as a common reference point and is specific.\n\n      Behavior - Refer to a specific behavior that you observed and want to talk about. Make sure to not give any judgments and leave the interpretation out of it.\n\n      Impact - Talk about the impact that behavior had and what you think and feel about it. Feel free to address what other people think and how it impacted things.\n\n      Intent - Ask about the personâ€™s intention and try to uncover if the person is aware of what he did and why he did it. Then, work together with the person to see how things can be made better and how to overcome issues.\n\n\nAt the core of NVC is a straightforward communication pattern:\n\nâ€œWhen ____[observation], I feel ____[emotion] because Iâ€™m needing some ____[universal needs]. Would you be able to ____[request]?â€\nExamples\n\nTo a co-founder: â€œWhen you said, â€˜Iâ€™m not happy with your work,â€™ to me in front of the team, I felt embarrassed because it didnâ€™t meet my need for trust and recognition. Please, could we set up a weekly one-on-one session to share feedback in private?â€\nTo an investor: â€œI havenâ€™t received any responses from the last three monthly updates. Iâ€™m feeling concerned because I need input. Please, would you mind getting back to me with responses to my questions in the last update?â€\nTo a teammate: â€œYou arrived 10 minutes late to the last three team meetings. I am frustrated because, as a team, we have a need for efficiency. Please, could you help me understand whatâ€™s happening?â€\n\n\nObservations (vs evaluations)\n\nExamples\n\nEvaluation: â€œYou are lazyâ€ (which is a character attack). Observation: â€œYou said that youâ€™d send the document last week, and I havenâ€™t received it.â€\nEvaluation: â€œYour work is sloppyâ€ (which is a criticism). Observation: â€œThree of the numbers in the report were inaccurate.â€\nEvaluation: â€œYouâ€™re always late,â€ (which is a generalization). Observation: â€œYou arrived 10 minutes late to the meeting this morning.â€\nEvaluation: â€œYou ignored me.â€ (which implies intent). Observation: â€œI sent you two emails, and I havenâ€™t received a response.â€\n\nCheck\n\nask yourself, â€œWhat did I actually see or hear?â€\n\n\nEmotions (vs thoughts, vs evaluations)\n\nUsing an evaluation or thought instead of an emotion, can result in a defensive reply\nExamples\n\nEmotion: â€œI feel frustrated.â€ Thought: â€œI feel that you arenâ€™t taking this seriously.â€\nEvaluation: â€œI feel judged.â€ Impact: â€œI feel resentful.â€\n\ndefensive reply: â€œI didnâ€™t judge you.â€\n\nEvaluation: â€œI feel misunderstood.â€ Impact: â€œI feel frustrated.â€\nEvaluation: â€œI feel rejected.â€ Impact: â€œI feel hurt.â€\n\nCheck\n\nFor thoughs, if you can substitute â€œI feelâ€ with â€œI thinkâ€ and the phrase still worksâ€Šâ€”â€Šbecause itâ€™s a thought, not an emotion.\n\n\nUniversal Need (vs strategy for obtaining a need)\n\nExamples\n\nStrategy: â€œI need you to copy me into every email.â€ Universal Need: â€œI need some transparency.â€\nUniversal: â€œâ€œI need support.â€ NOT Universal: â€œI need support from you.â€\n\nNOT Universal is more easily interpreted as a veiled accusation and implication that â€œYou arenâ€™t supporting me.â€\n\n\n\nRequests (vs demands)\n\nrequests are invitations for another person to meet our needsâ€Šâ€”â€Šbut only if it doesnâ€™t conflict with one of their needs.\nCharacteristics of a good request\n\nMake them specific\n\nâ€œI request that you arrive to meetings on time.â€ instead of â€œI request that you be more respectful of everyoneâ€™s time.â€\n\nSay what you want, not what you donâ€™t want\n\nDonâ€™t want: â€œI request that you donâ€™t dismiss other peopleâ€™s ideas straightawayâ€\nWant: â€œI request that when a team member shares an idea, you ask two or three probing questions before sharing your conclusion.â€\n\nStay curious\n\nBe optimistic that everyoneâ€™s needs can be met.\nTreat â€œnoâ€ to a request or a defensive reply as an invitation to explore the needs stopping someone from saying â€œyes.â€\nThink about how the other person is feeling and consider what unmet needs may be stopping them from saying â€œyes.â€\n\nAre you feeling hurt because you need some understanding?\nAre you feeling angry because you need your hard work to be recognized?\nIs there more youâ€™d like to say?\n\nSimilarly, if youâ€™re on the receiving end of a request and have to say â€œno,â€ state the underlying need that stops you from saying â€œyes.â€\n\n\n\nDiplomatically confirm communication if needed\n\nâ€œJust so we know weâ€™re on the same page, could you play back what Iâ€™m asking of you?â€\n\n40-word rule\n\nDuring difficult conversations, itâ€™s important to be extremely concise. Aim to describe your observations, feelings, needs, and requests in fewer than 40 words. Using more words suggests youâ€™re justifying your needs, and that decreases their power.\n\nFace-to-Face is better\n\nNVC loses some of its power when itâ€™s in an email.\n\nConsequences should be protective, not punitive\n\nAs a manager, you are responsible for the effectiveness of your teamâ€Šâ€”â€Šand every team needs effectiveness. If deadlines continue to be missed (the boundary), you might have to switch their responsibilities or move them on (the consequence). Itâ€™s not personal, itâ€™s just what youâ€™ll do to protect your need for effectiveness.\n\n\nWays to promote projects completed by your team\n\nAnnouncement emails\n\nUnlike â€œsharingâ€ emails (brief description, link), announcement emails have more pomp associated with them\nCharacteristics\n\nUse catchy subject lines â€” e.g.Â â€˜Retention Dashboard is here!â€™ or â€˜Introducing Retention Dashboardâ€™\nIn addition to stating what the dashboard contains, tie it to key insights, recommendations and next steps\nUse icons & visuals â€” Adding relevant icons and visuals makes the email easier to consume and provides a nice break from all the heavy text. Caution: Do not overuse!\n\nExample\n\nReadouts\n\nAn analysis that is packaged in a way that is easy to read through\nTypically a one-time analysis\n\n(deep dive) e.g.Â what drives customer retention\n(root cause analysis) e.g.Â why did top of funnel conversion decline or analyzing an experiment / launch / campaign performance\n\nAlso reoccurring\n\nCould be weekly or monthy, depending on topics important to your stakeholders\nActively sharing summarized findings from dashboards to the stakeholders can change perception that these dashboards are just another source of data\n\nMonthly or Quarterly Business Reviews\n\nPresentations where you review health of business based on trends in key metrics (month over month, quarter over quarter)\n\nAutomate frequent requests from Marketing managers, Product managers, and Operations managers\n\nProduce a readout that covers insights from multiple dashboards that managers are frequently asking about.\n\n\nNewletter\n\nHighlight goals for ongoing work-streams and outcomes for those completed, always connecting to business outcomes or stakeholder needs\nSample Layout\n\nSummary â€” Key Wins & Whatâ€™s Coming\nDetailed updates by themes\nNewsletter FAQâ€™s â€”\n\nGoals: What is the goal of this newsletter. e.g.Â Providing visibility and aligning on prioritization\nCadence: Weekly / Bi-weekly / Monthly\nAudience: Sr.Â Leadership of company\nTeam members\nPOC: Who should they reach out to if they have questions\n\n\nExample"
  },
  {
    "objectID": "qmd/job-on-the-job.html#sec-job-otj-misc",
    "href": "qmd/job-on-the-job.html#sec-job-otj-misc",
    "title": "47Â  On the job",
    "section": "47.1 Misc",
    "text": "47.1 Misc\n\nResource\n\nThe ICâ€™s Guide to Driving Career Conversations â€” 25 Tips for Purposeful Career Planning\n\nDevelop a business acumen and keep up to date on industry advances pertaining to your company by subscribing to newsletters, blogs, podcasts, and YouTube channels by industry leaders and enthusiasts.\nMaintain a â€œBrag Documentâ€ (article)\n\nLists accomplishments, projects, etc.\nYou or your manager isnâ€™t likely to remember what you did over the past year\nImportant from performance reviews\nUseful when your manager goes to bad for you (e.g.Â get a raise)\n\nPull Request Descriptions\n\nDocument your additions, removals, and changes to the code.\n\nGit does this, so Iâ€™m not sure exactly what this means\n\nSummarize what problem youâ€™re solving with this pull request, and how you solved it.\n\nData Team Structure\n\nIC (individual contributor)\n\nExample: Schedule of Senior Data Scientist\n\n8:30â€“9:00 â€” Starting My Day\n\nGo through and respond to emails\nGo through group chats you might have missed\nCheck if there were any errors in production runs that youâ€™re responsible for\n\n9:00â€“10:00 â€” Pair Programming\n\nWork on a task with a junior data scientist\nHelp each other with coding problems that are blocking each otherâ€™s individual tasks\n\n10:00â€“10:30 â€” Scrum\n\nSee Project, Development &gt;&gt; Development Frameworks &gt;&gt; Agile\n\n10:30â€“11:00 â€” Prep for a Presentation\n\neducational presentations - Executives hear buzzwords and want explanations\nresults of an initial model to business stakeholders\nRead more email if you donâ€™t have a presentation\n\n11:30â€“12:00 â€” 1â€“1 with Manager\n\nFeedback, discuss career goals\nGet help with problems outside the data team\n\n12:00â€“1:00 â€” Get Feedback from Lead Data Scientist\n\ne.g.Â for projects youâ€™re working on\n\n1:00â€“4:30 â€” Code!"
  },
  {
    "objectID": "qmd/job-on-the-job.html#sec-job-otj-fmon",
    "href": "qmd/job-on-the-job.html#sec-job-otj-fmon",
    "title": "47Â  On the job",
    "section": "47.2 First Month",
    "text": "47.2 First Month\n\nUnderstand the business\n\nGet a general understanding of the scope and context of the business that your company operates\n\nAnticipate the needs of key stakeholders\n\nHave meetings with departments and get an understanding of how they currently use data\nGet a sense of their internal workflows and how you might be able to improve them (e.g.Â automation, easing access, etc.)\nFigure out what the company needs and your bosses donâ€™t like doing. Then improve it or automate it.\n\nExplore ways that you can use data to help the companyâ€™s core mission\n\nFind out which KPIs are used\nFind out how the data is collected\n\nGet familiar with the data, the used tech stack, and current projects as soon as possible.\nManager 1-1s\n\nSet up frequent 1:1 sessions with your manager from once to twice per week.\n\nAlso see Performance Reviews/Self-Assessments &gt;&gt; Regularly check in with management\n\nFocus on the most important points in your 1:1s and how your boss can help\nDonâ€™t talk about problems only â€” also achievements\nClarify expectations early and often\nFind out how your performance score is calculated.\n\nThis score will likely be one of the few things that someone a few more rungs up the food chain will have to determine who gets laid off, so while this score shouldnâ€™t be your only focus, it should be front and center.\n\nNegotiate timelines for diagnosis and action planning\nDonâ€™t let yourself get caught up immediately in firefighting â€” get a clear picture first\n\nMeet with colleagues (teammates, IT)\n\nHelps to better understand the challenges they are facing, their expectations of your role, and what is important to them\nQuestions\n\nWhat is your background?\nWhat are the current projects you are working on?\nWhat are the biggest challenges and why?\nWhat are Data (Science)/tech topics you are interested in?\nWhat tools/software/frameworks are in use?\nHow is our relationship with other departments?\nIf you were me, what would you focus on?\nWhat are your expectations of my role?\nWhat is important to you in working together?\n\n\nMeet with non-technical stakeholders\n\nHelps to get a better picture of the companyâ€™s strategy and political landscape\nQuestions\n\nWhatâ€™s your background?\nWhat are the biggest challenges the organization is facing?\nWhat are your expectations of our team/unit?"
  },
  {
    "objectID": "qmd/job-on-the-job.html#sec-job-otj-meet",
    "href": "qmd/job-on-the-job.html#sec-job-otj-meet",
    "title": "47Â  On the job",
    "section": "47.3 Meetings",
    "text": "47.3 Meetings\n\nExample\n\nScenario from https://towardsdatascience.com/how-did-machine-learning-interpret-problems-and-save-cost-for-ecommerce-companies-6c92796e5ed8\n\nMeeting where the discussion was about the high amount of total failed delivery (FD) cases, its increment, the potential root cause of each case, the major reasons, its trending.\n\nFD is when delivery of product is refused at the last moment\n\n\nTwo groups of people in the room\n\nThe Questioning group: CEO, CFO, Marketing Team, they asked tons of questions. Most of the questions began with â€œWhatâ€ and â€œWhyâ€, some others were â€œHow toâ€ and â€œWhat ifâ€.\nThe Answering group: CDO, CCO, BI Team, they gave answers for roughly 30% of asked questions, most of them answered for the group of â€œWhatâ€ and â€œWhyâ€ questions, while the group of â€œHow toâ€ and â€œWhat ifâ€ went to the void.\n\nInefficient: Hypothesis-Test-Report loop (trial and error approach)\n\nthe Business Team or Financial Team kept asking about the reason why this problem happened. Then they also put you in some specific circumstances such as â€œwhat if we do this?â€ or â€I think this happened because of thatâ€.\nThe loop\n\nThey give you a potential cause\nYou look at your data, did something you called analysis\nReport findings\nIf the data doesnâ€™t support the hypothesis, they give another hypothesis, and the loop repeats\n\n\nRecommended: Run a preliminary analysis instead\n\nThe business-side wonâ€™t wait too long to get the decision from the Data Team, they just need something that sounds reasonable enough to make a decision. So, thereâ€™s a time constraint.\nRun a decision tree or other quick running algorithm on a sample of the data (if the data is â€œbigâ€)\nLook at the top 1 or 2 levels of decision rules (decision tree) or feature importance (rf) or characteristics of the clusters(k-means, etc.) for a latent variable\nInterpret and report results\nThe business-side can further develop a list of hypotheses (and test) from results\n\nSolutions:\n\n3 decision rules were found to be informative and actionabel\n\nCustomers that have had 12 successful deliveries tended to continue to have successful deliveries\nCustomers who paid with a credit card tended to have successful deliveries\nCustomers who paid with a credit card and ordered over a certain $ amount tended to have successful deliveries\n\nFor the â€œNumber of orders in the pastâ€ problem, we can collaborate with the Customer Service Team to give more support to new users, keep tracking their orders and have proper action like picking up the phone and confirming their orders when they placed any high value order. Or we can give them more instructions on how to place orders, how to contact us when they need help, and how to cancel any order when they lose their interest.\nFor the â€œPayment methodâ€ problem, we had a campaign in which we reserved VND30,000 (equivalent to USD1.2) and split it into 3 vouchers (VND10,000 for each). When they placed the order and chose Cost on Delivery method, we offered them a voucher (VND10,000) to reduce shipping fee if they agreed to change to another prepaid method. Of course, we offered to â€œremember my payment methodâ€ for them to use it later (actually thatâ€™s the way we drive them to use that method again in future), and keep doing it 3 times. Most of the cases, we succeeded in keeping them using the prepaid method after the 3rd order, and they keep using it forever after. After that, in the second campaign, we applied the first 3 free shipping fees to get more effectiveness.\nFor the â€œCost of bookingâ€ problem, we used the same method with the â€œNumber of orders in the pastâ€ problem, letting the Call Center keep interacting with Buyers who have high-value purchasing."
  },
  {
    "objectID": "qmd/job-on-the-job.html#sec-job-otj-perfrev",
    "href": "qmd/job-on-the-job.html#sec-job-otj-perfrev",
    "title": "47Â  On the job",
    "section": "47.4 Performance Reviews/Self-Assessments",
    "text": "47.4 Performance Reviews/Self-Assessments\n\nMisc\n\nTypically occur in February\n\nTrack achievements throughout the year\n\nIf you arenâ€™t disciplined about keeping track of your achievements, youâ€™ll be forced to comb through multiple records, including Jira tickets, pull requests, emails, and design documents, to compile your self-assessment at the last minute.\nSee Get your work recognized: write a brag document for a template\nKeep the stories short and sweet when turning your work log into a self-appraisal.\nIf you are reviewing your self-evaluation and feel the need for a â€œTL;DRâ€ summary at the top, it indicates that it is too lengthy â€” you may need to consider using bullet points instead of writing an essay.\nIdentify which accomplishments truly stand out â€” these could be instances where you went above and beyond in your day-to-day job (e.g.Â worked at the weekend to get a feature over the line; evaluated multiple options before implementation, etc.).\n\nBackup contributions with measurable metrics\n\nFocus on specific accomplishments X, measurable metrics Y, and the methods used to achieve results Z (Google X-Y-Z formula for resume writing)\n\nDevelopers can highlight their achievements, quantify their impact, and support their contributions with concrete data.\n\nMetrics like lines of code, bug fixes, or SLAs are typical to demonstrate the effectiveness of their work.\n\n**See Metrics section for more examples\n\nExample: a full stack developer\n\nReduced the overall page load time by 80% (X), as proven by benchmark testing (Y), by implementing a caching pattern and simplifying the HTML DOM (Z).\n\nTo make your accomplishments stand out, try to use descriptive adjectives and relevant keywords that emphasise their importance\n\nExample: instead of saying: â€œdelivered the ABC featureâ€, you can add: â€œdespite the increased scope on the ABC feature, the deployment went out on the committed sprint goals and timelinesâ€\n\n\nRegularly check in with management\n\nScheduling meetings specifically to discuss personal performance-related topics since most meetings will be dominated by the current pressing matter.\nSeek feedback on progress towards objectives, and identify areas for improvement, as this will avoid any surprises during the year-end reviews.\nQuestions\n\nHow am I doing in relation to my goals and expectations so far?\nIs there any additional support or resources that I can access to help me be more effective in my role?\nAre there any opportunities for me to take on new challenges or responsibilities within the team or organisation?\n\nConnect your actions with what your manager needs to be successful.\n\nDo they write progress reports to senior leadership? â€” provide them with metrics and extra info.\nDo they negotiate timelines/budgets in case of scope creep? â€” give them enough notice so they can keep the project on track.\n\n\nConsider the non-technical competencies\n\nDemonstrating a well-rounded set of competencies, can position yourself for a leadership role\nExamples: communication, teamwork, critical thinking, proactiveness, adaptability, or leadership.\nBe sure to link the deployment of those attributes directly to the results you have achieved.\n\nInclude feedback you have received from people beyond your direct manager.\n\ne.g.Â extracts from emails you have received from internal/external customers or kudos from your peers.\n\n\n\nAcknowledge the shortcomings\n\nManagement wants to see is how they were overcome, or how one pivoted in response to them, or otherwise what the lessons learnt going forward were.\nExample: â€œI only achieved part of this goal because of {very clear reason}, and I am now working on {this new method} to overcome thisâ€.\nDo not be afraid to seek advice or ask for additional training when needed.\n\nMetrics\n\nCoding\n\nNumber of PRs\nNumber of code reviews\nNumber of technical documents\nNumber of tech debt tickets addressed\nImpact of refactoring on performance/code quality etc\nTest coverage increase by X%\nNumber of reusable frameworks/libraries created\nNumber of successful releases (i.e.Â lack of emergency releases after a release)\n\nReliability\n\nSLAs increased by X%\nTTD/TTM/TTR average times\n\nProductivity\n\nTools created to automate a repetitive task to reduce time by X%\nNumber of tickets completed ahead of time\nNumber of overtime hours worked\n\nCost saving\n\nSaved $$ by using open source instead of paid library\nSaved $$ by using cloud services\nSaved $$ by implementing DevOps pipelines\n\nPeople\n\nNumber of people onboarded\nNumber of people interviewed\nNumber of people mentored\nImpact on the above in the team setting (e.g.Â faster delivery capability)\n\nLearning\n\nNumber of completed courses\nNumber of new tech stacks introduced as a result of keeping up with tech\n\nOther glue work\n\nNumber of KT sessions organised\nNumber of stakeholder meetings organised\nNumber of process improvements implemented\nActing as a liaison between cross-functional teams to facilitate communication and understanding\nImpact of proactivity to solving problems\nDesign discussions that you have been part of"
  },
  {
    "objectID": "qmd/job-on-the-job.html#sec-job-otj-ksitl",
    "href": "qmd/job-on-the-job.html#sec-job-otj-ksitl",
    "title": "47Â  On the job",
    "section": "47.5 Keeping Stakeholders in the Loop",
    "text": "47.5 Keeping Stakeholders in the Loop\n\nRemind them where you left off\n\nAssume theyâ€™re reading with partial attention. Remind them where you left off so they can task switch faster\nExample\n\nBad: â€œHereâ€™s the updated link.â€\nGood: â€œHereâ€™s the updated pitch. I incorporated your feedback & included a change summary below. Let me know if you have any questions. Iâ€™ll plan on shipping tomorrow morning.â€\n\n\nBe specific & explicit about what you need.\n\nDonâ€™t assume your boss knows what you need from them.\nBe specific about what you need and what the next step is.\nExample\n\nBad: â€œThe new ad is updated in the Google Doc. Itâ€™s published in FB, but not running.â€\nGood: â€œPlease approve the new ad copy (screenshot below). Once you approve, Iâ€™ll publish and go live on FB.â€\n\n\nMention if itâ€™s an FYI.\n\nDonâ€™t make people guess if they need to take action.\nExample:\n\nBad: [no context]\nGood: â€œFYI. Sharing because you mentioned wanting to see examples of investor updates.â€\n\n\nAdopt an action-oriented posture\n\nIf youâ€™re stuck, donâ€™t just stay stuck. Speak up to get what you need to do your job.\nExample\n\nBad: â€œI didnâ€™t do it because I donâ€™t have the right permissions in Google Analytics.â€\nGood: â€œCan you add to me to Google Analytics with x permissions? I need it in order to do y.â€\n\n\nMention your criteria and assumptions.\n\nMost bosses want to know that you did your due diligence and are thinking ahead\nExample\n\nBad: â€œI recommend this platform.â€\nGood: â€œI recommend this platform because of x, y, z criteria. My assumptions were a and b. The potential trade-off is x, but seems manageable because y. I vetted options, including [insert options] but [my recommendation] better fits our needs.â€\n\n\nPut the recommendation at the top, then context below.\n\nLeading with a recommendation allows your recipient to read as much or as little of the context as they need.\nExample:\n\nBad: [Actions, backstory, and context all jumbled together]\nGood: \n\n\nYou can share LESS context whenâ€¦\n\nYouâ€™ve made this type of decision many times & you have task relevant maturity\nDecision is reversible and inexpensive\nThis is top of mind for your boss (not one of 25 projects theyâ€™re managing)\n\nAim for MORE context when the decision isâ€¦\n\nIrreversible and expensive\nCustomer-facing\nYouâ€™re making this type of decision for the first time"
  },
  {
    "objectID": "qmd/job-on-the-job.html#sec-job-otj-reqs",
    "href": "qmd/job-on-the-job.html#sec-job-otj-reqs",
    "title": "47Â  On the job",
    "section": "47.6 Requests",
    "text": "47.6 Requests\n\nMisc\n\nAlso see Job, Organizational and Team Development &gt;&gt; Starting a Data Science Department &gt;&gt; Create an environment &gt;&gt; Limit ad-hoc requests\nAd-Hoc Requests Queue\n\nLimited to something an individual data practitioner can handle in no more than 1â€“2 weeks.\nSometimes appropriate to develop a prototype as part of an ad-hoc request, and if the stakeholder is happy with the results it de-risks a larger effort to put something into production\n\nProject-Level Work\n\nNeeds cross-functional support from multiple teams (say, putting some model into production) and requires more formal planning.\n\nTriage process\n\nSince the queue is for unplanned work and balanced against all the other incoming requests (plus ongoing, committed project priorities), avoid using it for extremely urgent requests\nA small core team reviews new requests and pending status about once weekly, and a typical request can take anywhere from 1â€“2 weeks to complete once itâ€™s been reviewed and resourced.\n\nAnything more urgent is left to teams with dedicated on-call staff or more appropriate processes.\n\n\nMake results discoverable for other stakeholders and not just internally for the data team\n\nWhen possible, migrate results to a user-friendly, searchable platform to increase discoverability and provide a better-defined surface for those one-off work products â€œin production.â€\n\nBatch and Time-box\n\nBatch: group some similar tasks (or in this case, ad-hoc requests) to be completed at once, reducing the time and effort on context-switching (i.e analyzing different tables) or moving between tools.\nTime-Box: preplan your schedule, allocate a fixed period to do projects, and only do ad-hoc requests if itâ€™s an emergency (p0)\n\n\nTypes\n\nThe GPS: â€œI need this info, but Iâ€™m asking you because I do not know where to start looking.â€\n\nUsually are relatively simple requests that require just applying a filter on the right place or transforming a table into a chart\nSolution (if itâ€™s a Iikely recurrent request)\n\nSet up a time to show them how to do it themselves\nShow them step-by-step where the data is, how to filter it, and convert into a format theyâ€™re comfortable using (e.g.Â Excel pivot table)\nEmail them the steps\nWhen they come back with the same request, open the email, and follow the steps to show with them that following the email solves their need\nWhen they come back yet again with the same request, forward them the email or refer to the email\n\n\nThe Personal Assistant: â€œI need this info and I could get it myself, but I cannot / donâ€™t want to use my time.â€\n\nIf a peer asking\n\nHelp them as long as it doesnâ€™t mean that by doing so you are not postponing tasks and requests that are within your job description and that may affect the way your work is perceived.\n\nIf it is a senior stakeholder in a meeting wanting a quick answer\n\nIf itâ€™s something thatâ€™s not readily available (e.g.Â screenshot off a dashboard), give an estimation and emphasize that its an estimate\nMake sure to specify the source of the data and filters or conditions applied. This is important in case someone challenges or wants to validate it and will also help us in case it comes up in a conversation after the meeting.\nIf this happens often, ask the stakeholder to inform you in advance of when this type of meetings are going to happen, so you can be prepared.\n\n\nThe Gatherer: â€œThis information is scattered across multiple sources or not properly structured for the need. I need you to do it.â€\n\nCommon request when the reporting tools are not properly fitted to the userâ€™s needs\nIf the tool is from outside the company:\n\nAsk reason for the request and what are they going to do with the answer. That will define the scale of what you need to build.\nIf the same request happens frequently, then you should contact the relevant team to make sure that the business has all the tools they need for their day to day operations.\n\nIf the tool is from inside the company:\n\nAsk if it is a one-of request or if it is a signal of a business need not covered by the reporting tool. And if so, add an update of it to your to-do list.\n\nExample: email format of a response to such a request\n\nAn excel with the requested data / report\nA copy-paste of the requested table / chart so they can see it on the email\nA couple of call outs from the data just to highlight what Iâ€™ve identified (no idea what a call-out is)\nA â€œProcessâ€ section at the bottom of the email explaining the sources, filters, queriesâ€¦\n\nHelpful in case you need to reproduce the report in the future\n\n\n\nThe HAL: â€œI need you to run a diagnostic analysis to check why this is happening / I need to know what we need to do to fix this / â€¦â€\n\nType of question you want. Request refers to a business need.\nSteps\n\nIdentify the â€œwhyâ€ of the request. Sometimes the question is crystal clear. Others it can be veiled or masked.\nResponse should answer 3 questions: â€œWhat, So What, Now Whatâ€\n\nWhat: make sure the type of viz or table is clear in communicating the answer\nSo What: annotate the chart and include a description of all key insights\nNow What: State your recommendations\n\nAlways deliver by email and include a tweet-like analysis summary. Even if you present it in a meeting or by sending a link (or a file) via chat, always send an email with the analysis conclusions and a brief headline of the outcome.\\\nInclude a methodology slide or part in your answer. As with previous requests, on many occasions your answer will be challenged (someone will want to understand how you got to this conclusion), or you (or someone else) will have to repeat the exercise in the future. Including a methodology (even if it is very high level) will help in repeating it. And since it is included on the email where it is sent, it will be referred to, together with the results.\n\n\nThe Workaround: â€œI can request it to someone else, but it will take longer and/or generate a less comprehensive / insightful output than yours. So Iâ€™m asking you.â€\n\nOn many occasions, it will involve more office politics than actual analyst work\nIt can be requested to you either because the stakeholder does not know that this is not your main job or because the stakeholder trusts you more than the (tool, task, project) owner.\n\n** Always make sure that the stakeholder is aware that it should be done by X (team or person). **\nWhen it is by lack of knowledge of the task owner, try to mediate between them and the requester.\nWhen it is because of trust, then I just state the point (when delivering the answer) to make clear that this is someone elseâ€™s job.\n\nIf these requests become frequent\n\nContact the tool owner and explain that there is a mismatch between the business and the tools offered, as with the â€œGathererâ€ requests.\nThen present what you have been delivering as a proof-of-concept to evaluate and implement in the solution.\n\n\n\nHandling Ad-Hoc Requests\n\nMisc\n\nMe: this list of inquiries is great, but it will take time for the requester to think through and give answers for this stuff. Some people just want an answer to their question and not to have to go through this type of interrogation. So maybe there can be a request form that covers this stuff. That way they know whatâ€™s expected upon making a request. Then, any further clarification can be handled through email or a quick phone call when the requester has the time.\n\nDonâ€™t ask for metrics\n\nSince most business requestors donâ€™t have an intimate understanding of all the assets in our analytics platform, the answers were often confusing (what logs are they referring to?) or impossible (we just donâ€™t capture that type of data).\n\nDonâ€™t let the stakeholder to just request a solution\n\nBy making sure that you understand the problem, you confirm that there is indeed a problem and you guarantee that your stakeholder has defined the business case.\nMake sure that you fully understand the request and that the requester fully understands what they really want.\n\nAsk them how they will use the output, or what decision will be made with the answer\n\nHelps to further contextualize the requests, and get at the heart of â€œwhyâ€ someone is asking for help and the root of a stakeholderâ€™s need\nProvides pivotal information for prioritization. Can infer the urgency and relative priority of a request from how the information is going to be used\n\nDonâ€™t ask for a deadline\n\nDeadlines can result in a lot of things due yesterday and mismanaged expectations\nStakeholders tend to pick what they think is â€œreasonable,â€ but without any of the context to make that call (like other priorities in the queue, ongoing projects, complexities of the data, etc.) the timeline is arbitrary.\nFollow-up with the requester with your proposed delivery date after triage of new requests is completed. This allows your teams to manage expectations without wrecking sprints with constantly shifting priorities\n\nAsk them what happens if they donâ€™t get the answer (on time)\n\nMe: Iâ€™m not sure about this one. Maybe require some tact. Perhaps word it differently. The requester may think theyâ€™ll never get an answer if they say anything but a catastrophe will happen if they donâ€™t get their solution on time.\nSimilar to asking what is going to be done with the output, this helps to contextualize the need and drive prioritization. This is an even stronger tool for triaging and prioritizing requests. If someone canâ€™t be bothered to justify what will happen without the analysis, they probably donâ€™t need it right now.\n\nInvestigate and determine if any existing solutions may help to answer the questions that are being raised\n\nYouâ€™ll likely have to maintain a solution indefinitely, so make sure a solution doesnâ€™t already exist before building something new.\n\nWhen appropriate, go the extra mile to provide more than the initial ask and with minimalism/interpretability in mind\nCatalog all analysis results internally\n\nSo you can re-share, recycle, or refresh results as needed.\nIf the stakeholder has follow-up questions at a later date, whoever is picking up the request has an existing body of work to carry forward or a point-of-contact with domain expertise who worked with similar data assets.\n\nCollect feedback for each individual request\n\nGives stakeholders a chance to comment on the work products and create more opportunities for process improvement\nHelps you understand the real impact of work products that make it into â€œproductionâ€\n\n\nProject-Level\n\nDepending on the complexity, these requests may or may not be appropriate for the ad-hoc requests queue (gray area type projects)\nMost of the Ad-Hoc stuff above + this stuff\nMake sure that you know the proposed solution well enough to be able to explain the requirements thoroughly before reaching out to the data engineers.\nGive a sketch of the solution to the stakeholder for them to sign off on.\n\nIt should explain what the end product will look like and how it will be used\nIf this â€œsign-offâ€ involves multiple people who are perhaps difficult to reach or this process of confirmation is just dragging along, strongly consider starting work on the engagement while waiting for this feedback.\nExamples\n\nDashboard: Make a quick sketch of the various stats, KPIs, filters, graphs, etc. with a general layout.\nAnalysis: Make a rough outline of the required data, the possible user inputs, as well as the various outputs (eg. charts, tables) and their formats (eg. PNG, HTML, CSV).\n\n\nDocument progress on the project\n\nEnsures that timelines are clearly communicated, negotiated, and agreed upon by the various groups involved in the projectâ€™s delivery.\nHelps to hold all teams accountable for timelines as well as allot resources to ensure that your organization is distributing resources efficiently.\n\nEngage stackeholders\n\nInform them of the engagementâ€™s progress, and ideally show them a draft of the solution as a work in progress.\nAllows for additional feedback and potential pivots that are better surfaced partway through the project than upon its delivery.\n\nCreate documentation for the final deliverable\n\nFeatures\n\nHelps users navigate your solution\nInclude details on the data (eg. where it comes from, how itâ€™s transformed, etc.)\n\n\nWork with the users of your solution\n\nHelp them understand how it works, inform them of any nuances they need to be aware of, and invite any questions or concerns they may have about it\n\nFollow up at a later date\n\nSee how their use of the tools are going, what pain points theyâ€™re experiencing, potential areas for improvement, etc. and then work to fix any issues\nIterate upon deliverables to achieve continuous improvement and growth of insights"
  },
  {
    "objectID": "qmd/job-on-the-job.html#sec-job-otj-smallcomp",
    "href": "qmd/job-on-the-job.html#sec-job-otj-smallcomp",
    "title": "47Â  On the job",
    "section": "47.7 Small Company",
    "text": "47.7 Small Company\n\nProduce something quick initially (establishes respect for your expertise)\n\nGet a list of the most pressing problems that the executives wanted me to solve.\nFind a relatively small but high-impact problem.\nNail it and make sure that it is recognized as solving a high-priority problem.\n\nSmall companies have limited budgets, so demonstrating frugality and resourcefulness in your early projects may assist you in obtaining more resources in the future\nDevelop a strong relationship with those who are interested in analytics first. They will be invaluable as your advocates to excutive stakeholders and other more skeptical colleagues.\nDevelop a strong relationship with IT so you get some of the technical obstacles removed quickly.\n\nOpening ports, getting permissions\nRemote access to one of their servers so you can run resource-intensive analytics processes"
  },
  {
    "objectID": "qmd/job-organizational-and-team-development.html#sec-job-orgdev-misc",
    "href": "qmd/job-organizational-and-team-development.html#sec-job-orgdev-misc",
    "title": "Organizational and Team Development",
    "section": "Misc",
    "text": "Misc\n\nConsiderations for transitioning to cloud infrastructure\n\nMaximize your existing infrastructure\n\nKeep training on-premise and inference in the Cloud. If you have GPUs on-site, then get every ounce of training out of them. Theyâ€™re a sunk cost, yes, but already installed and operational. No need to move the most computationally expensive step to the Cloud just yet.\n\nDeploy automation activities by modules and stages, not by projects\n\nThe more you can reuse code across steps, the more youâ€™ll be able to scale on future projects.\n\nBuild your provisioning automation scripts as early as possible\n\nAlthough it seems like it should happen later, this gives your team the confidence to de-provision training and inference instances as soon as possible with no productivity loss.\n\n\nBuying data\n\nA common failure mode is to build a business on top of somebody elseâ€™s data. If you depend on a single upstream source for your data inputs, they can simply raise prices until they capture all of the economics of your product. Thatâ€™s a losing proposition.\n\nSo you should try to build your own primary data asset, or work with multiple upstream providers such that youâ€™re not at the mercy of any single one.\n\nAdd proprietary value of your own\n\nA sufficiently large transformation of your source data is tantamount to creating a new data product of your own\n\nEven merging multiple datasets can add substantial value\nOther ways: quality control, labelling and mapping, deduping, provenancing, and imposing data hygiene"
  },
  {
    "objectID": "qmd/job-organizational-and-team-development.html#sec-job-orgdev-startdsd",
    "href": "qmd/job-organizational-and-team-development.html#sec-job-orgdev-startdsd",
    "title": "Organizational and Team Development",
    "section": "Starting a Data Science Department",
    "text": "Starting a Data Science Department\n\nMisc\n\nNotes from\n\nBuilding a data team at a mid-stage startup: a short story\n\n\n\n\nConsiderations\n\nData\n\nwhere is it stored; how is it stored; is it ready to use?\nIs there a more efficient storage and processing pipeline?\n\nTransfer data to centralized relational db or warehouse (See Databases, Relational &gt;&gt; Transitioning from Spreadsheet to DB)\nCreate wrapper functions to connect to separate dbs (See below, Develop internal packages)\n\n\nStack\n\nWhatâ€™s the current tech stack?\nAre there other tools available that achieve better efficiency, reproducibility, automation, and reporting?\nSee Developing a Data Platform below\n\nPersonnel\n\nDo people need to be hired in order to fill necessary roles?\n\nMinimal team: data scientist, data engineer, (maybe) IT\n\n\n\n\n\nHiring\n\nPredictors of future job performance (thread)\nStart small by favoring â€œFull Data Stackâ€ capabilities and keeping your data teamâ€™s objectives in mind; you can grow the team one member at a time as your necessities evolve.\n1st hire: Data Analyst or a Data Engineer with Analytics skills (Python, SQL, etc.) will be more valuable as a first hire\n\nThis person could work alongside Software Engineers on a first POC, which would help identify the first pipeline needs.\nData Analyst primary responsibilities:\n\nCreating dashboards to communicate data to business teams\nWriting reports to answer business questions\nAnalyzing results of A/B tests and presenting findings\nSupporting business teams in new product launches, marketing campaigns, and accounting\n\n\n2nd hire: Data Engineer or Analytics Engineer in order to proceed with building the platform and making appropriate infrastructure choices\n\nAnalytics engineers move data, similar to data engineers, but are concerned with the actual data itself. They get into the nitty-gritty details such as data types, column names, and the time that the data arrived in the warehouse.\nAnalytics Engineer primary responsibilities:\n\nOwning ingestion to orchestration\nEnsuring data is high quality and investigating when it isnâ€™t\nSafeguarding the data warehouse\nWriting reusable data models\nPushing data models to production\nMonitoring and troubleshooting the data pipeline\n\nThe Data Analyst or Data Scientist will be the Analytic Engineerâ€™s stakeholder. If the analytics engineer is doing their job, the analyst should have to be:\n\nwriting complex, long-running queries directly within a business intelligence platform\nreformatting the same data columns over and over again in different dashboards\nrepeating the same business logic in multiple reports or dashboards\njoining related tables that should already be joined together in one dataset\n\n\nLater hires: base these hires on current level of projects and objectives\n\nExploratory: The company has some data, but you donâ€™t know how to capitalize on it because you donâ€™t know where to find it or whether you should trust it.\nAnalytics: Your leadership is convinced that becoming data-driven is key to making better business decisions. There might already be some attempts to do analytics in Microsoft Excel or other tools, but you want to take data usage to the next level.\nInnovation: When you already have the necessary insights for making decisions, you think AI/ML will help you create your next differentiating edge; therefore, you want to start investing in that direction.\n\n\n\n\nData Team Placement\n\nPush for centralization in the reporting structure, but keeping the work management decentralized.\n\n\nYou want analysts or data scientists embedded in departments to report to you, but to have ad-hoc requests by departments to go through them or members of the main data team.\n\nAvoids bottlenecks where any important but simple analyses donâ€™t have to go through you.\nAlso, strong data people want to report into a manager who understands data, not into a business person\nExample: assignments\n\ndata infrastructure, onboarding product team, supply chain team, checkout team, marketing team, support for the CEO and helping with investor/board decks\n\n\nSend out an email to a large group of people outlining this change, and make it very clear who people should work with for their data needs.\nAs you hire people going forward, you are planning to assign them to different teams throughout the company.\n\nMostly product/engineering teams, but in some cases other teams.\n\n\nWithin Engineering: in some organizations like LinkedIn, the data team is part of engineering. Having seen a similar setup play out in the past, I think that Data and Engineering teams should work as partners and, therefore, with separate reporting lines. Creating a reporting dynamic between the two might jeopardize the efficiency of the collaboration and distance the Data team from the business.\nWithin product: this makes sense when the product is tightly related to data and when the organization relies on data primarily for feature testing and other product analytics use cases.\nWithin a business entity: finance or marketing, e.g., This is usually the case for small data teams where the scope and objectives only pertain to this particular team (not recommended for larger companies).\nAs an Independent Entity: reporting directly to the CEO or CFO. This makes sense for an organization that has: a. reached a good level of data maturity and company-wide data literacy, and b. a wide variety of well-defined use cases across different business functions, and is considering a â€œData as a Productâ€ type of approach catering to various business domains.\n\n\n\nDevelop Relationships\n\nDevelop a strong relationship with those who are interested in analytics first. They will be invaluable as your advocates to excutive stakeholders and other more skeptical colleagues.\nDevelop a strong relationship with IT so you get some of the technical obstacles removed quickly.\n\nOpening ports, getting permissions\nRemote access to one of their servers so you can run resource-intensive analytics processes\n\n\n\n\nCreate Developement, Staging, and Production Environments\n\nDevelopment\n\nReproducibility\n\nEnables you to tranfer lockfiles or something similar to colleagues for collaboration\nResources\n\nBuilding Reproducible Pipelines in R\n\nPackages, Libraries\n\n{renv} - See Renv\n{{poetry}},{{pip-compile-multi}}, etc. - Python, General &gt;&gt; Environments, Python, General &gt;&gt; Dependencies\nNix\n\n\nDevelop internal packages\n\nAlso see Package Development\nDeveloping packages for your companyâ€™s specific use cases increases efficiency\nSee Building a team of internal R packages | Emily Riederer and VIDEO How to make internal R packages part of your team - RStudio\nFunctions that get/process data commonly used in most projects\nWrapper functions for connecting to separate databases (See Databases, Relational &gt;&gt; Misc &gt;&gt; Wrapper for db connections)\ncon_depA &lt;- localDB::connect_databaseA(username = ..., password = ...)\ncon_depB &lt;- localDB::connect_databaseB(username = ..., password = ...)\ncon_Lex &lt;- localDB::connect_databaseLex(username = ..., password = ...)\n\n\nSecurity\n\nNeed a secure way to download packages (e.g.Â remember log4j vulnerability)\nPosit Package Manager - repository management server that allows you download packages while being disconnected from the internet\n\nPosit Public Package ManagerÂ - a free service that provides complete mirrors of CRAN, Bioconductor, and PyPI, including historic package snapshots. It is based onÂ Posit Package Manager, the professional product for teams who need greater control over how they expose R, Python, and Bioconductor packages within their organizations.\n\nAlso {RDepot} for management might be an option\n\nStaging\n\nStaging environment should be as close as possible the production environment\n\nDeployment\n\nAuthentification system - system where a user or user group with permissions is allowed to access your application using a username and password\nScheduling system - scripts running on a schedule\nMonitoring - alerts you to any errors in the pipeline\n\n\n\n\nEarly Projects\n\nSmall companies have limited budgets, so demonstrating frugality and resourcefulness in your early projects may assist you in obtaining more resources in the future\nSet up a meeting with the CEO later that week.\n\nYour goal is to figure out a few metrics she wants reported on weekly in an automatic email.\n\nSet up weekly 1:1s with a number of key people across the org that need data.\n\nYour goal is to find data gaps and opportunities and dispatch them to the data scientists.\n\nIf data is decentralized, create a data warehouse that houses all data\n\n\nFind analysts or other people in departments that have experience with SQL or are interested in learning it.\n\nAfter the data has been centralized, they will then be able to answer some of the ad-hoc data questions for the managers of those departments\nThese people may go too far at times but giving them access to the data will be a net positive.\n\n\nCreate presentations for departments that arenâ€™t data driven\n\nExample:\n\nSituation:\n\nData team projects for product arenâ€™t being put into production.\n\nâ€œProduct managers put it on the backlog, but they keep pushing it off because other things keep coming up.â€\n\nProduct puts features into production without A/B testing, because theyâ€™re being â€œboldâ€ and donâ€™t want to wait for experiments that last months\n\nNo idea if the results of their features are significant or not.\n\n\nProduct managers are not thinking about data as a tool for building better features\nThere is a lack of alignment between what product teams want to build versus what data teams have\n\nPresention: Showcase many examples of tests with unexpected outcomes from your previous experience, and you make parts of the presentation a bit interactive where the audience has to guess whether test or control won.\n\nFind analyses or queries that are computationally expensive\n\nBuild pipelines to produce â€œderivedâ€ datasets. Once these datasets are built, costs of performing the analyses will be much lower\n\nSounds like a job for dbt.\n\n\nWork with every department team and make sure they have their own dashboard with the top set of metrics they care about.\n\n\nâ€œwhite-glove analyticsâ€ means give special attention (prioritize) to analytics for excecs and CEO.\n\nProduce something quick initially (establishes respect for your expertise)\n\nGet a list of the most pressing problems that the executives wanted me to solve.\nFind a relatively small but high-impact problem.Â  (see Project, Planning)\nNail it and make sure that it is recognized as solving a high-priority problem.\n\n\n\n\nCreate a Culture\n\nLimit ad-hoc requests\n\ne.g.Â creating one-time reports and pulling data\nSee Job, On the Job &gt;&gt; Requests\nMake it clear that automating your pipeline, cleaning up your dashboards, and streamlining your data definitions will have a massive impact in the future\nCreate or purchase a ticketing system to manage all incoming requests from other departments in a company (article)\n\nThis will help in managing the workload, and prevent the pressure of stakeholders sending direct emails or ping-ing team members on the requests.\nIt needs to manage the lifecycle of every individual request, from submission to resolution\nPrioritizes the most important problems and creates a sense of urgency for the team to focus and solve.\nForm should include:\n\nRequestorâ€™s team\nDetails of the request: what are the data needed? In which format (i.e spreadsheet, dashboard, analytics documents, database table, etc)? What/how the data will be used for?\nPriority/time when the data is needed\n\n\nEducate Stakeholders if theyâ€™re eager to learn\n\nVideo on navigating the repository (?)\nSimple utilization of the dashboards\n\nutilizing the filters and interactive parameters to get the data their needs.\n\nMore advanced education can be a workshop on data sources and SQL/querying courses, but this is not mandatory.\n\n\nWrite clear documentation of all processes and standards\n\nWithout documentation of team practices, losing team members means losing domain knowledge\nWith documentation comes team reviews of this documentation. Through reviews, everyone learns something new. The rest of your team will be introduced to new concepts and ideas that will only make your team stronger\nData documentation\n\nKnowledge graphs are a paradigm often leveraged by SaaS data solutions that automatically represents data as nodes in a graph, drawing connections via logic and metadata. Knowledge graph-based tools, like lineage, are often automated to help data teams save time and resources when it comes to generating quick documentation about the relationships between and about data across the organization.\nFormalizing service-level agreements (SLAs) and service-level indicators (SLIs), and putting together implicit and explicit contracts between teams can help everyone stay aligned on priorities as you work to meet your goals.\n\n**See Developing a Data Platform &gt;&gt; Data Observability &gt;&gt; Data Reliability\n\n\n\nStart having code reviews\n\nMake code reviews a common practice by enforcing them through Github\nBest way everyone can optimize their code\nTidyteam code review principles\n\nKnowledge-sharing presentations\n\nâ€œlunch and learnâ€\nAll about building one another up and using each other as learning opportunities\nGet rid of the pressure of having to do everything in a certain domain\nIf one personâ€™s area becomes flooded with tasks, having others with those same skills came lessen the load on one person\nIf a person gets sick or leaves, another team member can pick up the slack"
  },
  {
    "objectID": "qmd/job-organizational-and-team-development.html#sec-job-orgdev-datlit",
    "href": "qmd/job-organizational-and-team-development.html#sec-job-orgdev-datlit",
    "title": "Organizational and Team Development",
    "section": "Data Literacy",
    "text": "Data Literacy\n\nMisc\n\nWhy is it necessary? A recent report from ThoughtSpot and the Harvard Business Review found that successful companies are enabling frontline workers, like customer service representatives and delivery and repair workers, with better data and insights to make good decisions. When workers have that information, the report found, companies have higher rates of customer and employee satisfaction and higher productivity and top-line growth.\nLong process but measureable results can be had within a year\nData Literacy is not the final goal. Others:\n\nData Maturity â€” easy access throughout the organization to good data\n\nAlso see Data Maturity\n\nData-Driven Leadership â€” Meaning that leaders demonstrate the skills they require of workers\n\nAlso see Developing a Data Driven Organization\n\nData-Driven Decision-Making\n\nAlso see Developing a Data Driven Organization\n\n\n\n\n\nLevels\n\nFrom Data Literacy Personas\nData Skeptics : They donâ€™t believe in the value and power of analytics. They see analytics and data as a â€˜burdenâ€™ to their work. They can derail any Data Literacy project unless carefully nurtured into becoming Data Enthusiasts. A good data awareness program is imperative in turning these skeptics into enthusiasts.\nData Enthusiasts : They believe in the power of data. They are eager to learn more about how to use data and interpret it in their work. A good Data Literacy program could usher them and the company to new heights toward being data-driven.\nData Literates : They understand the analytics landscape and can be an active participant in discussions involving data. They are willing to hone up their analytics skills. A good recipe-based analytics program with hands-on practice on the most employed analytics techniques can take them and the company a long way toward being data-driven.\nCitizen Analysts: They are data-driven employees who can solve 80% of their business problems using a structured approach to analytics and, in the process, align the stakeholders as well. They can get to an actionable solution and move the critical metrics for the company. Some Citizen Analysts can also be taught advanced analytics.\nData Scientists: They are well versed in advanced analytics methodologies. They can solve almost 100% of business problems using analytics. They are adept in using cutting-edge tools like R, Python, and SAS to manipulate data and build models. These Data Scientists are capable of aligning stakeholders toward an actionable solution and excelling at the data-driven decision-making process.\nData-Driven Executives: They understand the power of analytics, which is the discovery and interpretation of meaningful patterns in data and their application for effective decision making. Data plays an integral role in the decision-making process. They hold their team accountable for their work and can understand when analytics has been executed in the right manner or not.\n\n\n\nCreate a Data Literacy Plan\n\nNotes from How to Build Data Literacy at Your Company\nDefine data literacy goals, assessing employeesâ€™ current skill levels, and laying out appropriate learning paths\nSteps:\n\nDistinguish between data literacy and technical literacy\n\nBeing able to use a tool and understanding how derive insights from it are two different things\nLiteracy ( https://dam-prod.media.mit.edu/x/2016/10/20/Edu_Dâ€™Ignazio_52.pdf )\n\nRead with data, which means understanding what data is and the aspects of the world it represents.\nWork with data, including creating, acquiring, cleaning, and managing it.\nAnalyze data, which involves filtering, sorting, aggregating, comparing, and performing other analytic operations on it.\nArgue with data, which means using data to support a larger narrative that is intended to communicate some message or story to a particular audience.\n\n\nStart with a baseline of employee skills\n\nAfter knowing a baseline, you can develop a plan to upskill employees\nExample survey: https://aryng.com/data-literacy-test\n\nItâ€™s not that good, but it gives an idea of how it should be formulated\n{shinysurvey} could be used to develop something suitable for a companyâ€™s particular business model\n\nScore and categorize using something like the â€œlevelsâ€ (above)\n\nUse common language\n\nUsing jargon or imprecise terms can create confusion and complicate communication about data.\nLanguage = Culture\n\nBuild a culture of learning and reward curiosity\n\nLeaders should make sure to foster an environment that rewards curiosity instead of punishing lack of data literacy.\nIf there is a culture of fear rather than of continuous learning and improvement, then people would feel ashamed that theyâ€™re not data-literate.\nDonâ€™t punish people for negative data. Confront the brutal facts of the negative data and learn from it. If punishment is the first reaction, then people will try to hide the data or manipulate it â€“ vanity metrics.\n\nTake into account different learning styles\n\nNot everyone is suited to a three-hour training classes â€” some employees learn best with hands-on exercises, while others might like self-led courses\n\nTrack progress and develop metrics\n\nNo real examples of metrics.\n\nLeadership must be involved\n\nChief Data Officers are often the ones in charge of literacy initiatives, but all top executives needs to be on board and modeling the desired results.\nExecutives should be part of the program"
  },
  {
    "objectID": "qmd/job-organizational-and-team-development.html#sec-job-orgdev-daddo",
    "href": "qmd/job-organizational-and-team-development.html#sec-job-orgdev-daddo",
    "title": "Organizational and Team Development",
    "section": "Developing a Data Driven Organization",
    "text": "Developing a Data Driven Organization\n\n\n\nMethods for getting buy-in from stakeholders\n\nUse your tools and skills to confirm something â€œobviousâ€ (establish trust) in an accessible (probably visual) way, then use same to show something non-obvious and actionable (establish value).\n\nStakeholders need to know what levers to pull that will affect lead indicators before being given metrics that measure those indicators (i.e.Â the tool or report theyâ€™re asking for).\n\nLag indicators - The thing you care about but can only measure in hindsight. (e.g monthly sales). They measure success.\nLead indicators - Things that are predictive of the lag indicators (e.g.Â site traffic, unit sales, customer webpage behaviour, etc.). Granular aspects of the business.\n\nData teams focus on measuring these. Stakeholders take actions based on these indicators to affect the lag indicators.\nThese indicators need to continue to be refined.\n\nIf they know what decision they want to make (i.e.Â levers to pull) and that decision can reasonably generate business value, then tool you create to calculate the metric will be used and used correctly.\n\nThe stakeholder/business user needs to provide an action plan that answers 4 questions\n\nWhat result are they influencing with this dataset?\nWhat do they expect the dataset to look like when it arrives?\nHow they will extract answers from this dataset?\nWhat levers will they move to action on the results?\n\nExample: â€œWe want to determine the sweet spot for each ad channel spend to get the best ROI for each channel. Once we have the optimal spends for each channel, weâ€™ll adjust our spends and regularly rerun the model to readjust our targets.â€\n2 and 3 (also maybe 1) sound like questions for the data scientist and not necessarily the business user. The dude who wrote this article is a data engineer so I guess heâ€™s writing from that perspective. This list of questions is likely what should be answered by the data scientist and business user before building an expensive pipeline.\n\n\nCompanies launching data driven initiatives should focus on small areas of primary need first, and advance these areas to a high degree before spreading out.\n\nSmaller focused areas that produce high quality will increase confidence of stakeholders.\nStarting wide and producing low quality will litter the decision making landscape with false conclusions and conflicting truths. Therefore, decreasing stakeholder confidence"
  },
  {
    "objectID": "qmd/job-organizational-and-team-development.html#sec-job-orgdev-datmat",
    "href": "qmd/job-organizational-and-team-development.html#sec-job-orgdev-datmat",
    "title": "Organizational and Team Development",
    "section": "Data Maturity",
    "text": "Data Maturity\n\nMisc\n\nNotes from A Maturity Model for Data Modeling and Design\nA few design decisions may have to be revisited in the maturing phase if proven false with real-life data\nIf the end reporting takes a lot longer to create than expected, performance tuning will have to be carried out on an ongoing basis\nA database admin may have requirements to collect statistics on the data\nAn entire operating model would be created around this warehouse to ensure it stays fit for purpose.\n\nTeams using it would need training on how to use it,\nData governance principles would be applied\nData quality and the cleansing process would be formalized.\n\n\n\n\nComponents\n\nAlso see Databases, Warehouses &gt;&gt; Designing a Warehouse\nMetamodeling:\n\nDefines how the conceptual, logical, and physical models are consistently linked together.\nProvides a standardized way of defining and describing models and their components (i.e.Â grammar, vocabulary), which helps ensure consistency and clarity in the development and use of these models.\nData ownership should be assigned based on a mapping of data domains to the business architecture domains (i.e.Â market tables to the marketing department?)\n\nConceptual Modeling - Involves creating business-oriented views of data that capture the major entities, relationships, and attributes involved in particular domains such as Customers, Employees, and Products.\nLogical Modeling - Involves refining the conceptual model by adding more detail, such as specifying data types, keys, and relationships between entities, and by breaking conceptual domains out into logical attributes, such as Customer Name, Employee Name, and Product SKU.\nPhysical Data Modeling - Involves translating the logical data model into specific database schemas that can be implemented on a particular technology platform\n\n\n\nDimensions to Assess Data Maturity Within an Organization\n\nStrategy â€” The organizationâ€™s overall data modeling strategy, including the alignment of data modeling efforts with business goals and objectives.\n\nMetamodel - There should be 1 and only 1 meta model in place that is used consistently across the organization\nPhysical Models - Should have well-designed and efficient database schemas in place that meet applicable performance and scalability requirements\n\nPeople/Talent â€” The articulation of specific roles and their responsibilities, as well as required expertise, skills, and training.\n\nMetamodel - There should be a single person with ultimate authority over the metamodel. He or she can take in feedback and collect change requests to ensure it is and stays fit-for-purpose.\nConceptual & Logical Models - There is a skilled person with core data modeling expertise and the ability to project it onto a real-life business domain to describe it in logical attributes that make sense to the business and technology organization alike.\nPhysical Models- Should have people who can design and implement the schemas\n\nProcesses â€” The processes and workflows, including the documentation of data modeling methodologies, the development of data modeling templates and standards, and the establishment of quality control and review processes.\n\nMetamodel - There should be description of how the metamodel is to be used in modeling activities which makes work easier as data people have a clear basis to start from.\nConceptual & Logical Models - The process of creating these models should have an owner and there should be a structured process to create new logical attributes, and to then have them reviewed, approved, and published.\nPhysical Models - A data dictionary can also be used to define and standardize the technical details such as data types, constraints, and other database objects.\n\nTechnology â€” The tools required to support data modeling efforts such data modeling software and tools, and the integration of data modeling tools with other systems and applications.\n\nConceptual & Logical Models - There should tools that can provide a visual representation of the models and can support collaboration, version control, and integration with other systems such as a data catalogue or metadata management system. A business glossary can be used to define and standardize the business concepts and terms that are used in the models.\nPhysical Models - Should have appropriate technology tools to support schema design and implementation. Database design software can be used to create and maintain physical data models. These tools can generate database schemas from the logical data model and can support collaboration, version control, and integration with other systems such as a data catalogue or metadata management system.\n\nAdoption â€” The adoption and usage of data modeling practices within and across the organization. This may include socialization programs, the resolution of barriers to adoption, and the tracking metrics to measure the effectiveness and impact of data modeling efforts.\n\n\n\nTips and Best Practices\n\nGet the metamodel right first. The metamodel drives reusability and consistency across the entire enterprise. It makes sure that all subsequent modeling efforts incrementally build out the overall model. If you donâ€™t have one in place, youâ€™re up for a gargantuan task of aligning and bridging existing, incompatible models in the future.\nConsider prebaked industry or universal models. Depending on where you are in your journey, you can consider adopting a preexisting data model. This can drive alignment with international best practices and standards, save you time and effort to build a model entirely from scratch, and enable efficient and reliable data exchanges with external parties. For example, BIAN provides a standardized banking services reference model that defines a common language, taxonomy, and business process framework for the banking industry.\nIterate between conceptual, logical, and physical. Data modeling takes time â€” the job will never be done. It is recommended to prioritize domains â€” reference domains like Customers and Products are good candidatesâ€”and start with 1 or 2, where you first complete the logical model and then guidelines for the physical model, before you move on to the next domain.\nDonâ€™t overdo the physical. Data modeling can be complex, time-consuming, and therefore expensive. Completing a basic conceptual and logical model is almost always definitely worth the effort, but once venturing into the physical domain, you may not need to centrally direct and capture all of the physical models. You may want to prioritize here as well â€” for example, identify â€œmission criticalâ€ systems and document physical models for those, but for other ones, it may be sufficient to ensure that local application owners abide by specific modeling norms and standards.\nStrategically implement technology. They can be expensive, and you might not need them for the first domain, but eventually your data models will grow exponentially in terms of their size and complexity. Consider a data catalogue, business glossary, and data dictionary, or something that can serve as all of these. Without it, consumption (and hence value creation) will be poor."
  },
  {
    "objectID": "qmd/job-organizational-and-team-development.html#sec-job-orgdev-datstrat",
    "href": "qmd/job-organizational-and-team-development.html#sec-job-orgdev-datstrat",
    "title": "Organizational and Team Development",
    "section": "Developing a Data Strategy",
    "text": "Developing a Data Strategy\n\nMisc\n\nThere is no one path to developing a data strategy since every situation is unique\n\nRather than looking at what other companies have done, the key is examining your own needs and prioritizing the best data investments for your company\n\nPlanning\n\nPlanning forces a discussion about prioritization that can lead to key realizations about hiring and resourcing.\nSet three OKRs every quarter that will truly move the needle that align with your companyâ€™s bottom line\nKeep your primary goal at the forefront and stay flexible by revisiting your plan often\nWorkflow Example: B2C (business-to-consumer) model\n\n\n\n\n\nObjectives and Key Results (OKRs)\n\nAlso see\n\nThread\nProduct Development &gt;&gt; Metrics\nKPIs\n\nOKRs are a framework for turning strategic intent into measurable outcomes for an organization.\n\nObjectives are written as a set of goals that the company or department wants to achieve over a given time horizon, usually a year. Key Results are quantitative measures that might indicate youâ€™ve reached that Objective.\nExample: a CEO will set goals for acquiring customers and the Chief Marketing Officer in turn will develop the objectives of marketing campaign reach and customer acquisitions that are expressed as the Key Results (outcomes) that will show that these goals have been achieved\nProduct outcomes measure a change in customer behavior. Product outcomes help drive business outcomes. Business outcomes measure the health of the business.\n\nExample: Education Platform\n\nBusiness Outcomes:\n\nObjective: Help more teams adopt â€œcontinuous discoveryâ€ (her business) as a way of working.\nKey result: Reduce the # of unsold seats in our courses.\nKey result: Sell more books\nKey result: Grow membership by x%\n\nProduct Outcomes:\n\nObjective: Help members invest in their â€œdiscovery habits.â€ (what she teaches)\nKey result: Increase % of members who interview every week.\nKey result: Increase % of members who assumption test every week\nKey result: Increase % of members who define outcomes every quarte\n\n\n\n\nAs OKRs are transparent across the company they can help plug the strategic planning gap.\n\nExample: The Data division can see what Marketing are trying to achieve and what their intended outcomes will be. The Marketing Data team can plan and see what activities they can help out with. Importantly the Marketing Data team can add in improvement objectives that might make them a better team and provide a better service.\n\nTest and verify Metrics\n\nItâ€™s critical to test metrics before using them in OKR, otherwise, metrics may lead to biased investments\nDeveloping the metrics and means of collection that will be used to measure progress against each OKR.\nCollect data and verify hypotheses with visualization or statistical methods.\n\nWhen different stakeholders agree upon metrics, teams can now build a dashboard to monitor how business decisions affect metrics and profits\n\nSocialize and fine tune the OKRs with the data teams that will be responsible for delivery.\nThe data teams will then determine the tasks and activities needed to make the key results happen. How this happens depends on how your company builds products and services. For example in an agile shop this will then lead to story development and sprint planning.\n\nReview OKRs regularly\n\nWhen businesses evolve, so should metrics\n\n\n\n\nComponents\n\nAlignment with customer needs\n\nData team needs to provide support for the product, marketing, customer services, mobile, website engineering\nExample\n\nO: Acquire more mobile app customers\nKR: 75% uplift in mobile app downloads\nData Strategy: Align with functional teams to meet company objectives\nData OKR: all customer journeys that lead to app store purchases must have metrics collection and analytics to measure progress or drop-out.\n\n\nData Platform Technology and Architecture: A plan to build a robust platform of data storage, data feeds, visualisation and modelling tools and a supporting connective infrastructure.\nAnalytics: An ability to apply models and perform deep analysis on the data that you have.\nDemocratization of data:\n\nMaking data available where necessary, cataloguing it, making it discoverable and well understood to encourage staff in the company to make effective use of it.\n\nPeople:\n\nHiring and retaining top talent, developing the staff you already have, fostering a culture of technical excellence and collaboration.\nExample\n\nO: Commit to developing our staff to reduce attrition and skills leaking out\nKR: Staff churn is kept below 10% each quarter\nData Strategy: Keep our teams technically skilled, engaged and current\nData OKR: 75% of staff in our data teams successfully complete 3 online technical courses in a year\n\n\nCompliance/Governance: Remaining compliant with regulatory data requirements and company policies with respect to data collection and usage. Having efficient and transparent processes in place to ensure data teams are applying regulations and policies when developing solutions.\nData Quality and Management: Setting the standards and mechanisms for data to be trusted as it flows through the company.\nSecurity: Keeping in lock step with the enterpriseâ€™s broader approach to keeping data and systems safe.\nData Literacy and Culture: Plugging the outputs of models and analytics into the decision fabric of the company. How to take data outcomes and operationalise them, turning them into actions for the business. The promotion of data as a first class concern for the company.\n\nExample\n\nO: Data plays a key part of the input to product development\nKR: Use Lifetime Value (LTV) calculations as an input to the product owners who are developing product features to engage higher value customers\nData Strategy: Improve Data Literacy for Decision Making\nData OKR: The output of LTV calculations are linked to &gt;200 feature development story points in the product team scrums\n\n\n\n\n\nPerforming an Organization Assessment\n\nThis assesment will provide the foundation for a data strategy\n\nThis is in the context of a nonprofit organization but the main parts should generalizable\n\nMission and Theory of Change\n\nA concrete outline which states\n\nThe impact that will be generated\nThe conditions needed to generate the impact\nThe programs in place to create those conditions.\n\nEach piece of the Theory of Change can then be stated in terms of a quantifiable measure of success which will serve as the starting point for developing a data strategy.\nExample\n\n\nStakeholders\n\nAnswer these questions:\n\nWho are your stakeholders?\n\nIdentify subgroups and individuals who fall into these groups\n\nDonors and Volunteers.\nManagement and Employees.\nBeneficiaries.\n\n\nWhat questions do stakeholders have that can be answered through data?\n\nDonors and Volunteers\n\nExample: A data-driven Impact Report which provides a holistic view of how the nonprofit utilizes their resources to achieve its mission.\n\nContains anectdotal stories with data that demonstrate how effectively their resources are being used\n\n\nManagement and Employees\n\nExample: More granular views on how individual programs and initiatives are performing on metrics related to their Theory of Change\n\nBeneficiaries\n\nExample: Data around how projects in different sectors are performing\n\n\nHow will the data affect stakeholder decision making?\n\nDonors and Volunteers\n\nCan influence decisions around donating time and money\n\nManagement and Employees\n\nProvides visibility into how resources are allocated internally and empowers internal decision makers to evaluate how to get the most impact out of the limited resources they have\n\nbeneficiaries\n\nCan be used to garner buy in and allow the nonprofit access to communities that they would otherwise not have\n\n\n\n\nData Gap Analysis\n\nIdentify gaps between current data capabilities and those needed to answer all stakeholder questions\nContents\n\nOutline all data needs in the form of questions derived from your Theory of Change and stakeholder analysis.\nDeep dive into the required data to answer the questions and an estimate of how much that data would cost to obtain. Donâ€™t forget that the same data could answer multiple questions.\nIdentify data that has already been collected and any existing efforts to collect additional data.\nConnect existing data and data efforts to questions and determine gaps between questions and data.\nPropose strategies to bridge data gaps and sustain data assets. Evaluate both the benefits of answering the question and costs of acquiring the data.\nPrioritize data gaps to close.\nCommunicate findings to relevant stakeholders.\n\n\n\n\n\nData Advantage Matrix\n\nNotes from Data Advantage Matrix: A New Way to Think About Data Strategy\nSystematic way of organizing a data strategy around data investments which will help you build sustainable competitive advantages to outperform your competitors\n\nThese investments wonâ€™t necessarily be ones that return the greatest ROI at least in the near-term.\n\nMatrix Format\n\n\nTypes (Y-Axis)\n\nOperational: This is about understanding the levers that drive your business, then using them to improve operations. A key aspect is making data available and understandable to those who are making daily decisions.\n\nExamples:\n\nDaily updates about key metrics.\nIs there a drop in conversion rate?\nAre we meeting our KPIs?\n\n\nStrategic: Every company makes a few critical strategic decisions each year. The more data-driven these decisions are, the more likely that they will jumpstart growth or success.\n\nExamples:\n\nWhich cities should we launch in?\nWhich customer segments should we focus on?\nHow much should we set the price of our product?\n\n\nProduct: This is when companies leverage data to drive a core product advantage, one that separates them from competitors.\n\nExamples:\n\nGmailâ€™s â€œsmart composeâ€ auto-completion feature.\nUberâ€™s Supply and Demand Optimization Algorithm\n\n\nBusiness opportunity: This involves using company data to find and create new business opportunities.\n\nExamples:\n\nNetflix Originals, where Netflix started to produce its own TV shows and movies based on its data about what people want to watch.\nTelecoms building Know-Your-Customer (KYC) services to monetize that data\n\n\n\nStages (X-Axis)\n\nBasic: This is a quick-and-dirty MVP that uses basic tools (e.g.Â SAAS products, Google Sheets, Zapier) and no data specialists.\n\nAble to quickly deploy and assess a solution\nCan start at Stage 2 if a data advantage is critical to your company and can be built on proper tooling from the start\n\nIntermediate: includes investments in data platform tooling and data specialists or teams\nAdvanced: includes specialized teams for each use case or project\n\n\nExample: SaaS software startup\n\nExample: Uber-like company"
  },
  {
    "objectID": "qmd/job-organizational-and-team-development.html#sec-job-orgdev-dtroi",
    "href": "qmd/job-organizational-and-team-development.html#sec-job-orgdev-dtroi",
    "title": "Organizational and Team Development",
    "section": "Determining a Data Teamâ€™s ROI",
    "text": "Determining a Data Teamâ€™s ROI\n\nMisc\n\nExample of model ROI calculation: Domain Knowledge Notebook, Banking/Credit &gt;&gt; Fraud &gt;&gt; Misc\nhttps://towardsdatascience.com/calculating-the-business-value-of-a-data-science-project-3b282de9be3c\nVisualizing Machine Learning Thresholds to Make Better Business Decisions\n\n\nUses a Telecom subscriptions churn example and incorporates available resources (queue rate) that can review flagged events in order to choose a threshold\n\ni.e.Â If you can only review 50 cases, then you model need only flag 50 cases\n\nAdds uncertainty by using multiple train/test splits and creating quantiles for CIs\nOptimizes resources, costs, precision, and recall to produce a threshold\n\n\nThe best way is for other department heads to tell it. If there are cutbacks to your team, it should illicit howls from department heads and should be fighting for more data resources.\nFor automating tasks, get an estimate of how much time and how many people it takes for that task to be completed during the projectâ€™s planning stages.\n\nUsing their salaries, estimate hourly wage and calculate sum(employee_wage * time_spent_on_task)\nAfter you automate task or make it more efficient, make the same estimation and the difference will be the ROI that data science provided\n\nRecord improvements from baseline or old model to new model using cost function\nExample Metrics\n\nCost Reduction: Raw material cost reduction per unit, cost of goods sold per unit, cost of goods manufactured per unit etc.\nYield Improvement: Percentage of waste/rejection reduced\nHuman Time Saving: Reducing manual labor by 30% per day, per batch, per sprint or any other suitable metrics\nProcess Improvement: Reduce cycle time by 20 hours, reduce waiting time by 2 hours, reduced exit cycle time by 8%, and minimize the customer return processing time by 6%\nSpeed: Reduce time to market by 100 days average delivery time by 10 minutes.\nQuality: Reduction in number of customer returns by 20%, decrease in number of customer complaints by 14%, reduction in warranty calls by 10%, and decrease in number of bugs by 12%\nMarket Share: Growth in market share by 5% compared to the previous year for a given brand, geographic region, customer segment or overall consumer base\nCustomer Base: Growth in customer base by 6% year over year, increase in average time spent per user session by 10%, increase in customer conversion rate by 25%, and increase in customer retention by 12%\n\nApplying Metrics\n\nSchedule meetings with business users to walk through the potential metrics and help decide which ones apply to your project\nEstablish baselines for each metric\n\nIf historical data isnâ€™t available, need to work with multiple business teams or users to make assumptions and approximate the raw numbers to come up with the required baseline value\n\nAfter a reasonable period (depends on context, e.g.Â 6 months), calculate the change between the baseline and post-treatment for each metric\n\nIf department heads arenâ€™t willing to go to bat for your team, then examine three areas\n\nSiloed data teams\n\nOften data products are only part of the decision making process and when the data teams are separated from other departments itâ€™s difficult to quantize their contribution.\n\nExample\n\nWhen deciding to purchase goods from a supplier, the demand forecast is obviously really important, but so does the allowed pack sizes, the minimum order quantity, the storage finite capacity, etc.\n\n\nSolutions\n\nConsider changing to an integrated schema, Models for Integrating Data Science Teams Within Organizations like the hybrid â€œproduct data scienceâ€ schema\nTrack Decision Impact (DI) metrics ($), which doesnâ€™t focus on intrinsic error (e.g.Â RMSE, MAE, F1, etc.), but rather on the quality of the decisions made that involved your data product.\n\nNotes from 10 Reasons to Implement the New Generation of Business Oriented Metrics\nAlso see Domain Knowledge Notebook, Logistics &gt;&gt; Decision Impact Metrics\nExample with forecasting\n\n\nOther viz of these values could include stacked bars, gauges, or areas\nTotal range thatâ€™s addressable for forecast improvement\n\nDIn-o = DIn - DIo\n\nDIn is the cost associated with the decisions made using a prior method or baseline or naive forecast\nDIo is the truth. What are the costs associated with a decision if perfect knowledge were available. Calculate one the future observed values become available\n\n\nAdded value delivered by the actual forecasting process\n\nDIn-a = DIn - DIa\n\nDIa is the cost associated with the decision made using your forecast\n\n\nThe maximum value that could still be delivered by improving the actual forecast process\n\nDIa-o = DIa - DIo\n\n\n\n\n\nPlanning\n\nObjectives and key results (OKR) is a goal-setting framework for defining and tracking objectives (O) and their outcomes (KR).\n\n\nInfrastructure-level objectives â€” like implementing a new data warehouse â€” can live separately, but should still have explicit callouts for how those investments are supporting the higher-level objectives.\n\nData people should be included in other department planning meetings and stakeholders should be involved in data team meetings where projects are prioritized. Gives them more insight into data team activities.\n\ne.g.Â If the Marketing team has a weekly planning meeting or daily stand-ups, the Data analysts supporting that team should be in the room (or Zoom, or whatever).\n\n\nTools\n\nInteractive apps (shiny) where stakeholders can run different scenarios can integrate them more into the process\n\nI think this is fine where applicable, but not to go overboard with it. This dude (article author) also builds these types of platforms and probably was pushing his product a bit."
  },
  {
    "objectID": "qmd/job-organizational-and-team-development.html#sec-job-orgdev-devdatplat",
    "href": "qmd/job-organizational-and-team-development.html#sec-job-orgdev-devdatplat",
    "title": "Organizational and Team Development",
    "section": "Developing a Data Stack",
    "text": "Developing a Data Stack\n\n\nPriority is from top to bottom\n\n\nBuild vs Buy Factors\n\nFactors for deciding on whether you should build, buy SaaS, or go with Open Source solutions\nThe size of your data team\n\nUnless you a big tech company (e.g.Â Airbnb), itâ€™s better to go SaaS or Open Source\n\nUnless you have a large data team, engineers and analysts are too busy to maintain or onboard new ppl on custom tools\nItâ€™s costly and time consuming to build your own unless this is already part of your companyâ€™s dna (i.e a tech firm).\n\n\nThe amount of data your organization stores and processes\n\nSelect one that will scale with your business\n\nYour data teamâ€™s budget\n\nLimited budget but many hands, then open source\n\nCaveats\n\nOnly about 2 percent of projects see growth after their first few years\n\ni.e.Â The need to be stabl and actively maintain over an extended period\n\nTypically on your own, so make sure the project maintainers respond to problems and thereâ€™s an active communtiy that uses the tool\n\n\n\n\n\n\nData Ingestion\n\nTools\n\nIngestion\n\nFivetran, Singer, Stitch, AirByte, Kafka\nSee\n\nDomain Knowledge Notebook, Product Development &gt;&gt; Behavioral Data\nProduction, Tools &gt;&gt; Ingestion\n\n\nOrchestration\n\nApache Airflow, Prefect, and Dagster\nSee Production, Tools &gt;&gt; Orchestration\n\n\n\n\n\nData Storage and Processing\n\nSnowflake, Google BigQuery, Amazon Redshift, Firebolt, Microsoft Azure, Amazon S3, Databricks, Dremio\nAlso see Databases, Lakes, Databases, Warehouses\nResources\n\nChoosing Open Wisely - Pros/cons of going open source\n\n\n\n\nData Transformation and Modeling\n\nTools\n\ndbt â€“ Short for data build tool, is the open source leader for transforming data once itâ€™s loaded into your warehouse.\n\nSee\n\nProduction, Tools &gt;&gt; ETL, ELT Orchestration\nDatabases, dbt\n\n\nDataform â€“ Now part of the Google Cloud, Dataform allows you to transform raw data from your warehouse into something usable by BI and analytics tools\nPython + Airflow\n\n\n\n\nBusiness Intelligence and Analytics\n\nTools\n\nLooker â€“ A BI platform that is optimized for big data and allows members of your team to easily collaborate on building reports and dashboards.\nTableau â€“ Often referred to as a leader in the BI industry, it has an easy-to-use interface.\nMode â€“ A collaborative data science platform that incorporates SQL, R, Python, and visual analytics in one single UI.\nPower BI â€“ A Microsoft-based tool that easily integrates with Excel and provides self-service analytics for everyone on your team\nR, Python\n\n\n\n\nData Observability\n\nAlso see Production, ML Monitoring\nCircumstances where it would be a good time to implement this stage:\n\nMigrating from trusted on-premises systems to the cloud or between cloud providers\n\nUsers of those older systems need to have trust that the new cloud-based technologies are as reliable as the older systems theyâ€™ve used in the past\n\nYour data stack is scaling with more data sources, more tables, and more complexity\n\nThe more moving parts you have, the more likely things are to break unless the proper focus is given to reliability engineering\nRule of thumb is more than 50 tables\n\nBut if you have fewer and the severity of data downtime for your organization is great, then data observability is still a very sensible investment\n\n\nYour data team is growing\n\nOften leads to changes in data team structures (from centralized to de-centralized), adoption of new processes, and knowledge with data sets living amongst a few early members of the data team.\nBecomes harder for data analysts to discern which tables are being actively managed vs.Â those that are obsolete\nTechnical debt will slowly pile up over time, and your data team will invest a large amount of their time into cleaning up data issues\n\nYour data team is spending at least 30% of their time firefighting data quality issues\nYour team has more data consumers than you did 1 year ago\nYour company is moving to a self-service analytics model\n\ni.e.Â allow every business user to directly access and interact with data\n\nData is a key part of the customer value proposition\n\ni.e.Â the company starts deriving substantial value from customer-facing applications\n\n\nMust be able to monitor and alert for the following pillars of observability:\n\nVolume: Has all the data arrived?\nSchema: What is the schema, and how has it changed? Who has made these changes and for what reasons?\nLineage: For a given data asset, what are the upstream sources and downstream assets which are impacted by it? Who are the people generating this data, and who is relying on it for decision-making?\nAvailability: Whether the data is available. Things like network issues or infra issues can prevent users from accessing data.\nFreshness: How up-to-date your data tables are, as well as the rhythm when your tables are updated.\n(In)Completeness:The percentage of unexpected missing data entries, can be both on the row level and column level. For example, is there any row from the upstream tables not being processed? Is there any field in the destination table missing data for &gt; X% of rows?\nDuplicates: The percentage of unexpected duplicated primary key(s).\nDistribution: The accepted range of certain fields. It can be an enumeration or a range of numbers.\nFormat: The expected format and schema of the data like CSV, or BigQuery table\nRelationship: Any test on complicated business logic which involves multiple columns or tables.\n\n\n\n\nData Reliability\n\n\nAlso see\n\nItâ€™s Time to Set SLA, SLO, SLI for Your Data Team â€” Only 3 Steps for more details on benefits\nSLOs, SLIs, SLAs, oh myâ€”CRE life lessons\nGlossary: DS terms\n\nSLAs\n\nIt improves the communication between engineers and stakeholders by clearly defining the scope of data reliability and what â€œokayâ€ and â€œnot okayâ€ means. Making it crystal clear avoids the needless discussion on what does and doesnâ€™t need attention.\n\nGives engineers an error budget which is a metric to prevent engineers from burning out and it helps the team allocate their time wisely\n\nIt helps engineers decide how much time should be spent on delivering features and how much time should be spent on making the existing pipelines more reliable.\n\nSLOs\n\n\nIf data availability threshold is set to 99.9% in SLA, then it should be 99.99% in SLO.\nIf a service breaks SLO, on-call engineers need to react quickly to avoid it breaking SLA, otherwise, the company (or the team) will lose money (or reputation).\nTo achieve overall availability 99.9%, the team needs to monitor the up-time of a few internal tools and each of them has its own SLO threshold.\n\nSLIs\n\nMetrics in the monitoring system\nYour SLIs will depend on your specific use case, but here are a few metrics used to measure data trust, a common KPI:\n\nThe number of data incidents for a particular data asset (N). Although this may be beyond your control, given that you likely rely on external data sources, itâ€™s still an important driver of data downtime and usually worth measuring.\nTime-to-detection (TTD): When an issue arises, this metric quantifies how quickly your team is alerted. If you donâ€™t have proper detection and alerting methods in place, this could be measured in weeks or even months. â€œSilent errorsâ€ made by bad data can result in costly decisions, with repercussions for both your company and your customers.\nTime-to-resolution (TTR): When your team is alerted to an issue, this measures how quickly you were able to resolve it.\n\n\nSteps\n\nCreate the SLA with your stakeholders\n\nDefine what reliable data means together with your stakeholders\n\nData engineers can assess the historical performance of the data to gain a baseline and understand its usage pattern, what fields are mostly queried, and at what frequency\nWhat do stakeholders care about the most? Freshness? Accuracy? Availability? Duplicates?\n\nStart low so engineers donâ€™t need to run 24/7 rotations and stakeholders are typically ok with a few hours of downtime initially (99% = ~1.68 hours downtime per week). As the situation gets more stable, you can increase it to the ideal number. (99.99% = few minutes downtime per month)\nExample: Critical Revenue Table on BigQuery\n\nAvailability 99.9%: Table should always be accessible.\n\nAvailability has a higher SLA because it mostly relies on BigQuery service which promises 99.99% up-time.\n\nFreshness 99%: Table should be refreshed daily before 9 am with yesterdayâ€™s revenue.\nUniqueness 99%: Table shouldnâ€™t contain any duplication on the composite key.\nCorrectness 99%: Amount fields like gross_booking, net_booking, net_revenue should be calculated correctly.\n\n\nSelect SLIs\n\nSelect metrics that will help you meet the terms of the SLA\nThe selection of indicators is very specific to the data infrastructure the team is using\nExample: Airflow + BigQuery + dbt to deliver data\n\nAvailability: Seconds since the latest heartbeat (i.e.Â timestamp emitted by a job) from Airflow\n\nThe downtime of Airflow doesnâ€™t have a direct impact on the BigQuery tableâ€™s availability, but as mentioned previously, itâ€™s worth monitoring internal tools which contribute to the final SLA.\n\nFreshness: Hours since the table was updated last time.\nUniqueness: The test result of the uniqueness test in dbt.\nCorrectness: The test result of other value checking in dbt.\n\n\nDefine the SLO and set up alerts\n\nSet the internally acceptable range of failure per indicator.\nTo give on-call engineers enough reaction time before thereâ€™s a violation of the SLA, the SLO should be more strict than the SLA.\nCreate alerting rules and rate incidents by the level of severity.\nCan display on a dashboard, making it effective to communicate and resolve the issue\nExample: See SLO and SLI examples\n\nThe services you use will have dependencies, so youâ€™ll need to take them into account when defining your SLOs\n\nIf the dependencies have stricter SLO than you have (i.e.Â what your customer needs to be happy), then there nothing more to do.\nIf some do NOT, then you need to mitigate those risks.\n\nSee article (bottom) which links blog, blog, spreadsheet tool (used to calculate risk)\n\n\n\n\n\n\n\nData Discovery\n\nNeed a reliable, scalable way to document and understand critical data assets\nFeatures\n\nSelf-service discovery and automation\n\nData teams should be able to easily leverage their data catalog without a dedicated support team.\nGreater accessibility naturally leads to increased data adoption, reducing the load for your data engineering team.\n\nScalability as data evolves\nReal-time visibility into data health\n\nUnlike a traditional data catalog, data discovery provides real-time visibility into the dataâ€™s current state, as opposed to its â€œcatalogedâ€ or ideal state.\nGlean insights such as which data sets are outdated and can be deprecated, whether a given data set is production-quality, or when a given table was last updated.\n\nSupport for governance and warehouse/lake optimization: From a governance perspective, querying and processing data in the lake often occurs using a variety of tools and technologies (Spark on Databricks for this, Presto on EMR for that, etc.), and as a result, there often isnâ€™t a single, reliable source of truth for reads and writes (like a warehouse provides). A proper data discovery tool can serve as that central source of truth."
  },
  {
    "objectID": "qmd/job-organizational-and-team-development.html#sec-job-orgdev-daoce",
    "href": "qmd/job-organizational-and-team-development.html#sec-job-orgdev-daoce",
    "title": "Organizational and Team Development",
    "section": "Developing an On-Call Environment",
    "text": "Developing an On-Call Environment\n\nMisc\n\nNotes from How to Build an On-Call Culture in a Data Engineering Team\n\n\n\nCreate Table of Production Issues\n\n\nThe more detailed the guideline, the more streamlined the on-call process will be.\n\n\n\nCreate a Workflow\n\n\nOn-call workflow tells engineers how to approach a variety of production requests in a consistent manner.\nMore preparation means fewer decisions we have to make on the spot, leading to fewer mistakes.\nMain Steps\n\nDefine the sources of alerts. Redirect all production issues into one or two channels. For example, use Slack integration to redirect pipeline issues, infrastructure failures, and test failures into a centralized Slack channel for easy tracking.\nIdentify the category of alerts, the scale of the impact, and its urgency. Every on-call should be able to assess the urgency of the issue based on its category, impact, and SLA requirements. By creating â€œdata productsâ€ with clear requirements, teams can benefit from the process that enables them to identify the impact and urgency efficiently. I recommend article â€” Writing data product pipelines with Airflow, a nice practice to write data requirements as code in Airflow dags.\nIdentify the root cause and solve the issue. When an urgent issue arises, on-call should do their best to find the root cause and solve the issue. However, not every data engineer knows all the nitty-gritty of data models maintained by data analysts. In such situations, following an escalation pattern can be helpful. It allows engineers to ask for help from other engineers or analysts with necessary expertise until the issue is resolved.\nPerform post-incident actions and update the on-call journal. Donâ€™t forget to perform post-incident actions like backfilling to correct historical data for incremental models. Itâ€™s also recommended to keep an on-call journal for knowledge sharing.\nUser communication. In a parallel thread, itâ€™s important to keep users in the loop. Effective communication during the â€œdata downtimeâ€ builds trust between the data team and users. One of my articles â€” Status Page for Data Products â€” We All Need One introduces the status page as a method to improve effective communications during data downtime.\n\n\n\n\nOn-call ownership\n\nClearly, engineers are responsible for technical failures, but when it comes to data model failure, ownership becomes controversial.\nOptions\n\nAssign an owner to each data model as much as you can. Simply assigning an owner to the model tremendously improves efficiency during on-call.\nTreat data model owners as â€œexternal partiesâ€. Itâ€™s not uncommon that software relies on an external party that is outside of engineersâ€™ control such as an IoT service that relies on a network provider. Similarly, data engineers may need to work with model owners who are outside of their immediate team to address the model failures. When external knowledge is required, engineers should feel comfortable reaching out and proactively working with them while informing users of the progress. Do not put stress on on-call engineers by expecting them to solve issues on their own.\n\n\n\n\nOn-call rotation\n\nSchedule\n\nCan use spreadsheets to manage rotation schedules and a cron job to propagate the schedule into a calendar in near real-time. (e.g.Â Google Sheets + Apps Script + Google Calendar)\nPlatforms ($): Opsgenie and PagerDuty\n\nPermissions\n\nOn-Call engineers may need additional permissions at times\nOptions\n\nPerform a permission escalation, temporarily granting the engineer additional privileges.\nCreate a high-privileged user group and rotate group members.\n\nEssential to ensure that the rotation of the group members must be in sync with the on-call calendar rotation.\n\n\n\n\n\n\nCommunication channels\n\nFinding the right balance between being informed and not being overwhelmed by alerts is crucial\nCentralized data alerts channel (alerts -&gt; team)\n\nBy having a dedicated channel where all alerts are sent, it becomes easier to monitor and manage alerts, reducing the risk of critical information being missed or overlooked\nSlack is a popular choice because it can easily integrate with various data sources such as Opsgenie, GCP Cloud logging, Sentry, service desk, etc.\n\nEscalation policies (team -&gt; team)\n\nA set of procedures that outlines how an organization will respond to issues that require additional resources beyond the initial response.\n\nUser communication (team -&gt; users)\n\nNeeds to start as soon as the issue is identified. Keeping the channel centralized by setting up a tool like status page.\n\n\n\n\nOn-call runbook\n\nA set of instructions that on-call can follow when responding to issues.\nMust be regularly updated to reflect the changes.\nComponents\n\nMetadata around the data product: owner, model incrementality, priority, schedule, SLA, and SLO.\nEscalation procedures (if not handled automatically).\nTroubleshooting guides: how to solve common issues. For example, perform full-refresh, check source data, logs, data observability tools and etc.\nPost-incident verification: how to verify if the issue is properly solved. For a cron job, the issue can only be verified in the next run which can be a few hours or days later.\n\n\n\n\nOn-call journal\n\n\nTool for documenting production issues\nHelps engineers who look for tested solutions and managers who search for trends.\nA templated journal ensures engineers approached each issue with the same scientific rigor.\nEach record includes intensive metadata around the issues and the in-depth investigation and what they did to fix the issue."
  },
  {
    "objectID": "qmd/job-reports.html#misc",
    "href": "qmd/job-reports.html#misc",
    "title": "48Â  Reports",
    "section": "48.1 Misc",
    "text": "48.1 Misc\n\nChicago manual of style for citations\nReread after 3 days to make sure it makes sense before publishing\n\nDancho does his labs on Wed afternoons, so maybe thatâ€™s a good time to release articles.\n\nPrint out article and highlight topic sentences\n\nDoes each topic sentence describe the paragraph. Do all the other sentences in the paragraph support the topic sentence.\nDo the topic sentences produce a good outline about the subject you wanted to discuss. Do they follow a logical data storytelling sequence.\n\nPrimary interests of business people: business question, budget, whether research is conclusive or not conclusive, and value the research or product provides.\nKeep color schemes for categoricals, metrics, etc.\n\nIf you used a color palette for male/female in an earlier section/slide, keep that same palette throughout.\n\nKeep date and other variable formats consistent throughout\nNo more than 3 dimensions on a chart\nPick the chart, graph or table that best fits with the paragraph and move on to the next point. Donâ€™t use multiple charts that show the same thing.\nNever introduce something into the conclusion that was not analyzed or discussed earlier in the report.\nDo not include more information than is necessary to support you report objectives\nPhrases for communicating uncertainty\n\nâ€™Hereâ€™s something we expect to see a lot,â€\nâ€œhereâ€™s something we expect to see sometimesâ€\nâ€œhereâ€™s something that could happen on rare occasions, but which is worth considering because of the high stakes.â€"
  },
  {
    "objectID": "qmd/job-reports.html#layouts",
    "href": "qmd/job-reports.html#layouts",
    "title": "48Â  Reports",
    "section": "48.2 Layouts",
    "text": "48.2 Layouts\n\nNotes from: https://towardsdatascience.com/how-i-create-an-analyst-style-guide-for-a-good-data-story-presentation-6ec9f2504ac8\nMost important details (i.e.Â the conclusion) always come first\n\ne.g.Â executive summaries at the beginning of reports; Conclusions/useful sentences for titles of sections and slide titles\nThe goal is to reduce the time required by the reader to understand what youâ€™re trying to tell them. If they want further details, they can read on further.\n\nUse consistent layouts so your audience can get used to where different types of information will be located\n\nExample: Driver layout\n\nplot the trend of the Goal KPI on the left side with a text description in the same box.\nuse the larger space on the right side to plot the trends of the Driver KPIs that can explain the development of the Goal KPI\n\nthe Goal KPI is Sales Revenue and the Driver KPIs are Leads (#), Conversion Rate (%) and Order Value (EUR)\n\n\nExample: Contrast layout\n\nUseful to highlight the difference in two or more KPIs given the same segmentation\nDivide the space equally depending on the number of the metrics I want to compare with.\n\nThe contrast is between the metrics\nThe segmentation is gender and age groups\nTakeaway: Females generate most revenue and cost the least to obtain"
  },
  {
    "objectID": "qmd/job-reports.html#emails-slack-etc.-on-project",
    "href": "qmd/job-reports.html#emails-slack-etc.-on-project",
    "title": "48Â  Reports",
    "section": "48.3 Emails, Slack, etc. on Project",
    "text": "48.3 Emails, Slack, etc. on Project\n\nremind stakeholder of what it is we agreed in last meeting youâ€™d do, what you did and how to interpret the results\nState what it is you need from the stakeholder.Â \nState whether the project progress in the middle, at itâ€™s end, youâ€™re wrapping up or whatâ€™s going on?\na summary slide or results peppered with comments leading me through what it is I am looking at\nThe Minto Pyramid Organize the message so that it starts with a conclusion which leads to the arguments that support it and ends in detailed information.\n  Process\n      Write conclusion (2-3 sentences max)\n\n      Supporting arguments: Try to make them concise bullet points\n\n      LinkÂ  to a more detailed explanation at the bottom if need be\nMight be useful to time the arrival when the stakeholder is most likely able to read it.\n\ne.g.Â if a stakeholder has a meeting at 9:30 every morning, it may be better to time the sending of the report to before or after that meeting."
  },
  {
    "objectID": "qmd/job-reports.html#exploratory-data-analysis-plan",
    "href": "qmd/job-reports.html#exploratory-data-analysis-plan",
    "title": "48Â  Reports",
    "section": "48.4 Exploratory Data Analysis Plan",
    "text": "48.4 Exploratory Data Analysis Plan\n\n(see code &gt;&gt; rmarkdown &gt;&gt; reports &gt;&gt; edarp-demo.Rmd) (notes fromÂ Pluralsight Designing an Exploratory Data Analysis Research Plan) (Each section should have an intro with a description about whats in it)\n\n\nAbstract\n\nHighlights the research questions\nWho the stakeholders are\nMetrics of success\nExample\n\nâ€œThe foundational task was to develop sales insights across stores. Through the identification and inclusion of various business groups, data were gathered and questions were formed. The business groups included are Marketing, IT, Sales and Data Science. From this process we defined the primary goal of this research. This research adds understanding to how sales are driven across stores and develops a predictive model of sales across stores. These outcomes fit within budget and offer an expected ROI of 10%.â€\n\n\nFigures and Tables\n\noptional depending on audience\nsection where all viz is at\n\nIntroduction\n\ndetailed description of metrics of success\n\nExample\n\nÂ ROI 8%\nR^2 75%\ninterpretability\n\n\n\nStakeholders\n\nMarketing\n\nlist of people\n\nIT\nSales\nData Science\n\nBudget and Financial Impact\n\nNot always known, but this section is valuable if youâ€™re able to include it.\nPotential vendor costs\ninfrastructure costs\napplication developement\nfinancial impact, completed by finance team, result in an expected ROI of blah%\n\nMethods\n\ndata description\ndata wrangling\n\nwhat were the variables of interest and why (â€œdata wrangling involved looking at trends in sales across stores, store types, and statesâ€)\n\nautocorrelation\n\nâ€œtesting for autocorrelation was completed leading to insights in seasonality across the stores. We examined by the ACF an PACF metrics in the assessment of autocorrelationâ€\n\nclustering\noutliers\n\ndescription of algorithm comparison and model selection\n\nwords not code or results\nexample\n\ninvolved training and testing regression, random forest,â€¦\nregression mod served as a benchmark comparison across 5 models\nA discussion of interpretability and expected ROI guided the choice of the final modelÂ \n\n\n\n\nResults and Discussion\n\nâ€œThis section highlights the thought process that went into wrangling the data and building the models. A few of the insights gained in observation of the data are shared. Also, the assessment of the model is discussed at the end of the section.â€\nVisualizing the Data (i.e.Â EDA viz - descriptive, outliers, clusters)\n\nfig\ninsights\nrepeat as needed\n\nVariable Importance\nFinal Model\n\nModel Assessment\n\nalgorithm comparison metrics\ndynamic visual of model output\n\nsimple shiny graph with a user input and a graph\n\ne.g.Â choose store number - graph of sales forecast\n\n\n\n\nConclusion\n\nâ€œThe research explored the possibility of building a predictive model to aid in forecasting sales across stores. We found that, given the established metrics of ROI greater than 8%, R-square of greater than .75 and interpretability in the models, this reasearch has resulted in a viable model for the business. Additionally, it was discovered the presence of some outlier phenomena in the data which has been identified by the stakeholders as acceptable noise. Further we discovered that there is a latent grouping to the stores across sales, store type and assortment. This insight will be used to guide marketings action in the future.â€\n\nAppendix\n\nSchedule of Maintenance\nFuture Research"
  },
  {
    "objectID": "qmd/job-reports.html#narrative-structures",
    "href": "qmd/job-reports.html#narrative-structures",
    "title": "48Â  Reports",
    "section": "48.5 Narrative Structures",
    "text": "48.5 Narrative Structures\n\nDeveloping a narrative when presenting results is imperative in order for recommendations to gain traction with stakeholders\n\nbudget at least 50% of time in the project plan for insight generation, and structuring a narrative (seems a bit large)\nWith each iteration (potentially dozens) of improving your presentation, you are looking to address any insight gaps, and improve the effectiveness in conveying the insight and recommendations\nAnticipate potential follow up questions they might ask and preemptively address them\nEliminate any distractions to the key message such as ambiguous statements, or erroneous facts that can derail the presentation\nIf possible find someone with tenure in the organization, or has expertise in the business area you are analyzing to lend a critical eye to your presentation.\n\nAlso may provide insight on how best to win the trust of key decision makers and potential areas that can derail the effort\n\n\nExample 1\n\nExecutive Summary\n\nBrief Description of problem\nApproach taken\nmodels used\nresults\nconclusion\nrecommendations\n\nDescribe the status quoÂ \n\nMaybe describe what each proceeding section will entail\n\nWhatâ€™sÂ the problem that needs fixing or improved upon\nProposed solutionÂ \nIssues that arose during process, maybe a new path discovered not previously thought of\nSolution\n\nDescription of data\n\nRecommendations or next steps\n\nThe stakeholder must understand the expected outcome, and the levers that need to be pulled to achieve that outcome.\nAn effective analysis owner will take on the responsibility for the stakeholderâ€™s understanding, through communicating both specific predictions and the supporting evidence in a consumable way.\n\n\nExample 2\n\nExecutive Summary\n\nBrief Description of problem\nApproach taken\nmodels used\nresults\nconclusion\nrecommendations\n\nIntroduction\n\nquestion\nbackground\nwhy important\ndescribe structure of the report\n\nmaybe a table of contents\n\n\nMethodology (EDA and Models)\n\ndescribe the data you are using\nthe types of analyses you have conducted & why\n\nResults\n\nmain body of the report split into sections according to the various business questions the report attempts to answer\nthe results generated for each question.\n\nDiscussion\n\nBring together patterns seen in EDA, model interpretations\ncompare with your prior beliefs and/or other papers results\nobjective recommendations for business actions to be taken\n\nConclusion/Summary\n\nrestate question,\nsteps taken,\nanswers to questions,\nissues faced,\nnext steps"
  },
  {
    "objectID": "qmd/job-reports.html#instructional-articles",
    "href": "qmd/job-reports.html#instructional-articles",
    "title": "48Â  Reports",
    "section": "48.6 Instructional Articles",
    "text": "48.6 Instructional Articles\n\nWhat?\n\nGiven a short description of the subject matter\n\nWhy?\n\nWhy is the subject matter important\nWhy is the subject matter useful\nWhy do it this way and not another\nState what each section will entail.\n\nBackground\n\nSome history\nContext surrounding the problem\nBusiness and Data Science interpretations of the problem or subject matter\n\nExample\n\nFramework\n\nDescribe the variables\nDescribe the model\nPotential issues/assumptions with approach\n\nAnalysis\nResults\n\nConclusion"
  },
  {
    "objectID": "qmd/job-reports.html#concepts",
    "href": "qmd/job-reports.html#concepts",
    "title": "48Â  Reports",
    "section": "48.7 Concepts",
    "text": "48.7 Concepts\n\nWhat is the problem we are solving -Â  why are we losing so many customers\nUnderstand the kind of story you want to tell -Â A one-time story:Â what caused the last monthâ€™s shortage. Updated, ongoing story: weekly rise and fall of sales, fraud detection\nKnow your audience - What knowledge your audience brings to the story. What kind of preconceptions does the audience have.\nInclude the critical elements of a traditional story structure - point of view: someone has to ask the question thatâ€™s answered with data. Empathy: need to have human protagonist whoâ€™s solving the problem. An antagonist: confusion or misunderstanding that makes achievementÂ of the solution difficult. AnÂ explicit narrative: this happened, then this happened, and thenâ€¦\nDevelop the right hook - what helps grab the attention of the managers?newspaper lead opening, startling statistics, teaser\nA picture is priceless people like visuals but good ones are really difficult to create\nwhatâ€™s your point? Resolve and close what does your story advise to do? a call to action\nIterateÂ -Â some stories need to be retold continuously when new data arrives, good stories live on"
  },
  {
    "objectID": "qmd/job-reports.html#explaining-your-model",
    "href": "qmd/job-reports.html#explaining-your-model",
    "title": "48Â  Reports",
    "section": "48.8 Explaining your Model",
    "text": "48.8 Explaining your Model\n\nMisc\n\nÂ For ML models use feature importance to pick predictors to use for partial dependence plots (with standardized predictors, these can also advise on feature importance) and go back to do descriptive/aggregated statistical explorations (box plots, bars, etc.). Explain whatâ€™s happening in the plots, potential reasons why itâ€™s happening, and potential solutions.\n\nTypes\n\nWhen talking to a colleague or regulator you may need to give more technical explanations. In comparison, customers would expect simpler explanations. It is also unlikely that you would need to give a global explanation to a customer. This is because they would typically only be concerned with decisions that affect them personally.\nGlobal: Explain what trends are being captured by the model in general\n\nâ€œWhich features are the most important?â€ or â€œWhat relationship does feature X have with the target variable?â€\n\nLocal: explain individual model predictions\n\nTypically needed to explain a decision that has resulted from a model prediction\nâ€œWhy did we reject this loan application?â€ or â€œWhy was I given this movie recommendation?â€\n\n\nCharacteristics\n\nTrue: Include uncertainty in your explanations of your model predictions\nCorrect level: Use the language of your audience instead of DS or statistical terminology\nNo.Â of Reasons & Significant: Only give the top features that are responsible for a prediction or trend, and those features should be responsible for a substantial contribution\nGeneral: Explain features that are important to large portion of predictions (e.g.Â feature importance, mean SHAP)\nAbnormal: Explain features that are important to extreme predictions or a representative prediction\n\nMight be a feature that isnâ€™t globally important but important for an individual prediction or an outlier prediction\n\nContrasting: Explain contrasting decisions made by your model\n\nâ€œWhy was my application rejected and theirs accepted?â€\nUse important features (ranges/levels of those features) that arenâ€™t common to both decisions"
  },
  {
    "objectID": "qmd/job-reports.html#general-guidelines",
    "href": "qmd/job-reports.html#general-guidelines",
    "title": "48Â  Reports",
    "section": "48.9 General guidelines",
    "text": "48.9 General guidelines\n\nKnow your audience Donâ€™t use technical terms when talking to non-technical people.\n  Fast-track the conversation to the technical stuff when talking to fellow data scientists.\n\n  The more senior the person youâ€™re talking to, the more to the point your message has to be.\n\n  Small talk with long-term clients is always essential to maintain a strong relationship.\n\n  The CEO only wants to know the result of your analysis and what it means for their company.\nSimplified\n\nIntent: This is your overall goal for the project, the reason you are doing the analysis and should signal how you expect the analysis to contribute to decision-making.\nObjectives: The objectives are the specific steps you are taking or have taken to achieve your above goal. These should likely form the reportâ€™s table of contents.\nImpact: The result of your analysis has to be usable â€” it must offer solutions to the problems that led to the analysis, to impact business actions. Any information which wonâ€™t trigger action, no matter how small, is useless.\n\nWhat is theÂ business question?\nWhy is it important?\n\nDoes your model enable us to better select our target audience?\nHow much better are we using your model?\nWhat will the expected response on our campaign be?\nWhat is the financial impact of using your model?\n\nWhat is the data science question?\nWhat is the data science answer?\nWhat is the business answer?\nShow a general form of the equation, definition of terms, before explaining how to fill it with values of particular to your problem\nHow would you recommend your model/results be used?\n\nBe direct. Communicate your thoughts in a forthright manner, otherwise the reader may begin to tune out.\nStart with an outline\n\nState your objective\nList out your main points\nNumber and underline your main points to guide the reader\nEnd with a summary.\n\nOpen with short paragraphs and short sentences\nUse short words. The goal is to reduce friction.\nUse adjectives and adverbs for precision, not exclamation points.\n\nCut *lazy* words like very, great, awfully, and basically. These do nothing for you.\n\nUse down-to-earth language and avoid jargon. Like explaining to a 6th grader\nDonâ€™t use generalities (e.g.Â â€œOur campaign was a great success and we came in under budgetâ€).\n\nBe specific (e.g.Â â€œWe increased click-through rates by 21% while spending 19% less than expected.â€)\n\nTake the time to build down what you have to say. Then, express it confidently in simple, declarative sentences.\n\nEspecially in memos and emails, put your declaration in the subject line or as the first line"
  },
  {
    "objectID": "qmd/job-reports.html#business-presentation",
    "href": "qmd/job-reports.html#business-presentation",
    "title": "48Â  Reports",
    "section": "48.10 Business presentation",
    "text": "48.10 Business presentation\n\nTheyâ€™re only interested in the story the data tells and the actions it influences\nPrep\n\nCreate an outline\nSituation-Complication-Resolution Framework\n\nSituation: Facts about the current state.\nComplication: Action is required based on the situation.\nResolution: The action is taken or recommended to solve the complication.\nExample\n\nOne minute per slide rule\n\nIf you have a 20-minute presentation, aim for 20 slides with content\n\nTry to stick to 3 bullet points\n\nOr if you need to include more information, structure the slide with some sort of â€œ3â€ framework\n\nExample: 3 columns\n\nEach column has 3 bullets\n\n\n\nFocus audience attention to important words\n\nbold, italics, a different color, or size for words you want to emphasize\n\n\nUse emotional elements as hooks to grab attention before starting the introduction. They generate these emotions but also curiosity about what comes next.\n\ngreed - â€œthis has the potential to double revenueâ€\nfear - â€œlayoffs may be comingâ€\npride - â€œwe can do this!â€\nanger - â€œItâ€™s the competitionâ€™s fault!â€\nsympathy - â€œtheyâ€™re counting on us to helpâ€\nsurprise - â€œyou wonâ€™t believe what we foundâ€\n\nUse meaningful sentences as slide titles.\n\nExamples\n\ninstead of â€œSales outlookâ€, use â€œSales outlook is promising in the next 12 monthsâ€.\nInstead of â€œAnnual Salesâ€, use â€œSales Up 22% In 2022â€\nInstead of â€œAlgorithm Training and Validationâ€ use â€œPredict Customer Churn with 92% Accuracyâ€\nInstead of â€œQ1 Conversation Ratesâ€ use â€œAccounts With Direct Contact are 5x More Likely to Purchaseâ€\nInstead of â€œUtilizing XGBoost to Classify Accountsâ€ use â€œMachine Learning Improves Close Rates by 22%â€\n\n\nRead (only) slide titles aloud\n\nBy reading just the tile and title only as you start each slide, the audience will be able to process the message much more easily than reading the written words and listening to you simultaneously.\nFor the rest of the slide, do not read the content, especially if you use a lot of bulleted or ordered lists. Reading all of your content can be monotonous\n\nIntroduction:\n\nproblem: â€œflat 4th quarter salesâ€ and maybe a why? it happened\ngoal: â€œrestore previous yearâ€™s growthâ€\nDescribe the presentation to come: â€œBy analyzing blah blah, we can forecast blah, blahâ€ and maybe a teaser on how it will be solved.\nDesired outcome: â€œOur goal here today is to leave with a budget, schedule, and brainstorm some potential advertising approaches that might be more successfulâ€\nIf analysis is negative, itâ€™s important to frame the story or somebody else will. Could become an investigation or witchhunt. Include something about the way the forward, so keep the focus positive and about teamwork.\nInclude disclaimers/assumptions but only those that directly pertain to the specific subject matter of the presentation\nLayout Q&A ground rules (questions only after or also during the presentation?)\n\nBody\n\nInterpret all visuals. Donâ€™t let the audience reach their own conclusions.\nBullets\n\nshould only cover key concepts so donâ€™t read\nyour narration should add more\n\nmore context\nmore interpretation\nmore content\nmore feeling\n\n\npresentation pattern: present visual â€“&gt; interpret visual\n\nStart with a visual that illustrates the problem â€“&gt; discuss problem â€“&gt; present hypothesis that explains a cause of the problem\npresent visual that is evidence for your hypothesis â€“&gt; interpret visual\n\nrepeat\nvisuals act as a chain of evidence\n\nProvide recommendation for a course of action â€“&gt; present visuals or data that support this action\n\ne.g.Â historical results from previous instances of taking this action\n\nhow this situation mimics the successful instances\n\nforecasts that support the recommendation\n\ntalk about the uncertainty, consequences of lower and upper bounds\n\nsurvey data\n\ninvite questions and comments about the data and visuals you shown if you have no recommendations or courses of action\n\ntake notes (yourself or assistent)\nIf you donâ€™t have an answer:\n\nâ€œI donâ€™t have an answer for that offhand but Iâ€™ll get back to you after we look into that.â€\nâ€œI donâ€™t have the answer to that. I can reanalyze the data and see if they support that idea.â€\n\n\n\n\nSatisfying Conclusion\n\nsummarize (especially if a lot was covered)\nIf you asked for questions or comments above, summarize them and any conclusions from the discussion, which ones require further study, etc.\nIf you provided recommendations, review them and include the rationale for them ideally tied to the data, and the expected results of such actions\n\nâ€œThe price reduction on  has resulted in a strong rebound in sales figures that analysis shows will increase further with additional marketing support. We recommend increasing the advertising budget for this line by 25% next quarter and would like the art department to take on design of a new campaign as their immediate action item.â€\n\nDefine success metrics and what values would require a rethink of the strategy.\nDefine a timeframe\n\nâ€œIt is our hope that the additional 25% marketing investment in the  will result in Q4 revenue that is 50% over last yearâ€™s Q4 revenue for that line. We will review the results next January and meet again to discuss them and determine any changes in course going forward.â€\n\nPotentially include consequences of not following recommendations\n\nâ€œâ€¦ it is unlikely sales will recover and weâ€™ll continue to lose market share.â€\n\nIf anyone made any commitments to other actions, note those.\nBring back emotional hook that you used in the intro\n\nâ€ our analysis shows that blah, blah will justify the further investment and eliminate the need for layoffs.â€\nâ€œâ€¦ should lead to a return to robust sales and profitability, along with stronger profit sharing.\nIf you used greed, conclude with how rewarding the action will be\nif you used fear, end with how the action will alleviate that fear\n\n\nQ&A\n\nplant questions with collegues about info you wanted to include but the topic didnâ€™t fit into the presentation\nprepare for likely questions will have tables or other slides that answer those questions\nDisagreements or questions you donâ€™t have an answer to:\n\nDONâ€™T BE DISMISSIVE\n\nDonâ€™t respone with any variant of, â€œyou donâ€™t trust data?â€ or blaming difficulties on someoneâ€™s lack of â€œdata literacy.â€\nWith so many potential sources of error or misunderstanding, it seems sensible for the data scientist to listen to concerns.\nClient questions provide an important counterweight against over-trust in data products.\n\nGive non-defensive responses\n\nA non-defensive response is helpful when youâ€™re wrong, but pure gold when you are right (and both things will happen from time to time). If you are right, but are argumentative or dismissive, the client is likely to be upset. if you take a clientâ€™s concerns seriously and are thoughtful about addressing the situation, then turn out to be correct on top of that, youâ€™re likely to make a very positive impression.\nPhrases\n\nThatâ€™s a great question. We need to collect more data before weâ€™ll be able to answer that.\nThank you for bringing that to my attention\nI need to think about that\nIâ€™m not prepared to give that the consideration it deserves, but can we make an appointment to discuss it later?\nI hadnâ€™t thought of it that way\nAnything is possible\n\n\nAnswer a question with a question (to clarify)\n\nA great many disagreements arise due to mismatched interpretation of goals and definitions. Itâ€™s important to fully understand the nature of the concern.\nReports sometimes are outdated or refer to a different product, department, etc.\nThey can speed up finding the root cause of your own error.\n\nUse email\n\nFollowing-up emails summarizing an issue, outlining plans, and suggesting timelines for investigations, are nearly always appreciated\n\nBe careful about taking lifelines from the audience\n\nDuring a disagreement, a helpful bystander will often offer a suggestion. Their ideas are usually generous, imagining a way that the data scientist might be correct. It might be tempting to agree, but be careful! Thoughtlessly taking a lifeline is a fast way to lose credibility.\nPhrases\n\nâ€œThatâ€™s a possibility, John, thanks for the suggestion!â€\nâ€œGreat idea, Sally, but I need more time to look at the data to be sure!â€\n\n\n\nDonâ€™t let anyone hijack q&a and turn it into a one and one conversations. Cut off or defer answering a follow up question.\n\nâ€œThanks for your great question, but we do need to let others ask their questions. Please follow-up with me afterwards.â€\n\nThank everyone for attending and leave the front of the room.\n\nFollow-up\n\nkeep promises\n\nanswer questions to promised to look into\npost slide deck if you said you would\nschedule and attend a meeting if you said you would\n\nSend summary email to participants if any actions resulted from the meeting\nSet up monitoring of success metrics. Someone could want an interim report before the settled upon timeframe has been reached."
  },
  {
    "objectID": "qmd/job-reports.html#domain-specific",
    "href": "qmd/job-reports.html#domain-specific",
    "title": "48Â  Reports",
    "section": "48.11 Domain Specific",
    "text": "48.11 Domain Specific\n\nTime Series Notes from Why Should I Trust Your Forecasts?\n\nIn Goodwin et al.Â (paper yet to be published, July 2021), people trusted forecasts more when they were presented as â€œbest caseâ€ and â€œworst caseâ€ values rather than as â€œbounds of a 90% prediction interval.â€\n\nWtf is â€œworst caseâ€? Outside an 80% CI? If so that has a 20% chance of happening.\n\nIn some situations, managers who are not mathematically inclined may be suspicious of forecasts presented using technical terminology and obscure statistical notation (Taylor and Thomas, 1982).\n\nSuch a manager may respect the forecast providerâ€™s quantitative skills, but simultaneously perceive that the provider has no understanding of managersâ€™ forecasting needs â€“ hence the manager distrusts the providerâ€™s forecasts\n\nI donâ€™t understand this one either. What could possibly be the different â€œforecasting needâ€ that the manager needs?\n\n\nExplanations (i.e.Â justifications, rationale, etc.) of the forecast can improve peopleâ€™s perceptions of a forecast. The higher the perceived value of the explanations, the higher the level of acceptance of the forecast. (GÃ¶nÃ¼l et al, 2006)\n\nPeople enjoy the â€œstoriesâ€ and it makes the forecasts more believable.\n\nProvide cues for how to evaluate the forecast in the report\nProvide accuracy metrics in relation to a reasonable benchmark\n\nExample: rolling average, naive, average for these days over the previous 5 yrs, whatever the current method is, etc.\nIn very unpredictable situations, this will help to show that relatively high forecast errors are unavoidable and not a result of the forecasterâ€™s lack of competence.\n\nBeing transparent about assumptions, and even presenting multiple forecasts based on different assumptions, will most likely reassure the user about the integrity of the provider."
  },
  {
    "objectID": "qmd/job-search.html#sec-job-search-misc",
    "href": "qmd/job-search.html#sec-job-search-misc",
    "title": "Search",
    "section": "Misc",
    "text": "Misc\n\nJob board sites like LinkedIn, ZipRecruiter, and Indeed\nCompany career pages\nRecruiters\nUnemployment Benefits Finder - To look up your stateâ€™s policy\nPlan Finder - state-sponsored health insurance plans\nStandard of Living Calculator for various cities\nCaveats Before Signing a Non-Compete\n\nSometimes severance is associated with the noncompete to ensure that enough time has passed before laid-off employees start a new job, so as to make any information they have obsolete.\nIf you donâ€™t have a noncompete, apply for jobs at your former companyâ€™s competitor. If you do have a noncompete, flag some companiesâ€™ career pages for later.\n\nSkill Breakdown"
  },
  {
    "objectID": "qmd/js.html#sec-js-misc",
    "href": "qmd/js.html#sec-js-misc",
    "title": "JS",
    "section": "Misc",
    "text": "Misc\n\nResources\n\nLearn Just Enough JavaScript\n\nBasics: variables, objects, arrays, functions, conditionals, loops\n\nHow to run R code in the browser with webR\n\nNice breakdown of generic JS code to run scripts on a webpage\n\nJavaScript for Data Science\n\nhrbmstr: â€œjavascript has the advantage over R/Python for both visualization speed â€” thanks to GPU integration â€” and interface creation â€” thanks to the ubiquity of HTML5 â€” means that people will increasingly bring their own data to websites for initial exploration firstâ€\nconsole.log is the print method"
  },
  {
    "objectID": "qmd/js.html#sec-js-basics",
    "href": "qmd/js.html#sec-js-basics",
    "title": "JS",
    "section": "Basics",
    "text": "Basics\n\nOperators\n\n// : comments\n... : If you want to copy all the values in your array, and add some new ones, you can use the {â€¦} notation.\n${&lt;code&gt;} :Â  Anything within the${} get ran as code\n\nExample: ${b.letter}: \\$(b.frequency*100).toFixed(2) }%\n\nBackticks indicate itâ€™s like a glue string or f string (i.e.Â uses code)\nb.letter and b.frequency are properties in an array\nto.Fixed is a method that rounds the value to to 2 decimal places\nThis was an example of a tooltip, so output would look like â€œF: 12.23%â€\n\n\n\nVariables\nmyNumber = 10 * 1000\nvariableSetToCodeBlock = {\nÂ  const today = new Date();\nÂ  return today.getFullYear()\n}\nObject: myObject = ({name: \"Paul\", age: 25})\n\nContained within curly braces, { }\nSubset property, name:\n\nmyObject.name which returns value, Paul\nmyObject[\"name\"] which is useful if you have spaces, etc. in your property names\n\nTypes\n\nMap: Object holds key-value pairs and remembers the original insertion order of the keys\n\ne.g.Â See Stats &gt;&gt; By Group\nD3 Groups, Rollup, Index Docs\n\n\n\nArrays\n\nList of objects\n\nContained within brackets, [ ]\nEach row is an object and each column is a property of that object and that property has a value associated with it\n\nBasic examples\nmyArray = [1, 2, 3, 4]\nmyArray = [[1, 2], [3, 4]] // arrays within arrays\nmyArray = [1, 'cat', {name: 'kitty'}] // objects within arrays\nDF-like array\nmyData = [\nÂ  {name: 'Paul', city: 'Denver'},\nÂ  {name: 'Robert', city: 'Denver'},\nÂ  {name: 'Ian', city: 'Boston'},\nÂ  {name: 'Cobus', city: 'Boston'},\nÂ  {name: 'Ayodele', city: 'New York'},\nÂ  {name: 'Mike', city: 'New York'},\n]\n\nEquivalent Functions: Traditional vs Arrow\n// traditional\nfunction myFunctionWithParameters(firstName, lastName) {\nÂ  return `My first name is ${firstName}, and my last name is ${lastName}.`\n}\n// arrow\nmyModernFunctionWithParameters = (firstName, lastName) =&gt; {\nÂ  return `My first name is ${firstName}, and my last name is ${lastName}.`\n}\n\nArrow: Arguments are in the parentheses and the function is inside the curly braces\nString with variables needs to be surrounded by backticks\n\nFunctions Inside Methods: Traditional vs Arrow\n// traditional\n[1, 2, 3, 4, 5].filter(function(d) { return d &lt; 3 })\n// arrow\n[1, 2, 3, 4, 5].filter(d =&gt; d &lt; 3)\n\nThe argument is d but without parentheses and the function is d &lt; 3 without the curly braces\nThe function inputs each row/value of the array, so d is a row/value of the array. Then, the function does something to that row.\n\nConditionals\n\n== vs ===\n1 == '1' // true\n1 === '1' // false\n\n== is a logical test to see if two values are the same\n\n=== is a logical test to see if two values are the same and also checks if the value types are the same\n\nIf/Then\nif(1 &gt; 2) {Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  // If this statement is true\nÂ  Â  return 'Math is broken'Â  Â  Â  Â  Â  Â  Â  // return this\n} else {Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  // if the first statement was not true\nÂ  Â  return 'Math still works!'Â  Â  Â  Â  Â  // return this\n}\n\n// using ternary operator \"?\"\n\nUsing ternary operator â€œ?â€\n\nSyntax: condition ? exprIfTrue : exprIfFalse\nExample: d =&gt; d.frequency &gt;= minFreq ? \"steelblue\" : \"lightgray\"\n\nSays if the frequency property is &gt;= the variable, minFreq, value, then use steelblue otherwise use lightgray\n\n\n\n\nFor-Loop\nlet largestNumber = 0; // Declare a variable for the largest number\n\nfor(let i = 0; i &lt; myValues.length - 1; i++) {Â  Â  // Loop through all the values in my array\nÂ  Â  if(myValues[i] &gt; largestNumber) {Â  Â  Â  Â  Â  Â  Â  // Check if the value in the array is larger that the largestNumber\nÂ  Â  Â  largestNumber = myValues[i]Â  Â  Â  Â  Â  Â  Â  Â  Â  // If so, assign the value as the new largest number\nÂ  Â  }\n}\n\nreturn largestNumber\n\nThe first statement sets a variable (let i = 0)\nThe second statement provides a condition for when the loop will run (whenever i &lt; myValues.length - 1)\nThe third statement says what to do each time the code block is executed (i++, which means to add 1 to i)\n\nWhile-Loop\nlet largestNumber = 0;Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  // Create a variable for the largest number\nlet i = 0;\nwhile(i &lt; myValues.length - 1) {\nÂ  Â  if(myValues[i] &gt; largestNumber) {Â  Â  Â  Â  // Check if the value in the array is larger that the largestNumber\nÂ  Â  Â  largestNumber = myValues[i]Â  Â  Â  Â  Â  Â  // If so, assign the value as the new largest number\nÂ  Â  }\nÂ  Â  i++;\n}\nreturn largestNumber"
  },
  {
    "objectID": "qmd/js.html#sec-js-cleaning",
    "href": "qmd/js.html#sec-js-cleaning",
    "title": "JS",
    "section": "Cleaning",
    "text": "Cleaning\n\nMisc\n\nNotes from: Horst article\n\nFilter objects: myData.filter(d =&gt; d.city == 'Denver')\nSelect properties: myNewArray = salesData.map(d =&gt; ({ date: d.date, product: d.product, totalRevenue: d.totalRevenue }))\n\nIn some contexts, this, d =&gt; d[\"mileage (mpg)\"] , is also used to select columns\n\nArrange objects: salesData.sort((a, b) =&gt; a.totalRevenue - b.totalRevenue)\n\nReorders salesData by totalRevenue (low to high)\n\nMutate properties: salesData.map(d =&gt; ({...d, discountedPrice: 0.9 * d.unitPrice }))\n\nAdds a new column to salesData with a discountedPrice, which takes 10% off each unitPrice.\n\nGroup_By: d3.rollup(salesData, v =&gt; d3.sum(v, d =&gt; d.totalRevenue), d =&gt; d.region)\n\nReturn the sum of totalRevenue for each region in salesData.\nrollup might actually be a summarize and the group_by is handled in the syntax\n\nRename: salesData.map(d =&gt; ({...d, saleDate: d.date }))\n\nAdds a new column called saleDate by storing a version of the date with new name saleDate and keeping all other columns.\n\nSubset value: salesData.map(d =&gt; d.description)[3]\n\nAccess the fourth value from the description property in salesData\n\nUnite:\nsalesData.map(d =&gt; ({...d, fullDescription: `${d.product} ${d.description}`}))\n\nUnite the product and description columns into a single column called fullDescription, using a comma as a separator.\n\nLeft Join: *using {{{arquero}}} tables* salesData.join_left(productDetails, ['product', 'product_id'])\n\nJoin information from a productDetails table to salesData. Join on product in salesData and product_id in productDetails."
  },
  {
    "objectID": "qmd/js.html#sec-js-stats",
    "href": "qmd/js.html#sec-js-stats",
    "title": "JS",
    "section": "Stats",
    "text": "Stats\n\nMisc\n\nNotes from: Horst article\n\nIn examples, waterUsage is the array; waterGallons is the property.\n\n\nMean: d3.mean(waterUsage.map(d =&gt; d.waterGallons))\n\nReturns a Value\n\nStd.Dev: d3.deviation(waterUsage.map(d =&gt; d.waterGallons))\nMedian: d3.median(waterUsage.map(d =&gt; d.waterGallons))\nMin/Max: d3.min(waterUsage.map(d =&gt; d.waterGallons))\nTotal Observations (i.e.Â nrow ): waterUsage.length\nBy Group:\n\npropertyId is the discrete, grouping variable\nMean: waterMeans = d3.rollup(waterUsage, v =&gt; d3.mean(v, d =&gt; d.waterGallons), d =&gt; d.propertyId)\n// Returns a map object\nwaterMeans\n{\nÂ  \"A001\" =&gt; 39.53389830508475\nÂ  \"B002\" =&gt; 53.57627118644068\nÂ  \"C003\" =&gt; 27.45762711864407\nÂ  \"D004\" =&gt; 80.1864406779661\n}\n\n// View in a JS Table\n// ** Must be in a separate cell **\nInputs.table(waterMeans.map(([propertyId, meanWaterGallons]) =&gt; ({propertyId, meanWaterGallons})))\nCount: d3.rollup(waterUsage, v =&gt; d3.count(v, d =&gt; d.waterGallons), d =&gt; d.propertyId)\n\nConditional Counts: waterUsage.filter(d =&gt; d.waterGallons &gt; 90 && d.propertyId == \"B002\").length\n\nApplies two conditionals and counts the observations\n\nRanks\nwaterUsage.map((d, i) =&gt; ({...d, rank: d3.rank(waterUsage.map(d =&gt; d.waterGallons), d3.descending)[i] + 1}))\n\n1 is added so that ranks start at 1 instead of 0\n\nPercentiles: d3.quantile(waterUsage.map(d =&gt; d.waterGallons), 0.9) (e.g.Â 90th)"
  },
  {
    "objectID": "qmd/js.html#sec-js-obs",
    "href": "qmd/js.html#sec-js-obs",
    "title": "JS",
    "section": "Observable",
    "text": "Observable\n\nA collaborative, online notebook platform that comes with libraries loaded to make it fairly straightforward to dive into ad hoc data analysis or produce complete reports.\nIn Observable, if youâ€™re running a JavaScript cell that contains more than just a simple variable assignment (like myVariable = 'Hello World' ), you need to run a code block (i.e.Â bracket lines of code in curly braces, {}).\nYou can open your notebook in Safe Mode and edit your work without running it.\n\nGood for debugging (e.g.Â infinite while-loops)"
  },
  {
    "objectID": "qmd/js.html#sec-js-def",
    "href": "qmd/js.html#sec-js-def",
    "title": "JS",
    "section": "Definitions",
    "text": "Definitions\n\nJSON vs R List\n{Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  list(\nÂ  Â  boolean: true,Â  Â  Â  Â  Â  Â  Â  Â  boolean = TRUE,\nÂ  Â  string: \"hello\",Â  Â  Â  Â  Â  Â  Â  string = \"hello\",\nÂ  Â  vector: [1,2,3]Â  Â  Â  Â  Â  Â  Â  Â  vector = c(1,2,3)\n}Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  )\n\n// AccessÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Access\njson.vectorÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  list$vector\nDependencies\nHTMLÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  R (shiny)\n&lt;head&gt;Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  tags$head(\nÂ  Â  &lt;!-- JavaScript --&gt;Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  tags$script(src = \"path/to/file.js\")\nÂ  Â  &lt;script src=\"path/to/file.js\"&gt;&lt;/script&gt;Â  Â  Â  Â  Â  Â  Â  tags$link(\nÂ  Â  &lt;!-- CSS --&gt;Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  rel = \"stylesheet\",\nÂ  Â  &lt;link rel=\"stylesheet\" href=\"path/to/file.css&gt;Â  Â  Â  Â  href = \"path/to/file.css\n&lt;/head&gt;Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ))\nd is each row and =&gt; is function\n(d) =&gt; d.year === 2020\n\nSays for each row in your data, the year column must equal 2020\n\nCallback Function - A function that is passed to another function as a parameter. In other words, a function â€œcalls backâ€ to previously defined function.\nfunction print(callback) {Â \nÂ  Â  callback();\n}\n\ncallback is the callback function and is a parameter of the print function\nCallbacks make sure that a function is not going to run before a task is completed but will run right after the task has completed.\nExample:\n// \"Click here\" button in a web app\n&lt;button id=\"callback-btn\"&gt;Click here&lt;/button&gt;\ndocument.queryselector(\"#callback-btn\")\nÂ  Â  .addEventListener(\"click\", function() {Â  Â \nÂ  Â  Â  console.log(\"User has clicked on the button!\");\n});\n\nFirst, button selected by its id, and then we add an event listener with the addEventListener method. It takes 2 parameters. The first one is its type, click, and the second parameter is a callback function, which logs the message when the button is clicked.\n\n\nAnonymous Function - Same as a callback but unnamed. Itâ€™s aÂ  function that is defined within another function.\nsetTimeout(function() {Â \nÂ  Â  console.log(\"This message is shown after 3 seconds\");\n}, 3000);\n\n// if the function were named\nconst message = function() {Â \nÂ  Â  console.log(\"This message is shown after 3 seconds\");\n}\n\n// as an arrow function\nsetTimeout(() =&gt; {Â \nÂ  Â  console.log(\"This message is shown after 3 seconds\");\n}, 3000);\n\nThe function used as a parameter has no name. console.log is the contents of the function."
  },
  {
    "objectID": "qmd/js.html#sec-js-nfcd",
    "href": "qmd/js.html#sec-js-nfcd",
    "title": "JS",
    "section": "Notes From Covidcast Dashboard",
    "text": "Notes From Covidcast Dashboard\n\nNotes from\n\nCovidcast Dashboard: reactable + sparkline tooltip (link)\n\ndiv = vertical label or container , span = horizontal\nFormat: type, styling, value\n2 divs would result in a 2 element vertical label while 2 spans would be a 2 element horizontal label\nExample: A div container holding 2 spans which creates a â€œdate valueâ€ horizontal label\n\"function (_ref) {\nvar datum = _ref.datum;\nreturn React.createElement(\nÂ  'div',\nÂ  null,\nÂ  datum.date && React.createElement(\nÂ  Â  Â  'span',\nÂ  Â  Â  {style: {\nÂ  Â  Â  Â  Â  backgroundColor: 'black', color: 'white',\nÂ  Â  Â  Â  Â  padding: '3px', margin: '0px 4px 0px 0px', textAlign: 'center'\nÂ  Â  Â  Â  }},\nÂ  Â  Â  datum.date[0].split('-').slice(1).join('/')\nÂ  ),\nÂ  React.createElement(\nÂ  Â  Â  'span',\nÂ  Â  Â  {style: {\nÂ  Â  Â  Â  fontWeight: 'bold', fontSize: '1.1em',\nÂ  Â  Â  Â  padding: '2px'\nÂ  Â  Â  }},\nÂ  Â  Â  datum.y ? datum.y.toLocaleString(undefined, {maximumFractionDigits: 0}) : '--'\nÂ  )\nÂ  );\n}\"\n\nCSS: margin, padding\n\nFormat is top, right, bottom, left (ordered like a clock)\nRequires units like â€œpxâ€\nNo commas separate the values\n{margin: '0px 4px', padding: '0px 0px 0px 4px'}\n\nMaybe for 0s it doesnâ€™t matter\nSee bkmk in css/definitions for explanations behind specifications with less than 4 numbers\n\ne.g.Â 2 is â€˜top/bottom left/rightâ€™\n\n\n\nString manipulation\ndatum.endDate[0].split('-').slice(1).join('/')\n\nTreats variable as a string object\nLooks in data arg, finds endDate variable\nIts a list variable so requires the [0] (0 part an index?)\nDate format is ymd, so splits value by â€œ-â€ separator, removes 1st value (year), joins the rest of the values (month, day) with â€œ/â€\n\nIf slice(2), removes first 2 values (left to right)\n\n\nConditional\nlabelPosition = htmlwidgets::JS(\"(d, i) =&gt; (i === 0 || i === 1 ? 'right' : 'left')\")\n\nSays that if index of data value, d, is 0 or 1 then label should be positioned on the right of the point, else place the label on the left of the point"
  },
  {
    "objectID": "qmd/json.html#sec-json-misc",
    "href": "qmd/json.html#sec-json-misc",
    "title": "JSON",
    "section": "Misc",
    "text": "Misc\n\nPackages\n\n{yyjsonr} - a fast JSON parser/serializer, which converts R data to/from JSON and NDJSON. It is around 2x to 10x faster than jsonlite at both reading and writing JSON.\n\nAlso see\n\nBig Data &gt;&gt; Larger than Memory\nSQL &gt;&gt; Processing Expressions &gt;&gt; Nested Data\nDatabases &gt;&gt; DuckDB &gt;&gt; Misc\n\nhrbmstr recommends trying duckdb before using the cli tools in â€œBig Dataâ€"
  },
  {
    "objectID": "qmd/json.html#sec-json-jsonlite",
    "href": "qmd/json.html#sec-json-jsonlite",
    "title": "JSON",
    "section": "{jsonlite}",
    "text": "{jsonlite}\n\nRead"
  },
  {
    "objectID": "qmd/json.html#sec-json-py",
    "href": "qmd/json.html#sec-json-py",
    "title": "JSON",
    "section": "Python",
    "text": "Python\n\nExample: Parse Nested JSON into a dataframe (article)\n\nRaw JSON\n\n\nâ€œentryâ€ has the data we want\nâ€œâ€¦â€ at the end indicates there are multiple objectss inside the element, â€œentryâ€\n\nProbably other root elements other than â€œfeedâ€ as well\n\n\nRead a json file from a URL using {{requests}} and convert to list\n\nimport requests\n\nurl = \"https://itunes.apple.com/gb/rss/customerreviews/id=1500780518/sortBy=mostRecent/json\"\n\nr = requests.get(url)\n\ndata = r.json()\nentries = data[\"feed\"][\"entry\"]\n\nIt looks like the list conversion also ordered the elements alphabetically\nThe output list is subsetted by the root element â€œfeedâ€ and the child element â€œentryâ€\n\nGet a feel for the final structure you want by hardcoding elements into a df\nparsed_data = defaultdict(list)\n\nfor entry in entries:\n    parsed_data[\"author_uri\"].append(entry[\"author\"][\"uri\"][\"label\"])\n    parsed_data[\"author_name\"].append(entry[\"author\"][\"name\"][\"label\"])\n    parsed_data[\"author_label\"].append(entry[\"author\"][\"label\"])\n    parsed_data[\"content_label\"].append(entry[\"content\"][\"label\"])\n    parsed_data[\"content_attributes_type\"].append(entry[\"content\"][\"attributes\"][\"type\"])\n    ... \nGeneralize extracting the properties of each object in â€œentryâ€ with a nested loop\nparsed_data = defaultdict(list)\n\nfor entry in entries:\n    for key, val in entry.items():\n        for subkey, subval in val.items():\n            if not isinstance(subval, dict):\n                parsed_data[f\"{key}_{subkey}\"].append(subval)\n            else:\n                for att_key, att_val in subval.items():\n                    parsed_data[f\"{key}_{subkey}_{att_key}\"].append(att_val)\n\ndefaultdict creates a key from a list element (e.g.Â â€œauthorâ€) and groups the properties into a list of values where the value may also be a dict.\n\nSee Python, General &gt;&gt; Types &gt;&gt; Dictionaries\n\nFor each item in â€œentryâ€, it looks at the first key-value pair knowing that value is always a dictionary (object in JSON)\nThen handles two different cases\n\nFirst Case: The value dictionary is flat and does not contain another dictionary, only key-value pairs.\n\nCombine the outer key with the inner key to a column name and take the value as column value for each pair.\n\nSecond Case: Dictionary contains a key-value pair where the value is again a dictionary.\n\nAssumes at most two levels of nested dictionaries\nIterates over the key-value pairs of the inner dictionary and again combines the outer key and the most inner key to a column name and take the inner value as column value.\n\n\n\nRecursive function that handles json elements with deeper structures\n\ndef recursive_parser(entry: dict, data_dict: dict, col_name: str = \"\") -&gt; dict:\n    \"\"\"Recursive parser for a list of nested JSON objects\n\n    Args:\n        entry (dict): A dictionary representing a single entry (row) of the final data frame.\n        data_dict (dict): Accumulator holding the current parsed data.\n        col_name (str): Accumulator holding the current column name. Defaults to empty string.\n    \"\"\"\n    for key, val in entry.items():\n        extended_col_name = f\"{col_name}_{key}\" if col_name else key\n        if isinstance(val, dict):\n            recursive_parser(entry[key], data_dict, extended_col_name)\n        else:\n            data_dict[extended_col_name].append(val)\n\nparsed_data = defaultdict(list)\n\nfor entry in entries:\n    recursive_parser(entry, parsed_data, \"\")\n\ndf = pd.DataFrame(parsed_data)\n\nNotice the check for a deeper structure with isinstance. If there is one, then the function is called again.\nFunction outputs a dict which is coerced into dataframe\nTo get rid of â€œlabelâ€ in column names: df.columns = [col if not \"label\" in col else \"_\".join(col.split(\"_\")[:-1]) for col in df.columns]\nobject types can be cast into more efficient types: df[\"im:rating\"] = df[\"im:rating\"].astype(int)"
  },
  {
    "objectID": "qmd/kubernetes.html#sec-kube-misc",
    "href": "qmd/kubernetes.html#sec-kube-misc",
    "title": "Kubernetes",
    "section": "Misc",
    "text": "Misc\n\nDocs\n\nComponents\n\nDatascience project components\n\nYour code (python/R/julia/matlab)\nA Dockerfile that packages up your code\nA configuration file (deployment.yaml, job.yaml) (sometimes someone else will do this for you)\n\nProcess\n\nBuild your machine learning code.\nPackage it up into a docker container and push that image into a registry.\nUsing the configuration file, you tell the kubernetes cluster where to find that image and how to build your service out of it (â€œmake two copies and give them lots of ramâ€).\n\nKubernetes has new instructions now, and makes sure the cluster state matches those instructions.\n\n\nData Scientist responisibilities\n\nMake sure your container actually runs, test that extensively!\n\nWrite R tests that check if you can handle the expected inputs.\nCheck that you are logging errors when they occur.\nCheck how you handle unexpected inputs (a person with an age of 200, a car with no weight, etc)\nTest the container, pass expected input to the container, pass unexpected input, test if the container fails and protests loudly when the required environmental variables are not found.\n\nArrange what your API should look like: when you use {plumber}: what endpoint will be called, what port should be reached for, and what will the data look like? Make sure you write some tests for that! When you use {shiny}: what port does it live on, what are the memory and CPU requirements for your application. In all cases: what secrets must be supplied to the container?\nArrange where logs should go and how they should look. My favorite R logging package is {logger} and it can do many many forms of logging. If something goes wrong you want the logs to tell you what happened and where you should investigate.\nUse {renv} to install specific versions of r-packages, and to record that in a lockfile.\n\nWrite process script for each of these steps\n\nRun all the tests\nBuild the docker image\nCheck the image\nPush the image to the registry\nDeploy the new version of your code to the production cluster"
  },
  {
    "objectID": "qmd/kubernetes.html#sec-kube-terms",
    "href": "qmd/kubernetes.html#sec-kube-terms",
    "title": "Kubernetes",
    "section": "Terms",
    "text": "Terms\n\netcd - Consistent and highly-available key value store used as Kubernetesâ€™ backing store for all cluster data. Docs\n\nIf your Kubernetes cluster uses etcd as its backing store, make sure you have a back up plan for those data.\n\nTime-to-Live (TTL) - Kubernetes mechanism for shutting down pods and gc - â€œKubernetes v1.23 [stable] TTL-after-finished controller provides a TTL (time to live) mechanism to limit the lifetime of resource objects that have finished execution. TTL controller only handles Jobs.â€ Docs"
  },
  {
    "objectID": "qmd/latex.html",
    "href": "qmd/latex.html",
    "title": "LaTeX",
    "section": "",
    "text": "R = \n\\begin{pmatrix}\n1 & r_0r & r_0r^2 & \\cdots & r_0r^{T-1} \\\\\nr_0r & 1 & r_0r & \\cdots & r_0r^{T-2} \\\\\nr_0r^2 & r_0r & 1 & \\cdots & r_0r^{T-3} \\\\ \n\\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\ \nr_0r^{T-1} & r_0r^{T-2} & r_0r^{T-3} & \\cdots & 1 \n\\end{pmatrix}\n\\[\nR =\n\\begin{pmatrix}\n1 & r_0r & r_0r^2 & \\cdots & r_0r^{T-1} \\\\\nr_0r & 1 & r_0r & \\cdots & r_0r^{T-2} \\\\\nr_0r^2 & r_0r & 1 & \\cdots & r_0r^{T-3} \\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\nr_0r^{T-1} & r_0r^{T-2} & r_0r^{T-3} & \\cdots & 1\n\\end{pmatrix}\n\\]\n\n\n\n\ns(\\\\boldsymbol{\\\\hat{\\\\theta}}, y) = \n\\begin{pmatrix} \ns(\\hat{\\theta}, y_1)_1 & s(\\hat{\\theta}, y_1)_2 & \\cdots & s(\\hat{\\theta}, y_1)_k \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\ns(\\hat{\\theta}, y_n)_1 & s(\\hat{\\theta}, y_n)_2 & \\cdots & s(\\hat{\\theta}, y_n)_k \\\\\n\\end{pmatrix}\n\\[\ns(\\boldsymbol{\\hat{\\theta}}, y) =\n\\begin{pmatrix}\ns(\\hat{\\theta}, y_1)_1 & s(\\hat{\\theta}, y_1)_2 & \\cdots & s(\\hat{\\theta}, y_1)_k \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\ns(\\hat{\\theta}, y_n)_1 & s(\\hat{\\theta}, y_n)_2 & \\cdots & s(\\hat{\\theta}, y_n)_k \\\\\n\\end{pmatrix}\n\\]\n\n\n\n\n[1 \\; x_{1t} \\; x_{2t} \\; \\cdots \\; x_{mt}] \n\\cdot \\left( \\begin{array}{ccc} \n\\hat{\\beta}_{01} & \\cdots & \\hat{\\beta}_{0k} \\\\ \n\\vdots & \\ddots & \\vdots \\\\ \n\\hat{\\beta}_{m1} & \\cdots & \\hat{\\beta}_{mk} \\end{array} \\right) \n= \n[\\hat{y}_{t1} \\; \\hat{y}_{t2} \\; \\cdots \\; \\hat{y}_{tk}]\n\\[\n[1 \\; x_{1t} \\; x_{2t} \\; \\cdots \\; x_{mt}]\n\\cdot \\left( \\begin{array}{ccc}\n\\hat{\\beta}_{01} & \\cdots & \\hat{\\beta}_{0k} \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\hat{\\beta}_{m1} & \\cdots & \\hat{\\beta}_{mk} \\end{array} \\right)\n=\n[\\hat{y}_{t1} \\; \\hat{y}_{t2} \\; \\cdots \\; \\hat{y}_{tk}]\n\\]\n\n\n\n\nICC_{tt} = ICC_{tt'} = \\frac {\\sigma_b^2} {\\sigma_b^2 + \\sigma_e^2} r_{tt'}\n\\[\nICC_{tt} = ICC_{tt'} = \\frac {\\sigma_b^2} {\\sigma_b^2 + \\sigma_e^2} r_{tt'}\n\\]\n\n\n\n\ny_{ict} = \\mu + \\beta_0t + \\beta_1X_{ct} + b_{ct} + e_{ict}\n\\[\ny_{ict} = \\mu + \\beta_0t + \\beta_1X_{ct} + b_{ct} + e_{ict}\n\\]\n\n\n\n\n\\hat{\\theta} = \\arg\\max_{\\theta \\in \\Theta} \\sum\\_{i=1}^n w_i^{\\text{forest}} \\\\cdot l(\\theta; y_i)\n\\[\n\\hat{\\theta} = \\arg\\max_{\\theta \\in \\Theta} \\sum_{i=1}^n w_i^{\\text{forest}} \\cdot l(\\theta; y_i)\n\\]\n\n\n\n\n\\begin{align*}\nH(Y|X_1,\\cdots, X_k) = \\sum_{d = 0,1} \\sum_{{i_1}=1}^c \\cdots \\sum_{{i_k}=1}^c \\\\\nP(y_d | x_{i_1}, \\cdots, x_{i_k}) \\log P(y_d | x_{i_1}, \\cdots, x_{i_k})\n\\end{align*}\n\\[\n\\begin{align*}\nH(Y|X_1,\\cdots, X_k) = \\sum_{d = 0,1} \\sum_{{i_1}=1}^c \\cdots \\sum_{{i_k}=1}^c \\\\\nP(y_d | x_{i_1}, \\cdots, x_{i_k}) \\log P(y_d | x_{i_1}, \\cdots, x_{i_k})\n\\end{align*}\n\\]\n\n\n\n\n\\begin{align*}\nx^2 + y^2 &= 1 \\\\\ny &= \\sqrt{1 - x^2}\n\\end{align*}\n\nWith the â€œ&â€ symbols, the 2nd line stays lined up with the end of the first line\n\n\\[\n\\begin{align*}\nx^2 + y^2 &= 1 \\\\\ny &= \\sqrt{1 - x^2}\n\\end{align*}\n\\]\n\n\n\n\n\\begin{tabular} {lll}\nTransformation & Function & Elasticity \\\\\n\\hline\nLevel Level & Y\\;=\\;a+bX & \\epsilon = b \\cdot \\frac {X} {Y} \\\\  \nLog Level & \\log Y = a+bx & \\epsilon = b \\cdot X \\\\\nLevel-Log & Y = a + b \\cdot \\log X & \\epsilon = \\frac {b} {Y} \\\\\nLog-Log & \\log Y = a + b \\cdot \\log X & \\epsilon = b \\\\\n\\hline\n\\end{tabular}\n\nThis is â€œRaw LaTeXâ€ and will be ignored when the output is HTML. (See Docs)\nIn the last column, for some unknown reason, using \\frac without another â€œ\\â€ symbol before it throws an error. Adding \\epsilon = to the expression made it work fine.\n\n\n\n\n\n\nc_{max}(t,\\mu, \\sigma) = \\max \\limits_{k = 1,..., dim(T)} \\left | \\frac {(t-\\mu)_k} {\\sqrt {\\Sigma_{kk}}} \\right |\n\nLarge absolute pipes require \\left, \\right\nThe text underneath â€œmaxâ€ requires \\limits\n\n\\[\nc_{max}(t,\\mu, \\sigma) = \\max \\limits_{k = 1,..., dim(T)} \\left | \\frac {(t-\\mu)_k} {\\sqrt {\\Sigma_{kk}}} \\right |\n\\]\n\n\n\n\n\\begin{align*}\n\\mbox{net present value (npv)} = &\\sum \\limits_{i = 0}^m benefit_i*(1-discount)^i -\\\\\n&\\sum \\limits_{i=0}^m cost_i*(1-discount)^i\n\\end{align*}\n\\[\n\\begin{align*}\n\\mbox{net present value (npv)} = &\\sum \\limits_{i = 0}^m benefit_i*(1-discount)^i -\\\\\n&\\sum \\limits_{i=0}^m cost_i*(1-discount)^i\n\\end{align*}\n\\]\n\n\n\n\n\\epsilon_Z \\stackrel{\\text{iid}}{\\sim} \\mathcal{N}(0, 0.1)\n\\[\n\\epsilon_Z \\stackrel{\\text{iid}}{\\sim} \\mathcal{N}(0, 0.1)\n\\]\n\n\n\n\n\\mbox{posterior} = \\frac {\\mbox{Prob of observed variables} \\;\\times\\; \\mbox{Prior}} {\\mbox{Normalizing constant}}\n\\[\n\\mbox{posterior} = \\frac {\\mbox{Prob of observed variables} \\;\\times\\; \\mbox{Prior}} {\\mbox{Normalizing constant}}\n\\]\n\n\n\n\n\\lim\\limits_{x \\to +\\infty} \\sup \\frac{\\bar{F}(x)}{\\bar{F}_{\\exp}(x)} = \\frac{\\bar{F}(x)}{e^{-\\lambda x}}, \\;\\; \\forall \\lambda &gt; 0\n\\[\n\\lim\\limits_{x \\to +\\infty} \\sup \\frac{\\bar{F}(x)}{\\bar{F}_{\\exp}(x)} = \\frac{\\bar{F}(x)}{e^{-\\lambda x}}, \\;\\; \\forall \\lambda &gt; 0\n\\]\n\n\n\n\n\\text{vec} = \\left( \\begin{array}{cc} \\text{rand var1} \\\\\n \\text {rand var2} \\end{array} \\right)\n\\[\n\\text{vec} = \\left( \\begin{array}{cc} \\text{rand var1} \\\\\n\\text {rand var2} \\end{array} \\right)\n\\]\n\n\n\n\nSBD(x,y) = 1 - \\frac {\\max (\\text{NCCc}(x,y))} {\\left\\lVert x \\right\\rVert_2 \\; \\left\\lVert y \\right\\rVert_2}\n\\[\nSBD(x,y) = 1 - \\frac {\\max (\\text{NCCc}(x,y))} {\\left\\lVert x \\right\\rVert_2 \\; \\left\\lVert y \\right\\rVert_2}\n\\]\n\n\n\n\nD \\not\\!\\perp\\!\\!\\!\\perp A \\\\\nD \\!\\perp\\!\\!\\!\\perp A \\\\\nY \\!\\perp\\!\\!\\!\\perp X|Z\n\nThis one isnâ€™t rendering correctly in HTML\n\n\\[\nD \\not\\!\\perp\\!\\!\\!\\perp A\\\\\nD \\!\\perp\\!\\!\\!\\perp A\\\\\nY \\!\\perp\\!\\!\\!\\perp X|Z\n\\]\n\n\n\n\n||y-f||^2 + \\lambda \\int \\left(\\frac {\\partial^2 f(\\text{log[baseline profit]})}{\\partial \\; \\text{log[baseline profit]}^2}\\right)^2 \\partial x\n\\[\n||y-f||^2 + \\lambda \\int \\left(\\frac {\\partial^2 f(\\text{log[baseline profit]})}{\\partial \\; \\text{log[baseline profit]}^2}\\right)^2 \\partial x\n\\]\n\n\n\n\nE_{iz_{nm}} = \n\\left\\{ \\begin{array}{lcl}\n-\\beta_z z_{zm}P_{nm} \\left(1+ \\frac{1-\\lambda_k}{\\lambda_k} \\frac {1}{P_{nB_k}} \\right) & \\mbox{if} \\; m \\in \\beta_k \\\\\n-\\beta_z z_{zm}P_{nm} & \\mbox{if} \\; m \\in \\beta_k \n\\end{array}\\right.\n\nThere is no dash between {array} and {lcl}. Itâ€™s just two curly brackets touching each other\n\nIf you want the expressions on the left side to right-align use {rcl} â€” for center-align, leave it blank I think\n\nDo not forget that period at end â€” to the right of \\right\n\n\\[\nE_{iz_{nm}} =\n\\left\\{ \\begin{array}{lcl}\n-\\beta_z z_{zm}P_{nm} \\left(1+ \\frac{1-\\lambda_k}{\\lambda_k} \\frac {1}{P_{nB_k}} \\right) & \\mbox{if} \\; m \\in \\beta_k \\\\\n-\\beta_z z_{zm}P_{nm} & \\mbox{if} \\; m \\notin \\beta_k\n\\end{array}\\right.\n\\]\n\n\n\n\n\\begin{align}\n&P_{ni} = \\int L_{ni}(\\beta) \\; f(\\beta\\;|\\;\\boldsymbol{\\theta}) d\\beta\\\\\n&\\mbox{where} \\; L_{ni}(\\beta) = \\frac {e^{V_{ni}(\\beta)}}{\\sum_{j=1}^J e^{V_{ni}(\\beta)}} \n\\end{align}\n\nAlignment occurs where the & symbols are positioned\n\\boldsymbol is used bold the vector \\(\\theta\\)\n\n\\[\n\\begin{align}\n&P_{ni} = \\int L_{ni}(\\beta) \\; f(\\beta\\;|\\;\\boldsymbol{\\theta}) \\; d\\beta\\\\\n&\\mbox{where} \\; L_{ni}(\\beta) = \\frac {e^{V_{ni}(\\beta)}}{\\sum_{j=1}^J e^{V_{ni}(\\beta)}}\n\\end{align}\n\\]\n\n\n\n\n\\pi_i = \\mbox{Pr}(i \\in S) = \\sum \\limits_{i \\in s \\in S} \\mbox{Pr}(S = s) = \\frac {\\binom{N-1}{n-1}}{\\binom{N}{n}} = \\frac {n}{N}\n\n\\binom used for binomial coefficient/combination notation\n\n\\[\n\\pi_i = \\mbox{Pr}(i \\in S) = \\sum \\limits_{i \\in s \\in S} \\mbox{Pr}(S = s) = \\frac {\\binom{N-1}{n-1}}{\\binom{N}{n}} = \\frac {n}{N}\n\\]\n\n\n\n\n\\begin{aligned}\n&R_{i,j} = \\frac{S_i + S_j}{M_{i,j}}\\\\\n&\\begin{aligned}\n\\text{where} \\;\\; &S_i = \\left(\\frac{1}{T_i} \\sum_{j=1}^{T_i} \\lVert X_j - A_i \\rVert_{p}^q \\right)^{\\frac{1}{q}} \\;\\; \\text{and} \\\\\n&M_{i,j} = \\lVert A_i - A_j \\rVert_p = \\left(\\sum_{k=1}^n |a_{k,i} - a_{k,j}|^p\\right)^{\\frac{1}{p}}\n\\end{aligned}\n\\end{aligned}\n\nThe key for this nested align is the â€œ&â€ before the second \\begin{aligned}. Otherwise â€œwhereâ€ and \\(R_{i,j}\\) wonâ€™t be aligned flushly on the left side.\n\n\\[\n\\begin{aligned}\n&R_{i,j} = \\frac{S_i + S_j}{M_{i,j}}\\\\\n&\\begin{aligned}\n\\text{where} \\;\\; &S_i = \\left(\\frac{1}{T_i} \\sum_{j=1}^{T_i} \\lVert X_j - A_i \\rVert_{p}^q \\right)^{\\frac{1}{q}} \\;\\; \\text{and} \\\\\n&M_{i,j} = \\lVert A_i - A_j \\rVert_p = \\left(\\sum_{k=1}^n |a_{k,i} - a_{k,j}|^p\\right)^{\\frac{1}{p}}\n\\end{aligned}\n\\end{aligned}\n\\]\n\n\n\n\n\\begin{aligned}\n&Y_{ij} = [\\alpha_0 + \\beta_0 \\text{large}_{ij}] + [u_i + v_i \\text{large}_{ij} + \\epsilon_{ij}] \\\\\n&\\begin{aligned}\n\\text{where}\\quad \\epsilon &\\sim \\mathcal{N}(0, \\sigma^2) \\: \\text{and}\\\\\n\\left[ \\begin{array}{cc} u_i \\\\ v_i \\end{array} \\right] &\\sim \\mathcal{N} \\left(\\left[\\begin{array}{cc} 0\\\\0 \\end{array}\\right], \\left[\\begin{array}{cc} \\sigma^2_u\\\\\\rho\\sigma_u \\sigma_v \\quad \\sigma^2_v \\end{array}\\right]\\right)\n\\end{aligned}\n\\end{aligned}\n\\[\n\\begin{aligned}\n&Y_{ij} = [\\alpha_0 + \\beta_0 \\text{large}_{ij}] + [u_i + v_i \\text{large}_{ij} + \\epsilon_{ij}] \\\\\n&\\begin{aligned}\n\\text{where}\\quad \\epsilon &\\sim \\mathcal{N}(0, \\sigma^2) \\: \\text{and}\\\\\n\\left[ \\begin{array}{cc} u_i \\\\ v_i \\end{array} \\right] &\\sim \\mathcal{N} \\left(\\left[\\begin{array}{cc} 0\\\\0 \\end{array}\\right], \\left[\\begin{array}{cc} \\sigma^2_u\\\\\\rho\\sigma_u \\sigma_v \\quad \\sigma^2_v \\end{array}\\right]\\right)\n\\end{aligned}\n\\end{aligned}\n\\]\n\n\n\n\n\\left[\\begin{array}{cc} u_i\\\\v_i\\\\w_i\\\\x_i\\\\y_i\\\\z_i \\end{array} \\right] \n\\sim \\mathcal{N} \\left(\n\\left[\\begin{array}{cc} 0\\\\0\\\\0\\\\0\\\\0\\\\0\\end{array} \\right],\n\\begin{bmatrix}\n\\sigma_u^2 \\\\\n\\sigma_{uv} & \\sigma_v^2 \\\\\n\\sigma_{uw} & \\sigma_{vw} & \\sigma^2_w \\\\\n\\sigma_{ux} & \\sigma_{vx} & \\sigma_{wx} & \\sigma_x^2 \\\\\n\\sigma_{uy} & \\sigma_{vy} & \\sigma_{wy} & \\sigma_{xy} & \\sigma_{y}^2 \\\\\n\\sigma_{uz} & \\sigma_{vz} & \\sigma_{wz} & \\sigma_{xz} & \\sigma_{yz} & \\sigma_z^2\n\\end{bmatrix}\n\\right)  \n\nbmatrix stands for bracket matrix. Previous matrices (above) used pmatrix which stands for parentheses matrix.\nOthers:\n\nBmatrix for curly braces matrix\nvmatrix for a matrix with â€œrectangular line boundaryâ€\nVmatrix for a matrix with double vertical bars\nmatrix for a matrix without brackets\n\n\n\\[\n\\left[\\begin{array}{cc} u_i\\\\v_i\\\\w_i\\\\x_i\\\\y_i\\\\z_i \\end{array} \\right]\n\\sim \\mathcal{N} \\left(\n\\left[\\begin{array}{cc} 0\\\\0\\\\0\\\\0\\\\0\\\\0\\end{array} \\right],\n\\begin{bmatrix}\n\\sigma_u^2 \\\\\n\\sigma_{uv} & \\sigma_v^2 \\\\\n\\sigma_{uw} & \\sigma_{vw} & \\sigma^2_w \\\\\n\\sigma_{ux} & \\sigma_{vx} & \\sigma_{wx} & \\sigma_x^2 \\\\\n\\sigma_{uy} & \\sigma_{vy} & \\sigma_{wy} & \\sigma_{xy} & \\sigma_{y}^2 \\\\\n\\sigma_{uz} & \\sigma_{vz} & \\sigma_{wz} & \\sigma_{xz} & \\sigma_{yz} & \\sigma_z^2\n\\end{bmatrix}\n\\right)  \n\\]\n\n\n\n\n\\begin{align}\n&\\ \\textbf{Registered provinces for INGO } i \\\\\n\\text{Count of provinces}\\ \\sim&\\ \\operatorname{Ordered\\,Beta}(\\mu_{i_j}, \\phi_y, k_{0_y}, k_{1_y}) \\\\[8pt]\n&\\textbf{Model of Outcome Average} \\\\\n\\mu_i = &\\\n  \\mathrlap{\\begin{aligned}[t]\n  &\\beta_0 + \\beta_1\\ \\text{Issue[Arts and Culture]} + \\beta_2\\ \\text{Issue[Education]}\\ + \\\\\n  &\\beta_3\\ \\text{Issue[Industry and Humanitarian]} + \\beta_4\\ \\text{Issue[Economy and Trade]}\n  \\end{aligned}}\\\\[8pt]\n&\\ \\textbf{Priors} \\\\\n\\beta_0\\ \\sim&\\ \\operatorname{Student\\,t}(\\nu = 3, \\mu = 0, \\sigma = 2.5) && \\text{Intercept} \\\\\n\\beta_{1..12}\\ \\sim&\\ \\mathcal{N}(0, 5) && \\text{Coefficients}\n\\end{align}\n\ntextbf is bold font text\n\\operatorname treats text as functions like \\max or \\sin in terms of spacing\n\\mathrlap + \\begin{aligned}allows you to wrap extra long equations\n\n\\[\n\\begin{align}\n&\\ \\textbf{Registered Provinces for INGO } i \\\\\n\\text{Count of Provinces}\\ \\sim&\\ \\operatorname{Ordered\\,Beta}(\\mu_{i_j}, \\phi_y, k_{0_y}, k_{1_y}) \\\\[8pt]\n&\\textbf{Model of Outcome Average} \\\\\n\\mu_i = &\\\n  \\mathrlap{\\begin{aligned}[t]\n  &\\beta_0 + \\beta_1\\ \\text{Issue[Arts and Culture]} + \\beta_2\\ \\text{Issue[Education]}\\ + \\\\\n  &\\beta_3\\ \\text{Issue[Industry and Humanitarian]} + \\beta_4\\ \\text{Issue[Economy and Trade]}\n  \\end{aligned}}\\\\[4pt]\n&\\ \\textbf{Priors} \\\\\n\\beta_0\\ \\sim&\\ \\operatorname{Student\\,t}(\\nu = 3, \\mu = 0, \\sigma = 2.5) && \\text{Intercept} \\\\\n\\beta_{1..12}\\ \\sim&\\ \\mathcal{N}(0, 5) && \\text{Coefficients}\n\\end{align}\n\\]"
  },
  {
    "objectID": "qmd/loss-functions.html#sec-lossfun-misc",
    "href": "qmd/loss-functions.html#sec-lossfun-misc",
    "title": "49Â  Loss Functions",
    "section": "49.1 Misc",
    "text": "49.1 Misc\n\nAlso see\n\nLogistics &gt;&gt; Decision Impact Metrics\n\nMake sure your Loss Function and Performance metrics correlate\n\nLoss function are typically minimized and metrics are typically maximized. So, when choosing a loss function, you should choose one that negatively correlates with your metric.\nSimulate some data, fit a model that using various loss functions, and score them with your performance metric. Then, choose the loss function that correlates with your metric the best.\nExample: Cross-Entropy vs.Â Accuracy\n\n\nImage is from a video, disregard the solid gridlines (actuallly video screens)\nX-Axis is Accuracy.\nShows how the low values of the loss function correlate well with the performance metric, Accuracy.\n\n\nMean Absolute Error (MAE) isnâ€™t typically used (instead of MSE) because it is not differentiable, thus gradient descent canâ€™t be used to find its minimum. Stochastic Gradient Descent is used to overcome this problem. (See Algorithms, ML &gt;&gt; SVM &gt;&gt; Regression)\n\nWhen you optimize a model using the MAE or MSE, your model will end up predicting the median or mean of the distribution, respectively\nAbsolute error is more robust to outliers\n\nMinimizes the median value whereas MSE minimizes the mean value\n\nStochastic Gradient Descent: the gradient of the loss is estimated each sample at a time and the model is updated along the way with a decreasing strength schedule (aka learning rate)\n\nAlso see Model building, sklearn &gt;&gt; Algorithms&gt;&gt; Stochaistic Gradient Descent (SGD)"
  },
  {
    "objectID": "qmd/loss-functions.html#sec-lossfun-01bin",
    "href": "qmd/loss-functions.html#sec-lossfun-01bin",
    "title": "49Â  Loss Functions",
    "section": "49.2 0/1 Loss (aka binary loss)",
    "text": "49.2 0/1 Loss (aka binary loss)\n\n\nItâ€™s basically the misclassification rate (additive inverse of accuracy) but as a loss function\nSum the 1s for misclassified labels and 0 for correct classifications. Then, take the average\n1 (or maybe 0 too) doesnâ€™t have to be the incorrect classification value. You could weight the incorrect classifications differently. If itâ€™s 2, then youâ€™d double the weight against an incorrect classifcation. Although it wouldnâ€™t be 0/1 loss anymore.\nOften used to evaluate classifiers, but is not useful in guiding optimization since it is non-differentiable and non-continuous."
  },
  {
    "objectID": "qmd/loss-functions.html#sec-lossfun-asym-hubloss",
    "href": "qmd/loss-functions.html#sec-lossfun-asym-hubloss",
    "title": "49Â  Loss Functions",
    "section": "49.3 Asymmetric Huber Loss",
    "text": "49.3 Asymmetric Huber Loss\n\n\nÎ¸ is the Î´ in Huber loss. There are two â€œdeltaâ€ parameters now to adjust the amount of asymmetry on either the left/lower half (Î¸L) or right/upper half (Î¸R)\nx is â€œy-f(x)â€ in Huber loss\nThe bottom condition, Î¸R(2x - Î¸R) is equivalent to the bottom condition in Huber loss\nThe top condition is just the conjugate(?) of the bottom condition\nThe â€œ1/2â€ isnâ€™t present in the middle condition for some reason as it is for the top condition in Huber loss.\n\nChecked Wiki and 1/2 is present in Huber loss there. Guess you could add it.\n\nPotentially a more programmer-friendly version"
  },
  {
    "objectID": "qmd/loss-functions.html#sec-lossfun-brier",
    "href": "qmd/loss-functions.html#sec-lossfun-brier",
    "title": "49Â  Loss Functions",
    "section": "49.4 Brier Loss",
    "text": "49.4 Brier Loss\n\n\nHow far your predictions lie from the true values\n\na mean square error in the probability space"
  },
  {
    "objectID": "qmd/loss-functions.html#sec-lossfun-focal",
    "href": "qmd/loss-functions.html#sec-lossfun-focal",
    "title": "49Â  Loss Functions",
    "section": "49.5 Focal Loss",
    "text": "49.5 Focal Loss\n\n\nWhere Î³ is the focusing parameter and Î± is a class balancing weight\n\nBoth are hyperparameters that can be tuned during CV\nÎ³ = 0 results in the cross-entropy formula\nAs Î³ gets larger, the more importance will be given to misclassified examples and less importance towards easy examples.\n\nÎ³ = 2 and Î± =.25 is a recommended starting points (paper)\n\nÎ±Â  can be set as the inverse class frequency\n\n\n\nGist with Focal loss function for keras\nFocuses on the examples that the model gets wrong rather than the ones that it can confidently predict, ensuring that predictions on hard examples improve over time rather than becoming overly confident with easy ones"
  },
  {
    "objectID": "qmd/loss-functions.html#geometric-trace-mean-squared-error-gtmse",
    "href": "qmd/loss-functions.html#geometric-trace-mean-squared-error-gtmse",
    "title": "49Â  Loss Functions",
    "section": "49.6 Geometric Trace Mean Squared Error (GTMSE)",
    "text": "49.6 Geometric Trace Mean Squared Error (GTMSE)\n\\[\nGTMSE = \\sum_{j=1}^h \\log \\frac{1}{T-j}\\sum_{t=1}^{T-j} e^2_{t+j|t}\n\\]\n\nMulti-step Estimators and Shrinkage Effect in Time Series Models\nForecasting loss function that imposes shrinkage on parameters similar to other estimators but does it more mildly because of the logarithms in the formula\n\nWhat the logarithms do is make variances of all forecast errors similar to each other. As a result, when used, GTMSE does not focus on the larger variances as other methods do but minimizes all of them simultaneously similarly.\n\nMaking models less stochastic and more inert. Therefore, performance depends on each specific situation and the available data and shouldnâ€™t be used universally.\nPackages: {smooth}, {greybox}"
  },
  {
    "objectID": "qmd/loss-functions.html#sec-lossfun-hubloss",
    "href": "qmd/loss-functions.html#sec-lossfun-hubloss",
    "title": "49Â  Loss Functions",
    "section": "49.7 Huber Loss",
    "text": "49.7 Huber Loss\n\n\n\nHuber loss (green, delta =1) and squared error loss (blue) as a function of y-f(x)\nA combination of squared loss and absolute loss\n\nFor residuals &gt; Î´, the absolute value of the residual is used which is based on the unbiased estimator of the median\n\nTherefore more robust to outliers\n\nThe loss is the square of the usual residual (y â€” f(x)) only when the absolute value of the residual is smaller than a fixed parameter, Î´.\n\nDelta is a tuneable parameter\nThe function is quadratic for small values of |y - f(x)|, and linear for large values\nCommonly used in deep learning where it helps to avoid the exploding gradient problem due to its insensitivity to large errors.\n{yardstick::huber_loss}"
  },
  {
    "objectID": "qmd/loss-functions.html#sec-lossfun-male",
    "href": "qmd/loss-functions.html#sec-lossfun-male",
    "title": "49Â  Loss Functions",
    "section": "49.8 Mean Absolute Log Error (MALE)",
    "text": "49.8 Mean Absolute Log Error (MALE)\n\n\narticle\nf is the forecast and y is the observed\nSimilar to RMSLE\nRequirement: data are strictly positive: they do not take zero or negative values\nPredicts the median of your distribution\nUnderestimates and overestimates are punished equally harshly\nTransform it back onto a relative scale by taking the exponential (exp(MALE)) which is the (geometric) mean relative error\nInterpetation:\n\nExample: exp(MALE) of 1.2 means you expect to be wrong by a factor of 1.2 in either direction, on average. (Explain this to your boss as a 20% average percentage error.)\n\nComparison with MAPE, sMAPE\n\n\nProvided your errors are sufficiently small (e.g.Â less than 10%), it probably doesnâ€™t make much difference which of the MALE, sMAPE or MAPE you use.\nMALE is is between the MAPE and sMAPE in sensitivity to outliers. For\n\nExample: compared to being wrong by +10x, being wrong by +100x gives you 1.2 times your original sMAPE, 2 times your original MALE, and 9 times your original MAPE."
  },
  {
    "objectID": "qmd/loss-functions.html#sec-lossfun-mase",
    "href": "qmd/loss-functions.html#sec-lossfun-mase",
    "title": "49Â  Loss Functions",
    "section": "49.9 Mean Absolute Scaled Error (MASE)",
    "text": "49.9 Mean Absolute Scaled Error (MASE)\n\nAlternative to using percentage errors (e.g.Â MAPE)Â  when comparing forecast accuracy across series with different units since it is independent of the scale of the data\nGreater than one: worse than the average naive forecast\nLessÂ  than one: better than the average naive forecast\nNon-seasonal ts scaled error\n\nSeasonal ts scaled error\n\nMASE = mean(|qj|)"
  },
  {
    "objectID": "qmd/loss-functions.html#sec-lossfun-masle",
    "href": "qmd/loss-functions.html#sec-lossfun-masle",
    "title": "49Â  Loss Functions",
    "section": "49.10 Mean Absolute Scaled Log Error (MASLE)",
    "text": "49.10 Mean Absolute Scaled Log Error (MASLE)\n\n\nThen, MASLE = Mean(SLE)\nThe area cut-off in the numerator should read â€œf T+hâ€ for hth-step ahead forecast\nf is the forecast and y is the observed\nScaled version of MALE\n\nItâ€™s been normalized by the error of a benchmark method (e.g.Â the naive or seasonal naive methods) to place the errors on the same scale"
  },
  {
    "objectID": "qmd/loss-functions.html#sec-lossfun-mbll",
    "href": "qmd/loss-functions.html#sec-lossfun-mbll",
    "title": "49Â  Loss Functions",
    "section": "49.11 Mean Balanced Log Loss (aka Balanced Cross-Entropy)",
    "text": "49.11 Mean Balanced Log Loss (aka Balanced Cross-Entropy)\n\n\nAdds a weight, Î², to the log loss formula to handle class imbalance\n\nCan tune during CV\n\nWhere p is the observed 0/1 and p^ is the predicted probability\nFor just balanced log-loss, remove the summation and 1/N"
  },
  {
    "objectID": "qmd/loss-functions.html#sec-lossfun-xentrop",
    "href": "qmd/loss-functions.html#sec-lossfun-xentrop",
    "title": "49Â  Loss Functions",
    "section": "49.12 Mean log loss (aka cross-entropy)",
    "text": "49.12 Mean log loss (aka cross-entropy)\n\n\nWhere p(yi) is the model output and yi is the observed 0/1\nFor just log-loss, remove the summation and 1/N\nIf the model predicts that an observation should be labeled 1 and assigns a high probability to that prediction, a high penalty will be incurred when the true label is 0. If the model had assigned a lower probability to that prediction, a lower penalty would have been incurred\n\n(might) be equivalent to K-L Divergence\n\nAbove equation is for a binary model but can be extended multiclassification models\n\nYouâ€™d replace the above expression and have a term for each category â€” each being yi * log(p(yi)) where each p(yi) is the probability for that category\n\n{yardstick::mn_log_loss}\nNotes\n\nHandles class imbalance poorly\n\nIn sliced season 2 w/Silge on bird strike dataset, mean log loss scored a balanced outcome variable better than the unbalanced one.\n\nFails to distinguish between hard and easy examples. Hard examples are those in which the model repeatedly makes huge errors, whereas easy examples are those which are easily classified. As a result, Cross-Entropy loss fails to pay more attention to hard examples."
  },
  {
    "objectID": "qmd/loss-functions.html#sec-lossfun-mse",
    "href": "qmd/loss-functions.html#sec-lossfun-mse",
    "title": "49Â  Loss Functions",
    "section": "49.13 Mean Squared Error (MSE)",
    "text": "49.13 Mean Squared Error (MSE)\n\nA quadratic loss function thatâ€™s calculation is based on the squared errors, so it is more sensitive to the larger deviations associated with tail events\n\nA custom asymmetric MSE loss function can better account for large deviations in errors\n\n\nWith Î±âˆˆ(0,1) being the parameter we can adjust to change the degree of asymmetry"
  },
  {
    "objectID": "qmd/loss-functions.html#sec-lossfun-quant",
    "href": "qmd/loss-functions.html#sec-lossfun-quant",
    "title": "49Â  Loss Functions",
    "section": "49.14 Quantile loss function",
    "text": "49.14 Quantile loss function\n\nWhich is a linear function. Benefit of allowing you to predict a certain percentile of the outcome variable\nNot effective at predicting tail events\n\n\nWith ğ¬âˆˆ(0,1) as the required quantile"
  },
  {
    "objectID": "qmd/loss-functions.html#sec-lossfun-rae",
    "href": "qmd/loss-functions.html#sec-lossfun-rae",
    "title": "49Â  Loss Functions",
    "section": "49.15 Relative Absolute Error",
    "text": "49.15 Relative Absolute Error\n\n( |y_pred â€” y_true | / y_true)"
  },
  {
    "objectID": "qmd/loss-functions.html#sec-lossfun-rmsle",
    "href": "qmd/loss-functions.html#sec-lossfun-rmsle",
    "title": "49Â  Loss Functions",
    "section": "49.16 Root Mean Squared Logarithmic Error (RMSLE)",
    "text": "49.16 Root Mean Squared Logarithmic Error (RMSLE)\n\nAlso see Understanding the metric: RMSLE\n\nmainly used when predictions have large deviations. Values range from 0 up to millions and we donâ€™t want to punish deviations in prediction as much as with MSE\nPredicts the geometric mean of your distribution\n\nThe geometric mean is most useful when numbers in the series are not independent of each other or if numbers tend to make large fluctuations.\n\nSimilar to MALE except the exp(RMSLE) is still not directly intepretable\nYou can just add 1 to your outcome then log it then used RMSE for your loss\n\nDrob did log(outcome) + 1 but that doesnâ€™t match the formula so I dunno if heâ€™s right.\nmutate(outcome = log(outcome + 1)\n\nOR extend {yardstick} by creating a custom metric to calculate RMSLE\nlibrary(rlang)\nrmsle_vec &lt;- function(truth, estimate, na_rm = TRUE, ...) {\nÂ  rmsle_impl &lt;- function(truth, estimate) {\nÂ  Â  sqrt(mean((log(truth + 1) - log(estimate + 1))^2))\nÂ  }\nÂ  metric_vec_template(\nÂ  Â  metric_impl = rmsle_impl,\nÂ  Â  truth = truth,\nÂ  Â  estimate = estimate,\nÂ  Â  na_rm = na_rm,\nÂ  Â  cls = \"numeric\",\nÂ  Â  ...\nÂ  )\n}\nrmsle &lt;- function(data, ...) {\nÂ  UseMethod(\"rmsle\")\n}\nrmsle &lt;- new_numeric_metric(rmsle, direction = \"minimize\")\nrmsle.data.frame &lt;- function(data, truth, estimate, na_rm = TRUE, ...) {\nÂ  metric_summarizer(\nÂ  Â  metric_nm = \"rmsle\",\nÂ  Â  metric_fn = rmsle_vec,\nÂ  Â  data = data,\nÂ  Â  truth = !!enquo(truth),\nÂ  Â  estimate = !!enquo(estimate),\nÂ  Â  na_rm = na_rm,\nÂ  Â  ...\nÂ  )\n}"
  },
  {
    "objectID": "qmd/loss-functions.html#sec-lossfun-smape",
    "href": "qmd/loss-functions.html#sec-lossfun-smape",
    "title": "49Â  Loss Functions",
    "section": "49.17 Symmetric Mean Absolute Percentage Error (SMAPE)",
    "text": "49.17 Symmetric Mean Absolute Percentage Error (SMAPE)\n\nSMAPE = (1/n) * Î£(|forecast â€“ actual| / ((|actual| + |forecast|)/2) * 100\nsmape &lt;- function(a, f) {return (1/length(a) * sum(2*abs(f-a) / (abs(a)+abs(f))*100))}\npackages: yardstick, Metrics\nCheck notebook â€” think this metric has issues with values around 0\nHandles data where the scale varies over time; it is relatively comparable across time series; deals reasonably well with outliers\nUnderestimates and overestimates are punished equally harshly\n\nMAPE punished overestimates more severely than underestimates"
  },
  {
    "objectID": "qmd/loss-functions.html#sec-lossfun-wrmsse",
    "href": "qmd/loss-functions.html#sec-lossfun-wrmsse",
    "title": "49Â  Loss Functions",
    "section": "49.18 Weighted Root Mean Square Scaled Error (WRMSSE)",
    "text": "49.18 Weighted Root Mean Square Scaled Error (WRMSSE)\n\n\n guessing the 42840 is nrow of the dataset in the example and there was no explanation of how wi is calculated"
  },
  {
    "objectID": "qmd/mathematics-linear-algebra.html#sec-math-linalg-resc",
    "href": "qmd/mathematics-linear-algebra.html#sec-math-linalg-resc",
    "title": "Linear Algebra",
    "section": "Resources",
    "text": "Resources\n\nSee Matrix Cookbook pdf in R &gt;&gt; Documents &gt;&gt; Mathematics\n\nderivatives, inverses, statistics, probability, etc.\n\nLink - A lot of matrix properties as related to regression, covariance, coefficients, etc.\nEBOOK statistical linear algebra: basics, transformations, decompositions, linear systems, regression - Matrix Algebra for Educational Scientists"
  },
  {
    "objectID": "qmd/mathematics-linear-algebra.html#sec-math-linalg-matmult",
    "href": "qmd/mathematics-linear-algebra.html#sec-math-linalg-matmult",
    "title": "Linear Algebra",
    "section": "Matrix Multiplication",
    "text": "Matrix Multiplication"
  },
  {
    "objectID": "qmd/mathematics-linear-algebra.html#sec-math-linalg-matalg",
    "href": "qmd/mathematics-linear-algebra.html#sec-math-linalg-matalg",
    "title": "Linear Algebra",
    "section": "Matrix Algebra",
    "text": "Matrix Algebra\n\nAn expected value equation (VC stands for variance-covariance in example) multiplied by a matrix, C.\n\n\nC is factored out of an expected value as C\nC is factored out of a transpose as CT"
  },
  {
    "objectID": "qmd/mathematics-linear-algebra.html#sec-math-linalg-fact",
    "href": "qmd/mathematics-linear-algebra.html#sec-math-linalg-fact",
    "title": "Linear Algebra",
    "section": "Factorization",
    "text": "Factorization"
  },
  {
    "objectID": "qmd/mathematics-glossary.html",
    "href": "qmd/mathematics-glossary.html",
    "title": "Glossary",
    "section": "",
    "text": "A priori - a type of knowledge that can be derived by reason alone\n\nA priori analyses are performed as part of the research planning process.\n\nA posteriori - a type of knowledge that expresses an empirical fact unknowable by reason alone.\n\nSame as post-hoc. Post-Hoc analysis is conducted after the experiment.\n\nbias - see unbiased estimator\nceteris paribus - latin for â€œall things being equalâ€ or â€œother things held constant.â€\nclosed form - a mathematical expression that uses a finite number of standard operations. It may contain constants, variables, certain well-known operations, and functions, but usually no limit, differentiation, or integration.\nconsistency - Requires that the outcome of the procedure with unlimited data should identify the underlying truth. Usage is restricted to cases where essentially the same procedure can be applied to any number of data items. In complicated applications of statistics, there may be several ways in which the number of data items may grow. For example, records for rainfall within an area might increase in three ways: records for additional time periods; records for additional sites with a fixed area; records for extra sites obtained by extending the size of the area. In such cases, the property of consistency may be limited to one or more of the possible ways a sample size can grow\ndegrees of freedom - When discussed about variable-sample size tradeoff, usually means n-p, where is the number of rows and p is the number of variables. The more variables used in the model the fewer degrees of freedom and therefore less power and precision.\nexchangeability - means we can swap around, or reorder, variables in the sequence without changing their joint distribution.\n\nEvery IID (independent, identically distributed) sequence is exchangeable - but not the other way around. Every exchangeable sequence is identically distributed, though\n\nExample: If you draw a sequence of red and blue marbles from a bag without replacement, the sample is exchangeable but not independent. e.g.Â drawing a red marble affects the probability of drawing a red or blue marble next.\n\n\nefficiency - A test, estimator, etc. is more efficient than another test, estimator, etc. if it requires fewer observation to obtain the same level of performance.\nergodicity - the idea that a point of a moving system, either a dynamical system or a stochastic process, will eventually visit all parts of the space that the system moves in, in a uniform and random sense\nexternal validity - Our estimates are externally valid if inferences and conclusions can be generalized from the population and setting studied to other populations and settings. (also see internal validity)\nidentifiable (aka point-indentifiable) - theoretically possible to learn the true values of this modelâ€™s underlying parameters after obtaining an infinite number of observations from it (see non-identifiability, partially-indentifiable)\nill-conditioned - In SVD decomposition, when thereâ€™s a huge difference between largest and smallest eigenvalue ofÂ the original matrix, A, the ratio of which is called condition number.\ninternal validity - our estimates are internally valid if statistical inferences about causal effects are valid for the population being studied. (also see external validity)\nintractable - problems for which there exist no efficient algorithms to solve them. Most intractable problems have an algorithm â€“ the same algorithm â€“ that provides a solution, and that algorithm is the brute-force search\nlocality - effects have causes and chains of cause and effect must be unbroken in space and time (not the case in â€˜entanglementâ€™)\nnon-identifiability - the structure of the data and model do not make it possible to estimate the parameterâ€™s value. Multicollinearity is a type of non-identifiability problem. (i.e.Â two or more parametrizations of the model are observationally equivalent) (see identifiable, partially-indentifiable)\noverdetermined system - In linear regression, when there are more observations than features, n &gt; p\npartial coefficient - The coefficient of a variable in a multivariable regression. In a simple regression, the coefficient of the variable is just called the â€œregression coefficient.â€\npartially-indentifiable (aka set identifiable) - non-identifiable but possible to learn the true values of a certain subset of the model parameters\nrobust - a â€œrobustâ€ estimator in statistics is one that is insensitive to outliers, whereas a â€œrobustâ€ estimator in econometrics is insensitive to heteroskedasticity and autocorrelation (hyndman)\nsupport (aka range) - the set of values that the random variable can take.\n\nFor discrete random variables, it is the set of all the realizations that have a strictly positive probability of being observed.\nFor continuous random variables, it is the set of all numbers whose probability density is strictly positive.\nSee link for examples\n\nunderspecification - In general, the solution to a problem is underspecified if there are many distinct solutions that solve the problem equivalently.\nAn unbiased estimator is an accurate statistic thatâ€™s used to approximate a population parameter.\n\nâ€œAccurateâ€ in this sense means that itâ€™s neither an overestimate nor an underestimate. If an overestimate or underestimate does happen, the mean of the difference is called a â€œbias.â€\n\nWeak Law of Large Numbers (Bernoulliâ€™s theorem) - states that if you have a sample of independent and identically distributed random variables, as the sample size grows larger, the sample mean will tend toward the population mean"
  },
  {
    "objectID": "qmd/mathematics-notation.html",
    "href": "qmd/mathematics-notation.html",
    "title": "50Â  Mathematics, Notation",
    "section": "",
    "text": "means dot product; also covariance of x and y\nZ = X \\ Y\n\nthe set of random variables in set X that are not in set Y\n\n Â means x is orthogonal to y which means x is uncorrelated with y\nX is not independent of Y\nconditional independence : Y is not associated with some variable X, after conditioning on some other variable Z\n\nthey are statements of which variables should be associated with one another (or not) in the data.\nthey are statements of which variables become dis-associated when we condition on some other set of variables.\nThere is no other path of influence from X to Y except through Z\n\np(biâˆ£Ti,Î´i,yt; Î¸) or f(y|x; Î¸)\n\nâ€œ;â€ acts as a grand comma to visually emphasize that x is of different kind (e.g.Â data vs parameters, random vs fixed quantities) than Î¸\n\ni.e.Â separates data values from parameters to improve readability\n\n\nplim - probability limit\n\nInterpretations:\n\nThe value of the random variables gets close to a real number, x, in the sense that the probability that Xn is very different from x gets very small as n gets big.\nThe distribution of Xn gets very close to the distribution of some other random variable Y as n gets large (then we would need a definition for the distance between distributions)."
  },
  {
    "objectID": "qmd/mathematics-probability.html#sec-math-prob-misc",
    "href": "qmd/mathematics-probability.html#sec-math-prob-misc",
    "title": "51Â  Probability",
    "section": "51.1 Misc",
    "text": "51.1 Misc\n\nScoring predictions\n\nExample: Probability of rain (1 = Rain, 0 = No Rain) scoring rules: -5 happiness for being rained on, -1 happiness for having to carry an umbrella Your chance of carrying an umbrella is equal to the forecast probability of rain. Your job is now to maximize your happiness by choosing a weatherperson\n\n\nForecaster total score: 3 Ã— (âˆ’1) + 7 Ã— (âˆ’0.6) = -7.2 happiness\nNewcomer total score: 3 x 5 + 7 x 0 = -15 happiness\nThis looks like an expected value â€” sum ( forecasted_probabilityevent * cost (or reward))"
  },
  {
    "objectID": "qmd/mathematics-probability.html#sec-math-prob-fund",
    "href": "qmd/mathematics-probability.html#sec-math-prob-fund",
    "title": "51Â  Probability",
    "section": "51.2 Fundamentals",
    "text": "51.2 Fundamentals\n\nAddition Rule\n\n\nExclusive P(A) + P(B)\nNot exclusive P(A) + P(B) - P(A and B)\n\ne.g A - King, B - Clubs\n\n\nComplementary Rule\n\nConditional Rule\n\nMultiplication Rule\n\nPermutations\n\nDefinition: A permutation of n elements is any arrangement of those n elements in a definite order. There are n factorial (n!) ways to arrange n elements. Note the bold: order matters!\nThe number of permutations of n things taken r-at-a-time is defined as the number of r-tuples that can be taken from n different elements and is equal to the following equation: \nExample Question: How many permutations does a license plate have with 6 digits? \n\nCombinations\n\nDefinition: The number of ways to choose r out of n objects whereÂ order doesnâ€™t matter.\nThe number of combinations of n things taken r-at-a-timeÂ is defined as the number of subsets with r elements of a set with n elements and is equal to the following equation: \nExample Question: How many ways can you draw 6 cards from a deck of 52 cards?"
  },
  {
    "objectID": "qmd/mathematics-probability.html#sec-math-prob-not",
    "href": "qmd/mathematics-probability.html#sec-math-prob-not",
    "title": "51Â  Probability",
    "section": "51.3 Notation",
    "text": "51.3 Notation\n\npmfs use Pr( ) while pdfs use P( ) or p( ) or f( ) (despite when I have written below)\nW ~ Binomial(n,p) is read â€œthe event W is distributed Binomially or assumed to be Binomial with sample size n and probability p.â€\nP(B|A) - Conditional probability: meaning the probability of B given that A has occurred (example: Dependent events, P(A and B) â€“&gt; pick a queen, P(A) = 4/52; given a queen was chosen, pick a jack, P(B|A) = 4/51; multiply)"
  },
  {
    "objectID": "qmd/mathematics-probability.html#sec-math-prob-terms",
    "href": "qmd/mathematics-probability.html#sec-math-prob-terms",
    "title": "51Â  Probability",
    "section": "51.4 Terms",
    "text": "51.4 Terms\n\nProbability - the number of occurrences of a certain event expressed as a proportion of all events that could occur.\n\nExample: In our black bag there are three blue balls, but there are ten balls in total, so the probability that you pull out a blue ball is three divided by ten which is 30% or 0.3.\nConvert probability of an event to odds for an event: O = P/(1 â€” P)\n\nOdds - the number of occurrences of a certain event expressed as a proportion of the number of non-occurrences of that event.\n\nExample: In our black bag there are three blue balls, but there are seven balls which are not blue, so the odds for drawing a blue ball are 3:7.\nOdds are often expressed as odds for, which in this case would be three divided by seven, which is about 43% or 0.43, or odds against, which would be seven divided by three, which is 233% or 2.33.\nConvert odds for an event to probability: P = O/(1 + O)\n\nOdds Ratio - describes the percent change in odds\nStochastic - no value of a variable is known with certainty. Some values may be more likely than others (probabilistic). Variable gets mapped onto a distribution."
  },
  {
    "objectID": "qmd/mathematics-probability.html#sec-math-prob-func",
    "href": "qmd/mathematics-probability.html#sec-math-prob-func",
    "title": "51Â  Probability",
    "section": "51.5 Functions",
    "text": "51.5 Functions\n\nProbability Mass Functions (pmf): The probability that discrete random variable, X = a value.\n\n\\(P(X = x) = f(x)\\) of a discrete random variable X is a function that satisfies the following properties:\n\n(1) says the probability that x is a certain value is &gt; 0 if x is in the support (set of possible values) of S\n(2) says all the probabilities for the supported values add up to 1\n(3) says to determine the probability associated with the event A, you just sum up the probabilities of the x values in A.\n\nProbability Density Function (pdf): The probability that a continuous random variable is within an interval.\n\nProbability density is the rate of change in cumulative probability. So the probability density can exceed 1, but the area under the density cannot.\nContinuous random variables are uncountably infinite (discrete are countably infinite). Therefore, the probability of X = a value is zero which is why finding the probability of finding the value within in interval is calculated, \\(P(a &lt; X &lt; b)\\).\nThe pdf of a continuous random variable \\(X\\) with support \\(S\\) is an integrable function \\(f(x)\\) satisfying the following:\n\n\nUses of integrals instead of summations\n\n\nCumulative Distribution Function (CDF): The probability that a random variable value is less than or equal another value. Also called the distribution function.\n\n\nSays that \\(F\\) is the cdf of \\(X\\) but is a function of \\(t\\)\nThe CDF has the following properties:\n\n\n(3) says any probability for a value less than the minimum value is zero in the sample space. The other properties are self-explanatory\n\nâ€œCumulativeâ€ because weâ€™re asking, â€œwhatâ€™s the probability of value 1 or value 2 or value 3 etc.?â€ which is the sum of the probabilities.\n\nSimulation of random variable values of a distribution using its cdf (whatâ€™s happening when you use rnorm( ))\n\nChange of Variable Technique\n\nRandomly draw 1000 (or whatever) numbers from \\(\\mathcal{U}(0,1)\\) (i.e.Â the values of the y-axis of the CDF which are bdd between 0 and 1)\nFind the inverse of the distributionâ€™s CDF (solving for \\(X\\))\nPlug the random numbers into the inverse CDF to calculate \\(X\\) (i.e.Â the random variable values of the distribution) \n\nFig shows the example where 0.8 is randomly drawn from \\(\\mathcal{U}(0,1)\\) and plugged into the inverse CDF function to obtain x = 8\n\n\n\nFig illustrates the Change-of-Variable technique (the mu in y-axis label is a typo, should be a u).\n\nTaking the inverse of \\(Y = u(X)\\) yields \\(X = v(Y)\\)"
  },
  {
    "objectID": "qmd/mathematics-probability.html#sec-math-prob-plot",
    "href": "qmd/mathematics-probability.html#sec-math-prob-plot",
    "title": "51Â  Probability",
    "section": "51.6 Plots",
    "text": "51.6 Plots\n\na Qâ€“Q (quantile-quantile) plot is a probability plot, which is a graphical method for comparing two probability distributions by plotting their quantiles against each other.\n\nIf the two distributions being compared are similar, the points in the Qâ€“Q plot will approximately lie on the line y = x.\nIf the distributions are linearly related after one is transformed, the points in the Qâ€“Q plot will approximately lie on a line, but not necessarily on the line y = x\nUsing to compare two samples of data can be viewed as a non-parametric approach to comparing their underlying distributions\nInterpretation\n\nIf distribution on y-axis is more dispersed than the distribution on the x-axis, then the line is steeper than y = x.\nIf distribution on x-axis is more dispersed than the distribution on the y-axis, then the line is flatter than y = x.\nArcs or â€œSâ€ shapes indicate one distribution is more skewed than the other or one of the distributions has heavier tails than the other"
  },
  {
    "objectID": "qmd/mathematics-statistics.html#sec-math-statc-",
    "href": "qmd/mathematics-statistics.html#sec-math-statc-",
    "title": "Statistics",
    "section": "Misc",
    "text": "Misc\n\nAge Adjustment of per 100K rate\n\n\nAllows communities with different age structures to be compared\nThe crude (unadjusted)1994 cancer mortality rate in New York State is 229.8 deaths per 100,000 men. The age-adjusted rate is 214.7 deaths per 100,000 men.\n\nNotice that 214.7 isnâ€™t 229.8*1 of course but the sum of all the individual age-group weighted rates.\n\nProcess (Formulas in column headers)\n\nCalculate the diseaseâ€™s rate per 100K for each age group\nMultiply the age-specific rates of disease by age-specific weights\n\nThe weights are the proportion of the US population within each age group. (e.g.Â 0-14 year olds are 28.4% of the 1970 US population)\n\nThe weighted rates are then summed over the age groups to give the (total) age-adjusted rate"
  },
  {
    "objectID": "qmd/mathematics-statistics.html#sec-math-statc-terms",
    "href": "qmd/mathematics-statistics.html#sec-math-statc-terms",
    "title": "Statistics",
    "section": "Terms",
    "text": "Terms\n\nCoefficient of Variation (CV) - aka Relative Standard Deviation (RSD) - aka Dispersion Parameter - Measure of the relative dispersion of data points in a data series around the mean. Usually expressed as a percentage.\n\\[\nCV = \\frac{\\sigma}{\\mu}\n\\]\n\nWhile most often used to analyze dispersion around the mean, a quartile, quintile, or decile CV can also be used to understand variation around the median or 10th percentile, for example.\nShould only be used with variables that have minimum at zero (e.g.Â counts, prices) and not interval data (e.g celsius or fahrenheit)\nWhen the mean value is close to zero, the coefficient of variation will approach infinity and is therefore sensitive to small changes in the mean. This is often the case if the values do not originate from a ratio scale.\nA more robust possibility is the quartile coefficient of dispersion, half the interquartile range divided by the average of the quartiles (the midhinge)\n\\[\nCV_q = \\frac{0.5(Q_3 - Q_1)}{0.5(Q_3 + Q_1)}\n\\]\nFor small samples (Normal Distribution)\n\\[\nCV_*= (1 + \\frac{1}{4n}) CV\n\\]\nFor log-normal distribution\n\\[\nCV_{LN} = \\sqrt{e^{\\sigma^2_{ln}} - 1}\n\\]\n\nWhere \\(\\sigma_{ln}\\) is the standard deviation after a \\(\\ln\\) transformation of the data\n\n\nCovariance - Between two random variables is a measure of how correlated are their variations around their respective means\nKernel Smoothing - Essence is the simple concept of a local average around a point, x; that is, a weighted average of some observable quantities, those of which closest to x being given the highest weights\nMargin of Error (MoE) - The range of values below and above the sample statistic in a confidence interval.\n\\[\n\\text{MOE}_\\gamma = z_\\gamma \\sqrt{\\frac{\\sigma^2}{n}}\n\\]\n\nZ-Score with confidence level Î³ â¨¯ Standard Error\nIn general, for small sample sizes (under 30) or when you donâ€™t know the population standard deviation, use a t-score to get the critical value. Otherwise, use a z-score.\n\nSee Null Hypothesis Significance Testing (NHST) &gt;&gt; Misc &gt;&gt; Z-Statistic Table for an example\n\nExample: a 95% confidence interval with a 4 percent margin of error means that your statistic will be within 4 percentage points of the real population value 95% of the time.\nExample: a Gallup poll in 2012 (incorrectly) stated that Romney would win the 2012 election with Romney at 49% and Obama at 48%. The stated confidence level was 95% with a margin of error of Â± 2. We can conclude that the results were calculated to be accurate to within 2 percentages points 95% of the time.\n\nThe real results from the election were: Obama 51%, Romney 47%. So the Obama result was outside the range of the Gallup pollâ€™s margin of error (2 percent).\n\n48 â€“ 2 = 46 percent\n48 + 2 = 50 percent\n\n\n\nNormalization - Rescales the values into a specified range, typically [0,1]. This might be useful in some cases where all parameters need to have the same positive scale. However, the outliers from the data set are lost.\n\\[\n\\tilde{X} = \\frac{X-X_{\\text{min}}}{X_{\\text{max}}-X_{\\text{min}}}\n\\]\n\nSome functions have the option to normalize with range [-1, 1]\nBest option if the distribution of your data is unknown.\n\nParameter - Describes an entire population (also see statistic)\nP-Value - \\(\\text{p-value}(y) = \\text{Pr}(T(y_{\\text{future}}) &gt;= T(y) | H)\\)\n\n\\(H\\) is a â€œhypothesis,â€ a generative probability model\n\\(y\\) is the observed data\n\\(y_{\\text{future}}\\) are future data under the model\n\\(T\\) is a â€œtest statistic,â€ some pre-specified specified function of data\n\nSampling Error - The difference between population parameter and the statistic that is calculated from the sample (such as the difference between the population mean and sample mean).\nStandard Error of the Mean (SEM) - Measures how far the sample mean (average) of the data is likely to be from the true population mean (Also see Fundamentals &gt;&gt; Interpreting s.d., s.e.m, and CI Bars)\n\\[\n\\text{SEM} = \\frac{\\text{SD}}{\\sqrt{n}}\n\\]\n\nAssumes a simple random sample with replacement from an infinite population\n\nStandardization rescales data to fit the Standard Normal Distribution which has a mean (Î¼) of 0 and standard deviation (Ïƒ) of 1 (unit variance).\n\\[\n\\tilde X = \\frac{X-\\mu}{\\sigma}\n\\]\n\nRecommended for PCA and if your data is known to come from a Gaussian distribution.\n\nStatistic - Describes a sample (also see parameter)\nVariance (\\(\\sigma^2\\))- Measures variation of a random variable around its mean.\nVariance-Covariance Matrix - Square matrix containing variances of the fitted modelâ€™s coefficient estimates and the pair-wise covariances between coefficient estimates.\n\\[\n\\begin{align}\n&\\text{Cov}(\\hat\\beta) = (X^TX)^{-1} \\cdot \\text{MSE}\\\\\n&\\text{where}\\;\\; \\text{MSE} = \\frac{\\text{SSE}}{\\text{DSE}} = \\frac{\\text{SSE}}{n-p}\n\\end{align}\n\\]\n\nDiagnonal is the variances, and the rest of the values are covariances\nThereâ€™s also a variance/covariance matrix for error terms\nExample\nx &lt;- sin(1:100)\ny &lt;- 1 + x + rnorm(100)\nMSE &lt;- sum(residuals(lm(y ~ x))^2)/98 # where 98 is n-2\nvcov_mat &lt;- MSE * solve(crossprod(cbind(1, x)))\n\nvcov_mat is the same as vcov(lm(y ~ x))"
  },
  {
    "objectID": "qmd/mathematics-statistics.html#sec-math-statc-nhst",
    "href": "qmd/mathematics-statistics.html#sec-math-statc-nhst",
    "title": "Statistics",
    "section": "Null Hypothesis Significance Testing (NHST)",
    "text": "Null Hypothesis Significance Testing (NHST)\n\nMisc\n\nWhy would we not always use a non-parametric test so we do not have to bother about testing for normality? The reason is that non-parametric tests are usually less powerful than corresponding parametric tests when the normality assumption holds. Therefore, all else being equal, with a non-parametric test you are less likely to reject the null hypothesis when it is false if the data follows a normal distribution. It is thus preferred to use the parametric version of a statistical test when the assumptions are met.\nIf your statistic value is greater than the critical value, then itâ€™s significant and you reject H0\n\n\nThink in terms of a distribution with statistic values on the x-axis, and greater than the critical value means youâ€™re in the tail (one-sided)\n\nT-Statistic Table\nZ-Statistic Table\n\nExample: 95% CI â†’ Î± = 100% - 95% = 0.05 â†’ Î±/2 (1-tail) = 0.025\n\n1 - 0.025 = 0.975 (subtract from 1 because the z-score table cells are for the area left of the critical value\nThe z-score is 1.96 for a 95% CI\n\n\nZ-score comes from adding the row value with the column value that has the cell value of our area (e.g.Â 0.975) left of the critical value\nIf the area was between 0.97441 and 0.97500, then the z-score would be the row value, 1.9, added to the column value thatâ€™s half way between 0.05 and 0.06, which results in a z-score of 1.955\n\n\n\n\nType I Error - False-Positive; occurs if an investigator rejects a null hypothesis that is actually true in the population\n\nThe models perform equally well, but the A/B test still produces a statistically significant result. As a consequence, you may roll out a new model that doesnâ€™t really perform better.\nYou can control the prevalence of this type of error with the p-value threshold. If your p-value threshold is 0.05, then you can expect a Type I error in about 1 in 20 experiments, but if itâ€™s 0.01, then you only expect a Type I error in only about 1 in 100 experiments. The lower your p-value threshold, the fewer Type I errors you can expect.\n\nType II Error - False-Negative; occurs if the investigator fails to reject a null hypothesis that is actually false in the population\n\nThe new model is in fact better, but the A/B test result is not statistically significant.\nYour test is underpowered, and you should either collect more data, choose a more sensitive metric, or test on a population thatâ€™s more sensitive to the change.\n\nType S Error (Sign Error): The A/B test shows that the new model is significantly better than the existing model, but in fact the new model is worse, and the test result is just a statistical fluke. This is the worst kind of error, as you may roll out a worse model into production which may hurt the business metrics.\n\n{retrodesign} - Provides tools for working with Type S (Sign) and Type M (Magnitude) errors. (Vignette)\n\nType M error (Magnitude Error): The A/B test shows a much bigger performance boost from the new model than it can really provide, so youâ€™ll over-estimate the impact that your new model will have on your business metrics.\n\n{retrodesign} - Provides tools for working with Type S (Sign) and Type M (Magnitude) errors. (Vignette)\n\nFalse Positive Rate (FPR)(\\(\\alpha\\)) - ; Probability of a type I error; Pr(measured effect is significant | true effect is â€œnullâ€)\n\\[\n\\text{FPR} = \\frac{v}{m_0}\n\\]\n\n\\(v\\): Number of times thereâ€™s a false positive\n\\(m_0\\): Number of non-significant variables\n\nFalse Discovery Rate (FDR) - Pr(measured effect is null | true effect is significant)\n\n\\[\n\\text{FDR} = \\frac{\\alpha \\pi_0}{\\alpha \\pi_0 + (1-\\beta)(1-\\pi_0)}\n\\]\n\n\\(\\alpha\\) - Type I error rate (False Positive Rate)\n\\(\\beta\\) - Type II error rate (False Negative Rate)\n\\(1-\\beta\\) - Power\n\\(\\pi_0\\) - Count of true null effects\n\\(1âˆ’\\pi_0\\) - Count of true non-null effects\n\nPower - 1-Î² where beta is the Probability of a type II error\nFamily-Wise Error Rate (FWER) - the risk of at least one false positive in a family of S hypotheses.\n\\[\nFWER = \\frac{v}{R}\n\\]\n\n\\(v\\): Number of times thereâ€™s a false positive\n\\(R\\): Number of times we claim Î² â‰  0\ne.g.Â Using the same data and variables to fit multiple models with different outcome variables (i.e.Â different hypotheses)\n\nRomano and Wolfâ€™s Ccorrection\n\nAccounting for the dependence structure of the p-values (or of the individual test statistics) produces more powerful procedures than Bonferroi and Holms. This can be achieved by applying resampling methods, such as bootstrapping and permutations methods.\n\nPermutation tests of regression coefficients can result in rates of Type I error which exceed the nominal size, and so these methods are likely not ideal for such applications\n\nSee Stata docs of the procedure\nPackages\n\n{wildrwolf}: Implements Romano-Wolf multiple-hypothesis-adjusted p-values for objects of type fixest and fixest_multi from the fixest package via a wild cluster bootstrap."
  },
  {
    "objectID": "qmd/mathematics-statistics.html#sec-math-statc-boot",
    "href": "qmd/mathematics-statistics.html#sec-math-statc-boot",
    "title": "Statistics",
    "section": "Bootstrapping",
    "text": "Bootstrapping\n\nMisc\n\nPost-Hoc Analysis, General &gt;&gt; Frequentist &gt;&gt; Bootstrap\nDo NOT bootstrap the standard deviation (article)\n\nBootstrap is â€œbased on a weak convergence of momentsâ€\nif you use an estimate based standard deviation of the bootstrap, you are being overly conservative (i.e.Â youâ€™re overestimating the sd and CIs are too wide)\n\nBootstrapping uses the original, initial sample as the population from which to resample, whereas Monte Carlo simulation is based on setting up a data generation process (with known values of the parameters of a known distribution). Where Monte Carlo is used to test drive estimators, bootstrap methods can be used to estimate the variability of a statistic and the shape of its sampling distribution\nUse bias-corrected bootstrapped CIs (article)\n\nâ€œpercentile and BCa methods were the only ones considered here that were guaranteed to return a confidence interval that respected the statisticâ€™s sampling space. It turns out that there are theoretical grounds to prefer BCa in general. It isâ€second-order accurateâ€, meaning that it converges faster to the correct coverage. Unless you have a reason to do otherwise, make sure to perform a sufficient number of bootstrap replicates (a few thousand is usually not too computationally intensive) and go with reporting BCa intervals.â€\n\nPackages\n\n{rsample}\n{DescTools::BootCI}\nboot and boot.ci\n{ebtools::get_boot_ci}\n\n\nSteps\n\nResample with replacement\nCalculate statistic of resample\nStore statistic\nRepeat 10K or so times\nCalculate mean, sd, and quantiles for CIs across all collected statistics\n\nBayesian Bootstrapping (aka Fractional Weighted Bootstrap)\n\nMisc\n\nNotes from\n\nThe Bayesian Bootstrap\nThread\n\nPackages\n\n{fwb}\n\n\nDescription\n\nDoesnâ€™t resample the dataset, but samples a set of weights from the Uniform Dirichlet distribution and computes weighted averages (or whatever statistic)\nWeights sum to â€˜nâ€™ but may be non-integers\nEach row gets a frequency weight based on the number of times they appear\nIn this way, every row is included in the analysis but given a fractional weight that represents its contribution to the statistic\n\nIn a traditional bootstrap, some rows of data may not be sampled and therefore excluded from the calculation of the statistic\n\nParticularly useful with rare events, where a row excluded from a traditional bootstrap sample might cause the whole estimation to explode (e.g., in a rare-events logistic regression where one sample has no events!)\n\n\n\nShould be faster and consume less RAM\nPython implementation\ndef classic_boot(df, estimator, seed=1):\nÂ  Â  df_boot = df.sample(n=len(df), replace=True, random_state=seed)\nÂ  Â  estimate = estimator(df_boot)\nÂ  Â  return estimate\n\ndef bayes_boot(df, estimator, seed=1):\nÂ  Â  np.random.seed(seed)\nÂ  Â  w = np.random.dirichlet(np.ones(len(df)), 1)[0]\nÂ  Â  result = estimator(df, weights=w)\nÂ  Â  return result\n\nfrom joblib import Parallel, delayed\ndef bootstrap(boot_method, df, estimator, K):\nÂ  Â  r = Parallel(n_jobs=8)(delayed(boot_method)(df, estimator, seed=i) for i in range(K))\nÂ  Â  return r\n\ns1 = bootstrap(bayes_boot, dat, np.average, K = 1000)"
  },
  {
    "objectID": "qmd/mathematics-statistics.html#sec-math-statc-desc",
    "href": "qmd/mathematics-statistics.html#sec-math-statc-desc",
    "title": "Statistics",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics\n\nMeans\n\nGeometric Mean\nsummarize_revenue &lt;- function(tbl) {\nÂ  Â  tbl %&gt;%\nÂ  Â  Â  Â  summarize(geom_mean_revenue = exp(mean(revenue)),\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  n = n())\n}\n\n\n\nProportions\n\nVariance of a proportion\n\nAssume that p applies equally to all n subjects\n\\[\n\\sigma^2_p = \\frac{p(1-p)}{n}\n\\]\n\nExample\n\nSample of 100 subjects where there are 40 females and 60 males\n10 of the females and 30 of the males have the disease\n\nMarginal estimate of the probability of disease is (30+10)/100 = 0.4\n\nVariance of the estimator assuming constant risk (i.e.Â assuming risk for females = risk for males)\n\n(prob_of_disease Ã— prob_not_disease) / n = (0.4 Ã— 0.6) / 100 = 0.0024\n\np = (10 + 30) / 100 = 0.40\n\n\n\n\nAssume p depends on a variable (e.g.Â sex)\n\\[\n\\sigma^2_p = \\frac{p^2_{1,n} \\cdot p_1(1-p_1)}{n_1} + \\frac{p^2_{2,n} \\cdot p_2(1-p_2)}{n_2}\n\\]\n\nExample\n\nDescription same as above\nAdjusted marginal estimate of the probability of disease is\n\n(prop_female Ã— prop_disease_female) + (prop_male Ã— prop_disease_male)\n(0.4 Ã— 0.25) + (0.6 Ã— 0.5) = 0.4\nSame marginal estimate as before\n\nVariance of the estimator assuming varying risk (i.e.Â assumes risk for females \\(\\neq\\) risk for males)\n\n1st half of equation:\n\nprop_female2 Ã— (prop_disease_female Ã— prop_not_disease_female) / n_female = [0.42 Ã— (0.25 Ã— 0.74) / 40]\n\n2nd half of equation\n\nprop_male2 Ã— (prop_disease_male Ã— prop_not_disease_male) / n_male = [0.62 Ã— (0.5 Ã— 0.5) / 60]\n\n1st half + 2nd half = 0.00224\n\nVariance is smaller than before\n\n\n\n\n\nCIs\n\nPackages:\n\n{binomCI} - 12 confidence intervals for one binomial proportion or a vector of binomial proportions are computed\n\nJeffreyâ€™s Interval\n# probability of event\n# n_rain in the number of events (rainy days)\n# n is the number of trials (total days)\nmutate(pct_rain = n_rain / n, \n          # jeffreys interval\n          # bayesian CI for binomial proportions\n          low = qbeta(.025, n_rain + .5, n - n_rain + .5), \n          high = qbeta(.975, n_rain + .5, n - n_rain + .5))\n\n\n\n\nSkewness\n\nPackages:\n\n{moments} - Standard algorithm\n{e1071} - 3 alg options\n{DescTools::Skew} - Same algs but with bootstraps CIs\n\nFrom the paper referenced in e1071, b1 (type 3) is better for non-normal population distributions and G1 (type 2) is better for normal population distributions\nSymmetric: Values between -0.5 to 0.5\nModerated Skewed data: Values betweenÂ -1 and -0.5Â or betweenÂ 0.5 and 1\nHighly Skewed data: ValuesÂ less than -1Â orÂ greater than 1\nRelationship between Mean and Median under different skewness\n\n\n\n\nKurtosis\n\nA high kurtosis distribution has a sharper peak and longer fatter tails, while a low kurtosis distribution has a more rounded peak and shorter thinner tails.\nTypes\n\n\nMesokurtic: kurtosis = ~3\n\nExamples: normal distribution. Also binomial distribution when p = 1/2 +/- sqrt(1/12)\n\nLeptokurtic: This distribution has fatter tails and a sharper peak. Excess kurtosis &gt; 3\n\nExamples: Studentâ€™s t-distribution, Rayleigh distribution, Laplace distribution, exponential distribution, Poisson distribution and the logistic distribution\n\nPlatykurtic: The distribution has a lower and wider peak and thinner tails. Excess kurtosis &lt; 3\n\nExamples: continuous and discrete uniform distributions, raised cosine distribution, and especially the Bernoulli distribution\n\nExcess Kurtosis is the kurtosis value - 3\n\n\n\n\nUnderstanding CI, SD, and SEM Bars\n\narticle\nP-values test whether the sample means are different from each other\nsd bars: Show the population spread around each sample mean. Useful as predictors of the range of new sample.\n\nNever seen these and it seems odd to mix a sample statistic with a population parameter and that the range is centered on the sample mean (unless the sample size is large I guess).\n\ns.e.m. is the â€œstandard error of the meanâ€ (See Terms)\n\nIn large samples, the s.e.m. bar can be interpreted as a 67% CI.\n95% CI â‰ˆ 2 Ã— s.e.m. (n &gt; 15)\n\nFigure 1\n\n\nEach plot shows 2 points representing 2 sample means\nPlot a: bars of both samples touch and are the same length\n\nsem bars intepretation: Commonly held view that â€œif the s.e.m. bars do not overlap, the difference between the values is statistically significantâ€ is NOT correct. Bars touch here but donâ€™t overlap and the difference in sample means is NOT significant.\n\nPlot b: p-value = 0.05 is fixed\n\nsd bar interpretation: Although the means differ, and this can be detected with a sufficiently large sample size, there is considerable overlap in the data from the two populations.\nsem bar intepretation: For there to be a significant difference in sample means, sem bars have to much further away from each other than there just being a recognizable space between the bars.\n\n\nFigure 2\n\n\nPlot a: shows how a\n\n95% CI captures the population mean 95% of the time but as seen here, only 18 out of 20 sample CIs (90%) contained the population mean (i.e.Â this is an asymptotic claim)\nA common misconception about CIs is an expectation that a CI captures the mean of a second sample drawn from the same population with a CI% chance. Because CI position and size vary with each sample, this chance is actually lower.\n\nPlot b:\n\nHard to see at first but the outer black bars are the 95% CI and the inner gray bars are the sem.\nBoth the CI and sem shrink as n increases and the sem is always encompassed by the CI\n\n\nFigure 3\n\n\nsem bars must be separated by about 1 sem (which is half a bar) for a significant difference to be reached at p-value = 0.05\n95% CI bars can overlap by as much as 50% and still indicate a significant difference at p-value = 0.05\n\nIf 95% CI bars just touch, the result is highly significant (P = 0.005)"
  },
  {
    "objectID": "qmd/mathematics-statistics.html#sec-math-statc-desc-pvfun",
    "href": "qmd/mathematics-statistics.html#sec-math-statc-desc-pvfun",
    "title": "Statistics",
    "section": "P-Value Function",
    "text": "P-Value Function\n\nNotes from https://ebrary.net/72024/health/value_confidence_interval_functions\n{concurve} creates these curves\nGives a more complete picture than just stating the p-value (strength and precision of the estimate)\n\nShows level of precision of the point estimate via shape of the curve\n\nnarrow-based, spikey curves = more precise\n\nVisualizes strength of the effect along the x-axis. Helps in showing â€œsignificantâ€ effect is not necessarily a meaningful effect.\n\nShows other estimate(s) that are also consistent with that p-value\nShows p-values associated with other estimates for the Null Hypothesis\n\nsee the end of the article for discussion on using this fact in an interpretation context\n\nThe P-value function is closely related to the set of all confidence intervals for a given estimate. (see example 3)\nExample 1\n\n\np-value of the point estimate is (always?) 1 which says, â€œgiven a null hypothesis =  is true (i.e.Â the true risk ratio = ),Â  the probability of seeing data produce this estimate or this estimate with more strength (ie smaller std error) is 100%.â€\n\nI.e. the pt est is the estimate most compatible with the data.\nThis pval language is mine. The whole â€œthis data or data more extremeâ€ has never sit right with me. I think this is more correct if my understanding is right.\n\nThe pval for the data in this example is at 0.08 for a H0 of 1. So unlikely, but typically not unlikely enough in order to reject the null hypothesis.\nA pval of 0.08 is identical for a pt est = 1 or pt est = 10.5\nWide base of the curve indicates the estimate is imprecise. Thereâ€™s potentially a large effect or little or no effect.\n\nExample 2\n\n\nmore data used for the second curve which indicates a precise point estimate.\npt est very close to H0\npval = 0.04 (not shown in plot)\n\nso the arbitrary pval = 0.05 threshold is passed and says a small effect is probably present\nIs that small of an effect meaningful even if itâ€™s been deemed statistically present?\n\nIn this case a plot with the second curve helps show that â€œstatistically significantâ€ doesnâ€™t necessarily translate to meaningful effect\n\nExample 3\n\n\nThe different confidence intervals reflect the same degree of precision (i.e.Â the curve width doesnâ€™t change when moving from one CI to another).\nThe three confidence intervals are described as nested confidence intervals. The P-value function is a graph of all possible nested confidence intervals for a given estimate, reflecting all possible levels of confidence between 0% and 100%."
  },
  {
    "objectID": "qmd/misc.html#sec-misc-misc",
    "href": "qmd/misc.html#sec-misc-misc",
    "title": "Misc",
    "section": "Misc",
    "text": "Misc\n\nWindows\n\nBrowser\n\nTo Address Bar: ctrl + L\nOpen New Tab: ctrl + T\nOpen Recently Closed Tab: ctrl + shift + T\n\n\nR-devel (&gt;= 4.4.0) gained a command-line option to adjust the limit connections (previous limit was 128 parallel workers)\n$ R\n&gt; parallelly::availableConnections()\n[1] 128\n\n$ R --max-connections=512\n&gt; parallelly::availableConnections()\n[1] 512"
  },
  {
    "objectID": "qmd/misc.html#sec-misc-rstud",
    "href": "qmd/misc.html#sec-misc-rstud",
    "title": "Misc",
    "section": "RStudio",
    "text": "RStudio\n\nJob: Run script in the background\nlibrary(rstudioapi)\njobRunScript(\"wfsets_desperation_tune.R\", name = \"tune\", exportEnv = \"R_GlobalEnv\")\n\nneed to look up args\nI think exportEnv takes the variables in your current environment and runs the script with them as inputs\n\nShortcuts\n\nKeyboard Shortcuts: Alt + Shift + k\nCommand Palette: Ctrl + Shift + p\nFind in Files: Ctrl + Shift + f\nMultiple Cursors: Ctrl + Alt + up/down"
  },
  {
    "objectID": "qmd/misc.html#sec-misc-hack",
    "href": "qmd/misc.html#sec-misc-hack",
    "title": "Misc",
    "section": "Hackathon Criteria",
    "text": "Hackathon Criteria"
  },
  {
    "objectID": "qmd/misc.html#sec-misc-update",
    "href": "qmd/misc.html#sec-misc-update",
    "title": "Misc",
    "section": "Update R",
    "text": "Update R\n\nMisc\n\n{rig} - r version management system\nupdate.packages(checkBuilt = TRUE, ask = FALSE) is supposed to search for packages in other R versions and update them in the new R version, but I havenâ€™t tried it, yet.\nErrors when compiling from source may require installing libraries and theyâ€™ll supply code to install via â€œpacmanâ€\n\nOpen Start &gt;&gt; scroll down to RTools40 &gt;&gt; RTools Bash\nPaste pacman code and hit enter to install\n\nProblem packages in the past\n\n{brms} dependency, {igraph}, didnâ€™t have a binary on CRAN and wouldnâ€™t compile from source even with correct libraries installed.\n\nSolâ€™n: install.packages(\"igraph\", repos = 'https://igraph.r-universe.dev')\n\ninstalls dev version from r-universe\n\n\nSome {easystats} packages had gave {pak} some problems. No difficulties using install.packages with default repo or if they had a r-universe repo though.\n\n\nSteps\n\nCopy user installed packages in current R version\n\nIn R:\nsquirrel &lt;- names(installed.packages(priority = \"NA\")[, 1]) # user installed packages\nreadr::write_rds(squirrel, \"packages.rds\")\n\nThen, close RStudio\n\n\nRTools: Check to see if you have the latest because youâ€™ll need it to compile some of newest versions of packages.\n\nYour rtools folder has the version in itâ€™s folder name.\nrtools website has the latest version and an .exe to download\n\nCheck/Update rig version\n\nIn powershell: rig --version\nCheck current rig release: link\nDownload and install if your version isnâ€™t current\n\nInstall new version of R\n\nClose R if not already closed\nrig add release installs the latest version of R.\nrig default &lt;new_r_version&gt; sets that version as the default\n\nAdd R and RTools to path\n\nRight-click Windows &gt;&gt; System &gt;&gt; (right panel) Advanced System Settings &gt;&gt; Environment Variables &gt;&gt; Under User Variables, highlight Path, click Edit &gt;&gt; Click Add\n\nR: Add path to directory with all the RScript, R exe, etc. e.g.Â â€œC:\\Program Files\\R\\R-4.2.3\\bin\\x64â€\nRTools: e.g.Â â€œC:\\rtools43\\usr\\binâ€\n\n\nOpen R and confirm new version\n\nIf RStudio\n\nThe setting of the new version to the â€œdefaultâ€ version of R in rig should result in RStudio loading the new version.\nIf not, Tools &gt;&gt; Global Options &gt;&gt; General\n\nUnder â€œR versionâ€, click â€œchangeâ€ button; choose new R version\nQuit session and restart RStudio\n\n\n\nInstall â€œhigh maintenanceâ€ packages\n\nIâ€™ve had issues with {pak} installing packages that need to be compiled. Maybe be worth trying {pak} first to see if theyâ€™ve fixed it.\n{cmdstanr} doesnâ€™t live on CRAN, so you have to use: install.packages(\"cmdstanr\", repos = c(\"https://mc-stan.org/r-packages/\", getOption(\"repos\")))\n\nCheck for latest cmdstan version\n\nAfter loading the package, library(cmdstanr) , it should run a check on your cmdstan version and tell you if thereâ€™s a newer version.\nTo update, first check toolchain: check_cmdstan_toolchain()\n\nMight tell you to update RTools or that you need some C++ library added\n\nFix C++ toolchain with check_cmdstan_toolchain(fix = TRUE)\nUpdate cmdstan: install_cmdstan()\nMay need to install {rstudioapi} and run rstudioapi::restartSession() (programmatically) or just ctrl + shift + f10 so that this package can be used as a dependency for other packages that need to be installed.\n\n\n{rstanarm}: install.packages(\"rstanarm\")\n\nInstall other packages\nmoose &lt;- readRDS(\"packages.rds\")\nmoose &lt;- moose[!moose %in% c(\"cmdstanr\", \"rstanarm\", \"ebtools\", \"translations\", \"&lt;RStudio add-ins&gt;\")]\n\n# Next time, add a try/catch? or maybe purrr::safely, so that it continues through errors. Also, need to log pkgs that do error.\nfor (i in seq_len(length(moose))) {\n  print(moose[i])\n  pak::pkg_install(moose[i])\n}\n\nfs::file_delete(\"packages.rds\")\n\n{ebtools} is my personal helper package.\n{translations} is a system package that shouldnâ€™t have been included when I saved the packages from previous version, but was when I recently updated. Might not be necessary to include it in the excuded packages in the future.\n\nCheck for updates of RStudio (link)\n\nCurrent version under Help &gt;&gt; About Rstudio\nPossible to check for updates under Help &gt;&gt; Check for Updates, but thatâ€™s failed me before."
  },
  {
    "objectID": "qmd/missingness.html#sec-missing-misc",
    "href": "qmd/missingness.html#sec-missing-misc",
    "title": "Missingness",
    "section": "Misc",
    "text": "Misc\n\nAlso see\n\nEDA &gt;&gt; Missingness\nModel Building, tidymodels &gt;&gt; Recipe &gt;&gt; Imputation\n\nBagging and knn methods for cross-sectional data\nRolling method for time series data\n\nHarrell RMS 3.5 Strategies for Developing an Imputation Model\n\nâ€œBut more precisely, even having the correct model of the analysis stage does not absolve the analyst of considering the relationship between the imputation stage variables, the causal model, and the missingness mechanism. It turns out that in this simple example, imputing with an analysis-stage collider is innocuous (so long as it is excluded at the analysis stage). But imputation-stage colliders can wreck MI even if they are excluded from the analysis stage.â€\n\nSee Multiple Imputation with Colliders\n\n**Donâ€™t impute missing values before your training/test split\nImputing Types full-information maximum likelihood\n\nMultiple imputation\nOne-Step Bayesian imputation\n\nMissness Types (MCAR, MAR, and MNAR)\n\nMultivariate Imputation with Chained Equation (MICE) assumes MAR\n\nMethod entails creating multiple imputations for each missing value as opposed to just one. The algorithm addresses statistical uncertainty and enables users to impute values for data of different types.\n\nStochastic Regression Imputation is problematic\n\nPopular among practitioners though\nIssues\n\nStochastic regression imputation might lead to implausible values (e.g.Â negative incomes).\nStochastic regression imputation has problems with heteroscedastic data\n\nBayesian PMM handles these issues\n\nMissingness in RCT due dropouts (aka loss to follow-up)\n\nNotes from To impute or not: the case of an RCT with baseline and follow-up measurements\n\n{mice} used for imputation\n\nBias in treatment effect due to missingness\n\nIf there are adjustment variables that affect unit dropout then bias increases as variation in treatment effect across units increases (aka hetergeneity)\n\nIn the example, a baseline measurement of the outcome variable, used an explanatory variable, was also causal of missingness. Greater values of this variable resulted in greater bias\nUsing multiple imputation resulted in less bias than just using complete cases, but still underestimated the treatment effect.\n\nIf there are no such variables, then there is no bias due to hetergeneous treatment effects\n\nComplete cases of the data can be used\n\n\nLast observation carried forward\n\nSometimes used in clinical trials because it tends to be conservative, setting a higher bar for showing that a new therapy is significantly better than a traditional therapy.\nMust assume that the previous value (e.g.Â 2008 score) is similar to the ahead value (e.g.Â 2010 score).\nInformation about trajectories over time is thrown away.\n\n\nAssessment of Imputations\n\nSee {naniar} vignette - Expanding Tidy Data Principles to Facilitate Missing Data Exploration, Visualization and Assessment of Imputations | Journal of Statistical Software"
  },
  {
    "objectID": "qmd/missingness.html#sec-missing-caim",
    "href": "qmd/missingness.html#sec-missing-caim",
    "title": "Missingness",
    "section": "Choosing an Imputation Method",
    "text": "Choosing an Imputation Method\n\n** Donâ€™t use this. Just putting it here to be aware of **) Standard Procedure for choosing an imputation method\n\nIssues\n\nSome methods will be favored based on the metric used\n\nConditional means methods (RMSE)\nConditional medians methos (MAE) Chosen methods tend to artificially strengthen the association between variables. As a consequence, statistical estimation and inference techniques applied to the so-imputed data set can be invalid.\n\n\nSteps\n\nSelect some observations\nSet their status to missing\nImpute them with different methods\nCompare their imputation accuracy\n\nFor numeric variables, RMSE or MAE typically used\nFor categoricals, percentage of correct predictions (PCP)\n\n\n\nInitial Considerations\n\nIf a datasetâ€™s feature has missing data in more than 80% of its records, it is probably best to remove that feature altogether.\nIf a feature with missing values is strongly correlated with other missing values, itâ€™s worth considering using advanced imputation techniques that use information from those other features to derive values to replace the missing data.\nIf a featureâ€™s values are missing not at random (MNAR), remove methods like MICE from consideration. I-Score {Iscores}, Paper\nA proper scoring rule metric\nConsistent for MCAR, but MAR requires additional assumptions\n\nâ€œvalid under missing at random (MAR) if we restrict the random projections in variable space to always include all variables, which in turn requires access to some complete observationsâ€\n\nKinda complicated. I need to read the paper"
  },
  {
    "objectID": "qmd/missingness.html#sec-missing-bayes",
    "href": "qmd/missingness.html#sec-missing-bayes",
    "title": "Missingness",
    "section": "Bayesian",
    "text": "Bayesian\n\nPredictive Mean Matching (PMM)\n\nNotes from:\n\nPredictive Mean Matching Imputation (Theory & Example in R)\nPredictive Mean Matching Imputation in R (mice Package Example)\n\nUses a bayesian regression to predict a missing value, then randomly picks a value from a group of observed values that are closest to the predicted value.\nSteps\n\nEstimate a linear regression model:\n\nUse the variable we want to impute as Y.\nUse a set of good predictors as X (Guidelines for the selection of X can be found in van Buuren, 2012, p.Â 128).\nUse only the observed values of X and Y to estimate the model.\n\nDraw randomly from the posterior predictive distribution of Î²^ and produce a new set of coefficients Î²âˆ—.\n\nThis bayesian step is needed for all multiple imputation methods to create some random variability in the imputed values.\n\nCalculate predicted values for observed and missing Y.\n\nUse Î²^ to calculate predicted values for observed Y.\nUse Î²âˆ— to calculate predicted values for missing Y.\n\nFor each case where Y is missing, find the closest predicted values among cases where Y is observed.\n\nExample:\n\nYi is missing. Its predicted value is 10 (based on Î²âˆ—).\nOur data consists of five observed cases of Y with the values 6, 3, 22, 7, and 12.\nIn step 3, we predicted the values 7, 2, 20, 9, and 13 for these five observed cases (based on Î²^).\nThe predictive mean matching algorithm selects the closest observed values (typically three cases) to our missing value Yi. Hence, the algorithm selects the values 7, 9, and 13 (the closest values to 10).\n\n\nDraw randomly one of these three close cases and impute the missing value Yi with the observed value of this close case.\n\nExample continued:\n\nThe algorithm draws randomly from 6, 7, and 12 (the observed values that correspond to the predicted values 7, 9, and 13).\nThe algorithm chooses 12 and substitutes this value to Yi.\n\n\nIn case of multiple imputation (which I strongly advise), steps 1-5 are repeated several times.\n\nEach repetition of steps 1-5 creates a new imputed data set.\nWith multiple imputation, missing data is typically imputed 5 times.\n\n\nExample\ndata_imp &lt;- \n  complete(mice(data,\n           m = 5,\n           method = \"pmm\"))\n\nm is the number of times to impute the data\ncomplete formats the data into different shapes according to an action argument\nRunning parmice instead of mice imputes in parallel"
  },
  {
    "objectID": "qmd/missingness.html#sec-missing-multimp",
    "href": "qmd/missingness.html#sec-missing-multimp",
    "title": "Missingness",
    "section": "Multiple Imputation",
    "text": "Multiple Imputation\n\nAKA â€œmultiplyâ€ imputed data\nPackages\n\n{NPBayesImputeCat}: Non-Parametric Bayesian Multiple Imputation for Categorical Data\n\nProvides routines to i) create multiple imputations for missing data and ii) create synthetic data for statistical disclosure control, for multivariate categorical data, with or without structural zeros\nImputations and syntheses are based on Dirichlet process mixtures of multinomial distributions, which is a non-parametric Bayesian modeling approach that allows for flexible joint modeling\nVignette\n\n\nFitting a regression model\n\nSee If you fit a model with multiply imputed data, you can still plot the line\nMethods\n\nPredict then Combine (PC)\nCombine then Predict (CP)"
  },
  {
    "objectID": "qmd/missingness.html#sec-missing-ts",
    "href": "qmd/missingness.html#sec-missing-ts",
    "title": "Missingness",
    "section": "Time Series",
    "text": "Time Series\n\nIf seasonality is present, mean, median, mode, random assignment, or previous value methods shouldnâ€™t be used."
  },
  {
    "objectID": "qmd/mlops.html#sec-mlops-mlflow",
    "href": "qmd/mlops.html#sec-mlops-mlflow",
    "title": "MLOps",
    "section": "MLflow",
    "text": "MLflow\n\nTracking\n\nMisc\n\nmlflow_log_batch(df) - A dataframe with columns key, value, step, timestamp\n\nKey can be names of metrics, params\nStep is probably for loops\nTimestamp can be from Sys.time() probably\n\nmlflow.search_runs() - querying runs\n\nAvailable columns greatly exceed those available in the experiments GUI\nExample: py\n# Create DataFrame of all runs in *current* experiment\ndf = mlflow.search_runs(order_by=[\"start_time DESC\"])\n\n# Print a list of the columns available\n# print(list(df.columns))\n\n# Create DataFrame with subset of columns\nruns_df = df[\nÂ  Â  [\nÂ  Â  Â  Â  \"run_id\",\nÂ  Â  Â  Â  \"experiment_id\",\nÂ  Â  Â  Â  \"status\",\nÂ  Â  Â  Â  \"start_time\",\nÂ  Â  Â  Â  \"metrics.mse\",\nÂ  Â  Â  Â  \"tags.mlflow.source.type\",\nÂ  Â  Â  Â  \"tags.mlflow.user\",\nÂ  Â  Â  Â  \"tags.estimator_name\",\nÂ  Â  Â  Â  \"tags.mlflow.rootRunId\",\nÂ  Â  ]\n].copy()\nruns_df.head()\n\n# add additional useful columns\nruns_df[\"start_date\"] = runs_df[\"start_time\"].dt.date\nruns_df[\"is_nested_parent\"] = runs_df[[\"run_id\",\"tags.mlflow.rootRunId\"]].apply(lambda x: 1 if x[\"run_id\"] == x[\"tags.mlflow.rootRunId\"] else 0, axis=1)\nruns_df[\"is_nested_child\"] = runs_df[[\"run_id\",\"tags.mlflow.rootRunId\"]].apply(lambda x: 1 if x[\"tags.mlflow.rootRunId\"] is not None and x[\"run_id\"] != x[\"tags.mlflow.rootRunId\"]else 0, axis=1)\nruns_df\n\n\nSet experiment name and get experiment id\n\nSyntax: mlflow_set_experiment(\"experiment_name\")\n\nThis might require a path e.g.Â â€œ/experiment-nameâ€ instead the name\n\nExperiment IDs can be passed to start_run() (see below) to ensure that the run is logged into the correct experiment\n\nExample: py\nmy_experiment = mlflow.set_experiment(\"/mlflow_sdk_test\")\nexperiment_id = my_experiment.experiment_id\nwith mlflow.start_run(experiment_id=experiment_id):\n\n\nStarting runs\n\nUsing a mlflow_log_ function automatically starts a run, but then you have to mlflow_end_run\nUsing with(mlflow_start_run(){}) stops the run automatically once the code inside the with() function is completed\nmlflow_start_run(\nÂ  Â  run_id = NULL,\nÂ  Â  experiment_id = only when run id not specified,\nÂ  Â  start_time = only when client specified,\nÂ  Â  tags = NULL,\nÂ  Â  client = NULL)\nExample: R\nlibrary(mlflow)\nlibrary(glmnet)\n\n# can format the variable outside the log_param fun or inside\nalpha &lt;- mlflow_param(args)\n\n# experiment contained inside start_run\nwith(mlflow_start_run( ) {\n\nÂ  Â  alpha_fl &lt;- mlflow_log_param(\"alpha\" = alpha)\nÂ  Â  lambda_fl &lt;- mlflow_log_param(\"lambda\" = mlflow_param(args))\n\nÂ  Â  mod &lt;- glmnet(args)\n\nÂ  Â  # preds\nÂ  Â  # error\n\nÂ  Â  # see Models section below for details\nÂ  Â  mod_crate &lt;- carrier::crate(~glmnet::glmnet.predict(mod, train_x), mod)\nÂ  Â  mlflow_log_model(mod_crate, \"model_folder\")\n\nÂ  Â  mlflow_log_metric(\"MAE\", error)\n\n})\n# this might go on the inside if you're looping the \"with\" FUN and want to log results of each loop\nmlflow_end_run()\n\n# not working, logs run, but doesn't log metrics\n# run saved script\nmlflow::mlflow_run(entry_point = \"script.R\")\nExample: python\n\n\n# End any existing runs\nmlflow.end_run()\n\n\nwith mlflow.start_run() as run:\nÂ  Â  # Turn autolog on to save model artifacts, requirements, etc.\nÂ  Â  mlflow.autolog(log_models=True)\n\n\nÂ  Â  print(run.info.run_id)\n\n\nÂ  Â  diabetes_X = diabetes.data\nÂ  Â  diabetes_y = diabetes.target\n\n\nÂ  Â  # Split data into test training sets, 3:1 ratio\nÂ  Â  (\nÂ  Â  Â  Â  diabetes_X_train,\nÂ  Â  Â  Â  diabetes_X_test,\nÂ  Â  Â  Â  diabetes_y_train,\nÂ  Â  Â  Â  diabetes_y_test,\nÂ  Â  ) = train_test_split(diabetes_X, diabetes_y, test_size=0.25, random_state=42)\n\n\nÂ  Â  alpha = 0.9\nÂ  Â  solver = \"cholesky\"\nÂ  Â  regr = linear_model.Ridge(alpha=alpha, solver=solver)\n\n\nÂ  Â  regr.fit(diabetes_X_train, diabetes_y_train)\n\n\nÂ  Â  diabetes_y_pred = regr.predict(diabetes_X_test)\n\n\nÂ  Â  # Log desired metrics\nÂ  Â  mlflow.log_metric(\"mse\", mean_squared_error(diabetes_y_test, diabetes_y_pred))\nÂ  Â  mlflow.log_metric(\nÂ  Â  Â  Â  \"rmse\", sqrt(mean_squared_error(diabetes_y_test, diabetes_y_pred))\n\nCustom run names\n\nExample: py\n\n\n# End any existing runs\nmlflow.end_run()\n\n\n# Explicitly name runs\ntoday = dt.today()\n\nrun_name = \"Ridge Regression \" + str(today)\n\nwith mlflow.start_run(run_name=run_name) as run:\nPreviously unlogged metrics can be retrieved retroactively with the run id\n# py\nwith mlflow.start_run(run_id=\"3fcf403e1566422493cd6e625693829d\") as run:\nÂ  Â  mlflow.log_metric(\"r2\", r2_score(diabetes_y_test, diabetes_y_pred))\n\nThe run_id can either be extracted by print(run.info.run_id) from the previous run, or by querying mlflow.search_runs() (See Misc above).\n\n\nNested Runs\n\nUseful for evaluating and logging parameter combinations to determine the best model (i.e.Â grid search), they also serve as a great logical container for organizing your work. With the ability to group experiments, you can compartmentalize individual data science investigations and keep your experiments page organized and tidy.\nExample: py; start a nested run\n# End any existing runs\nmlflow.end_run()\n\n\n# Explicitly name runs\nrun_name = \"Ridge Regression Nested\"\n\n\nwith mlflow.start_run(run_name=run_name) as parent_run:\nÂ  Â  print(parent_run.info.run_id)\n\n\nÂ  Â  with mlflow.start_run(run_name=\"Child Run: alpha 0.1\", nested=True):\nÂ  Â  Â  Â  # Turn autolog on to save model artifacts, requirements, etc.\nÂ  Â  Â  Â  mlflow.autolog(log_models=True)\n\n\nÂ  Â  Â  Â  diabetes_X = diabetes.data\nÂ  Â  Â  Â  diabetes_y = diabetes.target\n\n\nÂ  Â  Â  Â  # Split data into test training sets, 3:1 ratio\nÂ  Â  Â  Â  (\nÂ  Â  Â  Â  Â  Â  diabetes_X_train,\nÂ  Â  Â  Â  Â  Â  diabetes_X_test,\nÂ  Â  Â  Â  Â  Â  diabetes_y_train,\nÂ  Â  Â  Â  Â  Â  diabetes_y_test,\nÂ  Â  Â  Â  ) = train_test_split(diabetes_X, diabetes_y, test_size=0.25, random_state=42)\n\n\nÂ  Â  Â  Â  alpha = 0.1\nÂ  Â  Â  Â  solver = \"cholesky\"\nÂ  Â  Â  Â  regr = linear_model.Ridge(alpha=alpha, solver=solver)\n\n\nÂ  Â  Â  Â  regr.fit(diabetes_X_train, diabetes_y_train)\n\nÂ  Â  Â  Â  diabetes_y_pred = regr.predict(diabetes_X_test)\n\n\nÂ  Â  Â  Â  # Log desired metrics\nÂ  Â  Â  Â  mlflow.log_metric(\"mse\", mean_squared_error(diabetes_y_test, diabetes_y_pred))\nÂ  Â  Â  Â  mlflow.log_metric(\nÂ  Â  Â  Â  Â  Â  \"rmse\", sqrt(mean_squared_error(diabetes_y_test, diabetes_y_pred))\nÂ  Â  Â  Â  )\nÂ  Â  Â  Â  mlflow.log_metric(\"r2\", r2_score(diabetes_y_test, diabetes_y_pred))\n\nalpha 0.1 is the parameter value being evaluated\n\nExample: py; add child runs\n# End any existing runs\nmlflow.end_run()\n\nwith mlflow.start_run(run_id=\"61d34b13649c45699e7f05290935747c\") as parent_run:\nÂ  Â  print(parent_run.info.run_id)\n\n\nÂ  Â  with mlflow.start_run(run_name=\"Child Run: alpha 0.2\", nested=True):\nÂ  Â  Â  Â  # Turn autolog on to save model artifacts, requirements, etc.\nÂ  Â  Â  Â  mlflow.autolog(log_models=True)\n\n\nÂ  Â  Â  Â  diabetes_X = diabetes.data\nÂ  Â  Â  Â  diabetes_y = diabetes.target\n\n\nÂ  Â  Â  Â  # Split data into test training sets, 3:1 ratio\nÂ  Â  Â  Â  (\nÂ  Â  Â  Â  Â  Â  diabetes_X_train,\nÂ  Â  Â  Â  Â  Â  diabetes_X_test,\nÂ  Â  Â  Â  Â  Â  diabetes_y_train,\nÂ  Â  Â  Â  Â  Â  diabetes_y_test,\nÂ  Â  Â  Â  ) = train_test_split(diabetes_X, diabetes_y, test_size=0.25, random_state=42)\n\n\nÂ  Â  Â  Â  alpha = 0.2\nÂ  Â  Â  Â  solver = \"cholesky\"\n\n\nÂ  Â  Â  Â  regr = linear_model.Ridge(alpha=alpha, solver=solver)\n\nÂ  Â  Â  Â  regr.fit(diabetes_X_train, diabetes_y_train)\n\n\nÂ  Â  Â  Â  diabetes_y_pred = regr.predict(diabetes_X_test)\n\n\nÂ  Â  Â  Â  # Log desired metrics\nÂ  Â  Â  Â  mlflow.log_metric(\"mse\", mean_squared_error(diabetes_y_test, diabetes_y_pred))\nÂ  Â  Â  Â  mlflow.log_metric(\nÂ  Â  Â  Â  Â  Â  \"rmse\", sqrt(mean_squared_error(diabetes_y_test, diabetes_y_pred))\nÂ  Â  Â  Â  )\nÂ  Â  Â  Â  mlflow.log_metric(\"r2\", r2_score(diabetes_y_test, diabetes_y_pred))\n\nAdd to nested run by using parent run id, e.g.Â run_id=â€œ61d34b13649c45699e7f05290935747câ€\n\nObtained by print(parent_run.info.run_id) from the previous run or querying via mlflow.search_runs (see below)\n\n\n\nQuery Runs\n\nAvailable columns greatly exceed those available in the experiments GUI\nExample: py; Create Runs df\n# Create DataFrame of all runs in *current* experiment\ndf = mlflow.search_runs(order_by=[\"start_time DESC\"])\n\n\n# Print a list of the columns available\n# print(list(df.columns))\n\n\n# Create DataFrame with subset of columns\nruns_df = df[\nÂ  Â  [\nÂ  Â  Â  Â  \"run_id\",\nÂ  Â  Â  Â  \"experiment_id\",\nÂ  Â  Â  Â  \"status\",\nÂ  Â  Â  Â  \"start_time\",\nÂ  Â  Â  Â  \"metrics.mse\",\nÂ  Â  Â  Â  \"tags.mlflow.source.type\",\nÂ  Â  Â  Â  \"tags.mlflow.user\",\nÂ  Â  Â  Â  \"tags.estimator_name\",\nÂ  Â  Â  Â  \"tags.mlflow.rootRunId\",\nÂ  Â  ]\n].copy()\nruns_df.head()\n\n\n# add additional useful columns\nruns_df[\"start_date\"] = runs_df[\"start_time\"].dt.date\nruns_df[\"is_nested_parent\"] = runs_df[[\"run_id\",\"tags.mlflow.rootRunId\"]].apply(lambda x: 1 if x[\"run_id\"] == x[\"tags.mlflow.rootRunId\"] else 0, axis=1)\nruns_df[\"is_nested_child\"] = runs_df[[\"run_id\",\"tags.mlflow.rootRunId\"]].apply(lambda x: 1 if x[\"tags.mlflow.rootRunId\"] is not None and x[\"run_id\"] != x[\"tags.mlflow.rootRunId\"]else 0, axis=1)\nruns_df\nQuery Runs Object\n\nExample: Number of runs per start date\n\npd.DataFrame(runs_df.groupby(\"start_date\")[\"run_id\"].count()).reset_index()\nExample: How many runs have been tested for each algorithm?\n\npd.DataFrame(runs_df.groupby(\"tags.estimator_name\")[\"run_id\"].count()).reset_index()\n\n\n\n\n\nProjects\n\nName of the file is standard - â€œMLprojectâ€\n\nyaml file but he didnâ€™t give it an extension\nMulti-Analysis flows take the output of one script and input to another. The first script outputs the object somewhere in the working dir or a sub dir. The second script takes that object as a parameter with value = path.,\n\ne.g.Â dat.csv: path. See example https://github.com/mlflow/mlflow/tree/master/examples/multistep_workflow\n\n\nExample\nname: MyProject\n\nenvir: specify dependencies using packrat snapshot (didn't go into details)\n\nentry points:\nÂ  Â  # \"main\" is the default name used. Any script name can be an entry point name.\nÂ  Â  main:\nÂ  Â  Â  Â  parameters:\nÂ  Â  Â  Â  Â  Â  # 2 methods, looks like same args as mlflow_param or mlflow_log_param\nÂ  Â  Â  Â  Â  Â  # python types used, e.g. float instead of numeric used\nÂ  Â  Â  Â  Â  Â  alpha: {type: float, default: 0.5}\nÂ  Â  Â  Â  Â  Â  lambda:\nÂ  Â  Â  Â  Â  Â  Â  Â  type: float\nÂ  Â  Â  Â  Â  Â  Â  Â  default: 0.5\nÂ  Â  Â  Â  # CLI commands to execute the script\nÂ  Â  Â  Â  # sigh, he used -P in the video and -r on the github file\nÂ  Â  Â  Â  Â  Â  Â  Â  # he used a -P for each param when executing from CLI, so that might be correct\nÂ  Â  Â  Â  Â  Â  Â  Â  # Although that call to Rscript makes me think it might not be correct\nÂ  Â  Â  Â  command: \"Rscript &lt;script_name&gt;.R -P alpha={alpha} -P lambda={lambda}\"\nÂ  Â  Â  Â  # another one of their python examples\nÂ  Â  Â  Â  command: \"python etl_data.py --ratings-csv {ratings_csv} --max-row-limit {max_row_limit}\"\nÂ  Â  Â  Â  # This is similar to one of python their examples and it jives with Rscript practice, except there's a special function in the R script to take the args\nÂ  Â  Â  Â  # command: \"Rscript &lt;script_name&gt;.R {alpha} {lambda}\"\n\nÂ  Â  # second script, same format as 1st script\nÂ  Â  validate:\nÂ  Â  Â  Â  blah, blah\nRun script with variable values from the CLI\nmlflow\nmlflow run . --entry-point script.R -P alpha=0.5 -P lambda=0.7\n\nmlflow starts mlflow.exe\n. says run from current directory\nalso guessing entry point value is a path from the working directory\n\nRun script from github repo\n$mlflow run https://github.com/ercbk/repo --entry-point script.R -P alpha=0.5 -P lambda=0.7\n\nAdds link to repo in source col in ui for that run\nAdds link to commit (repo version) at the time of the run in the version col in the ui for that run\n\n\n\n\nModels\n\nTypically, models in R exist in memory and can be saved as .rds files. However, some models store information in locations that cannot be saved using save() or saveRDS() directly. Serialization packages can provide an interface to capture this information, situate it within a portable object, and restore it for use in new settings.\n\n{crate} - formats the model into a binary file so it can be run by a system (e.g.Â API) regardless of the language used to create it\n\nsaves it as a bin file, crate.bin\n\n{bundle} - similar for tidymodelsâ€™ objects\n\nmlflow_save_modelÂ  creates a directory with the bin file and a MLProject file\nExamples\n\nUsing a function\nmlflow_save_model(carrier::crate(function(x) {\nÂ  Â  Â  Â  Â  Â  library(glmnet)\nÂ  Â  Â  Â  Â  Â  # glmnet requires a matrix\nÂ  Â  Â  Â  Â  Â  predict(model, as.matrix(x))\n}, model = mod), \"dir_name\")\n\npredict usually takes a df but glmnet requires a matrix\nmodel = mod is the parameter being passed into the function environment\n[dir_name][var.text] is the name of the folder that will be created\n\nUsing a lambda function\nmlflow_save_model(carrier::crate(~glmnet::predict.glmnet(model, as.matrix(.x)), model = mod), \"penal_glm\")\n\nRemoved the library function (couldâ€™ve done that before as well)\n*** lambda functions require .x instead of just x ***\nThe folder name is penal_glm\n\n\nServing a model as an API from the CLI\n&gt;&gt; mlflow models serve -m file:penal_glm\n\nmlflow runs mlflow.exe\nserve says create an API\n-m is for specifying the URI of the bin file\n\nCould be an S3 bucket\nfile: says itâ€™s a local path\n\nDefault host:port 127.0.0.1:5000\n\n-h, -p can specify others\n\n*** Newdata needs to be in json column major format ***\n\nPrediction is outputted in json as well\nExample: Format in column major\njsonlite::toJSON(newdata_df, matrix = \"columnmajor\")\nExample: Send json newdata to the API\n# CLI example for \ncurl http://127.0.0.1:5000/invocations -H 'Content-Type: application/json' -d '{Â  Â  \"columns\": [\"a\", \"b\", \"c\"],Â  Â  \"data\": [[1, 2, 3], [4, 5, 6]]}'\n\n\n\n\n\nUI\n\nmlflow_ui( )\nClick date,\n\nmetric vs runs\nnotes\nartifact\n\nIf running through github\n\nlink to repo in source column for that run\nlink to commit (repo version) at the time of the run in the version column"
  },
  {
    "objectID": "qmd/mlops.html#sec-mlops-targets",
    "href": "qmd/mlops.html#sec-mlops-targets",
    "title": "MLOps",
    "section": "Targets",
    "text": "Targets\n\nMisc\n\nuse_targets\n\nCreates a â€œ_targets.Râ€ file in the projectâ€™s root directory\n\nConfigures and defines the pipeline\n\nload packages\nHPC settings\nLoad Functions from scripts\nTarget pipeline\n\n\nFile has commented lines to guide you through the process\n\nCheck Pipeline\n\ntar_manifest(fields = command)\n\nlists names of targets and the functions to execute them\n\ntar_visnetwork()\n\nShows target dependency graph\nCould be slow if you have a lot of targets, so may want to comment in/out sections of targets and view them in batches.\n\n\nRun tar_make in the background\n\nPut into .Rprofile in project\n\nmake &lt;- function() {\nÂ  Â  job::job(\nÂ  Â  Â  Â  {{ targets::tar_make() }},\nÂ  Â  Â  Â  title = \"&lt;whatever&gt;\"\nÂ  Â  )\n}\nGet a target from another project\nwithr::with_dir(\nÂ  Â  Â  Â  Â  \"~/workflows/project_name/\",\nÂ  Â  Â  Â  Â  targets::tar_load(project_name)\nÂ  Â  Â  )\n\n\n\nTarget Pipeline\n\nExample\nlist(\nÂ  Â  tar_target(file, \"data.csv\", format = \"file\"),\nÂ  Â  tar_target(data, get_data(file)),\nÂ  Â  tar_target(model, fit_model(data)),\nÂ  Â  tar_target(plot, plot_model(model, data))\n)\n\n1st arg is the target name (e.g.Â file, data, model, plot)\n2nd arg is a function\n\nFunction inputs are target names\nExcept first target which has a file name for the 2nd arg\n\nâ€œformatâ€ arg says that this target is a file and if the contents change, a re-hash should be triggered.\n\n\n\ntar_make() - Execute pipeline\n\nOutput saved in _targets &gt;&gt; objects\n\ntar_read(target_name) - Reads the output of a target\n\ne.g.Â If itâ€™s a plot output, a plot will be rendered in the viewer."
  },
  {
    "objectID": "qmd/mlops.html#sec-mlops-dask",
    "href": "qmd/mlops.html#sec-mlops-dask",
    "title": "MLOps",
    "section": "Dask",
    "text": "Dask\n\nMisc\n\nNotes from Saturn Dask in the Cloud video\nXGBoost, RAPIDS, LightGLM libraries can natively recognize Dask DataFrames and use parallelize using Dask\n{{dask-ml}} can be used to simplify training multiple models in parallel\nPyTorch DDP (Distributed Data Parallel)\n\n{{dask_pytorch_ddp}} for Saturn\nEach GPU has itâ€™s own version of the model and trains concurrently on a data batch\nResults are shared between GPUs and a combined gradient is computed\n\nfrom dask_pytorch_ddp import dispatch\nfutures = dispatch.run(dask_client, model_training_function)\n\n\n\nBasic Usage\n\nDask Collections\n\nDask DataFrames - Mimics Pandas DataFrames\n\nTheyâ€™re essentially collection of pandas dfs spread across workers\n\nDask Arrays - Mimics NumPy\nDask Bags - Mimics map, filter, and other actions on collections\n\nStorage\n\nCloud storage (e.g.Â S3, EFS) can be queried by Dask workers\nSaturn also provides shared folders that attach directly to Dask workers.\n\nUse Locally\nimport dask.dataframe as dd\nddf = dd.read.csv(\"data/example.csv\")\nddf.groupby('col_name').mean().compute()\n\ncompute starts the computation and collects the results.\n\nEvidently other functions can have this effect (see example). Need to check docs.\n\n\nSpecify chunks and object type\nfrom dask import dataframe as dd\nddf = dd.read_csv(r\"FILEPATH\", dtype={'SimillarHTTP': 'object'},blocksize='64MB')\nFit sklearn models in parallel\nimport joblib\nfrom dask.distributed import Client\nclient = Client(processes=False)\n\nwith joblib.parallel_backend(\"dask\"):\nÂ  Â  rf.fit(X_train, y_train)\n\nNot sure if client is needed here\n\n\n\n\nEvaluation Options\n\nDask Delayed\n\nFor user-defined functions â€” allows dask to parallelize and lazily compute them\n\n@dask.delayed\ndef double(x):\nÂ  Â  return x*2\n\n@dask.delayed\ndef add(x, y):\nÂ  Â  return x + y\n\na = double(3)\nb = double(2)\ntotal = add(a,b) # chained delayed functions\ntotal.compute() # evaluates the functions\nFutures\n\nEvaluated immediately in the background\nSingle function\ndef double(x):\nÂ  Â  return x*2\nfuture = client.submit(double, 3)\nIterable\nlearning_rates = np.arange(0.0005, 0.0035, 0.0005)\nfutures = client.map(train_model, learning_rates) # map(function, iterable)\ngathered_futures = client.gather(futures)\nfutures_computed = client.compute(futures_gathered, resources = {\"gpu\":1})\n\nresources tells dask to only send one task per gpu-worker in this case\n\n\n\n\n\nMonitoring\n\nLogging\nfrom distributed.worker import logger\n@dask.delayed\ndef log():\nÂ  Â  logger.info(f'This is sent to the worker log')\n# ANN example\nlogger.info(\nÂ  Â  f'{datetime.datetime.now().isoformat(){style='color: #990000'}[}]{style='color: #990000'} - lr {lr} - epoch {epoch} - phase {phase} - loss {epoch_loss}'\n)\n\nDonâ€™t need a separate log function. You can just include logger.info in the model training function.\n\nBuilt-in dashboard\n\n\nTask Stream - each bar is a worker; colors show activity category (e.g.Â busy, finished, error, etc.)\n\n\n\n\nError Handling\n\nThe Dask scheduler will continue the computation and start another worker if one fails.\n\nIf your code is what causing the error then it wonâ€™t matter\n\nLibraries\nimport traceback\nfrom distributed.client import wait, FIRST_COMPLETED\nCreate a queue of futures\nqueue = client.compute(results)\nfutures_idx = {fut: i for i, fut in enumerate(queue){style='color: #990000'}[}]{style='color: #990000'}\nresults = [None for x in range(len(queue))]\n\nSince weâ€™re not passsing [sync = True]{arg-text}, we immediately get back futures which represent the computation that hasnâ€™t been completed yet.\nEnumerate each item in the future\nPopulate the â€œresultsâ€ list with Nones for now\n\nWait for results\nwhile queue:\nÂ  Â  result = wait(queue, return_when = FIRST_COMPLETED)\nFutures either succeed (â€œfinishedâ€) or they error (chunk included in while loop)\nÂ  Â  for future in result.done:\nÂ  Â  Â  Â  index = futures_idx[future]Â  Â  Â  Â \nÂ  Â  Â  Â  if future.status == 'finished':\nÂ  Â  Â  Â  Â  Â  print(f'finished computation #[{index}]{style='color: #990000'}')\nÂ  Â  Â  Â  Â  Â  results[index] = future.result()\nÂ  Â  Â  Â  else:\nÂ  Â  Â  Â  Â  Â  print(f'errored #[{index}]{style='color: #990000'}')\nÂ  Â  Â  Â  Â  Â  try:\nÂ  Â  Â  Â  Â  Â  Â  Â  future.result()\nÂ  Â  Â  Â  Â  Â  except Exception as e:\nÂ  Â  Â  Â  Â  Â  Â  Â  results[index] = e\nÂ  Â  Â  Â  Â  Â  Â  Â  traceback.print_exc()\n\nÂ  Â  queue = result.not_done\n\nfuture.status contains results of computation so you know what to retry\nSucceeds: Print that it finished and store the result\nError: Store exception and print the stack trace\nSet queue to those futures that havenâ€™t been completed\n\n\n\n\nCloud\n\nSaturn\n\nStarting Dask from Jupyter Server thatâ€™s running JupyterLab, the Dask Cluster will have all the libraries loaded into Jupyter Server\nOptions\n\nSaturn Cloud UI\n\nOnce you start a Jupyter Server, thereâ€™s a button to click that allows you to specify and run a Dask Cluster\n\nDo work on a JupyterLab notebook\n\nBenefits\n\nIn a shared environment\nLibraries automatically get loaded onto the Dask cluster\n\n\nProgrammatically (locally)\n\nSSH into Jupyter Server (which is connected to the Dask Cluster) at Saturn\nConnect directly to Dask Cluster at Saturn\nCons\n\nHave to load packages locally and on Jupyter Server and/or Dask Cluster\nMake sure versions/environments match\n\nConnection (basic)\nfrom dask_saturn import SaturnCluster\ncluster = SaturnCluster()\nclient = Client(cluster)\n\n\nExample\n\nFrom inside a jupyterlab notebook on a jupyter server with a dask cluster running\nImports\nimport dask.dataframe as dd\nimport numpy as np\nfrom dask.diagnostics import ProgressBar\nfrom dask.distributed import Client, wait\nfrom dask_saturn import SaturnCluster\nStart Cluster\nn_workers = 3\ncluster = SaturnCluster()\nclient = Client(cluster)\nclient.wait_for_workers(n_workers = n_workers) # if workers aren't ready, wait for them to spin up before proceding\nclient.restart()\n\nFor bigger tasks like training ANNs on GPUs, you to specify a gpu instance type (i.e.Â â€œworker_sizeâ€) and scheduler with plenty of memory\ncluster = SaturnCluster(\nÂ  Â  n_workers = n_workers,\nÂ  Â  scheduler_size = 'large',\nÂ  Â  worker_size = 'g3dnxlarge'\n)\n\nIf youâ€™re bringing back sizable results from your workers, your scheduler needs plenty of RAM.\n\n\nUpload Code files\n\n1 file - client.upload_file(\"functions.py\")\n\nUploads a single file to all workers\n\nDirectory\nfrom dask_saturn import RegesterFiles, sync_files\nclient.register_worker_plugin(RegisterFiles())\nsync_files(client, \"functions\")\nclient.restart()\n\nPlugin allows you to sync directory among workersjjj\n\n\nData\nddf = dd.read_parquet(\nÂ  Â  \"/path/to/file.pq\"\n)\n\nddf = ddf.persist()\n_ = wait(ddf) # halts progress until persistance is done\n\nPersist saves the data to the Dask workers\n\nNot necessary, but if you didnâ€™t, then each time you call .compute() youâ€™d have to reload the file\n\n\nDo work\nddf[\"signal\"] = (\nÂ  Â  ddf[\"ask_close\"].rolling(5 * 60).mean() - ddf[\"ask_close\"].rolling(20 * 60).mean()\n)\n\n# ... blah, blah, blah\n\nddf[\"total\"] = ddf[\"return\"].cumsum().apply(np.exp, meta = \"return\", \"float64\"))\n\nSyntax just like pandas except:\n\nmeta = (column, type) - Daskâ€™s lazy computation sometimes gets column types wrong, so this specifies types explicitly\n\n\nCompute and bring back to client\ntotal_returns = ddf[\"total\"].tail(1)\nprint(total_returns)\n\nEvidently .tail does what compute is supposed to do."
  },
  {
    "objectID": "qmd/mlops.html#sec-mlops-targets-vetr",
    "href": "qmd/mlops.html#sec-mlops-targets-vetr",
    "title": "MLOps",
    "section": "Vetiver",
    "text": "Vetiver\n\nMisc\n\nDocs\nAvailable in Python\n\n\nHelps with 3 aspects of MLOps\n\nVersioning\n\nKeeps track of metadata\nHelpful during retraining\n\nDeploying\n\nUtilizes REST APIs to serve models\n\nMonitoring\n\nTracks model performance\n\n\nVersioning\n\nWrite model to a {pins} board (storage)\nlibrary(pins)\nboard &lt;- board_connect()\nboard %&gt;% vetiver_pin_write(v)\n\nboard_* has many options including the major cloud providers\n\nHere â€œconnectâ€ stands for Posit Connect\n\nâ€œvâ€ is a {tidymodels} workflow obj\n\nCreate a REST API\nlibrary(plumber)\npr() %&gt;%\n  vetiver_api(v)\n#&gt; # Plumber router with 2 endpoints, 4 filters, and 1 sub-router.\n#&gt; # Use `pr_run()` on this object to start the API.\n#&gt; â”œâ”€â”€[queryString]\n#&gt; â”œâ”€â”€[body]\n#&gt; â”œâ”€â”€[cookieParser]\n#&gt; â”œâ”€â”€[sharedSecret]\n#&gt; â”œâ”€â”€/logo\n#&gt; â”‚  â”‚ # Plumber static router serving from directory: /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/library/vetiver\n#&gt; â”œâ”€â”€/ping (GET)\n#&gt; â””â”€â”€/predict (POST)\n## next pipe to `pr_run()` for local API\n\nThen next step is pipe this into pr_run() to start the API locally\nHelpful for development or debugging\n\n\nDeploy\n\nPosit Connect has a 1-liner: vetiver_deploy_rsconnect()\nCreate a docker file\nvetiver_prepare_docker()\n\nCreates a docker file, renv.lock file, and plumber app that can be uploaded and deployed anywhere (e.g.Â AWS, GCP, digitalocean)\n\n\nMonitor\n\nWrite metrics to storage\nnew_metrics &lt;-\n  augment(v, housing_val) %&gt;%\n  vetiver_compute_metrics(date, \"week\", price, .pred)\n\nvetiver_pin_metrics(\n  board,\n  new_metrics, \n  \"julia.silge/housing-metrics\",\n  overwrite = TRUE\n)\n#&gt; # A tibble: 90 Ã— 5\n#&gt;    .index                 .n .metric .estimator  .estimate\n#&gt;    &lt;dttm&gt;              &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;           &lt;dbl&gt;\n#&gt;  1 2014-11-02 00:00:00   224 rmse    standard   206850.   \n#&gt;  2 2014-11-02 00:00:00   224 rsq     standard        0.413\n#&gt;  3 2014-11-02 00:00:00   224 mae     standard   140870.   \n#&gt;  4 2014-11-06 00:00:00   373 rmse    standard   221627.   \n#&gt;  5 2014-11-06 00:00:00   373 rsq     standard        0.557\n#&gt;  6 2014-11-06 00:00:00   373 mae     standard   150366.   \n#&gt;  7 2014-11-13 00:00:00   427 rmse    standard   255504.   \n#&gt;  8 2014-11-13 00:00:00   427 rsq     standard        0.555\n#&gt;  9 2014-11-13 00:00:00   427 mae     standard   147035.   \n#&gt; 10 2014-11-20 00:00:00   376 rmse    standard   248405.   \n#&gt; # â„¹ 80 more rows\nAnalyze metrics\nnew_metrics %&gt;%\n  ## you can operate on your metrics as needed:\n  filter(.metric %in% c(\"rmse\", \"mae\"), .n &gt; 20) %&gt;%\n  vetiver_plot_metrics() + \n  ## you can also operate on the ggplot:\n  scale_size(range = c(2, 5))"
  },
  {
    "objectID": "qmd/model-building-brms.html#sec-modbld-brms-misc",
    "href": "qmd/model-building-brms.html#sec-modbld-brms-misc",
    "title": "brms",
    "section": "Misc",
    "text": "Misc\n\nHow brms fits models\n\nThe marginal Sigma posterior distribution is the distribution of residualsÂ \nSampling the posterior\n\nLogistic\n# sampling the posterior\nf &lt;-Â \nÂ  fitted(b3.1,Â \nÂ  Â  Â  Â  summary = F,Â  Â  Â  Â  Â  Â  # says we want simulated draws and not summary statsÂ \nÂ  Â  Â  Â  scale = \"linear\") %&gt;%Â  Â  # linear outputs probabilitiesÂ \nÂ  as_tibble() %&gt;%Â \nÂ  set_names(\"p\")\n\nOptimization\n\n{cmdstanr} as the backend\n\nIn the brms model function\nbackend=\"cmdstanr\", stan_model_args=list(stanc_options = list(\"O1\"))\n\nstanc_options = list(\"O1\") might be made default in a future update\nSee thread for details\n\nSet using options: options(brms.backend = \"cmdstanr\""
  },
  {
    "objectID": "qmd/model-building-brms.html#sec-modbld-brms-lr",
    "href": "qmd/model-building-brms.html#sec-modbld-brms-lr",
    "title": "brms",
    "section": "Linear Regression",
    "text": "Linear Regression\n\nExample: lm, Cont ~ Cont (SR, Ch.4)\nb4.3 &lt;-Â  brm(data = dat, family = gaussian,\nÂ  Â  Â  height ~ 1 + weight,\nÂ  Â  Â  prior = c(prior(normal(178, 100), class = Intercept),\nÂ  Â  Â  Â  Â  Â  Â  Â  prior(normal(0, 10), class = b),\nÂ  Â  Â  Â  Â  Â  Â  Â  prior(uniform(0, 50), class = sigma)),\nÂ  Â  Â  iter = 41000, warmup = 40000, chains = 4, cores = 4,\nÂ  Â  Â  seed = 4, backend = \"cmdstanr\",\n)\nExample: Multivariable, Cont ~ Cont + Cont (SR, Ch.5)\nb5.3 &lt;-Â  brm(data = dat, family = gaussian,\nÂ  Â  Â  Divorce ~ 1 + Marriage_s + MedianAgeMarriage_s,\nÂ  Â  Â  prior = c(prior(normal(10, 10), class = Intercept),\nÂ  Â  Â  Â  Â  Â  Â  Â  prior(normal(0, 1), class = b),\nÂ  Â  Â  Â  Â  Â  Â  Â  prior(exponential(1), class = sigma)),\nÂ  Â  Â  iter = 2000, warmup = 1000, chains = 4, cores = 4,\nÂ  Â  Â  seed = 5, backend = \"cmdstanr\", file = \"fits/b05.03\")\n\nOutcome = Divorce rate, Predictor 1 = stateâ€™s marriage rate, Predictor 2 = stateâ€™s median marriage age\n\nExample: Binary/Cat, Cont ~ Binary (SR, Ch. 5)\ndata(Howell1, package = \"rethinking\")Â \nd &lt;- Howell1\nd &lt;-Â  d %&gt;%Â \nÂ  mutate(sex = ifelse(male == 1, 2, 1), # create index variable\nÂ  Â  Â  Â  sex = factor(sex)) # transforming it into a factor tells brms it's indexed\nb5.8 &lt;-Â  brm(data = d,Â \nÂ  Â  Â  Â  Â  Â  family = gaussian,Â \nÂ  Â  Â  Â  Â  Â  height ~ 0 + sex, # \"0 +\" notation means calculate separate intercepts for each category\nÂ  Â  Â  Â  Â  Â  prior = c(prior(normal(178, 20), class = b),Â \nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  prior(exponential(1), class = sigma)),Â \nÂ  Â  Â  Â  Â  Â  iter = 2000, warmup = 1000, chains = 4, cores = 4,Â \nÂ  Â  Â  Â  Â  Â  seed = 5, backend = \"cmdstanr\",\nÂ  Â  Â  Â  Â  Â  file = \"fits/b05.08\")\nprint(b5.8)\n\nFor nominal variable, increase the s.d. of the Î± prior (as compared to the binary prior) â€œto allow the different  to disperse, if the data wants them to.â€\nâ€œI encourage you to play with that prior and repeatedly re-approximate the posterior so you can see how the posterior differences among the categories depend upon it.â€\n\nExample: Multi-Categorical, Cont ~ Cat + Cat (SR, Ch. 5)\nb5.11 &lt;-Â  brm(data = d,Â \nÂ  Â  Â  family = gaussian,Â \nÂ  Â  Â  # bf() is an alias for brmsformula() that lets you specify model formulas\nÂ  Â  Â  bf(kcal.per.g_s ~ 0 + a + h,Â \nÂ  Â  Â  Â  a ~ 0 + clade,Â \nÂ  Â  Â  Â  h ~ 0 + house,Â \nÂ  Â  Â  Â  # tells brm we're using non-linear syntax\nÂ  Â  Â  Â  nl = TRUE),Â \nÂ  Â  Â  prior = c(prior(normal(0, 0.5), nlpar = a),Â \nÂ  Â  Â  Â  Â  Â  Â  Â  prior(normal(0, 0.5), nlpar = h),Â \nÂ  Â  Â  Â  Â  Â  Â  Â  prior(exponential(1), class = sigma)),Â \nÂ  Â  Â  iter = 2000, warmup = 1000, chains = 4, cores = 4,Â \nÂ  Â  Â  seed = 5, backend = \"cmdstanr\",\nÂ  Â  Â  file = \"fits/b05.11\")\n\n(As of May 2022) When using the typical formula syntax with more than one categorical variable, {brms} drops a category from every categorical variable except for the first one in the formula.\n{brms} was orginally designed to wrap Stan multi-level models w/lme4 syntax, so maybe that has something do with it.\nKurz has links and discussion in Section 5.3.2 of his ebook\n\nExample: Interaction, continuous * categorical (SR, Ch.8)\n# same as coding for cat vars except adding a slope that is also conditioned on the index\nb8.3 &lt;-Â \nÂ  brm(data = dd,Â \nÂ  Â  Â  family = gaussian,Â \nÂ  Â  Â  # bf = \"brms formula\"\nÂ  Â  Â  bf(log_gdp_std ~ 0 + a + b * rugged_std_c,Â \nÂ  Â  Â  Â  a ~ 0 + cid,Â \nÂ  Â  Â  Â  b ~ 0 + cid,Â \nÂ  Â  Â  Â  # nl = \"nonlinear\" syntax\nÂ  Â  Â  Â  nl = TRUE),Â \nÂ  Â  Â  prior = c(prior(normal(1, 0.1), class = b, coef = cid1, nlpar = a),Â \nÂ  Â  Â  Â  Â  Â  Â  Â  prior(normal(1, 0.1), class = b, coef = cid2, nlpar = a),Â \nÂ  Â  Â  Â  Â  Â  Â  Â  prior(normal(0, 0.3), class = b, coef = cid1, nlpar = b),Â \nÂ  Â  Â  Â  Â  Â  Â  Â  prior(normal(0, 0.3), class = b, coef = cid2, nlpar = b),Â \nÂ  Â  Â  Â  Â  Â  Â  Â  prior(exponential(1), class = sigma)),Â \nÂ  Â  Â  iter = 2000, warmup = 1000, chains = 4, cores = 4,Â \nÂ  Â  Â  seed = 8, backend = \"cmdstanr\",\nÂ  Â  Â  file = \"fits/b08.03\")\n\nCategorical variable needs an interaction spec and an intercept (main effect) spec\n\nExample: Interaction, continuous * continuous (SR, Ch.8)\nb8.5 &lt;-Â \nÂ  brm(data = d,Â \nÂ  Â  Â  family = gaussian,Â \nÂ  Â  Â  blooms_std ~ 1 + water_cent + shade_cent + water_cent:shade_cent,Â \nÂ  Â  Â  prior = c(prior(normal(0.5, 0.25), class = Intercept),Â \nÂ  Â  Â  Â  Â  Â  Â  Â  prior(normal(0, 0.25), class = b, coef = water_cent),Â \nÂ  Â  Â  Â  Â  Â  Â  Â  prior(normal(0, 0.25), class = b, coef = shade_cent),Â \nÂ  Â  Â  Â  Â  Â  Â  Â  prior(normal(0, 0.25), class = b, coef = \"water_cent:shade_cent\"),Â \nÂ  Â  Â  Â  Â  Â  Â  Â  prior(exponential(1), class = sigma)),Â \nÂ  Â  Â  iter = 2000, warmup = 1000, chains = 4, cores = 4,Â \nÂ  Â  Â  seed = 8, backend = \"cmdstanr\",\nÂ  Â  Â  file = \"fits/b08.05\")\nExample: Interaction, categorical * categorial (mc-stan question)(SR, Lecture 9 video, Ch. 11 Note)\nfit &lt;-\nÂ  brm(bf(y ~ 0 + a + b + c,\nÂ  Â  Â  Â  a ~ 0 + F1,\nÂ  Â  Â  Â  b ~ 0 + F2,\nÂ  Â  Â  Â  # this is the interaction\nÂ  Â  Â  Â  c ~ (0 + F1) : (0 + F2),\nÂ  Â  Â  Â  nl = TRUE),\nÂ  Â  data = dat)\n\n# interaction-only model (should include main effects)\nbrm_mod &lt;- brm(data = ucb_01,\nÂ  Â  Â  Â  Â  Â  Â  family = bernoulli,\nÂ  Â  Â  Â  Â  Â  Â  bf(admitted ~ 0 + gd,Â \nÂ  Â  Â  Â  Â  Â  Â  Â  Â  # this is the interactionÂ \nÂ  Â  Â  Â  Â  Â  Â  Â  Â  gd ~ (0 + gender) : (0 + dept),Â \nÂ  Â  Â  Â  Â  Â  Â  Â  Â  nl = TRUE),Â \nÂ  Â  Â  Â  Â  Â  Â  prior = prior(normal(0,1), nlpar = gd),Â \nÂ  Â  Â  Â  Â  Â  Â  iter = 2000, warmup = 1000, cores = 3, chains = 3,Â \nÂ  Â  Â  Â  Â  Â  Â  seed = 10, backend = \"cmdstanr\")"
  },
  {
    "objectID": "qmd/model-building-brms.html#sec-modbld-brms-logreg",
    "href": "qmd/model-building-brms.html#sec-modbld-brms-logreg",
    "title": "brms",
    "section": "Logistic Regression",
    "text": "Logistic Regression\n\nExample: Logistic, cont ~ cat + cat (SR, Ch.11)\nb11_bern &lt;- brm(data = dat_sim,Â \nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  family = bernoulli(link = \"logit\"),Â \nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  bf(admit ~ a + d,Â \nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  a ~ 0 + gid,Â \nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  d ~ 0 + dept,Â \nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  nl = TRUE),Â \nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  prior = c(prior(normal(0, 1), nlpar = a),Â \nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  prior(normal(0, 1), nlpar = d)),Â \nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  iter = 4000, warmup = 1000, cores = 4, chains = 4,Â \nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  seed = 11, backend = \"cmdstanr\")\nprint(b11_bern)\nÂ  Â  Â  Â  Â  Â  Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\na_gidmaleÂ  Â  Â  -1.06Â  Â  Â  0.51Â  Â  -2.05Â  Â  -0.07 1.00Â  Â  1765Â  Â  2356\na_gidfemaleÂ  Â  -0.97Â  Â  Â  0.50Â  Â  -1.96Â  Â  0.00 1.00Â  Â  1758Â  Â  2366\nd_dept1Â  Â  Â  Â  -1.54Â  Â  Â  0.51Â  Â  -2.55Â  Â  -0.54 1.00Â  Â  1773Â  Â  2419\nd_dept2Â  Â  Â  Â  -0.50Â  Â  Â  0.50Â  Â  -1.46Â  Â  0.48 1.00Â  Â  1763Â  Â  2249\n\nb11.4_bin &lt;-Â  brm(data = dat_sim,Â  Â \nÂ  Â  Â  Â  Â  Â  Â  Â  Â  family = binomial,Â \nÂ  Â  Â  Â  Â  Â  Â  Â  Â  bf(admit | trials(1) ~ a + d,Â \nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  a ~ 0 + gid,Â  Â \nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  d ~ 0 + dept,Â \nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  nl = TRUE),Â \nÂ  Â  Â  Â  Â  Â  Â  Â  Â  prior = c(prior(normal(0, 1), nlpar = a),Â \nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  prior(normal(0, 1), nlpar = d)),Â \nÂ  Â  Â  Â  Â  Â  Â  Â  Â  iter = 2000, warmup = 1000, chains = 4, cores = 4,Â \nÂ  Â  Â  Â  Â  Â  Â  Â  Â  seed = 11, backend = \"cmdstanr\")\nprint(b11_bin)\nÂ  Â  Â  Â  Â  Â  Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\na_gidmaleÂ  Â  Â  -1.02Â  Â  Â  0.52Â  Â  -2.05Â  Â  0.01 1.01Â  Â  Â  702Â  Â  Â  832\na_gidfemaleÂ  Â  -0.94Â  Â  Â  0.52Â  Â  -1.98Â  Â  0.06 1.01Â  Â  Â  688Â  Â  Â  808\nd_dept1Â  Â  Â  Â  -1.58Â  Â  Â  0.53Â  Â  -2.60Â  Â  -0.53 1.01Â  Â  Â  693Â  Â  Â  787\nd_dept2Â  Â  Â  Â  -0.53Â  Â  Â  0.52Â  Â  -1.53Â  Â  0.49 1.01Â  Â  Â  685Â  Â  Â  718\n\nSlightly different results (100ths). I feel more comfortable using the bernoulli spec if itâ€™s just a typical logistic regression.\nThe â€œ1â€ in â€œ|trials(1)â€ says that this is case-level data\n\nIncluding a | bar on the left side of a formula indicates we have extra supplementary information about our criterion. In this case, that information is that each pulled_left value corresponds to a single trial (i.e., trials(1)), which itself corresponds to theÂ  n = 1 in the model specification (above) for the outcome variable (e.g.Â pulled_left).\n\nThe rest of the brms specification is standard for having two categorial explanatory variables (e.g.Â actor, treatment) (see Ch.5 &gt;&gt; categoricals &gt;&gt; multiple nominal)\n\ni.e.Â ingredients for Logistic Regression: family = Binomial and binary_outcome|trials(1)\n\n\nExample: Multinomial Logistic Regression (SR, Ch. 11)\nb11.13io &lt;-Â \nÂ  brm(data = d,Â  Â  Â  Â \nÂ  Â  Â  # refcat sets the reference category to the 3rd levelÂ \nÂ  Â  Â  family = categorical(link = logit, refcat = 3),Â \nÂ  Â  Â  career ~ 1,Â \nÂ  Â  Â  prior = c(prior(normal(0, 1), class = Intercept, dpar = mu1),Â \nÂ  Â  Â  Â  Â  Â  Â  Â  prior(normal(0, 1), class = Intercept, dpar = mu2)),Â \nÂ  Â  Â  iter = 2000, warmup = 1000, cores = 4, chains = 4,Â \nÂ  Â  Â  seed = 11, backend - \"cmdstanr\",\nÂ  Â  Â  file = \"fits/b11.13io\")\nb11.13io\n##Â  Â  Â  Â  Â  Â  Â  Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESSÂ \n## mu1_InterceptÂ  Â  -2.01Â  Â  Â  0.15Â  Â  -2.30Â  Â  -1.73 1.00Â  Â  3324Â  Â  2631Â \n## mu2_InterceptÂ  Â  -1.53Â  Â  Â  0.12Â  Â  -1.77Â  Â  -1.29 1.00Â  Â  2993Â  Â  2768\n\nas of brms 2.12.0, â€œspecifying global priors for regression coefficients in categorical models is deprecated.â€ Meaning â€” if we want to use the same prior for both, we need to use the dpar argument for each\nThe reference level, refcat ,Â  was set to the 3rd level so the scores for the levels 1 and 2 are shown\n\nDefault is level 1.\nHad we used the brms default and used the first level of â€œcareerâ€ as the pivot (aka reference category), those lines would have instead been dpar = mu2 , dpar = mu3\n\nThese are the score values that weâ€™d get if we centered all the scores at level 3â€™s score value\n\nNothing to infer from these. We want the probabilities which are in the next section.\n\nAlternate ways to fit this model\n# verbose syntax\nb11.13io_verbose &lt;-\nÂ  brm(data = d,Â \nÂ  Â  Â  family = categorical(link = logit, refcat = 3),\nÂ  Â  Â  bf(career ~ 1,\nÂ  Â  Â  Â  mu1 ~ 1,\nÂ  Â  Â  Â  mu2 ~ 1),\nÂ  Â  Â  prior = c(prior(normal(0, 1), class = Intercept, dpar = mu1),\nÂ  Â  Â  Â  Â  Â  Â  Â  prior(normal(0, 1), class = Intercept, dpar = mu2)),\nÂ  Â  Â  iter = 2000, warmup = 1000, cores = 4, chains = 4,\nÂ  Â  Â  seed = 11,\nÂ  Â  Â  file = \"fits/b11.13io_verbose\")\n\n# nonlinear syntax\nb11.13io_nonlinear &lt;-\nÂ  brm(data = d,Â \nÂ  Â  Â  family = categorical(link = logit, refcat = 3),\nÂ  Â  Â  bf(career ~ 1,\nÂ  Â  Â  Â  nlf(mu1 ~ a1),\nÂ  Â  Â  Â  nlf(mu2 ~ a2),\nÂ  Â  Â  Â  a1 + a2 ~ 1),\nÂ  Â  Â  prior = c(prior(normal(0, 1), class = b, nlpar = a1),\nÂ  Â  Â  Â  Â  Â  Â  Â  prior(normal(0, 1), class = b, nlpar = a2)),\nÂ  Â  Â  iter = 2000, warmup = 1000, cores = 4, chains = 4,\nÂ  Â  Â  seed = 11,\nÂ  Â  Â  file = \"fits/b11.13io_nonlinear\")\n\nThe verbose (and nonlinear) syntax makes it clear that we are fitting k-1 models, since â€œcareerâ€ has 3 categories\nHad we used the brms default and used the first level of career as the pivot (aka reference category), those lines would have instead been mu2 ~ 1, mu3 ~ 1"
  },
  {
    "objectID": "qmd/model-building-brms.html#sec-modbld-brms-discrete",
    "href": "qmd/model-building-brms.html#sec-modbld-brms-discrete",
    "title": "brms",
    "section": "Discrete Distribution Models",
    "text": "Discrete Distribution Models\n\nBinomial (SR Ch. 11)\nb11.6 &lt;-Â \nÂ  brm(data = d_aggregated,Â \nÂ  Â  Â  family = binomial,Â \nÂ  Â  Â  bf(left_pulls | trials(18) ~ a + b,Â \nÂ  Â  Â  Â  a ~ 0 + actor,Â \nÂ  Â  Â  Â  b ~ 0 + treatment,Â \nÂ  Â  Â  Â  nl = TRUE),Â \nÂ  Â  Â  prior = c(prior(normal(0, 1.5), nlpar = a),Â \nÂ  Â  Â  Â  Â  Â  Â  Â  prior(normal(0, 0.5), nlpar = b)),Â \nÂ  Â  Â  iter = 2000, warmup = 1000, chains = 4, cores = 4,Â \nÂ  Â  Â  seed = 11, backend = \"cmdstanr\",\nÂ  Â  Â  file = \"fits/b11.06\")\n\nIncluding a | bar on the left side of a formula indicates we have extra supplementary information about our criterion.\n\nTo fit an aggregated binomial model with brms, we augment the  | trials() syntax where the value that goes in trials() is either a fixed number, as in this case, or variable (e.g.Â trials(applications) ) in the data indexing the number of trials, n.\n\nCan also use a logistic model, but need case-level data (e.g.Â 0/1)\n\nDeaggregate count data into 0/1 case-level data\n\ndata(UCBadmit, package = \"rethinking\")\nucb &lt;- UCBadmit %&gt;%Â \nÂ  mutate(applicant.gender = relevel(applicant.gender, ref = \"male\"))\n\n# deaggregate to 1/0\ndeagg_ucb &lt;- function(x, y) {\nÂ  UCBadmit %&gt;%\nÂ  Â  select(-applications) %&gt;%\nÂ  Â  group_by(dept, applicant.gender) %&gt;%\nÂ  Â  tidyr::uncount(weights = !!sym(x)) %&gt;%\nÂ  Â  mutate(admitted = y) %&gt;%\nÂ  Â  select(dept, gender = applicant.gender, admitted)\n}\nucb_01 &lt;- purrr::map2_dfr(c(\"admit\", \"reject\"),\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  c(1, 0),\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ~ disagg_ucb(.x, .y)\n)\n\nLogistic (SR Ch. 11)\nfull_mod &lt;- brm(bf(admitted ~ 0 + g + d + gd,Â \nÂ  Â  Â  Â  Â  Â  Â  Â  Â  g ~ 0 + gender,Â \nÂ  Â  Â  Â  Â  Â  Â  Â  Â  d ~ 0 + dept,Â \nÂ  Â  Â  Â  Â  Â  Â  Â  Â  # this is the interactionÂ \nÂ  Â  Â  Â  Â  Â  Â  Â  Â  gd ~ (0 + gender) : (0 + dept),Â \nÂ  Â  Â  Â  Â  Â  Â  Â  Â  nl = TRUE),Â \nÂ  Â  Â  Â  Â  Â  Â  Â  prior = c(prior(normal(0,1), nlpar = g),\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  prior(normal(0,1), nlpar = d),\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  prior(normal(0,1), nlpar = gd)),\nÂ  Â  Â  Â  Â  Â  Â  Â  data = ucb_01,\nÂ  Â  Â  Â  Â  Â  Â  Â  family = bernoulli,\nÂ  Â  Â  Â  Â  Â  Â  Â  iter = 2000, warmup = 1000, cores = 3, chains = 3,\nÂ  Â  Â  Â  Â  Â  Â  Â  seed = 10, backend = \"cmdstanr\")\nPoisson (SR Ch. 11)\n# cat * cont interaction modelÂ \nb11.10 &lt;-Â \nÂ  brm(data = d,Â \nÂ  Â  Â  family = poisson,Â \nÂ  Â  Â  bf(total_tools ~ a + b * log_pop_std,Â \nÂ  Â  Â  Â  a + b ~ 0 + cid,Â \nÂ  Â  Â  Â  nl = TRUE),Â \nÂ  Â  Â  prior = c(prior(normal(3, 0.5), nlpar = a),Â \nÂ  Â  Â  Â  Â  Â  Â  Â  prior(normal(0, 0.2), nlpar = b)),Â \nÂ  Â  Â  iter = 2000, warmup = 1000, chains = 4, cores = 4,Â \nÂ  Â  Â  seed = 11, backend = \"cmdstanr\",\nÂ  Â  Â  file = \"fits/b11.10\")"
  },
  {
    "objectID": "qmd/model-building-brms.html#sec-modbld-brms-nonlin",
    "href": "qmd/model-building-brms.html#sec-modbld-brms-nonlin",
    "title": "brms",
    "section": "Non-linear",
    "text": "Non-linear\n\nExample: Basis Splines (SR, Ch.4)\n\n# get recommended prior specifications\n# s is the basis function brms imports from mgcv pkg\nbrms::get_prior(data = d2,Â \nÂ  Â  Â  Â  Â  Â  Â  Â  family = gaussian,Â \nÂ  Â  Â  Â  Â  Â  Â  Â  doy ~ 1 + s(year))\n##Â  Â  Â  Â  Â  Â  Â  Â  Â  priorÂ  Â  classÂ  Â  coef group resp dpar nlpar boundÂ  Â  Â  sourceÂ \n##Â  Â  Â  Â  Â  Â  Â  Â  Â  (flat)Â  Â  Â  Â  bÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  defaultÂ \n##Â  Â  Â  Â  Â  Â  Â  Â  Â  (flat)Â  Â  Â  Â  b syear_1Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  (vectorized)Â \n##Â  student_t(3, 105, 5.9) InterceptÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  defaultÂ \n##Â  Â  student_t(3, 0, 5.9)Â  Â  Â  sdsÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  defaultÂ \n##Â  Â  student_t(3, 0, 5.9)Â  Â  Â  sds s(year)Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  (vectorized)Â \n##Â  Â  student_t(3, 0, 5.9)Â  Â  sigmaÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  default\n\n# multi-level method\nb4.11 &lt;- brm(data = d2,Â \nÂ  Â  Â  Â  Â  Â  family = gaussian,Â \nÂ  Â  Â  Â  Â  Â  # k = 19, corresponds to 17 basis functions I guess ::shrugs::Â \nÂ  Â  Â  Â  Â  Â  # The default for s() is to use whatâ€™s called a thin plate regression splineÂ \nÂ  Â  Â  Â  Â  Â  # bs uses a basis splineÂ \nÂ  Â  Â  Â  Â  Â  temp ~ 1 + s(year, bs = \"bs\", k = 19),Â \nÂ  Â  Â  Â  Â  Â  prior = c(prior(normal(100, 10), class = Intercept),Â \nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  prior(normal(0, 10), class = b),Â \nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  prior(student_t(3, 0, 5.9), class = sds),Â \nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  prior(exponential(1), class = sigma)),Â \nÂ  Â  Â  Â  Â  Â  iter = 2000, warmup = 1000, chains = 4, cores = 4,Â \nÂ  Â  Â  Â  Â  Â  seed = 4, backend = \"cmdstanr\",\nÂ  Â  Â  Â  Â  Â  control = list(adapt_delta = .99))"
  },
  {
    "objectID": "qmd/model-building-concepts.html#sec-modbld-misc",
    "href": "qmd/model-building-concepts.html#sec-modbld-misc",
    "title": "Concepts",
    "section": "Misc",
    "text": "Misc\n\nPackages\n\n{multiverse} - makes it easy to specify and execute all combinations of reasonable analyses of a dataset\n\n\n\nPaper, Summary of itâ€™s usage\nLots of vignettes\n\n\nRegression Workflow (Paper)\n\nBasic approach to algorithm choice\n\nModel is performing well on the training set but much worse on the validation/test set\n\n\nAndrew Ng calls the validation set the â€œDev Setâ€ ğŸ™„\nTest: Random sample the training set and use that as your validation set. Score your model on this new validation set\n\nâ€œTrain-Devâ€ is the sampled validation set\nPossibilities\n\nVariance: The data distribution of the training set is the same as the validation/test sets\n\n\nThe model has been overfit to the training data\n\nData Mismatch: The data distribution of the training set is NOT the same as the validation/test sets\n\n\nUnlucky and the split was bad\n\nSomething maybe is wrong with the splitting function\n\nSplit ratio needs adjusting. Validation set isnâ€™t getting enough data to be representative.\n\n\n\n\nModel is performing well on the validation/test set but not in the real world\n\nInvestigate the validation/test set and figure out why itâ€™s not reflecting real world data. Then, apply corrections to the dataset.\n\ne.g.Â distributions of your validation/tests sets should look like the real world data.\n\nChange the metric\n\nConsider weighting cases that your model is performing extremely poorly on.\n\n\nSplits\n\nHarrell: â€œnot appropriate to split data into training and test sets unless n&gt;20,000 because of the luck (or bad luck) of the split.â€\nIf your dataset is over 1M rows, then having a test set of 200K might be overkill (e.g.Â ratio of 60/20/20).\n\nMight be better to use a ratio of 98/1/1 for big data projects and 60/20/20 for smaller data projects\n\nlink\n\nShows that simple data splitting does not give valid confidence intervals (even asymptotically) when one refits the model on the whole dataset. Thus, if one wants valid confidence intervals for prediction error, we can only recommend either data splitting without refitting the model (which is viable when one has ample data), or nested CV."
  },
  {
    "objectID": "qmd/model-building-ensembling.html#sec-modbld-ensemb-misc",
    "href": "qmd/model-building-ensembling.html#sec-modbld-ensemb-misc",
    "title": "Ensembling",
    "section": "Misc",
    "text": "Misc\n\nMay be better to used a ML model for the ensembling procedure since the features are probably highly correlated\nBayesian Stacking (paper)\nFeature-Weighted Linear Stacking (FWLS) (Paper)\n\nIncorporates meta-features for improved accuracy while retaining the well-known virtues of linear regression regarding speed, stability, and interpretability\nuse of meta-features, additional inputs describing each observation in a dataset, can boost the performance of ensemble methods, but the greatest reported gains have come from nonlinear procedures requiring significant tuning and training time.\nMeta-features - should be discrete variables\n\nExample (product sales model): current season, year, information about the products, and vendors variables\n\n\nâ€œbenchmarkâ€ is just a 28-day MA model, prediction_CNNLSTM is a DL model, and prediction_XGBoost is an xgboost model\n\n\nNeed to read the paper but the article I read that used this method used predictors from the base models as these â€œmeta-features.â€\n\n\nManually\n\nWith tidymodels objects from drob 1:58:08, Predicting box office performance\non_test_set &lt;- function(d) {\nÂ  Â  bind_cols(predict(mod_obj1, d) %&gt;% rename(mod_name1 = .pred),\nÂ  Â  Â  Â  Â  Â  Â  predict(mod_ojt2, d) %&gt;% rename(mod_name2 = .pred)) %&gt;%\nÂ  Â  Â  Â  augment(combination, newdata = .) %&gt;%\nÂ  Â  Â  Â  transmute(id = d$id, outcome_var = 2 ^ fitted # outcome was transformed with log2 during preprocessing\n\non_test_set(test_set) %&gt;%\nÂ  Â  rename(.pred = outcome_var) %&gt;%\nÂ  Â  bind_cols(test) %&gt;%\nÂ  Â  rmse(log2(.pred), outcome_var) # loss metric is RMSLE (Root Mean Squared Log Error)"
  },
  {
    "objectID": "qmd/model-building-ensembling.html#sec-modbld-ensemb-stacks",
    "href": "qmd/model-building-ensembling.html#sec-modbld-ensemb-stacks",
    "title": "Ensembling",
    "section": "Stacks",
    "text": "Stacks\n\n{stacks}\nSteps:\n\nAdd models and tune a penalyzed regression model to determine weights\n\nblend_predictions args: penalty, mixture, metric, control\n\ncontrol is for tune::control_grid\nmetric used to determine penalty. Use the same one you used to tune the models.\n\n\n\nlin_best &lt;- lin_location_tune %&gt;% filter_parameters(parameters = select_best(lin_location_tune))\nxg_best &lt;- xg_tune %&gt;% filter_parameters(parameters = select_best(xg_tune))\nlin_xg_blend &lt;- stacks() %&gt;%\nÂ  add_candidates(lin_best) %&gt;%\nÂ  add_candidates(xg_best) %&gt;%\nÂ  blend_predictions()\n\nFit the ensemble on the full training set\n\nlin_xg_fit &lt;- lin_xg_blend %&gt;%\nÂ  fit_members()\n\nMeasure performance on the test set\n\npredictions &lt;- lin_xg_fit %&gt;%\nÂ  predict(test, type = \"prob\", members = TRUE)\n\n# Log loss by model, or by the blend\npredictions %&gt;%\nÂ  select(contains(\"_Rained\")) %&gt;%\nÂ  bind_cols(select(test, rain_tomorrow)) %&gt;%\nÂ  gather(model, prediction, -rain_tomorrow) %&gt;%\nÂ  mutate(prediction = 1 - prediction) %&gt;%\nÂ  group_by(model) %&gt;%\nÂ  mn_log_loss(rain_tomorrow, prediction)"
  },
  {
    "objectID": "qmd/model-building-ensembling.html#sklearn",
    "href": "qmd/model-building-ensembling.html#sklearn",
    "title": "Ensembling",
    "section": "sklearn",
    "text": "sklearn\n\nMajority Vote\nfrom sklearn.ensemble import VotingClassifier\nX, y = make_classification(n_samples=1000)\nensemble = VotingClassifier(\nÂ  Â  estimators=[\nÂ  Â  Â  Â  (\"xgb\", xgb.XGBClassifier(eval_metric=\"auc\")),\nÂ  Â  Â  Â  (\"lgbm\", lgbm.LGBMClassifier()),\nÂ  Â  Â  Â  (\"cb\", cb.CatBoostClassifier(verbose=False)),\nÂ  Â  ],\nÂ  Â  voting=\"soft\",\nÂ  Â  # n_jobs=-1,\n)\n_ = ensemble.fit(X, y)\n\nIf the classes are probabilities or predictions are continuous, the predictions are averaged.\nvoting = â€œsoftâ€ says use probabilities\nvoting = â€œhardâ€ says each model makes a binary classification (guessing 50/50 threshold), and the ensemble model tallyâ€™s up the votes to output its final result\nweights argument can be used to assign different coefficients for more accurate models\nvotingRegressor also available\n\nStacking\nfrom sklearn.ensemble import StackingClassifier, StackingRegressor\nfrom sklearn.linear_model import LogisticRegression\nX, y = make_classification(n_samples=1000)\nensemble = StackingClassifier(\nÂ  Â  estimators=[\nÂ  Â  Â  Â  (\"xgb\", xgb.XGBClassifier(eval_metric=\"auc\")),\nÂ  Â  Â  Â  (\"lgbm\", lgbm.LGBMClassifier()),\nÂ  Â  Â  Â  (\"cb\", cb.CatBoostClassifier(verbose=False)),\nÂ  Â  ],\nÂ  Â  final_estimator=LogisticRegression(),\nÂ  Â  cv=5,\nÂ  Â  passthrough=False\nÂ  Â  # n_jobs=-1,\n)\n_ = ensemble.fit(X, y)\n\nStackingRegressor also available"
  },
  {
    "objectID": "qmd/model-building-h2o-automl.html#sec-h2o-misc",
    "href": "qmd/model-building-h2o-automl.html#sec-h2o-misc",
    "title": "H2O AutoML",
    "section": "Misc",
    "text": "Misc\n\nThe amount of ram needed on local machineÂ equals four to five times the size of your data (usually) but really depends on max number of models to be trained.\nDiagnostic metrics used in classification models are AUC, log loss, and mean per class error\nSeems to use generic variable importance (mean decrease gini)\nI think a glm was usedÂ to ensemble its models\nIn the binary classification example, the top models were â€œextremely randomized treesâ€, xgboost, and gbm.\nEnsemble modelÂ reproducibility requires arguments:Â set seed and exclude_model = â€œDNNâ€Â  (deep neural network)\nExample: CV with LIME Explainer"
  },
  {
    "objectID": "qmd/model-building-modeltime.html#sec-modbld-mdltm-misc",
    "href": "qmd/model-building-modeltime.html#sec-modbld-mdltm-misc",
    "title": "modeltime",
    "section": "Misc",
    "text": "Misc\n\nPlotting\nplot_times_series(data, date_var, value_var)\nCombining multiple models into a table\nmodel_tbl &lt;- modeltime_table(\nÂ  Â  model_arima,\nÂ  Â  model_prophet,\nÂ  Â  model_glmnet\n)\n\nAllows you to apply functions across multiple models\n\nAccessing a fitted model object to see coefficient values, hyperparameter values, etc.\nmodel_arima &lt;- arima_reg() %&gt;%\nÂ  Â  set_engine(\"auto_arima\") %&gt;%\nÂ  Â  fit(value_var ~ date_var, training(splits_obj))\nmodel_arima\nThe fit obj shows coefficients, AICc, etc."
  },
  {
    "objectID": "qmd/model-building-modeltime.html#sec-modbld-mdltm-steps",
    "href": "qmd/model-building-modeltime.html#sec-modbld-mdltm-steps",
    "title": "modeltime",
    "section": "Steps",
    "text": "Steps\n\nset-up\nrecipe\nspecify model(s)\nfit models\nforecast on test/assessment dataset\nassess performance\nchoose model\nforecast and assess performance on holdout dataset\nrefit on whole training set\nforecast the future"
  },
  {
    "objectID": "qmd/model-building-modeltime.html#sec-modbld-mdltm-setup",
    "href": "qmd/model-building-modeltime.html#sec-modbld-mdltm-setup",
    "title": "modeltime",
    "section": "Set-up",
    "text": "Set-up\n\nSome packages\n\ntidyverse - cleaning\ntidymodels - model building\ntimetk - loss metrics\nlubridate - ts cleaning functions\nstacks - ensembling\n\nParallelize/Cluster\n\nFor all methods, you must also set allow_par = TRUEÂ  in the control arg of the function\nParallel\nÂ  Â  Â  Â  parallel_start(6, .method = \"parallel\") # uses 6 vcpus\nÂ  Â  Â  Â  parallel_stop()\nSpark\nÂ  Â  Â  Â  sc &lt;- spark_connect(master = \"local\")\nÂ  Â  Â  Â  parallel_start(sc, .method = \"spark\")\nÂ  Â  Â  Â  parallel_stop()\nÂ  Â  Â  Â  spark_disconnect_all()\n\nFor Databricks, replace â€œlocalâ€ with â€œdatabricksâ€\n\n\nSplits\nsplits &lt;- time_series_split(\nÂ  Â  data,\nÂ  Â  assess = \"3 months\",\nÂ  Â  cumulative = TRUE\n)\n\nThe above test set will be 3 months long\n\ndataset is daily so assess arg doesnâ€™t havenâ€™t to on the same scale as the dataset\n\n\nVisualize the split\nsplits %&gt;%\nÂ  Â  # extract cv plan from split obj\nÂ  Â  tk_time_series_cv_plan() %&gt;%\nÂ  Â  plot_time_series(date_var, value_var)"
  },
  {
    "objectID": "qmd/model-building-modeltime.html#sec-modbld-mdltm-modspec",
    "href": "qmd/model-building-modeltime.html#sec-modbld-mdltm-modspec",
    "title": "modeltime",
    "section": "Model Specification, Fit",
    "text": "Model Specification, Fit\n\nWindow mean/median forecast\nmedian_fit &lt;- window_reg(id = \"id\", window_size = 6) %&gt;%\nÂ  Â  set_engine(\"window_function\", window_function = median) %&gt;%\nÂ  Â  fit(value ~ ., data = training(splits))\n\nBasically just calculates the median value for the window size thatâ€™s specified (not rolling)\n\nThe forecast is just this value repeated for the length of the forecast horizon\n\nNotes\n\nUsed mostly as a baseline forecast, but with tuning, it supposedly performs pretty well for some series\n\nArgs\n\nwindow_function - can be mean or whatever\nid - used for global modeling\n\nTuning\nÂ  Â  Â  Â  grid_tbl &lt;- tibble(\nÂ  Â  Â  Â  Â  Â  window_size = 1:12\nÂ  Â  Â  Â  ) %&gt;%\nÂ  Â  Â  Â  Â  Â  create_model_grid(\nÂ  Â  Â  Â  Â  Â  Â  Â  f_model_spec = \"window_reg\",\nÂ  Â  Â  Â  Â  Â  Â  Â  id = \"id\",\nÂ  Â  Â  Â  Â  Â  Â  Â  engine_name = \"window_function\",\nÂ  Â  Â  Â  Â  Â  Â  Â  engine_params = list(\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  window_function = ~ median(.)\nÂ  Â  Â  Â  Â  Â  Â  Â  )\nÂ  Â  Â  Â  Â  Â  )\n\nSlightly different than the normal procedure where you have to specify the window function like a lambda function\n\n\nSeasonal Naive\nnaive_fit &lt;- naive_reg(seasonal_period = 12, id = \"id\") %&gt;%\nÂ  Â  set_engine(\"snaive\") %&gt;%\nÂ  Â  fit(value ~ ., data = training(splits))\n\nSeasonal version of a naive forecast which just repeats the last value of the series for the length of the forecast horizon.\n\nauto_arima\nmodel_arima &lt;- arima_reg() %&gt;%\nÂ  Â  set_engine(\"auto_arima\") %&gt;%\nÂ  Â  fit(value_var ~ date_var, training(splits_obj))\nprophet\nmodel_prophet &lt;- prophet_reg(seasonality_yearly = TRUE) %&gt;%\nÂ  Â  set_engine(\"prophet\") %&gt;%\nÂ  Â  fit(value_var ~ date_var, training(split_obj))\n\nSeems to work better for highly seasonal or cyclical data otherwise itâ€™s ğŸ’©\n\nglmnet\nmodel_glmnet &lt;- linear_reg(penalty = 0.01) %&gt;%\nÂ  Â  set_engine(\"glmnet\") %&gt;%\nÂ  Â  # date_var is daily in this example\nÂ  Â  fit(value_var ~ wday(date_var, label = TRUE) +\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  month(date_var, label = TRUE) +\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  # trend feature\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  as.numeric(date_var),\nÂ  Â  Â  Â  training(splits_obj)\nÂ  Â  )\ngluonts deepar\nfit_deepar &lt;- deep_ar(\nÂ  Â  id = \"id\",\nÂ  Â  freq = \"M\",\nÂ  Â  prediction_length = 6,\nÂ  Â  lookback_length = 6*3,\nÂ  Â  epochs = 10\n) %&gt;%\nÂ  Â  # set_engine(\"gluonts_deepar\") %&gt;%\nÂ  Â  set_engine(\"torch\") %&gt;%\nÂ  Â  fit(value ~ date + id, data = training(splits))\n\nDescription\n\nDeveloped by Amazon; LSTM RNN\nRequires more data than arima or prophet\n\nUses weighted resampling techniques to help with lower n, larger p data\n\nHandles seasonality with minimal tuning\nOutputs probabilitistic forecasts\n\nin the form of Monte Carlo samples\ncan be used to develop quantile forecasts\n\nSupports non-normal likelihood functions\n\ne.g.Â discrete target variables\n\n\nNotes\n\nâ€œgluonts_deeparâ€ is the older style engine that uses reticulate to use the python gluonts library. â€œtorchâ€ uses the torch pkg which is written in R and is faster.\nStandardizing predictors is recommended\nimpute missing values\n\nRecommended the imputing algorithm uses predictor variables to calculate values\n\nlearning rate and encoder/decoder length are subject-specific and should be tuned manually (not sure how these are subject specific)\nTo ensure the model is not fitting based on the index of our dependent variable in the TS, the authors suggest training the model on â€œemptyâ€ data prior our start period. Just use a bunch of zeros as our dependent variable.\n\nNot sure if this can be implemented in modeltime\n\n\nargs\n\nâ€œidâ€ is for fitting a global model with panel data\nfreq - A pandas timeseries frequency such as â€œ5minâ€ for 5-minutes or â€œDâ€ for daily. Refer to Pandas Offset Aliases.\nprediction_length - forecast horizon\nlookback_length - Number of steps to unroll the RNN for before computing predictions (default: NULL, in which case context_length = prediction_length)\n\ni.e.Â like the number of lags to use as predictors in a ML or AR model\n\nepochs - default 5, the number of backpropagations before stopping (I think)\nand many moreâ€¦\n\n\ngluonts_gp_forecaster\nfit_gp_forecaster &lt;- gp_forecaster(\nÂ  Â  id = \"id\",\nÂ  Â  freq = \"M\",\nÂ  Â  prediction_length = 6,\nÂ  Â  epochs = 30\n) %&gt;%\nÂ  Â  set_engine(\"gluonts_gp_forecasster\") %&gt;%\nÂ  Â  fit(value ~ date + id, data = training(splits))\n\nGaussian Process (GP) Forecaster model\nNotes\nArgs\n\nâ€œidâ€ is for fitting a global model with panel data\nfreq - A pandas timeseries frequency such as â€œ5minâ€ for 5-minutes or â€œDâ€ for daily. Refer to Pandas Offset Aliases.\nprediction_length - forecast horizon\nepochs - default 5, the number of backpropagations before stopping (I think)\nand many moreâ€¦\n\n\ngluonts_deepstate\nfit_deepar &lt;- deep_state(\nÂ  Â  id = \"id\",\nÂ  Â  freq = \"M\",\nÂ  Â  prediction_length = 6,\nÂ  Â  lookback_length = 6*3,\nÂ  Â  epochs = 20\n) %&gt;%\nÂ  Â  set_engine(\"gluonts_deepstate\") %&gt;%\nÂ  Â  fit(value ~ date + id, data = training(splits))\n\ndeep learning state-space model\nArgs\n\nâ€œidâ€ is for fitting a global model with panel data\nfreq - A pandas timeseries frequency such as â€œ5minâ€ for 5-minutes or â€œDâ€ for daily. Refer to Pandas Offset Aliases.\nprediction_length - forecast horizon\nlookback_length - Number of steps to unroll the RNN for before computing predictions (default: NULL, in which case context_length = prediction_length)\n\ni.e.Â like the number of lags to use as predictors in a ML or AR model\n\nepochs - default 5, the number of backpropagations before stopping (I think)\nand many moreâ€¦"
  },
  {
    "objectID": "qmd/model-building-modeltime.html#sec-modbld-mdltm-tune",
    "href": "qmd/model-building-modeltime.html#sec-modbld-mdltm-tune",
    "title": "modeltime",
    "section": "Tune",
    "text": "Tune\n\ncreate_model_grid\ngrid_tbl &lt;- tibble(\nÂ  Â  learn_rate = c(0.001, 0.01, 0.1)\n) %&gt;%\nÂ  Â  create_model_grid(\nÂ  Â  Â  Â  f_model_spec = boost_tree,\nÂ  Â  Â  Â  engine_name = \"xgboost\",\nÂ  Â  Â  Â  mode = \"regression\nÂ  Â  )\n\nAlternative\ngrid_tbl &lt;- dials::grid_regular(\nÂ  Â  learn_rate(),\nÂ  Â  levels = 3\n)\n\ngrid_tbl %&gt;%\nÂ  Â  create_model_grid(\nÂ  Â  Â  Â  f_model_spec = boost_tree,\nÂ  Â  Â  Â  engine_nameÂ  = \"xgboost\",\nÂ  Â  Â  Â  # Static boost_tree() args\nÂ  Â  Â  Â  mode = \"regression\",\nÂ  Â  Â  Â  # Static set_engine() args\nÂ  Â  Â  Â  engine_params = list(\nÂ  Â  Â  Â  Â  Â  max_depth = 5\nÂ  Â  Â  Â  )\nÂ  Â  )\nCreates a list of model specifications\n\nI think you could create more than 1 of these objects in order to tune multiple algorithms.Â  Then append the $.models together and feed it to workflow_set(models= ) (see below)\nExample of another way\nmodels &lt;- union(snaive_grid_tbl %&gt;% select(.models), window_grid_median_tbl %&gt;% select(.models))\n\ncombination of seasonal naive and window-median models\n\n\nArgs\n\nf_model_spec - the parsnip model function\nengine_name - the parsnip model engine\nmode - always regression for forecasting\n\n\nworkflow_set\nwfset &lt;- workflow_set() %&lt;%\nÂ  Â  preproc = list(\nÂ  Â  Â  Â  recipe_spec),\nÂ  Â  models = grid_tbl$.models,\nÂ  Â  cross = TRUE\n)\n\nInstantiates workflfow\nArgs\n\ncross = TRUE says fit each recipe to each model\n\n\nmodeltime_fit_workflowset\n# parallel_start(6)\n\nwkflw_fit &lt;- wfset %&gt;%\nÂ  Â  modeltime_fit_workflowset(\nÂ  Â  Â  Â  data = training(splits),\nÂ  Â  Â  Â  control = control_fit_workflowset(\nÂ  Â  Â  Â  Â  Â  verbose = TRUE,\nÂ  Â  Â  Â  Â  Â  allow_par = TRUE\nÂ  Â  Â  Â  )\nÂ  Â  )\n\n# parallel_stop()\n\nFits all the models with different hyperparameter combinations\nparallel_start/stop is a wrapper around parallel::doParallel stuff\n\nThink Iâ€™d rather use doFuture\n\nArgs\n\nallow_par - turns on parallel processing"
  },
  {
    "objectID": "qmd/model-building-modeltime.html#sec-modbld-mdltm-perf",
    "href": "qmd/model-building-modeltime.html#sec-modbld-mdltm-perf",
    "title": "modeltime",
    "section": "Assessing Performance",
    "text": "Assessing Performance\n\nmodeltime_calibration: Calculate Residuals\ncalib_tbl &lt;- model_table %&gt;%\nÂ  Â  modeltime_calibration(testing(split_obj), id = \"id\")\n\nadds residuals to model table\nâ€œidâ€ allows you to assess accuracy for each group member\n\nmodeltime_accuracy :\ncalib_tbl %&gt;%\nÂ  Â  modeltime_accuracy(acc_by_id = TRUE) #%&gt;%\nÂ  Â  # select best model by rmse for each group\nÂ  Â  # slice_min(rmse)\n\nuses the residual data to calculate mae, mape, mase, smape, rmse, rsq\nacc_by_id = TRUE assess accuracy by group id (for global models)\n\ntable_modeltime_accuracy\ncalib_tbl %&gt;%\nÂ  Â  group_by(id)\nÂ  Â  table_modeltime_accuracy()\n\nhtml table of results (might be interactive)"
  },
  {
    "objectID": "qmd/model-building-modeltime.html#sec-modbld-mdltm-fcast",
    "href": "qmd/model-building-modeltime.html#sec-modbld-mdltm-fcast",
    "title": "modeltime",
    "section": "Forecast",
    "text": "Forecast\n\nmodeltime_forecast : makes predictions, calculates prediction intervals\n\nOn the test set:\n\n# Using test set\nforcast_obj &lt;- calibration_table %&gt;%\nÂ  Â  modeltimes_forecast(\nÂ  Â  Â  Â  new_data = testing(splits_obj),\nÂ  Â  Â  Â  actual_data = whole_training_dataset,\nÂ  Â  Â  Â  conf_by_id = TRUE\nÂ  Â  )\n\nconf_by_id = TRUE is for global models to get individual PIs for each group\n\nmust have included an id arg in the model function\n\n\nmodeltime_refit : forecast the future\nfuture_forecast_tbl &lt;- calibration_tbl %&gt;%\nÂ  Â  modeltime_refit(whole_training_set) %&gt;%\nÂ  Â  modeltime_forecast(\nÂ  Â  Â  Â  h = \"3 months\",\nÂ  Â  Â  Â  actual_data = whole_training_set\nÂ  Â  )\n\nIn example, dataset is daily, so horizon time scale doesnâ€™t have to match the datasetâ€™s scale.\n\nplot_modeltime_forecast\nplot_modeltime_forecast(forecast_obj)"
  },
  {
    "objectID": "qmd/model-building-rstanarm.html#sec-modbld-rstanarm-misc",
    "href": "qmd/model-building-rstanarm.html#sec-modbld-rstanarm-misc",
    "title": "rstanarm",
    "section": "Misc",
    "text": "Misc"
  },
  {
    "objectID": "qmd/model-building-rstanarm.html#sec-modbld-rstanarm-lm",
    "href": "qmd/model-building-rstanarm.html#sec-modbld-rstanarm-lm",
    "title": "rstanarm",
    "section": "Linear Regression",
    "text": "Linear Regression\n\nInteraction: continuous x categorical\nstan_mod &lt;- stan_glm(log_gdp_std ~ rugged_std_c * cid,Â \nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  data = dd,Â \nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  family = gaussian(link = \"identity\"),Â \nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  seed = 12345)\n\nLinear models still use stan_glm but with a gaussian distribution family\n\nInteraction: categorical x categorical\nrstan_mod1 &lt;- stan_glm(\nÂ  admitted ~ gender*dept,\nÂ  data = ucb_01,\nÂ  family = binomial\n)"
  },
  {
    "objectID": "qmd/model-building-rstanarm.html#sec-modbld-rstanarm-discrete",
    "href": "qmd/model-building-rstanarm.html#sec-modbld-rstanarm-discrete",
    "title": "rstanarm",
    "section": "Discrete Distribution Models",
    "text": "Discrete Distribution Models\n\nBinomial (SR, Ch. 11)\ntot_mod &lt;- stan_glm(Â \nÂ  cbind(admit, reject) ~ 0 + applicant.gender,Â \nÂ  prior = normal(0,1),Â \nÂ  data = ucb,Â \nÂ  family = binomialÂ \n)\n\nCan also use a logistic model, but need case-level data (e.g.Â 0/1)\n\nDeaggregate count data into 0/1 case-level data\n\ndata(UCBadmit, package = \"rethinking\")\nucb &lt;- UCBadmit %&gt;%\nÂ  mutate(applicant.gender = relevel(applicant.gender, ref = \"male\"))\n\n# deaggregate to 1/0\ndeagg_ucb &lt;- function(x, y) {\nÂ  UCBadmit %&gt;%\nÂ  Â  select(-applications) %&gt;%\nÂ  Â  group_by(dept, applicant.gender) %&gt;%\nÂ  Â  tidyr::uncount(weights = !!sym(x)) %&gt;%\nÂ  Â  mutate(admitted = y) %&gt;%\nÂ  Â  select(dept, gender = applicant.gender, admitted)\n}\n\nucb_01 &lt;- purrr::map2_dfr(c(\"admit\", \"reject\"),\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  c(1, 0),\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ~ disagg_ucb(.x, .y)\n)\nLogistic\nrstan_mod1 &lt;- stan_glm(\nÂ  admitted ~ gender*dept,\nÂ  data = ucb_01,\nÂ  family = binomial\n)"
  },
  {
    "objectID": "qmd/model-building-sklearn.html#sec-modbld-sklearn-misc",
    "href": "qmd/model-building-sklearn.html#sec-modbld-sklearn-misc",
    "title": "sklearn",
    "section": "Misc",
    "text": "Misc\n\nExtensions\n\n{{DeepChecks}} - for model diagnostics\n{{feature-engine}} - multiple transformers to engineer and select features to use in machine learning models.\n\nPreventing data leakage in sklearn\n\nUse fit_transform on the train data - this ensures that the transformer learns from the train set only and transforms it simultaneously. Then, call the transformmethod on the test set to transform it based on the information learned from the training data (i.e mean and variance of the training data).\n\nPrevents data leakage\nSomehow the transform parameters calculated on the training data have to saved, so they can be applied to the production data. This can be done with Pipelines (see Pipelines &gt;&gt; Misc) by serializing the Pipeline objects.\n\nu_transf = LabelEncoder()\nitem_transf = LabelEncoder()\n# encoding\ndf['user'] = u_transf.fit_transform(df['user'])\ndf['item'] = item_transf.fit_transform(df['item'])\n# decoding\ndf['item'] = item_transf.inverse_transform(df['item'])\ndf['user'] = u_transf.inverse_transform(df['user'])\n\nA more robust way is to use sklearnâ€™s Pipelines (see Pipelines below)\n\n\nTuning\n\nPick a metric for GridSearchCV and RandomizedSearchCV\n\nDefault is accuracy for classification which is rarely okay\nUsing metric from sklearn\ngs = GridSearchCV(estimator=svm.SVC(),Â \nÂ  Â  Â  Â  Â  Â  Â  Â  Â  param_grid={'kernel':('linear', 'rbf'), 'C':[1, 10]},\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  scoring=â€˜f1_microâ€™)\n\nList of metrics available\n\nUsing a custom metric\nfrom sklearn.metrics import fbeta_score, make_scorer\ncustom_metric = make_scorer(fbeta_score, beta=2)\ngs = GridSearchCV(estimator=svm.SVC(),\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  param_grid={'kernel':('linear','rbf'), 'C':[1,10]},\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  scoring=custom_metric)\nAlso see Diagnostics, Classification &gt;&gt; Scores &gt;&gt; Lift Score\n\n\nScore model\n\nAlso see Pipelines &gt;&gt; Tuning a Pipeline\nBasic\nmodel = RandomForestClassifier(max_depth=2, random_state=0, warm_start=True, n_estimators=1)\nmodel.fit(X_train, y_train)\nmodel.score(X_test, y_test)\n\nClassification\nfrom sklearn.metrics import classification_report\nrep = classification_report(y_test, y_pred, output_dict = True)\n\nSee Diagnostics, Classification &gt;&gt; Multinomial for definitions of multinomial scores\n\nMultiple metrics function\nfrom sklearn.metrics import precision_recall_fscore_support\ndef score(y_true, y_pred, index):\nÂ  Â  \"\"\"Calculate precision, recall, and f1 score\"\"\"\n\nÂ  Â  metrics = precision_recall_fscore_support(y_true, y_pred, average='weighted')\nÂ  Â  performance = {'precision': metrics[0], 'recall': metrics[1], 'f1': metrics[2]}\nÂ  Â  return pd.DataFrame(performance, index=[index])\npredict_proba outputs probabilities for classification models"
  },
  {
    "objectID": "qmd/model-building-sklearn.html#sec-modbld-sklearn-opt",
    "href": "qmd/model-building-sklearn.html#sec-modbld-sklearn-opt",
    "title": "sklearn",
    "section": "Optimization",
    "text": "Optimization\n\nMisc\n\nFor fast iteration and if you have access to GPUs, itâ€™s better to use {{XGBoost, LightGBM or CatBoost}} since sklearn is only CPU capable.\n\nFastest to slowest: CaBoost, LightGBM, XGBoost\n\n\n{{sklearnex}}\n\nIntelÂ® Extension for Scikit-learn that dynamically patches scikit-learn estimators to use Intel(R) oneAPI Data Analytics Library as the underlying solver\n\nRequirements\n\nThe processor must have x86 architecture.\nThe processor must support at least one of SSE2, AVX, AVX2, AVX512 instruction sets.\nARM* architecture is not supported.\nIntelÂ® processors provide better performance than other CPUs.\n\n\nMisc\n\nDocs\nBenchmarks\nAlgorithms\nCurrently extension does not support multi-output and sparse data for the Random Forest and K-Nearest Neighbors\n\nNon-interactively\npython -m sklearnex my_application.py\nInteractively for all algorithms\nfrom sklearnex import patch_sklearn\npatch_sklearn()\n# then import sklearn estimators\nInteractively for specific algorithms\nfrom sklearnex import patch_sklearn\npatch_sklearn([\nÂ  Â  'RandomForestRegressor,\nÂ  Â  'SVC',\nÂ  Â  'DBSCAN'\n])\nUnpatch unpatch_sklearn()\nNeed to reimport estimators after executing\n\nSpeeding up retraining a model (article)\n\nPotentially useful for slow training models that need to be retrained often.\nNot all models have these methods available\nwarm_start = True permits the use of the existing fitted model attributes to initialize a new model in a subsequent call to fit\n\nDoesnâ€™t learn new parameters so shouldnâ€™t be used to fix concept drift\n\ni.e.Â The new data should have the same distribution as the original data which maintains the same relationship with the output variable\n\nExample\n# original fit\nmodel = RandomForestClassifier(max_depth=2, random_state=0, warm_start=True, n_estimators=1)\nmodel.fit(X_train, y_train)\n\n# fit with new data\nmodel.n_estimators+=1\nmodel.fit(X2, y2)\n\n\nPartial Fit\n\nDoes learn new model parameters\nExample\n# original fit\nmodel = SGDClassifier()Â \nmodel.partial_fit(X_train, y_train, classes=np.unique(y))\n\n# fit with new data\nmodel.partial_fit(X2, y2)"
  },
  {
    "objectID": "qmd/model-building-sklearn.html#sec-modbld-sklearn-preproc",
    "href": "qmd/model-building-sklearn.html#sec-modbld-sklearn-preproc",
    "title": "sklearn",
    "section": "Preprocessing",
    "text": "Preprocessing\n\nMisc\n\nResources\n\nSciKit-Learn Transformation Docs\n\ntrain_test_split doesnâ€™t choose random rows to be in each partition by default. For example, a 90-10 split has the first 90% of the rows in Train which leaves the last 10% of the rows to be in Test\n\nâ€œshuffle=Trueâ€ will randomly shuffle the rows before it partitions which would be equivalent to randomly selecting rows for each partition\n\n\nStratified train/test splits\nnp.random.seed(2019)\n# Generate stratified split\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random.state=42)\n# Train set class weights\n&gt;&gt;&gt; pd.Series(y_train).value_counts(normalize=True)\n1Â  Â  0.4\n0Â  Â  0.4\n2Â  Â  0.2\ndtype: float64\n# Test set class weights\n&gt;&gt;&gt; pd.Series(y_test).value_counts(normalize=True)\n1Â  Â  0.4\n0Â  Â  0.4\n2Â  Â  0.2\n\nOr you can use StratifiedKFold which stratifies automatically.\n\nStratified Train/Validate/Test splits\nX_train, X_, y_train, y_ = train_test_split(\nÂ  Â  df['token'], tags, train_size=0.7, stratify=tags, random_state=RS\n)\n\nX_val, X_test, y_val, y_test = train_test_split(\nÂ  Â  X_, y_, train_size=0.5, stratify=y_, random_state=RS\n)\n\nprint(f'train: {len(X_train)} ({len(X_train)/len(df):.0%})\\n'\nÂ  Â  Â  f'val: {len(X_val)} ({len(X_val)/len(df):.0%})\\n'\nÂ  Â  Â  f'test: {len(X_test)} ({len(X_test)/len(df):.0%})')\nCheck proportions of stratification variable (e.g.Â â€œtagsâ€) in splits\nsplit = pd.DataFrame({\nÂ  Â  'y_train': Counter(', '.join(i for i in row) for row in mlb.inverse_transform(y_train)),\nÂ  Â  'y_val': Counter(', '.join(i for i in row) for row in mlb.inverse_transform(y_val)),\nÂ  Â  'y_test': Counter(', '.join(i for i in row) for row in mlb.inverse_transform(y_test))\n}).reindex(tag_dis.index)\n\nsplit = split / split.sum(axis=0)\n\nsplit.plot(\nÂ  Â  kind='bar',Â \nÂ  Â  figsize=(10,4),Â \nÂ  Â  title='Tag Distribution per Split',Â \nÂ  Â  ylabel='Proportion of observations'\n);\nBin numerics: KBinsDiscretizer(n_bins=4)\nImpute Nulls/Nas\n\nSimpleImputer (Docs)\ncol_transformer = ColumnTransformer(\nÂ  Â  # Transformer name, Transformer Object and columns\nÂ  Â  [(\"ImputPrice\", SimpleImputer(strategy=\"median\"), [\"price\"])],\nÂ  Â  # Any other columns are ignored\nÂ  Â  remainder= SimpleImputer(strategy=\"constant\", fill_value=-1)\nÂ  )\n\nTakes â€œpriceâ€ and replaces Nulls with median; all other columns get constant, -1, to replace their Nulls\nâ€œmost frequentâ€ also available\n\nIterativeImputer (Docs) - Multivariate imputer that estimates values to impute for each feature with missing values from all the others.\nKNNImputer (Docs) - Multivariate imputer that estimates missing features using nearest samples.\n\nLog: FunctionTransformer(lambda value: np.log1p(value))\nStandardize\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nstandardized_data = scaler.fit_transform(df)\nstandardized_df = pd.DataFrame(standardized_data, columns=df.columns)\n\nCheck: standardized_df.apply([\"mean\", \"std\"]\n\nScaling\n\nMin/Max\nfrom sklearn.preprocessing import MinMaxScaler\n# Create a scaler object\n\nmm_scaler = MinMaxScaler()\n\n# Transform the feature values\nchurn[[\"Customer_Age\", \"Total_Trans_Amt\"]] = mm_scaler.fit_transform(churn[[\"Customer_Age\", \"Total_Trans_Amt\"]])\n\n# check the feature value range after transformation\nchurn[\"Customer_Age\"].apply([\"min\", \"max\"])\n\nâ€œfeature_range=(0,1)â€ parameter allows you to change the range\n\ndefault range for the MinMaxScaler is [0,1]\n\n\n\nOrdinal\nencode_cat = ColumnTransformer(\nÂ  [('cat', OrdinalEncoder(), make_column_selector(dtype_include='category'))],\nÂ  remainder='passthrough'\n)\nCategorical DType (like R factor type)\n\nAssign predefined unordered values so that whenever some value for some column does not exist in the training set, receiving such a value in the test set would not lead to incorrectly assigned labels or any other logical error.\n\nWith multiple categorical columns, it can be difficult to stratify them in the training/test splits\nMust know all possible categories for each categorical variable\n\nimport pandas as pd\nfrom pandas import CategoricalDtype\n\ndef transform_data(df: pd.DataFrame, target: pd.Series, frac: float = 0.07,\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  random_state: int = 42) -&gt; pd.DataFrame:\n\nÂ  Â  \"Transform non-numeric columns into categorical type and clean data.\"\n\nÂ  Â  categories_map = {\nÂ  Â  Â  Â  Â  'gender': {1: 'male', 2: 'female'},\nÂ  Â  Â  Â  Â  'education': {1: 'graduate', 2: 'university', 3: 'high_school',\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  4: 'others', 5: 'others', 6: 'others', 0: 'others'},\nÂ  Â  Â  Â  Â  'marital_status': {1: 'married', 2: 'single', 3: 'others', 0: 'others'}\nÂ  Â  Â  Â  }\n\nÂ  Â  for col_id, col_map in categories_map.items():\nÂ  Â  Â  Â  Â  Â  df[col_id] = df[col_id].map(col_map).astype(\nÂ  Â  Â  Â  Â  Â  Â  Â  CategoricalDtype(categories=list(set(col_map.values())))\nÂ  Â  Â  Â  Â  Â  )\n\nOne-hot encoding\n\n** Donâ€™t use pandas get_dummies because it doesnâ€™t handle categories that arenâ€™t in your test/production set **\n\nWonder if get_dummies creates 1 less dummy than the number of categories like it should\n\nBasic\n# Create a one-hot encoder\nonehot = OneHotEncoder()\n\n# Create an encoded feature\nencoded_features = onehot.fit_transform(churn[[\"Marital_Status\"]]).toarray()\n\n# Create DataFrame with the encoded features\nencoded_df = pd.DataFrame(encoded_features, columns=onehot.categories_)\n\nPipeline\nfrom sklearn.preprocessing import OneHotEncoder\n# one hot encode categorical features\nohe = OneHotEncoder(handle_unknown='ignore')\n\nfrom sklearn.pipeline import Pipeline\n# store one hot encoder in a pipeline\ncategorical_processing = Pipeline(steps=[('ohe', ohe)])\n\nfrom sklearn.compose import ColumnTransformer\n# create the ColumnTransormer object\npreprocessing = ColumnTransformer(transformers=[('categorical', categorical_processing, ['gender', 'job'])],\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  remainder='passthrough')\nClass Imbalance\n\nSMOTE with {{imblearn}}\nfrom sklearn.datasets import make_classification\nfrom imblearn.over_sampling import SMOTE\nX_train, y_train = make_classification(n_samples=500, n_features=5, n_informative=3)\nX_res, y_res = SMOTE().fit_resample(X_train, y_train)"
  },
  {
    "objectID": "qmd/model-building-sklearn.html#sec-modbld-sklearn-pip",
    "href": "qmd/model-building-sklearn.html#sec-modbld-sklearn-pip",
    "title": "sklearn",
    "section": "Pipelines",
    "text": "Pipelines\n\nMisc\n\nHelps by creating more maintainable and clearly written code. Reduces mistakes by transforming train/test sets automatically.\nPipeline objects are estimators and can be serialized and saved like any other estimator\nimport joblib\n\n#saving the pipeline into a binary file\njoblib.dump(pipe, 'wine_pipeline.bin')\n\n#loading the saved pipeline from a binary file\npipe = joblib.load('wine_pipeline.bin')\n\n\n\nBasic Feature Transforming and Model Fitting Pipeline\n\nformat: Pipeline(steps = [(â€˜&lt;step1_name&gt;â€™, function), (â€˜&lt;step2_name&gt;â€™, function)])\nExample\nnp.random.seed(2019)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n\n# creating the pipeline with its different steps\n# fitting the pipeline with data\n# making predictions\npipe = Pipeline([\nÂ  Â  ('feature_selection', VarianceThreshold(threshold=0.1)),\nÂ  Â  ('scaler', StandardScaler()),\nÂ  Â  ('model', KNeighborsClassifier())\n])\npipe.fit(X_train, y_train)\npredictions = pipe.predict(X_test)\n\n#checking the accuracy\naccuracy_score(y_test, predictions) #sklearn function; multi-class: accuracy, binary: jaccard index (similarity)\n\nPipeline transforms according to the sequence of the steps inserted into the list of tuples\n\n\n\n\nColumn Transformers\n\nExample: transform by column type\n# creating pipeline for numerical features\nnumerical_pipe = Pipeline([\nÂ  Â  ('imputer', SimpleImputer(missing_values=np.nan, strategy='mean')),\nÂ  Â  ('scaler', StandardScaler())\n])\n\n# creating pipeline for categorical features\ncategorical_pipe = Pipeline([\nÂ  Â  ('imputer', SimpleImputer(missing_values=np.nan, strategy='most_frequent')),\nÂ  Â  ('one_hot', OneHotEncoder(handle_unknown='ignore'))\n])\npreprocessor = ColumnTransformer([\nÂ  Â  ('numerical', numerical_pipe, make_column_selector(dtype_include=['int', 'float'])),\nÂ  Â  ('categorical', categorical_pipe, make_column_selector(dtype_include=['object', 'category'])),\n])\npipe = Pipeline([\nÂ  Â  ('column_transformer', preprocessor),\nÂ  Â  ('model', KNeighborsClassifier())\n])\n\npipe.fit(X_train, y_train)\npredictions = pipe.predict(X_test)\n\nColumnTransformertakes list of tuples: name, transformer, columns\n\nn_jobs, verbose args also available\nremainder=â€œpassthroughâ€ says all other columns not listed are ignored (might be a default)\n\nCan also provide a transformer object\n\nmethods: fit_transform, get_feature_names_out, get_params, etc\n\nmake_column_selector allows your to select the type of column to apply the transformer (docs)\n\nExample: Apply sequence of transformations to a column\ncol_transformer = ColumnTransformer(\nÂ  Â  [\nÂ  Â  Â  (\nÂ  Â  Â  Â  Â  \"PriceTransformerPipeline\",\nÂ  Â  Â  Â  Â  # Pipeline -&gt; multiple transformation steps\nÂ  Â  Â  Â  Â  Pipeline([\nÂ  Â  Â  Â  Â  Â  (\"MeanImputer\"Â  Â  Â  , SimpleImputer(strategy=\"median\")),\nÂ  Â  Â  Â  Â  Â  (\"LogTransformation\", FunctionTransformer(lambda value: np.log1p(value)) ),\nÂ  Â  Â  Â  Â  Â  (\"StdScaler\",Â  Â  Â  Â  StandardScaler() ),\nÂ  Â  Â  Â  Â  ]),\nÂ  Â  Â  Â  Â  [\"price\"]\nÂ  Â  Â  Â  ),\nÂ  Â  ],\nÂ  Â  remainder=SimpleImputer(strategy=\"constant\", fill_value=-1)\nÂ  )\n\nFor the â€œpriceâ€ columns, it replaces Nulls with median, then log transforms, then standardizes. All other columns get their Nulls replaces with -1.\n\n\n\n\nFunction Transformers\n\nMisc\n\nIf using a method in the sklearn.preprocessing module, then able to use fit, transform, and fit_transform methods (I think)\nData must be the first argument of the function\ninverse_func argument for FunctionTransformer allows you to include a back-transform function\n\nSteps\n\nCreate function that transforms data\nCreate FunctionTransformer object using function as the argument\nAdd function-tranformer to the pipeline by including it as an argument to make_pipeline\n\nExample: Make numerics 32-bit instead 64-bit to save memory\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import FunctionTransformer\n\ndef reduce_memory(X: pd.DataFrame, y=None):\nÂ  Â  \"\"\"Simple function to reduce memory usage by casting numeric columns to float32.\"\"\"\nÂ  Â  num_cols = X.select_dtypes(incluce=np.number).columns\nÂ  Â  for col in num_cols:\nÂ  Â  Â  Â  X[col] = X.astype(\"float32\")\nÂ  Â  return X, y\n\nReduceMemoryTransformer = FunctionTransformer(func = reduce_memory)\n\n# Plug into a pipeline\n&gt;&gt;&gt; make_pipeline(SimpleImputer(), ReduceMemoryTransformer)\n\nData goes through the SimpleImputer first then the ReduceMemoryTransformer\n\n\n\n\nCustom Transformers Classes\n\nMisc\n\nFor more complex transforming tasks\n\nSteps\n\nCreate a class that inherits from BaseEstimator and TransformerMixin classes of sklearn.base\n\nInheriting from these classes allows Sklearn pipelines to recognize our classes as custom estimators and automatically adds fit_transform to your class\n\nAdd transforming methods to Class\n\nClass Skeleton\nclass CustomTransformer(BaseEstimator, TransformerMixin):\nÂ  Â  def __init__(self):\nÂ  Â  Â  Â  pass\nÂ  Â  def fit(self):\nÂ  Â  Â  Â  pass\nÂ  Â  def transform(self):\nÂ  Â  Â  Â  pass\nÂ  Â  def inverse_transform(self):\nÂ  Â  Â  Â  pass\nExample: Log transforming outcome variable\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.preprocessing import PowerTransformer\n\nclass CustomLogTransformer(BaseEstimator, TransformerMixin):\n\nÂ  Â  def __init__(self):\nÂ  Â  Â  Â  self._estimator = PowerTransformer()Â  # init a transformer\n\nÂ  Â  def fit(self, X, y=None):\nÂ  Â  Â  Â  X_copy = np.copy(X) + 1Â  # add one in case of zeroes\nÂ  Â  Â  Â  self._estimator.fit(X_copy)\nÂ  Â  Â  Â  return self\n\nÂ  Â  def transform(self, X):\nÂ  Â  Â  Â  X_copy = np.copy(X) + 1\nÂ  Â  Â  Â  return self._estimator.transform(X_copy)Â  # perform scaling\n\nÂ  Â  def inverse_transform(self, X):\nÂ  Â  Â  Â  X_reversed = self._estimator.inverse_transform(np.copy(X))\nÂ  Â  Â  Â  return X_reversed - 1Â  # return subtracting 1 after inverse transform\n\nreg_lgbm = lgbm.LGBMRegressor()\nfinal_estimator = TransformedTargetRegressor(\nÂ  Â  regressor=reg_lgbm, transformer=CustomLogTransformer()\n)\nfinal_estimator.fit(X_train, y_train)\n\nfit returns the tranformer itself since it returns self\n\nEstimates the optimal parameter lambda for each feature\n\ntransform returns transformed features\n\nApplies the power transform to each feature using the estimated lambdas in fit\n\nfit_transform does both at once\ncustom_log.fit(tps_df)Â \ntransformed_tps = custom_log.transform(tps_df)\n# or\ntransformed_tps = custom_log.fit_transform(tps_df)\ninverse_transform returns the back-transformed features\nTransformedTargetRegressor transforms the targets y before fitting a regression model. The predictions are mapped back to the original space via an inverse transform. It takes as an argument the regressor that will be used for prediction, and the transformer that will be applied to the target variable\n\nThe regressor parameter accepts both regressors or pipelines that end with regressors\nIf the transformer is a function, like np.log, you can pass it to func argument\n\n\nExample: Dummy transformer + FeatureUnion (article)\n\nThis classes (below) that inherit this class with get these methods along with fit_transform\nclass DummyTransformer(BaseEstimator, TransformerMixin):\n  \"\"\"\n  Dummy class that allows us to modify only the methods that interest us,\n  avoiding redudancy.\n  \"\"\"\n  def __init__(self):\n  Â  Â  return None\n\n  def fit(self, X=None, y=None):\n  Â  Â  return self\n\n  def transform(self, X=None):\n  Â  Â  return self\nTransformer classes\nclass Preprocessor(DummyTransformer):\n\"\"\"\n  Class used to preprocess text\n\"\"\"\ndef __init__(self, remove_stopwords: bool):\nÂ  Â  self.remove_stopwords = remove_stopwords\nÂ  Â  return None\n\ndef transform(self, X=None):\nÂ  Â  preprocessed = X.apply(lambda x: preprocess_text(x, self.remove_stopwords)).values\nÂ  Â  return preprocessed\n\nclass SentimentAnalysis(DummyTransformer):\n\"\"\"\nÂ  Â  Class used to generate sentiment\nÂ  Â  \"\"\"\nÂ  Â  def transform(self, X=None):\n     Â  Â  sentiment = X.apply(lambda x: get_sentiment(x)).values\n     Â  Â  return sentiment.reshape(-1, 1) # &lt;-- note the reshape to transfor\n\nEach class inherits the dummy transformer and its methods\npreprocess_text and get_sentiment are user functions that are defined earlier in the article\n\nCreate pipeline\nvectorization_pipeline = Pipeline(steps=[\n  ('preprocess', Preprocessor(remove_stopwords=True)), # the first step is to preprocess the text\n  ('tfidf_vectorization', TfidfVectorizer()), # the second step applies vectorization on the preprocessed text\n  ('arr', FromSparseToArray()), # the third step converts a sparse matrix into a numpy array in order to show it in a            dataframe\n])\n\nTfidVectorizer and FromSparseArray are other classes in the article that I didnâ€™t include in the Transformer classes chunk to save space\n\nCombine transformed features\n# vectorization_pipeline is a pipeline within a pipeline\nfeatures = [\n  ('vectorization', vectorization_pipeline),\n  ('sentiment', SentimentAnalysis()),\n  ('n_chars', NChars()),\n  ('n_sentences', NSententences())\n]\ncombined = FeatureUnion(features) # this is where we merge the features together\n\n# Get col names: subsets the second step of the second object in the vectorization_pipeline to retrieveÂ \n# the terms generated by the tf-idf then adds the other three column names to it\ncols = vectorization_pipeline.steps[1][1].get_feature_names() + [\"sentiment\", \"n_chars\", \"n_sentences\"]\nfeatures_df = pd.DataFrame(combined.transform(df['corpus']), columns=cols)\n\nPipelines are combined with FeatureUnion, features are transformed, and coerced into a pandas df which can be used to train a model\nNChars and NSentences are other classes in the article that I didnâ€™t include in the Transformer classes chunk to save space\n\n\n\n\n\nTuning a Pipeline\n\nExample\nparameters = {\nÂ  Â  'column_transformer__numerical__imputer__strategy': ['mean', 'median'],\nÂ  Â  'column_transformer__numerical__scaler': [StandardScaler(), MinMaxScaler()],\nÂ  Â  'model__n_neighbors': [3, 6, 10, 15],\nÂ  Â  'model__weights': ['uniform', 'distance'],\nÂ  Â  'model__leaf_size': [30, 40]\n}\n\n# defining a scorer and a GridSearchCV instance\nmy_scorer = make_scorer(accuracy_score, greater_is_better=True)\nsearch = GridSearchCV(pipe, parameters, cv=3, scoring=my_scorer, n_jobs=-1, verbose=1)\n\n# search for the best hyperparameter combination within our defined hyperparameter space\nsearch.fit(X_train, y_train)\n\n# changing pipeline parameters to gridsearch results\npipe.set_params(**search.best_params_)\n\n# making predictions\npredictions = pipe.predict(X_test)\n\n# checking accuracy\naccuracy_score(y_test, predictions)\n\nSee Column Transformer section example for details on this pipeline\nNote the double underscore used in the keys of the parameter dict\n\nDouble underscores separate names of steps inside a nested pipeline with last name being the argument of the transformer function being tuned\nExample: â€˜column_transformer__numerical__imputer__strategyâ€™\n\ncolumn_transformer (pipeline step name) &gt;&gt; numerical (pipeline step name) &gt;&gt; imputer (pipeline step name) &gt;&gt; strategy (arg of SimpleImputer function)\n\n\nView tuning results\nbest_params = search.best_params_\nprint(best_params)\n\n# Stores the optimum model in best_pipe\nbest_pipe = search.best_estimator_\nprint(best_pipe)\n\nresult_df = DataFrame.from_dict(search.cv_results_, orient='columns')\nprint(result_df.columns)\n\n\n\n\nDisplay Pipelines in Jupyter\n\nfrom sklearn import set_configÂ \nset_config(display=\"diagram\")\ngiant_pipeline"
  },
  {
    "objectID": "qmd/model-building-sklearn.html#sec-modbld-sklearn-algs",
    "href": "qmd/model-building-sklearn.html#sec-modbld-sklearn-algs",
    "title": "sklearn",
    "section": "Algorithms",
    "text": "Algorithms\n\nMisc\nHistogram-based Gradient Boosting Regression Tree\n\nHistogram-based models are more efficient since they bin the continuous features\nInspired by LightGBM. Much faster than GradientBoostingRegressor for big datasets (n_samples &gt;= 10 000).\n\nsklearn.ensemble.HistGradientBoostingRegressor\n\nuser guide\n\nStochastic Gradient Descent (SGD)\n\nalgorithm\nNot a class of models, just merely an optimization technique\n\nSGDClassifier(loss='log') results in logistic regression, i.e.Â a model equivalent to LogisticRegression which is fitted via SGD instead of being fitted by one of the other solvers in LogisticRegression.\nSGDRegressor(loss='squared_error', penalty='l2') and Ridge solve the same optimization problem, via different means.\n\nPenalyzed regression hyperparameters are labelled different than in R\n\nlambda (R) is alpha (py)\nalpha (R) is 1 - L1_ratio (py)\n\nCan be successfully applied to large-scale and sparse machine learning problems often encountered in text classification and natural language processing Advantages:\n\nEfficiency.\nEase of implementation (lots of opportunities for code tuning). Disadvantages:\nSGD requires a number of hyperparameters such as the regularization parameter and the number of iterations.\nSGD is sensitive to feature scaling.\n\nProcessing\n\nMake sure you permute (shuffle) your training data before fitting the model or use shuffle=True to shuffle after each iteration (used by default).\nFeatures should be standardized using e.g.Â make_pipeline(StandardScaler(), SGDClassifier())\n\n\nBisectingKMeans\n\nCentroid is picked progressively (instead of simultaneously) based on the previous cluster. We would split the cluster each time until the number of K is achieved\nAdvantages\n\nIt would be more efficient with a large number of clusters\nCheaper computational costs\nIt does not produce empty clusters\nThe clustering result was well ordered and would create a visible hierarchy.\n\n\nXGBoost with GPU\ngbm = xgb.XGBClassifier(\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  n_estimators=100000,\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  max_depth=6,\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  objective=\"binary:logistic\",\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  learning_rate=.1,\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  subsample=1,\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  scale_pos_weight=99,\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  min_child_weight=1,\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  colsample_bytree=1,\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  tree_method='gpu_hist',\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  use_label_encoder=False\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  )\neval_set=[(X_train,y_train),(X_val,y_val)]Â \nfit_model = gbm.fit(Â \nÂ  Â  Â  Â  Â  Â  Â  Â  X_train, y_train,Â \nÂ  Â  Â  Â  Â  Â  Â  Â  eval_set=eval_set,\nÂ  Â  Â  Â  Â  Â  Â  Â  eval_metric='auc',\nÂ  Â  Â  Â  Â  Â  Â  Â  early_stopping_rounds=20,\nÂ  Â  Â  Â  Â  Â  Â  Â  verbose=TrueÂ \nÂ  Â  Â  Â  Â  Â  Â  )\n\ntree_method='gpu_hist' specifies the use of GPU\nPlot importance\nfig,ax2 = plt.subplots(figsize=(8,11))\nxgb.plot_importance(gbm, importance_type='gain', ax=ax2)\n\nGBM with Quantile Loss (PIs)\ngbm_lower = GradientBoostingRegressor(\nÂ  Â  loss=\"quantile\", alpha = alpha/2, random_state=0\n)\ngbm_upper = GradientBoostingRegressor(\nÂ  Â  loss=\"quantile\", alpha = 1-alpha/2, random_state=0\n)\ngbm_lower.fit(X_train, y_train)\ngbm_upper.fit(X_train, y_train)\ntest['gbm_lower'] = gbm_lower.predict(X_test)\ntest['gbm_upper'] = gbm_upper.predict(X_test)"
  },
  {
    "objectID": "qmd/model-building-sklearn.html#sec-modbld-sklearn-cv",
    "href": "qmd/model-building-sklearn.html#sec-modbld-sklearn-cv",
    "title": "sklearn",
    "section": "CV/Splits",
    "text": "CV/Splits\n\nK-Fold\n\nfrom sklearn.model_selection import KFold\ncv = KFold(n_splits=7, shuffle=True)\n\nShuffling: minimizes the risk of overfitting by breaking the original order of the samples\n\n\nStratifiedKFold\n\nfrom sklearn.model_selection import StratifiedKFold\ncv = StratifiedKFold(n_splits=7, shuffle=True, random_state=1121218)\n\nFor classification, class ratios are held to the same ratios in both the training and test sets.\n\nclass ratios are preserved across all folds and iterations.\n\n\nLeavePOut: from sklearn.model_selection import LeaveOneOut, LeavePOut\n\nData is so limited that you have to perform a CV where you set aside only a few rows of data in each iteration\nLeaveOneOut is P = 1 for LeavePOut\n\nShuffleSplit, StratifiedShuffleSplit\n\nfrom sklearn.model_selection import ShuffleSplit, StratifiedShuffleSplit\ncv = ShuffleSplit(n_splits=7, train_size=0.75, test_size=0.25)\ncv = StratifiedShuffleSplit(n_splits=7, test_size=0.5)\n\nNot a CV, just repeats the train/test split process multiple times\nUsing different random seeds should resemble a robust CV process if done for enough iterations\n\nTimeSeriesSplit\n\nfrom sklearn.model_selection import TimeSeriesSplit\ncv = TimeSeriesSplit(n_splits=7)\n\nWith time series data, the ordering of samples matters.\n\nGroup Data\n\nData is not iid (e.g.Â multi-level data)\nOptions\n\nGroupKFold\nStratifiedGroupKFold\nLeaveOneGroupOut\nLeavePGroupsOut\nGroupShuffleSplit\n\nWorks just like the non-group methods but with a group arg for the grouping variable"
  },
  {
    "objectID": "qmd/model-building-tidymodels.html#sec-modlbld-tidymod-misc",
    "href": "qmd/model-building-tidymodels.html#sec-modlbld-tidymod-misc",
    "title": "tidymodels",
    "section": "Misc",
    "text": "Misc\n\nCV dataset terminology: training, validation, test â€“&gt; analysis, assessment, test\n(I donâ€™t think this is necessary anymore. Outcome may remain categorical) Transform categorical outcome variables to factors\n# outside recipe\nmutate(rain_tomorrow = factor(ifelse(rain_tomorrow, \"Rained\", \"Didn't Rain\")))\n\n# skip = TRUE is so this is tried to be applied the test set which won't have the outcome var when its being preprocessed\nstep_mutate(rain_tomorrow = factor(ifelse(rain_tomorrow, \"Rained\", \"Didn't Rain\"),\nÂ  Â  Â  Â  Â  Â  skip = TRUE)\n\n# this would have 0 and 1 values though\nstep_num2factor(rain_tomorrow)\nWith long running models, you can work with a sample of the data in order to iterate through initial ranges of tuning grids. Once you start to narrow down the range of hyperparameter values, then you can adjust the portion of the data and/or number of folds.\ntrain_fold_small &lt;- train %&gt;%\nÂ  Â  Â  Â  Â  Â  sample_n(4000) %&gt;%\nÂ  Â  Â  Â  Â  Â  vfold_cv(v = 2)\n# change data obj in resamples arg of tune_grid\nAccessing a fitted model object to see coefficient values, hyperparameter values, etc.\nlin_trained &lt;- lin_workflow %&gt;%\nÂ  Â  Â  Â  finalize_workflow(select_best(lin_tune)) %&gt;%\nÂ  Â  Â  Â  fit(split_obj) # or train_obj or test_ob, etc.\n\nlin_trained$fit$fit %&gt;%\nÂ  Â  Â  Â  broom::tidy()\n\n# returns a parsnip object\nextract_fit_parsnip(lin_trained) %&gt;%\nÂ  Â  tidy()\n# returns an engine specific object\nxgboost::xgb.importance(model = extract_fit_engine(xgb_fit)\nUtilizing a sparse matrix with blueprint arg\nwf_sparse &lt;-Â \nÂ  workflow() %&gt;%\nÂ  add_recipe(text_rec, blueprint = sparse_bp) %&gt;%\nÂ  add_model(lasso_spec)\n\nestimates, model performance metrics, etc. are unaffected\nAdvantages\n\nSpeed is gained from any specialized model algorithms built for sparse data.\nThe amount of memory this object requires decreases dramatically.\n\nEngines that can utilize a sparse matrix: glmnet, xgboost, and ranger.\n\nIf your CV results have close scores â€“&gt; increase from 5-fold to 10 fold cv\n\nCan take much longer though\n\nBack-transform predictions if you made a non-recipe transformation of the target variable\n# log 10 transformed target variable\npreds_intervals &lt;- predict(\nÂ  workflows::pull_workflow_fit(lm_wf),\nÂ  workflows::pull_workflow_prepped_recipe(lm_wf) %&gt;% bake(ames_holdout),\nÂ  type = \"pred_int\",\nÂ  level = 0.90\n) %&gt;%Â \nÂ  mutate(across(contains(\".pred\"), ~10^.x))\nGet CV coefs (?)\nget_glmnet_coefs &lt;- function(x) {\nÂ  x %&gt;%\nÂ  Â  extract_fit_engine() %&gt;%\nÂ  Â  tidy(return_zeros = TRUE) %&gt;%\nÂ  Â  rename(penalty = lambda)\n}\nparsnip_ctrl &lt;- control_grid(extract = get_glmnet_coefs)\n\nglmnet_res &lt;-\nÂ  glmnet_wflow %&gt;%\nÂ  tune_grid(\nÂ  Â  resamples = bt,\nÂ  Â  grid = grid,\nÂ  Â  control = parsnip_ctrl\nÂ  )\nShowing table of results from multiple models\nlist(\nÂ  \"Original\" = unbalanced_model,Â \nÂ  \"Undersampled\" = balanced_model,Â \nÂ  \"Smote\" = smote_model\n) %&gt;%\nÂ  map(tidy) %&gt;%\nÂ  imap(~ select(.x, \"Term\" = term, !!.y := \"estimate\")) %&gt;%\nÂ  reduce(inner_join, by = \"Term\") %&gt;%\nÂ  pretty_print()"
  },
  {
    "objectID": "qmd/model-building-tidymodels.html#sec-modbld-tidymod-steps",
    "href": "qmd/model-building-tidymodels.html#sec-modbld-tidymod-steps",
    "title": "tidymodels",
    "section": "Steps",
    "text": "Steps\n\nset-up\nrecipe\nSpecify model(s)\nworkflow, workflow_set\nfit, fit_resamples, or tune_grid\nautoplot\ncollect metrics\nRepeat as necessary\nfinalize_workflow\nlast_fit"
  },
  {
    "objectID": "qmd/model-building-tidymodels.html#sec-modbld-tidymod-debug",
    "href": "qmd/model-building-tidymodels.html#sec-modbld-tidymod-debug",
    "title": "tidymodels",
    "section": "Checks, Debugging",
    "text": "Checks, Debugging\n\ncheck number of predictors and preprocessing results\ncheck_recipe &lt;- function(recipe_obj) {\nrecipe_obj %&gt;%Â  # already created recipe instructions\nÂ  Â  prep() %&gt;%Â  # instantiates recipe (makes necessary calcs)\nÂ  Â  bake() %&gt;% # executes recipe on data\nÂ  Â  glimpse()\n}\ncheck tuning error\ntune_obj$.notes[[2]] # 2 is the fold number\ncheck a particular fold after preprocessing\nrecipe_obj %&gt;%\ntraining(cv_obj$splits[[2]]) %&gt;% # 2 is the fold number\nprep()\nshow_best: Display the top sub-models and their performance estimates\nshow_best(tune_obj, \"rmse\")\n\nexample orders models by rmse\n\nCheck learning curves after tuning a model\nautoplot(tune_obj)"
  },
  {
    "objectID": "qmd/model-building-tidymodels.html#sec-modbld-tidymod-helpers",
    "href": "qmd/model-building-tidymodels.html#sec-modbld-tidymod-helpers",
    "title": "tidymodels",
    "section": "Helpers",
    "text": "Helpers\n\npredictions + newdata tbl for workflow and stacks objects\naugment.workflow &lt;- function(x, newdata, ...) {\npredict(x, newdata, ...) %&gt;%\nbind_cols(newdata)\n}\n# model_stack is the class of a stack obj\naugment.model_stack &lt;- function(x, newdata, ...) {\npredict(x, newdata, ...) %&gt;%\nbind_cols(newdata)\n}"
  },
  {
    "objectID": "qmd/model-building-tidymodels.html#sec-modbld-tidymod-setup",
    "href": "qmd/model-building-tidymodels.html#sec-modbld-tidymod-setup",
    "title": "tidymodels",
    "section": "Set-up",
    "text": "Set-up\n\nSome packages\n\ntidyverse - cleaning\ntidymodels - model building\nscales - loss metrics\nlubridate - ts cleaning functions\ntextrecipes - tokenizers, etc.\nthemis - up/down-sampling for imbalanced/unbalanced outcomes\nstacks - ensembling\n\nSplit\nset.seed(2021)\nspl &lt;- initial_split(dataset, prop = .75)\ntrain &lt;- training(spl)\ntest &lt;- testing(spl)\ntrain_folds &lt;- train %&gt;%\nÂ  Â  Â  Â  vfold_cv(v = 5)\n\nMight be useful to create a row id column before splitting to keep track of observations after the split\n\ndf %&gt;% rowid_to_column(var = \"row_id\")Â  then use step_rm(row_id) to remove it in the recipe\n\n\nMetrics\nmset &lt;- metric_set(mn_log_loss)\nParallel\ndoFuture::registerDoFuture()\ndoFuture::plan(multisession, workers = 4)\nGrid control options\ngrid_control &lt;- control_grid(\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # save the out-of-sample predictions\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  save_pred = TRUE,\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  save_workflow = TRUE,\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  extract = extract_model)\nggplot theme\ntheme_set(theme_light)\nLiked this theme for calibration curves"
  },
  {
    "objectID": "qmd/model-building-tidymodels.html#sec-modbld-tidymod-wkflw",
    "href": "qmd/model-building-tidymodels.html#sec-modbld-tidymod-wkflw",
    "title": "tidymodels",
    "section": "Workflow",
    "text": "Workflow\n\nformat: workflow function, add_recipe, add_model\nimb_wf &lt;-\nÂ  workflow() %&gt;%\nÂ  add_recipe(bird_rec) %&gt;%\nÂ  add_model(bag_spec)\nCan also add fit(training(splits)) to the pipe to go ahead and fit the model if youâ€™re not tuning, etc.\nFit multiple models and multiple recipes\npreproc &lt;- list(none = basic_recipe, pca = pca_recipe, sp_sign = ss_recipe)\nmodels &lt;- list(knn = knn_mod, logistic = lr_mod)\nwf_set &lt;- workflow_set(preproc, models, cross = TRUE)\n\ncross = TRUE says fit each model with each recipe"
  },
  {
    "objectID": "qmd/model-building-tidymodels.html#sec-modbld-tidymod-fit",
    "href": "qmd/model-building-tidymodels.html#sec-modbld-tidymod-fit",
    "title": "tidymodels",
    "section": "Fit",
    "text": "Fit\n\nfit:Â  fit a model on a dataset\nimb_fit &lt;- fit(workflow_obj, data = dat)\nfit_resample: cross-validation\nimb_rs &lt;-\nÂ  fit_resamples(\nÂ  Â  workflow_obj,\nÂ  Â  resamples = cv_folds_obj,\nÂ  Â  metrics = metrics_obj\nÂ  )\ncollect_metrics(imb_rs)"
  },
  {
    "objectID": "qmd/model-building-tidymodels.html#sec-modbld-tidymod-tune",
    "href": "qmd/model-building-tidymodels.html#sec-modbld-tidymod-tune",
    "title": "tidymodels",
    "section": "Tune",
    "text": "Tune\n\nCV and Tune Model with Multiple Recipes (article)\ndoParallel::registerDoParallel()\nset.seed(345)\n\ntune_res &lt;-\nÂ  Â  workflow_map(\nÂ  Â  Â  Â  wf_set,\nÂ  Â  Â  Â  \"tune_grid\",\nÂ  Â  Â  Â  resamples = cv_folds,\nÂ  Â  Â  Â  grid = 15,\nÂ  Â  Â  Â  metrics = metric_set(accuracy, roc_auc, mn_log_loss, sensitivity, specificity),\nÂ  Â  Â  Â  param_info = parameters(narrower_penalty)\nÂ  Â  )\n\nâ€œwf_setâ€ is a workflow_set object\nâ€œgrid = 15â€: this workflow set is only tuning one model, glmnet, and the parameter being tuned is the penalty. So â€œgrid = 15â€ says try 15 different values.\nâ€œnarrow_penaltyâ€ is narrower_penalty &lt;- penalty(range = c(-3, 0)\nViz: autoplot(tune_res)\n\n\nInterpretation:\n\nWhen accuracy is high, specificity (accuracy at picking negative class) is very high but sensitivity (accuracy at picking positive class) is very low.\nAdd middling values of specificity and sensitivity, mean log loss is also middling\nmean log loss is high when sensitivity is high\nAUROC is high at middling values of specificity and sensitivity indicating a best model for discriminating both classes. (but which recipe is this; see next section)\n\nWorkflow Rank (X-Axis): In this example there were 15 values of penalty considered and 2 differerent recipes, so Iâ€™m guessing thatâ€™s where the 30 comes from.\n\nExtract a particular model/recipe combo and viz\n\ndownsample_rs &lt;-\nÂ  tune_rs %&gt;%\nÂ  extract_workflow_set_result(\"downsampling_glmnet\")\nautoplot(downsample_rs)\n\nY-Axis: metric value, X-Axis: parameter value (e.g.Â glmnet penalty value)\nWhere â€œdownsampling_glmnetâ€ is the model/recipe combo you want to extract\nShows how downsampling affects the glmnet model\nInterpretation:\n\nLooks like the 1st half of workflow set in above viz is the downsampling recipe\nShows downsampling without recalibration of the predicted probabilities results in a model that can only pick 1 class well based on the penalty\nThe GOF metrics: high accuracy, low mean log loss, high auroc prefer the model that picks the negative class really well.\n\n\nShow results ordered by a metric\nrank_results(tune_rs, rank_metric = \"mn_log_loss\")\n\nresults ordered from best to worst according to the metric\n\n\ntune_grid\n\nFormat: workflow, data, grid, metrics, control\n\nlin_tune &lt;- lin_wf %&gt;%\nÂ  tune_grid(train_fold,\nÂ  Â  Â  Â  Â  Â  grid = crossing(penalty = 10 ^ seq(-6, -.5, .05)),\nÂ  Â  Â  Â  Â  Â  metrics = mset,Â  # see set-up section\nÂ  Â  Â  Â  Â  Â  control = grid_control) # see set-up section\nTuning multiples of the same hyperparameter\n\nadd id to specification (e.g.Â recipe step)\nuse id name instead of parameter name in grid\n\nstep_ns(var1, deg_free = tune(\"df_var1\"))\nstep_ns(var2, def_free = tune(\"df_var2\"))\n\ntune_obj &lt;- workflow_obj %&gt;%\nÂ  Â  tune_grid(train_fold,\nÂ  Â  Â  Â  Â  Â  Â  grid = crossing(df_var1 = 1:4, df_var2 = 4:6),\nÂ  Â  Â  Â  Â  Â  Â  metrics = mset,\nÂ  Â  Â  Â  Â  Â  Â  control = grid_control)\nGrids\n\n{dials}\npurrr::crossing\ngrid = crossing(mtry = 2:7,\nÂ  Â  Â  Â  Â  Â  Â  Â  trees - seq(100, 1000, 100),\nÂ  Â  Â  Â  Â  Â  Â  Â  learn_rate = 0.1)\nLatin Hypercube\nxgb_grid &lt;- grid_latin_hypercube(\nÂ  Â  tree_depth(),\nÂ  Â  min_n = sample(2, 10, 3)\nÂ  Â  loss_reduction = sample(runif(min = 0.01, max = 0.1), 3),\nÂ  Â  sample_size = sample(0.5:0.8, 3),\nÂ  Â  mtry = sample(3:6, 3),\nÂ  Â  learn_rate = sample(runif(min = 0.03, max = 0.1), 3)\n)\n\nsize: total number of parameter value combinations (default: 3)\n\nRandom\nxgb_grid &lt;- grid_random(\nÂ  Â  tree_depth(),\nÂ  Â  min_n(),\nÂ  Â  loss_reduction(),\nÂ  Â  learn_rate(),\nÂ  Â  finalize(sample_prop(), train_data),\nÂ  Â  finalize(mtry(), train_baked),\nÂ  Â  size = 20\n)\n\nsize: default: 5\n\n\nGet coefficients for folds\nget_glmnet_coefs &lt;- function(x) {\nÂ  x %&gt;%Â \nÂ  Â  extract_fit_engine() %&gt;%Â \nÂ  Â  tidy(return_zeros = TRUE) %&gt;%Â \nÂ  Â  rename(penalty = lambda)\n}Â \nglmnet_res &lt;-Â \nÂ  glmnet_wflow %&gt;%Â \nÂ  tune_grid(\nÂ  Â  resamples = bt,\nÂ  Â  grid = grid,\nÂ  Â  control = control_grid(extract = get_glmnet_coefs)\nÂ  )\nglmnet_coefs &lt;-Â \nÂ  glmnet_res %&gt;%Â \nÂ  select(id, .extracts) %&gt;%Â \nÂ  unnest(.extracts) %&gt;%Â \nÂ  select(id, mixture, .extracts) %&gt;%Â \nÂ  group_by(id, mixture) %&gt;%Â  Â  Â  Â  Â  # â”\nÂ  slice(1) %&gt;%Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # â”‚ Remove the redundant results\nÂ  ungroup() %&gt;%Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # â”˜\nÂ  unnest(.extracts)\nglmnet_coefs %&gt;%Â \nÂ  select(id, penalty, mixture, term, estimate) %&gt;%Â \nÂ  filter(term != \"(Intercept)\")\n#&gt; # A tibble: 300 Ã— 5\n#&gt;Â  Â  idÂ  Â  Â  Â  penalty mixture termÂ  Â  Â  estimate\n#&gt;Â  Â  &lt;chr&gt;Â  Â  Â  Â  &lt;dbl&gt;Â  &lt;dbl&gt; &lt;chr&gt;Â  Â  Â  Â  &lt;dbl&gt;\n#&gt;Â  1 Bootstrap1 1Â  Â  Â  Â  Â  0.1 Clark_LakeÂ  Â  0.391\n#&gt;Â  2 Bootstrap1 0.464Â  Â  Â  0.1 Clark_LakeÂ  Â  0.485\n#&gt;Â  3 Bootstrap1 0.215Â  Â  Â  0.1 Clark_LakeÂ  Â  0.590\n#&gt;Â  4 Bootstrap1 0.1Â  Â  Â  Â  0.1 Clark_LakeÂ  Â  0.680\n#&gt;Â  5 Bootstrap1 0.0464Â  Â  Â  0.1 Clark_LakeÂ  Â  0.746\n#&gt;Â  6 Bootstrap1 0.0215Â  Â  Â  0.1 Clark_LakeÂ  Â  0.793\n#&gt;Â  7 Bootstrap1 0.01Â  Â  Â  Â  0.1 Clark_LakeÂ  Â  0.817\n#&gt;Â  8 Bootstrap1 0.00464Â  Â  0.1 Clark_LakeÂ  Â  0.828\n#&gt;Â  9 Bootstrap1 0.00215Â  Â  0.1 Clark_LakeÂ  Â  0.834\n#&gt; 10 Bootstrap1 0.001Â  Â  Â  0.1 Clark_LakeÂ  Â  0.837\n#&gt; # â€¦ with 290 more rows\n\nextract_fit_engineadds an additional column â€œ.extractsâ€ to tuning output\nâ€œreturn_zerosâ€ keeps records of variables with penalized coefficients to zero during tuning\nViz\n\nglmnet_coefs %&gt;%Â \nÂ  filter(term != \"(Intercept)\") %&gt;%Â \nÂ  mutate(mixture = format(mixture)) %&gt;%Â \nÂ  ggplot(aes(x = penalty, y = estimate, col = mixture, groups = id)) +Â \nÂ  geom_hline(yintercept = 0, lty = 3) +\nÂ  geom_line(alpha = 0.5, lwd = 1.2) +Â \nÂ  facet_wrap(~ term) +Â \nÂ  scale_x_log10() +\nÂ  scale_color_brewer(palette = \"Accent\") +\nÂ  labs(y = \"coefficient\") +\nÂ  theme(legend.position = \"top\")\n\nInterpretation\n\nWith a pure lasso model (i.e., mixture = 1), the Austin station predictor is selected out in each resample. With a mixture of both penalties, its influence increases.\n\nAlso, as the penalty increases, the uncertainty in this coefficient decreases.\n\nThe Harlem predictor is either quickly selected out of the model or goes from negative to positive."
  },
  {
    "objectID": "qmd/model-building-tidymodels.html#sec-modbld-tidymod-final",
    "href": "qmd/model-building-tidymodels.html#sec-modbld-tidymod-final",
    "title": "tidymodels",
    "section": "Finalize",
    "text": "Finalize\n\nMisc\n\nMake sure to set control option, ctrl &lt;- control_grid(save_workflow = TRUE)\ncollect_notes(tuning_results) - If you need more information about errors or warnings that occur\n\nTo turn off the interactive logging, set the verbose control option to TRUE\n\n\nfit_best(x, metric = NULL, parameters = NULL, verbose = FALSE, ...) is a shortcut for:\nbest_param &lt;- select_best(tune_results, metric) # or other `select_*()`\nwflow &lt;- finalize_workflow(wflow, best_param)Â  # or just `finalize_model()`\nwflow_fit &lt;- fit(wflow, data_set)\n\nx: results of class, â€œtune_resultsâ€, from functions like tune_*\n\nSelect a simpler model or a more complex model\ndownsample_rs &lt;-Â \nÂ  tune_rs %&gt;%Â \nÂ  extract_workflow_set_result(\"downsampling_glmnet\")\n\nbest_penalty &lt;-Â \nÂ  downsample_rs %&gt;%\nÂ  select_by_one_std_err(-penalty, metric = \"mn_log_loss\")\n\nfinal_fit &lt;-Â \nÂ  wf_set %&gt;%Â \nÂ  extract_workflow(\"downsampling_glmnet\") %&gt;%\nÂ  finalize_workflow(best_penalty) %&gt;%\nÂ  last_fit(feeder_split)\n\nSelecting a simpler model can reduce chances of overfitting\nSelects a model that has a mean log loss score thatâ€™s 1 standard deviation from the top mean log loss value\n\nselect_by_pct_loss() also available\n\nThe first argument is passed to dplyr::arrange\n\nSo with â€œ-penalty,â€ penalty is a column in the results tibble that you want to sort by and the â€œ-â€ says you want start at the higher penalty parameter values\n\nA higher penalty (lasso model) means a simpler model\nâ€œtune_rsâ€ is a workflow_map object\nThe last chunk finalizes the original tuneable workflow with this value for the penalty, and then last_fit Â  fits the model one time to the (entire) training data and evaluates one time on the testing data.\nMetrics\ncollect_metrics(final_fit)\ncollect_predictions(final_fit) %&gt;%\nÂ  conf_mat(squirrels, .pred_class)\nVariable Importance\nlibrary(vip)\nfeeder_vip &lt;-\nÂ  extract_fit_engine(final_fit) %&gt;%\nÂ  vi()\nfeeder_vip\n\nfeeder_vip %&gt;%\nÂ  group_by(Sign) %&gt;%\nÂ  slice_max(Importance, n = 15) %&gt;%\nÂ  ungroup() %&gt;%\nÂ  ggplot(aes(Importance, fct_reorder(Variable, Importance), fill = Sign)) +Â \nÂ  geom_col() +\nÂ  facet_wrap(vars(Sign), scales = \"free_y\") +\nÂ  labs(y = NULL) +\nÂ  theme(legend.position = \"none\")\n\nfinalize_model, recipe, workflow\n\nafter finding the best parameter values, this updates the object\n\nlin_wf_best &lt;- lin_wf %&gt;%\nÂ  finalize_workflow(select_best(lin_tune))\n\n# or manually list parameter/vale pairs\nxgb_wf_best &lt;- lin_wf %&gt;%Â \nÂ  finalize_workflow(list(mtry = 6, trees = 1200, learn_rate = 0.01)\nlast_fit: fit finalized model on entire learning dataset and collect metrics\nxgb_wf_best %&gt;%\nÂ  Â  last_fit(initial_split_obj, metrics = metric_set_obj) %&gt;%\nÂ  Â  collect_metrics()\n\nManually\nÂ  Â  Â  Â  xgb_wf_best %&gt;%\nÂ  Â  Â  Â  Â  Â  fit(train) %&gt;%\nÂ  Â  Â  Â  Â  Â  augment(test, type_predict = \"prob\") %&gt;%\nÂ  Â  Â  Â  Â  Â  mn_log_loss(churned, .pred_yes)\n\nPredict on new data\nxgb_wf_best %&gt;%\nÂ  Â  fit(new_data) %&gt;%\nÂ  Â  predict(newdata, type = \"prob\") %&gt;%\nÂ  Â  bind_cols(newdata)\nPredict on test set and score\nwflw_fit %&gt;%\nÂ  Â  predict(testing(splits) %&gt;% select(outcome_var)) %&gt;%\nÂ  Â  metric_set(rmse, mae, rsq)(outcome_var, .pred)"
  },
  {
    "objectID": "qmd/model-building-tidymodels.html#sec-modbld-tidymod-modspec",
    "href": "qmd/model-building-tidymodels.html#sec-modbld-tidymod-modspec",
    "title": "tidymodels",
    "section": "Model Specification",
    "text": "Model Specification\n\nFormat: model function, set_engine, set_mode (â€œclassificationâ€ or â€œregressionâ€)\nbag_spec &lt;-\nÂ  bag_tree(min_n = 10) %&gt;%\nÂ  set_engine(\"rpart\", times = 25) %&gt;%\nÂ  set_mode(\"classification\")\nLogistic/Linear\n# lasso\nlogistic_reg(penalty = tune()) %&gt;%\nÂ  Â  set_engine(\"glmnet\")\n\nfunction: logistic_reg, linear_reg\n\nargs: penalty (Î»), mixture (Î±) (number or tune())\n\npenalty grid examples: 10^seq(-7, -1, .1), 10 ^ seq(-6, -.5, .05)\npenalty default: penalty(range = c(-10, 0), trans = log10_trans())\n\nSilge uses range = c(-3, 0)\n\n\n\nengines\n\ntypes: glm, glmnet, LiblineaR (only logistic), stan, spark, keras\nLiblinear\n\nbuilt for large data\n\nfaster than using glmnet even w/sparse matrix\n\nrequires mixture = 0 or 1; default: 0 (ridge)\nalso regularizes intercept which affects coef values\n\nglmnet\n\nmixture default = 1 (lasso)\n\nspark\n\nmixture default = 0 (ridge)\n\nargs\n\npath_values: sequence ofÂ  values for penalty\n\nassigns a collection of penalty values used by each glmnet fit (regardless of the data or value of mixture)\n\nWithout this arg, glmnet uses multiple penalty values which depend on the data set; changing the data (or the mixture amount) often produces a different set of values\n\nSo, if you want to look at results for a specific penalty value, you need to use this arg to make sure its included.\n\n\nmust still give penalty arg a number\nif using ridge, include 0 in sequence of values for penalty\n\npath_values = c(0, 10^seq(-10, 1, length.out = 20))\n# example in docs\n10^seq(-3, 0, length.out = 10)\n# drob's range\n10 ^ seq(-6, -.5, .05)\n\n\n\nSupport Vector Machines (SVM)\nsvm_linear() %&gt;%Â \nÂ  set_engine(\"LiblineaR\") %&gt;%Â \nÂ  set_mode(\"regression\")\n\nmodes: regression or classification\nengines\n\nLiblineaR\n\nbuilt for bigdata (C++ library)\nNo probability predictions for classification models\n\nIn order to use tune_grid, metrics must be hard classification metrics (e.g.Â accuracy, specificity, sensitivity, etc.)\nWonâ€™t be able to adjust thresholds either\n\nL2-regularized\n\nkernlab\n\nkernel = â€œvanilladotâ€\n\nargs\n\ncost: default = 1\nmargin: default = 0.1\n\n\n\nBoosted Trees\nxgb_mod &lt;- boost_tree(\"regression\",\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  mtry = tune(),\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  trees = tune(),\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  learn_rate = 0.01,\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  stop_iter = tune()) %&gt;%\nÂ  Â  Â  Â  set_engine(\"xgboost\")\n\nXGBoost\n\ndrob says\n\nlogging predictors has no effect on this algorithm\n\nargs\n\nmtry\ntrees\nlearn_rate\ntree_depth\nstop_iter: early stopping; stops if no improvement has been made after this many iterations\n\n\n\nRandom Forest\nrand_forest(\nÂ  mode = \"regression\",\nÂ  engine = \"ranger\",\nÂ  mtry = tune(),\nÂ  trees = tune(),\nÂ  min_n = tune()\n) %&gt;%\nÂ  set_engine(importance = \"permutation\",\nÂ  Â  Â  Â  Â  Â  seed = 63233,\nÂ  Â  Â  Â  Â  Â  quantreg = TRUE)\n\nranger doc\nargs\n\nmode: â€œclassificationâ€, â€œregressionâ€, â€œunknownâ€\nengine: ranger (default), randomForest, spark\ndefault hyperparameters: mtry, trees, min_n\nimportance: â€˜noneâ€™, â€˜impurityâ€™, â€˜impurity_correctedâ€™, â€˜permutationâ€™\nnum.threads = 1 (default)\nquantreg = TRUE for quantile regression\npreds &lt;- regr_fit$fit %&gt;% predict(airquality, type = 'quantiles')\nhead(preds$predictions)\n\nCurrently have to extract the ranger model in order to obtain quantile predictions\nissue still open"
  },
  {
    "objectID": "qmd/model-building-tidymodels.html#sec-modbld-tidymod-recipe",
    "href": "qmd/model-building-tidymodels.html#sec-modbld-tidymod-recipe",
    "title": "tidymodels",
    "section": "Recipe",
    "text": "Recipe\n\nStep args are tunable. Just use tune() for value in the arg and include it in tune_grid\n\ne.g spline degrees in step_ns or bs, number of bootstraps in step_impute_bag, or number of neighbors in step_impute_knn\n\nrecipe(formula, data)\nrecipe(rain_tomorrow ~ date + location + rain_today +\nÂ  Â  Â  Â  Â  Â  Â  Â  min_temp + max_temp + rainfall +\nÂ  Â  Â  Â  Â  Â  Â  Â  wind_gust_speed + wind_speed9am +\nÂ  Â  Â  Â  Â  Â  Â  Â  wind_speed3pm + humidity9am + humidity3pm + pressure9am +\nÂ  Â  Â  Â  Â  Â  Â  Â  pressure3pm + cloud9am + cloud3pm + temp9am + temp3pm +\nÂ  Â  Â  Â  Â  Â  Â  Â  wind_gust_dir + wind_dir9am + wind_dir3pm +\nÂ  Â  Â  Â  Â  Â  Â  Â  rain_today, data = train)\nVariable selectors\n\nbased on role\n\nhas_role, all_outcomes, all_predictors, all_numeric_predictors, all_nominal_predictors\n\nbased on type\n\nhas_type, all_numeric, all_nominal\n\n\nMisc\n\nzv, nzv\n\nRemoves predictors with zero variance or near-zero variance\n\nstep_nsv(all_predictors(), freq_cut = 90/10)\n\nfreq_cut: ratio that determines when indicator variables with too few 1s or too few 0s are removed.\n\ndefault: 95/5\n\nunique_cut: percentage (not a decimal), number of unique values divided by the total number of samples\n\ndefault: 10\n\n\n\nTransformations\n\nlog, BoxCox, YeoJohnson, sqrt\nstep_log(rainfall, offset = 1, base = 2)\n\nIf variable has zeros, offset adds 1 to the value.\nbase is the log base\n\nsplines\n\nbs (basis splines); ns (natural splines), poly\n\nMore are available, see docs\n\nboth have deg_free args where larger values â€“&gt; more complex curves\ndegree arg: bs-only, degree of polynomial spline (e.g.Â 2 quadratic, 3 cubic)\noriginal variable replaced with â€œvarname_ns_1â€\n\n\nCategorical\n\nAlso see\n\nDOCS: Handling Categorical Predictors\nFeature Engineering, General &gt;&gt; Categoricals\n\nstep_dummy: if there are C levels of the factor, there will be C - 1 dummy variables created and all but the first factor level are made into new columns\nstep_other\n\ncollapses categories that have a frequency below a specified threshold\nthreshold: proportion of total rows that a category will be collapsed into other\n\ndefault = 0.05\ngrid value examples: 0.001, 0.003, 0.01, 0.03\n\n\nstep_unknown\n\nIf NAs arenâ€™t random, they can be pooled into a category called â€œUnknownâ€\n\n\nImputation step_impute_*\n\nAlso see Missingness\ntldr;\n\nTry bag first, knn second\n\nSeems to me that bag would be more flexible\n\nrolling would be useful for time series\n\ncommon args\n\nimpute_with: arg for including other predictors (besides outcome and variable to be imputed) in the imputation model.\nstep_impute_&lt;method&gt;(var, impute_with = imp_vars(pred1, pred2))\nâ€œskipâ€ arg: When set to false, this step will not be used when the recipe is baked (unless newdata=NULL)\n\nuseful when down/up-sampling an outcome variable. New data should be asis in the wild. So usecases where the step is good for training but not production.\ndefault = T for step_filter, slice, sample, naomit\n\n\nbag\n\nbagged tree model\ncategoricals and numerics can be imputed\nPredictors can have NAs but canâ€™t be included in the list of variables to be imputed\n\nGuess you could do separate imputation steps if you want to include them as predictors\n\nspeed up by lowering bootstrap replications\n\n# keepX = FALSE says don't return a df of predictions - not sure why that's included (memory optimization?)\noptions = list(nbagg = 5, keepX = FALSE))\nknn\n\nK-Nearest Neighbors\ncategoricals and numerics can be imputed\n\ngower distance used (only distance available)\n\npossible that missing values will still occur after imputation if a large majority (or all) of the imputing variables (predictors?) are also missing\nCan be computationally intensive and is NOT robust to outliers\nneighbors = 5 by default\noptions: nthreads, eps (threshold for values to be set to 0, default = 1e-8)\n\nlinear, mean, median\n\nregression model, mean, median\nonly numerics can be imputed\nif missingness is missing not at random (MNAR) then mean shouldnâ€™t be used (probably not median either)\n\nmode\n\nmost common value\ndiscrete variable (maybe only nominal variables)\nif more than one mode, one of them will be chosen at random\n\nroll\n\nrolling window\nwindow: window of data that you want to use to calculate the statistic\n\nFor time series, maybe look at a acf, pacf plot to get an idea for a good window size\n\nstatistic: stat to calculate over the window (e.g.Â mean, median, etc)\n\nprobably takes a custom function with a single arg (data)\n\nvalues in the window that would be imputed in previous steps arenâ€™t used to impute the current value.\n\n\nup/down-sampling\n\n{themis} for unbalanced outcome variables\nCommon args\n\nâ€œskipâ€: When set to false, this step will not be used when the recipe is baked (unless newdata=NULL)\n\nuseful when down/up-sampling an outcome variable. New data should be asis in the wild. So usecases where the step is good for training but not production.\ndefault = T for step_filter, slice, sample, naomit, and themis data balancing steps\n\nâ€œover_ratioâ€: desired ratio of num_rows ofÂ  the minority level(s) to num_rows of the majority level. Default = 1 (i.e.Â all levels have equal number of rows)\n\nExample: Bring the minority levels up to about 200 rows each where the majority level has 16562 rows in the data. Therefore, 200/16562 is approx 0.0121. Set over_ratio to 0.0121 and each minority level will have 200 rows in the upsampled dataset.\n\nâ€œneighborsâ€:Â  number of clusters, default = 5\n\nUpsampling\n\nAll methods handle multi-level factor variables except rose.\nMisc\n\nApplying SMOTE without re-calibration is bad (paper)\n\ntldr;\n\nIf time isnâ€™t an issue AND you have a binary variable, rose sounds like a good approach, otherwise use bsmote (probably should tune the all_neighbor option)\n\nupsample\n\nrepeats rows of minority level until dataset has desired ratio\n\nsmote\n\nSMOTE alg performs knn, generates points between the that levelâ€™s points and the centroid for each cluster\nPreprocessing: Every predictor in the formula must be numeric with no missing data.\n\nimpute missing data and encode all categoricals before using this step\n\nGenerated points can form â€œbridgesâ€ between that levelâ€™s outlier points and the main distribution. This is like adding a feature that doesnâ€™t exist in the sample.\n\nbsmote\n\nborderline-SMOTE alg is the same as SMOTE except that it generates points in the border region between classes. Ignores outliers in the resampling process.\nPreprocessing: Every predictor in the formula must be numeric with no missing data.\n\nimpute missing data and encode all categoricals before using this step\n\nall_neighbors; default = FALSE\n\nFALSE then points will be generated between nearest neighbors in its own class.\n\nThis can produce mini-clusters of points\n\nTRUE then points will be generated between neighbors of the upsampled class and the other classes.\n\nMay blur the line between clusters (and levels? Therefore potentially making classification tasks more difficult)\n\n\nOnly selecting from the borderline region can be an issue if there arenâ€™t that many borderline points in the original dataset.\n\nadasyn\n\nsimilar to SMOTE, but generates the most points from points that have the smallest representation in the data, i.e.Â generates a lot of outlier or extreme value points.\n\nMight produce much more stable results with tree models as compared to regression models\n\nPreprocessing: Every predictor in the formula must be numeric with no missing data.\n\nimpute missing data and encode all categoricals before using this step\n\n\nrose\n\nUses a â€œsmoothedâ€ bootstrap approach to generate new samples. (paper was incoherent as fuck)\nOnly for binary (factor) variables\nPreprocessing: Every predictor in the formula must be numeric with no missing data.\n\nimpute missing data and encode all categoricals before using this step\n\nno mention of missing data in the docs but probably still a good idea.\n\n\nminority_prop: same function as over_ratio. Default = 0.5\nminority_smoothness and majority_smoothness\n\ndefault = 1. Reducing this value will shrink the (resampling?) region in the feature space associated with that class (minority or majority).\nIf these regions are too large, the boundary between class regions blurs. If too small, maybe you arenâ€™t getting the variance out of the resampling that you could be getting.\n\n\n\nDownsampling\n\nMisc\n\nManually\nminority_class &lt;- data %&gt;%\nÂ  count(class) %&gt;%\nÂ  filter(n == min(n))\n\nbalanced &lt;- data %&gt;%\nÂ  group_split(class, .keep = TRUE) %&gt;%\nÂ  map_dfr(\nÂ  Â  ~ {\nÂ  Â  Â  if (.x$class[1] == minority_class$class) {\nÂ  Â  Â  Â  .x\nÂ  Â  Â  } else {\nÂ  Â  Â  Â  slice_sample(\nÂ  Â  Â  Â  Â  .x,\nÂ  Â  Â  Â  Â  n = minority_class$n,\nÂ  Â  Â  Â  Â  replace = FALSE\nÂ  Â  Â  Â  )\nÂ  Â  Â  }\nÂ  Â  }\nÂ  )\n\nbalanced %&gt;%\nÂ  count(class) %&gt;%\nÂ  pretty_print()\n\ntldr;\n\nnone of the methods below sound too bad.\n\ndownsample\n\nremoves majority level(s) rows randomly to match the under_ratio (see over_ratio defintion above)\n\nnearmiss\nremoves majority level(s) rows by undersampling points in the majority level(s) based on their distance to other points in the same level. - Default: neighbors = 5 - Preprocessing: Every predictor in the formula must be numeric with no missing data.\n\nimpute missing data and encode all categoricals before using this step\n\ntomek\n\nundersamples by removing pairs of points that each otherâ€™s nearest neighbor but are of different levels (tomek links)\n\nDoesnâ€™t make sense to me this way. SInce itâ€™s binary, maybe the point thatâ€™s the majority class gets removed the other point in the pair (minority class) gets saved.\n\nOnly for binary (factor) variables\nPreprocessing: Every predictor in the formula must be numeric with no missing data.\n\nimpute missing data and encode all categoricals before using this step"
  },
  {
    "objectID": "qmd/model-building-tidymodels.html#sec-modbld-tidymod-ex",
    "href": "qmd/model-building-tidymodels.html#sec-modbld-tidymod-ex",
    "title": "tidymodels",
    "section": "Examples",
    "text": "Examples\n\nBasic workflow (article)\npacman::p_load(tidymodels, embed, finetune, vip)\n\n# \"Accredation\" is the binary outcome\nset.seed(123)\nmuseum_split &lt;- initial_split(museum_parsed, strata = Accreditation)\nmuseum_train &lt;- training(museum_split)\nmuseum_test &lt;- testing(museum_split)\nset.seed(234)\nmuseum_folds &lt;- vfold_cv(museum_train, strata = Accreditation)\n\n# \"Subject_Matter\" is a high cardinality categorical predictor\nmuseum_rec &lt;-Â \nÂ  recipe(Accreditation ~ ., data = museum_train) %&gt;%\nÂ  update_role(museum_id, new_role = \"id\") %&gt;%\nÂ  step_lencode_glm(Subject_Matter, outcome = vars(Accreditation)) %&gt;%\nÂ  step_dummy(all_nominal_predictors())\n\n# specify model and workflow\nxgb_spec &lt;-\nÂ  boost_tree(\nÂ  Â  trees = tune(),\nÂ  Â  min_n = tune(),\nÂ  Â  mtry = tune(),\nÂ  Â  learn_rate = 0.01\nÂ  ) %&gt;%\nÂ  set_engine(\"xgboost\") %&gt;%\nÂ  set_mode(\"classification\")\nxgb_wf &lt;- workflow(museum_rec, xgb_spec)\n\n# fit\ndoParallel::registerDoParallel()\nset.seed(345)\nxgb_rs &lt;- tune_race_anova(\nÂ  xgb_wf,\nÂ  resamples = museum_folds,\nÂ  grid = 15,\nÂ  control = control_race(verbose_elim = TRUE)\n)\n\n# evaluate\ncollect_metrics(xgb_rs)\n# viz for race_anova grid search strategy\nplot_race(xgb_rs)\n\n # fit on whole training set\nxgb_last &lt;- xgb_wf %&gt;%\nÂ  finalize_workflow(select_best(xgb_rs, \"accuracy\")) %&gt;%\nÂ  last_fit(museum_split)\n\n# evalute final model on test\ncollect_metrics(xgb_last)\ncollect_predictions(xgb_last) %&gt;%\nÂ  Â  conf_mat(Accreditation, .pred_class)\nxgb_last %&gt;%\nÂ  extract_fit_engine() %&gt;%\nÂ  vip()"
  },
  {
    "objectID": "qmd/network-analysis.html#sec-netanal-misc",
    "href": "qmd/network-analysis.html#sec-netanal-misc",
    "title": "Network Analysis",
    "section": "Misc",
    "text": "Misc\n\nPackages\n\n{statnet} - statnet is a collection of software packages for statistical network analysis that are designed to work together, with a common data structure and API, to provide seamless access to a broad range of network analytic and graphical methodology.\n\nList of individual package tutorials\nModels fit with MCMC, so can be slow.\n\n\nResources\n\nNetwork Analysis: Integrating Social Network Theory, Method, and Application with R\n\nAnalysis Questions\n\nAt a given moment in time:\n\nWho is connected to whom? Who is not connected?\nWhere, and who are the hubs?\nWhere and about what are the clusters? Are there silos?\n\nChanges over time:\n\nAre new connection forming?\nAre new patterns of connectivity forming?\nHow was our network before and after the introduction of an activity?\n\n\nIssues with Statistics\n\nThey are unable to leverage node features at all. All nodes with the same values for these summary statistics are indistinguishable from each other.\nThere is no learnable component in the production of these features. We cannot fit a custom objective or train them jointly with a downstream task."
  },
  {
    "objectID": "qmd/network-analysis.html#sec-netanal-terms",
    "href": "qmd/network-analysis.html#sec-netanal-terms",
    "title": "Network Analysis",
    "section": "Terms",
    "text": "Terms\n\nCentrality\n\nMeasures, abstractly, how important a given graph is to the connectivity of the overall graph\nHigher for nodes which lie in paths that efficiently connect many nodes to each other.\nTypes:\n\nBetweenness - Nodes with high betweenness centrality tend to be the â€œcrossroadsâ€ between nodes, i.e.Â when seeking to connect with another node that isnâ€™t immediately adjacent, it will typically involve a node with high betweeness centrality.\n\nThese nodes are important in keeping the network connected. Likely important intermediaries or bridges\nCalculated by counting the number of shortest paths that pass through a node and dividing by the total number of shortest paths in the network.\n\nCloseness - Nodes with high closeness centrality have quick access to many other nodes. These nodes have the shortest distance, in network terms, to all other nodes.\n\nThese nodes are important in spreading information to all other nodes as quickly and efficiently as possible.\nCloseness Centrality = 1 / (Sum of SPD from the node to all other nodes)\n\nWhere SPD is the shortest path distance. In practice, this would be done with a shortest path algorithm likeÂ Breadth-First SearchÂ orÂ Dijkstraâ€™s algorithm.\n\n\n\n\nClusters\n\nCluster Clique - a cluster that has at least one node thatâ€™s connected to another node outside of the cluster\nCluster Silo - a cluster that has no node connected to any other node outside of the cluster\n\nClustering coefficient\n\nMeasures the density of a nodeâ€™s local portion of the graph.\nNodes who have neighbors that are all connected to each other will have a higher clustering coefficient\n\nDegeneracy - A network model is degenerate when the space that an MCMC sampler can explore is so constrained that the only way to get the observed g(y) is essentially to flicker between full and empty graphs in the right proportion.\n\nA good indication that you have a degenerate model is that you have NA values for standard errors on your model parameter estimates. You canâ€™t calculate a variance â€“ and, therefore, a standard error â€“ if you simply flicker between full and empty graphs.\n\nDegree aka Degree Centrality - Total edges a given node has.\nDensity - the number of edges in the observed network divided by the number of possible edges\nEdges (aka Dyads)- connection between two nodes. Depending on the type of connection, the edge can have a direction.\nEdge Bundling - visually bundle together similar edges to reduce the visual clutter within a graph\nMultiplexity - The number of connections between two nodes\n\nCould be represented by the thickness, darkness, etc. of an edge between 2 nodes\nNodes that have high multiplicity with each other typically form clusters\n\nRandom Graph - A network with n nodes where the edges between nodes occur randomly with probability P (each potential edge is one Bernoulli trial). Network density is typically used for P.\nTransitivity of a relation means that when there is a tie from i to j, and also from j to h, then there is also a tie from i to h: friends of my friends are my friends\n\n\nTransitivity Index (aka Clustering Index) = # Transitive Triads / # Potentially Transitive Triads\n\nHas a range between 0 and 1 where 1 is a transitive graph.\nFor random graphs, the expected value of the transitivity index is close to the density of the graph."
  },
  {
    "objectID": "qmd/network-analysis.html#sec-netanal-layalg",
    "href": "qmd/network-analysis.html#sec-netanal-layalg",
    "title": "Network Analysis",
    "section": "Layouts",
    "text": "Layouts\n\nSpring\n\nFruchterman-Reingold force-directed algorithm\n\narranges the nodes so the edges have similar length and minimum crossing edges\n\n\nRandom - nodes positioned uniformly at random in the unit square\nCircular - nodes on a circle\nBipartite - nodes in two straight lines\nSpectral - nodes positioned using the eigenvectors of the graph Laplacian"
  },
  {
    "objectID": "qmd/network-analysis.html#exponential-random-graph-models-ergm",
    "href": "qmd/network-analysis.html#exponential-random-graph-models-ergm",
    "title": "Network Analysis",
    "section": "Exponential Random Graph Models (ERGM)",
    "text": "Exponential Random Graph Models (ERGM)\n\nAnalogous to logistic regression: ERGMs predict the probability that a pair of nodes in a network will have a tie between them.\nMisc\n\nNotes from Introduction to ERGMs\nPackages\n\n{statnet} - See Misc &gt;&gt; Packages\n{ergmito} - Simulation and estimation of Exponential Random Graph Models (ERGMs) for small networks (up to 5 vertices for directed graphs and 7 for undirected networks) using exact statistics\n\nIn the case of small networks, the calculation of the likelihood of ERGMs becomes computationally feasible, which allows us avoiding approximations and do exact calculation, ultimately obtaining MLEs directly.\n\n\nCan be used for directed, undirected, valued, unvalued, and bipartite networks.\nFitting a model with just edges is kind of like an intercept-only regression model.\nLess informative for dense networks.\n\nExamples: From Introduction to ERGMs\n\nDense\n\n\n\nNot Dense\n\n\nTriads aka triangles (i.e.Â transitive relationships) cause problems in ERGMs (more triads â†’ denser graph). They often lead to degeneracy.\n\nNAs for standard error estimates are a good indication of degeneracy.\nSince ERGMs donâ€™t handle triads well, it is NOT recommended using â€œtriangleâ€ as an adjustment variable in your model\n\n\n\nGoal: Describe the local selection forces that shape the global structure of a network\nExamples of networks examined using ERGM include knowledge networks, organizational networks, colleague networks, social media networks, and networks of scientific development.\nThe basic principle underlying the method is comparison of an observed network to Exponential Random Graphs.\n\nThe Null Hypothesis is a Erdos-Renyi graph\n\nA random graph where the degree of any node is binomially distributed (with n-1 Bernoulli trials per node, for a directed graph). (n is the number of nodes)\n\n\nEquation:\n\\[\n\\text{logit}(Y_{ij} = 1 \\; | \\; y_{ij}^c) = \\theta^T \\delta (y_{ij})\n\\]\n\n\\(\\theta\\) is a vector of coefficients\n\\(y_{ij}\\) denotes ijth dyad in graph \\(y\\)\n\nIf \\(y_{ij} = 1\\), then i and j are connected by an edge.\nIf \\(y_{ij} = 0\\), then i and j are NOT connected by an edge.\n\\(y_{ij}^c\\) is the complement (i.e.Â all other pairs of vertices in \\(y\\) other than (i, j)).\n\n\\(\\delta(y_{ij})\\) is the change statistic. A measure of how the graph statistic \\(g(y)\\) changes if the ijth vertex is toggled on or off.\n\n\\(\\delta(y_{ij}) = g(y_{ij}^+) - g(y_{ij}^-)\\)\n\\(y_{ij}^+\\) is the same network as \\(y\\) except that \\(y_{ij} = 1\\).\n\\(y_{ij}^-\\) is the same network as \\(y\\) except that \\(y_{ij} = 0\\).\n\n\nExample:{statnet} Edges model\nbottmodel.01 &lt;- ergm(bott[[4]] ~ edges)\n\n## Evaluating log-likelihood at the estimate.\nsummary(bottmodel.01)\n## \n## ==========================\n## Summary of model fit\n## ==========================\n## \n## Formula:   bott[[4]] ~ edges\n## \n## Iterations:  4 out of 20 \n## \n## Monte Carlo MLE Results:\n##       Estimate Std. Error MCMC %  p-value    \n## edges  -0.7621     0.2047      0 0.000313 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n##      Null Deviance: 152.5  on 110  degrees of freedom\n##  Residual Deviance: 137.6  on 109  degrees of freedom\n##  \n## AIC: 139.6    BIC: 142.3    (Smaller is better.)\nExample:{statnet} Edges and Triads model\nsummary(bott[[4]]~edges+triangle)\n##    edges triangle \n##       35       40\n\nbottmodel.02 &lt;- ergm(bott[[4]]~edges+triangle)\nsummary(bottmodel.02)\n## \n## ==========================\n## Summary of model fit\n## ==========================\n## \n## Formula:   bott[[4]] ~ edges + triangle\n## \n## Iterations:  2 out of 20 \n## \n## Monte Carlo MLE Results:\n##          Estimate Std. Error MCMC % p-value\n## edges    -0.55772    0.58201      0   0.340\n## triangle -0.05674    0.15330      0   0.712\n## \n##      Null Deviance: 152.5  on 110  degrees of freedom\n##  Residual Deviance: 137.5  on 108  degrees of freedom\n##  \n## AIC: 141.5    BIC: 146.9    (Smaller is better.)\nattributes of the individuals who make up our graph vertices may affect their propensity to form (or receive) ties\ntest this hypothesis, we can employ nodal covariates using the nodecov() term.\nbottmodel.03 &lt;- ergm(bott[[4]]~edges+nodecov('age.month'))\nsummary(bottmodel.03)\n## \n## ==========================\n## Summary of model fit\n## ==========================\n## \n## Formula:   bott[[4]] ~ edges + nodecov(\"age.month\")\n## \n## Iterations:  4 out of 20 \n## \n## Monte Carlo MLE Results:\n##                    Estimate Std. Error MCMC % p-value\n## edges             -1.526483   1.335799      0   0.256\n## nodecov.age.month  0.009501   0.016352      0   0.562\n## \n##      Null Deviance: 152.5  on 110  degrees of freedom\n##  Residual Deviance: 137.3  on 108  degrees of freedom\n##  \n## AIC: 141.3    BIC: 146.7    (Smaller is better.)\n\nresult suggests that in this dataset, the combined age of the children involved in a dyad as no effect on the probability of a tie between them, in either direction.\n\nimitation is a directed relationship. We might expect that older children are more likely to be role models to others. To test this hypothesis, we can use the directed variants ofÂ nodecov():Â nodeocov()Â (effect of an attribute on out-degree) andÂ nodeicov()Â (effect of an attribute on in-degree). We note that theÂ nodecov()Â group of terms are for numeric attributes;Â nodefactor()Â terms are available for categorical attributes.\nbottmodel.03b &lt;- ergm(bott[[4]]~edges+nodeicov('age.month'))\n\nsummary(bottmodel.03b)\n## \n## ==========================\n## Summary of model fit\n## ==========================\n## \n## Formula:   bott[[4]] ~ edges + nodeicov(\"age.month\")\n## \n## Iterations:  4 out of 20 \n## \n## Monte Carlo MLE Results:\n##                    Estimate Std. Error MCMC % p-value   \n## edges              -2.89853    0.96939      0 0.00345 **\n## nodeicov.age.month  0.05225    0.02272      0 0.02340 * \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n##      Null Deviance: 152.5  on 110  degrees of freedom\n##  Residual Deviance: 132.1  on 108  degrees of freedom\n##  \n## AIC: 136.1    BIC: 141.5    (Smaller is better.)\n\nThe number of other children who imitated a child increase with the childâ€™s age"
  },
  {
    "objectID": "qmd/network-analysis.html#sec-netanal-ndembd",
    "href": "qmd/network-analysis.html#sec-netanal-ndembd",
    "title": "Network Analysis",
    "section": "Node Embeddings",
    "text": "Node Embeddings\n\nLearnable vectors of numbers that can be mapped to each node in the graph, allowing us to learn a unique representation for each node.\n\nUse as features in a downstream model.\n\nMethods\n\nDeepWalk and Node2vec papers\n\nUse the concept of a random walk, which involves beginning at a given node and randomly traversing edges, to produce pairs of nodes that are nearby each other.\nTrained by maximizing the cosine similarity between nodes that co-occurred in random walks.\n\nThis training objective leverages the homophily assumption, which states that nodes that are connected to each other tend to be similar to each other.\n\n\n\nIssues with Embeddings\n\nThey do not use node features at all. They assume that close-by nodes are similar without actually using the node features to confirm this assumption.\n\nThey rely on a fixed mapping from node to embedding (i.e.Â this is a transductive method).\n\nFor dynamic graphs, where new nodes and edges may be added, the algorithm must be re-ran from scratch, and all node embeddings need to be recalculated. In real-world problems, this is quite a big issue, as most online platforms have new users signing up every day, and new edges being created constantly."
  },
  {
    "objectID": "qmd/network-analysis.html#sec-netanal-gcn",
    "href": "qmd/network-analysis.html#sec-netanal-gcn",
    "title": "Network Analysis",
    "section": "Graph Convolutional Networks (GCN)",
    "text": "Graph Convolutional Networks (GCN)\n\nLearns representations of nodes by learning a function that aggregates a nodeâ€™s neighborhood (the set of nodes connected to the original node), using both graph structure and node features.\n\nThese representations are a function of a nodeâ€™s neighborhood and are not hardcoded per node (i.e.Â this is an inductive method), so changes in graph structure do not require re-training the model.\n\nFor unsupervised learning tasks, the method is similar to Node2vec/DeepWalk\nLayers\n\nA layer takes a weighted average of the node features in the original nodeâ€™s neighborhood, and the weights are learned by training the network\nAdding layers produces aggregations that use more of the graph.\n\nThe span of the subgraph used to produce a nodeâ€™s embedding is expanded by 1 hop."
  },
  {
    "objectID": "qmd/nlp-fine-tuning.html#sec-nlp-fintun-misc",
    "href": "qmd/nlp-fine-tuning.html#sec-nlp-fintun-misc",
    "title": "Fine Tuning",
    "section": "Misc",
    "text": "Misc\n\nTuning an LLM\n\nNotes from:\n\nHackerâ€™s Guide to Language Models (Howard)\n\nStages\n\nLM Pre-Training - Trained on a large corpus (e.g.Â much of the internet) to predict the next word in a sentence or to fill in a word in a sentence\nLM Fine-Tuning - Trained on a specific task (e.g.Â solve problems, answer questions). Instruction Tuning is often used. OpenOrca is an example of a Q&A dataset to train a LM to answer questions. Still predicting the next word, like in Pre-Training, but more target-based on a specific task.\nClassifier Fine-Tuning - Reinforcement Learning from Human Feedback (RLHF)is often used. The LLM being trained gives a few answers to a question and then a human or better LLM will pick which one is best.\n\nPre-Trained LLMs are ones that are typically the open source ones being released and available for download\n\nThey will need to be Fine-Tuned, but not necessarily Classifier Fine-Tuned. Often times, LM Fine-Tuning is enough.\n\n\nWorkflow Example (paper)\n\nRaschka - An introduction to the core ideas and approaches to Finetuning Large Language Models - by Sebastian Raschka\nWith ChatGPT, you can have it answer questions from context that contains thousands of documents.\n\n\nStore all these documents as small chunks of text (allowable size of the context window) in a database.\nCreate embeddings of documents and question\nThe documents of relevance can then be found by computing similarities between the question and the document chunks. This is done typically by converting the chunks and question into word embedding vectors, and computing cosine similarities between chunks and question, and finally choosing only those chunks above a certain cosine similarity as relevant context.\nFinally, the question and context can be combined into a prompt as below, and fed into an LLM API like ChatGPT: prompt=f\"Answer the question. Context: {context}\\n Question: {question}\""
  },
  {
    "objectID": "qmd/nlp-general.html#sec-nlp-gen-misc",
    "href": "qmd/nlp-general.html#sec-nlp-gen-misc",
    "title": "General",
    "section": "Misc",
    "text": "Misc\n\nAlso see\n\nFeature Engineering, Tokenization\nFeature Engineering, Embeddings\nDiagnostics, NLP\nEDA, Text\n\nUse cases for discovering hyper/hyponym relationships\n\nTaxonomy prediction: identifying broader categories for the terms, building taxonomy relations (like WikiData GraphAPI)\nInformation extraction (IE): automated retrieval of the specific information from text is highly reliable on relation to searched entities.\nDataset creation: advanced models need examples to be learned to identify the relationships between entities.\n\nBaseline Models\n\nRegularized Logistic Regression + Bag-of-Words (BoW) (recommended by Raschka)\n\nExample: py, kaggle notebook (sentiment analysis)\n\nUses Compressed Sparse Row (CSR) type of sparse matrix\nUses a time series cv folds and scores by precision\n\nPrecision because â€œAmazon would be more concerned about the products with negative reviews rather than positive reviewsâ€\n\nSays random search &gt; grid search with regularized regression\nAlso fits a model with Tf-idf instead of BoW\n\nPreprocess\n\nTokenize\nRemove stopwords and punctuation\nStem\nunigrams and bigrams\nSparse token count matrix\nNormalize the matrix\n\nModel with regularized logistic regression\n\n\nGPT-4\n\nAccepts prompts of 25,000 words (GPT-3 accepted 1500-2000 words)\nAllegedly around 1T parameters (GPT-3 had 175B parameters)\nSome use cases: translation, q/a, text summaries, writing/getting news, creative writing\nMulti-modal training data (i.e.Â text and audio, pictures, etc.)\nStill hallucinates"
  },
  {
    "objectID": "qmd/nlp-general.html#sec-nlp-gen-terms",
    "href": "qmd/nlp-general.html#sec-nlp-gen-terms",
    "title": "General",
    "section": "Terms",
    "text": "Terms\n\nFlood Words - Words that are too common in the domain (i.e.Â noise)\nHypernym - A word with a broad meaning constituting a category into which words with more specific meanings fall\n\nExample: A device can use multiple storage units such as a hard drive or CD\n\nhyponym of storage units: hard drive, cd\nhypernym of hard drive/cd: storage units\n\n\nHyponym - Opposite of Hypernym; a word of more specific meaning than a general term applicable to it.\nNamed Entity Recognition (NER) - A subtask of information extraction that seeks to locate and classify named entities mentioned in unstructured text into pre-defined categories such as person names, organizations, locations, medical codes, time expressions, quantities, monetary values, percentages, etc.\n\n\nExample: An automated NER system will identify the incoming customer request (e.g.Â installation, maintenance, complaint, and troubleshoot of a particular product) and send it to the respective support desk\naka Named Entity Identification, Entity Chunking, and Entity Extraction\nOther use cases: filtering resumÃ©s, diagnose patients based on symptoms in healthcare data\n\nSequence to Sequence (aka String Transduction) - problems where the input and output is text\n\ne.g.Â Text summarization, Text simplification, Question answering, Chatbots, Machine translation\n\nSpam Words - Words that donâ€™t belong in the domain (i.e.Â noise)"
  },
  {
    "objectID": "qmd/nlp-general.html#sec-nlp-gen-hstpat",
    "href": "qmd/nlp-general.html#sec-nlp-gen-hstpat",
    "title": "General",
    "section": "Hearst Patterns",
    "text": "Hearst Patterns\n\n\nA set of test patterns that can be employed to extract Hypernyms and Hyponyms from text.\nIn the table, X is the hypernym and Y is the hyponym\nâ€œrhyperâ€ stands for reverse-hyper\nUsually, you donâ€™t want to extract all possible hyponyms relations, but only entities in the specific domain\nPackages {{SpaCy}} - example"
  },
  {
    "objectID": "qmd/nlp-general.html#sec-nlp-gen-uscas",
    "href": "qmd/nlp-general.html#sec-nlp-gen-uscas",
    "title": "General",
    "section": "Use Cases",
    "text": "Use Cases\n\nCreating labels for text that can later be used for supervised learning tasks like classification\nCreate metadata for columns in datasets (i.e.Â data dictionary)\n\nPredicting Metadata for Humanitarian Datasets Using GPT-3\n\nPrompt: Column name; sample of data from that column.\nCompletion: metadata which is a tag that includes column attributes.\nPotential Improvements\n\nTrying other models (â€˜adaâ€™ used here) to see if this improves performance (though it will cost more)\nModel hyperparameter tuning. The log probability cutoff will likely be very important\nMore prompt engineering to perhaps include column list on the table might provide better context, we well as overlying columns on two-row header tables.\nMore preprocessing. Not much was done for this article, blindly taking tables extracted from CSV files, so the data is can be a bit messy\n\n\nUsing GPT-3.5-Turbo and GPT-4 for Predicting Humanitarian Data Categories\n\nGPT-4 resulted in 96% accuracy when predicting category and 89% accuracy when predicting both category and sub-category.\n\nGPT-3.5-turbo for the same prompts, with 96% accuracy versus 66% for category.\n\nLimitations exist due to the maximum number of tokens allowed in prompts affecting the amount of data that can be included in data excerpts, as well as performance and cost challenges â€” especially if youâ€™re a small non-profit! â€” at this early stage of commercial generative AI.\nLikely related to being an early preview, GPT-4 model performance was very slow, taking 20 seconds per prompt to complete\n\n\nGeneralizations from OSINT (article)\n\nNote: most of these use a combination of the others to improve their overall performance â€” a document clustering model might use topic extraction and NER to improve the quality of their clusters.\nRecommendation Engines: Bespoke recommendation engines can be fine-tuned to recommend related documentation that may not be within the userâ€™s immediate sphere of interest, but still relevant.\n\nEnables the discovery of â€œunknown unknownsâ€.\n\nTopic extraction and Document clustering: generate topics from multiple texts and detect similarities between documents publishing by dozens, sometimes hundred information feeds.\n\nYou donâ€™t have the time to read every single document to get a higher view of the main problematics evolving within your multiple information feeds\n\nNamed (and unnamed) Entity Extraction and Disambiguation (NER / NED) (see Terms): identifying and categorizing named entities\n\nThe extraction part involves locating and tagging entities, while the disambiguation part involves determining the correct identity or meaning of an entity, especially when it can have multiple interpretations or references in a text.\nAllow you to build entire NLP logics to keep tracks of meaningful facts about this entity, order it by timeliness and relevance. This will allow you to start building bespoke, expert curated profiles.\n\nRelationship Extraction: identify the nature and type of relationships between different entities, such as individuals, organizations, and locations, and to represent them in a structured format that can be easily analyzed and interpreted.\n\nGenerating accurate connections between across thousands of documents will build expert driven, queryable knowledge graphs in a matter of days\n\nMulti-document abstractive summarization: automatically generated a concise and coherent summary of multiple documents on a given topic, by creating new sentences that capture the most important information from the original texts.\n\nEnable users to obtain a concise and coherent summary of the most important information from a large amount of text data.\n\n\nSearch Engine for Docs\n\nHow I Turned My Companyâ€™s Docs into a Searchable Database with OpenAI\nSteps\n\nConverted all of the docs to a unified format\n\nConverts all html files to markdown\n\nSplit docs into blocks and added some automated cleanup\n\nLinks to code for parsing markdown files to get text and code blocks\n\nComputed embeddings for each block\n\nUsed OpenAIâ€™s text-embedding-ada-002 model which is cheap and provides good performance\n\nGenerated a vector index from these embedding\n\nUsed Qdrant for the vector embeddings storage\n\nDefined the index query\n\nAdded ablilities for the user to specify looking only in text or code and included other meta data in the output\n\nWrapped it all in a user-friendly command line interface and Python API\n\nPotential extensions of the project\n\nHybrid search: combine vector search with traditional keyword search\nGo global: Use Qdrant Cloud to store and query the collection in the cloud\nIncorporate web data: use requests to download HTML directly from the web\nAutomate updates: use Github Actions to trigger recomputation of embeddings whenever the underlying docs change\nEmbed: wrap this in a Javascript element and drop it in as a replacement for a traditional search bar"
  },
  {
    "objectID": "qmd/nlp-llms.html#misc",
    "href": "qmd/nlp-llms.html#misc",
    "title": "52Â  NLP, LLMs",
    "section": "52.1 Misc",
    "text": "52.1 Misc\n\nWhat chatGPT is:\n\nâ€œWhat would a response to this question sound likeâ€ machine Researchers build (train) large language models like GPT-3 and GPT-4 by using a process called â€œunsupervised learning,â€ which means the data they use to train the model isnâ€™t specially annotated or labeled. During this process, the model is fed a large body of text (millions of books, websites, articles, poems, transcripts, and other sources) and repeatedly tries to predict the next word in every sequence of words. If the modelâ€™s prediction is close to the actual next word, the neural network updates its parameters to reinforce the patterns that led to that prediction.\nConversely, if the prediction is incorrect, the model adjusts its parameters to improve its performance and tries again. This process of trial and error, though a technique called â€œbackpropagation,â€ allows the model to learn from its mistakes and gradually improve its predictions during the training process. As a result, GPT learns statistical associations between words and related concepts in the data set.\nIn the current wave of GPT models, this core training (now often called â€œpre-trainingâ€) happens only once. After that, people can use the trained neural network in â€œinference mode,â€ which lets users feed an input into the trained network and get a result. During inference, the input sequence for the GPT model is always provided by a human, and itâ€™s called a â€œprompt.â€ The prompt determines the modelâ€™s output, and altering the prompt even slightly can dramatically change what the model produces.Iterative prompting is limited by the size of the modelâ€™s â€œcontext windowâ€ since each prompt is appended onto the previous prompt.  ChatGPT is different from vanilla GPT-3 because it has also been trained on transcripts of conversations written by humans. â€œWe trained an initial model using supervised fine-tuning: human AI trainers provided conversations in which they played both sidesâ€”the user and an AI assistant,â€\nChatGPT has also been tuned more heavily than GPT-3 using a technique called â€œreinforcement learning from human feedback,â€ or RLHF, where human raters ranked ChatGPTâ€™s responses in order of preference, then fed that information back into the model. This has allowed the ChatGPT to produce coherent responses with fewer confabulations than the base model. The prevalence of accurate content in the data set, recognition of factual information in the results by humans, or reinforcement learning guidance from humans that emphasizes certain factual responses.\nTwo major types of falsehoods that LLMs like ChatGPT might produce. The first comes from inaccurate source material in its training data set, such as common misconceptions (e.g., â€œeating turkey makes you drowsyâ€). The second arises from making inferences about specific situations that are absent from its training material (data set); this falls under the aforementioned â€œhallucinationâ€ label.\nWhether the GPT model makes a wild guess or not is based on a property that AI researchers call â€œtemperature,â€ which is often characterized as a â€œcreativityâ€ setting. If the creativity is set high, the model will guess wildly; if itâ€™s set low, it will spit out data deterministically based on its data set. If creativity is set low, â€œ[It] answers â€˜I donâ€™t knowâ€™ all the time or only reads what is there in the Search results (also sometimes incorrect). What is missing is the tone of voice: it shouldnâ€™t sound so confident in those situations.â€\nIn some ways, ChatGPT is a mirror: It gives you back what you feed it. If you feed it falsehoods, it will tend to agree with you and â€œthinkâ€ along those lines. Thatâ€™s why itâ€™s important to start fresh with a new prompt when changing subjects or experiencing unwanted responses.\nâ€œOne of the most actively researched approaches for increasing factuality in LLMs is retrieval augmentationâ€”providing external documents to the model to use as sources and supporting context,â€ said Goodside. With that technique, he explained, researchers hope to teach models to use external search engines like Google, â€œciting reliable sources in their answers as a human researcher might, and rely less on the unreliable factual knowledge learned during model training.â€ Bing Chat and Google Bard do this already by roping in searches from the web, and soon, a browser-enabled version of ChatGPT will as well. Additionally, ChatGPT plugins aim to supplement GPT-4â€™s training data with information it retrieves from external sources, such as the web and purpose-built databases.\nOther things that might help with hallucination include, â€œa more sophisticated data curation and the linking of the training data with â€˜trustâ€™ scores, using a method not unlike PageRankâ€¦ It would also be possible to fine-tune the model to hedge when it is less confident in the response.â€ (arstechnica article)\n\nOpenAI models\n\ndavinci (e.g.Â davinci-003) text-generation models are 10x more expensive than their chat counterparts (e.g.Â gpt-3.5-turbo)\nFor lower usage in the 1000â€™s of requests per day range ChatGPT works out cheaper than using open-sourced LLMs deployed to AWS. For millions of requests per day, open-sourced models deployed in AWS work out cheaper. (As of April 24th, 2023.) (article)\n\nUsed AWS Lambda for deployment\n\ndavinci hasnâ€™t been trained using reinforcement learning from human feedback (RLHF}\nchatgpt 3.5 turbo models\n\nPros\n\nPerforms better on 0 shot classification tasks than Davinci-003\nOutperforms Davinci-003 on sentiment analysis\nSignificantly better than Davinci-003 at math\ncheaper than davinci\n\nCons\n\nTends to produce longer responses than Davinci-003, which may not be ideal for all use cases\nIncluding k-shot examples can lead to inefficient resource usage in multi-turn use cases\n\n\ndavinci-003\n\nPros\n\nPerforms slightly better than GPT-3.5 Turbo with k-shot examples\nProduces more concise responses than GPT-3.5 Turbo, which may be preferable for certain use cases\n\nCons\n\nLess accurate than GPT-3.5 Turbo on 0 shot classification tasks and sentiment analysis\nPerforms significantly worse than GPT-3.5 Turbo on math tasks\n\n\n\nUse Cases\n\nUnderstanding code (Can reduce cognative load)(article)\n\nDuring code reviews or onboarding new programmers\nunder-commented code\n\nGenerating the code scaffold for a problem where you arenâ€™t sure where or how to start solving it.\nLLMs donâ€™t require removing stopwords during preprocessing of documents\n\nCost\n\nFor lower usage in the 1000â€™s of requests per day range ChatGPT works out cheaper than using open-sourced LLMs deployed to AWS. For millions of requests per day, open-sourced models deployed in AWS work out cheaper. (article, April 24th, 2023.)\n\nMethods for giving chatGPT data\n\nThink you can upload a file\nThrough prompt\n\nSee bizsci video\n\npaste actual data\npaste column names and types (glimse() with no values)\n\nGenerate a string for each row of data that contains the column name and value\n\nExample\n\nâ€œThe  is . The  is . â€¦â€\nâ€œThe fico_score is 578.0. The load_amount is 6000.0.Â  The annual income is 57643.54.â€\n\n\n\n\nEvolution of LLMs"
  },
  {
    "objectID": "qmd/nlp-llms.html#langchain",
    "href": "qmd/nlp-llms.html#langchain",
    "title": "52Â  NLP, LLMs",
    "section": "52.2 LangChain",
    "text": "52.2 LangChain\n\nFramework for developing applications powered by language models; connect a language model to other sources of data; allow a language model to interact with its environment\nImplementation of the paperÂ ReAct: Synergizing Reasoning and Acting in Language Models which demonstrates a prompting technique to allow the model to â€œreasonâ€ (with a chain-of-thoughts) and â€œactâ€ (by being able to use a tool from a predefined set of tools, such as being able to search the internet). This combination is shown to drastically improve output text quality and give large language models the ability to correctly solve problems.\nMisc\n\nNotes from:\n\nA Gentle Intro to Chaining LLMs, Agents, and utils via LangChain\n\nSee article for workflow for multi-chains\nAlso shows some diagnostic methods that are included in the library\n\n\nAvailable vector stores for document embeddings\n\nAlso see Databases, Vector Databases\n\nSee article for description and link to code for a manual, more controlled parsing of markdown files to get text and code blocks\n\nIn general, chains are what you get by sequentially connecting one or more large language models (LLMs) in a logical way.\nChains can be built using LLMs or â€œAgentsâ€.\n\nAgents provide the ability to answer questions that require recent or specialty information the LLM hasnâ€™t been trained on.\n\ne.g.Â â€œWhat will the weather be like tomorrow?â€\nAn agent has access to an LLM and a suite of tools for example Google Search, Python REPL, math calculator, weather APIs, etc. (list of supported agents)\nLLMs will use tools to interact with Agents (list of tools)\n\nTool process\n\nUses a LLMChain for building the API URL based on our input instructions and makes the API call.\nUpon receiving the response, it uses another LLMChain that summarizes the response to get the answer to our original question.\n\n\nReAct (Reason + Act) is a popular agent that picks the most usable tool (from a list of tools), based on what the input query is.\n\nOutput Components: observation, a thought, or it takes an action. This is mainly due to the ReAct framework and the associated prompt that the agent is using (See example with ZERO_SHOT_REACT_DESCRIPTION)\n\nserpapi is useful for answering questions about current events.\n\n\nChains can be simple (i.e.Â Generic) or specialized (i.e.Â Utility).\n\nGeneric â€” A single LLM is the simplest chain. It takes an input prompt and the name of the LLM and then uses the LLM for text generation (i.e.Â output for the prompt).\n\nGeneric chains are more often used as building blocks for Utility chains\n\nUtility â€” These are specialized chains, comprised of many LLMs to help solve a specific task. For example,\n\nLangChain supports some end-to-end chains (such as AnalyzeDocumentChain for summarization, QnA, etc) and some specific ones (such as GraphQnAChain for creating, querying, and saving graphs). Programme Aided Language Model reads complex math problems (described in natural language) and generates programs (for solving the math problem) as the intermediate reasoning steps, but offloads the solution step to a runtime such as a Python interpreter.\n2-Chain Examples\n\nChain 1 is used to clean the prompt (remove extra whitespaces, shorten prompt, etc) and chain 2 is used to call an LLM with this clean prompt. (link)\nChain 1 is used to generate a synopsis for a play and chain is used to write a review based on this synopsis. (link)\n\n\n\nDocument Loaders (docs)- various helper functions that take various formats and types of data and produce a document output\n\nFormats like like markdown, word docs, text, PowerPoint, images, HTML, PDF, csvs, AsciiDoc (adoc), etc.\nExamples\n\nGitLoader function clones the repository and load relevant files as documents\nYoutubeLoader - gets subtitles from videos\nDataFrameLoader - converts text columns in panda dfs to documents\n\nAlso, tons of other functions for googledrive or dbs like bigquery, duckdb or cloud storage like s3 or confluence or email or discord, etc.\n\nText Spitters (Docs) - After loading the documents, theyâ€™re usually fed to text splitter to create chunks of text due to LLM context constraints. The chunks of text can then be transformed into embeddings.\n\nFrom â€œSales and Support Chatbotâ€ article in example below\n\n\n# Define text chunk strategy\nsplitter = CharacterTextSplitter(\nÂ  chunk_size=2000,\nÂ  chunk_overlap=50,\nÂ  separator=\" \"\n)\n# GDS guides\ngds_loader = GitLoader(\nÂ  Â  clone_url=\"https://github.com/neo4j/graph-data-science\",\nÂ  Â  repo_path=\"./repos/gds/\",\nÂ  Â  branch=\"master\",\nÂ  Â  file_filter=lambda file_path: file_path.endswith(\".adoc\")\nÂ  Â  and \"pages\" in file_path,\n)\ngds_data = gds_loader.load()\n# Split documents into chunks\ngds_data_split = splitter.split_documents(gds_data)\nprint(len(gds_data_split)) #771\n\nEmbeddings\n\nOpenAIâ€™s text-embedding-ada-002 model is easy to work with, achieves the highest performance out of all of OpenAIâ€™s embedding models (on the BEIR benchmark), and is also the cheapest ($0.0004/1K tokens).\nHuggingFaceâ€™s sentence-transformers , which reportedly has better performance than OpenAIâ€™s embeddings, but that involves downloading the model and running it on your own server.\n\nExample: Generic\n\nBuild Prompt\n\n\nfrom langchain.prompts import PromptTemplate\nprompt = PromptTemplate(\nÂ  Â  input_variables=[\"product\"],\nÂ  Â  template=\"What is a good name for a company that makes [{product}]{style='color: #990000'}?\",\n)\nprint(prompt.format(product=\"podcast player\"))\n# OUTPUT\n# What is a good name for a company that makes podcast player?\n\nIf using multiple variables, then you need, e.g.Â print(prompt.format(product=\"podcast player\", audience=\"childrenâ€), to get the updated prompt.\nCreate LLMChain instance and run\n\nfrom langchain.llms import OpenAI\nfrom langchain.chains import LLMChain\nllm = OpenAI(\nÂ  Â  Â  Â  Â  model_name=\"text-davinci-003\", # default model\nÂ  Â  Â  Â  Â  temperature=0.9) #temperature dictates how whacky the output should be\nllmchain = LLMChain(llm=llm, prompt=prompt)\nllmchain.run(\"podcast player\")\n\nIf you had more than one input_variables, then you wonâ€™t be able to use run. Instead, youâ€™ll have to pass all the variables as a dict.\n\ne.g., LLMchain({â€œproductâ€: â€œpodcast playerâ€, â€œaudienceâ€: â€œchildrenâ€}).\n\nUsing the less expensive chat models: chatopenai = ChatOpenAI(model_name=\"gpt-3.5-turbo\")\nExample: Multiple Chains and Multiple Input Variables\n\nGoal: create an age-appropriate gift generator\nChain 1: Find age\n\n\n# Chain1 - solve math problem, get the age\nfrom langchain.agents import initialize_agent\nfrom langchain.agents import AgentType\nfrom langchain.agents import load_tools\n\nllm = OpenAI(temperature=0)\ntools = load_tools([\"pal-math\"], llm=llm)\nagent = initialize_agent(tools,\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  llm,\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  verbose=True)\n\npal-math is a math-solving tool\nReact agent uses the tool to answer the age problem\nChain 2: Recommend a gift\n\ntemplate = \"\"\"You are a gift recommender. Given a person's age,\\n\nit is your job to suggest an appropriate gift for them. If age is under 10,\\n\nthe gift should cost no more than [{budget}]{style='color: #990000'} otherwise it should cost atleast 10 times [{budget}]{style='color: #990000'}.\nPerson Age:\n[{output}]{style='color: #990000'}\nSuggest gift:\"\"\"\nprompt_template = PromptTemplate(input_variables=[\"output\", \"budget\"], template=template)\nchain_two = LLMChain(llm=llm, prompt=prompt_template)\n\nâ€œ{output}â€ is the name of the output from the 1st chain\n\nFind the name of the output of a chain: print(agent.agent.llm_chain.output_keys)\n\nThe prompt includes a conditional that transforms {budget} (more below)\nLLMchain is used when there are multiple variable in the template\nCombine Chains and Run\n\noverall_chain = SequentialChain(\nÂ  Â  Â  Â  Â  Â  Â  Â  input_variables=[\"input\"],\nÂ  Â  Â  Â  Â  Â  Â  Â  memory=SimpleMemory(memories={\"budget\": \"100 GBP\"}),\nÂ  Â  Â  Â  Â  Â  Â  Â  chains=[agent, chain_two],\nÂ  Â  Â  Â  Â  Â  Â  Â  verbose=True)\noverall_chain.run(\"If my age is half of my dad's age and he is going to be 60 next year, what is my current age?\")\n\nThe prompt is only for the 1st chain, and itâ€™s output, Age, will be input for the second chain.\nSimpleMemory is used pass the variable for the second prompt which adds some additional context to the second chain â€” the {budget} for the gift.\nOutput\n\n#&gt; Entering new SequentialChain chain...\n#&gt; Entering new AgentExecutor chain...\n# I need to figure out my dad's current age and then divide it by two.\n#Action: PAL-MATH\n#Action Input: What is my dad's current age if he is going to be 60 next year?\n#Observation: 59\n#Thought: I now know my dad's current age, so I can divide it by two to get my age.\n#Action: Divide 59 by 2\n#Action Input: 59/2\n#Observation: Divide 59 by 2 is not a valid tool, try another one.\n#Thought: I can use PAL-MATH to divide 59 by 2.\n#Action: PAL-MATH\n#Action Input: Divide 59 by 2\n#Observation: 29.5\n#Thought: I now know the final answer.\n#Final Answer: My current age is 29.5 years old.\n#&gt; Finished chain.\n# For someone of your age, a good gift would be something that is both practical and meaningful. Consider something like a nice watch, a piece of jewelry, a nice leather bag, or a gift card to a favorite store or restaurant.\\nIf you have a larger budget, you could consider something like a weekend getaway, a spa package, or a special experience.'}\n#&gt; Finished chain.\n\nExample: Sales and Support Chatbot (article)\n\nCreate embeddings from text data sources and store in Chroma vector store\n\n\n# Define embedding model\nOPENAI_API_KEY = \"OPENAI_API_KEY\"\nembeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\nsales_data = medium_data_split + yt_data_split\nsales_store = Chroma.from_documents(\nÂ  Â  sales_data, embeddings, collection_name=\"sales\"\n)\nsupport_data = kb_data + gds_data_split + so_data\nsupport_store = Chroma.from_documents(\nÂ  Â  support_data, embeddings, collection_name=\"support\"\n)\n\nSales data is from Medium articles and YouTube subtitles\nSupport data is from docs in a couple github repos and stackoverflow\nInstantiate chatgpt\n\nllm = ChatOpenAI(\nÂ  Â  model_name=\"gpt-3.5-turbo\",\nÂ  Â  temperature=0,\nÂ  Â  openai_api_key=OPENAI_API_KEY,\nÂ  Â  max_tokens=512,\n)\n\nSales prompt template\n\nsales_template = \"\"\"As a Neo4j marketing bot, your goal is to provide accurateÂ \nand helpful information about Neo4j, a powerful graph database used forÂ \nbuilding various applications. You should answer user inquiries based on theÂ \ncontext provided and avoid making up answers. If you don't know the answer,Â \nsimply state that you don't know. Remember to provide relevant informationÂ \nabout Neo4j's features, benefits, and use cases to assist the user inÂ \nunderstanding its value for application development.\n[{context}]{style='color: #990000'}\nQuestion: [{question}]{style='color: #990000'}\"\"\"\nSALES_PROMPT = PromptTemplate(\nÂ  Â  template=sales_template, input_variables=[\"context\", \"question\"]\n)\nsales_qa = RetrievalQA.from_chain_type(\nÂ  Â  llm=llm,\nÂ  Â  chain_type=\"stuff\",\nÂ  Â  retriever=sales_store.as_retriever(),\nÂ  Â  chain_type_kwargs={\"prompt\": SALES_PROMPT},\n)\n\nâ€œ{context}â€ in the template is the data stored in the vector store\nâ€œretrieverâ€ arg points to vector store (e.g.Â sales_store) and uses the as_retriever method to get the embeddings\nSupport prompt template\n\nsupport_template = \"\"\"\nAs a Neo4j Customer Support bot, you are here to assist with any issuesÂ \na user might be facing with their graph database implementation and Cypher statements.\nPlease provide as much detail as possible about the problem, how to solve it, and steps a user should take to fix it.\nIf the provided context doesn't provide enough information, you are allowed to use your knowledge and experience to offer you the best possible assistance.\n[{context}]{style='color: #990000'}\nQuestion: [{question}]{style='color: #990000'}\"\"\"\nSUPPORT_PROMPT = PromptTemplate(\nÂ  Â  template=support_template, input_variables=[\"context\", \"question\"]\n)\nsupport_qa = RetrievalQA.from_chain_type(\nÂ  Â  llm=llm,\nÂ  Â  chain_type=\"stuff\",\nÂ  Â  retriever=support_store.as_retriever(),\nÂ  Â  chain_type_kwargs={\"prompt\": SUPPORT_PROMPT},\n)\n\nSee Sales template\nAdd React agent to determine whether LLM should use Sales or Support templates and contexts\n\ntools = [\nÂ  Â  Tool(\nÂ  Â  Â  Â  name=\"sales\",\nÂ  Â  Â  Â  func=sales_qa.run,\nÂ  Â  Â  Â  description=\"\"\"useful for when a user is interested in various Neo4j information,Â \nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  use-cases, or applications. A user is not asking for any debugging, but is only\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  interested in general advice for integrating and using Neo4j.\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Input should be a fully formed question.\"\"\",\nÂ  Â  ),\nÂ  Â  Tool(\nÂ  Â  Â  Â  name=\"support\",\nÂ  Â  Â  Â  func=support_qa.run,\nÂ  Â  Â  Â  description=\"\"\"useful for when when a user asks to optimize or debug a Cypher statement or needs\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  specific instructions how to accomplish a specified task.Â \nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Input should be a fully formed question.\"\"\",\nÂ  Â  ),\n]\n\nagent = initialize_agent(\nÂ  Â  tools,Â \nÂ  Â  llm,Â \nÂ  Â  agent=\"zero-shot-react-description\",Â \nÂ  Â  verbose=True\n)\nagent.run(\"\"\"What are some GPT-4 applications with Neo4j?\"\"\")\n\nIn this example, the tools used by the Agent are custom data sources and prompt templates"
  },
  {
    "objectID": "qmd/nlp-prompt-engineering.html#sec-nlp-prompt-misc",
    "href": "qmd/nlp-prompt-engineering.html#sec-nlp-prompt-misc",
    "title": "Prompt Engineering",
    "section": "Misc",
    "text": "Misc\n\nDefinition\n\nAsking the right question\nâ€œPrompt engineering is the process of designing and optimizing prompts to LLMs for a wide variety of applications and research topics. Prompts are short pieces of text that are used to guide the LMâ€™s output. They can be used to specify the task that the LM is supposed to accomplish, provide additional information that the LM needs to complete the task, or simply make the task easier for the LM to understand.â€\n\nComponents\n\nAsk the question (e.g.Â â€œWhatâ€™s 1+1?â€)\nSpecify the type of response you want. (e.g.Â Only return the numeric answer.)\n\nPersona\n\nâ€œExplain this to my like Iâ€™m a fifth grader.â€\n\nStyle\n\nâ€œUse a style typical of scientific abstracts to write this.â€\n\nFormat\n\nIf you say â€œFormat the output as a JSON object with the fields: x, y, zâ€ you can get better results and easily do error handling.\n\n\n\nLLMs donâ€™t understand the complexities or nuances of various subjects\n\nIf an industry term is used in multiple ways, the LLM might not understand the meaning just by context alone.\nLLMs can have problems with information in complex formats.\n\nTables sometimes have this same issue, because tables are the mechanism used for layout structure and not a content structure (e.g.Â sentence)\n\nThe models themselves continue to evolve so if it doesnâ€™t understand something today doesnâ€™t mean that it wonâ€™t understand it tomorrow\n\nWhen the output is incomplete, type â€œcontinueâ€ for the next prompt and it will finish the output.\nDonâ€™t give LLMs proprietary data\n\nAlternative: slice(0)\ndat |&gt;\nÂ  slice(0) |&gt;\nÂ  glimpse()\n\nGives the column names and classes\nDepending on the use case, you might want to make the column names unabbreviated and meaningful.\n\n\nSecurity\n\nDonâ€™t let the user have the last word: When taking a userâ€™s prompt, incorporating it with your own prompt, and sending it to ChatGPT or some other similar application, always add a line like â€œIf user input doesnâ€™t make sense for doing xyz, ask them to repeat the requestâ€ after the userâ€™s input. This will stop the majority of prompt injections.\nDonâ€™t just automatically run code or logic that is output from a LLM\n\nTips when used for writing\n\nBe specific on word count and put higher than you need\nDonâ€™t be afraid to ask it to add more information or expand on a particular point\\\nItâ€™s better at creating outlines rather than full pieces of content.\nBe as specific as possible, and use keywords to help ChatGPT understand what you are looking for\nYou can ask it to rephrase its response\nAvoid using complex language or convoluted sentence structures\n**Review the content for accuracy"
  },
  {
    "objectID": "qmd/nlp-prompt-engineering.html#sec-nlp-prompt-dsex",
    "href": "qmd/nlp-prompt-engineering.html#sec-nlp-prompt-dsex",
    "title": "Prompt Engineering",
    "section": "Templates",
    "text": "Templates\n\nMisc\n\nSpecify language, libraries, and functions\n\nExample: BizSci Lab 82\n\n\nShow the prompt he used in a markdown file. He just copied and pasted it into the prompt.\nSpecify libraries to use; models to use; that you want to tune the models in parallel\nThis is not an ideal prompt. You should iterate prompts and guide gpt through complete ds process\n\ni.e.Â prompt for collection then a prompt for cleaning, and so on with eda, preprocessing, modelling, cv, model selection, app\nUse the phrases like:\n\nâ€œPlease update code to include &lt;new feature&gt;â€\nâ€œPlease update feature to be &lt;new thing&gt; instead of &lt;old thing&gt;â€\n\n\n\nExample: Various DS Activities\n\nExample: Student Feedback\n\nFrom Now is the time for grimoires\nComponents\n\nRole: Tell the AI who it is. Context helps the AI produce tailored answers in useful ways, but you donâ€™t need to go overboard.\nGoal: Tell the AI what you want it to do.\nStep-by-Step Instructions: Research has found that it often works best to give the AI explicit instructions that go step-by-step through what you want.\n\nOne approach, called Chain of Thought prompting, gives the AI an example of how you want it to reason before you make your request\nYou can also give it step-by-step directions the way we do in our prompts.\n\nConsider Examples: Few-shot prompting, where you give the AI examples of the kinds of output you want to see, has also proven very effective in research.\nAdd Personalization: Ask the user for information to help tailor the prompt for them.\nAdd Your Own Constraints: The AI often acts in ways that you may not want. Constraints tell it to avoid behaviors that may come up in your testing.\nFinal Step: Check your prompt by trying it out, giving it good, bad, and neutral input. Take the perspective of your usersâ€“ is the AI helpful? Does the process work? How might the AI be more helpful? Does it need more context? Does it need further constraints? You can continue to tweak the prompt until it works for you and until you feel it will work for your audience.\n\nPrompt"
  },
  {
    "objectID": "qmd/nlp-sentiment.html#sec-nlp-sent-misc",
    "href": "qmd/nlp-sentiment.html#sec-nlp-sent-misc",
    "title": "Sentiment",
    "section": "Misc",
    "text": "Misc\n\nLexicons\n\nFinance: Loughran-McDonald\nECB conferences: Picault-Renault"
  },
  {
    "objectID": "qmd/nlp-sentiment.html#sec-nlp-sent-asba",
    "href": "qmd/nlp-sentiment.html#sec-nlp-sent-asba",
    "title": "Sentiment",
    "section": "Aspect-based Sentiment Analysis (ASBA)",
    "text": "Aspect-based Sentiment Analysis (ASBA)\n\nNotes from: NLP Project With Augmentation, Attacks, & Aspect-Based Sentiment Analysis\nModel\n\nIdentify observations from the dataset that are relevant to our aspect.\n\nExample: Analyze consumer sentiment specifically related to the color of dresses\n\nColor is the aspect\n\nOptions\n\nFilter text column to rows that only contain the aspect key word (e.g.Â color, colors)\n\nregex?\n\nSometimes with more abstract aspects, such as Experience, Service, or Location, you may need to leverage topic modeling to predict which aspect is most relevant to a text.\n\n\nTokenize text into smaller pieces\n\nSome simple options would be to split text by punctuation (e.g.Â commas) and/or conjunctions\n\nCalculate the Polarity (aka sentiment) relating to the aspect\n\nApply a pre-trained sentiment classifier model\n\nModels: TextBlob\n\nPolarity range: -1 to 1\n\nExtract the descriptors associated with our aspect\n\nDescriptors help explain the â€œwhyâ€ behind the sentiment.\n\ne.g.Â the customer comment had a positive sentiment about color because they thought it was beautiful. â€œBeautifulâ€ is the descriptor.\n\nOptions\n\nspaCyâ€™s token classification features to automatically analyze the linguistic structure of the sentence and extract what adjectives/adverbs are associated with our noun\n\n\n\nAnalysis\n\nPolarity histogram related to aspect\n\n\nInterpretation\n\nCustomers typically respond positively to dress colors.\nTherefore, color is not a primary contributor to Not Recommended reviews\n\n\nDescriptors\n\n\nInterpretation\n\nCustomers love products with bright, vivid, and vibrant colors.\nCustomers tend to complain about a productâ€™s color when it appears darker in person than on online pictures or is too muted or dull\nCompany should focus on using bright, vibrant colorways and materials"
  },
  {
    "objectID": "qmd/nlp-summarization-extraction.html#sec-nlp-summ-misc",
    "href": "qmd/nlp-summarization-extraction.html#sec-nlp-summ-misc",
    "title": "Summarization and Extraction",
    "section": "Misc",
    "text": "Misc"
  },
  {
    "objectID": "qmd/nlp-summarization-extraction.html#sec-nlp-summ-lngch",
    "href": "qmd/nlp-summarization-extraction.html#sec-nlp-summ-lngch",
    "title": "Summarization and Extraction",
    "section": "LangChain",
    "text": "LangChain\n\nCurrent {{LangChain}} summarization implementations:\n\nRecursive Summarization, in which the long text is split equally into shorter chunks which can fit inside the LLMâ€™s context window. Each chunk is summarized, and the summaries are concatenated together to and then passed through GPT-3 to be further summarized. This process is repeated until one obtains a final summary of desired length.\n\nMajor downside is that existing implementations e.g.Â LangChainâ€™s summarize chain using map_reduce, split the text into chunks with no regard for the logical and structural flow of the text.\nFor example, if the article is 1000 words long, a chunk size of 200 would mean that we would get 5 chunks. What if the author has several main points, the first of which takes up the first 250 words?\n\nRefine method, which passes a chunk of text, along with a summary of the previous chunks, through the LLM, which progressively refines the summary as it sees more of the text. See prompt for details. - Sequential nature of the process means that it cannot be parallelized and takes linear time, far longer than a recursive method which takes logarithmic time - Meaning from the beginning parts of the text could be overrepresented in the final summary. This would be bad if the beginning text has something not germane like advertisements in it.\n\n2-Stage Topic Summarization Method:\n\nDescription:\n\nSplit the summary outputs from one step of the recursive summarization into chunks to be fed into the next step.\nCluster chunks semantically into topics and pass topics into the next iteration of the summarization.\nIt does not drastically increase the LLM costs â€” we are still passing just as much input as the conventional method into the LLM, yet we get a much richer summarization.\n\nNotes from Summarize Podcast Transcripts and Long Texts Better with NLP and AI\nExample: Summarize Biden state of the union speech\n\nPreprocessing:\n\nSplit the raw text it into sentences, restricting sentences to have a minimum length of 20 words and maximum length of 80.\nCreate chunks of sentences. Chunk size should be the number of sentences it generally takes to express a discrete idea. Example uses 5 sentences (but you can experiment with other numbers) with 1-sentence overlap between chunks, just to ensure continuity so that each chunk has some contextual information about the previous chunk.\n\nMaybe figure out the average count of sentences per paragraph.\nThis article tried something a little more extreme by chunking each paragraph (At least I think thatâ€™s what this means. Itâ€™s a fucking run-on sentence.) Might not of worked because maybe he had a lot of 1 sentence paragraphs?\n\nâ€œInitially, I also tried splitting the text blocks into paragraphs, hypothesizing that because a section may contain information about many different topics, the embedding for that entire section may not be similar to an embedding for a text prompt concerned with only one of those topics. This approach, however, resulted in top matches for most search queries disproportionately being single line paragraphs, which turned out to not be terribly informative as search results.â€\nResulted in 65 chunks, with an average chunk length is 148 words, and a range from 46â€“197 words\n\n\n\nProduce a Title and Summary for each chunk.\n# Prompt to get title and summary for each chunk\nmap_prompt_template = \"\"\"\nÂ  Â  Firstly, give the following text an informative title. Then, on a new line, write a 75-100 word summary of the following text:\nÂ  Â  [{text}]{style='color: #990000'}\nÂ  Â  Return your answer in the following format:\nÂ  Â  Title | Summary...\nÂ  Â  e.g.\nÂ  Â  Why Artificial Intelligence is Good | AI can make humans more productive by automating many repetitive processes.\nÂ  Â  TITLE AND CONCISE SUMMARY:\n\"\"\"\n\nPrompt for gpt3 model with title and summary output separated by a bar, |.\n\nTransform summaries of chunks into embeddings\n\nPass summaries to an openai or huggingface model to get embeddings\n\nGroup semantically-similar chunks together into topics\n\nCreate a chunk similarity matrix, where the (i,j)th entry denotes the cosine similarity between the embedding vectors of the ith and jth chunk ({{scipy}} for cosine similarity function)\nUse the Louvain community detection algorithm to detect topics from the chunks ({{networkx}} for Louvain algorithm)\n\nHas a hyperparameter called resolution â€” small resolutions lead to smaller clusters.\nAdditionally, a hyperparameter proximity_bonusÂ  is created, which bumps up the similarity score of chunks if their position in the original text is closer to each other. You can interpret this as treating the temporal structure of the text as a prior (i.e.Â chunks closer to each other are more likely to be semantically similar).\nAnother example, using a Bloomberg podcast transcript, shows how this method detected blocks of advertisements.\n\n\nProduce a title for each topic\n\nFor much longer texts like books, repeat this process several times until there are ~10 topics left whose topic summaries can fit into the context window.\nEach topic has a number of chunks associated with it. Titles were produced for each chunk earlier in the process. So, loop each topicâ€™s chunksâ€™ list of titles through an LLM to get a refined title for that topic.\nDo this concurrently (i.e.Â simultaneously) for all topics to prevent the topicsâ€™ titles from being too similar with one another.\nThis output will be the label of each topic (see Visualization section)\n\nProduce a summary for each topic\n\nSimilar to the previous section.\nFeed chunk summaries of each topic to GPT-3, and get a refined summary\nSame as befeore, this needs to be done concurrently\n\nFinal Summary: To arrive at the overall summary of the text, concatenate the topic summaries together and prompt GPT-3 to summarize them.\n\nVisualization"
  },
  {
    "objectID": "qmd/nlp-summarization-extraction.html#sec-nlp-summ-openai",
    "href": "qmd/nlp-summarization-extraction.html#sec-nlp-summ-openai",
    "title": "Summarization and Extraction",
    "section": "OpenAI",
    "text": "OpenAI\n\nMisc\n\nNotes from:\n\nMastering ChatGPT: Effective Summarization with LLMs\n\n\nPrompt Format\nimport openai\nimport os\n\nopenai.api_key_path = \"/path/to/key\"\n\ndef get_completion(prompt, model=\"gpt-3.5-turbo\"):\n    \"\"\"\n    This function calls ChatGPT API with a given prompt\n    and returns the response back.\n    \"\"\"\n    messages = [{\"role\": \"user\", \"content\": prompt}]\n    response = openai.ChatCompletion.create(\n        model=model,\n        messages=messages,\n        temperature=0\n    )\n    return response.choices[0].message[\"content\"]\n\nuser_text = f\"\"\"\n    &lt;Any given text&gt;\n\"\"\"\n\nprompt = f\"\"\"\n  &lt;Any prompt with additional text&gt;\n  \\\"\\\"\\\"{user_text}\\\"\\\"\\\"\n\"\"\"\n\n# A simple call to ChatGPT\nresponse = get_completion(prompt)\n\nThe function â€œget_completionâ€ calls the ChatGPT API with a given prompt. If the prompt contains additional user text, it is separated from the rest of the code by triple quotes.\n\nExample: Customer Reviews\n\nSummarization Prompt\nprompt = f\"\"\"\n  Your task is to generate a short summary of a product \\\n  review from an e-commerce site. \\\n  Summarize the review below, delimited by triple \\\n  backticks, in exactly 20 words. Output a json with the \\\n  sentiment of the review, the summary and original review as keys. \\\n  Review: ```{prod_review}```\n\"\"\"\nresponse = get_completion(prompt)\nprint(response)\nResponse from ChatGPT\n{\n   \"sentiment\": \"positive\",\n    \"summary\": \"Durable and engaging children's computer with intuitive interface and educational games. Volume could be louder.\",\n    \"review\": \"I purchased this children's computer for my son, and he absolutely adores it. He spends hours exploring its various features and engaging with the educational games. The colorful design and intuitive interface make it easy for him to navigate. The computer is durable and built to withstand rough handling, which is perfect for active kids. My only minor gripe is that the volume could be a bit louder. Overall, it's an excellent educational toy that provides hours of fun and learning for my son. It arrived a day earlierthan expected, so I got to play with it myself before I gave it to him.\"\n}\n\nâ€œsummaryâ€ is accurate and well structured, although it misses some information we could be interested on as the owners of the e-commerce, such as information about the delivery of the product.\n\nTherefore, you should iterate and try to improve the response\n\n\nAdd focus to prompt to improve summary\nprompt = f\"\"\"\n  Your task is to generate a short summary of a product \\\n  review from an ecommerce site. \\\n  Summarize the review below, delimited by triple \\\n  backticks, in exactly 20 words and focusing on any aspects \\\n  that mention shipping and delivery of the product. \\\n  Output a json with the sentiment of the review, \\\n  the summary and original review as keys. \\\n  Review: ```{prod_review}```\n\"\"\"\n\nresponse = get_completion(prompt)\nprint(response)\n\nâ€œfocusing on any aspects that mention shipping and delivery of the productâ€\n\nImproved response\n{\n   \"sentiment\": \"positive\",\n    \"summary\": \"Durable and engaging children's computer with intuitive interface. Arrived a day earlier than expected.\",\n    \"review\": \"I purchased this children's computer for my son, and he absolutely adores it. He spends hours exploring its various features and engaging with the educational games. The colorful design and intuitive interface make it easy for him to navigate. The computer is durable and built to withstand rough handling, which is perfect for active kids. My only minor gripe is that the volume could be a bit louder. Overall, it's an excellent educational toy that provides hours of fun and learning for my son. It arrived a day earlier than expected, so I got to play with it myself before I gave it to him.\"\n}\n\nâ€œsummaryâ€ includes the focus but lost the negative input, so you can either increase the word limit or extract the additional information separately.\n\nExtract Specific Information\nprompt = f\"\"\"\n  Your task is to extract relevant information from \\ \n  a product review from an ecommerce site to give \\\n  feedback to the Shipping department. \\\n  From the review below, delimited by triple quotes \\\n  extract the information relevant to shipping and \\ \n  delivery. Use 100 characters. \\\n  Review: ```{prod_review}```\n\"\"\"\n\nresponse = get_completion(prompt)\nprint(response)\n&gt;&gt; Shipping: Arrived a day earlier than expected.\n\nâ€œextract the information relevant to shipping and deliveryâ€\nHave also seen the phrase, â€œinclude only,â€ used instead of â€œextractâ€ in the prompt\n\nBatch Extraction\nreviews = [\n    \"The children's computer I bought for my daughter is absolutely fantastic! She loves it and can't get enough of the educational games. The delivery was fast and arrived right on time. Highly recommend!\",\n    \"I was really disappointed with the children's computer I received. It didn't live up to my expectations, and the educational games were not engaging at all. The delivery was delayed, which added to my frustration.\",\n    \"The children's computer is a great educational toy. My son enjoys playing with it and learning new things. However, the delivery took longer than expected, which was a bit disappointing.\",\n    \"I am extremely happy with the children's computer I purchased. It's highly interactive and keeps my kids entertained for hours. The delivery was swift and hassle-free.\",\n    \"The children's computer I ordered arrived damaged, and some of the features didn't work properly. It was a huge letdown, and the delivery was also delayed. Not a good experience overall.\"\n]\n\nprompt = f\"\"\"\n    Your task is to generate a short summary of each product \\ \n    review from an e-commerce site. \n    Extract positive and negative information from each of the \\\n    given reviews below, delimited by triple \\\n    backticks in at most 20 words each. Extract information about \\\n    the delivery, if included. \\\n    Review: ```{reviews}```\n\"\"\"\nResponse\n1. Positive: Fantastic children's computer, fast delivery. Highly recommend.\n2. Negative: Disappointing children's computer, unengaging games, delayed delivery.\n3. Positive: Great educational toy, son enjoys it. Delivery took longer than expected.\n4. Positive: Highly interactive children's computer, swift and hassle-free delivery.\n5. Negative: Damaged children's computer, some features didn't work, delayed delivery."
  },
  {
    "objectID": "qmd/nlp-topic.html#sec-nlp-top-misc",
    "href": "qmd/nlp-topic.html#sec-nlp-top-misc",
    "title": "Topic",
    "section": "Misc",
    "text": "Misc\n\nUnsupervised Machine Learning problem\n\nTakes a set of documents (far left), and returns a set of k topics (middle, e.g.Â Magic, Evil, Travel, Creatures) that summarize the documents (far right)\n\nâ€œWandâ€ has the highest probability in the Magic topic\nTopic probablilities can be used to classify documents\n\nTopics donâ€™t come with titles like are shown in figure. Researchers need to determine these topic categories on their own\n\nExample: 2020 Election Tweets\n\nEach row is a topic\nTopics could be: Gun Violence (row 1), Political Parties (row 2), a general Twitter topic (row 3), Covid-19 (row 4), Pro-Biden Phrases and Hashtags (row 5), Democratic Voters (row 6), and Pro-Trump Phrases and Hashtags (row 7)\n\n\n\nUse Cases\n\nAnnotation - â€˜automaticallyâ€™ label, or annotate, unstructured text documents based on the major themes that run through them\n\nWith labels, supervised ML models can now be used\n\neDiscovery - Legal discovery involves searching through all the documents relevant for a legal matter, and in some cases the volume of documents to be searched is very large. A 100% search of the documents isnâ€™t always viable, so itâ€™s easy to miss out on relevant facts. (article)\nContent recommendation - The NYT uses topic modeling in two waysâ€”firstly to identify topics in articles and secondly to identify topic preferences amongst readers. The two are then compared to find the best match for a reader.\n\nIncoherent topics in this context means difficult for humans to interpret"
  },
  {
    "objectID": "qmd/nlp-topic.html#sec-nlp-top-conc",
    "href": "qmd/nlp-topic.html#sec-nlp-top-conc",
    "title": "Topic",
    "section": "Concepts",
    "text": "Concepts\n\nMisc\n\nNotes from\n\nTopic Modeling with LSA, pLSA, LDA, NMF, BERTopic, Top2Vec: a Comparison\n\nAt the end of the article\n\nTable that compares algorithms on a number of categories (too large to put in this note)\nGreat discussion on topic representation: human vs model\n\nUse Cases\n\nFind trending topics in Tweets with little pre-processing effort â€“&gt; BERTopic/Top2Vec better\nFinding how a given document may contain a mixture of multiple topics â€“&gt; LDA/NMF better\n\nSee BERTopic/Top2Vec &gt;&gt; Cons &gt;&gt; Single Topic for potential hack though\n\n\n\n\n\nLatent Semantic Analysis (LSA) (DeerwesterÂ¹ et al.Â 1990), probabilistic Latent Semantic Analysis (pLSA) (HofmannÂ², 1999), Latent Dirichlet Allocation (LDA) (BleiÂ³ et al., 2003) and Non-Negative Matrix Factorization (LeeÂ³ et al., 1999)\n\nRepresent a document as a bag-of-words and assume that each document is a mixture of latent topics\nDocument-Term Matrix (DTM) aka Term-Document Matrix (TDM) created\n\nEach cell (i, j) contains a count, i.e.Â how many times the word j appears in document i"
  },
  {
    "objectID": "qmd/nlp-topic.html#sec-nlp-top-lsa",
    "href": "qmd/nlp-topic.html#sec-nlp-top-lsa",
    "title": "Topic",
    "section": "Latent Semantic Analysis (LSA)",
    "text": "Latent Semantic Analysis (LSA)\n\nDescription\n\nTopics corresponding to largest singular values from SVD are used to summarize the corpus\n\nProcess\n\nTruncated SVD applied to the Document-Term Matrix (DTM) (See above) \n\nDecomposed into the product of three distinct matrices: DTM = U âˆ™ Î£ âˆ™ Váµ—\n\nU and V are of size m x m and n x n respectively\n\nm the number of documents in the corpus\nn the number of words in the corpus.\n\nÎ£ (Topic Importance) is m x n and only its main diagonal is populated: it contains the singular values of the DTM.\n\n\n\nResults\n\nFirst tÂ  largest singular values selected from Î£\n\nt â‰¤ min(m, n)\n\nThe topics that correspond to the selected singular values are considered representative of the corpus\nTopics are open to human interpretation through the V matrix\n\nIssues\n\nThe DTM disregards the semantic representation of words in a corpus. Similar concepts are treated as different matrix elements. Pre-processing techniques may help, but only to some extent. For example, stemming may help in treating â€œItalyâ€ and â€œItalianâ€ as similar terms (as they should be), but close words with a different stem like â€œmoneyâ€ and â€œcashâ€ would still be considered as different. Moreover, stemming may also lead to less interpretable topics.\nLSA requires an extensive pre-processing phase to obtain a significant representation from the textual input data.\nThe number of singular values t (topics) to maintain in the truncated SVD must be known a priori.\nA property of SVD is that the basis vectors are orthogonal to each other, forcing some elements in the bases to be negative. Therefore, U and V may contain negative values. This poses a problem for interpretability. We donâ€™t know how a component contributes to the whole."
  },
  {
    "objectID": "qmd/nlp-topic.html#sec-nlp-top-plsa",
    "href": "qmd/nlp-topic.html#sec-nlp-top-plsa",
    "title": "Topic",
    "section": "Probabilistic Latent Semantic Analysis (pLSA)",
    "text": "Probabilistic Latent Semantic Analysis (pLSA)\n\nModels the joint probability \\(P(d, w)\\) of seeing a word, \\(w\\), and a document, \\(d\\), together as a mixture of conditionally independent multinomial distributions\n\\[\n\\begin{align}\nP(d,w) &= P(d)P(w|d) \\\\\nP(w|d) &= \\sum_{z \\in Z} P(w|Z)P(z|d)\n\\end{align}\n\\]\n\n\\(w\\) indicates a word.\n\\(d\\) indicates a document.\n\\(z\\) indicates a topic.\n\\(P(z|d)\\) is the probability of topic z being present in a document d.\n\\(P(w|z)\\) is the probability of word w being present in a topic z.\nWe assume \\(P(w|z, d) = P(w|z)\\).\n\nModel equations (above) equivalent to (See LSA algorithm above)\n\\[\nP(d,w) = \\sum_{z\\in Z} P(z)P(d|z)P(w|z)\n\\]\n\n\\(P(d, w)\\) corresponds to the DTM.\n\\(P(z)\\) is analogous for the main diagonal of \\(\\Sigma\\).\n\\(P(d|z)\\) and \\(P(w|z)\\) correspond to \\(U\\) and \\(V\\), respectively.\n\nModel can be fit using the expectation-maximization algorithm (EM)\n\nEM performs maximum likelihood estimation in the presence of latent variables (in this case, the topics)\n\nIssues\n\nThe number of parameters grows linearly with the number of documents, leading to problems with scalability and overfitting.\nIt cannot assign probabilities to new documents."
  },
  {
    "objectID": "qmd/nlp-topic.html#sec-nlp-top-lda",
    "href": "qmd/nlp-topic.html#sec-nlp-top-lda",
    "title": "Topic",
    "section": "Latent Dirichlet Allocation (LDA)",
    "text": "Latent Dirichlet Allocation (LDA)\n\nUses Dirichlet priors to estimate the document-topic and term-topic distributions in a Bayesian approach\nModel (Plate Notation)\n\n\n\\(\\alpha\\) - The parameter of the Dirichlet prior on the per-document topic distributions\n\\(\\theta\\) - Topic distribution for a document\n\\(z\\) - Topic\n\\(w\\) - Word\nSubscripts\n\n\\(M\\) - Indicates the number of documents\n\\(N\\) - The number of words in a document\n\\(d\\) - Cocument\n\nTop to Bottom\n\nFrom the Dirichlet distribution with parameter \\(\\alpha\\), we draw a random sample representing the topic distribution \\(\\theta\\) for a document/article/corpus.\n\nExample: Topic distribution for an article the politics section of a newspaper\n\nA mixture (0.99 politics, 0.05 sports, 0.05 arts) describing the distribution of topics for an article\n\n\nFrom the selected mixture \\(\\theta\\), we draw a topic \\(z_i\\) based on the distribution (in our example, politics).\n\nBottom to Top\n\nFrom the Dirichlet distribution with parameter \\(\\beta\\), we draw a random sample representing the word distribution \\(\\phi\\) for the per-topic word distribution given the topic \\(z_i\\).\nFrom selected mixture Ï†, we draw a word \\(w_i\\) based on the distribution.\n\n\nProcess\n\nEstimate the probability of a topic \\(z\\) given a document \\(d\\) and the parameters \\(alpha\\) and \\(\\beta\\), i.e.Â \\(P(z|d, \\alpha, \\beta)\\)\n\nFind parameters by minimizing the Kullback-Leibler divergence between the approximate distribution and the true posterior \\(P(\\theta, z|d, \\alpha, \\beta)\\)\n\\[\np(\\theta,z|d,\\alpha,\\beta) = \\frac{p(\\theta,z, d|\\alpha,\\beta)}{p(d|\\alpha,\\beta)}\n\\]\nCompute \\(P(z|d, \\alpha, \\beta)\\), which, in a sense, corresponds to the document-topic matrix U. Each entry of \\(\\beta_1, \\beta_2, \\ldots, \\beta_t\\) is \\(p(w|z)\\), which corresponds to the term-topic matrix \\(V\\)\n\n\\(U\\) and \\(V\\) are matrices from SVD in the LSA section. See above for details.\n\n\n\nPros:\n\nIt provides better performances than LSA and pLSA.\nUnlike pLSA, LDA can assign a probability to a new document thanks to the document-topic Dirichlet distribution.\nIt can be applied to both short and long documents.\nTopics are open to human interpretation.\nAs a probabilistic module, LDA can be embedded in more complex models or extended. Cons:\nThe number of topics must be known beforehand.\nThe bag-of-words approach disregards the semantic representation of words in a corpus, similarly to LSA and pLSA.\nThe estimation of Bayes parameters Î± and ğ›½ lies under the assumption of exchangeability for the documents.\nIt requires an extensive pre-processing phase to obtain a significant representation from the textual input data.\nStudies report LDA may yield too general (Rizvi et al., 2019) or irrelevant (Alnusyan et al., 2020) topics. Results may also be inconsistent across different executions (Egger et al., 2021)."
  },
  {
    "objectID": "qmd/nlp-topic.html#sec-nlp-top-nmf",
    "href": "qmd/nlp-topic.html#sec-nlp-top-nmf",
    "title": "Topic",
    "section": "Non-Negative Matrix Factorization (NMF)",
    "text": "Non-Negative Matrix Factorization (NMF)\n\nNMF is the same as SVD used in LSA except that it applies the additional constraints that U and Váµ— can only contain non-negative elements.\n\nDecomposition has no sigma matrix, just U âˆ™ Váµ—\n\nMisc\n\nAlso see\n\nAlgorithms, Recommendation &gt;&gt; Collaboritive Filtering &gt;&gt; Non-Negative Matrix Factorization (NMF)\n\nDeep dive into the algorithm, packages, example, etc.\n\n\n\nOptimize problem by minimizing the difference between the DTM and its approximation.\n\nFrequently adopted distance measures are the Frobenius norm and the Kullback-Leibler divergence\n\nPros:\n\nLiterature argues the superiority of NMF compared to SVD (hence LSA) in producing more interpretable and coherent topics (Lee et al.Â 1999, Xu et al.Â 2003; Casalino et al.Â 2016).\n\nCons:\n\nThe non-negativity constraints make the decomposition more difficult and may lead to inaccurate topics.\nNMF is a non-convex problem. Different U and Váµ— may approximate the DTM, leading to potentially inconsistent outcomes for different runs."
  },
  {
    "objectID": "qmd/nlp-topic.html#sec-nlp-top-bertttv",
    "href": "qmd/nlp-topic.html#sec-nlp-top-bertttv",
    "title": "Topic",
    "section": "BERTopic and Top2Vec",
    "text": "BERTopic and Top2Vec\n\nCreates semantic embeddings from input documents\nBERTopic\n\nUsed BERT Sentence Transformers (SBERT) to manufacture high-quality, contextual word and sentence vector representations.\nCurrently has a broader coverage of embedding models (Oct 2022)\n\n\nTop2Vec\n\nUsed Doc2Vec to create jointly embedded word, document, and topic vectors.\n\nClustering the Embeddings\n\nBoth original papers used HDBSCAN\nBERTopic supports K-Means and agglomerative clustering algorithms\nK-Means allows to select the desired number of clusters and forces every document into a cluster. This avoids the generation of outliers, but may also result in poorer topic representation and coherence.\n\nTopic Representation\n\nBERTopic - concatenates all documents within the same cluster (topic) and applies a modified TF-IDF (class-based TF-IDF or cTF-IDF)\n\nThe â€œmodificationâ€ is that documents are replaced with clusters in the original TF-IDF formula\ncTF-IDF estimates the importance of words in clusters instead of documents.\n\nTop2Vec - manufactures a representation with the words closest to the clusterâ€™s centroid\n\nFor each dense area obtained through HDBSCAN, it calculates the centroid of document vectors in original dimension, then selects the most proximal word vectors\n\n\nPros\n\nThe number of topics is not necessarily given beforehand.\n\nBoth BERTopic and Top2Vec support for hierarchical topic reduction to optimize the number of topics.\n\nHigh-quality embeddings take into account the semantic relationship between words in a corpus, unlike the bag-of-words approach. This leads to better and more informative topics.\nDue to the semantic nature of embeddings, textual pre-processing (stemming, lemmization, stopwords removal, â€¦) is not needed in most cases.\nBERTopic supports dynamic topic modeling.\nModularity. Each step (document embedding, dimensionality reduction, clustering) is virtually self-consistent and can change or evolve depending on the advancements in the field, the peculiarities of a specific project or technical constraints.\n\nexample: use BERTopic with Doc2Vec embeddings instead of SBERT, or apply K-Means clustering instead of HDBSCAN.\n\nThey scale better with larger corpora compared with conventional approaches.\nBoth BERTopic and Top2Vec provide advanced built-in search and visualization capabilities. They make simpler to investigate the quality of the topics and drive further optimization, as well as producing high-quality charts for presentations.\n\nCons\n\nThey work better on shorter text, such as social media posts or news headlines.\n\nMost transformers-based embeddings have a limit on the number of tokens they can consider when they build a semantic representation. It is possible to use these algorithms with longer documents. One may, for example, split the documents in sentences or paragraphs before the embedding step. Nevertheless, this may not necessarily benefit the generation of meaningful and representative topics for longer documents.\n\nEach document is assigned to one topic only.\n\nTraditional approaches like LDA, instead, were built on the assumption that each document contains a mixture of topics.\nThe probability distribution of the HDBSCAN may be used as proxy of the topics distribution\n\nThey are slower compared to conventional models (Grootendorst, 2022).\n\nFaster training and inference may require more expensive hardware accelerators (GPU).\n\nAlthough BERTopic leverages transformers-based large language models to manufacture document embeddings, the topic representation still uses a bag-of-word approach (c TF-IDF).\nThey may be less effective for small datasets (&lt;1000 docs) (Egger et al., 2022)."
  },
  {
    "objectID": "qmd/nlp-topic.html#sec-nlp-top-tnm",
    "href": "qmd/nlp-topic.html#sec-nlp-top-tnm",
    "title": "Topic",
    "section": "Topic-Noise Models",
    "text": "Topic-Noise Models\n\nNotes from\n\nPaper\nAn Introduction to Topic-Noise Models\n\nGenerate more coherent, interpretable topics than traditional topic models when using text from noisy domains like social media\n\nTopic models are pretty good at identifying topics in traditional documents like books, newspaper articles, and research papers, because they have less noise\nExample: Domain = Covid-19 pandemic\n\n\nUses a fake dataset with Harry Potter characters tweeting about the pandemic\nRed words are noise.\n\nVariations of â€œCovidâ€ are repeated in each topic\nPotter words not relevant to the domain\n\n\n\n*Recommended to ensemble these models with traditional topic models to produce the best results*\n\nTND topics are not always as intuitive as those generated using other topics models\n\nMisc\n\n{{gdtm}}\nIt is not unusual with noisy data that the model focuses on unigrams due to the issue of sparsity in short documents. Two word phrases tend to appear less frequently in a document collection than single words. That means that when you have a topic model with unigrams and ngrams, the ngrams will naturally not rise to the top of the topic word list\nBecause social media documents are much shorter than traditional texts, they rarely contain all of the topics in the topic set\nBest results on data sets of tens or hundreds of thousands of tweets or other social media posts\n\nThe training of the noise distribution is accomplished using a randomized algorithm. With smaller data sets, TND is not always able to get an accurate noise distribution\n\n\nTopic-Noise Discriminator (TND)\n\nJointly approximates the topic and noise distributions on a data set to allow for more accurate noise removal and more coherent topics\nOriginal topic-noise model (Churchill and Singh, 2021)\nHas been empirically shown that TND works best when combined with more traditional topic models\nAssigns each word in each document to either a topic or noise distribution, based on the wordâ€™s prior probability of being in each distribution\nProcess:\n\nFor each document \\(d\\) in \\(D\\):\n\nProbabilistically choose a topic \\(z\\)áµ¢ from the topic distribution of \\(d\\)\nFor each word \\(w\\) in \\(d\\), assign \\(w\\) to \\(z\\)áµ¢ or the noise distribution \\(H\\), based on the probabilities of \\(w\\) in \\(z\\)áµ¢ and \\(H\\).\nRe-approximate the topic distribution of \\(d\\) given the new topic-word assignments\n\nRepeat the above for X iterations (usually 500 or 1,000 in practice)\n\nOver a large number of iterations, each word will have a probability of being a topic word and a probability of being a noise word.\n\n\nHyperparameters:\n\n\\(k\\) - the number of topics in the topic set\n\\(\\alpha\\) - how many topics there are per document\n\\(\\beta_0\\) - how many topics a word can be in\n\\(\\beta_1\\) - the skew of a word towards a topic (and away from the noise distribution)\n\nHigh \\(\\beta_1\\) \\(\\rightarrow\\) the more likely any given word will be a topic word instead of a noise word\nLike placing an extra weight on the Topic Frequency side of the scales\n\nRecommendations for domain-specific social media data:\n\n\\(k â‰¥ 20\\)\n\nTwitter: \\(k = 30\\)\n\n\\(9 \\geq \\beta â‰¤ 25\\).\n\nTwitter: \\(\\beta_1 = 25\\)\nReddit: \\(\\beta_1 = 9\\) or \\(16\\) or \\(16\\) for Reddit comments.\nIncrease when data sets are noisier to try to keep more words in topics that truly belong there.\n\n\n\n\nNoiseless Latent Dirichlet Allocation (NLDA)\n\nTND provides accurate noise removal while maintaining the same topic quality and performance that people expect from state-of-the-art topic models used on more traditional document collections.\nEnsemble of TND model and a LDA model\n\nThe noise distribution of TND and the topic distribution of LDA are combined to create more coherent, interpretable topics than we would get with either TND or LDA alone\n\nA scaling parameter \\(\\phi\\) allows the noise and topic distributions to be compared even if they are not generated using the same number of topics, \\(k\\), by scaling the underlying frequencies to relatable values."
  },
  {
    "objectID": "qmd/nlp-transformers.html",
    "href": "qmd/nlp-transformers.html",
    "title": "53Â  Transformers",
    "section": "",
    "text": "TOC\n\nMisc\nTerms\nLoss Functions\nMetrics\nAttention\nTraining\nBERT\n\nMisc\n\nDescription\n\nDeep feed-forward artificial neural network architectures that utilizes self-attention mechanisms and can handle long-range correlations between the input-sequence items.\nAdopts the mechanism of self-attention which differentially weights the significance of each part of the input data.\nLike RNNs, theyâ€™re designed to process sequential input data\nUnlike RNNs, transformers process the entire input all at once. The attention mechanism provides context for any position in the input sequence.\n\nif the input data is a natural language sentence, the transformer does not have to process one word at a time. This allows for more parallelization than RNNs and therefore reduces training times\n\n\nNotes\n\nFucked up notation\n\nannotation: backward hidden state: does hâ†’T = h â† ?\nallignment model:\n\nis st the same as hâ†’ since you calling them both hidden states.\nwtf? how can ci be in this model when c is what weâ€™re calculating\n\nvisual: you canâ€™t have yt = p(yt). Is y(t) the probability of itself which is a probability, so it would be the probability of a probability.\n\nlanguage translation model (Bahdanau et al [5])\n\noverview (soft-)searches for the most relevant information located in different positions in the source sentence\nIt then generates translations for the source word jointly using the context vector of these relevant positions and previously generated (translated?) words.\n\nbi-directional encoder\n\nAn annotation is for X_i by concatenating the forward and backward hidden states\n\nhiâ†’ is a forward hidden state which is just the input at the ith position of the source sentence\nhiâ†’Tis the input at the ith position of a backwards source sentence\n\nattention block (?)\n\nÎ±tj is the attention weight for each hj annotation\n\n\n\na is an alignment model (typically a feedforward network) which represents how well the annotation hj is well-suited for the next (or is it current?) decoder hidden state st considering previous decoder hidden state st-1\n\n\n\n\ncontext vector: computed by attention block\ndecoder\n\ndecoder: RNN + attention block\n\n\n\nOutput of the RNN using this context vector is the probability of the translated word given the previous translated words that are before it in the sentence and the source sentence\nModern attention mechanisms (Vaswani et al.Â [7])\n\nThe first one is the computation of attention distribution on the inputs, and the next one is the calculation of the context vector based on this distribution\n\ninfer the keys (K) from the source data (method depends on task)\n\nK can be a text or document embeddings, a portion of an image features (an area from the image), or hidden states of sequential models (RNN, etc.).\n\nquery, Q, similar to s_t-1 (previous decoder hidden state)\nkey-value attention mechanisms, K and values (V) are different representations of the same input data and in the case of self-attention, all K, Q, and V are segregated embeddings of the same data i.e.Â inputs.\n\nfigure out the relationship (weights) between the Q and all Ks through a scoring function f (also called energy function and compatibility function)\n\ncalculate the energy score that shows the importance of Q with respect to all Ks before generating the next output\n\n\n\nLearnable parameters are\n\nk, v, b, W, W2, W3;\ndk is the dimension of the input vector (keys);\nthe act is a nonlinear activation function (ie. tanh and ReLU).\n\n\n\n\nenergy scores are fed into an attention distribution function named g like the softmax layer to compute the attention weights Î± by normalizing all energy scores to a probability distribution\n\nsparsemax assigns zero value to the â€œirrelevantâ€ elements (zero values?) in the output\n\nsoftmax computes a value for every input value which produces computational overhead when dealing sparse matrices.\n\nlogistic sigmoid scales energy scores to the [0â€“1] range\n\ncontext\n\n\n\n\n\n\n\nStrong (first real) Model: RoBERTa\n\n{{DistilRoBERTa}}\nA combination of speed and accuracy\nAfter squeezing the juice out of this model, you can scale up to the bigger versions and get better accuracy\nRaschka: Bert &gt;&gt; LSTMs (Thread)\n\nLSTMs outdated method for modeling text; out-of-the-box BERT model wins every time no matter the sample size\nCode included\n\n\nReasons that embeddings change:\n\nChange in your modelâ€™s architecture: This is a bigger change than before. Changing your modelâ€™s architecture can change the dimensionality of the embedding vectors. If the layers become larger/smaller, your vectors will too.\nUse another extraction method: In the event of the model not changing, you can still try several embedding extraction methods and compare between them.\nRetraining your model: Once you retrain the model from which you extract the embeddings, the parameters that define it will change. Hence, the values of the components of the vectors will change as well.\n\nPre-trained models\n\nGoogleâ€™s T5-11b (open sourced in 2019) requires a cluster of expensive GPUs simply to load and make predictions. Fine-tuning these systems requires even more resources\nOpenAI\n\nGPT-3 requires a significant number of GPUs simply to load.\nGPT-2 was originally announced in early 2019 and, thus, trained on data collected on or before that date\nGPT-J was released in 2021\n\ntechniques such as model distillation and noisy student training are being rapidly developed by the research community to reduce model size\n\nTransformers Compared to Other DL Architecture\n\nRNNs implement sequential processing: The input (letâ€™s say sentences) is processed word by word.\nTransformers use non-sequential processing: Sentences are processed as a whole, rather than word by word\nThe LSTM requires 8 time-steps to process the sentences, while BERT requires only 2\n\nBERT is better able to take advantage of parallelism, provided by modern GPU acceleration\n\nRNNs are forced to compress their learned representation of the input sequence into a single state vector before moving to future tokens.\nLSTMs solved the vanishing gradient issue that vanilla RNNs suffer from, but they are still prone to exploding gradients. Thus, they are struggling with longer dependencies\nTransformers, on the other hand, have much higher bandwidth. For example, in the Encoder-Decoder Transformer model, the Decoder can directly attend to every token in the input sequence, including the already decoded.\nTransformers use a special case called Self-Attention: This mechanism allows each word in the input to reference every other word in the input.\nTransformers can use large Attention windows (e.g.Â 512, 1048). Hence, they are very effective at capturing contextual information in sequential data over long ranges.\n\n\nTerms\n\nCoreference Resolution (article)\n\nâ€˜Daddyâ€™, â€˜Heâ€™ and â€˜Hisâ€™ refer to the same entity, identifying those entities and linking them to their antecedent is called co-reference resolution\nUse cases: Information Extraction, Question Answering, Summarization, Machine Translation\nModels:\n\nF-coref - {{fastcoref}}, Fastcoref â€” A Practical Package for Coreference Resolution\n\n\nSemantic Search - A text query is transformed into an embedding then the most similar document embeddings are found and returned\n\ne.g.Â a search engines; Google uses BERT on a large percentage of their queries\n\nZipfâ€™s Law - the frequency of any word is inversely proportional to its rank in the frequency table. Thus the most frequent word will occur approximately twice as often as the second most frequent word, three times as often as the third most frequent word, etc.\n\nLoss functions\n\nCategorical Cross-Entropy\n\nUse when your labels are one-hot encoded\n\nSparse Categorical Cross-Entropy\n\nUse when your labels are integers\n\n\nMetrics\n\nChecklist - Microsoft-led effort from 2020 to establish behavioral tests for NLP\n\nAttention\n\nAttention heads -Â  enable the Transformer to learn relationships between a time step and every other time step in the input sequence\nGeneral Process\n\nSearch for the most relevant information located in different positions in the input sequence when translating a word in each step.\nGenerate translations for the source token (word) with regard to.:\n\nThe context vector of these relevant positions\nPreviously generated words, simultaneously\n\n\nCharacteristics\n\nThe softness of attention:\n\nSoft: uses a weighted average of all keys to build the context vector.\n\nDifferentiable, so trainable by standard back-propagation methods.\n\nHard: context vector is computed from stochastically sampled keys.\n\nDue to stochastic sampling, hard attention is computationally less expensive compared with soft attention which tries to compute all attention weights at each step\nNon-differentiable, so optimized via maximizing an approximate variational lower bound or reinforcing\n\nLocal: soft attention in a window around a position. (i.e.Â only uses a subset of keys)\n\na mixture of hard and soft attention (i.e.Â uses a subset but not chosen stochastically)\nReduces the computational complexity and, unlike hard attention, it is differentiable which makes it easy to implement and optimize.\n\nGlobal: similar to soft attention (?)\n\nForms of input feature:\n\nItem-wise: input representation is an encoding\nLocation-wise: ?\n\nusually used in visual tasks\n\n\nInput representation:\n\nCo-attention, Self-attention, Distinctive attention, Hierarchical attention\n\nOutput representation:\n\nMulti-head, Single output, Multi-dimensional\n\nCombinations used for various tasks\n\n\nDifference between Attention Scores and the weights of a fully-connected layer (thread)\nCategories of Attention Modifications\n\nFor much more detail on these, see The Map Of Transformers\nSparse attention: This technique tries to lower the computation time and the memory requirements of the attention mechanism by taking a smaller portion of the inputs into account instead of the entire input sequence, producing a sparse matrix in contrast to a full matrix.\nLinearized attention: Disentangling the attention matrix using kernel feature maps, this method tries to compute the attention in the reverse order to reduce the resource requirements to linear complexity.\nPrototype and memory compression: This line of modification tries to decrease the queries and key-value pairs to achieve a smaller attention matrix which in turn reduces the time and computational complexity.\nLow-rank self-attention: By explicitly modeling the low-rank property of the self-attention matrix using parameterization or replacing it with a low-rank approximation tries to improve the performance of the transformer.\nAttention with prior: Leveraging the prior attention distribution from other sources, this approach, combines other attention distributions with the one obtained from the inputs.\nModified multi-head mechanism: There are various ways to modify and improve the performance of the multi-head mechanism which can be categorized under this research direction.\n\n\n\nTraining\n\nMisc\nStrategies (Raschka thread, code)\n\nMisc\n\nComputation time correlates w. Accuracy\n\nFeature-based: 9 min, 83% acc\nFinetuning I (last 2 layers): 20 min, 87% acc\nFinetuning II (whole): 49 min, 93% acc\n\n\nTrain from scratch\n\nNot feasible since transformers require large amounts of data. So, we use unlabeled data for pretraining (as opposed to training from scratch in a supervised fashion)\n\nFeature-based: Train new model on embeddings\n\nTake a pre-trained transformer and cut off the last layer. Then, run Large Language Model (LLM) in inference mode and train a new classifier on the embeddings.\n\nFinetuning I: Freeze all but output layer weights\n\nAdd randomly initialized output layers and train in a transfer-learning fashion.\nOnly update the fully-connected output layer and keep the base LLM frozen\n\nFinetuning II: Update all weights\n\nAdd randomly initialized output layers and train in a transfer-learning fashion.\nUpdate all model weights (the LLM base model as well as the output layers)\n\nZero-shot learning (ZSL)\n\nA problem setup in which the model is asked to solve a prediction task that it was not trained on. This often involves recognizing or classifying data into concepts it had not explicitly seen during training.\nIn most cases, a supervised model trained on a given task will still outperform a model using ZSL, so ZSL is more often used before supervised labels are readily available.\n\nCan significantly reduce the cost of obtaining labels\n\nIf a model is able to automatically classify data into categories without having been trained on that task, it can be used to generate labels for a downstream supervised model. These labels can be used to bootstrap a supervised model, in a paradigm similar to active learning or human-in-the-loop machine learning.\nMake predictions without training data.\nUse a pre-trained model and provide a task via the model-pr\n\nFew-shot learning\n\nLearning from a few labeled examples.\n\nEither as an extension of the zero-shot example (with more Q&A); or embed examples\n\nThen do a nearest neighbor search and select the most similar example.\n\n\nBlock-Recurrent Transformer\n\nThe main breakthrough of this model is the Recurrent Cell: A modified Transformer layer that works in a recurrent fashion.\n\nMisc\n\nNotes from\n\nBlock-Recurrent Transformer: LSTM and Transformer Combined\n\n\n\n\n\nBERT\n\nBidirectional Encoder Representations of Transformers\nUsually avaiable as a pre-trained model that has been trained across a huge corpus of data like Wikipedia to generate similar embeddings as Word2Vec\n\nThe user then fine tunes the model by training it on a corpus of data that fits their context well, for example, medical literature.\n\nEmbeddings take into account the context of the word.\n\ne.g.Â the word â€œplayâ€ in â€œIâ€™m going to see a playâ€ and â€œI want to playâ€ will correctly have different embeddings\n\nIssues\n\nThe initial BERT model has a limit of 512 tokens. The naive approach to addressing this issue is to truncate the input sentences.\n\nAlternatively, we can create Transformer Models that surpass that limit, making it up to 4096 tokens. However, the cost of self-attention is quadratic with respect to the sentence length.\n\nscalability becomes quite challenging"
  },
  {
    "objectID": "qmd/optimization-equation-systems.html#sec-opt-misc",
    "href": "qmd/optimization-equation-systems.html#sec-opt-misc",
    "title": "Optimization",
    "section": "Misc",
    "text": "Misc\n\nPackages\n\nCRAN Task View"
  },
  {
    "objectID": "qmd/optimization-equation-systems.html#sec-opt-solvers",
    "href": "qmd/optimization-equation-systems.html#sec-opt-solvers",
    "title": "Optimization",
    "section": "Solvers",
    "text": "Solvers\n\nBase R\n\nsolve - The default method is an interface to the LAPACK routines DGESV and ZGESV. LAPACK is from https://netlib.org/lapack/.\nExample\n\\[\n\\begin{align}\n3x + 2y - z &= 7\\\\\nx - y + 2z &= -1\\\\\n2x + 3y + 4z &= 12\n\\end{align}\n\\]\na &lt;- \n  matrix(c(3, 2, -1, 1, -1, 2, 2, 3, 4),\n         nrow = 3, \n         byrow = TRUE)\nb &lt;- c(7, -1, 12)\nsolve(a, b)\n#&gt; [1] 0.6571429 2.8000000 0.5714286\n\nOpen Source\n\nSee packages in bookmarks\nCBC\nGoogle OR Tools\n\nCommercial\n\nXPress, CPLEX\nGurobi\n\nFor a DoorDashâ€™s vehicle routing problem, Gurobi was 34 times faster on average than CBC\nPros\n\nScalability of the solvers, ease of abstracting feasible solutions when optimality is hard, ability to tune for different formations, relatively easy API integration to Python and Scala, flexibility of the prototype and deployment licensing terms from the vendors, and professional support."
  },
  {
    "objectID": "qmd/optimization-equation-systems.html#sec-opt-bmp",
    "href": "qmd/optimization-equation-systems.html#sec-opt-bmp",
    "title": "Optimization",
    "section": "Bipartite Matching Problem",
    "text": "Bipartite Matching Problem\n\nA bipartite graph has 2 sets of vertices (e.g.Â Dashers and Merchants) and the edges (ETA to merchants) only travel between boths sets (Dashers and Merchants) and not within the sets (e.g.Â Dasher to Dasher).\nSolution: Hungarian Algorithm and a lecture (15 min)\nExample: Optimally assign Dashers to Merchants\n\n\nShows DoorDash delivery persons (Dashers) with ETA times to merchants, estimated food pick-up times from merchants, estimated customer drop-off times\n\nIssues:\n\nRuntime of large instances is excessive for a real-time dynamic system (polynomial time)\nDoesnâ€™t support more complicated routes with 2 or more tasks\n\nExample shows 5 merchants and only 4 Dashers, so at least one Dasher needs to travel to 2 Merchants"
  },
  {
    "objectID": "qmd/optimization-equation-systems.html#sec-opt-vrp",
    "href": "qmd/optimization-equation-systems.html#sec-opt-vrp",
    "title": "Optimization",
    "section": "Vehicle Routing Problem",
    "text": "Vehicle Routing Problem\n\nhttps://en.wikipedia.org/wiki/Vehicle_routing_problem\n\nAllows multiple deliveries in a route\n\nExample: Optimizing Deliverers (Dashers) with food orders (Merchant/Customers) (See example in bipartite matching section)\n \nCan be solved using Mixed Integer Programming (MIP)\n\n{ompr}\nAlso see Logistics &gt;&gt; Case Study: DoorDash &gt;&gt; Optimization"
  },
  {
    "objectID": "qmd/optimization-equation-systems.html#sec-opt-ba",
    "href": "qmd/optimization-equation-systems.html#sec-opt-ba",
    "title": "Optimization",
    "section": "Budget Allocation",
    "text": "Budget Allocation\n\nNotes from Automate Budget Planning Using Linear Programming\nAlso see Project Management &gt;&gt; Decision Models (details on calculations for a project budget application)\nMisc\n\nTop Management Guidelines are for constraints but there may be other management objectives that are part of the decision making process\n\nConsiderations\n\nReturn on investment (ROI) of each project after three years (â‚¬)\nTotal costs and budget limits per year (â‚¬/Year)\n\nTop management guidelines\n\nSustainable Development (CO2 Reduction)\nDigital Transformation (IoT, Automation and Analytics)\nOperational Excellence (Productivity, Quality and Continuous Improvement)\n\n\n\nProcess\n\nScenario: Budget Planning Process\n\nAs a Regional Director you need to allocate your budget on projects\n\n\n8 â€œMarket Verticalsâ€: Luxury, Cosmetics, Fashion, etc.\n\n\nBuild your Model\n\nExploratory Data Analysis\n\nAnalyze the budget applications received\n\n\nLinear Programming Model\n\nDecisions variables, objective function and constraints\nObjective Function: Maximize Total ROI\n\n\nDecision Variable: P is a boolean for whether project i is accepted or rejected\n\nConstraints\n\nBudget\n\n4.5 Mâ‚¬ that you split in three years (1.25Mâ‚¬, 1.5Mâ‚¬, 1.75Mâ‚¬)\n\n\nStrategic (Top Management Guidelines)\n\n\nOmin, Smin, and Dmin = 1 Mâ‚¬\n\nOther potential constraints\n\nMaximum budget allocation per country, market vertical, or warehouse\nBudget allocation target (95% of the budget should be allocated)\n\n\n\nInitial Solution: Maximum ROI\n\nWhat would be the results if you focus only on ROI maximization? (i.e.Â 0 constraints)\n\nMight need to apply budget constraints in this step.\n\n\nFinal Solution: Apply Budget and Strategic constraints to the model\n\nConclusion & Next Steps\n\nCreate an app or dashboard\n\nApp may involve the user being able to select different constraints or values of those constraints\nUploads budget applications (e.g.Â spreadsheets)\nVisuals for EDA\n\nBar or circular chart: count or percent of applications per Market Vertical\nBar: count of applications per Management Objective\n\nVisuals for results of optimization\n\nBar chart for each variation of constraints chosen: allocation per Management Objective"
  },
  {
    "objectID": "qmd/outliers.html#sec-outliers-eda",
    "href": "qmd/outliers.html#sec-outliers-eda",
    "title": "Outliers",
    "section": "Misc",
    "text": "Misc\n\nResources\n\nNeed to examine this article more closely, Taking Outlier Treatment to the Next Level\n\nDiscusses detailed approach to diagnosing outliers , eda, diagnostics, robust regression, winsorizing, nonlinear approaches for nonrandom outliers.\n\nFor Time Series, see bkmks, pkgs in time series &gt;&gt; cleaning/processing &gt;&gt; outliers"
  },
  {
    "objectID": "qmd/outliers.html#eda",
    "href": "qmd/outliers.html#eda",
    "title": "Outliers",
    "section": "EDA",
    "text": "EDA\n\nIQR\n\nObservations above \\(q_{0.75} + (1.5 \\times \\text{IQR})\\) are considered outliers\nObservations below \\(q_{0.25} - (1.5 \\times \\text{IQR})\\) are considered outliers\nWhere \\(q_{0.25}\\) and \\(q_{0.75}\\) correspond to first and third quartile respectively, and IQR is the difference between the third and first quartile\n\nHampel Filter\n\nObservations above \\(\\text{median} + (3 \\times \\text{MAD})\\) are considered outliers\nObservations below \\(\\text{median} - (3 \\times \\text{MAD})\\) are considered outliers\nUse mad(vec, constant = 1)Â  for the MAD"
  },
  {
    "objectID": "qmd/outliers.html#sec-outliers-tests",
    "href": "qmd/outliers.html#sec-outliers-tests",
    "title": "Outliers",
    "section": "Tests",
    "text": "Tests\n\n** All tests assume data is from a Normal distribution **\nSee the EDA section for ways to find potential outliers to test\nGrubbsâ€™s Test\n\nTest either a maximum or minimum point\n\nIf you suspect multiple points, you have remove the max/min points above/below the suspect point. Then test the subsetted data. Repeat as necessary\n\nH0: There is no outlier in the data.\nHa: There is an outlier in the data.\nTest statistics\n\\[\nG = \\frac {\\bar{Y} - Y_{\\text{min}}}{s}G = \\frac {Y_{\\text{max}} - \\bar{Y}}{s}\n\\]\n\nStatistics for whether the minimum or maximum sample value is an outlier\n\nThe maximum value is outlier if\n\\[\nG &gt; \\frac {N-1}{\\sqrt{N}} \\sqrt{\\frac {t^2_{\\alpha/(2N),N-2}}{N-2+t^2_{\\alpha/(2N),N-2}}}\n\\]\n\nâ€œ&lt;â€ for minimum\nt is denotes the critical value of the t distribution with (N-2) degrees of freedom and a significance level of Î±/(2N).\nFor testing either the maximum or minimum value, use a significance level of level of Î±/N\n\nRequirements\n\nNormally distributed\nMore than 7 observations\n\noutliers::grubbs.test(x, type = 10, opposite = FALSE, two.sided = FALSE)\n\nx: a numeric vector of data values\ntype=10: check if the maximum value is an outlier, 11 = check if both the minimum and maximum values are outliers, 20 = check if one tail has two outliers.\nopposite:\n\nFALSE (default): check value at maximum distance from mean\nTRUE: check value at minimum distance from the mean\n\ntwo-sided: If this test is to be treated as two-sided, this logical value indicates that.\n\nsee bkmk for examples\n\nDixonâ€™s Test\n\nTest either a maximum or minimum point\n\nIf you suspect multiple points, you have remove the max/min points above/below the suspect point. Then test the subsetted data. Repeat as necessary.\n\nMost useful for small sample size (usually nâ‰¤25)\nH0: There is no outlier in the data.\nHa: There is an outlier in the data.\noutliers::dixon.test\n\nWill only accept a vector between 3 and 30 observations\nâ€œopposite=TRUEâ€ to test the maximum value\n\n\nRosnerâ€™s Test (aka generalized (extreme Studentized deviate) ESD test) Tests multiple points\n\nAvoids the problem of masking, where an outlier that is close in value to another outlier can go undetected.\nMost appropriate when nâ‰¥20\nH0: There are no outliers in the data set\nHa: There are up to k outliers in the data set\nres &lt;- EnvStats::rosnerTest(x,k)\n\nx: numeric vector\nk: upper limit of suspected outliers\nalpha: 0.05 default\nThe results of the test, res , is a list that contains a number of objects\n\nres$all.stats shows all the calculated statistics used in the outlier determination and the results\n\nâ€œValueâ€ shows the data point values being evaluated\nâ€œOutlierâ€ is True/False on whether the point is determined to be an outlier by the test\nRs are the test statistics\nÎ»s are the critical values"
  },
  {
    "objectID": "qmd/outliers.html#sec-outliers-meth",
    "href": "qmd/outliers.html#sec-outliers-meth",
    "title": "Outliers",
    "section": "Methods",
    "text": "Methods\n\nRemoval\n\nAn option if thereâ€™s sound reasoning (e.g.Â data entry error, etc.)\n\nWinsorization\n\nA typical strategy is to set all outliers (values beyond a certain threshold) to a specified percentile of the data\nExample: A 90% winsorization would see all data below the 5th percentile set to the 5th percentile, and data above the 95th percentile set to the 95th percentile.\nPackages\n\n({DescTools::Winsorize})\n({datawizard::winsorize})"
  },
  {
    "objectID": "qmd/outliers.html#sec-outliers-mod",
    "href": "qmd/outliers.html#sec-outliers-mod",
    "title": "Outliers",
    "section": "Models",
    "text": "Models\n\nBayes has different distributions for increasing uncertainty\nIsolation Forests - See Anomaly Detection &gt;&gt; Isolation Forests\nSupport Vector Regression (SVR) - See Algorithms, ML &gt;&gt; Support Vector Machines &gt;&gt; Regression\nExtreme Value Theory approaches\n\nfat tail stuff (need to finish those videos)\n\nRobust Regression (see bkmks &gt;&gt; Regression &gt;&gt; Other &gt;&gt; Robust Regression)\n\n{robustbase}\nCRAN Task View\n\nHuber Regression\n\nSee Loss Functions &gt;&gt; Huber Loss\nSee bkmks, Regression &gt;&gt; Generalized &gt;&gt; Huber\n\nTheil-Sen estimator\n\n{mblm}"
  },
  {
    "objectID": "qmd/package-development.html#sec-pkgdev-misc",
    "href": "qmd/package-development.html#sec-pkgdev-misc",
    "title": "Package Development",
    "section": "Misc",
    "text": "Misc\n\nPackages\n\n{goodpractices} - Give advice about good practices when building R packages. Advice includes functions and syntax to avoid, package structure, code complexity, code formatting, etc\n\nVersion Number Syntax\n\n0.0.0.9000 = the â€œ.9000â€ means intital experimental or in development version (.9001 would be the second experimental version)\n0.1.0 = the â€œ1â€ means this is your first â€œminorâ€ version\n1.0.0 = the â€œ1â€ means this is your first â€œmajorâ€ version\n\nIn function docs, always use @examples and NOT @example\nIf you have internal functions (e.g.Â little helper functions) that you donâ€™t want your users to have the regular access to then:\n\nDonâ€™t include @export for that functionâ€™s script\nDocumentation of the function isnâ€™t required\n\nUsing â€œStarting a Packageâ€ (below) takes some time when you want to test newly written functions.\n\nAlternative: load_all() - Quickly makes function available for interactive testing, (ctrl + shift + L)\n\nChange package name (if not on CRAN): changer::changer(\"current_pkg_name\", \"new_pkg_name\")\n\nShouldnâ€™t have the package project open when running this. I set the working dir to the root of my â€œProjectsâ€ directory locally (e.g Projects &gt;&gt; current_proj_directory, then set working directory to Projects)\nThis changes EVERYTHING automagically, but be sure and make a copy of the directory before running just in case.\nNeed to change repo name in the github repoâ€™s settings before pushing changes\n\nCreate Citation file for package: usethis::use_citation()\n\nOnly execute after youâ€™ve gone through putting your package on zenodo and have a generated citation to populate the fields of the CITATION file.\nSee indianacovid19data project for example\n\nIf planning to release on CRAN, use usethis::use_release_issue, which creates a checklist of things to fix/do before submitting to CRAN\n\nCRAN requires your pkg run without issues on all platforms. {covr} tracks test coverage for your R package and view reports locally or (optionally) upload the results to codecov or coveralls\n\nDeveloping Internal Packages\n\nDeveloping packages for your companyâ€™s specific use cases increases efficiency\n\nExamples\n\nCollection\n\nPulling data from public sources\n\nDatabase\n\nConnections and Querying\nAPI requests\nETL processes\n\nReport building\n\nData manipulation\nVisualization\n\n\n\nSee Building a team of internal R packages | Emily Riederer and VIDEO How to make internal R packages part of your team - RStudio\n\nAlso {RDepot} for management"
  },
  {
    "objectID": "qmd/package-development.html#starting-a-package",
    "href": "qmd/package-development.html#starting-a-package",
    "title": "Package Development",
    "section": "Starting a Package",
    "text": "Starting a Package\n\nCheck package name, Create pkg, DESCRIPTION, Set-Up Git\n\nCheck if package name already taken on CRAN: available::available(\"package_name\", browse = FALSE)\n\nAsks you stuff about using â€œurban dictionaryâ€ to see if your package name might be offensive. Just say yes. (or it keeps asking)\nYou want it to say that your package name is available on CRAN\nShows sentiment analysis of your package name according to different dictionaries\n\nCreate directory, a project, a basic package skeleton in that directory\nsetwd(\"~/R/Projects\")\nusethis::create_package(\"package_name\")\nOpen project and fill out some of DESCRIPTION\n\nTitle, Authors, Description\n\nGo to Build pane in RStudio \\(\\rightarrow\\) more \\(\\rightarrow\\) Configure Build Tools \\(\\rightarrow\\) Make sure the â€œGenerate documentation with Roxygenâ€ box is ticked (tick it and click ok)\nSet-Up Git: usethis::use_git()\n\nIt will ask to commit the package skeleton to github \\(\\rightarrow\\) Choose yes to commit locally\nItâ€™ll ask you to restart RStudio to activate the git pane \\(\\rightarrow\\) Choose yes\nSet-up of GH Auth token, if you donâ€™t have one\n\nThen use usethis::create_github_token() and follow steps\nRefresh session once youâ€™ve updated .Renviron\n\nFor private repo: usethis::use_github(private = TRUE)\nFor public repo: usethis::use_github(private = FALSE, protocol = \"ssh\")\n\nChoose 1 to use ssh key\nâ€œAre title and description okay?â€ \\(\\rightarrow\\) choose 3. Yes\n\n\n\n\n\nBasic Set-up\n\nUse Markdown for documentation: usethis::use_roxygen_md()\nAdd license: usethis::use_mit_license(copyright_holder = \"Eric Book\")\nAdd Readme.Rmd: usethis::use_readme_rmd()\n\nYouâ€™ll still need to render/knit README.Rmd regularly.\nTo keep README.md up-to-date, devtools::build_readme() is handy.\nYou could also use GitHub Actions to re-render README.Rmd every time you push. An example workflow can be found here: https://github.com/r-lib/actions/tree/master/examples.\n\nAdd News/Changlog file: usethis::use_news_md()\nAdd Article/Vignette: usethis::use_vignette\nDocument, Install, and Check package\n\nBuild pane \\(\\rightarrow\\)\n\nmore \\(\\rightarrow\\) Run Document\nRun Install and Restart\nRun Check\n\n\nCommit files and Push\n\n\n\nDevelopment\n\nAdd common imported functions to DESCRIPTION\nusethis::use_tidy_eval()\nusethis::use_tibble()\nAdd data\n# read data into environment\npkg_dat &lt;- readr::read_rds(\"../path/to/data.rds\")\nusethis::use_data(pkg_dat, overwrite = TRUE)\n\nCreates data directory and adds data as an .rda file\nDocument the data: usethis::use_r(\"data\")\n\nSee indianacovid19data for examples\n\nIf you want to store binary data and make it available to the user, put it in data/. This is the best place to put example datasets. If you want to store parsed data, but NOT make it available to the user, put it in R/sysdata.rda. This is the best place to put data that your functions need.\nIf you want to store raw data, put it in inst/extdata.\n\nAdd Functions\n\nCreate R file: usethis::use_r(\"name-function-file\")\nWrite function\n\nUse pkg::function format for every external package function\n\nAdd documentation (e.g.Â @description, @params, @returns, @details, @references, @export, and @examples\n\nWhen pasting code into examples, use the RStudio multiple cursors feature (ctrl + alt + up/down) to add â€œ#&gt;â€ to all the lines of code at once\n\nAdd packages used in function to DESCRIPTION: usethis::use_package(\"package_name\")\nMake sure hardcoded variable names are in global variables file\nMake sure any data used in @examples is added and documentedf\nIf submitting to CRAN: need examples and they should be wrapped in \\dontrun{} and \\donttest{}.\nTest\n\nBuild pane \\(\\rightarrow\\) more \\(\\rightarrow\\) Run Document (Ctrl+Shift+D) \\(\\rightarrow\\) Run Install and Restart (ctrl+shift+B) \\(\\rightarrow\\) Run Check (ctrl+shift+E)\n\nAfter writing a function or two, set-up {pkgdown}, and return here.\nAdd new function to _pkgdown.yml\nAdd new function to any additional pages of the site (e.g.Â vignettes, Readme.Rmd)\n\nRemember to knit\n\nRun pkgdown::build_site()\n\nKnits html pages for reference function pages\n\nCommit and Push\nRinse and Repeat for each function\n\n\n\n\npkgdown\n\nDocs\nSet-Up: usethis::use_pkgdown()\n\nCreates yaml file: _pkgdown.yml\nFill out yaml\n\nPublish website and set-up Github Actions: usethis::use_pkgdown_github_pages()\nCustomize site\n\nName css file, â€œextra.cssâ€, and place in folder called, â€œpkgdownâ€ (also extra.js, extra.scss)\nWhen testing out themes, css features, etc., iterating is much faster when using build_home_index(); init_site() instead of build_site(). Then, refresh html page in browser.\nSee docs for bslib variables that can be used in _pkgdown.yml (requires using bootsrap 5)."
  },
  {
    "objectID": "qmd/package-development.html#sec-pkgdev-depend",
    "href": "qmd/package-development.html#sec-pkgdev-depend",
    "title": "Package Development",
    "section": "Dependencies",
    "text": "Dependencies\n\nImports and Depends\n\nImports just loads the package\n\nUnless there is a good reason otherwise, you should always list packages in Imports not Depends. Thatâ€™s because a good package is self-contained, and minimises changes to the global environment (including the search path)\n\nDepends attaches it.\n\nLoading and Attaching\n\nLoading\n\nThe package is available in memory, but because itâ€™s not in the search path (path that R searches for functions), you wonâ€™t be able to access its components without using ::.\n\nAttaching\n\nPuts the package in the search path. You canâ€™t attach a package without first loading it\nBoth library() (throws error when pkg not installed )or require() (just returns false when pkg not installed) load then attach the package"
  },
  {
    "objectID": "qmd/package-development.html#sec-pkgdev-test",
    "href": "qmd/package-development.html#sec-pkgdev-test",
    "title": "Package Development",
    "section": "Testing",
    "text": "Testing\n\n{testthat} explicitly evaluates the outputs of your function but you can add a test that makes sure the checks on inputs within the function are working\nRun tests (Ctrl+Shift+T): test()\n\nAlso ran when using, check()\n\nSet-up - Run usethis::use_testthat within the project directory\n\nCreates:\n\nA tests folder in your working directory\nA testthat folder in the tests folder where your tests will live\nA testthat.R file in the tests folder which organizes things for running. Donâ€™t modify this manually.\n\n\nNames of your testing files must begin with â€˜testâ€™\n\ne.g.Â testing file, â€˜test-my-function.Râ€™, which lives in testthat folder\n\nWriting Tests\n\nWrite tests using the test_that function, and with each test, thereâ€™s an â€œexpectation.â€\nYou can have one or more tests in each file and one or more expectations within each test.\nExample\ntest_that(\"description of the test\", {\n        test_output1 &lt;- dat %&gt;%\n            your_package_function()\n\n        expect_equal(nrow(test_output1), 6)\n        # etc...\n})\nExample\n\n\nWhere sse is the function youâ€™re testing\n\nExample expect_error(compute_corr(data = faithful, var1 = erruptions, var2 = waiting)\n\nerruptions isnâ€™t a column in the dataset and should throw an error because of a check inside the function\nAlso expect_warning() available\n\nExample: Compare optional outputs of a nested function to the individual functions within that nested function\n\n\nerror is the nested function with optional outputs of sse error or mape error\nSecond chunk should say â€œmape calculations workâ€\n1st chunk checks if error with sse option output is the same as sse output\n2nd chunk checks if error with mape option output is the same as mape output\n\n\nRun Tests\n\nUse the test_file function\n\nUse the test_dir function: e.g.Â test_dir(wd$test_that) for running all tests.\nPress the Run Tests button in R Studio if you open the test file.\nHighlight and run the code."
  },
  {
    "objectID": "qmd/post-hoc-analysis-ab.html#misc",
    "href": "qmd/post-hoc-analysis-ab.html#misc",
    "title": "54Â  A/B",
    "section": "54.1 Misc",
    "text": "54.1 Misc\n\n\nNotes from: How to Select the Right Statistical Tests for Different A/B Metrics\nAlso see\n\nPost-Hoc Analysis, general &gt;&gt; Frequentist\nExperiments, A/B Testing\n\n&gt;&gt; Workflow &gt;&gt; Post-Experiment - Analyze the Results\n&gt;&gt; Potential Biases in the Experiment\n\n\nSome business metric distributions significantly differ from the nice uni-modal normal/student-t distribution\nAsk\n\nDoes the sampling distribution match the assumption (e.g.Â independence, normality, etc.) of the proposed test methods?\nDoes the randomization unit align with the analysis unit?"
  },
  {
    "objectID": "qmd/post-hoc-analysis-ab.html#metrics",
    "href": "qmd/post-hoc-analysis-ab.html#metrics",
    "title": "54Â  A/B",
    "section": "54.2 Metrics",
    "text": "54.2 Metrics\n\nUser Average Metrics\n\nIndicators that average the total business performance by the number of unique users\n\nExamples\n\naverage number of likes per user\naverage stay time per user during the experiment period.\n\n\n\nUser-level Conversion Metrics\n\nmetrics out of binary outcomes, 1/0\n\ne.g conversion rate = # converted users/# all users\n\n\nPageview-level Conversion Metrics\n\ne.g.Â click-through rate (CTR) = # click events / # view events\nThe essential problem is that the unit of analysis (i.e.Â event level â€” click events and impression events) is different from the unit of randomization (i.e.Â user level), which could invalidate our t-test and underestimate the variance.\n\nSince each sample observation is a view event, we need to ensure all the view events are independent. Otherwise, we are not able to use CLT to assert normality for our mean statistics.\nIn A/B experiments where we randomize by users (this is usually the default option because each user would have an inconsistent product experience if we randomize by sessions), a user can have multiple click events or view events, and these events will be correlated.\n\nTherefore, sample variance will no longer be an unbiased estimate of the population variance\nCan result in spuriously low p-values\n\n\nDelta method converts the pageview metric to a user-level metric where independence and iid assumption hold\n\n\narticle with py code\nWith the correct variance, you can perform the t-test and get the p-value\n\nBootstrap method - repeatedly draw many samples from all the view events in each group stratified by user ids, then calculate many mean statistics from these bootstrap samples, and estimate the variance for these statistics accordingly.\n\nStratifying by user ID ensures i.i.d. assumption is met and the variance estimation is trustworthy.\nComputationally expensive as the simulation is basically manipulating and aggregating the tens of billions of user logs for many iterations (Complexity ~ O(nB) where n is the sample size and B is the number of bootstrap iterations).\nWith the correct variance, you can perform the t-test and get the p-value\n\n\nPercentile Metrics\n\nMany business aspects are characterized by edge cases and thus are better described by quantile metrics\n\ne.g.Â 95th percentile page load time/latency\n\nIssue is that some of these defined at the Pageview level and not at the user-level (i.e.Â randomization level)(For more details, see Pageview-level Conversion Metrics)\nBootstrap method - Use to estimate the empirical variance and t-test the sample means of percentile metrics ((For more details, see Pageview-level Conversion Metrics)\nProportional Z-test\n\nFor details, see the article and Ch 8.6 of Causal Inference and Its Applications in Online Industry\n\n\nSum Metrics (Not Recommended)\n\nAggregated counts\n\ne.g.Â total number of the article read, the total Gross Merchandise Value (GMV), the total videos posted\n\nArticle says these sum metrics are problematic because of business fluctuations (user average metrics) and by the inevitable random error introduced by the traffic diversion\n\nI donâ€™t know what this means and the article didnâ€™t explain it well.\nThere may be a better explanation in the book in the previous section\nThere are also references at the end of the article\n\nRecommended to test the effectiveness/statistical significance of the product feature using a user average metric and calculating the incremental lift in the SUM metrics"
  },
  {
    "objectID": "qmd/post-hoc-analysis-ab.html#bayesian",
    "href": "qmd/post-hoc-analysis-ab.html#bayesian",
    "title": "54Â  A/B",
    "section": "54.3 Bayesian",
    "text": "54.3 Bayesian\n\nMisc\n\npackages:\n\n{bayesAB} - Independent Beta Estimation (IBE)\n{abtest} - Logit Transformation Testing (LTT)\n{bartCausal} - Bayesian Additive Regression Trees (BART)\n\nNo website or vignettes so see bkmks (Algorithms &gt;&gt; Decision Tree/Random Forests &gt;&gt; Bayes) for articles, papers\n1st or 2nd in causal competitons from 2016 to 2022\n\n\n\nvs Frequentist analysis\n\nDifferentiates between different hypotheses and can prove the absence of an effect (e.g., A and B do not differ) in light of the data. A low p-value however is only indirect evidence suggests that the data are too inconclusive or not odd enough to reject the null hypothesis.\n\nâ€œDifferentiates between different hypothesesâ€ - maybe this has something do with working with distributions in Bayesianism\n\nInforms economic decision-making. Because frequentist statistics do not estimate the probability of parameters (e.g., conversion rate for A vs.Â B), we cannot translate it back into relevant business outcomes like expected profit or survival rates.\nIs truly intuitive or straightforward to non-stats people. It naturally complements the human way of thinking about evidence. For Frequentists, the interpretation of probability is reversed which makes it a pain to understand directly.\n\nBayesian updating is more intuitive and thus easier to explain than p-values (i.e.Â imagine a universe where the null hypothesis is true, thenâ€¦)\n\nRequires much less data and is more time-efficient. Unlike frequentist statistics, Bayesians do not follow a sampling plan that would make an analysis of existing data illegitimate before the full sample size is reached. (?)"
  },
  {
    "objectID": "qmd/post-hoc-analysis-anova.html#sec-phoc-anova-misc",
    "href": "qmd/post-hoc-analysis-anova.html#sec-phoc-anova-misc",
    "title": "ANOVA",
    "section": "Misc",
    "text": "Misc\n\nANOVA vs.Â Regression (GPT-3.5)\n\nDifferent Research Questions:\n\nANOVA is typically used when you want to compare the means of three or more groups to determine if there are statistically significant differences among them. Itâ€™s suited for situations where youâ€™re interested in group-level comparisons (e.g., comparing the average test scores of students from different schools).\nRegression, on the other hand, is used to model the relationship between one or more independent variables and a dependent variable. Itâ€™s suitable for predicting or explaining a continuous outcome variable.\n\nData Type:\n\nANOVA is often used with categorical independent variables and a continuous dependent variable. It helps assess whether the categorical variable has a significant impact on the continuous variable.\nRegression can be used with both categorical and continuous independent variables to predict a continuous dependent variable or to examine the relationship between variables.\n\nMultiple Factors:\n\nANOVA is designed to handle situations with multiple categorical independent variables (factors) and their interactions. It is useful when you are interested in understanding the combined effects of several factors.\nRegression can accommodate multiple independent variables as well, but it focuses on predicting the value of the dependent variable rather than comparing groups.\n\nHypothesis Testing:\n\nANOVA tests for differences in means among groups and provides p-values to determine whether those differences are statistically significant.\nRegression can be used for hypothesis testing, but itâ€™s more often used for estimating the effect size and making predictions.\n\nAssumptions:\n\nANOVA assumes that the groups are independent and that the residuals (the differences between observed values and group means) are normally distributed and have equal variances.\nRegression makes similar assumptions about residuals but also assumes a linear relationship between independent and dependent variables.\n\n\nAssumptions\n\nEach group category has a normal distribution.\nEach group category is independent of each other and identically distributed (iid)\nGroup categories have of similar variance (i.e.Â homoskedastic variance)\n\nIf this is violated\n\nand the ratio of the largest variance to the smallest variance is less than 4, then proceed with one-way ANOVA (robust to small differences)\nand the ratio of the largest variance to the smallest variance is greater than 4, we may instead choose to perform a Kruskal-Wallis test. This is considered the non-parametric equivalent to the one-way ANOVA. (example)\n\nEDA\ndata %&gt;%\nÂ  group_by(program) %&gt;%\nÂ  summarize(var=var(weight_loss))\n#&gt; A tibble: 3 x 2\n#&gt; Â  programÂ  varÂ  Â \n#&gt; 1 AÂ  Â  Â  0.819\n#&gt; 2 BÂ  Â  Â  1.53Â \n#&gt; 3 CÂ  Â  Â  2.46\nPerform a statisical test to see if these variables are statistically significant (See Post-Hoc Analysis, Difference-in-Means &gt;&gt; EDA &gt;&gt; Tests for Equal Variances)\n\n\nEta Squared\n\nMetric to describe the effect size of a variable\nRange: [0, 1]; values closer to 1 indicating that a specific variable in the model can explain a greater fraction of the variation\nlsr::etaSquared(anova_model) (use first column of output)\nGuidelines\n\n0.01: Effect size is small.\n0.06: Effect size is medium.\nLarge effect size if the number is 0.14 or above"
  },
  {
    "objectID": "qmd/post-hoc-analysis-anova.html#sec-phoc-anova-math",
    "href": "qmd/post-hoc-analysis-anova.html#sec-phoc-anova-math",
    "title": "ANOVA",
    "section": "Mathematics",
    "text": "Mathematics\n\nAsides:\n\nthese look like theÂ  variance formula except for not dividing by the sample size to get the â€œaverageâ€ squared distance\nSSA formula - the second summation just translates to multiplying by ni, the group category sample size, since there is no j in that formula\n\nCalculate SSA and SSE\n\\[\n\\begin{align}\n\\text{SST} &= \\text{SSA} + \\text{SSE} \\\\\n&= \\sum_{i = 1}^a \\sum_{j=i}^{n_i} (x_{i,j} - \\mu)^2 \\\\\n&= \\sum_{i = 1}^a \\sum_{j=i}^{n_i} (\\bar x_i - \\mu)^2 + \\sum_{i = 1}^a \\sum_{j=i}^{n_i} (x_{i,j} - \\bar x_i)^2\n\\end{align}\n\\]\n\n\\(\\text{SST}\\): Sum of Squares Total\n\\(\\text{SSA}\\): Sum of Squares between categories, treatments, or factors\n\nâ€œAâ€ stands for attributes (i.e.Â categories)\n\n\\(\\text{SSE}\\): Sum of Squares of Errors; randomness within categories, treatments, or factors\n\\(x_{ij}\\): The jth observation of the ith category\n\\(\\bar x_i\\): The sample mean of category i\n\\(\\mu\\): The overall sample mean\n\\(n_i\\): The group category sample size\n\\(a\\): The number of group categories\n\nCalculate MSA and MSE\n\\[\n\\begin{align}\n\\text{MSE} &= \\frac{\\text{SSE}}{N-a} \\\\\n\\text{MSA} &= \\frac{\\text{SSA}}{a-1}\n\\end{align}\n\\]\n\nWhere N is the total sample size\n\nCalculate the F statistic and P-Value\n\\[\nF = \\frac{\\text{MSA}}{\\text{MSE}}\n\\]\n\nFind the p-value (need a table to look it up)\nIf our F statistic is less than the critical value F statistic for a Î±=0.05 than we cannot reject the null hypothesis (no statistical difference between categories)\n\nDiscussion\n\nIf there is a group category that has more variance than the others attribute error (SSA) we should then pick that up when we compare it to the random error (SSE)\n\nIf a group is further away from the overall mean then it will increase SSA and thus influence the overall variance but might not always increase random error"
  },
  {
    "objectID": "qmd/post-hoc-analysis-anova.html#sec-phoc-gen-fdim-panov",
    "href": "qmd/post-hoc-analysis-anova.html#sec-phoc-gen-fdim-panov",
    "title": "ANOVA",
    "section": "Post-ANOVA Tests",
    "text": "Post-ANOVA Tests\n\nAssume approximately Normal distributions\nFor links to more details about each test, https://www.statisticshowto.com/probability-and-statistics/statistics-definitions/post-hoc/\nDuncanâ€™s new multiple range test (MRT)\n\nWhen you run Analysis of Variance (ANOVA), the results will tell you if there is a difference in means. However, it wonâ€™t pinpoint the pairs of means that are different. Duncanâ€™s Multiple Range Test will identify the pairs of means (from at least three) that differ. The MRT is similar to the LSD, but instead of a t-value, a Q Value is used.\n\nFisherâ€™s Least Significant Difference (LSD)\n\nA tool to identify which pairs of means are statistically different. Essentially the same as Duncanâ€™s MRT, but with t-values instead of Q values.\n\nNewman-Keuls\n\nLike Tukeyâ€™s, this post-hoc test identifies sample means that are different from each other. Newman-Keuls uses different critical values for comparing pairs of means. Therefore, it is more likely to find significant differences.\n\nRodgerâ€™s Method\n\nConsidered by some to be the most powerful post-hoc test for detecting differences among groups. This test protects against loss of statistical power as the degrees of freedom increase.\n\nScheffÃ©â€™s Method\n\nUsed when you want to look at post-hoc comparisons in general (as opposed to just pairwise comparisons). Scheffeâ€™s controls for the overall confidence level. It is customarily used with unequal sample sizes.\n\nTukeyâ€™s Test\n\nThe purpose of Tukeyâ€™s test is to figure out which groups in your sample differ. It uses the â€œHonest Significant Difference,â€ a number that represents the distance between groups, to compare every mean with every other mean.\n\nDunnettâ€™s Test\n\nLike Tukeyâ€™s this post-hoc test is used to compare means. Unlike Tukeyâ€™s, it compares every mean to a control mean.\n{DescTools::DunnettTest}\n\nBenjamin-Hochberg (BH) Procedure\n\nIf you perform a very large amount of tests, one or more of the tests will have a significant result purely by chance alone. This post-hoc test accounts for that false discovery rate."
  },
  {
    "objectID": "qmd/post-hoc-analysis-anova.html#sec-phoc-anova-oneway",
    "href": "qmd/post-hoc-analysis-anova.html#sec-phoc-anova-oneway",
    "title": "ANOVA",
    "section": "One-Way",
    "text": "One-Way\n\nMeasures if thereâ€™s a difference in means between any group category\nExample: 1 control, 2 Test groups\n\nData\ndata &lt;- data.frame(Group = rep(c(\"control\", \"Test1\", \"Test2\"), each = 10),\nvalue = c(rnorm(10), rnorm(10),rnorm(10)))\ndata$Group&lt;-as.factor(data$Group)\nhead(data)\n#&gt; Â  GroupÂ  Â  Â  value\n#&gt; 1 controlÂ  0.1932123\n#&gt; 2 control -0.4346821\n#&gt; 3 controlÂ  0.9132671\n#&gt; 4 controlÂ  1.7933881\n#&gt; 5 controlÂ  0.9966051\n#&gt; 6 controlÂ  1.1074905\nFit model\nmodel &lt;- aov(value ~ Group, data = data)\nsummary(model)\n#&gt; Â  Â  Â  Â  Â  Â  DfÂ  Â  Sum SqÂ   Mean SqÂ  F valueÂ  Pr(&gt;F)Â \n#&gt; GroupÂ  Â  Â  Â  2Â  Â   4.407Â  Â  2.2036Â  Â   3.71Â  0.0377 *\n#&gt; ResidualsÂ   27Â  Â  16.035Â  Â  0.5939\n\n# or\nlm_mod &lt;- lm(value ~ Group, data = data)\nanova(lm_mod)\n\nP-Value &lt; 0.05 says at least 1 group category has a statistically significant different mean from another category\n\nDunnettâ€™s Test\nDescTools::DunnettTest(x=data$value, g=data$Group)\n\n#&gt; Dunnett's test for comparing several treatments with a control :Â \n#&gt; Â  Â  95% family-wise confidence level\n#&gt; $control\n#&gt; Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  diffÂ  Â  lwr.ciÂ  Â  Â  upr.ciÂ  pvalÂ  Â \n#&gt; Test1-control -0.8742469 -1.678514 -0.06998022 0.0320 *Â \n#&gt; Test2-control -0.7335283 -1.537795Â  0.07073836 0.0768 .\n\nMeasures if there is any difference between treatments and the control\nThe mean score of the test1 group was significantly higher than the control group. The mean score of the test2 group was not significantly higher than the control group.\n\nTukeyâ€™s HSD\nstats::TukeyHSD(model, conf.level=.95)\n\nMeasures difference in means between all categories and each other"
  },
  {
    "objectID": "qmd/post-hoc-analysis-anova.html#sec-phoc-anova-ancova",
    "href": "qmd/post-hoc-analysis-anova.html#sec-phoc-anova-ancova",
    "title": "ANOVA",
    "section": "ANCOVA",
    "text": "ANCOVA\n\nAnalysis of Covariance is used to measure the main effect and interaction effects of categorical variables on a continuous dependent variable while controlling the effects of selected other continuous variables which co-vary with the dependent variable.\nMisc\n\nAnalysis of covariance is classical terminology from linear models but we often use the term also for nonlinear models (Harrell)\nSee also\n\nHarrell - Biostatistics for Biomedical Research Ch. 13\n\n\nAssumptions\n\nIndependent observations (i.e.Â random assignment, avoid is having known relationships among participants in the study)\nLinearity: the relation between the covariate(s) and the dependent variable must be linear.\nNormality: the dependent variable must be normally distributed within each subpopulation. (only needed for small samples of n &lt; 20 or so)\nHomogeneity of regression slopes: the beta-coefficient(s) for the covariate(s) must be equal among all subpopulations. (regression lines for these individual groups are assumed to be parallel)\n\nFailure to meet this assumption implies that there is an interaction between the covariate and the treatment.\nThis assumption can be checked with an F test on the interaction of the independent variable(s) with the covariate(s).\n\nIf the F test is significant (i.e., significant interaction) then this assumption has been violated and the covariate should not be used as is.\nA possible solution is converting the continuous scale of the covariate to a categorical (discrete) variable and making it a subsequent independent variable, and then use a factorial ANOVA to analyze the data.\n\n\nThe covariate (adjustment variable) and the treatment are independent\nmodel &lt;- aov(grade ~ technique, data = data)\nsummary(model)\n\n#&gt; Â  Â  Â  Â  Â  Â  Df Sum Sq Mean Sq F value Pr(&gt;F)\n#&gt; techniqueÂ  Â  2Â  Â  9.8Â  Â  4.92Â  Â  0.14Â  0.869\n#&gt; ResidualsÂ  87 3047.7Â  35.03\n\nH0: variables are independent\n\n\nHomogeneity of variance: variance of the dependent variable must be equal over all subpopulations (only needed for sharply unequal sample sizes)\n# response ~ treatment\nleveneTest(exam ~ technique, data = data)\n\n#&gt; Â  Â  Â  Df F valueÂ  Â  Pr(&gt;F)Â  Â \n#&gt; groupÂ  2Â  13.752 6.464e-06 ***\n#&gt; Â  Â  Â  87\n\n# alt test\nfligner.test(size ~ location, my.dataframe)\n\nH0: Homogeneous variance\nThis one fails\n\nFit\nancova_model &lt;- aov(exam ~ technique + grade, data = data)\ncar::Anova(ancova_model, type=\"III\")\n\n#&gt; Â  Â  Â  Â  Â  Â  Â  Â  Sum Sq Df F valueÂ  Â  Pr(&gt;F)Â  Â \n#&gt; Â  Â  (Intercept) 3492.4Â  1 57.1325 4.096e-11 ***\n#&gt; Â  Â  techniqueÂ  1085.8Â  2Â  8.8814 0.0003116 ***\n#&gt; Â  Â  gradeÂ  Â  Â  Â  Â  4.0Â  1Â  0.0657 0.7982685Â  Â \n#&gt; Â  Â  ResidualsÂ  5257.0 86\n\nWhen adjusting for current grade (covariate), study technique (treatment) has a significant effect on the final exam score (response).\n\nDoes the effect differ by treatment\npostHocs &lt;- multicomp::glht(ancova_model, linfct = mcp(technique = \"Tukey\"))\nsummary(postHocs)\n\n#&gt; Â  Â  Â  Â  Â  Â  Estimate Std. Error t value Pr(&gt;|t|)Â  Â \n#&gt; B - A == 0Â   -5.279Â  Â  Â  2.021Â  -2.613Â  0.0284 *Â \n#&gt; C - A == 0Â  Â  3.138Â  Â  Â  2.022Â   1.552Â  0.2719Â  Â \n#&gt; C - B == 0Â  Â  8.418Â  Â  Â  2.019Â   4.170Â  &lt;0.001 ***\n\nAlso see Post-Hoc Analysis, Multilevel &gt;&gt; Tukeyâ€™s Test\n\\(A\\), \\(B\\), and \\(C\\) are the study techniques (treatment)\nSignificant differences between \\(B\\) and \\(A\\) and a pretty large difference between \\(B\\) and \\(C\\).\n\nExample: RCT\n\\[\n\\begin{align}\n\\text{post}_i &\\sim \\mathcal{N}(\\mu_i, \\sigma_\\epsilon)\\\\\n\\mu_i &= \\beta_0 + \\beta_1 \\text{tx}_i + \\beta_2 \\text{pre}_i\n\\end{align}\n\\]\nw2 &lt;- glm(\nÂ  data = dw,\nÂ  family = gaussian,\nÂ  post ~ 1 + tx + pre)\n\nSpecification\n\npost, pre: The post-treatment and pre-treatment measurement of the outcome variable\ntx: The treatment indicator variable\n\\(\\beta_0\\): Population mean for the outcome variable in the control group\n\\(\\beta_1\\): Parameter is the population level difference in pre/post change in the treatment group, compared to the control group.\n\nAlso a causal estimate for the average treatment effect (ATE) in the population, Ï„\n\nBecause pre is added as a covariate, both \\(\\beta_0\\) and \\(\\beta_1\\) are conditional on the outcome variable, as collected at baseline before random assignment."
  },
  {
    "objectID": "qmd/post-hoc-analysis-emmeans.html#sec-phoc-emmeans-eqwtmm",
    "href": "qmd/post-hoc-analysis-emmeans.html#sec-phoc-emmeans-eqwtmm",
    "title": "55Â  emmeans",
    "section": "55.1 Equally weighted marginal means",
    "text": "55.1 Equally weighted marginal means\n\nemmeans - estimated outcome means (i.e.Â predictions) by category based on if you had balanced categorical features\nemtrends - estimated slopes\nExample (basic): cont &lt;- lm(loss~hours,data=dat)\n\nPrediction for when hours = 2\n\n\nemmeans(cont, ~ hours, at = list(hours = 2)\nhours emmeanÂ  Â  SEÂ  df lower.CL upper.CL\nÂ  Â  2Â  Â  10 0.469 898Â  Â  Â  9.1Â  Â  10.9\nConfidence level used: 0.95\n\nPrediction = 10 (rounded)\n\nFor interaction models (e.g.Â loss ~ hours*effort)\n\n\n(mylist &lt;- list(hours=2,effort=30))\nemmeans(contcont, ~ hours*effort, at=mylist)\nhours effort emmeanÂ  Â  SEÂ  df lower.CL upper.CL\nÂ  Â  2Â  Â  30Â  10.2 0.453 896Â  Â  9.35Â  Â  11.1\nConfidence level used: 0.95\n\nNotice â€œ~hours*effortâ€ is used for the formula which includes the main effects for lm models\nSlope of hours variable\n\nemtrends(cont, ~ 1, var=\"hours\")\n1Â  Â  Â  hours.trendÂ  Â  SEÂ  df lower.CL upper.CL\noverallÂ  Â  Â  Â  2.47 0.948 898Â  Â  0.609Â  Â  4.33\nConfidence level used: 0.95\n\nThe ~ 1 tells the function to obtain the overall slope and var=â€œhoursâ€ tells the function that we want the slope for the variable Hours.\nPlot the slope\n\nmylist &lt;- list(hours=seq(0,4,by=0.4))\nemmip(cont,~hours,at=mylist, CIs=TRUE)\n\nâ€œmylistâ€ is vector of reasonable values for the â€œhoursâ€ explanatory variable\n\nTOC\n\nMisc\nExplainer\nInteractions\nLogistic Regression"
  },
  {
    "objectID": "qmd/post-hoc-analysis-emmeans.html#sec-phoc-emmeans-misc",
    "href": "qmd/post-hoc-analysis-emmeans.html#sec-phoc-emmeans-misc",
    "title": "55Â  emmeans",
    "section": "55.2 Misc",
    "text": "55.2 Misc\n\nCIs for emmeans are wider than that of emmeans calculated by {ggeffects}. (no change when I tried different lmer.df args)\n\nCanâ€™t really figure out why. There is a lmer.df arg that should affect the std.errors and CIs for lmer models.\n\ndefault = kenward-roger; Others are asymptotic and satterthwaite (used by lmerTest pkg)\n\nkenward-roger and satterthwaite essentially the same except kr makes an additional bias adjust to the vcov matrix. Guessing that makes krâ€™s CIs wider.\nasymptotic sets dof to INF which switches from t-stats to z-stats."
  },
  {
    "objectID": "qmd/post-hoc-analysis-emmeans.html#sec-phoc-emmeans-expl",
    "href": "qmd/post-hoc-analysis-emmeans.html#sec-phoc-emmeans-expl",
    "title": "55Â  emmeans",
    "section": "55.3 Explainer",
    "text": "55.3 Explainer\nemmeans(Oats.lmer, \"nitro\", lmer.df = \"satterthwaite\")\n\nExample (Explainer)\n\nunbalanced experiment where pigs are given different percentages of protein (percent) from different sources (source) in their diet, and later we measure the concentration (conc) of leucine.\n\n\npigs %&gt;%Â \nfilter(percent == 18) %&gt;%\nsummarize(avg_conc = mean(log(conc)))\n\n# avg_conc\n#Â  3.65515\n\nThis is the mean concentration leucine for all instances of 18 percent protein\nThe problem with this estimate of the mean is that the experiment (i.e.Â groups) are unbalanced and the cell counts affect the mean calculation.\n\nThe aggregated conc values per category are actually weighted by the cell counts\n\n\nwith(pigs, table(source, percent))\n##Â  Â  Â  percent\n## source 9 12 15 18\n##Â  fish 2Â  3Â  2Â  3\n##Â  soyÂ  3Â  3Â  3Â  1\n##Â  skim 3Â  3Â  2Â  1\n\nWhether we want the unbalanced or balanced version of the mean depends on the situation\nExample\n\ndata: sales of a product given different packaging and features\n\nThe data is observed, so itâ€™s most likely unbalanced.\n\nInferential question: which categories of packaging and features are inherently more profitable?\n\nWeâ€™d like the categories to be balanced to get an unbiased analysis of how each category or combination of categories affects sales. Therefore, equally weighted EMMs may be appropriate.\n\nPrediction question: how much variation between packaging categories and sales? More variation, more predictive.\n\nMaybe thereâ€™s a reason for the imbalance and having the imbalance gives us a representative sample of the true population.\nThe (unequally) weighted EMMs will provide better estimates of what we can expect in the marketplace\n\n\nemmeans gives an equally weighted estimation of the mean\n\npigs_fit &lt;- lm(log(conc) ~ factor(percent) + source, data = pigs)\nemmeans(pigs_fit, \"percent\")\n\n## percent emmeanÂ  Â  SEÂ  dfÂ  lower.CL upper.CL\n##Â  Â  9Â  3.45Â  Â  0.0409 23Â  Â  3.36Â  Â  3.53\n##Â  Â  12Â  3.62Â  Â  0.0384 23Â  Â  3.55Â  Â  3.70\n##Â  Â  15Â  3.66Â  Â  0.0437 23Â  Â  3.57Â  Â  3.75\n##Â  Â  18Â  3.75Â  Â  0.0530 23Â  Â  3.64Â  Â  3.85\n##\n## Results are averaged over the levels of: sourceÂ \n## Results are given on the log (not the response) scale.Â \n## Confidence level used: 0.95\n\nUnequally weighted mean = 3.66\nEqually weighted mean = 3.75\nemmeans are also dependent on the specification of the model\n\npigs_fit &lt;- lm(log(conc) ~ factor(percent) + source, data = pigs)\nemmeans(pigs_fit, \"source\")\n\nsourceÂ  emmeanÂ  Â  SEÂ  dfÂ  lower.CL upper.CL\nfishÂ  Â  3.39Â  0.0367 23Â  Â  3.32Â  Â  3.47\nsoyÂ  Â  Â  3.67Â  0.0374 23Â  Â  3.59Â  Â  3.74\nskimÂ  Â  3.80Â  0.0394 23Â  Â  3.72Â  Â  3.88\n\npigs_fit2 &lt;- lm(log(conc) ~ percent + source, data = pigs)\nemmeans(pigs_fit2, \"source\")\n\nsourceÂ  emmeanÂ  Â  SEÂ  dfÂ  lower.CL upper.CL\nfishÂ  Â  3.38Â  0.0373 25Â  Â  3.30Â  Â  3.46\nsoyÂ  Â  Â  3.65Â  0.0369 25Â  Â  3.58Â  Â  3.73\nskimÂ  Â  3.78Â  0.0390 25Â  Â  3.70Â  Â  3.86"
  },
  {
    "objectID": "qmd/post-hoc-analysis-emmeans.html#sec-phoc-emmeans-inter",
    "href": "qmd/post-hoc-analysis-emmeans.html#sec-phoc-emmeans-inter",
    "title": "55Â  emmeans",
    "section": "55.4 Interactions",
    "text": "55.4 Interactions\n\nNotes from Getting started with emmeans\nMisc\n\nWith interaction terms, main effects arenâ€™t usually whatâ€™s interpreted\n\nYou can get main effects response mean estimates from a model with interactions but the standard errors/CIs will be different than a model w/o an interaction\n\n\nExample: Categorical:Categorical\n\nÂ  str(dat)\nÂ  # 'data.frame': 20 obs. ofÂ  3 variables:\nÂ  # $ f1Â  : Factor w/ 2 levels \"a\",\"c\": 1 1 1 1 1 1 1 1 1 1 ...\nÂ  # $ f2Â  : Factor w/ 2 levels \"m\",\"n\": 1 2 1 2 1 2 1 2 1 2 ...\nÂ  # $ resp: numÂ  1.6 0.3 3 0.1 3.2 0.2 0.4 0.4 2.8 0.7 ...\n\nfit1 = lm(log(resp) ~ f1 + f2 + f1:f2, data = dat)\n\nNote that since the log transformation is specified in the model formula and not in a preprocessing step, emmeans can detect it.\nPairwise Contrasts\n\nEach category-level interaction response mean estimate is differenced with each of the other category-level interaction estimates\n\n\nÂ  Â  Â  Â  emm1 &lt;- emmeans(fit1, specs = pairwise ~ f1:f2)\nÂ  Â  Â  Â  emm1$contrasts\nÂ  Â  Â  Â  # contrastÂ  estimateÂ  Â  SE df t.ratio p.value\nÂ  Â  Â  Â  # a m - c mÂ  Â  0.671 0.629 16Â  1.065Â  0.7146Â \nÂ  Â  Â  Â  # a m - a nÂ  Â  1.847 0.629 16Â  2.934Â  0.0434Â \nÂ  Â  Â  Â  # a m - c nÂ  -0.766 0.629 16 -1.217Â  0.6253Â \nÂ  Â  Â  Â  # c m - a nÂ  Â  1.176 0.629 16Â  1.869Â  0.2795Â \nÂ  Â  Â  Â  # c m - c nÂ  -1.437 0.629 16 -2.283Â  0.1438Â \nÂ  Â  Â  Â  # a n - c nÂ  -2.613 0.629 16 -4.152Â  0.0038\nÂ  Â  Â  Â  # Results are given on the log (not the response) scale.Â \nÂ  Â  Â  Â  # P value adjustment: tukey method for comparing a family of 4 estimates\n\nFor response mean estimates on the the response scale (e.g.Â backtransforming log(resp) in the model)\n\nÂ  Â  Â  Â  emm1.1 &lt;- emmeans(fit1, specs = pairwise ~ f1:f2, type = \"response\")\nÂ  Â  Â  Â  emm1.1$contrasts\nÂ  Â  Â  Â  # $contrasts\nÂ  Â  Â  Â  #Â  contrastÂ  ratioÂ  Â  SE df t.ratio p.value\nÂ  Â  Â  Â  #Â  a m / c m 1.9553 1.2306 16Â  1.065Â  0.7146Â \nÂ  Â  Â  Â  #Â  a m / a n 6.3396 3.9900 16Â  2.934Â  0.0434Â \nÂ  Â  Â  Â  #Â  a m / c n 0.4648 0.2926 16 -1.217Â  0.6253Â \nÂ  Â  Â  Â  #Â  c m / a n 3.2422 2.0406 16Â  1.869Â  0.2795Â \nÂ  Â  Â  Â  #Â  c m / c n 0.2377 0.1496 16 -2.283Â  0.1438Â \nÂ  Â  Â  Â  #Â  a n / c n 0.0733 0.0461 16 -4.152Â  0.0038Â \nÂ  Â  Â  Â  # P value adjustment: tukey method for comparing a family of 4 estimatesÂ \nÂ  Â  Â  Â  # Tests are performed on the log scale\n\ncomparisons have changed from additive comparisons (via subtraction) to multiplicative comparisons (via division).\nWith Confidence Intervals\n\nÂ  Â  Â  Â  emm1.1 %&gt;%\nÂ  Â  Â  Â  Â  Â  confint()\nÂ  Â  Â  Â  #Â  contrastÂ  ratioÂ  Â  SE df lower.CL upper.CL\nÂ  Â  Â  Â  #Â  a m / c m 1.9553 1.2306 16Â  0.5150Â  Â  7.424\nÂ  Â  Â  Â  #Â  a m / a n 6.3396 3.9900 16Â  1.6696Â  24.072\nÂ  Â  Â  Â  #Â  a m / c n 0.4648 0.2926 16Â  0.1224Â  Â  1.765\nÂ  Â  Â  Â  #Â  c m / a n 3.2422 2.0406 16Â  0.8539Â  12.311\nÂ  Â  Â  Â  #Â  c m / c n 0.2377 0.1496 16Â  0.0626Â  Â  0.903\nÂ  Â  Â  Â  #Â  a n / c n 0.0733 0.0461 16Â  0.0193Â  Â  0.278\nÂ  Â  Â  Â  # Confidence level used: 0.95Â \nÂ  Â  Â  Â  # Intervals are back-transformed from the log scale\n\nÂ  Â  Â  Â  emm1.1$contrasts %&gt;%\nÂ  Â  Â  Â  Â  Â  summary(infer = TRUE)\nÂ  Â  Â  Â  #Â  contrastÂ  ratioÂ  Â  SE df lower.CL upper.CL t.ratio p.value\nÂ  Â  Â  Â  #Â  a m / c m 1.9553 1.2306 16Â  0.5150Â  Â  7.424Â  1.065Â  0.3025Â \nÂ  Â  Â  Â  #Â  a m / a n 6.3396 3.9900 16Â  1.6696Â  24.072Â  2.934Â  0.0097Â \nÂ  Â  Â  Â  #Â  a m / c n 0.4648 0.2926 16Â  0.1224Â  Â  1.765 -1.217Â  0.2412Â \nÂ  Â  Â  Â  #Â  c m / a n 3.2422 2.0406 16Â  0.8539Â  12.311Â  1.869Â  0.0801Â \nÂ  Â  Â  Â  #Â  c m / c n 0.2377 0.1496 16Â  0.0626Â  Â  0.903 -2.283Â  0.0365Â \nÂ  Â  Â  Â  #Â  a n / c n 0.0733 0.0461 16Â  0.0193Â  Â  0.278 -4.152Â  0.0008Â \nÂ  Â  Â  Â  # Confidence level used: 0.95Â \nÂ  Â  Â  Â  # Intervals are back-transformed from the log scaleÂ \nÂ  Â  Â  Â  # Tests are performed on the log scale\n\nWithin group comparisons\n\nThe code f1|f2 translates to â€œcompare levels of f1 within each level of f2\n\n\nÂ  Â  Â  Â  emm2 = emmeans(fit1, specs = pairwise ~ f1|f2, type = \"response\")\nÂ  Â  Â  Â  emm2\n\nÂ  Â  Â  Â  # $emmeans\nÂ  Â  Â  Â  # f2 = m:\nÂ  Â  Â  Â  #Â  f1 responseÂ  Â  SE df lower.CL upper.CL\nÂ  Â  Â  Â  #Â  aÂ  Â  1.767 0.786 16Â  Â  0.688Â  Â  4.538\nÂ  Â  Â  Â  #Â  cÂ  Â  0.903 0.402 16Â  Â  0.352Â  Â  2.321\nÂ  Â  Â  Â  # f2 = n:\nÂ  Â  Â  Â  #Â  f1 responseÂ  Â  SE df lower.CL upper.CL\nÂ  Â  Â  Â  #Â  aÂ  Â  0.279 0.124 16Â  Â  0.108Â  Â  0.716\nÂ  Â  Â  Â  #Â  cÂ  Â  3.800 1.691 16Â  Â  1.479Â  Â  9.763\nÂ  Â  Â  Â  # Confidence level used: 0.95Â \nÂ  Â  Â  Â  # Intervals are back-transformed from the log scaleÂ \n\nÂ  Â  Â  Â  # $contrasts\nÂ  Â  Â  Â  # f2 = m:\nÂ  Â  Â  Â  #Â  contrastÂ  ratioÂ  Â  SE df t.ratio p.value\nÂ  Â  Â  Â  #Â  a / cÂ  Â  1.9553 1.2306 16Â  1.065Â  0.3025Â \nÂ  Â  Â  Â  # f2 = n:\nÂ  Â  Â  Â  #Â  contrastÂ  ratioÂ  Â  SE df t.ratio p.value\nÂ  Â  Â  Â  #Â  a / cÂ  Â  0.0733 0.0461 16 -4.152Â  0.0008Â \nÂ  Â  Â  Â  # Tests are performed on the log scale\n\nThis is just a subset of the complete pairwise contrasts\n\ne.g.Â the f2 = m section is the same as am-cm"
  },
  {
    "objectID": "qmd/post-hoc-analysis-emmeans.html#sec-phoc-emmeans-logreg",
    "href": "qmd/post-hoc-analysis-emmeans.html#sec-phoc-emmeans-logreg",
    "title": "55Â  emmeans",
    "section": "55.5 Logistic Regression",
    "text": "55.5 Logistic Regression\n\nExample\n\nstr(neuralgia)\n'data.frame': 60 obs. ofÂ  5 variables:\n$ Treatment: Factor w/ 3 levels \"A\",\"B\",\"P\": 3 2 3 3 2 2 2 1 2 1 ...\n$ SexÂ  Â  Â  : Factor w/ 2 levels \"F\",\"M\": 1 2 1 2 2 1 1 1 1 2 ...\n$ AgeÂ  Â  Â  : intÂ  68 74 67 66 70 67 77 71 72 65 ...\n$ Duration : intÂ  1 16 30 26 22 28 16 12 50 15 ...\n$ PainÂ  Â  : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 2 1 1 1 1 1 1 ...\n# interaction\nneuralgia.glm &lt;- glm(Pain ~ Treatment * Sex + Age, family = binomial, data = neuralgia)\n# only main effects\nneuralgia.glm2 &lt;- glm(Pain ~ Treatment + Sex + Age, family = binomial(), data = neuralgia)\n\nTreatments A and B with Placebo, P.\nMain effects (no interaction)\n\nneuralgia.emm2 &lt;- emmeans(neuralgia.glm2, \"Treatment\", type = \"response\")\nneuralgia.emm2\n# TreatmentÂ  probÂ  Â  SEÂ  df asymp.LCL asymp.UCL\n#Â  AÂ  Â  Â  Â  0.198 0.1056 InfÂ  Â  0.0630Â  Â  0.476\n#Â  BÂ  Â  Â  Â  0.125 0.0814 InfÂ  Â  0.0322Â  Â  0.381\n#Â  PÂ  Â  Â  Â  0.856 0.0842 InfÂ  Â  0.6092Â  Â  0.958Â \n# Results are averaged over the levels of: SexÂ \n# Confidence level used: 0.95Â \n# Intervals are back-transformed from the logit scale\n\nPairwise Contrasts (also see Marginal Effects section in Classification)\n\nMain Effect Contrast (no interaction)\n\nmeans and contrasts on the probability scale with CIs\n\n\n\nemm2_prob &lt;- emmeans(neuralgia.glm2, \"Treatment\", type = \"response\") %&gt;%Â \nÂ  regrid() %&gt;%Â \nÂ  contrast(method = \"revpairwise\")\n# contrast estimateÂ  Â  SEÂ  df z.ratio p.value\n#Â  B - AÂ  Â  -0.0731 0.125 Inf -0.586Â  0.8276Â \n#Â  P - AÂ  Â  Â  0.6577 0.143 InfÂ  4.584Â  &lt;.0001Â \n#Â  P - BÂ  Â  Â  0.7308 0.132 InfÂ  5.554Â  &lt;.0001\n\nconfint(emm2_prob)\n# contrast estimateÂ  Â  SEÂ  df asymp.LCL asymp.UCL\n#Â  B - AÂ  Â  -0.0731 0.125 InfÂ  Â  -0.366Â  Â  0.219\n#Â  P - AÂ  Â  Â  0.6577 0.143 InfÂ  Â  0.321Â  Â  0.994\n#Â  P - BÂ  Â  Â  0.7308 0.132 InfÂ  Â  0.422Â  Â  1.039Â \n# Results are averaged over the levels of: SexÂ \n# Confidence level used: 0.95Â \n# Conf-level adjustment: tukey method for comparing a family of 3 estimates\n\nregrid back-transforms all the results to a new set of EMMs as if there had never been a transformation\nMain Effect Contrast (interaction)\n\nmeans on the probability scale but contrasts are odds ratios\n\n\nÂ  Â  neuralgia.emm2 &lt;- emmeans(neuralgia.glm, \"Treatment\", type = \"response\")\nÂ  Â  #Â  TreatmentÂ  probÂ  Â  SEÂ  df asymp.LCL asymp.UCLÂ \nÂ  Â  #Â  AÂ  Â  Â  Â  0.211 0.1109 InfÂ  Â  0.0675Â  Â  0.497Â \nÂ  Â  #Â  BÂ  Â  Â  Â  0.121 0.0835 InfÂ  Â  0.0285Â  Â  0.391Â \nÂ  Â  #Â  PÂ  Â  Â  Â  0.866 0.0883 InfÂ  Â  0.5927Â  Â  0.966Â \nÂ  Â  # Results are averaged over the levels of: SexÂ \nÂ  Â  # Confidence level used: 0.95Â \nÂ  Â  # Intervals are back-transformed from the logit scale\nÂ  Â  pairs(neuralgia.emm, reverse = TRUE)Â \nÂ  Â  #Â  contrast odds.ratioÂ  Â  SEÂ  df z.ratio p.valueÂ \nÂ  Â  #Â  B / AÂ  Â  Â  Â  0.513Â  0.515 Inf -0.665Â  0.7837Â \nÂ  Â  #Â  P / AÂ  Â  Â  Â  24.234 25.142 InfÂ  3.073Â  0.0060Â \nÂ  Â  #Â  P / BÂ  Â  Â  Â  47.213 57.242 InfÂ  3.179Â  0.0042Â  Â \nÂ  Â  # Results are averaged over the levels of: SexÂ \nÂ  Â  # P value adjustment: tukey method for comparing a family of 3 estimatesÂ \nÂ  Â  # Tests are performed on the log odds ratio scale\n\nInterpretation (odds ratios)\n\nSo there is evidence of considerably more pain being reported with placebo (treatment P) than with either of the other two treatments. The estimated odds of pain with B are about half that for A, but this finding is not statistically significant.\n\nThis model produces a warning after emmeans is executed\n\nif two factors interact, it may be the case that marginal averages of predictions (i.e.Â the â€œprobsâ€ in the main effects summary) donâ€™t reflect what is happening at any level of the factors being averaged over.\nTo find out, look at an interaction plot of the fitted model using emmip(neuralgia.glm, Sex ~ Treatment)\n\nThere is no practical difference between females and males in the patterns of response to Treatment, so the main effects mean response probability estimates are likely reasonable.\n\ne.g.Â if treatment B was lowest in Females but NOT lowest in Males, then using the main effects mean estimates would be misleading.\n\n\n\nInteraction: 2 factor variables\n\nmeans on the probability scale and contrasts are odds ratios\n\n\nÂ  Â  neuralgia.emm1 &lt;- emmeans(neuralgia.glm, pairwise ~ Treatment:Sex, type = \"response\")\nÂ  Â  neuralgia.emm1\nÂ  Â  # $emmeans\nÂ  Â  #Â  Treatment SexÂ  probÂ  Â  SEÂ  df asymp.LCL asymp.UCL\nÂ  Â  #Â  AÂ  Â  Â  Â  FÂ  0.1164 0.1124 InfÂ  Â  0.0152Â  Â  0.528\nÂ  Â  #Â  BÂ  Â  Â  Â  FÂ  0.0498 0.0581 InfÂ  Â  0.0047Â  Â  0.367\nÂ  Â  #Â  PÂ  Â  Â  Â  FÂ  0.6900 0.1564 InfÂ  Â  0.3469Â  Â  0.903\nÂ  Â  #Â  AÂ  Â  Â  Â  MÂ  0.3516 0.1727 InfÂ  Â  0.1094Â  Â  0.705\nÂ  Â  #Â  BÂ  Â  Â  Â  MÂ  0.2644 0.1605 InfÂ  Â  0.0665Â  Â  0.644\nÂ  Â  #Â  PÂ  Â  Â  Â  MÂ  0.9496 0.0604 InfÂ  Â  0.6134Â  Â  0.996Â \nÂ  Â  # Confidence level used: 0.95Â \nÂ  Â  # Intervals are back-transformed from the logit scaleÂ \nÂ  Â  # $contrasts\nÂ  Â  #Â  contrastÂ  odds.ratioÂ  Â  Â  SEÂ  df z.ratio p.value\nÂ  Â  #Â  A F / B FÂ  Â  2.51544 4.10261 InfÂ  0.566Â  0.9932Â \nÂ  Â  #Â  A F / P FÂ  Â  0.05920 0.07818 Inf -2.141Â  0.2664Â \nÂ  Â  #Â  A F / A MÂ  Â  0.24297 0.32216 Inf -1.067Â  0.8945Â \nÂ  Â  #Â  A F / B MÂ  Â  0.36662 0.49854 Inf -0.738Â  0.9772Â \nÂ  Â  #Â  A F / P MÂ  Â  0.00699 0.01176 Inf -2.951Â  0.0374Â \nÂ  Â  #Â  B F / P FÂ  Â  0.02353 0.03514 Inf -2.511Â  0.1208Â \nÂ  Â  #Â  B F / A MÂ  Â  0.09659 0.13584 Inf -1.662Â  0.5571Â \nÂ  Â  #Â  B F / B MÂ  Â  0.14575 0.20081 Inf -1.398Â  0.7285Â \nÂ  Â  #Â  B F / P MÂ  Â  0.00278 0.00525 Inf -3.113Â  0.0228Â \nÂ  Â  #Â  P F / A MÂ  Â  4.10430 4.40227 InfÂ  1.316Â  0.7760Â \nÂ  Â  #Â  P F / B MÂ  Â  6.19302 7.15364 InfÂ  1.579Â  0.6128Â \nÂ  Â  #Â  P F / P MÂ  Â  0.11806 0.16426 Inf -1.536Â  0.6411Â \nÂ  Â  #Â  A M / B MÂ  Â  1.50891 1.64607 InfÂ  0.377Â  0.9990Â \nÂ  Â  #Â  A M / P MÂ  Â  0.02876 0.04339 Inf -2.353Â  0.1733Â \nÂ  Â  #Â  B M / P MÂ  Â  0.01906 0.03059 Inf -2.468Â  0.1337Â \nÂ  Â  # P value adjustment: tukey method for comparing a family of 6 estimatesÂ \nÂ  Â  # Tests are performed on the log odds ratio scale\n\nInteraction: 1 continuous, 1 factor\n\nWe want estimates of slopes of the covariate trend for each level of the factor\nExample\n\n\nÂ  Â  Â  Â  Â  Â  fiber.lm &lt;- lm(strength ~ diameter*machine, data = fiber)\n\ndiameter is the continuous covariate and machine is a factor\nCompare trends for each factor level\n\nÂ  Â  Â  Â  Â  Â  emtrends(fiber.lm, pairwise ~ machine, var = \"diameter\")\nÂ  Â  Â  Â  Â  Â  ## $emtrends\nÂ  Â  Â  Â  Â  Â  ##Â  machine diameter.trendÂ  Â  SE df lower.CL upper.CL\nÂ  Â  Â  Â  Â  Â  ##Â  AÂ  Â  Â  Â  Â  Â  Â  Â  1.104 0.194Â  9Â  Â  0.666Â  Â  1.54\nÂ  Â  Â  Â  Â  Â  ##Â  BÂ  Â  Â  Â  Â  Â  Â  Â  0.857 0.224Â  9Â  Â  0.351Â  Â  1.36\nÂ  Â  Â  Â  Â  Â  ##Â  CÂ  Â  Â  Â  Â  Â  Â  Â  0.864 0.208Â  9Â  Â  0.394Â  Â  1.33\nÂ  Â  Â  Â  Â  Â  ## Confidence level used: 0.95Â \n\nÂ  Â  Â  Â  Â  Â  ## $contrasts\nÂ  Â  Â  Â  Â  Â  ##Â  contrast estimateÂ  Â  SE df t.ratio p.value\nÂ  Â  Â  Â  Â  Â  ##Â  A - BÂ  Â  0.24714 0.296Â  9Â  0.835Â  0.6919\nÂ  Â  Â  Â  Â  Â  ##Â  A - CÂ  Â  0.24008 0.284Â  9Â  0.845Â  0.6863\nÂ  Â  Â  Â  Â  Â  ##Â  B - CÂ  Â  -0.00705 0.306Â  9Â  -0.023Â  0.9997Â \nÂ  Â  Â  Â  Â  Â  ## P value adjustment: tukey method for comparing a family of 3 estimates\n\nContrasts: no significant difference in slopes between factor levels\nViz\n\nÂ  Â  Â  Â  Â  Â  emmip(fiber.lm, machine ~ diameter, cov.reduce = range)\n\nâ€œrangeâ€ required. Itâ€™s a function that gets the min, max values of diameter\nMachine â€œAâ€ slope looks different, but I guess itâ€™s not different enough."
  },
  {
    "objectID": "qmd/post-hoc-analysis-general.html#sec-phoc-gen-misc",
    "href": "qmd/post-hoc-analysis-general.html#sec-phoc-gen-misc",
    "title": "General",
    "section": "Misc",
    "text": "Misc\n\nAlso see Mathematics, Statistics &gt;&gt; Descriptive Statistics&gt;&gt; Understanding CI, SD, and SEM Bars\nNotes from\n\nhttps://www.andrewheiss.com/blog/2019/01/29/diff-means-half-dozen-ways/\n\nPackages\n\n{rstatix}\n{dabestr} for visualization\n\nVisualization for differences (Thread)"
  },
  {
    "objectID": "qmd/post-hoc-analysis-general.html#sec-phoc-gen-eda",
    "href": "qmd/post-hoc-analysis-general.html#sec-phoc-gen-eda",
    "title": "General",
    "section": "EDA",
    "text": "EDA\n\nMisc\n\nIn code examples, movies_clean is the data; rating is the numeric; and genre (action vs comedy) is the group variable\n\n\n\nCharts\n\nBox, Histogram, Density for two groups (factor(genre))\npacman::p_load(ggplot2movies,ggplot2, ggridges, patchwork)\n\n# Make a custom theme\n# I'm using Asap Condensed; download fromÂ \n# https://fonts.google.com/specimen/Asap+Condensed\ntheme_fancy &lt;- function() {\nÂ  theme_minimal(base_family = \"Asap Condensed\") +\nÂ  Â  theme(panel.grid.minor = element_blank())\n}\neda_boxplot &lt;- ggplot(movies_clean, aes(x = genre, y = rating, fill = genre)) +\nÂ  geom_boxplot() +\nÂ  scale_fill_manual(values = c(\"#0288b7\", \"#a90010\"), guide = FALSE) +Â \nÂ  scale_y_continuous(breaks = seq(1, 10, 1)) +\nÂ  labs(x = NULL, y = \"Rating\") +\nÂ  theme_fancy()\neda_histogram &lt;- ggplot(movies_clean, aes(x = rating, fill = genre)) +\nÂ  geom_histogram(binwidth = 1, color = \"white\") +\nÂ  scale_fill_manual(values = c(\"#0288b7\", \"#a90010\"), guide = FALSE) +Â \nÂ  scale_x_continuous(breaks = seq(1, 10, 1)) +\nÂ  labs(y = \"Count\", x = \"Rating\") +\nÂ  facet_wrap(~ genre, nrow = 2) +\nÂ  theme_fancy() +\nÂ  theme(panel.grid.major.x = element_blank())\neda_ridges &lt;- ggplot(movies_clean, aes(x = rating, y = fct_rev(genre), fill = genre)) +\nÂ  stat_density_ridges(quantile_lines = TRUE, quantiles = 2, scale = 3, color = \"white\") +Â \nÂ  scale_fill_manual(values = c(\"#0288b7\", \"#a90010\"), guide = FALSE) +Â \nÂ  scale_x_continuous(breaks = seq(0, 10, 2)) +\nÂ  labs(x = \"Rating\", y = NULL,\nÂ  Â  Â  subtitle = \"White line shows median rating\") +\nÂ  theme_fancy()\n(eda_boxplot | eda_histogram) /Â \nÂ  Â  eda_ridges +Â \nÂ  plot_annotation(title = \"Do comedies get higher ratings than action movies?\",\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  subtitle = \"Sample of 400 movies from IMDB\",\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  theme = theme(text = element_text(family = \"Asap Condensed\"),\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  plot.title = element_text(face = \"bold\",\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  size = rel(1.5))))\n\n\n\nTest for Equal Variances\n\nMisc\n\nâ€œGlass and Hopkins (1996 p.Â 436) state that the Levene and B-F tests areâ€fatally flawedâ€; It isnâ€™t clear how robust they are when there is significant differences in variances and unequal sample sizes. â€\n\nBartlett test: Check homogeneity of variances based on the mean\nbartlett.test(rating ~ genre, data = movies_clean)\nLevene test: Check homogeneity of variances based on the median, so itâ€™s more robust to outliers\ncar::leveneTest(rating ~ genre, data = movies_clean)\n\nAlso {DescTools}\nOther tests are better\n\nFligner-Killeen test: Check homogeneity of variances based on the median, so itâ€™s more robust to outliers\nfligner.test(rating ~ genre, data = movies_clean)\nBrown-Forsythe (B-F) Test (link)\n\nAttempts to correct for the skewness of the Levene Test transformation by using deviations from group medians.\n\nLess likely than the Levene test to incorrectly declare that the assumption of equal variances has been violated.\n\nThought to perform as well as or better than other available tests for equal variances\n\nonewaytests::bf.test(weight_loss ~ program, data = data)\n\np-value &lt; 0.05 means the difference in variances is statistically significant"
  },
  {
    "objectID": "qmd/post-hoc-analysis-general.html#sec-phoc-gen-fdim",
    "href": "qmd/post-hoc-analysis-general.html#sec-phoc-gen-fdim",
    "title": "General",
    "section": "Frequentist Difference-in-Means",
    "text": "Frequentist Difference-in-Means\n\nT-test\n\nTest whether the difference between means is statistically different from 0\nDefault is for non-equal variances\nt_test_eq &lt;- t.test(rating ~ genre, data = movies_clean, var.equal = TRUE)\nt_test_eq_tidy &lt;- tidy(t_test_eq) %&gt;%\nÂ  # Calculate difference in means, since t.test() doesn't actually do that\nÂ  mutate(estimate_difference = estimate1 - estimate2) %&gt;%\nÂ  # Rearrange columns\nÂ  select(starts_with(\"estimate\"), everything())\nFor unequal variances, Welchâ€™s T-Test:\n\nvar.equal = FALSE\nRecommended for large datasets\n\n\n\n\nHotellingâ€™s T2\n\nMultivariate generalization of Welchâ€™s T-Test\n{DescTools::HotellingsT2Test}\n{ICSNP}\n\n\n\nNon-Parametric Difference-in-Means Tests\n\nWilcoxon Rank Sum and Signed Rank: wilcox.test\n\n1 or 2 variables/â€œsamplesâ€\n2 variable aka â€œMann-Whitneyâ€\ncoin::wilcox_test\n\nfor exact, asymptotic and Monte Carlo conditional p-values, including in the presence of ties\n\n\nKruskal-Wallis: kruskal.test(rating ~ genre, data = movies_clean)\n\nMore than 2 variables/â€œsamplesâ€\n\n\n\n\nKolomogorov-Smirnov\n\nks.test\nCalculates the difference in cdf of each sample\n2 variables/â€œsamplesâ€\nFor mixed or discrete, see {KSgeneral}\n\n\n\nBootstrap\n\nAlso see Statistical Concepts &gt;&gt; Bootstrapping\nSteps\n\nCalculate a sample statistic, orÂ Î´:Â This is the main measure you care about: the difference in means, the average, the median, the proportion, the difference in proportions, the chi-squared value, etc.\nUse simulation to invent a world whereÂ Î´Â is null:Â Simulate what the world would look like if there was no difference between two groups, or if there was no difference in proportions, or where the average value is a specific number.\nLook atÂ Î´Â in the null world: Put the sample statistic in the null world and see if it fits well.\nCalculate the probability thatÂ Î´Â could exist in null world: This is the p-value, or the probability that youâ€™d see aÂ Î´Â at least that high in a world where thereâ€™s no difference.\nDecide ifÂ Î´Â is statistically significant: Choose some evidentiary standard or threshold (like 0.05) for deciding if thereâ€™s sufficient proof for rejecting the null world.\n\nStandard Method\nlibrary(infer)\n\n# Calculate the difference in means\ndiff_means &lt;- movies_clean %&gt;%Â \nÂ  specify(rating ~ genre) %&gt;%\nÂ  # Order here means we subtract comedy from action (Action - Comedy)\nÂ  calculate(\"diff in means\", order = c(\"Action\", \"Comedy\"))\n\nboot_means &lt;- movies_clean %&gt;%Â \nÂ  specify(rating ~ genre) %&gt;%Â \nÂ  generate(reps = 1000, type = \"bootstrap\") %&gt;%Â \nÂ  calculate(\"diff in means\", order = c(\"Action\", \"Comedy\"))\n\nboostrapped_confint &lt;- boot_means %&gt;% get_confidence_interval()\n\nboot_means %&gt;%Â \nÂ  visualize() +Â \nÂ  shade_confidence_interval(boostrapped_confint,\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  color = \"#8bc5ed\", fill = \"#85d9d2\") +\nÂ  geom_vline(xintercept = diff_means$stat, size = 1, color = \"#77002c\") +\nÂ  labs(title = \"Bootstrapped distribution of differences in means\",\nÂ  Â  Â  x = \"Action âˆ’ Comedy\", y = \"Count\",\nÂ  Â  Â  subtitle = \"Red line shows observed difference; shaded area shows 95% confidence interval\") +\nÂ  theme_fancy()\nDowneyâ€™s Process: Generate a world where thereâ€™s no difference by shuffling all the action/comedy labels through permutation\n# Step 1: Î´ = diff_means (see above)\n\n# Step 2: Invent a world where Î´ is null\ngenre_diffs_null &lt;- movies_clean %&gt;%Â \nÂ  specify(rating ~ genre) %&gt;%Â \nÂ  hypothesize(null = \"independence\") %&gt;%Â \nÂ  generate(reps = 5000, type = \"permute\") %&gt;%Â \nÂ  calculate(\"diff in means\", order = c(\"Action\", \"Comedy\"))\n\n# Step 3: Put actual observed Î´ in the null world and see if it fits\ngenre_diffs_null %&gt;%Â \nÂ  visualize() +Â \nÂ  geom_vline(xintercept = diff_means$stat, size = 1, color = \"#77002c\") +\nÂ  scale_y_continuous(labels = comma) +\nÂ  labs(x = \"Simulated difference in average ratings (Action âˆ’ Comedy)\", y = \"Count\",\nÂ  Â  Â  title = \"Simulation-based null distribution of differences in means\",\nÂ  Â  Â  subtitle = \"Red line shows observed difference\") +\nÂ  theme_fancy()\n\nIf line is outside null distribution, then the difference value doesnâ€™t fit in a world where the null hypothesis is the truth\n\nGenerate a p-value\n# Step 4: Calculate probability that observed Î´ could exist in null world\ngenre_diffs_null %&gt;%Â \nÂ  get_p_value(obs_stat = diff_means, direction = \"both\") %&gt;%Â \nÂ  mutate(p_value_clean = pvalue(p_value))"
  },
  {
    "objectID": "qmd/post-hoc-analysis-general.html#sec-phoc-gen-bdim",
    "href": "qmd/post-hoc-analysis-general.html#sec-phoc-gen-bdim",
    "title": "General",
    "section": "Bayesian Difference-in-Means",
    "text": "Bayesian Difference-in-Means\n\nMisc\n\nNotes from\n\nHalf a dozen frequentist and Bayesian ways to measure the difference in means in two groups | Andrew Heiss\n\nFrequentist null hypothesis significance testing (NHST) determines the probability of the data given a null hypothesis (i.e.Â \\(P(data|H)\\), yielding results that are often unwieldy, phrased as the probability of rejecting the null if it is true (hence all that talk of â€œnull worldsâ€). In contrast, Bayesian analysis determines the probability of a hypothesis given the data (i.e.P(H|data)), resulting in probabilities that are directly interpretable.\n\n\n\nRegression (Equal Variances)\n\nWith {brms}\nbrms_eq &lt;- brm(\nÂ  # bf() is an alias for brmsformula() and lets you specify model formulas\nÂ  bf(rating ~ genre),Â \nÂ  # Reverse the levels of genre so that comedy is the base case\nÂ  data = mutate(movies_clean, genre = fct_rev(genre)),\nÂ  prior = c(set_prior(\"normal(0, 5)\", class = \"Intercept\"),\nÂ  Â  Â  Â  Â  Â  set_prior(\"normal(0, 1)\", class = \"b\")),\nÂ  chains = 4, iter = 2000, warmup = 1000, seed = 1234,\nÂ  file = \"cache/brms_eq\"\n)\n# median of posterior and CIs\nbrms_eq_tidy &lt;-Â \nÂ  broom::tidyMCMC(brms_eq, conf.int = TRUE, conf.level = 0.95,Â \nÂ  Â  Â  Â  Â  estimate.method = \"median\", conf.method = \"HPDinterval\")\n\nfamily = gaussian (default)\nb_intercept: mean comedy score while the\nb_genreAction: difference from mean comedy score (i.e.Â difference in means)\n\nsSays â€œWeâ€™re 95% confident that the true population-level difference in rating is between -0.968 and -0.374, with a median of -0.666.â€\n\n\n\n\n\nRegression (unequal variances)\n\nWith {brms}\nbrms_uneq &lt;- brm(\nÂ  bf(rating ~ genre, sigma ~ genre),Â \nÂ  data = mutate(movies_clean, genre = fct_rev(genre)),\nÂ  prior = c(set_prior(\"normal(0, 5)\", class = \"Intercept\"),\nÂ  Â  Â  Â  Â  Â  set_prior(\"normal(0, 1)\", class = \"b\"),\nÂ  Â  Â  Â  Â  Â  # models the variance for each group (e.g. comedy and action)\nÂ  Â  Â  Â  Â  Â  set_prior(\"cauchy(0, 1)\", class = \"b\", dpar = \"sigma\")),\nÂ  chains = CHAINS, iter = ITER, warmup = WARMUP, seed = BAYES_SEED,\nÂ  file = \"cache/brms_uneq\"\n)\n\n# median of posterior and CIs\nbrms_uneq_tidy &lt;-Â \nÂ  tidyMCMC(brms_uneq, conf.int = TRUE, conf.level = 0.95,Â \nÂ  Â  Â  Â  Â  estimate.method = \"median\", conf.method = \"HPDinterval\") %&gt;%Â \nÂ  # sigma terms on log-scale so exponentiate them to get them back to original scale\nÂ  mutate_at(vars(estimate, std.error, conf.low, conf.high),\nÂ  Â  Â  Â  Â  Â  funs(ifelse(str_detect(term, \"sigma\"), exp(.), .)))\n\nInterpretation for intercept and main effect estimates same as before\nb_sigma_intercept and b_sigma_genreAction are the std.devs for those posteriors\n\n\n\n\nBayesian Estimation Supersedes the T-test (BEST)\n\nUnequal Variances, student-t distribution\nSame as before but with a coefficient for Î½, the degrees of freedom, for the student-t distribution.\nModels each group distribution (by removing intercept w/ 0 + formula notation), then calculates difference in means by hand\nbrms_uneq_robust_groups &lt;- brm(\nÂ  bf(rating ~ 0 + genre,\nÂ  Â  sigma ~ 0 + genre),Â \nÂ  family = student,\nÂ  data = mutate(movies_clean, genre = fct_rev(genre)),\nÂ  prior = c(\nÂ  Â  # Set group mean prior\nÂ  Â  set_prior(\"normal(6, 2)\", class = \"b\", lb = 1, ub = 10),\nÂ  Â  # Ser group variance priors. We keep the less informative cauchy(0, 1).\nÂ  Â  set_prior(\"cauchy(0, 1)\", class = \"b\", dpar = \"sigma\"),\nÂ  Â  set_prior(\"exponential(1.0/29)\", class = \"nu\")),\nÂ  chains = CHAINS, iter = ITER, warmup = WARMUP, seed = BAYES_SEED,\nÂ  file = \"cache/brms_uneq_robust_groups\"\n)\n\nbrms_uneq_robust_groups_tidy &lt;-Â \nÂ  tidyMCMC(brms_uneq_robust_groups, conf.int = TRUE, conf.level = 0.95,Â \nÂ  Â  Â  Â  Â  estimate.method = \"median\", conf.method = \"HPDinterval\") %&gt;%Â \nÂ  # Rescale sigmas\nÂ  mutate_at(vars(estimate, std.error, conf.low, conf.high),\nÂ  Â  Â  Â  Â  Â  funs(ifelse(str_detect(term, \"sigma\"), exp(.), .))\n\nbrms_uneq_robust_groups_post &lt;- posterior_samples(brms_uneq_robust_groups) %&gt;%Â \nÂ  # We can exponentiate here!\nÂ  mutate_at(vars(contains(\"sigma\")), funs(exp)) %&gt;%Â \nÂ  # For whatever reason, we need to log nu?\nÂ  mutate(nu = log10(nu)) %&gt;%Â \nÂ  mutate(diff_means = b_genreAction - b_genreComedy,\nÂ  Â  Â  Â  diff_sigma = b_sigma_genreAction - b_sigma_genreComedy) %&gt;%Â \nÂ  # Calculate effect sizes, just for fun\nÂ  mutate(cohen_d = diff_means / sqrt((b_sigma_genreAction + b_sigma_genreComedy)/2),\nÂ  Â  Â  Â  cles = dnorm(diff_means / sqrt((b_sigma_genreAction + b_sigma_genreComedy)), 0, 1))\n\nbrms_uneq_robust_groups_tidy_fixed &lt;-Â \nÂ  tidyMCMC(brms_uneq_robust_groups_post, conf.int = TRUE, conf.level = 0.95,Â \nÂ  Â  Â  Â  Â  estimate.method = \"median\", conf.method = \"HPDinterval\")\n## # A tibble: 9 x 5\n##Â  termÂ  Â  Â  Â  Â  Â  Â  Â  estimate std.error conf.low conf.high\n##Â  &lt;chr&gt;Â  Â  Â  Â  Â  Â  Â  Â  Â  &lt;dbl&gt;Â  Â  &lt;dbl&gt;Â  Â  &lt;dbl&gt;Â  Â  &lt;dbl&gt;\n## 1 b_genreComedyÂ  Â  Â  Â  5.99Â  Â  Â  0.109Â  Â  5.77Â  Â  Â  6.19Â \n## 2 b_genreActionÂ  Â  Â  Â  5.30Â  Â  Â  0.107Â  Â  5.09Â  Â  Â  5.50Â \n## 3 b_sigma_genreComedyÂ  1.47Â  Â  Â  0.0882Â  Â  1.30Â  Â  Â  1.64Â \n## 4 b_sigma_genreActionÂ  1.47Â  Â  Â  0.0826Â  Â  1.31Â  Â  Â  1.62Â \n## 5 nuÂ  Â  Â  Â  Â  Â  Â  Â  Â  1.48Â  Â  Â  0.287Â  Â  0.963Â  Â  2.04Â \n## 6 diff_meansÂ  Â  Â  Â  Â  -0.690Â  Â  Â  0.151Â  Â  -1.01Â  Â  -0.415\n## 7 diff_sigmaÂ  Â  Â  Â  Â  0.00100Â  Â  0.111Â  Â  -0.212Â  Â  0.217\n## 8 cohen_dÂ  Â  Â  Â  Â  Â  -0.571Â  Â  Â  0.126Â  Â  -0.818Â  Â  -0.327\n## 9 clesÂ  Â  Â  Â  Â  Â  Â  Â  0.368Â  Â  Â  0.0132Â  Â  0.341Â  Â  0.391\nCohenâ€™s d:Â  standardized difference in means (Also see Post-Hoc Analysis, Multilevel &gt;&gt; Cohenâ€™s D)\n\n\nMedium effect size in the example above\nThe denominator in this calculation is the square root of the average std.dev, but it doesnâ€™t look like any of the ones used in the wiki article\n\nLooks closer to the Strictly Standardized Mean Difference (SSMD) (wiki)\n\n\nCommon language effect size (CLES): Probability that a rating sampled at random from one group will be greater than a rating sampled from the other group.\n\n36.8% chance that we could randomly select an action rating from the comedy distribution"
  },
  {
    "objectID": "qmd/post-hoc-analysis-general.html#sec-phoc-gen-dd",
    "href": "qmd/post-hoc-analysis-general.html#sec-phoc-gen-dd",
    "title": "General",
    "section": "Dichotomous Data",
    "text": "Dichotomous Data\n\nMean (probability-of-event) + CI Estimation\n\nLarge Population\n\\[\n\\hat p \\pm z_{\\alpha/2} \\sqrt {\\frac{\\hat p(1-\\hat p)}{n}}\n\\]\nbinom::binom.asymp(x=x, n=n, conf.level=0.95)\n##Â  Â  Â  methodÂ  xÂ  nÂ  Â  Â  meanÂ  Â  Â  lowerÂ  Â  upper\n## 1 asymptotic 52 420 0.1238095 0.09231031 0.1553087\nSmall/Finite Population\n\\[\n\\hat p \\pm z_{\\alpha/2} \\sqrt {\\frac{\\hat p(1-\\hat p)}{n} \\cdot \\frac{N-n}{N-1}}\n\\]\n\nSee ?binom::binom.confint for many methods\nâ€œWhen the intracluster correlation coefficient is high and the prevalence, p, is less than 0.10 or greater than 0.90, the Agresti-Coull and Clopper-Pearson intervals perform best. In other settings, the Clopper-Pearson interval is unnecessarily wide. In general, the Logit, Wilson, Jeffreys, and Agresti-Coull intervals perform well, although the Logit interval may be intractable when the standard error is equal to zero.â€ (paper)\n\n1-Sample Proportion Test\n\nExample: Do 50% of infants start walking by 12 months of age?\n&gt; table(walkby12)\n\n#&gt; walkby12\n#&gt; 0 1\n#&gt; 14 36\n\nprop.test(36,50,p=0.5,correct=FALSE)\n\n#&gt; 1-sample proportions test without continuity correction\n#&gt; data: 36 out of 50, null probability 0.5\n#&gt; X-squared = 9.68, df = 1, p-value = 0.001863\n#&gt; alternative hypothesis: true p is not equal to 0.5\n#&gt; 95 percent confidence interval:\n#&gt; 0.5833488 0.8252583\n#&gt; sample estimates:\n#&gt; p\n#&gt; 0.72\n\np-value &lt; 0.05 therefore the null hypothesis of 50% of infants walking is rejected\ncorrect = FALSE says this is a large sample (See assumptions in difference of proportions &gt;&gt; Z-Test)\nCI for the population proportion estimate is given.\n\n\nBayesian\n# Mean proportion estimated with prior that mean lies between 0.05 and 0.15\n\n##Function to determine beta parameters s.t. the 2.5% and 97.5% quantile match the specified values\ntarget &lt;- function(theta, prior_interval, alpha=0.05) {\nÂ  sum( (qbeta(c(alpha/2, 1-alpha/2), theta[1], theta[2]) - prior_interval)^2)\n}\n## Find the prior parameters\nprior_params &lt;- optim(c(10,10),target, prior_interval=c(0.05, 0.15))$par\n## [1]Â  12.04737 116.06022\n# not really sure how this works. Guessing theta1,2 is c(10,10) but then there doesn't seem to be an unknown to optimize for.\n\n## Compute credibile interval from a beta-binomial conjugate prior-posterior approach\nbinom::binom.bayes(x=x, n=n, type=\"central\", prior.shape1=prior_params[1], prior.shape2=prior_params[2])\n##Â  methodÂ  xÂ  nÂ  shape1Â  shape2Â  Â  Â  meanÂ  Â  Â  lowerÂ  Â  upperÂ  sig\n## 1Â  bayes 52 420 64.04737 484.0602 0.1168518 0.09134069 0.1450096 0.05\n\n##Plot of the beta-posterior\np &lt;- binom::binom.bayes.densityplot(ci_bayes)\n##Add plot of the beta-prior\ndf &lt;- data.frame(x=seq(0,1,length=1000)) %&gt;% mutate(pdf=dbeta(x, prior_params[1], prior_params[2]))\np + geom_line(data=df, aes(x=x, y=pdf), col=\"darkgray\",lty=2) +\nÂ  coord_cartesian(xlim=c(0,0.25)) + scale_x_continuous(labels=scales::percent)\n\n# Estimated with a flat prior (essentially equivalent to the frequentist approach)\nbinom::binom.bayes(x=x, n=n, type=\"central\", prior.shape1=1, prior.shape2=1))\n##Â  methodÂ  xÂ  n shape1 shape2Â  Â  Â  meanÂ  Â  Â  lowerÂ  Â  upperÂ  sig\n## 1Â  bayes 52 420Â  Â  53Â  Â  369 0.1255924 0.09574062 0.158803 0.05\n\nFrom https://staff.math.su.se/hoehle/blog/2017/06/22/interpretcis.html\nInterpretation\n\nTechnical: â€œ95% equi-tailed credible interval resulting from a beta-binomial conjugate Bayesian approach obtained when using a prior beta with parameters such that the similar 95% equi-tailedÂ prior credible intervalÂ has limits 0.05 and 0.15. Given these assumptions the interval 9.1%- 14.5% contains 95% of your subjective posterior density for the parameter.â€\nNontechnical: the true value is in that interval with 95% probability or just this 95% Bayesian confidence interval is 9.1%- 14.5%.\n\n\n\n\n\nDifference in Proportions\n\nCochran-Mantel-Haenszel Test: This test is appropriate when you have data from multiple 2x2 tables (strata) and want to test the association between two categorical variables while controlling for the effects of a third variable (confounding variable).\n\nSee Discrete Analysis Notebook\n\nDo NOT use Fisherâ€™s Exact Test.\n\nSeveral different p-values can be associated with a single table, making scientific inference inconsistent\nDespite the fact that Fisherâ€™s test gives exact p-values, some authors have argued that it is conservative, i.e.Â that its actual rejection rate is below the nominal significance level. The issue has to do with Fisherâ€™s test conditioning on the margin totals.\n\nLikelihood ratios, posterior probabilities and mid-p-values - lead to more consistent inferences. Recommendations from this paper:\n\nA Bayesian interval for the log odds ratio with Jeffreysâ€™ reference prior\nConditional Likelihood Ratio Test\n\n{ProfileLikelihood}: LR.pvalue(y1, y2, n1, n2, interval=0.01)\n\n\nZ-Test\n\\[\n\\begin{align}\n&Z = \\frac{(\\hat p_1 - \\hat p_2)}{\\sqrt{\\hat p (1 - \\hat p) \\left(\\frac{1}{n_1}-\\frac{1}{n_2}\\right)}} \\\\\n&\\text{where} \\;\\; \\hat p = \\frac{Y_1 + Y_2}{n_1 + n_2}\n\\end{align}\n\\]\n\nThe z-test comparing two proportions is equivalent to the chi-square test of independence\nTerms\n\n\\(\\hat p_i\\) is the sample proportion\n\\(\\hat p\\) is the overall proportion\n\\(Y_i\\) is the sample count\n\\(n_i\\) is the sample size\n2-tail hypothesis test; If Z &gt; 1.96 or Z &lt; -1.96 (i.e.Â p-value &lt; 0.05), then the sample proportions are NOT equal.\n\nCheck Assumptions\n\nIndependent observations and sufficient sample sizes.\n\nFor each sample, i:\n\\[\n\\begin{align}\n&n_i \\cdot \\hat p_i &gt; 5 \\\\\n&n_i \\cdot (1-\\hat p_i) &gt; 5\n\\end{align}\n\\]\nThere is a â€œcontinuity correctionâ€ arg (correct = TRUE)that when set to TRUE can correctly compute the CIs for when proportions for each/one event are less than 5\n\n\nExample\nres &lt;- prop.test(x = c(490, 400), n = c(500, 500), correct = FALSE)\n\n#&gt; 2-sample test for equality of proportions with continuity correction\n\n#&gt; data:  c(490, 400) out of c(500, 500)\n#&gt; X-squared = 82.737, df = 1, p-value &lt; 2.2e-16\n#&gt; alternative hypothesis: two.sided\n#&gt; 95 percent confidence interval:\n#&gt;  0.1428536 0.2171464\n#&gt; sample estimates:\n#&gt; prop 1 prop 2 \n#&gt;   0.98   0.80 \n\np-value &lt; 0.05, so proportions are statistically different\nThe confidence interval given for the true proportion if there is one group, or for the difference in proportions if there are 2 groups and p argument isnâ€™t provided\n2-sided test is default\nchisq.test() is exactly equivalent to prop.test() but it works with data in matrix form.\n\n\nMcNemarâ€™s test\n\nFor comparing paired data of 2 groups\nMainly useful when the measurements are on the nominal or ordinal scale\nTests for significant difference in frequencies of paired samples when it has binary responses\n\nH0: There is no significant change in individuals after the treatment\nH1: There is a significant change in individuals after the treatment\n\n\n# data is unaggregated (i.e. paired measurements from individuals)\ntest &lt;- mcnemar.test(table(data$pretreatment, data$posttreatment))\n#&gt; McNemar's Chi-squared test with continuity correction\n#&gt; data: table(data$before, data$after)\n#&gt; McNemar's chi-squared = 0.5625, df = 1, p-value = 0.4533\n\ncorrect = TRUE (default) - Continuity correction (increases â€œusefulness and accuracy of the testâ€ so probably better to leave it as TRUE)\nx & y are factor vectors\nx can be a matrix of aggregated counts\n\nIf the 1st row, 2nd cell or 2nd row, 1st cell have counts &lt; 50, then use the Exact Tests to get accurate p-values (For details see above for page in notebook)\n\nInterpretation: p-value is 0.45, above the 5% significance level and therefore the null hypothesis cannot be rejected"
  },
  {
    "objectID": "qmd/post-hoc-analysis-general.html#sec-phoc-gen-catdat",
    "href": "qmd/post-hoc-analysis-general.html#sec-phoc-gen-catdat",
    "title": "General",
    "section": "Categorical Data",
    "text": "Categorical Data\n\nExample: Bayesian; 3 level Categorical Variable\n\nData and Model\n\nCalculate Differences\n\nVisualize (Code in previous chunk)"
  },
  {
    "objectID": "qmd/post-hoc-analysis-multilevel.html#sec-phoc-mixeff-misc",
    "href": "qmd/post-hoc-analysis-multilevel.html#sec-phoc-mixeff-misc",
    "title": "Multilevel",
    "section": "Misc",
    "text": "Misc\n\nAlso see Post-Hoc Analysis, general\nPackages\n\n{effectsize} - Has many of the metrics discussed here and others â€”Â with confidence intervals"
  },
  {
    "objectID": "qmd/post-hoc-analysis-multilevel.html#sec-phoc-mixeff-tukey",
    "href": "qmd/post-hoc-analysis-multilevel.html#sec-phoc-mixeff-tukey",
    "title": "Multilevel",
    "section": "Tukey Test",
    "text": "Tukey Test\n\nDifference in effects\nExample: Is there a statistically significant difference between the estimated effects of the categories of the fixed effect, â€œSeasonâ€ Data from Multilevel Modeling and Effects Statistics for Sports Scientists in R\n\n{multcomp}{emmeans}\n\n\nlibrary(multcomp)\n# pairwise comparisons\nfit_tukey &lt;- glht(fit, linfct=mcp(Season=\"Tukey\"))\nsummary(fit_tukey)\n##Â \n## Simultaneous Tests for General Linear Hypotheses\n##Â \n## Multiple Comparisons of Means: Tukey Contrasts\n##Â \n##Â \n## Fit: lmer(formula = Distance ~ Season + (1 | Athlete), data = data)\n##Â \n## Linear Hypotheses:\n##Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Estimate Std.  Error z value Pr(&gt;|z|)Â  Â \n## Postseason - Inseason == 0Â  Â      36.71Â  Â 90.08Â   0.408Â  Â  0.911Â  Â \n## Preseason - Inseason == 0Â  Â     1166.00Â  Â 90.08Â  12.944Â   &lt;1e-05 ***\n## Preseason - Postseason == 0Â     1129.29Â  110.32Â  10.236Â   &lt;1e-05 ***\n## ---\n## Signif. codes:Â  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## (Adjusted p values reported -- single-step method)\n\n\nemmeans(fit, specs = pairwise ~ Season)\n## $emmeans\n##Â  SeasonÂ  Â  emmeanÂ  SEÂ   df lower.CL upper.CL\n##Â  InseasonÂ  Â  5104 137 20.8Â  Â   4818Â  Â  5389\n##Â  PostseasonÂ  5140 151 30.6Â  Â   4831Â  Â  5449\n##Â  PreseasonÂ  Â 6270 151 30.6Â  Â   5961Â  Â  6579\n## Degrees-of-freedom method: kenward-rogerÂ \n## Confidence level used: 0.95Â \n## $contrasts\n##Â  contrastÂ  Â  Â  Â  Â  Â  Â   estimateÂ  Â  SEÂ  df t.ratio p.value\n##Â  Inseason - PostseasonÂ  Â   -36.7Â  90.1 978Â  -0.408 0.9125Â \n##Â  Inseason - PreseasonÂ  Â  -1166.0Â  90.1 978 -12.944 &lt;.0001Â \n##Â  Postseason - PreseasonÂ  -1129.3 110.3 978 -10.236 &lt;.0001Â \nDegrees-of-freedom method: kenward-rogerÂ \nP value adjustment: tukey method for comparing a family of 3 estimates\n\n\n\n\nInterpretation\n\nThere is NOT a difference between the effect that Postseason has on Distance and the effect that Inseason has on Distance.\nThere is a difference with between the other two pairs of categores\nEstimated mean distance given season type\n\nIâ€™m not sure these estimates are appropriate in this situation since the Season variable is inherently unbalanced.\nAlso see emmeans Post-Hoc Analysis, emmeans"
  },
  {
    "objectID": "qmd/post-hoc-analysis-multilevel.html#sec-phoc-mixeff-cohensd",
    "href": "qmd/post-hoc-analysis-multilevel.html#sec-phoc-mixeff-cohensd",
    "title": "Multilevel",
    "section": "Cohenâ€™s D",
    "text": "Cohenâ€™s D\n\nStandardized difference in means given a grouping variable\nGenerally recommended to use \\(g_{\\text{rm}}\\) or \\(g_{\\text{av}}\\)\n\nStandard practice is use whichever one of those two values is closer to \\(d_s\\) , because it helps make the result comparable with between-subject studies.\nAppropriate Version Per Use Case\n\n\n\n\n\n\n\nUse\nVersion\n\n\n\n\nIndependent groups, power analyses where \\(\\sigma_\\text{pop}\\) is known or \\(\\sigma\\) is calculated with \\(n\\)\n\\(d_{\\text{pop}}\\)\n\n\nIndependent groups, power analyses where \\(\\sigma_\\text{pop}\\) is unknown or \\(\\sigma\\) is calculated with \\(n-1\\)\n\\(d_s\\)\n\n\nIndependent groups, corrects for small sample bias; report for use in meta-analyses\n\\(g\\)\n\n\nIndependent groups, when treatment might affect SD\n\\(\\Delta\\)\n\n\nCorrelated groups; generally recommended over \\(g_{\\text{rm}}\\)\n\\(g_{\\text{av}}\\)\n\n\nCorrelated groups; more conservative than \\(g_{\\text{av}}\\)\n\\(g_{\\text{rm}}\\)\n\n\nCorrelated groups; power analyses\n\\(d_z\\)\n\n\n\n\nNotes from: Calculating and reporting effect sizes to facilitate cumulative science: a practical primer for t-tests and ANOVAs (Lakens)\nCan be used to compare effects across studies, even when the dependent variables are measured in different ways\n\nExamples\n\nWhen one study uses 7-point scales to measure dependent variables, while the other study uses 9-point scales\nWhen completely different measures are used, such as when one study uses self-report measures, and another study used physiological measurements.\n\n\nThe bias-corrected version is known as Hedgesâ€™ g, and in the r family of effect sizes, the correction for eta squared (Î·2) is known as omega squared (Ï‰2)\nGuidelines\n\nRange: 0 to \\(\\infty\\)\nCohen (1992)\n\n|d| &lt; 0.2 â€œnegligibleâ€\n|d| &lt; 0.5 â€œsmallâ€\n|d| &lt; 0.8 â€œmediumâ€\notherwise â€œlargeâ€\n\nOthers: Automated Interpretation of Indices of Effect Size\nValues should not be interpreted rigidly\n\ne.g.Â Small effect sizes can have large consequences, such as an intervention that leads to a reliable reduction in suicide rates with an effect size of d = 0.1.\n\nThe only reason to use these benchmarks is when the findings are extremely novel, and cannot be compared to related findings in the literature.\n\nTwo groups of Independent Observations (Between-Subjects)\n\\[\n\\begin{align}\nd_s &= \\frac{\\bar X_1 - \\bar X_2}{\\sqrt{\\frac{(n_1-1)SD^2_1 + (n_2-1)SD^2_2}{n_1 + n_2 - 2}}}\\\\\n&= t\\;\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}\\\\\n& \\approx \\frac{2t}{\\sqrt{N}}\n\\end{align}\n\\]\n\nWhere the denominator is the pooled standard deviation\n\\(t\\) is the t-value of two-sample t-test\nTypically used in an a priori power analysis for between-subjects designs\nHedgesâ€™ g (bias-corrected)\n\\[\ng_s = d_s \\times \\left(1-\\frac{3}{4(n_1 + n_2) - 9}\\right)\n\\]\n\nThe same correction is used for all types of Cohenâ€™s d\nThe difference between Hedgesâ€™s gs and Cohenâ€™s ds is very small, especially in sample sizes above 20\n\nInterpretation: A percentage of the standard deviation. Best to relate it to other effect sizes in the literature and itâ€™s practical consequences if possible.\n\ne.g.Â \\(d_s = 0.5\\) says the difference in means is half a standard deviation.\n\nWhenever standard deviations differ substantially between groups, Glassâ€™s \\(\\Delta\\) should also be reported\n\nOne Sample or Correlated Samples (Within-Subjects)\n\\[\n\\begin{aligned}\n&d_z = \\frac{M_{\\text{diff}}}{S_{\\text{diff}}} = \\frac{t}{\\sqrt{n}} \\\\\n&\\begin{aligned}\n\\text{where} \\quad S_{\\text{diff}}^{(1)} &= \\sqrt{\\frac{\\sum(X_{\\text{diff}} - M_{\\text{diff}})^2}{N-1}} \\\\\nS_{\\text{diff}}^{(2)} &= \\sqrt{\\text{SD}_1^2 + \\text{SD}_2^2 - (2\\cdot r\\cdot \\text{SD}_1 \\cdot \\text{SD}_2)}\n\\end{aligned}\n\\end{aligned}\n\\]\n\n\\(M_{\\text{diff}}\\) is the difference between the mean (M) of the difference scores and the comparison value, \\(\\mu\\) (typically 0)\n\nFor paired data, the mean of the difference scores is equal to the difference in means of the two groups, so you may see it described or calculated either way.\n\n\\(X_{\\text{diff}}\\) are the difference scores (i.e.Â the difference between the repeated measurements)\n\\(S_\\text{diff}\\) is the SD of the difference scores.\n\nIt can be calculated two different ways, but I doubt both are equal to each other.\nThe second way seems to be the preferred way since it incorporates a correlation measure.\n\n\\(t\\) is the t-value of a paired samples t-test\n\\(r\\) is the correlation between measurements\n\nRepeated Measures (Within-Subjects)\n\\[\nd_{\\text{rm}} = d_z \\cdot \\sqrt{2(1-r)}\n\\]\n\nAlternative\n\\[\nd_{\\text{av}} = \\frac{M_{\\text{diff}}}{\\frac{\\text{SD}_1 + \\text{SD}_2}{2}}\n\\]\n\nIgnores the correlation between measures\n\nIf it is believe that the intervention/treatment affected the SD after the intervention, then it is advised to only use either (pre-treatment) \\(\\text{SD}_1\\) (recommended) or (post-treatment) \\(\\text{SD}_2\\) and report which one is used. The calculated effect is then known as Glassâ€™s \\(\\boldsymbol{\\Delta}\\)\n\nExample: Distance (outcome), Season (Grouping variable)\n\nComparing Distance means given Season (3 levels) type\nData from Multilevel Modeling and Effects Statistics for Sports Scientists in R\n\n\n{effsize}{rstatix}\n\n\nAnother package, {effectsize}, is similar in that its formula arg only allows for grouping variables with only 2 levels\n\nMay have other features though, since itâ€™s part of the easystats suite.\n\nlibrary(effsize)\neffsize::cohen.d(preseason_data$Distance, inseason_data$Distance)\n##Â \n## Cohen's d\n##Â \n## d estimate: 0.9157833 (large)\n## 95 percent confidence interval:\n##Â  Â  lowerÂ  Â  upperÂ \n## 0.7493283 1.0822383\n\nSeason is a categorical fixed effect with 3 levels\nOther Available Arguments: hedges.correction, pooled, paired, within, noncentral\n\n\n\nlibrary(rstatix)\ndata %&gt;%Â \nÂ  rstatix::cohens_d(Distance ~ Season, ci = TRUE)\n\n#&gt;     .y.Â  Â  Â    group1Â  Â   group2Â  Â  effsizeÂ  Â   n1Â  Â   n2   conf.low conf.high magnitudeÂ \n#&gt; *  &lt;chr&gt;Â  Â     &lt;chr&gt;Â  Â  Â  &lt;chr&gt;Â  Â  Â  Â &lt;dbl&gt;   &lt;int&gt;  &lt;int&gt;Â  Â  &lt;dbl&gt;Â  Â  &lt;dbl&gt; &lt;ord&gt;Â  Â  Â \n#&gt; 1 Distance   InseasonÂ  Postseason   -0.0317Â    600Â    200Â  Â   -0.18Â  Â  Â 0.13  negligible\n#&gt; 2 Distance   InseasonÂ   PreseasonÂ    -0.877Â  Â  600Â    200Â  Â   -1.06Â  Â  -0.71       largeÂ  Â  Â \n#&gt; 3 Distance Postseason   PreseasonÂ    -0.884Â  Â  200Â    200Â  Â   -1.09Â  Â  -0.68       large\n\nSame types of arguments as {effsize} are available and also bootstrap CIs\nMagnitude (interpretation) by Cohenâ€™s (1992) guidelines"
  },
  {
    "objectID": "qmd/post-hoc-analysis-multilevel.html#common-language-effect-size",
    "href": "qmd/post-hoc-analysis-multilevel.html#common-language-effect-size",
    "title": "Multilevel",
    "section": "Common Language Effect Size",
    "text": "Common Language Effect Size\n\nAKA Probability of Superiority\nConverts the effect size into a percentage which is supposed to more understandable for laymen\nMisc\n\nNotes from The Common Language Effect Size Statistic\nPackages\n\n{ebtools::cles}\n\n\nInterpretation\n\nBetween-Subjects: The probability that a randomly sampled person from the first group will have a higher observed measurement than a randomly sampled person from the second group\nWithin-Subjects: The probability that an individual has a higher value on one measurement than the other.\n\nFormula\n\nAssumes variables are normally distributed and \\(\\sigma_1 = \\sigma_2\\)\n\nOriginal paper gives some evidence that these formulas are pretty robust to violations though.\nRecommended only for continuous variables\n\nBetween-Subjects\n\\[\n\\begin{align}\n\\tilde d &= \\frac{|M_1 - M_2|}{\\sqrt{p_1\\text{SD}_1^2 + p_2\\text{SD}_2^2}} \\\\\nZ &= \\frac{\\tilde d}{\\sqrt{2}}\n\\end{align}\n\\]\n\n\\(M_i\\): The mean of the ith group variable\n\\(p_i\\): The proportion of the sample size of the ith group variable\n\nWithin Subjects\n\\[\nZ = \\frac{|M_1 - M_2|}{\\sqrt{\\operatorname{SD}_1^2 + \\operatorname{SD}_2^2 - 2 \\times r \\times \\operatorname{SD}_1 \\times \\operatorname{SD}_2}}\n\\]\n\n\\(r\\) is the Pearson correlation between the group variables\n\n\nAlternative Generalization\n\n\\(A_{1,2} = P(X_1 &gt; X_2) + 0.5 \\times P(X_1 = X_2)\\)\nApplies for any, not necessarily continuous, distribution that is at least ordinally scaled\n\nEqual to CL in the continuous case\nInterpreted as an estimate of the value of CL that would be obtained if the distribution of X were continuous."
  },
  {
    "objectID": "qmd/post-hoc-analysis-multilevel.html#eta-squared",
    "href": "qmd/post-hoc-analysis-multilevel.html#eta-squared",
    "title": "Multilevel",
    "section": "Eta Squared",
    "text": "Eta Squared\n\nNotes from: Calculating and reporting effect sizes to facilitate cumulative science: a practical primer for t-tests and ANOVAs (Lakens)\nEffect Size for ANOVA\nMeasures the proportion of the variation in Y that is associated with membership of the different groups defined by X, or the sum of squares of the effect divided by the total sum of squares\neta squared is an uncorrected effect size estimate that estimates the amount of variance explained based on the sample, and not based on the entire population.\npartial eta squared (Î·2p) to improve the comparability of effect sizes between studies, which expresses the sum of squares of the effect in relation to the sum of squares of the effect and the sum of squares of the error associated with the effect.\nAlthough Î·2p is more useful when the goal is to compare effect sizes across studies, it is not perfect, because Î·2p differs when the same two means are compared in a within-subjects design or a between-subjects design.\nAn \\(\\eta^2\\) of 0.13 means that 13% of the total variance can be accounted for by group membership.\nCIs should be at 90%, because if you use 95%, itâ€™s possible that even with a significant F-test, the CI will contain 0. For 90%, this doesnâ€™t happen.\nEta Squared\n\\[\n\\eta^2 = \\frac{\\text{SS}_{\\text{effect}}}{\\text{SS}_{\\text{total}}}\n\\]\n\n\\(\\text{SS}_{\\text{effect}}\\) and \\(\\text{SS}_{\\text{total}}\\) are obtained from the ANOVA results\nThe correction for eta squared (\\(\\eta^2\\)) is known as omega squared (\\(\\omega^2\\)). Still biased but less biased. The difference is typically small, and the bias decreases as the sample size increases.\n\\[\n\\begin{align}\n\\omega^2 &= \\frac{\\operatorname{df}_{\\text{effect}}(\\operatorname{MS_{\\text{effect}}}-\\operatorname{MS_{\\text{error}}})}{\\operatorname{SS_{\\text{total}}} + \\operatorname{MS_{\\text{error}}}} \\quad \\text{(between-subjects)} \\\\\n\\omega^2 &= \\frac{\\operatorname{df}_{\\text{effect}}(\\operatorname{MS_{\\text{effect}}}-\\operatorname{MS_{\\text{error}}})}{\\operatorname{SS_{\\text{total}}} + \\operatorname{MS_{\\text{subjects}}}} \\quad \\text{(within-subjects)} \\\\\n\\end{align}\n\\]\n\nPartial Eta Squared\n\\[\n\\begin{align}\n\\eta_p^2 &= \\frac{\\operatorname{SS_{\\text{effect}}}}{\\operatorname{SS_{\\text{effect}}} + \\operatorname{SS_{\\text{error}}}} \\quad \\text{(fixed and measured variables)}\\\\\n\\eta_p^2 &= \\frac{F \\times \\operatorname{df}_{\\text{effect}}}{F \\times \\operatorname{df}_{\\text{effect}} + \\operatorname{df}_{\\text{error}}} \\quad \\text{(fixed variables)}\n\\end{align}\n\\]\n\nfixed (e.g., manipulated), not random (e.g., measured)\nBias-Lessened\n\\[\n\\omega_p^2 = \\frac{\\operatorname{df}_{\\text{effect}}(\\operatorname{MS_{\\text{effect}}}-\\operatorname{MS_{\\text{error}}})}{\\operatorname{df}_{\\text{effect}} \\times  \\operatorname{MS_{\\text{effect}}} + (N - \\operatorname{df}_{\\text{effect}}) \\times \\operatorname{MS_{\\text{error}}}}\n\\]\n\nSame equation whether itâ€™s for between-subject designs and within-subject designs\n\n\nRecommend researchers report Î·2G and/or Î·2p, at least until generalized omega-squared is automatically provided by statistical software packages\n\nFor designs where all factors are manipulated between participants, Î·2p and Î·2G are identical, so either effect size can be reported. For within-subjects designs and mixed designs where all factors are manipulated, Î·2p can always be calculated from the F-value and the degrees of freedom using formula 13, but Î·2G cannot be calculated from the reported results,and therefore I recommend reporting Î·2G for these designs\nsupplementary spreadsheet provides a relatively easy way to calculate Î·2G for commonly used designs. For designs with measured factors or covariates, neither Î·2p nor Î·2G can be calculated from the\n\nAppropriate Version Per Use Cases\n\n\n\n\n\n\n\n\nUse Case\nVersion\nLess Biased Version\n\n\n\n\nComparisons within a single study\n\\(\\eta^2\\)\n\\(\\omega^2\\)\n\n\nPower analyses, and for comparisons of effect sizes across studies with the same experimental design\n\\(\\eta_p^2\\)\n\\(\\omega_p^2\\)\n\n\nMeta-Analyses to compare across various experimental designs\n\\(\\eta_G^2\\)\n\\(\\omega_G^2\\)\n\n\n\nGuidelines\n\nCohenâ€™s benchmarks were developed for comparisons between unrestricted populations (e.g., men vs.Â women), and using these benchmarks when interpreting the Î·2p effect size in designs that include covariates or repeated measures is not consistent with the onsiderations upon which the benchmarks were based.\n\nAlthough \\(\\eta_G^2\\) can be compared using these guidelines, it is preferable to compare effect sizes with those in the literature."
  },
  {
    "objectID": "qmd/privacy.html#sec-priv-tag",
    "href": "qmd/privacy.html#sec-priv-tag",
    "title": "Privacy",
    "section": "Tags",
    "text": "Tags\n\nTag sensitive information in dataframes\nnames(df)\n[1] \"date\" \"first_name\" \"card_number\" \"payment\"\n# assign pii tags\nattr(df, \"pii\") &lt;- c(\"name\", \"ccn\", \"transaction\")\n\nPersonally Identifiable Information (PII)\n\nTag dataframes with the names of regulations that are applicable\nattr(df, \"regs\") &lt;- c(\"CCPA\", \"GDPR\", \"GLBA\")\n\nCCPA is the privacy regulation for California\nGDPR is the privacy regulation for the European Union\nGLBA is the financial regulation for the United States\n\nNeeded because df has credit card and financial information\n\nSaving objects as .rds files preserves tags"
  },
  {
    "objectID": "qmd/privacy.html#sec-priv-hash",
    "href": "qmd/privacy.html#sec-priv-hash",
    "title": "Privacy",
    "section": "Hashing",
    "text": "Hashing\n\n{digest}\n\nHash Function\n\nApply Hash Function to PII Fields"
  },
  {
    "objectID": "qmd/production-data-validation.html#sec-prod-datval-misc",
    "href": "qmd/production-data-validation.html#sec-prod-datval-misc",
    "title": "Data Validation",
    "section": "Misc",
    "text": "Misc\n\nPackages\n\n{pointblank} - This is a heavy-duty package that helps you validate all aspects of datatsets with extensive reporting capabilities.\n{{great_expectations}}\n{{pydantic}}\n\nAlso see DB, Engineering &gt;&gt; Data Quality"
  },
  {
    "objectID": "qmd/production-data-validation.html#sec-prod-datval-pipe",
    "href": "qmd/production-data-validation.html#sec-prod-datval-pipe",
    "title": "Data Validation",
    "section": "Pipeline",
    "text": "Pipeline\n\nCollection\n\ne.g.Â people working in your stores, your call centrrs or perhaps as online support agents. It could be your online sign-up forms or physical documents that your agents must manually input into your systems\nChecks\n\nCompleteness: The data being collected and captured is complete (not NULLs), i.e.Â all mandatory fields have information added, and it is not missing any key data points.\nUniqueness: The data is kept as unique as possible, i.e.Â if a client already has an account, another account is not being set up. If the mobile number already exists in the system, the current order is linked to the old order etc.\nValidity: The data being captured conforms to the corporate standards, i.e.Â account number is eight digits long and starts with a number 9 is conformed with at the time of capturing\n\n\nTransfer\n\nMake sure that the data sent is the same as the data received\nCheck\n\nConsistency: The data is consistent across all the tables with the same values. This could translate to well-reconciled data between source and target, i.e.Â 100 records sent, 100 records received. Or that the table has specific values like date of birth and is consistent with other tables that have the same or similar information. Orphaned records (exist in A and not in B) should be highlighted, monitored and remediated.\n\n\nStorage\n\nData spends most of its time here, so take advantage of the time that itâ€™s not being used in a product.\nChecks\n\nCompleteness: Null reporting â€” how many columns are Null, and why are they Null? Can we change the data capture process to avoid these Nulls coming through?\nUniqueness: Are non-mandatory attributes unique? Are duplications going to impact downstream reporting?\n\n\nTransformation\n\nOften the place where most validation takes place.\nChecks\n\nTimeliness: Ensure data is available promptly to meet agreed SLAs.\nConsistency: Reconciliation checks from source to target\n\nExample: tolerance checks on tables processed; we generally receive 100 records, and we have received just two records today; how do we alert the user of this discrepancy?\n\nValidity: Non-conformance under the validity dimension could render the transformation and subsequent consumption useless. This is especially helpful when data capture doesnâ€™t have robust controls.\n\n\nConsumption\n\nEnsure the business problem is solved\nChecks\n\nAccuracy: The data is accurate enough for reporting, such as board metrics. Account numbers are associated with the correct customer segments, or the date of birth is not the default value like 01/01/1901.\nTimeliness: It is not early that it excludes some recent records. It is not late that it misses the deadline for reporting. All agreed SLAs must be met to ensure the data consumption layer has the data available when required and stays fit for purpose."
  },
  {
    "objectID": "qmd/production-data-validation.html#sec-prod-datval-py",
    "href": "qmd/production-data-validation.html#sec-prod-datval-py",
    "title": "Data Validation",
    "section": "Python",
    "text": "Python\n\nMisc\n\nComparison between {{pydantic}} and {{pandas_dq}}\n\nDeclarative syntax: arguably, Pydantic allows you to define the data schema and validation rules using a more concise and readable syntax. This can make it easier to understand and maintain your code. I find it super helpful to be able to define the ranges of possible values instead of merely the data type.\nBuilt-in validation functions: Pydantic provides various powerful built-in validation functions like conint, condecimal, and constr, which allow you to enforce constraints on your data without having to write custom validation functions.\nComprehensive error handling: When using Pydantic, if the input data does not conform to the defined schema, it raises a ValidationError with detailed information about the errors. This can help you easily identify issues with your data and take necessary action.\nSerialization and deserialization: Pydantic automatically handles serialization and deserialization of data, making it convenient to work with different data formats (like JSON) and convert between them.\n\n\nExample: {{pydantic}}\n\nSet schema and create sample data\n# data validation on the data dictionary\nfrom pydantic import BaseModel, Field, conint, condecimal, constr\nclass LoanApplication(BaseModel):\nÂ  Â  Loan_ID: int\nÂ  Â  Gender: conint(ge=1, le=2)\nÂ  Â  Married: conint(ge=0, le=1)\nÂ  Â  Dependents: conint(ge=0, le=3)\nÂ  Â  Graduate: conint(ge=0, le=1)\nÂ  Â  Self_Employed: conint(ge=0, le=1)\nÂ  Â  ApplicantIncome: condecimal(ge=0)\nÂ  Â  CoapplicantIncome: condecimal(ge=0)\nÂ  Â  LoanAmount: condecimal(ge=0)\nÂ  Â  Loan_Amount_Term: condecimal(ge=0)\nÂ  Â  Credit_History: conint(ge=0, le=1)\nÂ  Â  Property_Area: conint(ge=1, le=3)\nÂ  Â  Loan_Status: constr(regex=\"^[YN]$\")\n\n# Sample loan application data\nloan_application_data = {\nÂ  Â  \"Loan_ID\": 123456,\nÂ  Â  \"Gender\": 1,\nÂ  Â  \"Married\": 1,\nÂ  Â  \"Dependents\": 2,\nÂ  Â  \"Graduate\": 1,\nÂ  Â  \"Self_Employed\": 0,\nÂ  Â  \"ApplicantIncome\": 5000,\nÂ  Â  \"CoapplicantIncome\": 2000,\nÂ  Â  \"LoanAmount\": 100000,\nÂ  Â  \"Loan_Amount_Term\": 360,\nÂ  Â  \"Credit_History\": 1,\nÂ  Â  \"Property_Area\": 2,\nÂ  Â  \"Loan_Status\": \"Y\"\n}\n\n# Validate the sample data using the LoanApplication Pydantic model\nloan_application = LoanApplication(**loan_application_data)\nValidate and print report\n# data validation on the data dictionary\nfrom pydantic import ValidationError\nfrom typing import List\n\n# Function to validate DataFrame and return a list of failed LoanApplication objects\ndef validate_loan_applications(df: pd.DataFrame) -&gt; List[LoanApplication]:\nÂ  Â  failed_applications = []\nÂ  Â  for index, row in df.iterrows():\nÂ  Â  Â  Â  row_dict = row.to_dict()\n\nÂ  Â  Â  Â  try:\nÂ  Â  Â  Â  Â  Â  loan_application = LoanApplication(**row_dict)\nÂ  Â  Â  Â  except ValidationError as e:\nÂ  Â  Â  Â  Â  Â  print(f\"Validation failed for row {index}: {e}\")\nÂ  Â  Â  Â  Â  Â  failed_applications.append(row_dict)\nÂ  Â  return failed_applications\n\n# Validate the entire DataFrame\nfailed_applications = validate_loan_applications(df_loans.reset_index())\n\n# Print the failed loan applications or \"No data quality issues\"\nif not failed_applications:\nÂ  Â  print(\"No data validation issues\")\nelse:\nÂ  Â  for application in failed_applications:\nÂ  Â  Â  Â  print(f\"Failed application: [{application}]{style='color: #990000'}\")\n\nExample: {{pandas_dq}}\n\nCheck schema\n\nfrom pandas_dq import DataSchemaChecker\nschema = {\nÂ  Â  'Loan_ID': 'int64',\nÂ  Â  'Gender': 'int64',\nÂ  Â  'Married': 'int64',\nÂ  Â  'Dependents': 'int64',\nÂ  Â  'Graduate': 'int64',\nÂ  Â  'Self_Employed': 'int64',\nÂ  Â  'ApplicantIncome': 'float64',\nÂ  Â  'CoapplicantIncome': 'float64',\nÂ  Â  'LoanAmount': 'float64',\nÂ  Â  'Loan_Amount_Term': 'float64',\nÂ  Â  'Credit_History': 'int64',\nÂ  Â  'Property_Area': 'int64',\nÂ  Â  'Loan_Status': 'object'\n}\nchecker = DataSchemaChecker(schema)\nchecker.fit(df_loans.reset_index())\n\nShows 3 variables with incorrect types\n\nFix issues\n\ndf_fixed = checker.transform(df_loans.reset_index())\ndf_fixed.info()\n\nVariables have been cast into the correct types according to the schema"
  },
  {
    "objectID": "qmd/production-deployment.html#sec-prod-deploy-misc",
    "href": "qmd/production-deployment.html#sec-prod-deploy-misc",
    "title": "Deployment",
    "section": "Misc",
    "text": "Misc\n\nQuestions:\n\nDoes it need to be &lt;10ms or offline?\nDo you know your approximate â€œoptimizingâ€ and â€œsatisficingâ€ metrics thresholds?\nDid you verify that your input features can be looked up in a low-read-latency DB?\nCould you find anything that can be precomputed and cached?\n\nAdditional Stages\n\nRun a silent deployment\n\nfile outputs, alerts, emails are silenced\nUseful for finding bugs\n\nRun a pilot deployment\n\nOnly a few groups are given permission to use the product\nReceive feedback (e.g.Â weekly meetings), fix bugs, and make changes"
  },
  {
    "objectID": "qmd/production-deployment.html#sec-prod-deploy-auth",
    "href": "qmd/production-deployment.html#sec-prod-deploy-auth",
    "title": "Deployment",
    "section": "Authentification",
    "text": "Authentification\n\nUsers of your product need to go through some kind of authentification\nPosit Connect\n\nInteracts with your local directory so users can use their company usernames and passwords\n\nalso allows you to only give access for specific applications to specific user groups\n\nDevelopers donâ€™t need to keep track of extra usernames and passwords"
  },
  {
    "objectID": "qmd/production-deployment.html#sec-prod-deploy-moddepl",
    "href": "qmd/production-deployment.html#sec-prod-deploy-moddepl",
    "title": "Deployment",
    "section": "Model Deployment Strategies",
    "text": "Model Deployment Strategies\n\nMisc\n\ntl;dr\n\nThe model predicts on data generated by a user action (e.g.Â text entry), and the user/UI is waiting for a response\n\nServe the model with an HTTP endpoint (i.e.Â API)\n\nA system event/user action (e.g.Â a file upload) triggers the model prediction and a separate process acts on the model prediction asynchronously\n\nPrediction is NOT needed immediately\nSpark Streaming (a little different)\n\nStreaming data gets input into model thats in Spark as it becomes available, Spark outputs predictions to S3 bucket that is used by an app.\nRefreshing the app, reads data from the S3 bucket and therefore has the latest predictions\n\n\nPredictions are made on periodic batches of data\n\nNotes from Deploying ML Models Using Containers in Three Ways\n\nHas py code for each strategy\nProvides tips and best practices\n\n\nStrategies\n\nThe model predicts on data generated by a user action (e.g.Â text entry), and the user/UI is waiting for a response\n\nServe the model with an HTTP endpoint (i.e.Â API)\n\n\nmodel prediction time should be less than 300 ms (standard response time expected for any HTTP service)\n\n&gt; 300ms means the service is at risk of running behind prediction requests volume, to a point where they start getting timed out\n\nTools: plumber, vetiver, flask\n\nFlask\n\nServing flask in production uses gunicorn and gevent behind a nginx server. The serve file gets these set up.\nIf you want to change some timeouts and the number of workers, the SERVER_TIMEOUT and SERVER_WORKERS env var can be used.\n\n\nYou can set an auto-scale property with your container orchestration system to scale on CPU utilization (e.g.Â auto-scale the model service to up to 10 pods if average CPU utilization is above 40%).\nThere are ways to hold multiple requests in memory (e.g.Â using cache) for a really short time (25ms) so that your model can fully utilize memory and perform a mini-batch prediction. Takes effort to implement and ensure threads can respond well.\n\n\nA system event/user action (e.g.Â a file upload) triggers the model prediction and a separate process acts on the model prediction asynchronously\n\nPrediction is NOT needed immediately\nSpark Streaming\n\nStreaming data gets input into model thats in Spark as it becomes available, Spark outputs predictions to S3 bucket that is used by an app.\n\nRefreshing the app, reads data from the S3 bucket and therefore has the latest predictions\n{sparklyr} + {tidymodels} has many models available.\n\nSee Apache, Spark &gt;&gt; Streaming\nBuild low-latency and scalable ML model prediction pipelines using Spark Structured Streaming and MLflow\n\npy example; shows how to include MLflow model versioning/tracking\n\n\nUse a message queue architecture\n\n\nPerform mini-batch predictions - pull a few requests at a time and make predictions\nTools: redis, RabbitMQ, ActiveMQ, pm2\n\nFor loads that need robust message queue stability, and multiple models working on the same data â€” you can use multiple consumer groups features in Kafka.\n\nClearly mark topics (?) for prediction requests to this model and the outputs of the model\nOnce the model makes a prediction, youâ€™ll need to have a sync service that does something with the prediction (e.g.Â update the database)\n\n\nPredictions are made on periodic batches of data\n\nCreate a long-running task\n\n\nMount the elastic volume to the container when it comes up\n\n\n\nInclude the model in the container build\n\nThis is better than downloading a model each time you want to scale up\n\ne.g.Â hugginface model or a pickled model from a S3 bucket or other storage\n\nDrawback is the large size of the container to push and load from the repo\nMake sure that the model gets downloaded in the right place, especially if youâ€™re using a non-root user in the Dockerfile\nOptions\n\nDownload the model in the Dockerfile\n\nThe model is downloaded when the container is built and is included in the image pushed to the repo.\nWhen the container instance is created, the model is right there, and we can load it faster.\n\nInclude the model in the code repo\n\nWhen you copy code into the container, the model get copied as well\nAllows you to version your model.\nTo overcome the file size limitation that most repos have, use Git LFS.\n\nStore in a volume\n\nSave your model in a block store, so that it can be mounted as a volume to your container\nNeed to provision the container memory (as the model will be loaded from the file system into memory)\nMaking the volume storage part of your model versioning operations.\nHaving to managing different paths for different environments complicates things.\n\n\n\nLoad model during health check\n# Flask\n@app.route(\"/health_check\", methods=[\"GET\"])\ndef health_check():\nÂ  ZeroShotTextClassifier.load()\nÂ  return jsonify({\"success\": True}), 200\n\nOrchestration systems periodically hit a /ping or /health_check endpoint to check if the container started correctly and/or if the database connection is alive.\nOnly once the health check is successful does the load balancer start sending the container the HTTP traffic.\nMake sure you let the DevOps team know the time itâ€™ll take to get the model warmed up so that they can configure the health check accordingly. Also, do quick mental arithmetic to know how long the check would have taken if the model was being downloaded. (see below)"
  },
  {
    "objectID": "qmd/production-deployment.html#sec-prod-deploy-batch",
    "href": "qmd/production-deployment.html#sec-prod-deploy-batch",
    "title": "Deployment",
    "section": "Batch Prediction",
    "text": "Batch Prediction\n\n\nPeriodically generate predictions offline and store in a db with fast reading capabilities\nMisc\n\nNotes from ML Latency No More\n{butcher} can reduce model storage sizes\n\nInputs\n\nEntity: prediction service receives a known entity ID (e.g.Â product_id, movie_id, order_id, device_id, etc)\nFeature combo: the prediction service receives a combo of feature values.\n\nAlternative when data privacy is a concern\n\nYouâ€™ll need a static hashing method to generate a key for each combination of values\n\n**order is important here: a hash(country, gender, song_category) will differ from a hash(song_category, country, gender)**\nStore a prediction for each key\n\n\nExample: Ad targeting\n\n\nStrategies for High Cardinality Entities/Feature Combos High cardinality entities or feature combos can be expensive to compute\n\nTop-n\n\nEntities: generate predictions for top-n entities (e.g.Â top viewed, most purchased)\n\nFor the remaining entities, you make the client wait while you call the model directly instead of pulling the predictions from the prediction store\n\nFeature Combos: generate predictions for top-n most frequent feature combinations\n\nFor the remaining feature combinations, you make the client wait while you call the model directly instead of pulling the predictions from the prediction store\n\n\nSimilarity Matching\n\nProcess\n\nTrain a model on the entity (e.g productsâ€™) similarity using entity-user interactions or entity-entity co-location.\nExtract the embeddings of the entities.\nBuild an index of the embeddings using an approximate nearest neighbor method.\nLoad the index in the ML prediction service.\nUse the index at prediction time to retrieve the similar entity IDs.\nPeriodically update the index to keep things fresh and relevant.\n\nIf the index is too large, or the prediction latency is too high, reduce the embedding size to get a smaller index.\n\nReduce until modelâ€™s prediction metric falls below the acceptable threshold or the latency (aka satisficing metric) decreases to an acceptable level.\n\n\nReduce the number of features included in the combination until modelâ€™s prediction metric falls below the acceptable threshold or the latency (aka satisficing metric) decreases to an acceptable level.\n\nTips\n\nThe DB will have lots of rows, but only a few columns. Choose a DB that handles single key lookups well.\nKeep an eye on the categoriesâ€™ cardinality and the number of keys generated. If you have a batch job doing this, then monitor the cardinality and raise alarms if you get a spike in new categories to count. That will prevent blowing up the DB lookup latency.\nContinuous values are going to need to be binned. Thatâ€™s going to be a hyper-parameter that you need to tune.\nAny technique that can be used to lower the cardinality of categories is your friend. Lower the cardinality as much as your optimizing metric allows."
  },
  {
    "objectID": "qmd/production-deployment.html#sec-prod-deploy-online",
    "href": "qmd/production-deployment.html#sec-prod-deploy-online",
    "title": "Deployment",
    "section": "Online Prediction",
    "text": "Online Prediction\n\n\nMisc\n\nNotes from ML Latency No More\n{butcher} can reduce model storage sizes\n\nUse cases for online prediction\n\nGenerating ad recommendations for an ad request when the browser loads a page.\nOptimizing a bid in a competitive real-time bidding ad marketplace.\nPredicting if a critical piece of equipment will fail in the next few seconds (based on sensor data).\nPredicting the grocery delivery time based on the size of the order, the current traffic situation, and other contextual information about the order.\n\nAsync Predictions\n\n\nthe caller will ask for a prediction, but the generated predictions will be delivered later\nOptions\n\nPush: caller sends the required data to generate the predictions but does not wait for the response\n\nExample: When using your credit card, you donâ€™t want to wait for a fraud check response for every transaction. Normally, the bank will push a message to you if they find a fraudulent transaction.\n\nPoll: caller sends the required data and then periodically checks if a prediction is available. The models are set up to generate predictions and store the predictions in a read-optimized low latency DB\n\n\nSynchronous Online Predictions\n\nBasic networking tasks\n\nSecuring the endpoint\nLoad balancing the prediction traffic\nAuto-scaling the number of ML gateways\n\n2 levels need to be optimized in order to reduce latency\n\nPrediction construction - This is where you reduce the time it takes a model to construct predictions from a fully formed, well-behaving, enriched and massaged prediction request.\n\nRemove supporting components such as logging, hooks, monitoring, transformation pipelines, etc. that are used to help train, evaluate, and debug the model during development\nChoose the model that balances the prediction metric and the satisficing metric (e.g.Â 50ms latency)\n\nâ€œSatisficingâ€ refers to the context in which the model will be served.\n\nIs the model going to fit on my device in terms of storage size?\nCan the model run with the type of CPUs on the device? Does it require GPUs?\nCan the feature preprocessing finish within specific time bounds?\nDoes the model prediction satisfy the latency limits that our use case requires?\n\nIn general, the lower the complexity of the model and the fewer feature, the faster the response time\n\nTrim the number of levels in a tree model\nTrim the number of trees in a random forest and gradient boosting tree model\nTrim the number of layers in a neural network\nTrim the number of variables in a logistic regression model\n\n\nSelect the proper hardware to generate the predictions at the right price/latency point\n\nTry to use custom hardware, such as GPUs or specific inference chips.\nTry to use custom compilation methods to optimize the model components.\n\n\nPrediction serving - Includes any pre-computing, pre-processing, enriching, massaging of input prediction events as well as any post-processing, caching, and optimizing the delivery of the output predictions.\n\nThis is where the most of the latency can be reduced\nStructuring the supporting historical datasets in quick-enough data stores and computing real-time contextual dynamic features\nInput feature types\n\nUser-supplied features: These come directly from the request.\nStatic reference features: These are infrequently updated values.\nDynamic real-time features: These values will come from other data streams. They are processed and made available continuously as new contextual data arrives.\n\nStatic features\n\n\nProcess\n\nThe client sends an entity ID that needs predictions. For example, recommend a list of movies for user_id=â€œxâ€.\nThe entity is enriched/hydrated by the attributes present in the feature lookup API.\nThe ML gateway then consolidates the input features into a prediction request forwarded to the ML Model API.\nWhen the ML Model API returns predictions, the ML gateway post-processes them and returns them to the client.\n\nâ€œSinglesâ€ are numeric values (e.g.Â number of rooms in a house, or the ID for the advertiser associated with a campaign)\nâ€œAggregatesâ€ are summary stats (e.g.Â median house price in the zip code or the average ad budget of campaigns targeting a specific audience segment)\nâ€œStatic Features Storageâ€ - optimized for for singleton lookup (i.e.Â read) operations\n\nML gateway fetching pattern for static features is: â€œI need a single row with one column for each of the features of customer X.â€\nThe typical data warehouse is not optimized for low latency queries. Instead, data warehouses are optimized for large aggregations, joins, and filtering on extensive star schemas\n\nBatch jobs that update static features cost quite a bit of cash if you run it every 15 minutes. So you\n\nSolution:\n\nExponentially lower the frequency of the update until the modelâ€™s prediction metric falls below the acceptable threshold.\nThen raise the frequency to its previous value.\nAutomate that.\n\n\n\nDynamic Real-time Features\n\n\nProcess\n\nFresh events land in your favorite messaging system. Then, they get picked up by the streaming pipeline. The generated features, probably aggregated over time windows, land in a low-latency feature store. Exiting features are updated with fresh values.\nThe streaming pipeline generates the predictions using the features and the model API.\nThe ML gateway receives client prediction requests. The gateway then checks if there are any predictions in the database, or the messaging system. Then the gateway returns them to the client. Finally, it optionally push them to the messaging system if some other system downstream is interested. (e.g.Â governance team)\n\nâ€œlow latency feature storeâ€ should have fast read and write abilities"
  },
  {
    "objectID": "qmd/production-deployment.html#sec-prod-deploy-aws",
    "href": "qmd/production-deployment.html#sec-prod-deploy-aws",
    "title": "Deployment",
    "section": "AWS",
    "text": "AWS\n\nContainers\n\nIf working alone or extra flexibility isnâ€™t necessary â€“&gt; ECS\nIf working with a team and extra flexibility is necessary â€“&gt; Kubernetes\n\nNo Containers\n\nProduction â€“&gt; Lambda\nNot production â€“&gt; EC2\n\nEC2 Solution for users who need to do quick demos or just showcase something temporarily\n\nPros\n\nQuick and dirty\nCheap (potentially free)\nEasy setup/teardown\nLittle to no infrastructure/networking experience required\n\nCons\n\nNot very scalable\nNot production grade\nLittle to no automation\nNot robust to errors\n\n\nLambda\n\nNeed to work with some other services such as API Gateway, but the setup will be far more robust than deploying your app to a standalone EC2 machine.\nFor production, this would probably be the cheapest option. Pros:\n\nProduction grade\nGreat for small simple apps/functions\nServerless (extremely cheap) Cons:\nLess flexible than other solutions\nRequires knowledge of additional AWS services\n\n\nKubernetes\n\nautomates many production-level concerns such as load balancing or autoscaling\nHave to deal with deploying an application and managing a cluster which can prove no simple task\nKubernetes networking is complex, and requires lots of experience to understand and operate in depth.\nWhile a Kubernetes cluster may also seem cheaper than a more managed ML solution, a poorly managed cluster can lead to even worse unexpected monetary costs. Pros:\n\nVery scalable\nGood amount of automation\nProduction grade\nLots of community support\nHighly flexible\nExperience with a popular framework and lower-level CS! Cons:\nPotentially lots of work\nRisky for beginners\nIn some cases, just straight up unnecessary\nLots of setup require for feature parity with managed services\n\n\nECS\n\nIn terms of flexibility, it sits in between Lambda and the highly flexible Kubernetes.\nPros:\n\nSignificantly easier setup than Kubernetes\nMore features out of the box\nEasier to manage as an individual developer (with container experience)\nFirst-class support for containerized applications\n\nCons:\n\nLess granular controls\nPotentially more expensive\n\n\nSagemaker Endpoint\n\nFeels like creating deployments locally on your machine\nComes with a whole suite of services that empower users to build and deploy production ready ML apps with all the bells and whistles youâ€™d have to manually configure for other options Pros:\n\nFirst-class machine learning support\nManaged infrastructure and environments\nProduction grade, scalable Cons:\nPotentially more expensive than some other solutions\nPotentially less flexible"
  },
  {
    "objectID": "qmd/production-deployment.html#sec-prod-deploy-kub",
    "href": "qmd/production-deployment.html#sec-prod-deploy-kub",
    "title": "Deployment",
    "section": "Kubernetes",
    "text": "Kubernetes\n\nCanary Deployment\n\n\nTo start, 90% of client traffic is directed to the old app, and 10% is directed to the new app.\n\nkind: Service\nspec:\nÂ  Â  selector:\nÂ  Â  Â  Â  app: my-app\n\nVersion isnâ€™t specified in the selector so traffic is sent to all pods regardless of the version label which will allow Service route traffic to both app deployments\n$ kuberctl apply -f my-app-v2.yaml\n$ kuberctl scale deploy/my-app-v2 -replicas=10\n\nOnce the new app is deemed stable, 100% of the traffic can be routed to the new app\n$ kuberctl delete -f my-app-v1.yaml\nEach connection is treated independently. So every client may be exposed to the new deployment\n\nIn Service, the session affinity field can be set to client ip if you want a clients first connection to be determinate for all future connections, i.e.Â each client only experiences 1 version of the app.\nIstio, https://istio.io/latest/ , can be used to more finely control traffic.\n\nOther methods: Shadow testing, A/B testing\nRolling back deployment\n$ kuberctl rollout undo deployment\n{DEPOYMENT_NAME}\n\nReverts deployment back to previous revision\n\n$ kuberctl rollout undo deployment\n{DEPOYMENT_NAME} --to-revision=2\n\nReverts deployment back to a specific revision (e.g.Â 2)\n\n$ kuberctl rollout history deployment\n{DEPOYMENT_NAME} --to-revision=2\n\ninspect diff of a specific revision\n\nby default, 10 revisions are saved"
  },
  {
    "objectID": "qmd/production-development.html#sec-prod-dev-misc",
    "href": "qmd/production-development.html#sec-prod-dev-misc",
    "title": "Development",
    "section": "Misc",
    "text": "Misc"
  },
  {
    "objectID": "qmd/production-development.html#sec-prod-dev-plan",
    "href": "qmd/production-development.html#sec-prod-dev-plan",
    "title": "Development",
    "section": "Planning",
    "text": "Planning\n\nNotes fromÂ Automating the end-to-end lifecycle of Machine Learning applicationsÂ andÂ https://www.thoughtworks.com/insights/blog/curse-data-lake-monster\n\nQuestions\n\nWhat are your priorities and burning issues? â€” prioritize the use cases the data platform should resolve for you promptly, which can generate immediate business value.\nWhat are your constraints? â€” think and quantify everything â€” from software and human resources to time and effort required, level of internal knowledge, and monetary resources.\n\nGoals\n\nStart with quick wins â€” donâ€™t dive directly into data science and machine learning model development, but instead start with quick win use cases (usually descriptive statistic use cases).\nBe realistic â€” when setting the data platform goals, the important thing is to be realistic about whatâ€™s feasible to achieve given current constraints.\n\nComponents\n\nBuilding data pipelines â€” properly developed data pipelines will save you money, time, and nerves. Developing pipelines is the most crucial part of the development, i.e.Â that your pipelines are properly tested and deliver new data to business users without constant brakes due to various data and system exceptions.\nOrganizing and maintaining the data warehouse â€” with the new data sources, a data warehouse can quickly become messy. Implement development standards and naming conventions for a better data warehouse organization.\nData preprocessing â€” think about acquiring data preprocessing tool(s) as early as possible to improve the dashboard performance and reduce computational costs by de-normalizing your datasets.\nData governance and security â€” set the internal standards and data policies on the data lifecycle (data gathering, storing, processing, and disposal)."
  },
  {
    "objectID": "qmd/production-development.html#sec-prod-dev-check",
    "href": "qmd/production-development.html#sec-prod-dev-check",
    "title": "Development",
    "section": "Checklist",
    "text": "Checklist\n\nMisc\n\nIf youâ€™re using database for cross-validation splits, make sure the correct order of data is maintained by making sure a logic to sort or keep the ordering of data is set.\nNotes From:\n\nHow to Build a Data Product that Wonâ€™t Come Back to Haunt You\n\n\nBroad strokes\n\na way to ingest data for training\na way to ingest data for prediction\na place to output the predictions\na place to train the model (a machine)\nsome insight into what the process is doing: logging\nnotifications to the right people when something goes wrong: alerting\n\nMore finely grained\n\nlintr run / all probs fixed on model-related code\nModel can be run independently (single R file)\nSign-off from engineering on model input / output / purpose\nCode review\nAlgorithm review\nStandardized headers in R file that describe input, output, data, algorithm, description\nCode in Bitbucket\nAll acceptance criteria are met\nModel validation documented\nModel validation review meeting held\n\nSelf-Explanatory\n\nThe more self-explanatory the data product is, the easier it will be to support future users and maintain by future developers. Data Inputs:\n\nIs the data I am using well-documented?\nAm I using the data in line with its intended use?\nAm I reusing as much of the existing engineering pipeline as possible to lower overall maintenance effort and ensure that my use is consistent with other uses of the same data items? Pipeline:\nAre the requirements, constraints and implementation of the data process documented well enough that someone else who may be taking over maintenance from me can understand it? Final Product:\nIs the report or dashboard presented in a way that is easily accessible and understandable even to people who will be viewing it, and without explanation from me?\nAm I using vocabulary and visualizations that the end user understands, and that they understand in exactly the same way I do?\nAm I providing good supporting documentation and information in a way that is intuitive and easily accessible from the data product itself?Data Inputs: Trustworthy\n\nTrust is hard to gain and easy to lose. Usually, when you first deliver a data product, there is a lot of hope and some trust. Data Inputs:\n\nAm I connected into the input data in a way that is well-supported in the production pipeline?\nDo I have explicit buy-in from those maintaining the data sets I am using?\nIs this input data likely to be maintained for a significant time in the future?\nWhen might I need to check back to be sure the data is still being maintained?\nHow do I report any problems that I see in the input data?\nWho is responsible for notifying me of issues with the data?\nWho is responsible for fixing those issues? Pipeline:\nHave I set up a regular schedule to review the data and report to ensure that the data pipeline is still functioning well and the report is conforming to the requirements?\nWhat are the conditions under which this report should be marked as deprecated?\nHow can I ensure that the user is informed should the report become deprecated? Final Product:\nHow does the user know when they can trust the report is accurate and up to date?\nIs there an efficient and/or automated way of communicating possible problems to the end user?\nIs there a clear and accessible process in place for the user to report concerns with the data or report, and for the user to be notified of any remediation processes in place? Adaptable\n\nData inputs and shapes change over time. In addition to monitoring relevant issues, when someone notices a problem, you need to be set up to react and formulate a solution without undue complications. Data Inputs:\n\nWhat features in the input data am I depending on for my analysis?\nHow will I know if those features stop being supported, are affected by a schema change, or change shape in a way that may affect my analysis?\nHow will I know if the scale of the data grows to a point where I need to refactor my process in order to keep up with my productâ€™s requirements for freshness? Pipeline:\nHave I set up a regular schedule for re-examining the requirements to ensure that I am still producing what the user needs and expects?\nWhat is the process for users to indicate changes in requirements, and for those changes to be addressed?\nWhat is the process for refactoring and retesting the data pipeline when the inputs change in some relevant way? Final Product:\nIs the product or report set up in a way that it is easy to request a change and/or a new feature? Reliable\n\nEnsure that the different parts that make up your pipeline are reliable and the processes that orchestrate those parts are robust. Data Inputs:\n\nDoes my process fit well into the data practices and engineering production system in my organization?\nDo I have an automatic notification system in place to monitor the availability, freshness and reliability of my input data? Pipeline:\nIs each stage in my pipeline executing and completing in a timely manner?\nIs there drift in the processing time and/or amount of data being processed at any stage that may indicate a degradation in pipeline function? Final Product:\nWho is responsible for the ongoing monitoring, reviewing, troubleshooting, and maintenance of the dashboard itself?\nAre responsibilities and procedures clearly in place for reporting and resolving issues internally?"
  },
  {
    "objectID": "qmd/production-development.html#sec-prod-dev-doc",
    "href": "qmd/production-development.html#sec-prod-dev-doc",
    "title": "Development",
    "section": "Documentation",
    "text": "Documentation\n\nModel / architecture selection\nHyper-parameters\nRough description of the data (origin, size, date, featuresâ€¦)\nResults (ie: precision, recall, f1â€¦).\nA link to a snapshot of data (if possible)\nCommentary and learnings\nModels\n\nmodel object, training, testing data\nModel name and description (origin, goal, size, date, featuresâ€¦)\nDevelopment stage (Implemented for use, under development or recently retired)\nDiagnostic Metrics\nModel assumptions\nModel limitations\nThe model owner: the model owner is responsible for ensuring that the models are appropriately developed, implemented and used.\nThe model developers: model developers follow the lead of the model owner to create and implement the Machine Learning models.\nThe model users: model users can be either internal to the business or external. For both cases it is important to clearly identify their needs and expectations. They should also be involved in the model development and can help validate the modelâ€™s assumptions.\nOther comments\n\nWhat was learned\n\n\nProjects\n\nAn introduction\nA description of the problem\nA description of the data set\nThe methodology that you used:\nMethodology to prepare the data\nMachine Learning / statistical analysis approach taken to achieve the results\nThe Results\nRecommendations based off the results"
  },
  {
    "objectID": "qmd/production-development.html#sec-prod-dev-mat",
    "href": "qmd/production-development.html#sec-prod-dev-mat",
    "title": "Development",
    "section": "Levels of Maturity",
    "text": "Levels of Maturity\n\nFrom 5 Levels of MLOps Maturity\n\nAuthorâ€™s recommendations based on reading Googleâ€™s and Microsofts docs on MLOps\nAdditional Thoughts\n\nYou should monitor your model as soon as business decisions are taken based on its output, regardless of maturity level. At the very least, performance monitoring to be employed. In addition to determining model failure, results can be used to calculate ROI i.e.Â business value.\n\nLevel 1- Manual\n\n\nData processing, experimentation, and model deployment processes are entirely manual.\nRelies heavily on skilled data scientists, with some assistance from a data engineer to prepare the data and a software engineer to integrate the model with the product/business processes if needed.\nUse Cases\n\nEarly-stage start-ups and proof of concept projects\nSmall-scale ML applications with minimal data dependencies and real-time requirements â€” applications with limited scope or a small user base, like a small online fashion store.\nAd hoc ML tasks â€” In specific scenarios like marketing campaigns, one-time ML tasks or analyses may not require full MLOps implementation.\n\nLimitations\n\nLack of monitoring system â€” thereâ€™s no visibility on the modelâ€™s performance. If it degrades, it will have a negative business impact.\nNo frequent retrains of production models. Releases of the models happen only a couple of times per year.\nLimited documentation and no versioning\n\n\nLevel 2 - Repeatable\n\n\nDevOps aspect added to the infrastructure by converting the experiments to the source code and storing them in the source repository using a version control system like Git.\nData catalog â€” a centralized repository that includes information such as data source, data type, data format, owner, usage, and lineage. It helps to organize, manage, and maintain large volumes of data in a scalable and efficient manner.\nAdd Automated testing â€” unit tests, integration tests, or regression tests. These will help us deploy faster and make things more reliable by ensuring our code changes donâ€™t cause errors or bugs.\nStill lacks a monitoring system\n\nLevel 3 - Reproducible\n\n\nTwo key reasons why reproducibility is crucial: troubleshooting and collaboration.\nNew Features\n\nAutomated training pipeline â€” handles the end-to-end process of training models, from data preparation to model evaluation.\nMetadata store â€” a database is a way to track and manage metadata, including data sources, model configurations, hyperparameters, training runs, evaluation metrics, and all the experiments data.\nModel registry â€” is a repository to store ML models, their versions, and their artifacts necessary for deployment, which helps to retrieve the exact version if needed.\nFeature store â€” which is there to help data scientists and machine learning engineers to develop, test, and deploy machine learning models more efficiently by providing a centralized location for storing, managing, and serving features. It also can be used to track the evolution of features over time and preprocess and transform features as needed.\n\n\nLevel 4 - Automated\n\n\nNew Features\n\nCI/CD â€” where Continuous Integration (CI) ensures integration of code changes from different team members into a shared repository, while Continuous Deployment (CD) automates the deployment of validated code to production environments. This allows for rapid deployment of model updates, improvements, and bug fixes.\nA/B testing of models â€” this model validation method involves comparing predictions and user feedback between an existing model and a candidate model to determine the better one.\n\n\nLevel 5 - Continuous Improvement\n\n\nModel is automatically retrained based on the trigger from the monitoring system. This process of retraining is also known as continuous learning. The objectives of continuous learning are:\n\nCombat sudden data drifts that may occur, ensuring the model remains effective even when faced with unexpected changes in the data.\nAdapt to rare events such as Black Friday, where patterns and trends in the data may significantly deviate from the norm.\nOvercoming the cold start problem, which arises when the model needs to make predictions for new users lacking historical data\n\n\n\nGoogle (E-Commerce Analysis Pipeline)\n\nInitial Stage\n\n\nData collection layer: presents the most relevant data sources that had to be initially imported to our data warehouse.\nData integration layer: presents cron jobs used for importing e-commerce datasets and the Funnel.io platform for importing performance marketing datasets to our data warehouse.\nData storage layer: presents the selected data warehouse solution, i.e.Â BigQuery.\nData modelling and presentation layer: presents the data analytics platform of choice, i.e.Â Looker.\nResources\n\n2 tools â€” BigQuery and Looker,\n6 people â€” for managing data pipelines (cron jobs + Funnel.io platform) and initial analytical requirements (data modelling),\n3 months â€”from acquiring Google Cloud to presenting the first analytical insights.\n\n\nAdvanced Stage\n\n\nCloud storage â€” for storing our external files in Google Cloud.\nCloud Run â€” used for deploying analytical pipelines developed in Python and wrapped as Flask applications.\nGoogle Functions â€” for writing simple, single-purpose functions attached to events emitted from the cloud services.\nGoogle Workflows â€” used for orchestrating connected analytical pipelines that needed to be executed in a specific order.\nGoogle Colab â€” for creating quick PoC data science models.\nResources\n\nFrom 2 to 7 tools â€” from using only BigQuery and Looker, we started using Cloud storage, Cloud Run, Google Functions, Google Workflows, and Google Colab.\nFrom 6 people in two teams (IT and Business Development) to 8 people in one team (Data and Analytics) â€” the Data and Analytics team was established and now has complete ownership over all data layers.\nFrom 3 months for creating initial insights to 2+ years of continuous development â€” we are gradually developing more advanced analytical use cases."
  },
  {
    "objectID": "qmd/production-development.html#sec-prod-dev-arch",
    "href": "qmd/production-development.html#sec-prod-dev-arch",
    "title": "Development",
    "section": "Architecture Examples",
    "text": "Architecture Examples\n\nEmbedded modelâ€ deployment. (i.e.) Take model object, insert into app, and deploy.\n\nClean\n\nremove values not pertinent to analysis\ncolumn names, types\ndummy variables, other categorical coding\nvariable transformations\nfeature engineering\n\ncan this at least be partially done with spark?\n\nJoin dataframes\nsave to file (.csv, rds, .feather, .fst, .parquet)\n\nTransfer data file to a storage system.Â \n\nIf project subject matter isnâ€™t company-wide, then transfer the cleaned, pertinent subset of data to a data mart. (A data mart is focused on a single functional area of an organization and contains a subset of data stored in a Data Warehouse or Lake)\nMay have data from multiple sources (lakes, warehouses, etc)\nDoes apache airflow coordinate multiple sources? see bkmkâ€™ed video\n\nuse spark to feed data to model\n\n\nModel Selection experiments in git branches. MLflow ($?) can monitor experiments and their metrics\n\nresearch potential models\npick a base model for comparison\ntune and train models\ncross-validation\nchoose model\n\nVersion control pipeline (DVC, Git, etc) - DVC (dvc.org) (open source) allows for version control of the large files like data sets and model objects. Works is conjunction with Git and cloud storage platforms.\n\npull data\nvalidate data\nsplit data to training and validation sets (and test set?)\ntrain model\ntest with validation set\nvalidate model within metric thresholds\nvalidate model fairness and not biased for specific variable values (race, gender, etc)\nmodel, training data, and metrics need to be linked, i.e.Â version controlled\ncommunicate metrics (See FluentD service below)\nwrite model object to file\nwrite training set (or maybe all data sets?) to file\ndvc push files to storage\n\nEmbed model obj + application (shiny app) in docker image\n\ndvc pull model file\nthe Docker image becomes our application+model artifact that gets versioned and deployed to production\n(before or after creating docker image?)Perform integration test with validation data: make sure the model produces the same results inside the application as in the modelling pipeline\npush image\n\nDeploy imageÂ to a Kubernetes production cluster\nCI/CD Pipeline skeleton can be orchestrated through a GoCD ($?)\nMonitor Cluster performance using the EFK stack which is composed of three main tools:\n\nElasticsearch: an open source search engine.\nFluentD: an open source data collector for unified logging layer, i.e.Â for log ingestion and forwarding\nKibana: an open source web UI that makes it easy to explore and visualize the data indexed by Elasticsearch.\nÂ Add code to model script to log the model inputs and predictions as an event in FluentD\nOther tools: Logstash (alternate to FluentD), Splunk\n\n\n\n\n\n\n\n\n\nDescribed in An introduction to Monzoâ€™s data stack\nâ€œanalytics events processor + shipperâ€ is something custom they built to â€œsanitizeâ€ (i.e.Â remove personally identifiable information (PII)) events data\nI think NSQ is like Kafka and itâ€™s there for redundancy(?)\n\nPaper: Machine Learning Operations (MLOps): Overview, Definition, and Architecture"
  },
  {
    "objectID": "qmd/production-ml-monitoring.html#stuff-i-havent-organized",
    "href": "qmd/production-ml-monitoring.html#stuff-i-havent-organized",
    "title": "56Â  ML Monitoring",
    "section": "56.1 Stuff I havenâ€™t organized",
    "text": "56.1 Stuff I havenâ€™t organized\n\n\narticles\n\nhttps://eugeneyan.com/writing/practical-guide-to-maintaining-machine-learning/?utm_campaign=Data_Elixir&utm_medium=social\nhttps://towardsdatascience.com/the-playbook-to-monitor-your-models-performance-in-production-ec06c1cc3245\nInferring Concept Drift Without Labeled Data\n\nExample: RStudio Connect and {pins} (article)\n\nDeploy a model as a RESTful API using Plumber\nCreate an R Markdown document to regularly assess model performance by:\n\nSending the deployed model new observations via httr\nEvaluating how the model performed with these new predictions using model metrics from yardstick\nVersioning the model metrics using the pins package\nSummarize and visualize the results using flexdashboard\n\nSchedule the R Markdown dashboard to regularly evaluate the model and notify us of the results\n\nMisc\n\nDL Monitoring points\n\n\nData Drift Monitor and SoftMax Monitor (i.e.Â monitor prediction metrics) are typical of any monitoring system\nModel Monitor is an auxiliary model (or models) trained to recognize basic patterns that emerge in the baseline operations of the primary model.\n\nExample: monitor the values of normalized outputs of various layers within the model.\n\nThese values could be input to a neural network trained to distinguish the patterns of normal operation from out-of-range examples included in the training set. This model monitor could then flag potential drift during operation of the primary system\n\n\n\nIf predictions result in an intervention, it will be difficult to determine drift.\n\nExample: patient is determined to be at high risk by a model, gets treated by clinician, and lives. Was this a false positive by the model or was the model correct and the reason for the patient living is the clinicianâ€™s intervention.\n\nOther tracking\n\nNumber of alerts triggered by model\n\nWork with users to minimize false positives and hence unnecessary alerts. Otherwise model alerts might be treated as the boy that cried wolf and will be ignored or taken less seriously.\n\nInterventions after a trigger\n\nThis will let you know if your model predictions are being adhered to\nExample: model that predicts a patient is at high risk of death and triggers an alert to a clinician\n\nPart of the standard protocol for intervening after a patient is labelled high risk is to perform additional vitals measurements. The number of vitals measurements for a patient are recorded, so this metric can be used a proxy. If there is an increase in a high risk patientâ€™s vital measurments, then the data team can infer that itâ€™s modelâ€™s alerts are being adhered to.\n\n\nAutomated Retraining Triggers\n\nRequirements:\n\nThe number of models in production is limited so retraining costs are low.\n\nRetail, logistics, etc. may involve thousands of related (e.g geography) models\n\nThe frequency of triggering events is rare.\nThere are no strict requirements on model availability.\n\n\nPredicting model drift\n\nImportant, so:\n\nSLA requirements related to model availability can be met.\nAnalysis can be done on such cases for root cause analysis in pre-production settings.\nComputational requirements for retraining can be calculated in advance knowing the frequency and prevalence of these events across the models.\n\nsurvival analysis to model performance decay in predictive models.\n\nData requirements:\n\nSufficient historical scoring data is available on 1 model\nOr scoring data is on a large number of closely related models\n\nOnce you have the probability of survival distribution, you can could use the 95th percentile survival time to trigger a model retrain to ensure that the model degradation is below the performance threshold with the specified probability\n\n\nModel calls\n\nIf model usage drops or becomes irratic, it could be a signal something is wrong\nOnly useful for frequently used apps\nDepending on the model environment, you might want to check requests and responses separately.\n\nWas the model not asked (e.g., because a recommendation widget crashed) or failed to answer (e.g., the model timed out and we had to use a static recommendation instead)? The answer would point to where you should start debugging.\n\n\nCompetitor feature distributions\n\nWould be useful in diagnosing if the changes in data are just your company or happening to the industry sector\n\nSHAP for Drift Detection: Effective Data Shift Monitoring Feeds distribution of SHAP values into a logistic regression, then applies ks test on probability predictions for Y==1, Y==0\nMeasuring Precision and Recall\n\nActions taken by model\n\nSoft Actions - Events are flagged for humans to take action\nHard Actions - Events are flagged and actions are automatically taken by an algorithm\n\nPrecision: ratio of positive predictions that are correct (Precision = TP / (TP + FP))\n\nSoft Action models - Record human decisions that are made on the flagged events\nHard Action models - Set up a control group: for a fraction of the volume, let a human decide, instead of your model. Then the precision in the control group is an estimate of the precision of your model\n\nRecall: ratio of all positive events that the model is detecting (Recall = TP / (TP + FN))\n\nNeed to audit your negative predictions to count the number of False Negatives (FN)\nIssue: Imbalanced classes (e.g.Â fraud model)\n\nIf recall is expected to be 99.9%. That means that, on average, youâ€™d need to audit at least 1K negatives per day just to find just one false negative, and even more for that number to be statistically meaningful.\n\nSolution: Importance Sampling (Google blog post)\n\nSample negative predictions based on their model scores\nSample more heavily from datapoints with high model scores because thatâ€™s where we expect most of our false negatives to be\n\n\n\n\nIf you are dealing with a high-risk domain, it is best to design model fallbacks from the get-go in case of model failure from broke code, concept drift, etc.\n\nExamples\n\nMake a human do what the model was doing (e.g.Â insurance claims processing, manufacturing quality control, or sales lead scoring)\nRule-based approach: You can often design a set of rules or heuristics that will be less precise but more robust than a rogue model. (e.g.Â simply show the most popular items to all customers)\n\n\nNotes from https://blog.anomalo.com/effective-data-monitoring-8bce3ddf87b4\n\nAll the stuff that platform, Anomalo, does\n\nIdentifying issues and retraining models will minimize losses\n\ndeviations between baseline and production distributions\nfeature and cohort performance\n\nML transparency regulation requires it to understand why a model made a particular prediction to ensure broader governance, fairness, and mitigate bias\n\nKeep track of predictions by group\n\nDynamic testing can prevent false positives/negatives\n\nUse a forecasting method to generate prediction intervals. If data values for a variable fall outside the PIs, then a data quality alert is triggered\n\nLimiting tests to the most recent data can reduce data warehouse costs\nCreate application (or subscribe to a service) that enables a user to adjust commonly changed rules/thresholds without writing code\n\nShould be well documented\nThe types of changes users often make include:\n\nWidening the expected range for a data outcome\nNarrowing the scope of a rule using a where SQL clause\nWaiting for updated-in-place data to arrive before applying a rule\nChanging thresholds for machine learning alerts\neasy reversion to prior state\n\nso will need a log/versioning\n\n\n\nFor data that is critical. Checks should be included in the pipeline\n\nIf a check does fail, you could run automated tasks to fix the bad data, abort the remainder of the DAG (sometimes, no data is better than bad data), or quarantine bad records using SQL produced in the API to query for good and bad data.\n\nIf you have hundreds of variables, managing data quality rules for each column may be untenable.\n\nUse unsupervised data monitoring\n\nno increase in NULL values\n\nA constrained model looking for significant increases in NULL values\n\nno anomalous records\n\nmachine learning algorithm, which identifies changes in continuous distributions, categorical values, time durations, or even relationships between columns\nscore severity of anomaly somehow\n\n\n\nRoute alerts (slack) for a particular table only to teams that regularly use or maintain that table\n\nAs alerts arrive, they can use emoji reactions to classify their response to alerts. Common reactions include: * âœ… the issue has been fixed * ğŸ”¥ an important alert * ğŸ› ï¸ a fix is underway * ğŸ†— expected behavior, nothing needed * ğŸ‘€ under review\n\nAlerts should have contextual information\n\nExample\n\nWhy does this alert matter?\nWhat # and % ofÂ user_idÂ values are affected?\nHow often has this alert failed in the recent past?\nWho configured this alert, and why?\nWhat dashboards or ML models depend onÂ fact_table?\nWhat raw data source contributedÂ user_idÂ toÂ fact_tableÂ ?\n\nInclude image with a row with a value that triggered the alert and a row that has a valid value\n\nRoot cause analysis\n\nUses statistical method to find out where the issue is\n\ngroup_by(cat) %&gt;% summarize(pct_bad = â€¦ whatever)\npermute rows of anomalous column and see where thereâ€™s a relationship change between anomalous column/rows and other columns. The columns where the relationship changes might be potentially involved in the anomalous values of the permuted column\n\nclustering or correlation?\n\n\n\nGet user feedback on alerts (useful or not useful?)\n\nGround Truth Latency - how long does it take to know if your prediction was right\n\nRealtime/near realtime\n\nstocks, gambling, food delivery time estimates, digital advertising\nAble to determine whether thereâ€™s an issue with your model almost immediately\n\ndelayed\n\ncredit card fraud\nRequires monitoring of proxy metrics (metrics that are associated with the ground truth) until ground truth arrives\n\n\nProblematic ground truth types\n\nBiased\n\nExample: loan default model\n\nThe ground truth only includes results from customers who were approved for a loan. So thereâ€™s no information about whether a person who was denied a loan wouldâ€™ve repaid it back or not\n\nSolution: An occasional A/B test where a group of customers applying for the loan isnâ€™t subject to model predictions of whether theyâ€™re credit worthy or not (control group) and a group that is subject to model predictions (treatment group)\n\nLittle to zero or sporadic ground truth feedback\n\nRequires monitoring of proxy metrics (metrics that are associated with the ground truth)\nManual collection of ground truth data\n\ncan be expensive but high quality ground truth data is very important\n\n\n\ndata required for monitoring\n\nIf customer facing, then the data should that which is necessary to calculate service level indicators (SLI) which will help the company keep its customer obligations which are outlined in the service level agreement (SLA) (see link for more details)\nTypes\n\nUnique Id per request provided by the system that called the ML model. This unique identifier will be stored with each log and will allow us to follow the path of a prediction before, during and after the ML model.\nInput features before feature engineering\nInput features after feature engineering\nOutput probabilities\nPredicted value\n\nData size required to be able to measure a metric accurately\n\nHow many observations do I need so that I know the metric Iâ€™m measuring is accurate.\nDepends on metric and threshold for the accuracy of that metric.\nExample:\n\nclassification with imbalanced dataset, 1/100 is a positive event\nRecall = 90%\n\nThe model should get 90% of all true positive correct\n\nThreshold (aka shot noise) = 0.1%\n\nThe maximum error in measuring recall is 0.1%. So, a model with 90.0009% recall would trigger a model retrain\n\nshot_noise &lt;- ((pos_rate * data_size) * (1-recall)) / data_size\n\nI donâ€™t see how to solve this for data_size (i.e.Â data_size cancels out) so I guess this is an optimization problem\n\n\n\n\nChanges in the distribution of input and output features\n\nTriggers for retraining model (if possible, use both)\n\nusing performance based triggers is good for use-cases where there is fast feedback and high volume of data, like real time bidding, where you are able to measure the modelâ€™s performance as close as possible to the time of predictions, in short time intervals and with high confidence (high volume).\n\nThe main limitation when relaying on performance only, is the time it takes for you to obtain your ground truth â€” if you obtain it at all. In user conversion prediction cases, it can take 30 or 60 days until you will get a ground truth, or even 6 months or more in cases such as transaction fraud detection or LTV. If you need to wait so long to have full feedback, that means youâ€™ll retrain the model too late, after the business has already been impacted.\n\nBy measuring changes in the input data, i.e.Â changes in the distribution of features that are used by the model, you can detect data drifts that indicate your model may be outdated and needs to be retained on fresh data.\n\nMissing values can occur regularly at ML model inference. Even when missing values are allowed in features, a model can see a lot more missing values than in the training set. An example of a missing value error is an ML model making an inference based on a form input where a previously optional field is now always sending a null value input due to a code error.\nRange violation happens when the model input exceeds the expected range of its values. It is quite common for categorical inputs to have typos and cardinality mismatches to cause this problem, e.g.Â free form typing for categories and numerical fields like age, etc. An unknown product SKU, an incorrect country, and inconsistency in categorical values due to pipeline state are all examples of range violation.\nType mismatch arises when the model input type is different from the one provided at inference time. One way types get mismatched is when column order gets misaligned during some data wrangling operations.\n\n\n\nTrack distance metrics between reference variable distribution and a production variable distribution\n\nThe production variable distributions should include feature variable data entering the pipeline and prediction output from the models\nUpward trends in the distance between baseline and operational data distributions can be the basis of data drift detection.\nPotential reference distributions (i.e.Â monitoring window)\n\na distribution across a fixed time window (distribution doesnâ€™t change). Examples:\n\ntraining distribution\nvalidation/test set distribution.\ninitial model deployment distribution (or a time when the distribution was thought to be stable)\n\na distribution across a moving time window (distribution can change)\n\nlast weekâ€™s input data with this weekâ€™s input data\n\nConsiderations\n\nRepresentation differences: Class ratios across windows may not be the same (e.g., the fraction of positives in one window may be very different from the fraction of positives in another window).\nVarying sample sizes: The number of data points in each window may vary (e.g., the number of requests received on a Sunday is less than the number of requests received on a Monday).\nDelayed feedback: Due to reasonable events (e.g., loss of Internet connection), labels may come in at a lag, making it impossible to factor in predictions without a label into the current windowâ€™s evaluation metric.\n\n\nTrack predictions by group (cohort performance)\n\nDrift may impact groups differently.\n\ne.g.Â persons with high net worth, low FICO scores, recent default, etc.\n\nGroups that are very important to the company should be tracked at the very least.\nMakes misclassified or poorly predicted observations exportable, so they can be studied further\n\nmaybe version these\n\n\nOther stuff to track\n\nNumber of predictions made\nPrediction latency â€” How long it takes to make a prediction\nIf the product is customer facing, then customer satisfaction/usage type metrics should also be tracked\n\nThresholds\n\nSee Automating Data Drift Thresholding in Machine Learning Systems\n\nAuthor was lame. He coded the impractical solution and didnâ€™t code the good solution.\n\nComputationally intensive method (may not be practical)\n\nBootstrap or MC simulate the feature at the size of the production dataset or whatever the amount of data youâ€™re going to test for drift\nFor each simulation, measure the drift (e.g.Â PSI, JS Divergence) between the simulated dataset and the training dataset.\nThreshold =Â  mean(drift) of all the simulations If any production/inference dataset has a drift &gt; threashold then that should trigger an alert\n\nClosed method for PSI\n\nSupposedly can be calculated for other distance measures.\nFinal Solution for the threshold ğŸ¥´\n\n\nK is the number of bins (numeric) or levels (categorical)\nPk is the percentage of total training observations == level_K or the percent of total training observations in bin K\nÎ±k = 1 + Nq + Pk\n\nNq = sample size of the production/inference set, Q. So it should be a constant for all K.\nHe used lower-case pk here instead of Pk, but my understanding was that they were the same thing. May want to double check that.\n\nÎ¨ is called the digamma function. Will need to look that one up."
  },
  {
    "objectID": "qmd/production-ml-monitoring.html#sec-prod-mlmon-ddmet",
    "href": "qmd/production-ml-monitoring.html#sec-prod-mlmon-ddmet",
    "title": "56Â  ML Monitoring",
    "section": "56.2 Distribution Distance Metrics",
    "text": "56.2 Distribution Distance Metrics\n\nPackages See article for examples of computing wasserstein (aka Kantorovich) distance in R, py, Julia.\n\n{philentropy} has a ton of distribution distance measures + some helper functions, Docs {{cinnamon}} LIB handles py models and calculates wasserstein and kolmogorov-smirnov\n{KSgeneral} can perform KS tests between continuous, mixed, or discrete distributions\n\nSee Distributions &gt;&gt; Tests for more details on kolmogorov-smirnov\n\n\nPopulation Stability Index (PSI)\n\nFormula\n\n\ni âˆˆ length(bins)\np is the percent of total observations in bin i\nref is the reference variable\nprod is the production variable\n\nMisc\n\nNotes from\n\nPopulation Stability Index | Matthewâ€™s Blog\nChecking model stability and population shift with PSI and CSI\n\nOften seen in the finance industry\nFor both numeric and categorical features\nSometimes referred to as Characteristic Stability Index (CSI) when used on predictor variables\n\nSteps\n\nDivide the reference variable variable range into 10 bins (arbitrary but seems to be common) or however many bins you want depending on how fine a resolution you want.\n\nFor categorical variables, the levels can be used as bins or levels can be collapsed into fewer bins\nContinuous\n\nSlicing the range of the reference variable into sections of the same interval length\nSlicing the reference variable into quantiles where each bin has the same number of observations\n\n\nCount the number of values in each of those bins.\nDivide each bin count by the sample size to get a percentage\nRepeat for the production variable\nCalculate PSI\n\nGuidelines Also see Thresholds section above from https://scholarworks.wmich.edu/cgi/viewcontent.cgi?article=4249&context=dissertations\n\n\nN is reference sample size and M is production sample size (although itâ€™s symmetric so doesnâ€™t matter which you column/row you use for each)\nB is the number of bins used.\nCells have the PSI values for 95% level significance\nPSIs &gt;= the appropriate cell value can reliably be interpreted that a shift in the variable distribution has occurred.\nSee paper tables in Appendix B for other significance levels and B values. Can also use a chisq distribution.\n\nExample: model predictions\n\n\n\nEquation is slightly different but may be equivalent\n\ninitial - model predictions that are used as a reference (e.g.Â predictions from when current model first went into production)\nnew - current model predictions\n\nAverage PSI is used to represent the model\nGuidelines\n\nPSI &lt; 0.1 = The population hasnâ€™t changed, and we can keep the model\n0.1 â‰¤ PS1 &lt; 0.2 = The population has slightly changed, and it is advisable to evaluate the impacts of these changes\nPSI â‰¥ 0.2 = The changes in population are significant, and the model should be retrained or even redesigned.\n\n\nExample from arize ai (model monitoring platform)\n\n\nA comparison in the distributions of how a person spent their money this last year as compared to the year prior\n\nY-axis represents the percentage of the total money spent in each category, as denoted on the X-axis.\n\nSteps\n\nCalculate the difference in percentage between the reference distribution A (budget last year) and the actual distribution B (budget this year)\nMultiply that difference by the natural log of (A %/ B%)\n\nThe larger the PSI, the less similar your distributions are.\nYou can set up thresholding alerts on the drift in your distributions.\n\n\nJensen-Shannon Divergence\n\nMisc\n\nNotes from https://docs.aws.amazon.com/sagemaker/latest/dg/clarify-data-bias-metric-jensen-shannon-divergence.html\nAlso see How to Understand and Use the Jensen-Shannon Divergence (Havenâ€™t read but looks more in-depth)\n\nFormula \n\nWhere pmix = 0.5(pref + pprod) and\n\n\ni âˆˆ length(bins)\np is the percent of total observations in bin i\nSimilar for KL(pprod || pmix)\n\n\n\nSteps: Same preparation steps as with the PSI distance metric (binning, proportions, etc.)\nSymmetric version of K-L divergence ( see Information Theory &gt;&gt; K-L Divergence) which means it satisfies the triangle inequality which means itâ€™s a true distance metric\nfyi using log2 means KL is in units of â€œbitsâ€ and using ln means KL is in â€œnatsâ€\nThe range of JS values for binary, multicategory, continuous outcomes\n\nUsing ln, JS âˆˆ [0, ln(2) â‰ˆ 0.693]\nUsing log2, JS âˆˆ [0, 1] Values near zero mean the labels are similarly distributed.\n\nPositive values mean the label distributions diverge, the more positive the larger the divergence.\nSupposedly the usage of the mixture reference makes this an unstable metric for using a moving window. It makes the JS score not comparable to past values.\n\nSeems like all moving window reference distributions, mixed or not, will have some variability to it, but maybe this produces extra variability that makes it unreliable.\n\nOther measures\n\nKullback-Leibler Divergence (KL Divergence)\nWassersteinâ€™s Distance"
  },
  {
    "objectID": "qmd/production-ml-monitoring.html#sec-prod-mlmon-ddarc",
    "href": "qmd/production-ml-monitoring.html#sec-prod-mlmon-ddarc",
    "title": "56Â  ML Monitoring",
    "section": "56.3 Data Drift Architectures",
    "text": "56.3 Data Drift Architectures\n\nExample: How to Build a Fully Automated Data Drift Detection Pipeline\n\n\nUses Kestra for orchestration\nExample relies on scheduled data pulls for detecting drift. Kestra can make use of Graphana to create a real-time detection pipeline."
  },
  {
    "objectID": "qmd/production-ml-monitoring.html#sec-prod-mlmon-nlpm",
    "href": "qmd/production-ml-monitoring.html#sec-prod-mlmon-nlpm",
    "title": "56Â  ML Monitoring",
    "section": "56.4 NLP monitoring",
    "text": "56.4 NLP monitoring\n\nNotes from Monitoring NLP models in production\nDescriptive Statistics\n\nFeatures: length of text, out-of-vocabulary (OOV) words %, and the share of non-letter character %\nExample: {{evidently}}\ncolumn_mapping = ColumnMapping()Â \ncolumn_mapping.target = 'is_positive'Â  Â  Â  Â  # binary target\ncolumn_mapping.prediction = 'predict_proba'Â  # predicted probabilities\ncolumn_mapping.text_features = ['review']Â  Â  # text feature\n\ndata_drift_report = Report(Â \nÂ  Â  metrics=[Â \nÂ  Â  Â  Â  ColumnDriftMetric('is_positive'),\nÂ  Â  Â  Â  ColumnDriftMetric('predict_proba'),Â \nÂ  Â  Â  Â  TextDescriptorsDriftMetric(column_name='review'), # text feature\nÂ  Â  ]Â \n)Â \ndata_drift_report.run(reference_data=reference,Â \nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  current_data=valid_disturbed,Â \nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  column_mapping=column_mapping)Â \ndata_drift_report\n\nStat tests on distributions are performed (e.g.Â K-S test) with p-values given\nOther drill down charts are provided if drift is detected\n\n\nDomain Classifier\n\n{{evidently}}\n\nBuilds a binary classifier model to predict with the text feature data came from the reference dataset (1) or the current dataset (0)\nThe ROC AUC of the binary classifier shows if the drift is detected. If a model can reliably identify the samples that belong to the current or reference dataset, the two datasets are probably sufficiently different.\nCan be biased if there are time related text (e.g.Â month names or dates)(makes it easier for the classifier), but these can be detected by looking at feature importance plot and looking for date/time related tokens\nIf drift is detect, you can drill down further\n\nTypical words in the current and reference dataset - These words are most indicative when predicting which dataset a specific review belongs to.\nExamples of texts - from current and reference datasets that were the easiest for a classifier to label correctly (with predicted probabilities being very close to 0 or 1).\n\nExample\ndata_drift_dataset_report = Report(metrics=[Â \nÂ  Â  ColumnDriftMetric(column_name='review')Â \n])Â \ndata_drift_dataset_report.run(reference_data=reference,Â \nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  current_data=new_content,Â \nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  column_mapping=column_mapping)Â \ndata_drift_dataset_report\n\n\nInvariance testing\n\nTests whether an ML model produces consistent results under different conditions\nSee {{behave}}, Write Readable Tests for Your Machine Learning Models with Behave\nExample\n\n\nDirectional Testing\n\nStatistical method used to assess whether the impact of an independent variable on a dependent variable is in a particular direction, either positive or negative.\nFor NLP, this test checks whether the presence of a specific word has a positive or negative effect on the sentiment score of a given text.\nSee {{behave}}, Write Readable Tests for Your Machine Learning Models with Behave\nExample\n\n\nSentiment score should increase due to the new wordâ€™s (â€œawesomeâ€)_ presence"
  },
  {
    "objectID": "qmd/production-ml-monitoring.html#sec-prod-mlmon-invdd",
    "href": "qmd/production-ml-monitoring.html#sec-prod-mlmon-invdd",
    "title": "56Â  ML Monitoring",
    "section": "56.5 Investigating Data Drift",
    "text": "56.5 Investigating Data Drift\n\nIf feature distributions change, it may be something else besides a true data generating process (dgp) shift.\n\nCheck for pipeline: code infrastructure, processing, data sources, hardware, and input model issues. (see below for details)\n\nIf itâ€™s not a data quality, then is it concept drift or data drift?\n\nData Drift\n\nDistributions change but relationships between features remain\n\nConcept Drift\n\nDistributions might remain similar, but the relationships change instead: in between the features, or between the features and the model output.\n\n\nQuestions\n\nWhich features are drifting?\nHow strongly?\n\nMay require domain expert to determine whether there are substantial changes in associations\n\nWhatâ€™s the process behind it?\n\nExamples\n\nA change in the socio-economic relations, such as inflation, diseases, or political changes;\nUnaccounted events, such as holidays, world cups, or even natural disasters;\nThe entrance of a new competitor in the market, and/or the shift of customers;\nChanges in the offered product, or the marketing campaign.\n\n\n\nActions\n\nData Pipeline: fix it\nData Drift:\n\nlow to moderate: leave it alone and see if the model handles it or gets worse\nhigh or meaningful: retraining the model should suffice\n\nConcept Drift: Model predictions should suffer, so a complete overhaul (EDA, algorithm selection, production tools, etc.) might be necessary.\nAdditional actions (may need if new labels/target variable data isnâ€™t immediately available)\n\nTake the component of the application that uses the model offline (e.g remove recommendations for website)\nMake a human do what the model was doing (e.g.Â insurance claims processing, manufacturing quality control, or sales lead scoring)\nRule-based approach: You can often design a set of rules or heuristics that will be less precise but more robust than a rogue model. (e.g.Â simply show the most popular items to all customers)\nAdd business logic or an adjustment on top of the model output\n\nProbably wonâ€™t generalize well so should be context specific\n\n\n\nCode infrastructure\n\nWrong source - A pipeline points to an older version of the marketing table, or there is an unresolved version conflict.\nLost access - Someone moved the table to a new location but did not update the permissions.\nBroken queries - These JOINSs and SELECTs might work well until the first complication. Say, a user showed up from a different time zone and a new category of time zone is in the data. Some queries might not hold up.\nInfrastructure update - You got a new version of a database and some automated spring cleaning. Spaces replaced with underscores, all column names in lowercase. All looks fine until your model wants to calculate its regular feature as â€œLast month income/Total incomeâ€ with hard-coded column titles.\n\nProcessing\n\nBroken feature code - For instance, the promo discounts were never more than 50% in training. Then marketing introduces a â€œfreeâ€ offer and types 100% for the first time. Some dependent feature code suddenly makes no sense and returns negative numbers.\nDealing with outliers and missing values\n\nNotes from https://towardsdatascience.com/why-data-integrity-is-key-to-ml-monitoring-3843edd75cf5\nSkip the prediction or have a back-up system â€” If the data is bad, the serving system can skip the prediction to avoid erroring out or making an inaccurate prediction. While this can be a solution when the model makes a large number of non-critical decisions (e.g.Â product recommendation), itâ€™s not an option when it makes business or life-critical decisions (e.g.Â healthcare). In those cases, there needs to be a backup decision-making system to ensure an outcome. However, these backup systems can further complicate the solution.\nImpute or predict missing values â€” A key challenge of this approach is that it hides the problems behind the data issue. Consistently replacing bad data can shift the expected featureâ€™s distribution (aka data drift) causing the model to degrade. A drift as a result of this data replacement could be very difficult to catch, impacting the modelâ€™s performance slowly over time.\nSet default values â€” When the value is out of range, it can be replaced by a known high or low or unique value, e.g.Â replacing a very high or low age with the closest known minimum or maximum value. This can also cause gradual drift over time impacting performance.\nAcquire missing data â€” In some critical high value use cases like lending, ML teams also have the option to acquire the missing data to fill the gap. This is not typical for the vast majority of use cases.\nDo nothing â€” This is the simplest and likely the best approach to take depending on the criticality of your use case. It allows for bad data to surface upstream or downstream so that the problem behind it can be resolved. Itâ€™s likely that most inference engines might throw an error depending on the ML algorithm used to train the model. A prediction made on bad data can show up as an outlier of either the output or the impacted input helping surface the issue.\n\n\nData source\n\nnew data formats, types, and schemas\n\nAn update in the original business system leads to a change of unit of measurements (think Celsius to Fahrenheit) or dates formats (DD/MM/YY or MM/DD/YY?)\nNew product features in the application add the telemetry that the model never trained on.\nThere is a new 3rd party data provider or API, or an announced change in the format.\n\nThe website being scraped changes urls or webpage format\n\nHardware\n\nSensor breaks, Server goes down\nData collection can stop or the data thatâ€™s collected could be corrupted\n\nInput model\n\nA modelâ€™s results that are used as inputs to another model\nWould need to determine if it was model drift or data drift"
  },
  {
    "objectID": "qmd/production-ml-monitoring.html#sec-prod-mlmon-modd",
    "href": "qmd/production-ml-monitoring.html#sec-prod-mlmon-modd",
    "title": "56Â  ML Monitoring",
    "section": "56.6 Model Drift",
    "text": "56.6 Model Drift\n\nNotes from\n\nGetting a Grip on Data and Model Drift with Azure Machine Learning\nhttps://towardsdatascience.com/why-data-integrity-is-key-to-ml-monitoring-3843edd75cf5\n\nTesting for drift\n\n\nOption 1: Youâ€™ll already have a trained candidate ready for deployment if model 2 outperforms model 1.\n\nExample: An aggregated dataset consists of 45,000 timestamped observations which we spilt into 20,000 references, 20,000 current, and 5,000 most recent observations for the test.\n\nScore Models\nThe current model outperforms the reference model by a large margin. Therefore, we can conclude that we indeed have identified model drift and that the current model is a promising candidate for replacing the production model.\n\n\nOption 2: Will likely reduce false positives, at the expense of being less sensitive to drift.\nOption 3: Reduces unnecessary training cycles in cases where no drift is identified.\n\nArchitectures\n\nAzure ML\n\n\nIngest and version data in Azure Machine Learning\n\nFor automation, we use Azure Machine Learning pipelines which consume managed datasets. By specifying the version parameter (version=â€œlatestâ€) you can ensure to obtain the most recent data.\n\nTrain model\n\nModel is trained on the source data. This activity can also be part of an automated Azure Machine Learning pipeline.\nRecommend adding a few parameters like the dataset name and version to re-use the same pipeline object across multiple dataset versions.\n\nBy doing so, the same pipeline can be triggered in case model drift is present.\n\nOnce the training is finished, the model is registered in the Azure Machine Learning model registry.\n\nEvaluate model\n\nBesides looking at performance metrics to see how good a model is, a thorough evaluation also includes reviewing explanations, checking for bias and fairness issues, looking at where the model makes mistakes, etc. It will often include human verification.\n\nDeploy model - Deploy a specific version of the model\nMonitor model - Collect telemetry about the deployed model.\n\nAn Azure AppInsights workbook can be used to collect the number of requests made to the model instance as well as service availability and other user-defined metrics.\n\nCollect inference data and labels\n\nAs part of a continuous improvement of the service, all the inferences that are made by the model should be saved into a repository (e.g., Azure Data Lake) alongside the ground truth (if available).\n\nAllows us to figure out the amount of drift between the inference and the reference data.\n\nShould the ground truth labels not be available, we can monitor data drift but not model drift.\n\nMeasure data drift - Use the reference data and contrast it against the current data\nMeasure model drift - Determine if the model is affected by data or concept drift\nTrigger re-training\n\nIn case of model or concept drift, we can trigger a full re-training and deployment pipeline utilizing the same Azure ML pipeline we used for the initial training.\nRe-training triggers can either be:\n\nAutomatic â€” Comparing performance between the reference model and current model and automatically deploying if the current model performance is better than the reference model.\nHuman in the loop â€” Inspect data drift visualization alongside performance metrics between reference and current model and deploy with a data scientist/ model owner in the loop. This scenario would be suitable for highly regulated industries. This can be done using PowerApps, Azure DevOps pipelines, or GitHub Actions.\n\n\n\nNLP\n\nIssue\n\nAfter writing the new predictions with assigned labels (e.g.Â good/bad review) to a database., you typically do not get immediate feedback. There is no quick way to know if the predicted labels are correct. However, you do need something to keep tabs on the modelâ€™s performance to ensure it works as expected.\n\nOptions for collecting ground truth labels (reactive solâ€™ns, so there will be delay in awareness of drift)\n\nYou can have a feedback mechanism directly in the website UI. For example, you can allow the review authors or readers to report incorrectly assigned categories and suggest a better one. If you get a lot of reports or corrections, you can react to this and investigate.\nManual labeling as quality control. In the simplest form, the model creator can look at some of the model predictions to see if it behaves as expected. You can also engage external labelers from time to time to label a portion of the data. This way, you can directly evaluate the quality of the model predictions against expert-assigned labels.\n\nLead indicators of model drift are often data quality issues and changes in the input data distributions\n\nRegarding data quality, there might be corruption due to wrong encoding, the presence of special symbols, text in different languages, emojis, etc. being newly introduced into the data pipeline\nSee NLP Monitoring\n\n\n\nInvestigating\n\nAnalyze locally - For critical use cases the best practice is to begin with a fine grained approach of prediction analysis by replaying the inference with the issue and seeing its impact on the model.\n\nFor ML models, use model-agnostic diagnostic scores (e.g.Â shap, dalex, iml, etc.) to compare previous model prediction features to the drifted model prediction features.\n\nDefine the segments of low performance\n\nHave previously important features become not-so important. Is that a data issue or some outside event causing the change?\n\n\n\nAnalyze Globally - This involves analyzing the data for that feature over a broader range of time to see when the issue might have begun. Data changes typically coincide with product releases. So querying for data change timeline can tie the issue to a specific code and data release helping revert or address it quickly.\n\nImputing or other missing data methods may only gradually affect model results after a lengthy period and therefore may be difficult for data validation monitoring to detect."
  },
  {
    "objectID": "qmd/production-ml-monitoring.html#sec-prod-mlmon-retrain",
    "href": "qmd/production-ml-monitoring.html#sec-prod-mlmon-retrain",
    "title": "56Â  ML Monitoring",
    "section": "56.7 Retraining Model",
    "text": "56.7 Retraining Model\n\nRisks of Automated Retraining\n\nRetraining on delayed data\n\nIn some real-world scenarios, like loan-default prediction, labels may be delayed for months or even years. The ground truth is still coming, but you are retraining your model using the old data, which may not represent the current reality well.\n\nFailure to determine the root cause of the problem\n\nIf the modelâ€™s performance drops, it doesnâ€™t always mean that it needs more data. There could be various reasons for the modelâ€™s failure, such as changes in downstream business processes, training-serving skew, or data leakage. You should first investigate to find the underlying issue and then retrain the model if necessary.\n\nHigher risk of failure\n\nRetraining amplifies the risk of model failure. Besides the fact that it adds complexity to the infrastructure, the more frequently you update, the more opportunities the model has to fail. Any undetected problem appearing in the data collection or preprocessing will be propagated to the model, resulting in a retrained model on flawed data.\n\nHigher costs\n\nStoring and validating the retraining data\nCompute resources to retrain the model\nTesting a new model to determine if it performs better than the current one\n\n\nPerform an analysis on the data that caused the trigger\n\nDid the mean or sd (i.e.Â distribution) change?\nIs there a new seasonality or cyclic component present?\nAre there new correlations between variables?\nWhatâ€™s behind the change (expansion into a different area, new product, new vendor, new competitor, etc.)\n\nUsing the characteristics of the data found in the analysis, decide on the most relevant block of data that is representative the current state of the data being collected. This is the retraining dataset\n\nA sufficient sample size should also be a consideration.\nMaybe use upsampling or simulation to obtain a sufficiently sized dataset with the necessary characteristics\n\nPotential next steps. Which one or combination of steps depends on the severity and causes of the data drift/model performance degradation, time and budget constraints.\n\nRetrain using current algorithm\n\nRetrain with more weight on recent data points\nRetrain with recent data only\nRetrain on all your past dataset\n\nUpdate using current algorithm but with a new dataset\n\nUse initial weights and batch training\nThis might only be for DL models or I think there might be something in {sklearn} and/or {tidymodels}\n\nRedo the algorithm selection process\nRedo feature engineering\nRedo everything"
  },
  {
    "objectID": "qmd/production-ml-monitoring.html#sec-prod-mlmon-notif",
    "href": "qmd/production-ml-monitoring.html#sec-prod-mlmon-notif",
    "title": "56Â  ML Monitoring",
    "section": "56.8 Notifications",
    "text": "56.8 Notifications\n\nMisc\n\nAlso see Project, Management &gt;&gt; Event Auditing\nPrioritize different alerts (e.g.Â highly important, important, normal, warning, note)\nIf your model is triggering alerts to often, generate additional rules with users to limit the number of alerts\n\nExample: model that predicts patient risk of death can only trigger a re-alert to the clinician if:\n\nitâ€™s been 48 hrs since the previous alert\nif the patient has not just came from the ICU\n\n\n\nEmail\n\nExample: to developers\n\n\nâ€œjarvisâ€ is an internal package and this function probably wraps a template and email package function\n\nExample: to users\n\n\nâ€œCHARTWatchâ€ is the name of the data product\nAlerts user that the product is down and other pertinent information\n\n\nSlack\n\nExample\n\n\nâ€œjarvisâ€ is an internal package and this function probably wraps a template and slack package function"
  },
  {
    "objectID": "qmd/production-testing.html#sec-prod-test-misc",
    "href": "qmd/production-testing.html#sec-prod-test-misc",
    "title": "Testing",
    "section": "Misc",
    "text": "Misc\n\nExample: Dependencies (e.g.Â APIs), Databases, Filesystems\n\n{jarvis} is an internal package"
  },
  {
    "objectID": "qmd/production-tools.html#sec-prod-tools-misc",
    "href": "qmd/production-tools.html#sec-prod-tools-misc",
    "title": "57Â  Tools",
    "section": "57.1 Misc",
    "text": "57.1 Misc\n\nOverview of some 2021 tools Descriptions in article \nAWS Batch - Managed service for computational jobs. Alternative to having to maintain a kubernetes cluster\n\nTakes care of keeping a queue of jobs, spinning up EC2 instances, running code and shutting down the instances.\nScales up and down depending on how many jobs submitted.\nAllows you to execute your code in a scalable fashion and to request custom resources for compute-intensive jobs (e.g., instances with many CPUs and large memory) without requiring us to maintain a cluster\nSee bkmks: Hosting &gt;&gt; AWS &gt;&gt; Batch\nPackages:\n\n{crew.aws.batch}"
  },
  {
    "objectID": "qmd/production-tools.html#sec-prod-tools-stckcomprnk",
    "href": "qmd/production-tools.html#sec-prod-tools-stckcomprnk",
    "title": "57Â  Tools",
    "section": "57.2 Stack Component Rankings",
    "text": "57.2 Stack Component Rankings\n\nDB format\n\narrow files\n\nELT Operations\n\n*dbt\n*Spark\n*Google Big Query SQL\n*AWS Athena\n\nOrchestration and monitoring\n\n*Targets\n\n+ {cronR} for orchestration + scheduling\n\n*Mage-AI\n*AWS Glue\nPrefect\nAirflow\n\nData Ingestion\n\nAirbyte (data ingestion)\nfivetran (data ingestion)\n\nCan â€œprocess atomic REST APIs to extract data out of SAAS silos and onto your warehouseâ€\n\nterraform (multi-cloud management)\n\nTracking/Versioning for Model Building\n\n*DVC\nMLFlow\n\nReporting\n\nblastula (email), xaringan (presentation), RMarkdown (reports), flexdashboard (dashboards),\nRStudio Connect (publishing platform to stakeholders)\n\ndashboards, apps\non-demand and scheduled reports\npresentations\nAPIs (?)\nPublish R and Python\nEnterprise security\nCan stay in RStudio\n\n\nVisualization Platforms\n\nLooker*\nPowerBI, DataStudio"
  },
  {
    "objectID": "qmd/production-tools.html#sec-prod-tools-depman",
    "href": "qmd/production-tools.html#sec-prod-tools-depman",
    "title": "57Â  Tools",
    "section": "57.3 Dependency Management",
    "text": "57.3 Dependency Management\n\nR\n\nr2u for linux installations\n\nâ€œfor Ubuntu 20.04 and 22.04 it provides _all_ of CRAN (and portion of BioConductor) as binary #Rstats packages with full, complete and automatic resolution of all dependencies for full system integration. If you use `bspm` along with it you can even use this via `install.packages()` and friends. Everything comes from a well connected mirrorâ€"
  },
  {
    "objectID": "qmd/production-tools.html#data-versioning",
    "href": "qmd/production-tools.html#data-versioning",
    "title": "57Â  Tools",
    "section": "57.4 Data versioning",
    "text": "57.4 Data versioning\n\nFlat Table by Github\n\nHas a Github action associated with it\nHas a datetime commit message\nLists as a feature that it tracks differences from one commit to the next, but doesnâ€™t a normal data commit doe the same thing?\n\nLumberjack R package\n\nAdd functions to your processing script\ntracks using a log file\noptions for changes you want to track"
  },
  {
    "objectID": "qmd/production-tools.html#sec-prod-tools-dating",
    "href": "qmd/production-tools.html#sec-prod-tools-dating",
    "title": "57Â  Tools",
    "section": "57.5 Data Ingestion",
    "text": "57.5 Data Ingestion\n\nFiveTran\n\nSync raw data sources\n\nevery 1hr for starter plan, every 15 minutes both standard plans, every 5 min for enterprise plan"
  },
  {
    "objectID": "qmd/production-tools.html#sec-prod-tools-orch",
    "href": "qmd/production-tools.html#sec-prod-tools-orch",
    "title": "57Â  Tools",
    "section": "57.6 Orchestration",
    "text": "57.6 Orchestration\n\n57.6.1 Airflow\n\nwebpage\nOpen-source platform for authoring, scheduling, and executing data pipelines.\n\nFeatures for managing and monitoring data pipelines, including integration with various data storage and processing technologies. Similar to the Unix cron utility â€” you write scripts and schedule them to run every X minutes.\nAirflow can be used for any sort of scheduling task, but is often used for scheduling data modeling. schedule, run and monitor the refresh of our data warehouse\nMonitoring on-prem checking Airflow logs is not user-friendly (better in AWS MWAA)\n\ndifferent types of logs for task, web server, scheduler, worker, and DAGs\nhave to SSH into the server and run commands which becomes more complicated when you want to use distributed servers for scalability.\n\nRequires you to create a central logging storage and make additional setup to make all servers write logs into that single place\n\n\nServer-based remains active even when not running jobs â€“&gt; continually incurring cost\n\nNo latency since servers are always running\n\nProblems\nLong feedback loop\n\nWhile programming, instant feedback of your DAG becomes crucial when you want a sanity check before your code goes too far.\nTo see the graph view, which is mainly for visualizing dependencies in DAGs, your code needs to be in the folder of an Airflow scheduler that can be picked up. The airflow scheduler also takes time to render and parse your DAG until it shows up.\nMakes debugging difficult during the development cycle, so some engineers write more lines of code and test them all together. If the lines of code become unmanageable on one screen, you might vaguely remember what to validate and what dependencies to check.\n\nDifficult with local development\n\na docker image can be used to inject as much production-related information as possible. But itâ€™s still not 100% copy, and it takes tremendous effort to develop and maintain that docker image.\nEven if you set up dev, staging, and production environments for running Airflow, they arenâ€™t totally isolated and developers can end-up interfering with one another. Services/Extensions Astronomer offers a managed Airflow service.\nAmazon Managed Workflows for Apache Airflow (MWAA) - managed Airflow service\n\nOrchestrate jobs in EMR, Athena, S3, or Redshift\n\nGlue\n\nAirflow has the glue operator\n\nCloudFormation can be used to configure and manage\n\nallows for autoscaling which saves on costs by scaling down when usage is low\nstill needs a server running even when not running jobs\nmonitoring much easier since all the logs are written into CloudWatch search certain logs using Logs Insights\n\nhave a dashboard that displays usage of server resources like CPU, memory, and network traffic.\nmonitor numerous other Airflow-specific metrics.\nset up alerts and manage notification recipients programmatically.\n\n\nCost factors\n\nInstance size\nAdditional worker instance\nAdditional scheduler instance\nMeta database storage\n\nPotential Issues:\n\nResources are shared on multiple jobs so performance can suffer if:\n\nDonâ€™t distribute trigger times evenly\nMisconfigure your maximum worker count\n\n\nOperate through AWS SDK\n\nCan\n\ncreate, update, and delete MWAA environments and retrieve their environment information that includes logging policies, number of workers, schedulers\nrun Airflowâ€™s internal commands to control DAGs\n\nCanâ€™t\n\nSome of Airflowâ€™s native commands like backfill (check this AWS document), dags list, dags list-runs, dags next-execution, and more\n\n\n\n\n\n\n\n57.6.2 AWS Glue\n\nCloud-based data integration service that makes it easy to move data between data stores.\n\nIncludes a data catalog for storing metadata about data sources and targets, as well as a ETL (extract, transform, and load) engine for transforming and moving data.\nIntegrates with other AWS services, such as S3 and Redshift, making it a convenient choice for users of the AWS ecosystem. Serverless (i.e.Â costs only incurred when triggered by event) Each job triggers separate resources, so if one job overloads resources, it doesnâ€™t affect other jobs\nJobs experience latency since instances have to spin-up and install packages\nCost Charged by Data Processing Unit (DPU) multiplied by usage hours\n\nJob types:\n\nPython shell: you can choose either 0.0625 or 1 DPU.\nApache Spark: you can use 2 to 100 DPUs.\nSpark Streaming: you can use 2 DPUs to 100 DPUs.\n\n\n\nCan run Spark\nMonitoring\n\nCloudwatch\nGlueStudio within Glue Clicking number sends you to Cloudwatch where you can drill down into jobs\nCloudFormation can be used to configure and manage\nGlue SDK available\n\n\n\n\n57.6.3 Prefect\n\nEasier to manage for smaller data engineer teams or a single data engineer\nmore user friendly than Airflow; Better UI; more easily discover location and time of errors\npurely python\nMisc\n\nadd slack webhook for notifications\nHas slack channel to get immediate help with issues or questions\n\nautomatic versioning for every flow, within every project\n\nalso document the models deployed with each version in the README they provide with every flow\n\nComponents\n\nTasks - individual jobs that do one unit of work\n\ne.g.Â a step that syncs Fivetran data or runs a dbt model\n\nFlows - functions that consist of a bunch of smaller tasks, or units of work, that depend on one another\n\ne.g.Â 1 flow could be multiple tasks running Fivetran syncs and dbt models\n\nExample:\nfrom prefect import flow, task\n@flow(name=\"Create a Report for Google Trends\")\ndef create_pytrends_report(\nÂ  Â  keyword: str = \"COVID\", start_date: str = \"2020-01-01\", num_countries: int = 10\n):\n\nThese flows are then scheduled and run by whatever types of agents you choose to set up.\n\nSome options include AWS ECS, GCP Vertex, Kubernetes, locally, etc.\n\nDeployments (docs)\n\nAlso see Create Robust Data Pipelines with Prefect, Docker, and GitHub\nDefintions\n\nSpecify the execution environment infrastructure for the flow run\nSpecify how your flow code is stored and retrieved by Prefect agents\nCreate flow runs with custom parameters from the UI\nCreate a schedule to run the flow\n\nSteps\n\nBuild the deployment definition file and optionally upload your flow to the specified remote storage location\nCreate the deployment by applying the deployment definition\n\nSyntax: prefect deployment build [OPTIONS] &lt;path-to-your-flow&gt;:&lt;flow-name&gt;\nExample:\nprefect deployment build src/main.py:create_pytrends_report \\\nÂ  -n google-trends-gh-docker \\\nÂ  -q test\n\nDeployment for the flow create_pytrends_report (see flow example) from the file, â€œsrc/main.pyâ€\n-n google-trends-gh-docker specifies the name of the deployment to be google-trends-gh-docker.\n-q test specifies the work queue to be test . A work queue organizes deployments into queues for execution.\nOutput\n\nâ€œcreate_pytrends_report-deployment.yamlâ€ file and a â€œ.prefectignoreâ€ created in the current directory.\n\nâ€œcreate_pytrends_report-deployment.yamlâ€:Â  specifies where a flowâ€™s code is stored and how a flow should be run.\nâ€œ.prefectignoreâ€:Â  prevents certain files or directories from being uploaded to the configured storage location.\n\n\n\n\n\n57.6.4 Azure Data Factory\n\nAllows users to create, schedule, and orchestrate data pipelines for moving and transforming data from various sources to destinations.\nData Factory provides a visual designer for building pipelines, as well as a range of connectors for integrating with various data stores and processing technologies.\nExample: Demand Planning Project\n\n\n\n57.6.5 Mage-AI\n\nEnables users to define DAG regardless of the choice of languages (python/SQL/R)\nWeb-based IDE, so its mobility allows working from different devices, and sharing becomes more straightforward.\n\nUI layout feels like using RStudio. It has many sections divided into different areas.\nOne of the areas is the DAG visualization which provides instant feedback to the user on the task relationship.\n\nDAGs\n\nThe pipeline or DAG is constructed with modular blocksâ€”a block maps to a single file.\nBlock Options\n\nExecution with upstream blocks: this triggers all upstream blocks to get the data ready for the current block to run\nExecute and run tests defined in the current block: this focuses on the current block to perform testing.\nSet block as dynamic: this changes the block type into the dynamic block, and it fits better to create multiple downstream blocks at runtime.\n\nManipulate dependencies via drag and drop\n\nmage-ai keeps track of the UI changes the user made and automatically builds the dependencies DAG into the YAML file. (./pipelines/{your_awesome_pipeline_name}/metadata.yaml)\n\nVisualize data in each block\n\nHelpful for inspecting your input data and further validating the transformation.\nOnce the chart has been created, it will also be attached to the current block as the downstream_blocks.\n\n\nR\n\nAllows users to write the main ETL (Extraction, Transformation, and Loading) blocks using R.\n\n\n\n\n57.6.6 kestra\n\nPopular orchestration libraries such as Airflow, Prefect, and Dagster require modifications to the Python code to use their functionalities. You may need to modify the data science code to add orchestration logic\nKestra, an open-source library, allows you to develop your Python scripts independently and then â€‹â€‹seamlessly incorporate them into data workflows using YAML files."
  },
  {
    "objectID": "qmd/production-tools.html#sec-prod-tools-eltetl",
    "href": "qmd/production-tools.html#sec-prod-tools-eltetl",
    "title": "57Â  Tools",
    "section": "57.7 ELT/ETL Operations",
    "text": "57.7 ELT/ETL Operations\n\n57.7.1 Misc\n\ndbt - see DB, dbt\n\n\n\n57.7.2 AWS DataBrew\n\nfeatures to clean and transform the data to ready it for further processing or feeding to machine learning models\n\nNo coding; pay for what you use; scales automatically\nover 250 transformations\nAllows you to add custom transformations with lambda functions"
  },
  {
    "objectID": "qmd/production-tools.html#sec-prod-tools-modexptrk",
    "href": "qmd/production-tools.html#sec-prod-tools-modexptrk",
    "title": "57Â  Tools",
    "section": "57.8 Model Experimentation/Version Tracking",
    "text": "57.8 Model Experimentation/Version Tracking\n\n57.8.1 DVC\n\nTracks data and models while model building\nStore code and track changes in a Git repository while data/models are in AWS/GCP/Azure/etc. storage\nTracking changes\n\nSteps\n\nhashes every file in the directory data,\nadds it to .gitignore and\ncreates a small file data.dvc that is added to Git.\n\nBy comparing hashes, DVC knows when files change and which version to restore.\n\nInitial Steps\n\nGoto project directory -cd &lt;path to local github repo&gt;\nInitialize DVC - dvc init\nAdd a data path/uri - dvc remote add -d remote path/to/remote\n\ncan be Google Drive, Amazon S3, Google Cloud Storage, Azure Storage, or on your local machine\ne.g.Â Google Drive: dvc remote add -d remote gdrive://&lt;hash&gt;\n\nThe hash will the last part of the URL, e.g.Â â€œhttps://drive.google.com/drive/u/0/folders/1v1cBGN9vS9NT6-t6QhJGâ€\n\nConfirm data set-up: dvc config -l\n\nThe config file is located inside â€œ.dvc/â€\nTo version your config on github: git add .dvc/config\n\n\nAdd data/ to .gitignore\n\nExample showed adding every file in the repo manually but this seems easier\n\nAdd, commit, and push all files to repo\n\nMain differences to regular project initialization\n\ndata/ directory doesnâ€™t get pushed to github\ndata.dvc file gets pushed to github\n\n\nSet-up DVC data cache\n\nCan be local directory/s3/gs/gdrive/etc\nExample: S3\nÂ  Â  Â  Â  Â  Â  dvc remote add -d myremote s3://mybucket/path\nÂ  Â  Â  Â  Â  Â  git add .dvc/config\nÂ  Â  Â  Â  Â  Â  git commit -m \"Configure remote storage\"\nÂ  Â  Â  Â  Â  Â  git push\nÂ  Â  Â  Â  Â  Â  dvc push\n\n\nIâ€™m guessing .dvc/config is created with dvc remote addÂ  and wasnâ€™t there before. Otherwise in steps 3 and 4, I need to add the files manually."
  },
  {
    "objectID": "qmd/production-tools.html#sec-prod-tools-modmon",
    "href": "qmd/production-tools.html#sec-prod-tools-modmon",
    "title": "57Â  Tools",
    "section": "57.9 Model/Data Drift Monitoring",
    "text": "57.9 Model/Data Drift Monitoring\n\nArize AI\n\nDocs\nAccessed through Rest API, Python SDK, or Cloud Storage Bucket\n\nFiddler AI Monitoring: fiddler.ai has a suite of tools that help in making the AI explainable, aid in operating ML models in production, monitor ML models and yes data & model drift detection is one of them\nEvidently: EvidentlyAI is another open-source tool, which helps in evaluating and monitoring models in production. If you are not using Azure ML and looking for a non-commercial tool that is simple to use, evidentlyai is a good place to start.\nAzure ML\n\nMonitors data; uses wasserstein distance\n\nAWS Glue DataBrew\n\nmonitors features\ncalculates full suite of summary stats + entropy\n\nCan be exported to a bucket and then download to measure change over time\n\nAccessed through console or programmatically\nGenerates reports that can be viewed in console or be exported in html, pdf, etc."
  },
  {
    "objectID": "qmd/production-tools.html#sec-prod-tools-appclustmon",
    "href": "qmd/production-tools.html#sec-prod-tools-appclustmon",
    "title": "57Â  Tools",
    "section": "57.10 App/Cluster Monitoring",
    "text": "57.10 App/Cluster Monitoring\n\n57.10.1 Prometheus\n\nDonâ€™t use for ML monitoring (from article)(maybe for apps?)\n\nNeed to use multiple Prometheus Metric types for cross-component monitoring\nNeed to define histogram buckets up front for single-component monitoring\nCorrectness of query results depending on scraping interval\nInability to handle sliding windows\nDisgusting-looking PromQL queries\nHigh latency for cross-component metrics (i.e., high-cardinality joins)\n\nMisc\n\nPrometheus is not a time series database (TSDB). It merely leverages a TSDB.\nBecause Prometheus scrapes values periodically, some Metric types (e.g., Gauges) can lose precision if the Metric value changes more frequently than the scraping interval. This problem does not apply to monotonically increasing metrics (e.g., Counters).\nMetrics can be logged with arbitrary identifiers such that at query time, users can filter Metrics by their identifier value.\nPromQL is flexible â€“ users can compute many different aggregations (basic arithmetic functions) of Metric values over different window sizes, and these parameters can be specified at query time.\n\nMetric values (Docs):\n\nCounter: a cumulative Metric that monotonically increases. Can be used to track the number of predictions served, for example.\nGauge: a Metric that represents a single numerical value that can arbitrarily change. Can be used to track current memory usage, for example.\nHistogram: a Metric that categorizes observed numerical values into user-predefined buckets. This has a high server-side cost because the server calculates quantiles at query time.\nSummary: a Metric that tracks a user-predefined quantile over a sliding time window. This has a lower server-side cost because quantiles are configured and tracked at logging time. Also, the Summary Metric doesnâ€™t generally support aggregations in queries.\n\nProcess\n\n\nUsers instrument their application code to log Metric values.\nThose values are scraped and stored in a Prometheus server.\nThe values can be queried using PromQL and exported to a visualization tool like Grafana\n\nThe are R packages that might make querying these metrics easier so you donâ€™t have to learn PromQL"
  },
  {
    "objectID": "qmd/production-tools.html#sec-prod-tools-oth",
    "href": "qmd/production-tools.html#sec-prod-tools-oth",
    "title": "57Â  Tools",
    "section": "57.11 Other",
    "text": "57.11 Other\n\nTerraform\n\nProvision infrastructure across 300+ public clouds and services using a single workflow through yaml files\n\nautomates and makes these workflows reproducible\nArticle on using it with R"
  },
  {
    "objectID": "qmd/project-development.html#misc",
    "href": "qmd/project-development.html#misc",
    "title": "58Â  Project, Development",
    "section": "58.1 Misc",
    "text": "58.1 Misc\n\nNotes from\n\nAdapting Project Management Methodologies to Data Science\n\nAlong with overview of various methodologies, provides list of Agile foundational values and key principles\n\n\nLoose implementation of CRISP-DM with agile-based practices is recommended\nWaterfall or the newer variation with feedback loops between adjoining stages should not be used\n\nDesigned for manufacturing and construction where the progressive movement of a project is sequential. DS typically requires a lot of experimentation and a modification of requirements.\nAlthough can be useful for certain stages of the data science project such as planning, resource management, scope, and validation\n\nPrior to full deployment, run a pilot deployment\n\nOnly a few groups are given permission to use the product\nReceive feedback (e.g.Â weekly meetings), fix bugs, and make changes\n\nAfter full deployment\n\nHave an education and training session for users\n\nNote problem areas. These may be potential next steps to improving the product\n\nCheck-in periodically with users to get feedback\n\nProtyping and Testing\n\nSee Lean Data Science\n\nThe idea is to build things that deliver value quickly\n\nIterative Building Steps\n\nBuild â€˜good enoughâ€™ versions of the tool or project (MVPs)\nGive these to stakeholders to use and get feedback\nIncorporate feedback\nReturn to stakeholders to use and get more feedback\nIterate until project the stakeholder and you feel it has reached production-level\n\nBreak each project down into a set of smaller projects\n\nExample:\n\nMVP to test if the idea is feasible\nMore functional version of the MVP\nProductionized version of the product.\n\nTrack the impact of each of the sub-projects that comprise the larger projects\nAt each of these milestones, decide on whether to progress further on a project by using taking the impact score of the subproject into account\n\nExample: rule-based chatbot manages to\n\nChatbot: successfully helps 10,000 customers a month\n\n10,000 customers â¨¯ 3 min average call = 30,000 mins = 500 hours.\n\nCall Center Agent\n\nCall-center agentâ€™s time costs $200/hr in terms of salary and infrastructure,\n\nConclusion: MVP chatbot saves $100K a month and you could likely save even more with a more sophisticated chatbot."
  },
  {
    "objectID": "qmd/project-development.html#kanban",
    "href": "qmd/project-development.html#kanban",
    "title": "58Â  Project, Development",
    "section": "58.2 Kanban",
    "text": "58.2 Kanban\n\n\nPhysical or digital board where tasks are then outlined as story cards.\nEvery card will be extracted from left to right until it is completed.\nflexibility to execute tasks without getting constant deadlines\nMisc\n\nSeems like this could be used within a sprint (columns would have to be defined according to the sprint plan)\n\nAdvantages\n\nbottlenecks, overworked steps, etc. easily identified\neffective at communicating the work in progress for stakeholders and team members\noriented towards individual tasks instead of batches like in scrums\n\nDisadvantages\n\nlack of deadlines can lead to longer project times\nchallenging to define the columns for a data science Kanban board\nCustomer interaction is undefined. As such, customers may not feel dedicated to the process without the structured cadence of sprint reviews"
  },
  {
    "objectID": "qmd/project-development.html#crisp-dm",
    "href": "qmd/project-development.html#crisp-dm",
    "title": "58Â  Project, Development",
    "section": "58.3 CRISP-DM",
    "text": "58.3 CRISP-DM\n\n\nCross-Industry Standard Process for Data Mining\nDefacto standard for data mining\nSupports replication, generalizable to any DS project\nPhases (not all are sequential, some phases are iterative):\nBizSci Version\n\n\nView Business as a Machine\n\nIsolating business units\n\nInternal: Sales, Manufacturing, Accounting, etc\nExternal: customers, suppliers\nVisualizing the connections\n\nDefining objectives\nCollecting outcomes\n\nUnderstand The Drivers\n\nInvestigate if objectives are being met\nSynthesize outcomes\nHypothesize drivers\n\nAt this stage, itâ€™s critical to meet with subject-matter experts (SMEs). These are people in the organization that are close to process and customers. We need to understand what are the potential drivers of lead time. Form a general equation that they help create.\n\n\nMeasure Drivers\n\nCollect Data\n\nCollect data related to the high level drivers. This data could be stored in databases or it may need to be collected. We could collect competitor data, supplier data, sales data (Enterprise Resource Planning or ERP data), personnel data, and more.\nMay require effort to set up processes to collect it, but developing strategic data sources becomes a competitive advantage over time.\n\nDevelop KPIs\n\nRequires knowledge of customers and industry. Realize that a wealth of data is available outside of your organization. Learn where this data resides, and it becomes a tremendous asset.\n\n\nUncover Problems And Opportunities\n\nEvaluate performance vs KPIs\nHighlight potential problem areas\nReview the our project for what could have been missed\n\nTalk with SMEâ€™s to make sure they agree with your findings so far.\n\n\nEncode Decision Making Algorithms\n\nDevelop algorithms to predict and explain the problem\nOptimize decisions to maximize profit\n\ne.g.Â For classification, threshold optimization using a custom cost function to optimize resources, costs, precision, and recall (See Diagnostics, Classification &gt;&gt; Scores &gt;&gt; Custom Cost Functions\n\nUse systematic decision-making algorithms to improve decision making\n\nExample: Employee Churn\n\n\nApp uses LIME to get prediction-level feature importance\n\n\nMeasure The Results\n\nCapture outcomes\nSynthesize results\nVisualize outcomes over time\n\nWe are looking for progress. If we have experienced good outcomes, then we need to recognize what contributed to those good outcomes.\nQuestions\n\nWere the decision makers using the tools?\nDid they follow the systematic recommendation?\nDid the model accurately predict risk?\nWere the results poor? Same questions apply.\n\n\n\nReport Financial Impact\n\nMeasure actual results\nTie to financial benefits\nReport financial benefit to key stakeholders\n\nItâ€™s insufficient to say that we saved 75 employees or 75 customers. Rather, we need to say that the average cost of a lost employee or lost customer is $100,000 per year, so we just saved the organization $7.5M/year. Always report as a financial value.\n\n\nExample: Customer Churn\n\nView business as a machine\n\nIsolating business units: The interaction occurs between Sales and the Customer\nDefining objectives: Make customers happy\nCollecting outcomes: We are slowly losing customers. Itâ€™s lowering revenue for the organization $500K per year.\n\nUnderstand The Drivers\n\nInvestigate if objectives are being met\n\nCustomer Satisfaction: Loss of customers generally indicates low satisfaction. This could be related to availability of products, poor customer service, or competition offering lower prices and/or better service or quality.\n\nSynthesize outcomes\n\nCustomers are leaving for a competitor. In speaking with Sales, several customers have stated â€œCompetition has faster deliveryâ€. This is an indicator that lead time, or the ability to quickly service customers, is not competitive.\n\nHypothesize Drivers\n\nLead time is related to supplier delivery, inventory availability, personnel, and the scheduling process.\n\n\nMeasure Drivers\n\nAverage Lead Time: The level is 2-weeks, which is based on customer feedback on competitors.\nSupplier Average Lead Time: The level is 3 weeks, which is based on feedback related to our competitorâ€™s suppliers.\nInventory Availability Percentage: The level of 90% is related based on where customers are experiencing unmet demand. This data comes from the ERP data comparing sale requests to product availability.\nPersonnel Turnover: The level of 15% is based on the industry averages.\n\nUncover Problems and Opportunities\n\nOur average lead time is 6 weeks compared to the competitor average lead time of 2 weeks, which is the first order cause for the customer churn\nOur supplier average lead time is on par with our competitorâ€™s, which does not necessitate a concern.\nOur inventory percentage availability is 80%, which is too low to maintain a high customer satisfaction level. This could be a reason that churn is increasing.\nOur personnel turnover in key areas is zero over the past 12 months, so no cause for concern."
  },
  {
    "objectID": "qmd/project-development.html#agile",
    "href": "qmd/project-development.html#agile",
    "title": "58Â  Project, Development",
    "section": "58.4 Agile",
    "text": "58.4 Agile\n\n58.4.1 Misc\n\nResources\n\nhttps://www.atlassian.com/agile/project-management/overview\n\nfeatures adaptability, continuous delivery, iteration, and short time frames\n\n\n\n58.4.2 Terms\n\nEpic - collection of high level tasks that may represent several user stories\n\nHelps to map the model outcome and define the correct stakeholders for the project\nA helpful way to organize your work and to create a hierarchy.\nThe idea is to break work down into shippable pieces so that large projects can actually get done and you can continue to ship value to your customers on a regular basis\nDelivered over a set of sprints\n\nInitiatives - collections of epics that drive toward a common goal\nProduct Roadmap - plan of action for how a product or solution will evolve over time\n\nexpressed and visualized as a set of initiatives plotted along a timeline\n\nScrum - a framework thatâ€™s objective is to fulfill customer needs through transparent communication, continuous progress, and collective responsibility\n\nData-Driven Scrum (DDS) - Scrums, as defined, have fixed lengths which can be an issue with DS projects\n\nSprints - short periodic blocks that make up a scrum\n\neach usually ranges from 2-4 weeks\nEach sprint is an entity that delivers the full result.\nComposed of a starting point and requirements that complete the project plan\nTheme - an organization goal that drive the creation of epics and initiatives\n\nUser Story - smallest unit of work or a task; an informal, general explanation of a software feature written from the perspective of the end user. Its purpose is to articulate how a software feature will provide value to the customer.\n\nAfter reading a user story, the team knows why they are building, what theyâ€™re building, and what value it creates.\n\n\n\n\n58.4.3 Potential Agile values for data analysis\n\nDecisions over dashboards: By focusing on what people want to do with data, we can move past the first set of questions they ask, focus on the valuable iteration and follow-up questions, build trust, cultivate curiosity and drive action.\nFunctional analysis over perfect outputs: To enable quick iterations, weâ€™re going to have to spend less time crafting perfect outputs and focus on moving from one question to the next as quickly as possible.\nSharing data over gatekeeping data: Weâ€™re going to have to share responsibility for our data and data â€œproductsâ€ with our business partners. This will help build trust, and keep us all accountable for cultivating great data products and data-driven cultures.\nIndividuals and interactions over processes and tools: When in doubt, we need to rely on the relationships weâ€™ve built with the business over the tools weâ€™ve put in to help guide those relationships.\n\n\n\n58.4.4 Data Science Lifecycle\n\n\narticle\nIf at any point we are not satisfied with our results or faced with changing requirements we can return to a previous step since this methodology is focused on iterative development\nSteps\n\nBusiness Understanding\n\nDefine objectives: Work with customers/stakeholders to identify the business problem we are trying to solve.\nIdentify data sources: Identify the data sources that we will need to solve it.\n\nData Acquisition and Understanding\n\nIngest the data: Bring the data into our environment that we are using for analytics.\nExplore the data: Exploratory data analysis (EDA) and determinine if it is adequate for model development.\nSet up a data pipeline: Build a process to ingest new data. A data pipeline can either be batch-based, real-time or a hybrid of the previous options.\nNote: While the data scientists on the team are working on EDA, the data engineers may be working on setting up a data pipeline, which allows us to complete this stage quicker\n\nModeling\n\nFeature engineering: Creat data features from raw data for model training.\n\nEnhanced by having a good understanding of the data.\n\nModel training: Split the data into training, validation, and testing sets. Train models\nModel evaluation: Evaluate those models by answering the following questions:\n\nWhat are the metrics that the model achieves on the validation/testing set?\nDoes the model solve the business problem and fit the constraints of the problem?\nIs this model suitable for production?\n\nNote: Could train one model and find that the results are not satisfactory and return to the feature engineering and model training stages to craft better features and try different modeling approaches.\n\nDeployment (Options)\n\nExposing the model through an API that can be consumed by other applications.\nCreating a microservice or containerized application that runs the model.\nIntegrating the model into a web application with a dashboard that displays the results of the predictions.\nCreating a batch process that runs the model and writes the predictions to a data source that can be consumed.\n\nStakeholder/customer acceptance\n\nSystem validation: Confirm that the data pipeline, model, and deployment satisfy the business use case and meet the needs of the customers.\nProject hand-off: Transfer the project to the group that will manage it in production. l\n\n\n\n\n\n58.4.5 Product Roadmap examples\n\nExample\n\nInitiative: build a forecast system to predict sales for an ice cream company\nEpics:\n\nâ€œAs a Sales Manager, I need to understand which regions I need to focus my outbound effort based on the sales forecastâ€\nâ€œAs a Logistics Manager, I need to estimate demand so that I can prepare our production accordinglyâ€\n\nUser Story:\n\nâ€œAs a Logistics Manager, I need to see the forecast on my Production Dashboardâ€;\nâ€œAs a Logistics Manager, I need to have simulations around how weather predictions can change the forecastâ€;\n\n\n\n\n\n58.4.6 Sprint Workflow\n\n\nBad flow chart, should be a circle where review loops back to planning\nsprint review - the scrum team and stakeholders review what was accomplished in the sprint, what has changed in their environment, and collaborate on what to do next\n\nThese are necessary to avoiding issues that might destroy a project. (see below)\n\nData scientist participation will help with their communication skills and increase transparency in what theyâ€™re doing\nStakeholders might think a feature or ML result is feasible with the current data and tech stack. These are important to opportunities to explain why they arenâ€™t feasible.\nRoles often bleed together. The planning portion is a good way to converge on a strategy of what to do next.\n\n\nsprint planning (~15 minutes every two weeks)\n\nDevelop the next sprintâ€™s goals\n\nDo the next sprintâ€™s goals align with our goals in 3 months\nDo the next sprintâ€™s goals align with our annual team goals/strategic vision\nRevise the next sprintâ€™s goals to align with these goals if necessary\n\nBreak the sprint goals into tasks and sub tasks\nAssign the tasks/subtasks to members and estimate time to completion of these tasks\nExtended sprint planning (Every 3 months to roughly plan the next 3 months)\nStrategic meetings (6 months)\n\nSome technical details to starting a project\n\nNotes from The Technical Blockers to Turning Data Science Teams Agile\nStart a repo\n\nin the organization acct not under a personal acct\nuse readme as onboarding document\n\nlast person to join is in-charge of it\n\nThe last person will be best suited to edit/add details that clear up any confusion that they had when they were onboarded\nInclude â€œThis document is maintained by the most recent person to join the team. That person is currently: ____â€\n\nExplicitly state that anyone can review code in your README. If someone isnâ€™t familiar with a part of the code, they become so by reviewing it.\n\nEdit the settings of your repo. Make the main branch protected, donâ€™t allow anyone to push directly to the main branch, and only allow PRs that have passed unit-tests (more about this later) and have undergone a code review.\n\nUpdate the teamâ€™s skills related to Agile\n\nIn the beginning, may not have a lot of tasks to assign as there may be design/requirements discussions\nMake sure everyone knows git and how to write unit tests\n\nCheck team membersâ€™ personal accts to see how many commits they have, â€œhttps://github.com/search?q=author:â€\nCheck team membersâ€™ projects for unit tests\nIf it doesnâ€™t look like they donâ€™t have much experience, assign a udemy, etc. course on the subject and require a certificate in order to be assigned tasks\n\n\nAssign tasks through Agile tools like ZenHub, Jira, or Trello\nSet-up a CI tool\n\nexamples: Github Actions, TravisCI, CircleCI, or Jenkins\nadd learning this tool as part of your ZenHub task boards and donâ€™t allow people to move on until theyâ€™ve learned it.\nrun your unit tests every time someone makes a PR\n\nDaily Stand-ups\n\nused to discuss what your daily work will be, and it should be short\nProject strategy meeting should be immediately after the stand-up\nEach team member answers only 3 questions:\n\nWhat will you do today?\nWhat did you do yesterday?\n\nRather than a simple verbal status update. It can be better the show what you did.\n\ne.g.Â show your coding screen and walk everyone through you code\n\nBenefits\n\nSomeone else on the team will have an idea for a better, faster, or simpler way to solve the problem\nEasier to catch a flaw after a few lines of code than after 1000 during a code review\nIf you find out that someone on my team is doing something very similar, and you can save time by reusing code.\nCool to see incremental progress every day instead of just the final product\n\n\nWhat are you blocked by?\nScreen-share these three questions written out on a PowerPoint slide.Â \n\nCongratulate people on finishing the courses\nAssign a weekly changing role of scrum master\n\nThe scrum master makes sure the 3 questions above are answered by everyone.\nThis person doesnâ€™t have to be the boss or most senior person."
  },
  {
    "objectID": "qmd/project-management.html#sec-prod-mang-misc",
    "href": "qmd/project-management.html#sec-prod-mang-misc",
    "title": "Management",
    "section": "Misc",
    "text": "Misc\n\nProcessing Checklist for publicly releasing datasets"
  },
  {
    "objectID": "qmd/project-management.html#sec-prod-mang-eaudit",
    "href": "qmd/project-management.html#sec-prod-mang-eaudit",
    "title": "Management",
    "section": "Event Auditing",
    "text": "Event Auditing\n\nEvents that get flagged for further investigation need to be audited by humans\nMisc\n\nAlso see Production, ML Monitoring &gt;&gt; Notifications\n\nTerms\n\nBacklog - amount of events that you have flagged for human investigation.\nQueue Rate - ratio of all events that youâ€™re flagged.\nCapacity - how much you can at most queue in a given time period.\n\nExample: if you hire 100 human annotators/auditors of events working 8 hours per day, and a single annotation takes 5 minutes, then you can queue around 10K items per day, and thatâ€™s your systemâ€™s capacity.\n\n\nMaintain a stable backlog\n\nIf your queue rate is higher than your capacity, your backlog increases.\nIf your queue rate is lower than your capacity, your backlog decreases.\n\nHave elasticity in your labeling/auditing workforce\n\ni.e.Â being able to quickly increase and decrease the size of the workforce as needed.\nSpikes in events (e.g.Â fraud, new products, etc.) can lead to increased backlog\nWalmartâ€™s system (paper)\n\nUses a combination of both expert human analysts and â€œcrowd workersâ€ for product classification\nhuman experts do regular audits of the crowd-generated labels to ensure their quality"
  },
  {
    "objectID": "qmd/project-management.html#sec-prod-mang-pmover",
    "href": "qmd/project-management.html#sec-prod-mang-pmover",
    "title": "Management",
    "section": "Project Management Overview",
    "text": "Project Management Overview\n\nMisc\nInitiation\n\nUnderstand the key stakeholders of the project;\nUnderstand which data sources are available for the project;\nEvaluate the complexity of the data integration and the expected time effort;\nDefine the problem and draw the plan for available solutions;\nDefine the due date for the project\n\nPlanning\n\nDevelop the timeline of model development â€” including where key checkpoints and stakeholders meetings will sit.\nDevelop the timeline for data integration, baseline model development and model improvements.\nEvaluate available constraints for project development (for example, explainability of the models, reasonable training time, etc.);\nLeave some room for â€œcreative tasks.â€\n\nItâ€™s very common that new ideas will come up during the project development and having that time planned for some experimentation will be key to set expectations with stakeholders.\n\nRisk Management\n\nSee The Risk Management Process in Project Management for details on planning and mitigation solutions\nDraw plans to mitigate risks. Examples:\n\nWhat happens if one of my data scientists leave the team (due to sickness, voluntary leave, etc.)?\nWhat happens if we canâ€™t work with the data sources we need to train our model?\nIs there a regulatory risk involving any of the data contained in my data sources?\n\n\nThis high-level plan should be approved by all stakeholders\nTasks should be divided up among team members\n\nSee Development Frameworks section\n\n\nExecution & Controlling\n\nExecution is the development of the project and controlling is the constant evaluation of execution vs.Â the original plan.\nCommon tasks\n\nKeeping track of the timeline vs.Â planning. Comparing if there are project delays or assessing project risk.\nKeep track of business stakeholder management â€” particularly if the initial requirements are being met.\nRegular stand ups between technical teams and business stakeholders.\nAnd of course, development of the model(s) itself.\n\n\nClosure\n\nThe project is presented and main conclusions are discussed with the business stakeholders\nUnderstand if the business goals have been met.\nDiscussing the project result â€” in case of a failed hypothesis, discuss the gain from this new found knowledge.\nEncapsulate relevant documentation and handover the project to the final stakeholder\n\nKeep in mind the audiences when writing the documentation\n\nOther developers/data scientists that will need to maintain the solution in the future\nBusiness users that will use the output in specific use cases."
  },
  {
    "objectID": "qmd/project-management.html#sec-prod-mang-iss",
    "href": "qmd/project-management.html#sec-prod-mang-iss",
    "title": "Management",
    "section": "Issues",
    "text": "Issues\n\nScope creep\n\nStakeholders said, â€œI love the work you did on feature X, can you also do that for A, B, C)â€. Without further thought or consultation, you agree to work on that.\nResult: Itâ€™s too late when you realize that you donâ€™t have time to work on the core feature you planned to deliver. Deadlines get pushed back.\n\nTech creep\n\nStakeholders may see a new technology that needs to be incorporated as a feature, etc.\n\nPriority Shift\n\nNew issues keep popping up that take priority over the project\n\nTech debt\n\nYou commit to a task (e.g.Â a new feature for your model, more training data) that should really be in the backlog.\nResult: you hack it together to finish on a deadline\n\nLack of communication\n\nDevelopers say, â€œIf I deliver on my work on time, why do I need to be in meetingsâ€.\nResult: Project priorities change and team is out-of-sync.\nResult: Stakeholders become disengaged with the project because they arenâ€™t receiving progress updates."
  },
  {
    "objectID": "qmd/project-management.html#sec-prod-mang-dashm",
    "href": "qmd/project-management.html#sec-prod-mang-dashm",
    "title": "Management",
    "section": "Dashboard Management",
    "text": "Dashboard Management\n\nLifespan Factors\n\nDetermine how likely the underlying data sources for your dashboard will change.\n\nIf you know of plans for a schema change, your stakeholder may decide to push the dashboard into the backlog until the changes have been implemented.\n\nExtend your dashboard lifespan by keeping the data updated.\nAssess if the metrics for the dashboard are main KPIs used across the company and are likely to remain relevant.\n\nStartups especially may pivot business models and dashboards become outdated quickly. Your time may be better spent on other tasks than creating this dashboard.\n\n\nDashboard Literacy\n\nCreate documentation that explains the metric definitions. The best is to add a â€˜definitionsâ€™ tab to the dashboard or a link to a page with more information.\nSchedule a meeting with potential users of your new dashboard and explain how to interpret the data.\n\nBetter yet, record the meeting for new users to save you time from having to present the same information again.\n\n\nOnboard New Hires\n\nConsider onboarding new users to dashboards relevant to them. Often new hires arenâ€™t aware of the dashboards used by their predecessors.\nCreate a list of widely used dashboards for new hires to review. This helps familiarize them with the company KPIs and promotes dashboard adoption.\n\n\n\nDeprecation Process\n\nMisc\n\nNotes from Why and How to Deprecate Dashboards\n\nExample contains py code for Looker SDK\n\nDashboard bloatÂ  is the effect on an organization when time is wasted finding the relevant visual to answer a question or recreating visuals that already exist.\n\n\n\nWrite a script to dump all BI metadata into the warehouse\n\nGet data through SDKs/APIs (e.g.Â {{looker_sdk}}, {{Domo}}) or APIs (e.g.Â Tableau Server, PowerBI)\n\nExamples\n\nLooker: fetch all dashboards, looks, and users\nTableau: fetch workbooks, views, and users\n\nClean the response by either casting it as a JSON or extracting only specific fields of relevance (like ID, name, created date, user)\n\nGet last access date for each visual\n\nLooker (i__looker metadata) and PowerBI (raw audit logs), or pre-built reports with Tableau and Domo\nAlso other usage data if available\n\nWrite all these outputs into warehouse tables\n\nOverwrite tables with data dumps (like all visuals), and append data that builds over time (like historical access)\nExample: Looker\n\nTables for each visual (dashboards and looks for the Looker example). Call it `looker_dashboard` and `looker_look`\nTable of users. Call it `looker_user`\nTable of historical access (either raw or aggregated to latest access date per visual). Call it `looker_historical_access`\n\n\nSet script to run on a schedule (preferrably daily)\n\n\n\nQuery table to summarize ownership and last access date for each visual\n\nSometimes, creating the visual doesnâ€™t count as accessing it, so youâ€™ll need to make sure recently created visuals arenâ€™t flagged for deletion.\nTo alert users via Slack, youâ€™ll need to map their email to a Slack username.\nIf itâ€™s a table not a view, update this on a schedule.\nCan be a dbt model\nExample\nwith history as (\nÂ  Â  select visual_id,\nÂ  Â  Â  Â  Â  max(access_date) as latest_access_date\nÂ  Â  from looker_historical_access\nÂ  Â  group by visual_id\n), dashboards as (\nÂ  Â  select\nÂ  Â  Â  Â  id as visual_id,\nÂ  Â  Â  Â  name as visual_name,\nÂ  Â  Â  Â  user_id as visual_created_user_id,\nÂ  Â  Â  Â  created_at as visual_created_at,\nÂ  Â  Â  Â  'dashboard' as type\nÂ  Â  from dashboard\n), looks as (\nÂ  Â  select\nÂ  Â  Â  Â  id as visual_id,\nÂ  Â  Â  Â  name as visual_name,\nÂ  Â  Â  Â  user_id as visual_created_user_id,\nÂ  Â  Â  Â  created_at as visual_created_at,\nÂ  Â  Â  Â  'look' as type\nÂ  Â  from look\n), visuals as(\nÂ  Â  select * from dashboards union all select * from looks\n)\nselect\nÂ  Â  v.*,\nÂ  Â  coalesce(h.latest_access_date, v.visual_created_at) as latest_access_date,\nÂ  Â  u.email\nfrom visuals as v\nleft join history as h on h.visual_id =Â \nleft join user as u on v.visual_created_user_id;\n\nResult: one row per visual, when it was created, the user who created it, and the last date it was viewed or edited\n\n\n\n\nAutomatically warn users before deprecation, then delete visuals\n\n60 or 90 days as the threshold for â€œnot recentâ€ is recommended\nProcess\n\nCommunicate the reason for deprecating visuals\n\nDocument and communicate the benefits of keeping a clean BI instance to the broader organization\nThe purpose is not to delete othersâ€™ work; itâ€™s to enable everyone in the company to get insights from data faster.\nSee â€œDashboard Bloatâ€ above\n\nCreate a deprecation Slack channel for automated communication\n\nAnyone who is a user of the BI tool should be added to this channel.\n\nGive people a week to save the dashboard/visual\n\nQuery visuals that havenâ€™t been accessed in X-7 days and send a Slack message.\ne.g.Â Visuals included should be ones unused for 53 days if deleting at 60 days of idle time, or 83 days if deleting at 90 days of idle time.\nExample\n# Everything below is pseudo-code, with utility methods abstracted away\ndeprecation_days = 60\nwarn_visuals = get_warehouse_data( # Pseudo method\nÂ  Â  f'''\nÂ  Â  select visual_name, created_by_user\nÂ  Â  from modeled_looker_data\nÂ  Â  where current_date - last_accessed_date = {deprecation_days - 7}\nÂ  Â  ''')\nslack_message_template = '''\nÂ  Â  Visual {{visual_name}} created by @{{slack_username}} will be\nÂ  Â  deprecated in 7 days. If this is incorrect, please contact the\nÂ  Â  analytics team.\n'''\nfor visual in warn_visuals:\nÂ  Â  send_slack_message(slack_message_template, visual) # Pseudo method\n\nQuery visuals that are ready for deletion and delete them programatically\ndeprecation_days = 60\ndelete_visuals = get_warehouse_data( # Pseudo method\nÂ  Â  f'''\nÂ  Â  select visual_id\nÂ  Â  from modeled_looker_data\nÂ  Â  where current_date - last_accessed_date &gt;= {deprecation_days}\nÂ  Â  ''')\nfor visual in delete_visuals:\nÂ  Â  visual_id = visual['visual_id']\nÂ  Â  if visual['type'] == 'look':\nÂ  Â  Â  Â  sdk.delete_look(visual_id)\nÂ  Â  else:\nÂ  Â  Â  Â  sdk.delete_dashboard(visual_id)\n\nTrial period: Run the automated process for a few weeks by commenting out the actual deletion to ensure the logic is sound.\nShould also be aware of data collection or dbt models that are used for the visual being deprecated. If they arenâ€™t attached to anything else, they can be deleted, too."
  },
  {
    "objectID": "qmd/project-planning.html#sec-proj-plan-misc",
    "href": "qmd/project-planning.html#sec-proj-plan-misc",
    "title": "59Â  Planning",
    "section": "59.1 Misc",
    "text": "59.1 Misc\n\nSee Case Study: Applying a Data Science Process Model to a Real-World Scenario A VERY detailed article on the execution of a data science project within a manufacturing company, but can be generalized to other industries.\n  Goes through a scenario of step-by-step planning and execution of changing a manual stock replenishment process to an automated one\n\n  Notes on this article are in [Logistics](Logistics) &gt;&gt; Demand Forecasting &gt;&gt; Misc\nDL model cost calculator (github) (article)\nA clearly defined business problem and targeted success metrics thatâ€™s agreed upon by data scientists and stakeholders are essential before starting a project.\n\nIt should be measurable, clear, and actionable.\nUse the â€œChallenge Frameworkâ€ to solve difficult problems\n\nEvery situation is a function of:\n\nIncentives\nPersonalities\nPerspectives\nConstraints\nResources\n\nIn most â€œtoughâ€ situations, 2+ are misaligned. Figure out which and hone in on them.\n\n\nBum, Buy, then Build\n\nBum free solutions while also relaxing quality thresholds.\nLook at buyable options, especially from large, mature organizations that offer low-cost, stable products (with potential discounts if the project is for a non-profit).\nResort to building only if:\n\nIt is far too inefficient to adapt workflows to existing solutions and/or\nThere is an opportunity for reuse by other nonprofits.\n\n\nAdd a buffer\n\nif the business goal is a precision of 95%, you can try tuning your model to an operating point of 96â€“97% on the offline evaluation set in order to account for the expected model degradation from data drift.\n\nContracts\n\nOnly promise what is in your power to deliver\n\nExample: A contract with the business stakeholders was to guarantee X% recall on known (historic) data.\n\nIt doesnâ€™t try to make guarantees about something that the ML team doesnâ€™t have complete control over: the actual recall in production depends on the amount of data drift, and is not predictable.\n\n\n\nDeliver a Minimally Viable Product (MVP) first.\n\nShould be a product with only the primary features required to get the job done\nThis process with help decide:\n\nhow to implement a more fully fledged product\nwhich additional features might be infeasible or not worth the time and effort to get working\n\n\nFor details on Project/DS Team ROI, see Organizational and Team Development &gt;&gt; Determining a Data Teamâ€™s ROI\nSources of data\n\nInternal resources: Existing historical datasets could be repurposed for new insights.\n\nConsiderations for collecting data\n\nWhether you want to collect qualitative or quantitative data\nThe method for collecting (e.g., surveys, using other reports)\nThe timeframe for the data\nSample size\nOwners of the data\nData sensitivity\nData storage and reporting method (e.g., Salesforce)\nPotential pitfalls or biases in the data (e.g., sample bias, confirmation bias)\n\n\nExternal resources: Governmental organizations, nonprofits, and research institutions have free, accessible datasources that span all different sectors (e.g., agriculture, healthcare, education).\n\nExamples Data.gov\n  _[10 Great Nonprofit Research Resources](https://topnonprofits.com/10-great-nonprofit-research-resources/)_\n\n  _[Forbes â€” 35 Brilliant and Free Datasources](https://www.forbes.com/sites/bernardmarr/2016/02/12/big-data-35-brilliant-and-free-data-sources-for-2016/#5807bf00b54d)_\n\n  _[Springboard â€” Free Public Datasets](https://www.springboard.com/blog/free-public-data-sets-data-science-project/)_\n\n\nOutputs vs Outcomes\n\nLogic model\n\nIt focuses on how inputs and activities translate to outputs and eventually\n\nExample"
  },
  {
    "objectID": "qmd/project-planning.html#sec-proj-plan-gensteps",
    "href": "qmd/project-planning.html#sec-proj-plan-gensteps",
    "title": "59Â  Planning",
    "section": "59.2 General Steps for Starting a Project",
    "text": "59.2 General Steps for Starting a Project\n\nFraming a data question\n\nGuidelines\n\nPrecision: Be as detailed as possible in how you frame your questions. Avoid generic words like â€˜improveâ€™ or â€˜success.â€™ If you want to improve something, specify by how much. If you want to achieve something, specify by when.\n\nDecide before starting what the minimum project performance is to productionize (i.e.Â build a fully functional project) and launch (i.e.Â deliver to all your customers)\n\nSetting these thresholds at the beginning will help to prevent you from bargaining with yourself or stakeholders to deliver the project that might harm your business\n\nAfter working hard on a project and pouring resources into it, it can be difficult to end it.\n\n\n\nData-Centric: Consider the role of data in your organization. Can data help you answer this question? Is it clear what data you will need to collect to answer this question? Can progress on the task be codified into a metric that can be measured? If the answer to any of these questions is ambiguous or negative, investing in additional data resources may be an inefficient allocation of resources.\n\nExample\n\nFigure Out What Data You Need\n\nGuidelines\n\nNecessity: Are the data you are collecting necessary? Avoid data bloat, which is over-collecting data points for a â€œjust in caseâ€ scenario. This makes sustaining long-term data collection of those fields far more burdensome.\nAvailability: Are there external, publicly available data sources like government data that you can leverage? If the data needs to be collected, how easy is it to collect? If it is hard to collect, do you have a plan and resources in place as to how you can ensure it is collected at regular intervals over time? One-off data collection is rarely helpful as there is no reference point to measure the impact of interventions over time.\nMaintainability: Can you maintain and easily update this data over time? Is the cost of doing so sustainable? This is critical because longitudinal data collection with standard fields is one of the most valuable resources for a nonprofit. Avoid constantly changing field names, a moving target of data collection objectives, and costly data collection procedures (like purchasing third-party data) that are not sustainable given your overall budget.\nReliability: If you are using a third party data source, do you trust the quality of the data? What are the ways this data may be biased, incomplete, or inaccurate?\n\nExample\n\nA nonprofit with a mission to find long-term housing for the unhoused. This organization may want to answer the question: â€™what percentage of the unhoused have we been able to successfully rehabilitate in the area we serve?â€™\n\n\nOrganizational Buy-in\n\nProject manager tries to examine whether the project can fundamentally be classified as feasible and whether the requirements can be carried out with the available resources.\n\nExpert Interviews: Is the problem in general is very well suited for the deployment of data science and are there corresponding projects that have already been undertaken externally and also published?\nData science team: Are there a sufficient number of potentially suitable methods for this project and are the required data sources are available?\nIT department: check the available infrastructure and the expertise of the involved employees.\n\nMake sure everyone on the team agrees on what data that you want to collect and measure and who owns the data collection process.\nHaving someone of authority or thatâ€™s respected in each department involved in the development of the product will go a long way to building trust with users when itâ€™s fully deployed\n\nExample: Demand Forecasting Automation\n\nTeam should consist of Supply Chain department and close collaboration with Sales and IT\n\n\nExample\n\nSuppose teachers at a school are interested in fielding quantitative surveys to track student outcomes, but there exists little incentive for teachers to collect this data on top of regular work. As a result, only one teacher in the school volunteers to design and administer the survey to their class. However, the survey results will now be limited to the studentsâ€™ experiences and outcomes for just the one class. The measured outcomes will be biased because they will not capture any variance across students from different classes in the school.\n\n\nCalculate How Much Data You Need to Collect\nMake sure youâ€™ve answered these questions\n\nIs the problem definition clear and specific? Are there measurable criteria that define success or failure?\nIs it technically feasible to address the defined problem within the designated timeframe? Is the data required for the envisioned solution approach available?\nDo all relevant stakeholders agree with the problem definition, performance indicators, and selection criteria?\nDoes the intended technical solution resolve the initial business problem?"
  },
  {
    "objectID": "qmd/project-planning.html#sec-proj-plan-wkshp",
    "href": "qmd/project-planning.html#sec-proj-plan-wkshp",
    "title": "59Â  Planning",
    "section": "59.3 Workshopping",
    "text": "59.3 Workshopping\n\nHelps data scientists understand where their energy is most needed\nMisc\n\nNotes from: Successfully Combining Design Thinking and Data Science\nUsually last 1 hr\n\nSupplies\n\nDifferent coloured sticky notes â€” enough so that everyone has at least 15â€“20\nWhiteboard markers for the sticky notes. Permanent markers will likely cause some unintended permanent damage and thin, ballpoint pens are difficult to read unless youâ€™re right up close to them\n1 white board per group, or alternatively, 2 x large A2 pieces of paper\n\nParticipants\n\nKey business stakeholders involved in the area youâ€™re working on â€” you need management buy-in if anything is going to happen\nTwo or three people who will actually use the tools or insights youâ€™ll be delivering\nA facilitator (probably you) and a few members of your data science or analytics team\n\nChallenge Identification Workshop\n\nSuited for a situation when the business stakeholders know what their challenges are, but donâ€™t know what you can do for them.\nGoals:\n\nDS: Try to understand the customer pain points as well as you can.\n\nDo not try to develop solutions in this workshop\n\nStakeholders: get as many ideas out as possible in the time allotted.\n\nAdditional Supplies\n\nPoster or slide with user journey, or something similar, depending on the business\n\nHelps to guide participants to home-in on customer pain points\nNeeds to be done before the workshop\nExample\n\n\nProcess\n\nIntroduction\n\nBrief introduction of your team\nBrief 5-minute â€˜Best In Classâ€™ slide or two where I look at companies who are currently doing amazing things, preferably in the data science domain, in the area that our stakeholders work\nAddress goal of the workshop (see above for stakeholders)\n\nSplit into two or three groups\n\nIdeally between four to five people per group\nPut senior managers or executives in separate groups\n1 DS or analyst in each group\n\nWrite as many customer pain points as possible (aka Ideation)\n\nDuration: 25â€“30 minutes\nEverybody writes ideas on sticky note and puts on their board\n\nNo need for whole group to approve of the idea.\n\nâ€œBadâ€ ideas are weeded out later\n\n\nIf you see themes popping up, go ahead and group similar sticky-notes together\nDS or analyst needs to pay attention to each proposed idea, so as to be able to write a fuller description of the idea later on\n\nIf possible, note the author of each idea so as to be able to ask them questions later if needed.\n\n(You) Walk around to each group\n\nRemind them of the rules (get down as many ideas as possible)\nPrompt them with questions to get more ideation happening within the groups\n\n\nOne member from each of the groups presents their groupâ€™s key ideas back to the rest of the room\n\nDuration: 3-5 min per group\n\nPlace similar stickies into groups or themes\nVote on the best ideas\n\nDuration: 3 min\nEveryone gets three dot stickers and is allowed to vote for any ideas\nThey can put all their stickies on one idea or divide them up however they like\nNo discussions\nIf itâ€™s the case that one sticky within a theme of stickies gets all the votes, or even if the votes are all shared equally, consider all of those as votes towards the theme.\n\n\n\nPredefined Problem Sculpting Workshop\n\nThe difference between this approach and the first one is that here the business stakeholders already have an idea about what they need.\n\ne.g a metric of some sort, some kind of customer segmentation or some kind of tool\n\nExample\n\nDevelop â€˜affluency scoreâ€™ for each banking customer.\n\nGoal: Answer 2 questions\n\nWhat is it that weâ€™re trying to do?\n\nDefining what it is youâ€™re trying to do will help to define what is meant by the metric/segment/measure youâ€™re developing.\nFrom example (see Goal above), itâ€™s vital that everyone in the room understands what is meant by â€œaffluence.â€\n\nDoes it mean:\n\nhow much money someone currently has?\nIt is a measure of their potential spend?\nDoes it refer to their lifestyle and how much debt they may take on and can realistically pay?\n\n\n\nWhy do we want to do this?\n\nThe answer to why has design implications\nExamples\n\nIs it something weâ€™ll use to set strategic goals?\nDo we want to use it to identify cross or upsell opportunities?\n\n\n\nAdditional Supplies:\n\nPoster or slide with these 2 questions\n\nProcess\n\nIntroduction\n\nBrief introduction of your team\nAddress goals of the workshop\n\nSplit into two or three groups\n\nIdeally between four to five people per group\nPut senior managers or executives in separate groups\n1 DS or analyst in each group\n\nHave groups answer the â€œWhatâ€ question\n\nDuration: 15 min\n\nFeedback session with whole workshop\n\nDuration: 3 min\nGroup answers are presented and discussed\n\nHave groups answer the â€œWhyâ€ question\n\nDuration: 15 min\n\nFeedback session with whole workshop\n\nDuration: 3 min\nGroup answers are presented and discussed\n\n\nCompare and address gaps\n\nDuration: 4 min\nCompare all group answers against the current solution\n\ne.g.Â does the current metric represent these answers to â€œwhatâ€ and â€œwhyâ€?\n\nIf there are gaps between the group answers and the current solution, try to figure out how best the fill those gaps.\n\n\nPost-Workshop Debrief\n\nShould occur the same day as the workshop\nDocument sticky notes and key ideas\n\nThe data scientists/analysts embedded in the groups should be able to expand on the ideas on the sticky notes\nIf the author of the idea was recorded, that should be included as well\n\nAs youâ€™re starting to think of solutions to the pain points you discussed, reach out to authors/stakeholders to get their opinions on your thoughts and to understand where you might be able to get the data from.\n\n\nHave data team ideate on solutions to these painpoints, etc."
  },
  {
    "objectID": "qmd/project-planning.html#sec-proj-plan-thgscndr",
    "href": "qmd/project-planning.html#sec-proj-plan-thgscndr",
    "title": "59Â  Planning",
    "section": "59.4 Things to consider when choosing a project",
    "text": "59.4 Things to consider when choosing a project\n\nMisc\n\nAlso see Optimization, general &gt;&gt; Budget Allocation\n\nConsiderations\n\nBalancing the goals of your department with the desires of stakeholders\n\nBefore handling projects that are possible with the available data, address projects of immediate need to the business according to stakeholders.\n\nNeed to feel theyâ€™re getting what they think is value from the data department.\n\nAlso, have to ask, â€œhow are we going to make money from this output?â€ and metrics â€œWhat organizational KPIs are tied to these metrics and how?â€.\n\nIt is reasonable to (tactfully) say that the â€œwinsâ€ column of your self-evaluation needs some big victories this year, and the â€œtop 10 products shipped this hourâ€ dashboard isnâ€™t going to get us where we want to be as an organization. Some fluff is acceptable to keep the peace.\nNo one will rush to the defense of the data team come budget season, regardless of how many low-value asks you successfully delivered, as requested, in the prior year (this is called executive amnesia).\n\n\nBeing useful\n\nSituations\n\nImproving upon a metric score (business or model)\n\nâ€œanalyze actual failure cases and have a hard and honest think about what will fix the largest number of them. (My prediction: improving the data will by and large trump a more complex method.) The point is, you have to have a real think about what will actually improve the number, and that might involve work thatâ€™s stupid and boring. Donâ€™t just reach for a smart new shiny thing and hope for the best.â€\n\na lot of product people running around saying â€œwe want to do this thing asap, but we donâ€™t know if itâ€™s possible\n\nJust find a product person who seems sane, buddy up with them, and work on what they want. They should be experts in their product, have an understanding of what the potential market for it is, and so really know what will be useful and what wonâ€™t.\n\n\n\n\nProcess\n\nScore Potential Projects\n\nCreate a score for â€œimpactâ€\n\nPossible impact score features\n\nBack of the envelope monetary value\nCustomer experience score / Net Promoter Score (NPS)\n\n\nCreate a score for â€œdifficultyâ€\n\nPossible difficulty score features\n\nhow long you believe the project will take to build\nresource constraints into consideration\nhow difficult the data may be to acquire and clean\n\n\nCategorize each project as Low Hanging Fruit, Quick Wins, Long Term Projects, and Distractions based the Impact and Difficulty Scores\n\nSee chart above\n\nPrioritize Low Hanging Fruit\nConsult with stakeholders and decide which Quick Wins and Long Term Projects to pursue"
  },
  {
    "objectID": "qmd/project-planning.html#sec-proj-plan-elevptch",
    "href": "qmd/project-planning.html#sec-proj-plan-elevptch",
    "title": "59Â  Planning",
    "section": "59.5 Elevator Pitch",
    "text": "59.5 Elevator Pitch\n\nComponents\n\nValue Proposition:\n\nDescribe the problem\n\nWho is dissatisfied with the current environment\nThe targeted segment that your project is addressing\n\nDescribe the product/service that you are developing and what specific problem that it is solving\n\nYour Differentiation:\n\nDescribe the alternative (perhaps your competition or currently available product/service)\nDescribe a specific (not all) functionality or feature that is different from the currently available product/service.\n\n\nDelivery Formula\n\nFor [targeted segment of customers/users],\nWho are dissatisfied with [currently available product/service]\nOur product/service is a [new product category]\nThat provides a [key problem-solving capability]\nUnlike [product/service alternative],\nWe have assembled [Key product/service features for your specific application]\n\nExample For political election campaign managers,\n  **Who are dissatisfied with** the traditional polling products,\n\n  **Our application is a** new type of polling product\n\n  **That provides** the ability to design, implement, and get results within 24 hours.\n\n  **Unlike** the other traditional polling products that take over 5 days to complete,\n\n  **We have assembled** a quick and more economic yet comparably accurate polling product.\nExample For front line criminal investigators,\n  **Who are dissatisfied with** generic dashboards that display too much unnecessary information,\n\n  **Our application is a** new type of Intelligence product\n\n  **That provides** a highly customized and machine learning-enabled risk assessment tool that allows the investigator to uncover a hidden network of potentially criminals.\n\n  **Unlike** the current dashboard that provides information that is often not very useful,\n\n  **We have assembled** an intelligence product that allows them to make associations between known and unknown actors of interest."
  },
  {
    "objectID": "qmd/project-planning.html#sec-proj-plan-edarp",
    "href": "qmd/project-planning.html#sec-proj-plan-edarp",
    "title": "59Â  Planning",
    "section": "59.6 Exploratory Data Analysis Research Plan (EDARP)",
    "text": "59.6 Exploratory Data Analysis Research Plan (EDARP)\n\nImperative that this plan be formulated before work on the project begins\n\n\nDevelop a research question and objectives to be met\n\nA concise research question and objective allows the stakeholders to be informed on how their departments will be affected.\nSoft skills are important in being able to make a stakeholder comfortable with answering questions.\nIt helps if each stakeholder feels like their input is necessary to the project. More likely to get invested in its success.Â May require salesmanship to convince them that the knowledge of self has future benefits\nComing up with a concise question often involves drilling down into the mechanics of an organization.\n\nThis process can lead to discovering the importance of the research. Also, can lead to other research questions.\n\nIf stakeholder is ambiguous about what they want. Useful to use a backwards path by starting with what the stakeholder envisions as the final output\n\nâ€œCan you tell me about the decisions you are going to make based on the output of this request? Understanding those decisions will help us ensure that the (report/data/model) we build for you will fulfill your needs.â€\n\nIf a concise question cannot be obtained or the objective is open-ended, you should gather all the information obtained and use it determine what you think the objective of the project should be. Then, develop a prototype\n\nAfter a portion of the project is completed, it will be necessary to share the progress with the stakeholders and revise aspects of the project as needed. More of an iterative process.\nThe iteration can increase buy-in from stakeholders and develop a more concise question\n\nAfter the question is decided upon, what does success look like? What are the metrics of evaluation?Â Â \n\nDefine methods to address the question\n\ntype of method depends on the type of question\n\ndescriptive\n\nUsually answered with historical data, e.g historical trends. Often presented in dashboards\nmean, min, max, etc.\nplots for understanding relationships\noutlier detection\n\npredictive\n\npredictive models and forecasting\nregression, classification, clustering\n\nprescriptive\n\npredictive models + proposing actions\n\n\n\nDetermine which data is on-hand and which need to be acquired\n\nAlso see pipeline section below\nNote data sources\n\nStructured\nUnstructured\nInternal\nExternal\n\nWrangle data\n\nDetermine the budget and adjust prior steps to conform within this budget\n\nDetermined required tools\n\ncompute capacity\nend user requirements\nbusiness interests\n\nMake effort to minimize cost as much as possible. Makes dialogue with stakeholders easier\n\nStakeholders\n\nA departments that will make use of the output or be effected by should be involved\n\nConvincing them to be involved is important if any issues with budgets, etc. pop-up. Having more people on your side will help.\n\nUsing a proof-of-concept or information of past successes of such projects within or outside the organization can help.\nNeed to proof on how this research will increase their ROI. Prototypes with datascience outputs can go a long way convincing or increasing buy-in from skeptical stakeholders.\n\n\nTheir concerns should be noted and addressed.\nOften times they are the end-user, so having them involved can lead to a better output. A greater probability of the project being involved in decision making.\n\nHow will the end-users make use of the output?\n\nShould the output be a report or app?\n\nDevelop a plan of action for upkeep or maintenance of the project\n\nMonitoring model drift\nChanges in the business\nResolve any lingering user concerns\nDetermine schedule for updating projectÂ \nSchedule stakeholder meeting to discuss the project and determine if further steps needed"
  },
  {
    "objectID": "qmd/project-planning.html#sec-proj-plan-decmod",
    "href": "qmd/project-planning.html#sec-proj-plan-decmod",
    "title": "59Â  Planning",
    "section": "59.7 Decision Models",
    "text": "59.7 Decision Models\n\n\nNotes from https://towardsdatascience.com/learn-how-to-financially-evaluate-your-analytics-projects-with-a-robust-decision-model-6655a5229872\nAlso see\n\nAlgorithms, Business &gt;&gt; Cost-Benefit Analysis\nOptimization, general &gt;&gt; Budget Allocation\nVideo Bizsci Learning Lab 68\n\n{tidyquant} has excel functions for Net Present Value (NPV), Future Value of Cashflow (FV), and Present Value of Future Cashflow (PV)\n** A lot of this stuff wasnâ€™t used in the examples**\n\nFocus on fixed and variable costs, investment, depreciation, cost of capital, tax rate, revenue, qualitative factors\n\nNet Present Value (NPV) is used as the final quantitative metric (Value in flow chart)\n\nShort running projects can just use the orange boxes to calculate value\nLong running projects would use orange, blue, and gray boxes to calculate value\n\nThe values for each variable need to calculated for each year (or whatever time unit) of the project.\nDifferent scenarios (pessimistic, optimistic) can be created by modifying the parameters of the model like the life span, the initial investment, or the costs, to see the potential financial impact on Value.\nMaking a decision\n\nCompare your NPV and the Internal Rate of Return (IRR) with alternative analytics projects that the organization is considering to see which one is more attractive.\nBalance the quantitative and qualitative factors while considering the potential risks of the project.\n\nExample\n\nâ€œIn quantitative factors, we used the cash flow model where we calculated a net present value of USD $7,187,955. Related to qualitative factors and potential risks, we can conclude that if the project is back up by an influential group of leaders, then the positive aspects exceed the negative ones. On the balance, we would advise a GO decision for this project.â€\n\n\n\nExecutive Presentation\n\nThe conclusion should be short, precise, and convincing format\nCrucial to persuade key stakeholders to invest in your project\nUse a good action title, writing down the most relevant requirements of your project, use a chart to visualize your results, and have some key takeaways on the bottom with a recommendation.\n\nProposal\n\nSections 1-3 should be put into a concise executive summary\nTell a story\n\nEvery item should be consistent with every other part\n\ne.g.Â a deliverable shouldnâ€™t be mentioned if itâ€™s not part of solving the issue in the problem statement\n\nShould flow from more abstract ideas to more and more detail\n\nFormat\n\nProblem Statement\n\nPaint a bleak picture and explain how the problem couldâ€™ve been overcome if your project were in production when the problem occurred\nList examples where risk and costs occurred, opportunities were missed, etc. and how your project wouldâ€™ve prevented or taken advantage of these things if it had been in production\n\nskill upgrades or efficiency gains are NOT good answers\n\n\nVision\n\nTies your project to the companies long range strategy\n\nBenefits\n\ndetailsÂ  about the specific new capabilities provided or costs avoided\nneed to be quantifiable\n\nDeliverables\n\ndetails about other non-primary benefits\n\nSuccess Criteria\n\nCharacteristics\n\nspecific, measurable, bounded by a time period, realistic\n\n\nThe plan\n\noverview of the steps the project will take\n\nincludes deadlines, waterfall or agile approach, etc.\n\n\nCost/Budget\n\n\nTerms\n\nCost/Investment\n\nFixed\n\nPersonnel salaries\n\nIf youâ€™re using outside data consultants, Glassdoor has average salaries, https://www.glassdoor.com/Salaries/data-engineer-salary-SRCH_KO0,13.htm\n\nexample: ~$100K each\n\n\nAdditional investments could be access to external data sources, research, and software licenses\n\nTableau can cost around $48K per year\n\nTraining for any additional or current personnel on tools or subject matter\n\nworkshops, online learning\ntax-reclamation example: ~$12K total\n\n\nVariable\n\nCloud storage\n\ntax-reclamation example: AWS (pay-as-you-go) $75K-$95K per year\n\n\n\nTax rate\n\ntax-reclamation example: 28%\n\nRevenue\n\nForecasts of revenue\n\nCorrection Factors\n\nExplains the difference between profit after tax and cashflow\ne.g.Â depreciation of investment, changing working capital, investment, change in financing or borrowing\n\ncost of sales = (revenue*some%) - 1\n\nrefers to what the seller has to pay in order to create the product and get it into the hands of a paying customer.\n\nMaybe this is a catch-all for anything else not covered in the Cost/Investment section\n\naka cost of revenue (sales/service) or cost of goods (manufacturing)\n30% used in example\nBegins after year 1\n\ndepreciation per year = (investment - salvage_value)/(num_years - 1)\n\nFor an datascience project, the salvage_value is 0 most of the time.\ndepreciation value is listed for each year after the first year of the project\ngets subtracted from revenue in calculation of profit_before_tax calculation\ngets added to profit_after_tax in cashflow equation\n\nChange in working capital = (expected_yearly_revenue/365) * customer_credit_period\n\ncustomer_credit_period (days): number of days that aÂ customerÂ is allowed to wait before paying an invoice. The concept is important because it indicates the amount of working capital that a business is willing to invest in its accounts receivable in order to generate sales.\n\n60 days used in example\n\nOnly a cost in the 1st yr and then it is recouped in the last year (somehow)\n\nsounds like this whole thing is just an accounting procedure\n\nDunno how this relates to a datascience project\n\n\n\nCost of Capital (also see discount rate/hurdle rate, cost of capital in Business Algorithms note)\n\nUsed to characterize risk here. Think 5-10% is used for back-of-the-envelope calculations.\n\ntax-reclamation example: 8.3%\n\nAlso see\n\nAlgorithms, Product &gt;&gt; Cost Benefit Analysis (CBA) &gt;&gt; Internal Rate of Return\nMorgan Stanleyâ€™s Guide to Cost of Capital (Also in R &gt;&gt; Documents &gt;&gt; Business)\n\nThread about the guide\n\n\ncost of capital typically calculated as Weighted Average Cost of Capital (WACC)\n\nWACC = w1*cost_of_equity + w2*cost_of_debt\n\nw1, w2 depend on the companyâ€™s capital structure (e.g.Â 70% equity, 30% debt)\nCAPM = Rf + Î² (Rm - Rf)\n\ncost_of_equity approximated using capital asset pricing model (CAPM)\nRf = risk-free rate of return\nRm = market rate of return\nÎ² = risk estimate or a companyâ€™s stock beta or a similar public companyâ€™s stock beta\n\ncost_of_debt = (interest_expense / total_debt) * (1 - T)\n\ninterest_expense = interest paid on companyâ€™s current debt\n\nSince interest expense is tax-deductible, the debt is calculated on an after-tax basis\n\nT = companyâ€™s marginal tax rate\n\n\n\nAs of January 2019, transportation in railroads has the highest cost of capital at 11.17%. The lowest cost of capital can be claimed by non-bank and insurance financial services companies at 2.79%.\nHigh CoCs - Biotech and pharmaceutical drug companies, steel manufacturers, Internet (software) companies, and integrated oil and gas companies. Those industries tend to require significant capital investment in research, development, equipment, and factories.\nLow CoCs - Money center banks, power companies, real estate investment trusts (REITs), retail grocery and food companies, and utilities (both general and water). Such companies may require less equipment or benefit from very steady cash flows.\n\nNet Present Value (NPV)\n\nUses cashflow (excludes year 1) and cost of capital as inputs\n\nExcel has some sort of NPV wizard that does the calculation (see tidyquant for npv function)\nExample in article provided link to his googlesheet so the formula is there or maybe googlesheets has a wizard too.\n\n\nInternal Rate of Return (IRR)\n\nItâ€™s the value that the cost of capital (aka discount rate) would have to be for NPV and cost to zero out each other.\nThe interest rate at which a company borrows against itself as a proxy for opportunity cost. Typically, large and/or public organizations have a budgetary IRR of 10% to 15% depending on the industry and financial situation.\nHigher is better\nInvestopedia has an easy formula\n\nhttps://www.investopedia.com/terms/i/irr.asp\n\n\nQualitative Factors\n\nOther things that have value but are difficult to quantify. Might change a quantitatively negative value project into a positive value project\n\nUses columns negative, positive, impact\n\nnegative and positive are indicators\nimpact has values low, medium, and high\n\n\nExamples\n\nProject has flexibility (+) (calculated using real options analysis(?))\nStrategic Fit (+) - supports strategy of the company (?)\n\nincreases trust with public\nmarketing boosted because they can use buzzwords like â€œAIâ€ or â€œdata-drivenâ€ in ads\n\nIncreases competency (+) - may help the company down the road\n\nmore agile, data-driven, familiarity with newer technologies\n\nred tape or bureaucracy or politics causing delays (-)\nImplementation (-)\n\ncan be a negative, if the leadership thatâ€™s needed is tied up with other projects\nChallenges"
  },
  {
    "objectID": "qmd/project-planning.html#sec-proj-plan-capf",
    "href": "qmd/project-planning.html#sec-proj-plan-capf",
    "title": "59Â  Planning",
    "section": "59.8 Choosing a Pipeline Framework",
    "text": "59.8 Choosing a Pipeline Framework\n - Kind of like working backwards in the data analysis workflow (design driven development) - Quantify the problems and develop KPIs that can inform the direction of the business - 10-15 organizational KPIs is common - Examples: Number of daily sales, the number of new customers, decreasing cost of operations/logistics - Build data pipelines around these critical KPIs - What data can we use that we already have access to? - What kind of internal data do we need to capture? - What kind of third party/external data could be useful? - What is the least amount of data available that can be used to approximate each KPI metric? - After the simplest version of the metric is built, you can brainstorm on how it can be improved. - Personalize a general metric to your specific business. - Split a general metric into smaller metrics can increase accuracy (e.g.Â product line, geography, etc.) - What kind of storage will we need? - Adding a pipeline for a new data source - Capture a sample of the data to examine its potential. (e.g.Â cookies on your website) - If metric measured from new data is potentially valuable, build a simple, less robust (without sacrificing too much data integrity) pipeline to your lake. - Meaning fewer built-in quality checks - Doing it this way does add technical debt - If metric feeds into a company-wide metric, then additional checks will need to be added - E.g. no duplicate datasets, data corruption, or data losses. - After 6 months or so, if metric remains useful, make pipeline more robust. - Adding third party data pipeline (e.g.Â Equifax) - E.g. demographic, income data - If a product has a certain demographics associated with it, this data can be used to weight customers relative to the demographic specifics of that product. - Get a sample first to determine if itâ€™s worth it - 3rd party data is expensive to buy and usually a reoccurring cost. - Can be time-consuming and resource intensive to add the pipeline - Get transparency on how the data was collected and validity checks used - Periodically reevaluate whether the metrics fundamentally still make sense from a business perspective and how they can be improved. - Expand pipelines to capture additional data in order to continue to refine metrics - Drop and add metrics as business and trends change. - Redefine metrics as needed -"
  },
  {
    "objectID": "qmd/python-classes.html#sec-py-class-misc",
    "href": "qmd/python-classes.html#sec-py-class-misc",
    "title": "Classes",
    "section": "Misc",
    "text": "Misc\n\nWhen to use classes:\n\nWhen you have a group of functions and they have many of the same arguments, this indicates a class might be helpful. Also, if one or more of the functions is used in the other functions, this is also an indication that creating a class would be better.\n\nSee Python, Snippets &gt;&gt; Refactor a group of functions into a class\n\nWhen you have code and data that go together and need to keep track of the current state\n\ne.g.Â managing a bunch of students and grades\n\nWhen you see hierarchies, using classes leads to better code organization, less duplication, and reusable code.\n\nYou can make a single change in the base class (parent) and all child classes pick up the change\nExample: Report class\n\nYou can have a base class with shared attributes like report name, location and rows. But when you go into specifics like formats (xml, json, html), you could override a generate_report method in the subclass.\n\n\nEncapsulation\n\nWhen you want to separate external and internal interfaces in order to (ostensibly) hide internal code from the user.\nKeeps excess complexity from the user\n\n\nUse camel case for class names\nUse snake case for methods and attributes\nAlways use self as the first argument of a method\nWrite docstrings for your classes so that your code is more understandable to potential collaborators and future you.\nCreate a Class that allows method chaining\n\n\nreturn self is what allows the chaining to happen"
  },
  {
    "objectID": "qmd/python-classes.html#sec-py-class-terms",
    "href": "qmd/python-classes.html#sec-py-class-terms",
    "title": "Classes",
    "section": "Terms",
    "text": "Terms\n\nClass inheritance - mechanism by which one class takes on the attributes and methods of another\nclass Employee:\nÂ  Â  def __init__(self, name, salary=0):\nÂ  Â  Â  Â  self.name = name\nÂ  Â  Â  Â  self.salary = salary\n__init__() is a constructor method. It assigns (or initializes) attributes that every object (aka instance) for this class must have.\nself is the 1st argument in any method definition. It refers to a particular instance.\nself.salaryÂ  = salary creates an attribute calledÂ salaryÂ and assigns to it the value of theÂ salaryÂ parameter (default set to 0)\nClass attributes are attributes that have the same value for all class instances.\n\nAccessing a class attribute\n# access first employee's name attribute\ne1.name\n# access second employee's salary attribute\ne2.salary\n\nInstance Attributes - Values for these attribute depend on the instance (e.g.Â they vary depending on each employee)\nInstantiate - Creating a new object from a class\ne1 = Employee(\"yyy\", 5000)Â  # name, salary\ne2 = Employee(\"zzz\", 8000)"
  },
  {
    "objectID": "qmd/python-classes.html#sec-py-class-meth",
    "href": "qmd/python-classes.html#sec-py-class-meth",
    "title": "Classes",
    "section": "Methods",
    "text": "Methods\n\nMisc\n\nIn most classes, best practice to at least include __init__ and __repr__ methods\n\n\n\nInstance Method\n\nFunctions that can only be called by an object from this class\n\nSimilar to regular functions with the difference of having â€œselfâ€ as the first parameter\n\nclass Employee:\nÂ  Â  def __init__(self, name, salary=0):\nÂ  Â  Â  Â  self.name = name\nÂ  Â  Â  Â  self.salary = salary\n\n#Instance method\nÂ  Â  def give_raise(self, amount):\nÂ  Â  Â  Â  self.salary += amount\nÂ  Â  Â  Â  return f\"{self.name} has been given a {amount} raise\"\n\n# calling an instance method\n# instantiate object first\nobject = MyClass()Â \nobject.method()\n\n\n\nMagic Methods (aka Dunder or Special Methods)\n\nArenâ€™t meant to be called, usually invoked by an operation\n\nExamples\n\n__add__ invoked by myclass() + myclass()\n__str__ invoked by str(myclass())\n\nExample\nclass Address:\nÂ  Â  def __init__(self, street, city, state, zipcode, street2=''):\nÂ  Â  Â  Â  self.street = street\nÂ  Â  Â  Â  self.street2 = street2\nÂ  Â  Â  Â  self.city = city\nÂ  Â  Â  Â  self.state = state\nÂ  Â  Â  Â  self.zipcode = zipcode\nÂ  Â  def __str__(self):\nÂ  Â  Â  Â  lines = [self.street]\nÂ  Â  Â  Â  if self.street2:\nÂ  Â  Â  Â  Â  Â  lines.append(self.street2)\nÂ  Â  Â  Â  lines.append(f'{self.city}, {self.state} {self.zipcode}')\nÂ  Â  Â  Â  return '\\n'.join(lines)\n\n&gt;&gt;&gt; address = Address('55 Main St.', 'Concord', 'NH', '03301')\n&gt;&gt;&gt; print(address)\n55 Main St.\nConcord, NH 03301\n\nCan be an instance or class type of method\nfrom datetime import datetime, timedelta\nfrom typing import Iterable\nfrom math import ceil\nclass DateTimeRange:\nÂ  Â  def __init__(self, start: datetime, end_:datetime, step:timedelta = timedelta(seconds=1)):\nÂ  Â  Â  Â  self._start = start\nÂ  Â  Â  Â  self._end = end_\nÂ  Â  Â  Â  self._step = step\n\nÂ  Â  def __iter__(self) -&gt; Iterable[datetime]:\nÂ  Â  Â  Â  point = self._start\nÂ  Â  Â  Â  while point &lt; self._end:\nÂ  Â  Â  Â  Â  Â  yield point\nÂ  Â  Â  Â  Â  Â  point += self._step\n\nÂ  Â  def __len__(self) -&gt; int:\nÂ  Â  Â  Â  Â  Â  return ceil((self._end - self._start) / self._step)\n\nÂ  Â  def __contains__(self, item: datetime) -&gt; bool:\nÂ  Â  Â  Â  Â  Â  mod = divmod(item - self._start, self._step)\nÂ  Â  Â  Â  Â  Â  return item &gt;= self._start and item &lt; self._end and mod[1] == timedelta(0)\n\nÂ  Â  def __getitem__(self, item: int) -&gt; datetime:\nÂ  Â  Â  Â  n_steps = item if item &gt;= 0 else len(self) + item\nÂ  Â  Â  Â  return_value = self._start + n_steps * self._step\nÂ  Â  Â  Â  if return_value not in self:\nÂ  Â  Â  Â  Â  Â  raise IndexError()\nÂ  Â  Â  Â  return return_valueÂ \n\nÂ  Â  def __str__(self):\nÂ  Â  Â  Â  return f\"Datetime Range [{self._start}, {self._end}) with step {self._step}\"\n\nClass DateTimeRange has methods that allows you to treat a date-range object like a list\n\nJust for illustration. Think methods in pandas can do this stuff\n\n__iter__ method - generator function that creates one element at a time, yields it to the caller, and allows the caller to process it\n\nExample creates datetime ranges instead of numeric ranges\n\n__len__ - find out the number of elements that are part of your range\n__getitem__- uses indexing syntax to retrieve entries from your objects\n__contains__- checks if an element is part of your range. T/F\n\ndivmod returns quotient and remainder.\n\nUsing these magic methods\nmy_range = DateTimeRange(datetime(2021,1,1), datetime(2021,12,1), timedelta(days=12)) #instantiate\nprint(my_range)Â  Â  Â  Â  Â  Â  Â  # __init__ or maybe __str__\nfor r in my_range:Â  Â  Â  Â  Â  # __iter__\nÂ  Â  do_something(r)\nlen(my_range)Â  Â  Â  Â  Â  Â  Â  Â  # __len__\nmy_range[-2] in my_rangeÂ  Â  # __getitem__ (neg indexing), __contains__\n\nOthers\n\n__repr__ - Creates a string representation of the class object\n\nExample\nclass Person:\n    def __init__(self, name, age):\n        self.name = name\n        self.age = age\n\n    def __repr__(self):\n        return f\"Person(name='{self.name}', age={self.age})\"\n\nperson = Person(\"John\", 25)\nprint(person)\n#&gt; Person(name='John', age=25)\n\n__eq__ - Provides a method for comparing two class objects by their values.\n\nExample\nclass Person:\n  def __init__(self, age):\n    self.age = age\n\n  def __eq__(self, other):\n    return self.age == other.age\n\nalice = Person(18)\nbob = Person(19)\ncarl = Person(18)\n\nprint(alice == bob)\n#&gt; False\n\nprint(alice == carl)\n#&gt; True"
  },
  {
    "objectID": "qmd/python-classes.html#sec-py-class-inher",
    "href": "qmd/python-classes.html#sec-py-class-inher",
    "title": "Classes",
    "section": "Inheritance",
    "text": "Inheritance\n\n\nSome notes from this much more detailed example (article)\n\nShows how to combine different scriptsÂ  (aka modules) and diagram the hierarchies\nSome good debugging too\n\nRunning &lt;class&gt;.__mro__will show you the order of inheritance.\n\n\nInheritance models what is called an â€œis aâ€ relationship. This means that when you have a Derived (aka subclass, child) class that inherits from a Base (aka super, parent) class, you created a relationship where Derived is a specialized version of Base.\nChild classes inherit all of their parentâ€™s attributes and methods, but they can also define their own attributes and methods.\nCan override or extend parent class attributes and methods\nclass Manager(Employee):\nÂ  Â  pass\n\nm1 = Manager(\"aaa\", 13000)\n\nManager is the child class and Employee is the parent class (see top)\nChild classes donâ€™t require a constructor method for an object to be created\n\nExtending the instance attributes of the parent class\nclass Manager(Employee):\nÂ  Â  def __init__(self, name, salary=0, department):\nÂ  Â  Â  Â  Employee.__init__(self, name, salary=0)\nÂ  Â  Â  Â  self.department = department\n\nContructor method for the child class with the new attribute, â€œdepartment,â€ in the arguments.\nParent class (Employee) constructor method is called and a new attribute, department, is defined.\n\n\n\nsuper()\n\nAlternative way of extending instance attributes through inheritance\nExample\nclass Rectangle:\nÂ  Â  def __init__(self, length, width):\nÂ  Â  Â  Â  self.length = length\nÂ  Â  Â  Â  self.width = width\nÂ  Â  def area(self):\nÂ  Â  Â  Â  return self.length * self.width\nÂ  Â  def perimeter(self):\nÂ  Â  Â  Â  return 2 * self.length + 2 * self.width\nclass Square(Rectangle):\nÂ  Â  def __init__(self, length):\nÂ  Â  Â  Â  super().__init__(length, length)\nclass Cube(Square):\nÂ  Â  def surface_area(self):\nÂ  Â  Â  Â  face_area = super().area()\nÂ  Â  Â  Â  return face_area * 6\nÂ  Â  def volume(self):\nÂ  Â  Â  Â  face_area = super().area()\nÂ  Â  Â  Â  return face_area * self.length\n\n&gt;&gt;&gt; cube = Cube(3)\n&gt;&gt;&gt; cube.surface_area()\n54\n&gt;&gt;&gt; cube.volume()\n27\n\nClass Cube inherits from Square and extends the functionality of .area() (inherited from the Rectangle class through Square) to calculate the surface area and volume of a Cube instance Also notice that the Cube class definition does not have an .__init__(). Because Cube inherits from Square and .__init__() doesnâ€™t really do anything differently for Cube than it already does for Square, you can skip defining it, and the .__init__() of the other child class (Square) will be called automatically.\nsuper(Square, self).__init__(length, length) is equivalent to calling super without parameters (see above example)\nUsing super(Square, self).area() in class Cube. Setting the 1st parameter to Square instead of Cube causes super() to start searching for a matching method (in this case, .area()) at one level above Square in the instance hierarchy, in this case Rectangle.\n\nIf Square had an .area method, but you wanted to use Rectangleâ€™s instead, this would be a way to do that.\n\nNote another difference between using super() and using the class name (e.g.Â the first example) â€” â€œselfâ€ is NOT one of the args in super()\nExample: Child class of two separate hierarchies\n# Super class\nclass Rectangle:\nÂ  Â  def __init__(self, length, width, **kwargs):\nÂ  Â  Â  Â  self.length = length\nÂ  Â  Â  Â  self.width = width\nÂ  Â  Â  Â  super().__init__(**kwargs)\nÂ  Â  def area(self):\nÂ  Â  Â  Â  return self.length * self.width\nÂ  Â  def perimeter(self):\nÂ  Â  Â  Â  return 2 * self.length + 2 * self.width\n# Child class: Square class inherits from the Rectangle class\nclass Square(Rectangle):\nÂ  Â  def __init__(self, length, **kwargs):\nÂ  Â  Â  Â  super().__init__(length=length, width=length, **kwargs)\n# Child class: Cube class inherits from Square and also from Rectangle classes\nclass Cube(Square):\nÂ  Â  def surface_area(self):\nÂ  Â  Â  Â  face_area = super().area()\nÂ  Â  Â  Â  return face_area * 6\nÂ  Â  def volume(self):\nÂ  Â  Â  Â  face_area = super().area()\nÂ  Â  Â  Â  return face_area * self.length\n\n# Class (separate)\nclass Triangle:Â \nÂ  Â  def __init__(self, base, height, **kwargs):Â \nÂ  Â  Â  Â  self.base = baseÂ \nÂ  Â  Â  Â  self.height = heightÂ \nÂ  Â  Â  Â  super().__init__(**kwargs)Â \nÂ  Â  def tri_area(self):Â \nÂ  Â  Â  Â  return 0.5 * self.base * self.height\n\n# Inherits from a child class (and super class) and a class\nclass RightPyramid(Square, Triangle):\nÂ  Â  def __init__(self, base, slant_height, **kwargs):\nÂ  Â  Â  Â  self.base = base\nÂ  Â  Â  Â  self.slant_height = slant_height\nÂ  Â  Â  Â  kwargs[\"height\"] = slant_height\nÂ  Â  Â  Â  kwargs[\"length\"] = base\nÂ  Â  Â  Â  super().__init__(base=base, **kwargs)\nÂ  Â  def area(self):\nÂ  Â  Â  Â  base_area = super().area()\nÂ  Â  Â  Â  perimeter = super().perimeter()\nÂ  Â  Â  Â  return 0.5 * perimeter * self.slant_height + base_area\nÂ  Â  def area_2(self):\nÂ  Â  Â  Â  base_area = super().area()\nÂ  Â  Â  Â  triangle_area = super().tri_area()\nÂ  Â  Â  Â  return triangle_area * 4 + base_area\n\n&gt;&gt;&gt; pyramid = RightPyramid(base=2, slant_height=4)\n&gt;&gt;&gt; pyramid.area()\n20.0\n&gt;&gt;&gt; pyramid.area_2()\n20.0\n\nRightPyramid inherits from a child class (Square) and a class Triangle\n\nSince Square inherits from Rectangle, so does RightPyramid\nTriangle is a separate class not part of any hierarchy\n\nIf thereâ€™s an .area method in either of the classes, then super will search hierarchy of the class listed first (Square), then the hierarchy of the class listed second (Triangle)\n\nBest practice to make sure each class has different method names.\n\nEach class with an __init__ constructor gets a super().__init__() and **kwargs added to its args\n\nWhich is every class sans Cube. If Cube was inherited by a class, I think it would require an __init__ constructor and the super().__init__(**kwargs) expression.\nWithout doing this, calling .area_2() in RightPyramid will give us an AttributeError since .base and .height donâ€™t have any values. (donâ€™t completely understand this explanation)\n\nkwarg flow through super().__init__()\n\nIn RightPyramid __init__,Â  slant_height and base values are assigned to height and length keys in the kwargs dict\nsuper() passes base and the kwargs dict up to Square and Triangle\n\nTriangle uses height from kwargs and base\nSquare uses length from kwargs\n\nSquare passes length to Rectangle as values for both width and length\n\n\nAll classes now have the argument values necessary for their functions to work.\n\nNow RightPyramid can call those other classesâ€™ methods (e.g.Â .area and .perimeter from Rectangle and tri_area from Triangle)\n\nI believe since every class has **kwargs arg in their super().__init__, each has every value in the kwarg dict even if they donâ€™t need it.\n\nSo probably possible to add functions to those classes that would use those values\n\n\n\nExample: Using super with method other than __init__\nclass SalaryPolicy:Â \nÂ  Â  def __init__(self, weekly_salary):Â \nÂ  Â  Â  Â  self.weekly_salary = weekly_salaryÂ \nÂ  Â  def calculate_payroll(self):Â \nÂ  Â  Â  Â  return self.weekly_salary\n\nclass CommissionPolicy(SalaryPolicy):Â \nÂ  Â  def __init__(self, weekly_salary, commission):Â \nÂ  Â  Â  Â  super().__init__(weekly_salary)Â \nÂ  Â  Â  Â  self.commission = commissionÂ \nÂ  Â  def calculate_payroll(self):Â \nÂ  Â  Â  Â  fixed = super().calculate_payroll()Â \nÂ  Â  Â  Â  return fixed + self.commission\n\nIn CommissionPolicyâ€™s calculate_payroll, super() accesses SalaryPolicyâ€™s calculate_payroll method to get the weekly_salary value\n\n\n\n\n\nDiamond Problem\n\n\nAppears when youâ€™re using multiple inheritance and deriving from two classes that have a common base class.\n\nThis can cause the wrong version of a method to be called.\ne.g.Â TemporarySecretary uses multiple inheritance to derive from two classes that ultimately also derive from Employee. This causes two paths to reach the Employee base class, which is something you want to avoid in your designs.\n\n\n\n\nMixin Class\n\nOperates the same as Inheritance, but since it only provides simple behavior(s), it is easy to reuse with other classes without causing problems\nExample: Take certain class attributes and create a dict\n# In representations.py\nclass AsDictionaryMixin:\nÂ  Â  def to_dict(self):\nÂ  Â  Â  Â  return {\nÂ  Â  Â  Â  Â  Â  prop: self._represent(value)\nÂ  Â  Â  Â  Â  Â  for prop, value in self.__dict__.items()\nÂ  Â  Â  Â  Â  Â  if not self._is_internal(prop)\nÂ  Â  Â  Â  }\nÂ  Â  def _represent(self, value):\nÂ  Â  Â  Â  if isinstance(value, object):\nÂ  Â  Â  Â  Â  Â  if hasattr(value, 'to_dict'):\nÂ  Â  Â  Â  Â  Â  Â  Â  return value.to_dict()\nÂ  Â  Â  Â  Â  Â  else:\nÂ  Â  Â  Â  Â  Â  Â  Â  return str(value)\nÂ  Â  Â  Â  else:\nÂ  Â  Â  Â  Â  Â  return value\nÂ  Â  def _is_internal(self, prop):\nÂ  Â  Â  Â  return prop.startswith('_')\n\nto_dict is a dictionary comprehension\n\nmydict = {key:val for key, val in dict}\nReturns a dict with key:value (e.g.Â property (aka attribute):value) pairs from a classâ€™s __dict__ if the property (prop) doesnâ€™t have an underscore\n\n_represent makes sure the â€œvalueâ€ is a value and not object\n_is_interna1 checks whether the attribute has an underscore in the name\n\nApply the Mixin class to any class the same way as using Inheritance\nclass Employee(AsDictionaryMixin):\nÂ  Â  def __init__(self, id, name, address, role, payroll):\nÂ  Â  Â  Â  self.id = id\nÂ  Â  Â  Â  self.name = name\nÂ  Â  Â  Â  self.address = address\nÂ  Â  Â  Â  self._role = role\nÂ  Â  Â  Â  self._payroll = payroll\n\nAsDicitionaryMixin is used as an arg to the class\nâ€œself._roleâ€ and â€œself._payrollâ€ have underscores which tells to_dict not to include them in the resulting dictionary\nNot important to using a mixin class but note that â€œaddressâ€ is from the Address class via composition (see below). Therefore the Address class would also need to inherit AsDictionaryMixin for this to work\n\nUtilize\nimport json\n\ndef print_dict(d):\nÂ  Â  print(json.dumps(d, indent=2))\n\nfor employee in EmployeeDatabase().employees:\nÂ  Â  print_dict(employee.to_dict())\n\nprint_dict takes the dict output of employee._to_dict and converts it to a json format"
  },
  {
    "objectID": "qmd/python-classes.html#sec-py-class-comp",
    "href": "qmd/python-classes.html#sec-py-class-comp",
    "title": "Classes",
    "section": "Composition",
    "text": "Composition\n\n\nNotes from Inheritance and Composition: A Python OOP Guide\nComposition models a â€œhas aâ€ relationship. In composition, a class known as composite contains an object of another class known to as component.\nComposition design is typically more flexible than inheritance and is preferable to Inheritance\n\nPrevents â€œclass explosionâ€\n\nFor complex projects, too many classes can lead to conflicts and errors because of the inevitable complex network of classes that are connected to each other.\nYou change your programâ€™s behavior by providing new components that implement those behaviors instead of adding new classes to your hierarchy.\n\nOnly loose class connections in composition\n\nChanges to the component class rarely affect the composite class, and changes to the composite class never affect the component class\n\n\ntl;dr\n\nClasses are written in different py scripts and imported as â€œmodulesâ€ in another script.\nAttribute(s) from a component class (e.g.Â Address) are used in the composite class (e.g.Â Employee)\nThen a composite class attribute object is assigned to the instantiated composite classâ€™s empty attribute\n\nThis is the magic. One classâ€™s attribute can be used as input into another classâ€™s attribute without being tightly coupled to that other class (aka inheritance).\n\nThat input isnâ€™t a value. Itâ€™s class type object.\nSee Utilize code block below\n\n\n\n\n\nComposition Through __init__ Attributes\n\nExample:\n# In contacts.py\n# Component class\nclass Address:\nÂ  Â  def __init__(self, street, city, state, zipcode, street2=''):\nÂ  Â  Â  Â  self.street = street\nÂ  Â  Â  Â  self.street2 = street2\nÂ  Â  Â  Â  self.city = city\nÂ  Â  Â  Â  self.state = state\nÂ  Â  Â  Â  self.zipcode = zipcode\n\n# In employees.py\n# Composite class\nclass Employee:\nÂ  Â  def __init__(self, id, name):\nÂ  Â  Â  Â  self.id = id\nÂ  Â  Â  Â  self.name = name\nÂ  Â  Â  Â  self.address = None\n\n# ManagerRole and SalaryPolicy are classes from different modules\nclass Manager(Employee, ManagerRole, SalaryPolicy):\nÂ  Â  def __init__(self, id, name, weekly_salary):\nÂ  Â  Â  Â  SalaryPolicy.__init__(self, weekly_salary)\nÂ  Â  Â  Â  super().__init__(id, name)\n\nYou would import these two modules into a third script and do stuff (see next code block)\nYou initialize the Address.address attribute to â€œNoneâ€ for now to make it optional, but by doing that, you can now assign an Address to an Employee.\n\ni.e.Â the attributes of an Address instance from its __init__ are now available to be assigned to a Employee instance.\n\nManager is a child class of multiple other classes (inheritance) including Employee and therefore gets an .address attribute\n\nAside: ManagerRole doesnâ€™t have an __init__ (i.e.Â no attributes), so Iâ€™m not sure why super() is used here\n\nwhy not just use Employee.__init__?\nI wouldâ€™ve thought that ManagerRole wouldâ€™ve required the same inputs as Employee, so super() is used here to cover both at the same time.\n\nBut thatâ€™s not the case, MangerRole is there just for itâ€™s method and not itâ€™s attributes\n\nDoes being able to use ManageRole methods require super() (i.e.Â necessary for Inheritance)?\n\n\nUtilize\nmanager = employees.Manager(1, 'Mary Poppins', 3000)\nmanager.address = contacts.Address(\nÂ  Â  '121 Admin Rd',Â \nÂ  Â  'Concord',Â \nÂ  Â  'NH',Â \nÂ  Â  '03301'\n)\n# ... create other intances of different jobs in the company\n\n# guess this would be like a json\nemployees = [\nÂ  Â  manager,\nÂ  Â  secretary,\nÂ  Â  sales_guy,\nÂ  Â  factory_worker,\nÂ  Â  temporary_secretary,\n]\n\n# do work with the list class objs\nproductivity_system = productivity.ProductivitySystem()\nproductivity_system.track(employees, 40)\n\nThe Address class instance is assigned to the .address attribute of the Manager instance (which gets its .address attribute from Employee)\n\n\n\n\n\nComposition Through a Function Argument\n\nExample:\n# In hr.py\nclass PayrollSystem:\nÂ  Â  def calculate_payroll(self, employees):\nÂ  Â  Â  Â  print('Calculating Payroll')\nÂ  Â  Â  Â  print('===================')\nÂ  Â  Â  Â  for employee in employees:\nÂ  Â  Â  Â  Â  Â  print(f'Payroll for: {employee.id} - {employee.name}')\nÂ  Â  Â  Â  Â  Â  print(f'- Check amount: {employee.calculate_payroll()}')\nÂ  Â  Â  Â  Â  Â  if employee.address:\nÂ  Â  Â  Â  Â  Â  Â  Â  print('- Sent to:')\nÂ  Â  Â  Â  Â  Â  Â  Â  print(employee.address)\nÂ  Â  Â  Â  Â  Â  print('')\n\npayroll_system = hr.PayrollSystem()\npayroll_system.calculate_payroll(employees)\n\nThe input for calculate_payroll, employees, is a list of instantiated Employee class objects in the previous code chunk.\nName of the class is iterated and represents each instance\n\nHas attributes and methods available\n\n\n\n\n\nOther Module Classes Used as Attributes\n\nExample:\n# In employees.py\nfrom productivity import ProductivitySystem\nfrom hr import PayrollSystem\nfrom contacts import AddressBook\nclass EmployeeDatabase:\nÂ  Â  def __init__(self):\nÂ  Â  Â  Â  self._employees = [\nÂ  Â  Â  Â  Â  Â  {\nÂ  Â  Â  Â  Â  Â  Â  Â  'id': 1,\nÂ  Â  Â  Â  Â  Â  Â  Â  'name': 'Mary Poppins',\nÂ  Â  Â  Â  Â  Â  Â  Â  'role': 'manager'\nÂ  Â  Â  Â  Â  Â  },\nÂ  Â  Â  Â  Â  Â  {\nÂ  Â  Â  Â  Â  Â  Â  Â  'id': 2,\nÂ  Â  Â  Â  Â  Â  Â  Â  'name': 'John Smith',\nÂ  Â  Â  Â  Â  Â  Â  Â  'role': 'secretary'\nÂ  Â  Â  Â  Â  Â  }\nÂ  Â  Â  Â  ]\nÂ  Â  Â  Â  self.productivity = ProductivitySystem()\nÂ  Â  Â  Â  self.payroll = PayrollSystem()\nÂ  Â  Â  Â  self.employee_addresses = AddressBook()\nÂ  Â  @property\nÂ  Â  def employees(self):\nÂ  Â  Â  Â  return [self._create_employee(**data) for data in self._employees]\nÂ  Â  def _create_employee(self, id, name, role):\nÂ  Â  Â  Â  address = self.employee_addresses.get_employee_address(id)\nÂ  Â  Â  Â  employee_role = self.productivity.get_role(role)\nÂ  Â  Â  Â  payroll_policy = self.payroll.get_policy(id)\nÂ  Â  Â  Â  return Employee(id, name, address, employee_role, payroll_policy)\n\nProductivitySystem, PayrollSystem, AddressBook are classes imported from various modules\nAs attributes, these classesâ€™ methods are used in the _create_employee function\n\nReturn invokes the Employee class with values obtained by the various class methods\nEmployee class (not shown in this block) is already present in this module so it doesnâ€™t have to be imported."
  },
  {
    "objectID": "qmd/python-classes.html#sec-py-class-dec",
    "href": "qmd/python-classes.html#sec-py-class-dec",
    "title": "Classes",
    "section": "Decorators",
    "text": "Decorators\n\nThey can add additional features to a function\n\nUseful because you donâ€™t have to refactor downstream code\n\nFunctions that take a function as input\n\nSee use cases throughout note and check bkmks\n\n@ is placed above a â€œdecoratedâ€ function\n\nExample\n@decorator_1\ndef temperature():\nreturn temp\n\nCalling temperature() is actually calling decorator_1(temperature())\n\n\nExample: add additional features to a function\n\n\nAdds a timer to a function\n\nExample: Multiple decorators for a function\n@log_execution\n@timing\ndef my_function(x, y):\nÂ  Â  time.sleep(1)\nÂ  Â  return x + y\nSee Custom Examples for the â€œlog_executionâ€ decorator\n\n\nProperty\n\nArguments\nproperty(fget=None, fset=None, fdel=None, doc=None)\nBuilt-in decorator\nConstitutes a family of decorators\n\n@property: Declares the method as a property.\n\nfget - function to get value of the attribute\n\n@.setter: Specifies the setter method for a property that sets the value to a property.\n\nfset - function to set value of the attribute\nmust have the value argument that can be used to assign to the underlying private attribute\n\n@.deleter: Specifies the delete method as a property that deletes a property.\n\nfdel - function to delete the attribute\nmust have the value argument that can be used to assign to the underlying private attribute\n\n\nExample\n# Using @property decorator\nclass Celsius:\ndef __init__(self, temperature=0):\nÂ  Â  self._temperature = temperature\ndef to_fahrenheit(self):\nÂ  Â  return (self._temperature * 1.8) + 32\n\n\n# decorators\n# attribute getter\n@property\ndef temperature(self):\nÂ  Â  print(\"Getting value...\")\nÂ  Â  return self._temperature\n\n# also adds constraint to the temperature input\n@temperature.setter\ndef temperature(self, value):\nÂ  Â  print(\"Setting value...\")\nÂ  Â  if value &lt; -273.15:\nÂ  Â  Â  Â  raise ValueError(\"Temperature below -273 is not possible\")\nÂ  Â  self._temperature = value\n\n@temperature.deleter\ndef temperature(self, value):\nÂ  Â  print(\"Deleting value...\")\nÂ  Â  del self._temperature\n\n&gt;&gt; human = Celsius(37)\nSetting value...\n&gt;&gt; print(human.temperature)\nGetting value...\n37\n&gt;&gt; print(human.to_fahrenheit())\nGetting value...\n98.60000000000001\n&gt;&gt; del human.temperature\nDeleting value...\n&gt;&gt; coldest_thing = Celsius(-300)\nSetting value...\nTraceback (most recent call last):\nFile \"\", line 29, in\nFile \"\", line 4, in __init__\nFile \"\", line 18, in temperature\nValueError: Temperature below -273 is not possible\n\n.deleter didnâ€™t work for me and neither did the conditional. Donâ€™t my python version or what\n\n\n\n\nClass Method\n\nMethod that is bound to the class and not the object (aka instance) of the class.\nInstance attributes cannot be referred to with this method\nCan modify the class state that applies across all instances of the class\nUse Cases\n\nTo create new instances of the class without going trhough its normal __init__\nTo create a class instance that requires some async calls when instantiated, since __init__ cannot be async.\n\nStarts with a â€œclassmethodâ€ decorator\nclass MyClass:Â  Â \n@classmethod\ndef classmethod(cls):\nÂ  Â  return 'class method called', cls\nCalling a class method vs an instance method\n# calling a class method\n# no instantiation\nMyClass.classmethod()\n\n# calling an instance method\n# instantiates object first\nobject = MyClass()\nobject.method()\n\n\n\nStatic Method\n\nMethod bound to the class instance, not the class itself.\nDoes not take the class as a parameter.\nIt cannot access or modify the class at all.\nStarts with a â€œstaticmethodâ€ decorator\nDoesnâ€™t have any access to what the class isâ€”itâ€™s basically just a function, called syntactically like a method, but without access to the object and its internals (fields and other methods), which classmethod does have.\nSee SO thread for discussion on the differences between the static and class decorators and their uses\nclass Person:\nÂ  def __init__(self, name, age):\nÂ  Â  self.name = name\nÂ  Â  self.age = age\n\nÂ  @staticmethod\nÂ  def isAdult(age):\nÂ  Â  return age &gt; 18\n\nisAdult(age) function doesnâ€™t require the usual self argument, so it couldnâ€™t reference the class even if it wanted to.\nMost often used as utility functions that are completely independent of a classâ€™s state\nSee classmethod decorator for details on calling this method\n\n\n\n\nCustom Examples\n\nAlso see Code, Optimization &gt;&gt; Python &gt;&gt; Profile decorator\nUsing functools and decorators\nfrom functools import singledispatch\n\n@singledispatch\ndef process_data(data):\nraise NotImplementedError(f\"Type {type(data)} is unsupported\")\n\n@process_data.register\ndef process_dict(data: dict):\nprint(\"Dict is processed\")\n\n@process_data.register\ndef process_list(data: list):\nprint(\"List is processed\")\nMultiprocessing Function Execution Time Limiter\nimport multiprocessing\nfrom functools import wraps\n\nclass TimeExceededException(Exception):\nÂ  Â  pass\n## PART 1\nÂ  Â  def function_runner(*args, **kwargs):\nÂ  Â  Â  Â  \"\"\"Used as a wrapper function to handle\nÂ  Â  Â  Â  returning results on the multiprocessing side\"\"\"\n\nÂ  Â  Â  Â  send_end = kwargs.pop(\"__send_end\")\nÂ  Â  Â  Â  function = kwargs.pop(\"__function\")\nÂ  Â  Â  Â  try:\nÂ  Â  Â  Â  Â  Â  result = function(*args, **kwargs)\nÂ  Â  Â  Â  except Exception as e:\nÂ  Â  Â  Â  Â  Â  send_end.send(e)\nÂ  Â  Â  Â  Â  Â  return\nÂ  Â  Â  Â  send_end.send(result)\n\nÂ  Â  @parametrized\nÂ  Â  def run_with_timer(func, max_execution_time):\nÂ  Â  Â  Â  @wraps(func)\nÂ  Â  Â  Â  def wrapper(*args, **kwargs):\nÂ  Â  Â  Â  Â  Â  recv_end, send_end = multiprocessing.Pipe(False)\nÂ  Â  Â  Â  Â  Â  kwargs[\"__send_end\"] = send_end\nÂ  Â  Â  Â  Â  Â  kwargs[\"__function\"] = func\n\nÂ  Â  Â  Â  Â  Â  ## PART 2\nÂ  Â  Â  Â  Â  Â  p = multiprocessing.Process(target=function_runner, args=args, kwargs=kwargs)\nÂ  Â  Â  Â  Â  Â  p.start()\nÂ  Â  Â  Â  Â  Â  p.join(max_execution_time)\nÂ  Â  Â  Â  Â  Â  if p.is_alive():\nÂ  Â  Â  Â  Â  Â  Â  Â  p.terminate()\nÂ  Â  Â  Â  Â  Â  Â  Â  p.join()\nÂ  Â  Â  Â  Â  Â  Â  Â  raise TimeExceededException(\"Exceeded Execution Time\")\nÂ  Â  Â  Â  Â  Â  result = recv_end.recv()\n\nÂ  Â  Â  Â  Â  Â  if isinstance(result, Exception):\nÂ  Â  Â  Â  Â  Â  Â  Â  raise result\n\nÂ  Â  Â  Â  Â  Â  return result\n\nÂ  Â  Â  Â  return wrapper\n\nFrom Limiting Python Function Execution Time with a Parameterized Decorator via Multiprocessing\n\nRetry (e.g.Â for an API)\nimport time\nfrom functools import wraps\n\ndef retry(max_tries=3, delay_seconds=1):\nÂ  Â  def decorator_retry(func):\nÂ  Â  Â  Â  @wraps(func)\nÂ  Â  Â  Â  def wrapper_retry(*args, **kwargs):\nÂ  Â  Â  Â  Â  Â  tries = 0\nÂ  Â  Â  Â  Â  Â  while tries &lt; max_tries:\nÂ  Â  Â  Â  Â  Â  Â  Â  try:\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  return func(*args, **kwargs)\nÂ  Â  Â  Â  Â  Â  Â  Â  except Exception as e:\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  tries += 1\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  if tries == max_tries:\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  raise e\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  time.sleep(delay_seconds)\nÂ  Â  Â  Â  return wrapper_retry\nÂ  Â  return decorator_retry\n\n@retry(max_tries=5, delay_seconds=2)\ndef call_dummy_api():\nÂ  Â  response = requests.get(\"https://jsonplaceholder.typicode.com/todos/1\")\nÂ  Â  return response\n\nTries to get an API response. If it fails, we retry the same task 5 times. Between each retry, we wait for 2 seconds.\n\nCache Function Results\ndef memoize(func):\nÂ  Â  cache = {}\nÂ  Â  def wrapper(*args):\nÂ  Â  Â  Â  if args in cache:\nÂ  Â  Â  Â  Â  Â  return cache[args]\nÂ  Â  Â  Â  else:\nÂ  Â  Â  Â  Â  Â  result = func(*args)\nÂ  Â  Â  Â  Â  Â  cache[args] = result\nÂ  Â  Â  Â  Â  Â  return result\nÂ  Â  return wrapper\n\n@memoize\ndef fibonacci(n):\nÂ  Â  if n &lt;= 1:\nÂ  Â  Â  Â  return n\nÂ  Â  else:\nÂ  Â  Â  Â  return fibonacci(n-1) + fibonacci(n-2)\n\nUses a dictionary, stores the function args, and returns values. When we execute this function, the decorated will check the dictionary for prior results. The actual function is called only when thereâ€™s no stored value before.\nUsing a dictionary to hold previous execution data is a straightforward approach. However, there is a more sophisticated way to store caching data. You can use an in-memory database, such as Redis.\n\nLogging (e.g.Â ETL pipeline)\nimport logging\nimport functools\n\nlogging.basicConfig(level=logging.INFO)\n\ndef log_execution(func):\nÂ  Â  @functools.wraps(func)\nÂ  Â  def wrapper(*args, **kwargs):\nÂ  Â  Â  Â  logging.info(f\"Executing {func.__name__}\")\nÂ  Â  Â  Â  result = func(*args, **kwargs)\nÂ  Â  Â  Â  logging.info(f\"Finished executing {func.__name__}\")\nÂ  Â  Â  Â  return result\nÂ  Â  return wrapper\n\n@log_execution\ndef extract_data(source):\nÂ  Â  # extract data from source\nÂ  Â  data = ...\nÂ  Â  return data\n@log_execution\ndef transform_data(data):\nÂ  Â  # transform data\nÂ  Â  transformed_data = ...\nÂ  Â  return transformed_data\n@log_execution\ndef load_data(data, target):\nÂ  Â  # load data into target\nÂ  Â  ...\n\ndef main():\nÂ  Â  # extract data\nÂ  Â  data = extract_data(source)\nÂ  Â  # transform data\nÂ  Â  transformed_data = transform_data(data)\nÂ  Â  # load data\nÂ  Â  load_data(transformed_data, target)\n\noutput\nINFO:root:Executing extract_data\nINFO:root:Finished executing extract_data\nINFO:root:Executing transform_data\nINFO:root:Finished executing transform_data\nINFO:root:Executing load_data\nINFO:root:Finished executing load_data\n\nEmail Notification\nimport smtplib\nimport traceback\nfrom email.mime.text import MIMEText\ndef email_on_failure(sender_email, password, recipient_email):\nÂ  Â  def decorator(func):\nÂ  Â  Â  Â  def wrapper(*args, **kwargs):\nÂ  Â  Â  Â  Â  Â  try:\nÂ  Â  Â  Â  Â  Â  Â  Â  return func(*args, **kwargs)\nÂ  Â  Â  Â  Â  Â  except Exception as e:\nÂ  Â  Â  Â  Â  Â  Â  Â  # format the error message and traceback\nÂ  Â  Â  Â  Â  Â  Â  Â  err_msg = f\"Error: {str(e)}\\n\\nTraceback:\\n{traceback.format_exc()}\"\n\nÂ  Â  Â  Â  Â  Â  Â  Â  # create the email message\nÂ  Â  Â  Â  Â  Â  Â  Â  message = MIMEText(err_msg)\nÂ  Â  Â  Â  Â  Â  Â  Â  message['Subject'] = f\"{func.__name__} failed\"\nÂ  Â  Â  Â  Â  Â  Â  Â  message['From'] = sender_email\nÂ  Â  Â  Â  Â  Â  Â  Â  message['To'] = recipient_email\n\nÂ  Â  Â  Â  Â  Â  Â  Â  # send the email\nÂ  Â  Â  Â  Â  Â  Â  Â  with smtplib.SMTP_SSL('smtp.gmail.com', 465) as smtp:\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  smtp.login(sender_email, password)\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  smtp.sendmail(sender_email, recipient_email, message.as_string())\n\nÂ  Â  Â  Â  Â  Â  Â  Â  # re-raise the exception\nÂ  Â  Â  Â  Â  Â  Â  Â  raise\n\nÂ  Â  Â  Â  return wrapper\n\nÂ  Â  return decorator\n\n@email_on_failure(sender_email='your_email@gmail.com', password='your_password', recipient_email='recipient_email@gmail.com')\ndef my_function():\nÂ  Â  # code that might fail"
  },
  {
    "objectID": "qmd/python-classes.html#sec-py-class-examp",
    "href": "qmd/python-classes.html#sec-py-class-examp",
    "title": "Classes",
    "section": "Examples",
    "text": "Examples\n\nTransforms variables by logging, can add 1 if necessary, back-transform\nfrom sklearn.base import BaseEstimator, TransformerMixinÂ \nfrom sklearn.preprocessing import PowerTransformerÂ \n\nclass CustomLogTransformer(BaseEstimator, TransformerMixin):Â \nÂ  Â  def __init__(self):Â \nÂ  Â  Â  Â  self._estimator = PowerTransformer()Â  # init a transformerÂ \nÂ  Â  def fit(self, X, y=None):Â \nÂ  Â  Â  Â  X_copy = np.copy(X) + 1Â  # add one in case of zeroesÂ \nÂ  Â  Â  Â  self._estimator.fit(X_copy)Â \nÂ  Â  Â  Â  return selfÂ \nÂ  Â  def transform(self, X):Â \nÂ  Â  Â  Â  X_copy = np.copy(X) + 1Â \nÂ  Â  Â  Â  return self._estimator.transform(X_copy)Â  # perform scalingÂ \nÂ  Â  def inverse_transform(self, X):Â \nÂ  Â  Â  Â  X_reversed = self._estimator.inverse_transform(np.copy(X))Â \nÂ  Â  Â  Â  return X_reversed - 1Â  # return subtracting 1 after inverse transform\nPredictions for a Huggingface classifer\nimport sys\nfrom transformers import pipeline\nfrom typing import List\nimport numpy as np\nfrom time import perf_counter\nimport logging\n\n# Set up logger\nlogging.basicConfig(stream=sys.stdout, level=logging.INFO)\nlog = logging.getLogger(__name__)\n\nclass ZeroShotTextClassifier:\nÂ  Â  \"\"\"Class with only class methods\"\"\"\nÂ  Â  # Class variable for the model pipeline\nÂ  Â  classifier = None\nÂ  Â  @classmethod\nÂ  Â  def load(cls):\nÂ  Â  Â  Â  # Only load one instance of the model\nÂ  Â  Â  Â  if cls.classifier is None:\nÂ  Â  Â  Â  Â  Â  # Load the model pipeline.\nÂ  Â  Â  Â  Â  Â  # Note: Usually, this would also download the model.\nÂ  Â  Â  Â  Â  Â  # But, we download the model into the container in the Dockerfile\nÂ  Â  Â  Â  Â  Â  # so that it's built into the container and there's no download at\nÂ  Â  Â  Â  Â  Â  # run time (otherwise, each time we'll download a 1.5GB model).\nÂ  Â  Â  Â  Â  Â  # Loading still takes time, though. So, we do that here.\nÂ  Â  Â  Â  Â  Â  # Note: You can use a GPU here if needed.\nÂ  Â  Â  Â  Â  Â  t0 = perf_counter()\nÂ  Â  Â  Â  Â  Â  cls.classifier = pipeline(\nÂ  Â  Â  Â  Â  Â  Â  Â  \"zero-shot-classification\", model=\"facebook/bart-large-mnli\"\nÂ  Â  Â  Â  Â  Â  )\nÂ  Â  Â  Â  Â  Â  elapsed = 1000 * (perf_counter() - t0)\nÂ  Â  Â  Â  Â  Â  log.info(\"Model warm-up time: %d ms.\", elapsed)\nÂ  Â  @classmethod\nÂ  Â  def predict(cls, text: str, candidate_labels: List[str]):\nÂ  Â  Â  Â  assert len(candidate_labels) &gt; 0\nÂ  Â  Â  Â  # Make sure the model is loaded\nÂ  Â  Â  Â  cls.load()\nÂ  Â  Â  Â  # For the tutorial, let's create\nÂ  Â  Â  Â  # a custom object from the huggingface prediction.\nÂ  Â  Â  Â  # Our prediction object will include the label and score\nÂ  Â  Â  Â  t0 = perf_counter()\nÂ  Â  Â  Â  # pylint: disable-next=not-callable\nÂ  Â  Â  Â  huggingface_predictions = cls.classifier(text, candidate_labels)\nÂ  Â  Â  Â  elapsed = 1000 * (perf_counter() - t0)\nÂ  Â  Â  Â  log.info(\"Model prediction time: %d ms.\", elapsed)\nÂ  Â  Â  Â  # Create the custom prediction object.\nÂ  Â  Â  Â  max_index = np.argmax(huggingface_predictions[\"scores\"])\nÂ  Â  Â  Â  label = huggingface_predictions[\"labels\"][max_index]\nÂ  Â  Â  Â  score = huggingface_predictions[\"scores\"][max_index]\nÂ  Â  Â  Â  return {\"label\": label, \"score\": score}\nPayroll System\n\nEmployees\n# In employees.py\nfrom hr import (\nÂ  Â  SalaryPolicy,\nÂ  Â  CommissionPolicy,\nÂ  Â  HourlyPolicy\n)\nfrom productivity import (\nÂ  Â  ManagerRole,\nÂ  Â  SecretaryRole,\nÂ  Â  SalesRole,\nÂ  Â  FactoryRole\n)\nclass Employee:\nÂ  Â  def __init__(self, id, name):\nÂ  Â  Â  Â  self.id = id\nÂ  Â  Â  Â  self.name = name\nclass Manager(Employee, ManagerRole, SalaryPolicy):\nÂ  Â  def __init__(self, id, name, weekly_salary):\nÂ  Â  Â  Â  SalaryPolicy.__init__(self, weekly_salary)\nÂ  Â  Â  Â  super().__init__(id, name)\nclass Secretary(Employee, SecretaryRole, SalaryPolicy):\nÂ  Â  def __init__(self, id, name, weekly_salary):\nÂ  Â  Â  Â  SalaryPolicy.__init__(self, weekly_salary)\nÂ  Â  Â  Â  super().__init__(id, name)\nclass SalesPerson(Employee, SalesRole, CommissionPolicy):\nÂ  Â  def __init__(self, id, name, weekly_salary, commission):\nÂ  Â  Â  Â  CommissionPolicy.__init__(self, weekly_salary, commission)\nÂ  Â  Â  Â  super().__init__(id, name)\nclass FactoryWorker(Employee, FactoryRole, HourlyPolicy):\nÂ  Â  def __init__(self, id, name, hours_worked, hour_rate):\nÂ  Â  Â  Â  HourlyPolicy.__init__(self, hours_worked, hour_rate)\nÂ  Â  Â  Â  super().__init__(id, name)\nclass TemporarySecretary(Employee, SecretaryRole, HourlyPolicy):\nÂ  Â  def __init__(self, id, name, hours_worked, hour_rate):\nÂ  Â  Â  Â  HourlyPolicy.__init__(self, hours_worked, hour_rate)\nÂ  Â  Â  Â  super().__init__(id, name)\nProductivity\n# In productivity.py\nclass ProductivitySystem:\nÂ  Â  def track(self, employees, hours):\nÂ  Â  Â  Â  print('Tracking Employee Productivity')\nÂ  Â  Â  Â  print('==============================')\nÂ  Â  Â  Â  for employee in employees:\nÂ  Â  Â  Â  Â  Â  result = employee.work(hours)\nÂ  Â  Â  Â  Â  Â  print(f'{employee.name}: [{result}]')\nÂ  Â  Â  Â  print('')\nclass ManagerRole:\nÂ  Â  def work(self, hours):\nÂ  Â  Â  Â  return f'screams and yells for [{hours}] hours.'\nclass SecretaryRole:\nÂ  Â  def work(self, hours):\nÂ  Â  Â  Â  return f'expends [{hours}] hours doing office paperwork.'\nclass SalesRole:\nÂ  Â  def work(self, hours):\nÂ  Â  Â  Â  return f'expends [{hours}] hours on the phone.'\nclass FactoryRole:\nÂ  Â  def work(self, hours):\nÂ  Â  Â  Â  return f'manufactures gadgets for [{hours}] hours.'\nHR\n# In hr.py\nclass PayrollSystem:\nÂ  Â  def calculate_payroll(self, employees):\nÂ  Â  Â  Â  print('Calculating Payroll')\nÂ  Â  Â  Â  print('===================')\nÂ  Â  Â  Â  for employee in employees:\nÂ  Â  Â  Â  Â  Â  print(f'Payroll for: {employee.id} - {employee.name}')\nÂ  Â  Â  Â  Â  Â  print(f'- Check amount: {employee.calculate_payroll()}')\nÂ  Â  Â  Â  Â  Â  print('')\nclass SalaryPolicy:\nÂ  Â  def __init__(self, weekly_salary):\nÂ  Â  Â  Â  self.weekly_salary = weekly_salary\nÂ  Â  def calculate_payroll(self):\nÂ  Â  Â  Â  return self.weekly_salary\nclass HourlyPolicy:\nÂ  Â  def __init__(self, hours_worked, hour_rate):\nÂ  Â  Â  Â  self.hours_worked = hours_worked\nÂ  Â  Â  Â  self.hour_rate = hour_rate\nÂ  Â  def calculate_payroll(self):\nÂ  Â  Â  Â  return self.hours_worked * self.hour_rate\nclass CommissionPolicy(SalaryPolicy):\nÂ  Â  def __init__(self, weekly_salary, commission):\nÂ  Â  Â  Â  super().__init__(weekly_salary)\nÂ  Â  Â  Â  self.commission = commission\nÂ  Â  def calculate_payroll(self):\nÂ  Â  Â  Â  fixed = super().calculate_payroll()\nÂ  Â  Â  Â  return fixed + self.commission"
  },
  {
    "objectID": "qmd/python-general.html#sec-py-gen-misc",
    "href": "qmd/python-general.html#sec-py-gen-misc",
    "title": "General",
    "section": "Misc",
    "text": "Misc\n\nTools (see article, article for installation and usage)\n\nruff - linter and sorts imports\n\nFast, sensible default settings, focuses on more important things out of the box, and has less legacy burden\n\npydocstring - tool for checking compliance with Python docstring conventions\nblack - code formatter\nisort - sorts your imports\npytest, pytest-watch - unit tests\ncommitizen - guides you through a series of steps to create a commit message that conforms to the structure of a Conventional Commit\nnbQA - linting in jupyter notebooks\nmypy - type checker; good support and docs\npylance - checks type hinting in VSCode (see Functions &gt;&gt; Documentation &gt;&gt; Type Hinting)\ndoit - task runner; {targets}-like tool; tutorial\npre-commit - specify which checks you want to run against your code before committing changes to your git repository\nREADME templates - link\n\nPut as much config as possible into pyproject.toml. A lot of configurations tools will happily read from it, and it will give you one source of truth.\nAn underscore _ at the beginning is used to denote private variables in Python.\ndef set_temperature(self, value):\nÂ  Â  Â  Â  if value &lt; -273.15:\nÂ  Â  Â  Â  Â  Â  raise ValueError(\"Temperature below -273.15 is not possible.\")\nÂ  Â  Â  Â  self._temperature = value\n\nyou can still access â€œ_temperatureâ€ but itâ€™s just meant for internal use by the class and the underscore indicates this\n\n{{warnings::warnings.filterwarnings(â€˜ignoreâ€™)}}"
  },
  {
    "objectID": "qmd/python-general.html#terms",
    "href": "qmd/python-general.html#terms",
    "title": "General",
    "section": "Terms",
    "text": "Terms\n\nclasses - code template for creating objects, we can think of it as a blueprint. It describes the possible states and behaviors that every object of a certain type could have.\nobject - data structure storing information about the state and behavior of a certain entity and is an instance of a class\nstub file - a file containing a skeleton of the public interface of that Python module, including classes, variables, functions â€“ and most importantly, their types. (Source)"
  },
  {
    "objectID": "qmd/python-general.html#base",
    "href": "qmd/python-general.html#base",
    "title": "General",
    "section": "Base",
    "text": "Base\n\nInfo method\n\nX.info()\nRemove an object: del\nCheck object type\n\ntype() : outputs the type of an object\nisinstance() : outputs type and inheritance of an object\nSee article for details on differences\n\nImport Libraries\nimport logging\nimport bentoml\nfrom transformers import (\nÂ  Â  SpeechT5Processor,\nÂ  Â  SpeechT5ForTextToSpeech,\nÂ  Â  SpeechT5HifiGan,\nÂ  Â  WhisperForConditionalGeneration,\nÂ  Â  WhisperProcessor,\n)"
  },
  {
    "objectID": "qmd/python-general.html#sec-py-gen-fund",
    "href": "qmd/python-general.html#sec-py-gen-fund",
    "title": "General",
    "section": "Fundamentals",
    "text": "Fundamentals\n\nSlicing\n\nFormat my_list[start:stop:step]\n\n** start value is inclusive and the end value is exclusive\n\n1 element and more than 1 element\n\"Python\"[0] # P\n\"Python\"[0:1] # P\n\"Python\"[0:5] # Pytho\nillustrates how when using a range, the last element is exclusive\nNegative indexing my_list[0:-1]\n\nEverything but the last object\n\nSkip every second element\nmy_list = list(\"Python\")\nmy_list[0:len(my_list):2]\n&gt;&gt; ['P', 't', 'o']\nstart at 0, end at len(my_list), step = 2\nShortcuts\nmy_list[0:-1] == my_list[:-1]\nmy_list[0:len(my_list):2] == my_list[::2]\n\"Python\"[::-1] == \"Python\"[-1:-7:-1]\n\nDefaults\n\n0 for the start value\nlen(list) for the stop value\n1 for the step value\n\nDefaults for negative step value\n\n-1 for the start value\n-len(list) - 1 for the stop value\n\n\nAlias vs new object\nb = a # alias\nb = a[:] # new object\n\nWith the alias, changes to a will happen to b as well\n\nCommon use cases\n\n\n\n\n\n\n\nEvery element but the first and the last one\n[1:-1]\n\n\nEvery element in reverse order\n[::-1]\n\n\nEvery element but the first and the last one in reverse order\n[-2:0:-1]\n\n\nEvery second element but the first and the last one in reverse order\n[-2:0:-2]\n\n\n\nUsing slice function\nsequence = list(\"Python\")\nmy_slice = slice(None, None, 2) # equivalent to [::2]\nindices = my_slice.indices(len(sequence))\n&gt;&gt; (0, 6, 2)\n\nShows start = 0, stop = 6, step = 2\n\n\n\n\nF-Strings\n\nParameterize with {}\n&gt;&gt; x = 5\n&gt;&gt; f\"One icecream is worth [{x}]{style='color: #990000'} dollars\"\n'One icecream is worth 5 dollars'\n! - functions\n\n!r â€” Shows the string delimiter, calls the repr() method.\n\nreprâ€™s goal is to be unambiguous and strâ€™s is to be readable. For example, if we suspect a float has a small rounding error, repr will show us while str may not\n\n!a â€” Shows the Ascii for the characters.\n!s â€” Converts the value to a string.\n\nGuessing this the str() method (see !r for details)\n\n\nfood2brand = \"Mcdonalds\"\nfood2 = \"French fries\"\nf\"I like eating {food2brand} {food2!r}\"\n\"I like eating Mcdonalds 'French fries'\"\nChange format with â€œ:â€\n&gt;&gt; import datetime\n&gt;&gt; date = datetime.datetime.utcnow()\n&gt;&gt; f\"The date is {date:%m-%Y %d}\"\n'The date is 02-2022 15'\n\n\n\nOperators\n\n(docs)\nExponential: 5**3\nInteger division: 5//3\nModulo: 5%3\nIdentity: is\nx = 5\ny = 3\nprint(\"The result for x is y is\", x is y)\nThe result for x is y is false\n\nThink you can also use == here too\n\nLogical: and and or\nprint(\"The result for 5 &gt; 3 and 6 &gt; 8 is\", 5 &gt; 3 and 6 &gt; 8)\nprint(\"The result for 5 &gt; 3 or 6 &gt; 8 is\", 5 &gt; 3 or 6 &gt; 8)\nThe result for 5 &gt; 3 and 6 &gt; 8 is False\nThe result for 5 &gt; 3 or 6 &gt; 8 is True\nSubset: in and not in\nprint(\"Is the number 3 in the list [1,2,3]?\", 3 in [1,2,3])\nIs the number 3 in the list [1,2,3]? True\n\nprint(\"Is the number 3 not in the list [1,2,3]?\", 3 not in [1,2,3])\nIs the number 3 not in the list [1,2,3]? False\nAssignment"
  },
  {
    "objectID": "qmd/python-general.html#sec-py-gen-dattyp",
    "href": "qmd/python-general.html#sec-py-gen-dattyp",
    "title": "General",
    "section": "Types",
    "text": "Types\n\nScalars\n\nCreate scalars by subsetting a list\ninputs = [1, 0.04, 0.9]\n# 1 numericÂ \nrmse = inputs[0] # rmse = 1 and is type 'float'\n# multiple numerics\nrmse, mape, rsq = inputs\n\nTuples\n\nLists are mutable and tuples are not\n\ni.e.Â we can add or remove elements to a list after we create it but we cannot do such thing to a tuple\n\nSyntax: name_of_tuple = (a, b)\n\nLists\n\nCreate list of objects (e.g.Â floats)\nacc_values = [rmse, mape, rsq]\n\nalt method: asterisk-comma notation\n*acc_names, = \"RMSE\", \"MAPE\", \"R-SQ\"\n\nasterisk is â€œunzipping operatorâ€\n\n\nMake a copy\nold_list = [2, 3, 4]\nnew_list = list(old_list)\n\nDictionaries\n\n** if creating a simple dict, more performant to use curly braces **\n\nAvoid d = dict(1=1, x='x')\n\nJoin 2 dicts -Â  d.update(d2)\n\nIf d and d2 share keys, d2â€™s values for those keys will be used\n\nAccess a value from a key: sample_dict['key_name']\nMake a copy\nold_dict = {stuff: 2, more_stuff: 3}\nnew_dict = dict(old_dict)\nConvert list of tuples to a dict\nacc_dict = dict(acc_list)\n\nzip creates lists of tuples (See Loops &gt;&gt; zip section)\n\nAdd key, value pair to a dict\ntransaction_data['user_address'] = '221b Baker Street, London - UK'\n# or\ntransaction_data.update(user_address='221b Baker Street, London - UK')\nUnpack dict into separate tuples for key:value pairs\nrmse, mape, rsq = acc_dict.items()\nrmse\n('RMSE', 1)\n\n** fastest way to iterate over both keys and values in a dict **\ncan also use zip to unpack pairs into a list (see loops &gt;&gt; zip)\n\nUnpack dict into separate lists for keys and values\nacc_keys = list(acc_dict.keys())Â \nacc_values = list(acc_dict.values())\n\n** fastest way to iterate over a dictâ€™s keys or values **\n\nUnpack values from dicts into separate scalars\nrmse, mape, rsq = acc_dict.values()\nrmse\n1\nPull the value for a key (e.g.Â k) or return the default value - d.get(k, default)\n\nDefault is â€œNoneâ€. I think this can be set with d.setdefault(k, default)\n\nCheck for specific key (logical)\nâ€˜send_currencyâ€™ in transaction_data\nâ€˜send_currencyâ€™ in transaction_data.keys()\nâ€˜send_currencyâ€™ not in transaction_data.keys()\n\nLike %in% in R\n\nCheck for specific value (logical)\nâ€˜GBPâ€™ in transaction_data.values()\nCheck for key, value pair\n(â€˜send_currencyâ€™, â€˜GBPâ€™) in transaction_data.items()\nPretty printing of dictionaries\nÂ  Â  _ = [print(k, \":\", f'{v:.1f}') for k,v in acc_dict.items()]\nÂ  Â  RMSE : 1.00\nÂ  Â  MAPE : 0.04\nÂ  Â  R-sq : 0.90\n\nfor-in loop format (see Loops &gt;&gt; Comprehension)\nprint returns â€œnoneâ€ for each key:value at the bottom of the output for some reason. Assigning the print statement to a variable fixes it.\n\ndefaultdict\n\nCreates a key from a list element and groups the properties into a list of values where the value may also be a dict.\nFrom {{collections}}\nAlso see\n\nPybites video\nJSON &gt;&gt; Python &gt;&gt; Example: Parse Nested JSON into a dataframe\n\n\n\nSets\n\nIf performing set logic, always more performant to use sets instead of dicts or lists\n\n\nIf using numpy/pandas, using the .unique() syntax is more efficient for arrays/seriesâ€™ with numeric values\nIf using strings, itâ€™s more efficient to use list(set(my_array))\n\n\nStrings\n\nOperators\nOperator Description\n%d Signed decimal integer\n%u unsigned decimal integer\n%c Character\n%s String\n%f Floating-point real number\nExample\n\nname = \"india\"\nage = 19\nmarks = 20.56\nstring1 = 'Hey %s' % (name)\nprint(string1)\nstring2 = 'my age is %d' % (age)\nprint(string2)\nstring3= 'Hey %s, my age is %d' % (name, age)\nprint(string3)\nstring3= 'Hey %s, my subject mark is %f' % (name, marks)\nprint(string3)"
  },
  {
    "objectID": "qmd/python-general.html#sec-py-gen-pip",
    "href": "qmd/python-general.html#sec-py-gen-pip",
    "title": "General",
    "section": "pip",
    "text": "pip\n\nLooks for packages on https://pypi.org, downloads, and installs it\nMisc\n\nIf you installed python using the app-store, replace python with python3.\nDonâ€™t use sudo to install libraries, since it will install things outside of the virtual environment.\nNor should you use â€œ--userâ€, since itâ€™s made to install things outside of the virtual environment.\nDonâ€™t mix pip, venv and Anaconda. Avoid Anaconda if you can. If you have to use Anaconda, donâ€™t use pip and venv. Limit yourself to Anaconda tools.\nIf you get SSL errors (common if you are in a hotel or a company network) use the --trusted-host pypi.org --trusted-host files.pythonhosted.org options with pip to work around the problem.\n\ne.g.Â python -m pip install pendulum --trusted-host pypi.org --trusted-host files.pythonhosted.org\n\nIf you are behind a corporate proxy that requires authentication (common if you are in a company network), you can use the --proxy option with pip to give the proxy address and your credentials.\n\ne.g.Â python -m pip install pendulum --proxy http://your_username:yourpassword@proxy_address\nIt also works with the https_proxy environment variables\n\n\nInstall library\n$ python -m pip install &lt;library_name&gt;\n\n# inside ipython or a colab notebook, \"!\" signifies a shell command\n!pip install &lt;library_name&gt;\nInstall library from github\npython -m pip install git+https://github.com/bbalasub1/glmnet_python.git@1.0\n\nâ€œ@1.0â€ is the version number\n\nUninstall library\n$ python -m pip uninstall &lt;library_name&gt;\n\nWonâ€™t uninstall the dependencies of this library.\nIf you wish to also uninstall the unused dependencies as well, take a look at pip-autoremove\n\nRemove all packages in environment\n$ python -m pip uninstall -y -r &lt;(pip freeze)\nRemove all packages in environment but write the names of the packages to a requirements.txt file first\n$ python -m pip freeze &gt; requirements.txt && python3 -m pip uninstall -r         requirements.txt -y\nInstall requirements.txt\n$ python -m pip install -r requirements.txt\nWrite names of all the packages in your environment to a requirement.txt file\n$ python -m pip freeze &gt; requirements.txt\n\nWrites the specific version of the packages that you have installed in your environment (e.g.Â pandas==1.0.0)\n\nThis may not be what you always want, so youâ€™ll need to manually change to just the library name in that case (e.g.Â pandas)\n\nOnly aware of the packages installed using the pip install command\n\ni.e.Â any packages installed using a different approach such as peotry, setuptools, condaetc. wonâ€™t be included in the final requirements.txt file.\n\nDoes not account for dependency versioning conflicts\nSaves all packages in the environment including those that are not relevent to the project\nIf you are not using a virtual environment, pip freeze generates a requirement file containing all the libraries in including those beyond the scope of your project.\n\nList your installed libraries\n$ python -m pip list\nSee if you have a particular library installed\n$ python -m pip list | grep &lt;library_name&gt;\nGet library info (name, version, summary, license, dependencies and other)\n$ python -m pip show &lt;library_name&gt;\nCheck that all installed packages are compatible\n$ python -m pip check\nUpdate package\n$ python -m pip install package_name --upgrade\nSearch for PyPI libraries (pip source for libraries)\n$ python -m pip search &lt;search_term&gt;\n\nreturns all libraries matching search term\n\nDownload a package without installing it\npython -m pip download &lt;library name&gt;\n\nIt will download the package and all its dependencies in the current directory (the files, called, wheels, have a .whl extension).\nYou can then install them offline by doing python -m pip install on the wheels.\n\nBuild Wheel archives for the libraries and dependencies in your environment\n$ python -m pip wheel\n\nI think these are binaries, so they donâ€™t need compiled if installed in a future environment\nReal Python Tutorial\n\nManage configuration\n$ python -m pip config &lt;action name&gt;\n\nActions: edit, get, list, set or unset\nExample\n$ python -m pip config set global.index-url https://your.global.index\n\nDisplay debug information specific to pip\n$ python -m pip debug"
  },
  {
    "objectID": "qmd/python-general.html#sec-py-gen-anac",
    "href": "qmd/python-general.html#sec-py-gen-anac",
    "title": "General",
    "section": "Anaconda",
    "text": "Anaconda\n\nCheck configuration of current environment\nconda list\n\nShows python version used, package names installed and their versions\n\nInstall packages\nconda install &lt;package1&gt; &lt;package2&gt;\nInstall a package from a specific channel\nconda install &lt;package_name&gt; -c &lt;channel_name&gt; -y # Short form\nconda install &lt;package_name&gt; --channel &lt;channel_name&gt; -y # Long form\nPackage installation channels (some packages not available in default channel)\n\nCheck current channels\nconda config --show channels\n\nThe order in which these channels are displayed shows the channel priority.\n\nWhen a package is installed, anaconda will the check the channel at the top of list first then work itâ€™s way down\n\n\nAdd a channel\nconda config --add channels conda-forge\n\nAdds â€œconda-forgeâ€ to list of available channels\n\nRemove a channel\nconda config --remove channels conda-forge\n\nRemoves the â€œconda-forgeâ€ channel"
  },
  {
    "objectID": "qmd/python-general.html#sec-py-gen-env",
    "href": "qmd/python-general.html#sec-py-gen-env",
    "title": "General",
    "section": "Environments",
    "text": "Environments\n\nMisc\n\n{pyenv}\n\nset-up in RStudio (article)\nDoesnâ€™t support Windows\nCompiles Python under the hood when you install it. But compiling can fail in a thousand ways\n\nWhen youâ€™re in a virtual environment\n\nAnytime you use the â€œpythonâ€ command while your virtual environment is activated, it will be only the one from this env.\nIf you start a Python shell now, it will see only the things in the current directory, and the things installed in the virtual environment.\nIf you run a script, it will see only the things in the current directory, and the things installed in the virtual environment.\nIf you run a command, the command will be taken from the virtual environment.\nAnd they will only use exactly the version of Python of the virtual environment.\n\nStore environment files with project\nBreakage\n\nYou cannot move a virtual environment, it will stop working. Create a â€œrequirements.txtâ€ file, delete the virtual environment and create a new one.\nDonâ€™t rename a directory containing a virtual environment. Or if you do, prepare yourself to create a â€œrequirements.txtâ€ file, delete the virtual environment and create a new one.\nIf you change the Python used in the virtual environment, such as when uninstalling it, it will stop working.\n\nCreate one big virtual environment for all small scripts.\n\nIf you make a lot of venv, you may be tempted to install everything at the system level for convenience. After all, itâ€™s a bore to create and activate a virtual environment each time you want to write a five liner. A good balance is one single virtual environment you use for all things quick and dirty.\n\nCreate several virtual environments per versions of python if your project needs to support several versions. You may need several requirements.txt files as well, one for each env.\nRecommendations for a stable dependency environment for your project (article)\n\nDonâ€™t install the latest major version of Python\n\nMaximum: 1 version under the latest version\nMinimum: 4 versions under the latest version (e.g.Â latest = 3.11, min = 3.7)\n\nUse only the python.org installer on Windows and Mac, or official repositories on Linux.\nNever install or run anything outside of a virtual environment\nLimit yourself to the basics: â€œpipâ€ and â€œvenvâ€\nIf you run a command, use â€œ-mâ€\n\nIt lets you run any importable Python module, no matter where you are. Because most commands are Python modules, we can use this to say, â€œrun the module X of this particular pythonâ€.\nThere is currently no way for you to run any python command reliably without â€œ-mâ€.\nExamples:\n# Don't do :\npip install\n# Do:\npython -m pip install\n\n# Don't do :\nblack\n# Do:\npython -m black\n\n# Don't do :\njupyter notebook\n# Do:\npython -m jupyter notebook\n\nWhen creating a virtual environment, be explicit about which Python you use\n\nGet current python versions installed: py --list-paths (windows)\n\n\n\n\n\nvenv\n\nMisc\n\nShipped with Python\nDonâ€™t mix pip, venv and Anaconda. Avoid Anaconda if you can. If you have to use Anaconda, donâ€™t use pip and venv. Limit yourself to Anaconda tools.\n\nCreate\n\nWindows: py -&lt;py version&gt; -m venv &lt;env name&gt;\nMac/Linux: python3.8 -m venv .venv\n\nWhere the python version is 3.8 and the environment name is â€œ.venvâ€\nMac and Linux hide folders with names that have preceding â€œ.â€ by default, so make sure you have â€œdisplay hiddent foldersâ€ activated or you wonâ€™t see it.\n\nNaming Environments\n\nName your environment directory â€œ.venvâ€, because:\n\nSome editors check for this name and automatically load it.\nIt contains â€œvenvâ€, so itâ€™s explicit.\nIt has a dot, so itâ€™s hidden on Linux.\n\nIf you have more than one environment directory, use a suffix to distinguish them.\n\ne.g.Â A project that must work with two different versions of Python (3.9 and 3.10), I will have a â€œ.venv39â€ and a â€œ.venv310â€\n\nNaming enviroments for misc uses\n\nâ€œ.venv_testâ€: located in a personal directory to install new tools you want to play with. Itâ€™s disposable, and often broken, so you can delete it and recreate it regularly.\nâ€œ.venv_scriptsâ€: Used for all the small scripts. You donâ€™t want to create one virtual environment for each script, so centralize everything. Itâ€™s better than installing outside of a virtual environment, but is not a big constraint.\n\n\n\nActivate\n\nWindows: .venv\\Scripts\\activate\n\nWhere .venv is the name of the virtual environment\nMay need .bat as extension to activate\n\nMac/Linux: source .venv/bin/activate\n\nAfter that, you can use python -m pip install to install packages.\nDeactivate: deactivate\n\n\n\nvirtualenv\n\nDocs\nCreate a virtual environment\n python3 -m venv &lt;env_name&gt;\n\n-m venv tells python to run the virtual environment module, venv\nMake sure youâ€™re in your projects directory\nRemember to add â€œ&lt;env_name&gt;/â€ to .gitignore\n\nActivate environment\nsource venv/bin/activate # Mac or Linux\nvenv\\Scripts\\activate # Windows\n\n&gt;&gt; (&lt;env_name&gt;) $\n\nPrompt should change if the environment is activated\nAll pipÂ  installed packages will now be installed into the â€œ&lt;env_name&gt;/lib/python3.9/site-packagesâ€ directory\n\nUse the python contained within your virtual environment\npython main.py\n\nNot sure why you wouldnâ€™t just activate the environment.\n\nDeactivate environment\ndeactivate\n\nno pythonÂ  or env_name needed?\n\nReproducing environment\n\nDone using requirements.txt (see pip section for details on writing and installing)\n\nI donâ€™t think the python version is included, so that will need to communicated manually\n\n\n\n\n\nAnaconda\n\nList environments\nconda env list # method 1\nconda info --envs # method 2\n\ndefault environment is called â€œbaseâ€\nActive environment will be in parentheses\nActive environment will be the one in the list with an asterix\n\nCreate a new conda environment\nconda create -n &lt;env name&gt;\nconda activate &lt;env name&gt;\nCreate a new conda environment with a specific python version\nconda create -n py310 python=3.10\nconda activate py310\nconda install jupyter jupyterlab\njupyter lab\n\nAlso install and launch jupyter lab\n\nCreate an environment from a yaml file\nconda env create -f environment.yml # Short form\nconda env create --file environment.yml # Long form\nRemove an environment\nconda deactivate &lt;env_name&gt; # Need to deactivate the environment first\nconda env remove -n &lt;env_name&gt;\n\nShould also delete environment folders (conda env list shows path to folders)\n\nClone an existing environment\nconda create -n testclone --clone test # Short form\nconda create --name testclone --clone test # Long form\n\nâ€œtestcloneâ€ is a copy of â€œtestâ€\n\nActivate an environment\nconda activate &lt;env_name&gt;\nActivate environment with reticulate in R\nreticulate::use_python(\"/usr/local/bin/python\")Â  Â \nreticulate::use_condaenv(\"&lt;env name&gt;\", \"/home/jtimm/anaconda3/bin/conda\")\nDeactivate an environment\nconda activate # Option 1: activates base\nconda deactivate test # Option 2\nExport the specifications of the current environment into a YAML file into the current directory\nconda env export &gt; environment.yml # Option 1\nconda env export -f environment.yml # Option 2\nExample: Conda workflow\n\nCreate an environment that uses a specific python version\n\nWithout a specified python version, the environment will use the same version as â€œbaseâ€\n\nconda create -n anothertest python=3.9.7 -y\n\n-n is the name flag and â€œanothertestâ€ is the name of the environment\nUses Python 3.9.7\nWithout the -y flag, thereâ€™d be a prompt youâ€™d have to answer â€œyesâ€ to\n\nActivate the environment\nconda activate anothertest\nInstall packages\n\n\nInstalling packages one at time can lead to dependency conflicts.\nCondaâ€™s official documentation recommends to install all packages at the same time so that the dependency conflicts are resolved\nconda install \"numpy&gt;=1.11\" nltk==3.6.2 jupyter -y # install specific versions\nconda install numpy nltk jupyter -y # install all latest versions\n\nDo work and deactivate environment\nconda deactivate anothertest\n\nExample Raschka workflow\n# create & activate\nconda createÂ  --prefix ~/code/myproj python=3.8\nconda activate ~/code/myproj\n# export env\nconda env export &gt; myproj.yml\n# create new env from yaml\nconda env create --file myproj.yml --prefix ~/code/myproj2\n\n\n\nPoetry\n\nDocs (like renv)\nApparently buggy (article)\npipâ€™s dependency resolver is more flexible and wonâ€™t die on you if the package specifies bad metadata, while poetryâ€™s strictness may mean you canâ€™t install some packages at all.\nCreate project\n\npoetry new &lt;project-dir-name&gt;\n\nautomatically creates a directory for your project with a skeleton\nâ€œpyproject.tomlâ€ maintains dependencies for the project with the following sections:\n\ntool.poetry provides an area to capture information about your project such as the name, version and author(s).\ntool.poetry.dependencies lists all dependencies for your project.\ntool.poetry.dev-dependencies lists dependencies your project needs for development that should not be present in any version deployed to a production environment.\nbuild-system references the fact that Poetry has been used to manage the project.\n\n\nAdd library and create lock file: poetry add &lt;library name&gt;\n\nWhen the first library is added, a â€œpoetry.lockâ€ file wil be generated\n\nActivate environment: poetry shell\n\nDeactivate environment: exit\n\nRun script: poetry run python my_script.py\nPackage the project: poetry build\n\nCreates tar.gz and wheel files (.whl) in â€œdistâ€ dir\n\nExample: poetry workflow (+pyenv, virtualenv)\n# Create a virtual environment called \"my-new-project\"\n# using Python 3.8.8\npyenv virtualenv 3.8.8 my-new-project\n# Activate the virtual environment\npyenv activate my-new-project\n\n{{pyenv}} - For managing the exact version of Python and activating the environment\nName your package the same name as the directory which is the same name as the virtual environment.\n\nDashes for the latter two and underscores for the package\n\nIntitialize the project and add packages (similar to renv) bash              poetry init     poetry add numpy\nReinstall dependencies\n# navigate to my project directory and run\npoetry install\nTurn off virtualenv management\n# right after installing poetry, run:\npoetry config virtualenvs.create false\n\nDefault poetry behavior is that it will manage your virtual environments for you. This may not be desirable because:\n\nCanâ€™t just run a script from the command line. Instead, have to run poetry run my-script\n\nAwkward when you want to dockerize your code\n\nEnforces a virtual environment management framework on everybody in a shared codebase\nYour Makefile now needs to know about poetry"
  },
  {
    "objectID": "qmd/python-general.html#sec-py-gen-deps",
    "href": "qmd/python-general.html#sec-py-gen-deps",
    "title": "General",
    "section": "Dependencies",
    "text": "Dependencies\n\nMisc\n\nAfter mastering pip, {{pip-tools}} is recommended.\n\nGet a complete list of dependencies (e.g.Â dependencies of dependencies) with {{deptree}}\ndeptree\n# output\nFlask==2.2.2Â  # flask\nÂ  Werkzeug==2.2.2Â  # Werkzeug&gt;=2.2.2\nÂ  Â  MarkupSafe==2.1.1Â  # MarkupSafe&gt;=2.1.1\nÂ  Jinja2==3.1.2Â  # Jinja2&gt;=3.0\nÂ  Â  MarkupSafe==2.1.1Â  # MarkupSafe&gt;=2.0\nÂ  itsdangerous==2.1.2Â  # itsdangerous&gt;=2.0\nÂ  click==8.1.3Â  # click&gt;=8.0\n# deptree and pip trees\n\nFlask depends on Werkzeug which depends on MarkupSafe\n\nWerkzeug and MarkupSafe qualify as transitive dependencies for this project\n\nCommented part on the right is the compatible range\n\nrequirements.txt format\n# comment\npandas==1.0.0\npyspark\npip: write names of all the packages in your environment to a requirement.txt file\n$ python3 -m pip freeze &gt; requirements.txt\n\nSee pip section for issues with this method\n\n{{pipx}}\n\nA tool for installing Python CLI utilities that gives them their own hidden virtual environment for their dependencies\nAdds the tool itself to your PATH - so you can install stuff without worrying about it breaking anything else\nInstall\npipx install datasette\n\n{{pipreqs}}\n\nScans all the python files (.py) in your project, then generates the requirements.txt file based on the import statements in each python file of the project\nSet-up: pip install pipreqs\nGenerate requirements.txt file: pipreqs /&lt;your_project_root_path&gt;/\nUpdate requirements.txt:Â  pipreqs --force /&lt;your_project_root_path&gt;/Â \nIgnore the libraries of some python files from a specific subfolder\npipreqs /&lt;your_project_root_path&gt;/ --ignore  /&lt;your_project_root_path&gt;/folder_to_ignore/\n\n{{pip-compile-multi}}\n\nNotes from:\n\nEnd Python Dependency Hell with pip-compile-multi\n\nCreates and nests multiple requirement files\n\ne.g.Â Able to keep dev environment from production environment separate\n\nAutoresolution of cross-requirement file conflicts\n\nDependency DAG (how all requirement files are connected) must have exactly one â€œsinkâ€ node\n\nOrganize your most ubiquitous dependencies into a single â€œcoreâ€ set of dependencies that all other nodes require (a source node), and all of your development dependencies in a node that requires all others (directly or indirectly) require (a sink).\n\nSimplifies and allows use of autoresolution functionality\n\nExample: DAG (directionality of the arrows is opposite compared to library docs)"
  },
  {
    "objectID": "qmd/python-general.html#sec-py-gen-loadsav",
    "href": "qmd/python-general.html#sec-py-gen-loadsav",
    "title": "General",
    "section": "Loading/Saving",
    "text": "Loading/Saving\n\nMisc\n\n{{pickle}} needs custom class(es) to be defined in another module/file and then imported. Otherwise, PicklingError will be raised.\n\n\n\nFile paths\n\nMisc\n\n{{pathlib}} is recommended\n\n{{os}}\n\nGet current working directory: os.getcwd()\nList all files and directories in working directory: os.listdir()\nList all files and directories from a subdirectory: os.listdir(os.getcwd()+'\\\\01_Main_Directory')\nUsing os.walk(): gathers paths, folders, and files\n\nPaths\n\n\npath = os.getcwd()+'\\\\01_Main_Directory'\nfor folder_path, folders, files in os.walk(path):\nÂ  Â  print(folder_path)\nFolders\n\n\nSimilar code, just replace print(folder_path) with print(folders)\n\nFiles\n\n\n{{glob}}\n\nGet a file path string\nimport glob\npath = os.getcwd()+'\\\\01_Main_Directory'\nfor filepath in glob.glob(path):\nÂ  Â  print(filepath)\n# C:\\Users\\Suraj\\Challenges\\01_Main_Directory\nList all files and subdirectories from a path\npath = os.getcwd()+'\\\\01_Main_Directory\\\\*'\nfor filepath in glob.glob(path):\nÂ  Â  print(filepath)\n\nNote the * wildcard\n\nList all files and subdirectories with a â€œ1â€ in the name\npath = os.getcwd()+'\\\\01_Main_Directory\\\\*1.*'\nfor filepath in glob.glob(path):\nÂ  Â  print(filepath)\nGet a list of csv file paths from a directory: all_files = glob.glob(\"C:/Users/path/to/dir/*.csv\")\n\nNote that you donâ€™t need a loop to save to an object\n\nList all files and subdirectories and files in those subdirectories\npath = os.getcwd()+'\\\\01_Main_Directory\\\\**\\\\*.txt'\nfor filepath in glob.glob(path, recursive=True):\nÂ  Â  print(filepath)\n#Output\nC:\\Users\\Suraj\\Challenges\\01_Main_Directory\\ABCD_1.txt\nC:\\Users\\Suraj\\Challenges\\01_Main_Directory\\ABCD_2.txt\nC:\\Users\\Suraj\\Challenges\\01_Main_Directory\\ABCD_3.txt\nC:\\Users\\Suraj\\Challenges\\01_Main_Directory\\ABCD_4.txt\nC:\\Users\\Suraj\\Challenges\\01_Main_Directory\\ABCD_5.txt\nC:\\Users\\Suraj\\Challenges\\01_Main_Directory\\Sub_Dictionary_1\\File_1_in_SubDict_1.txt\nC:\\Users\\Suraj\\Challenges\\01_Main_Directory\\Sub_Dictionary_1\\File_2_in_SubDict_1.txt\nC:\\Users\\Suraj\\Challenges\\01_Main_Directory\\Sub_Dictionary_2\\File_1_in_SubDict_2.txt\nC:\\Users\\Suraj\\Challenges\\01_Main_Directory\\Sub_Dictionary_2\\File_2_in_SubDict_2.txt\n\nComponents: â€œ**â€ and â€œrecursive=Trueâ€\n\n\n{{pathlib}}\n\nProvides a single Path class with a range of methods (instead of separate functions) that can be used to perform various operations on a path.\nCreate a path object for a directory\nfrom pathlib import Path\npath = Path('origin/data/for_arli')\nCheck if a folder or a file is available in a given path\nif path.exists():\nÂ  Â  print(f\"[{path}]{style='color: #990000'} exists.\")\nÂ  Â  if path.is_file():\nÂ  Â  Â  Â  print(f\"[{path}]{style='color: #990000'} is a file.\")\nÂ  Â  elif path.is_dir():\nÂ  Â  Â  Â  print(f\"[{path}]{style='color: #990000'} is a directory.\")\nelse:\nÂ  Â  raise ValueError(f\"[{path}]{style='color: #990000'} does not exists\")\n\nChecks if the path â€˜origin/data/for_arliâ€™ exists\n\nif it does, it will check whether it is a file or a directory.\nIf the path does not exist, it will print a raise an Error indicating that the path does not exist.\n\n\nList all files/folders in a path\nfor f in path.iterdir():\nÂ  Â  print(f)\n\nUse it in combination with the previous is_dir() and is_file()Â  methods to list either files or directories.\n\nDelete files/folders in a path\nfor f in path.iterdir():\nÂ  Â  f.unlink()\n\npath.rmdir()\n\nunlink deletes each file in the path\nrmdir deletes the directory.\n\ndirectory must be empty\n\n\nCreate a sequence of directories\n# existing directory: D:\\scripts\\myfolder\np = Path(\"D:\\scripts\\myfolder\\logs\\newfolder\")\np.mkdir(parents=True, exist_ok=True)\n\nCreate path object with desired sequence of directories (e.g.Â logs\\newfolder)\nmkdir with parents=True creates the sequence of directories\n\nW/exist_ok=True no error with occur if the directory already exists\n\n\nRename directory: path.rename('origin/data/new_name')\nConcatenate a path with string\npath = Path(\"/origin/data/for_arli\")\n# Join another path to the original path\nnew_path = path.joinpath(\"la\")\nprint(new_path) # prints 'origin/data/for_arli/bla'\n\nIt also handles the join between two Path objects\n\nDirectory stats\nprint(path.stat()) # print statisticsÂ \nprint(path.owner()) # print owner\n\ne.g.Â creation time, modification time, etc.\n\nWrite to a file\n# Open a file for writing\npath = Path('origin/data/for_arli/example.txt')\nwith path.open(mode='w') as f:\nÂ  Â  # Write to the file\nÂ  Â  f.write('Hello, World!')\n\nYou do not need to create manually example.txt.\n\nRead a file\npath = Path('example.txt')\nwith path.open(mode='r') as f:\nÂ  Â  # Read from the file\nÂ  Â  contents = f.read()\nÂ  Â  print(contents) # Output: Hello World!\n\n\n\n\nModels\n\nSaving and Loading an estimator as a binary using {{joblib}} (aside: pipelines are estimators)\nimport joblib\n#saving the pipeline into a binary file\njoblib.dump(pipe, 'wine_pipeline.bin')\n#loading the saved pipeline from a binary file\npipe = joblib.load('wine_pipeline.bin')\nSaving and loading a trained model as a pickle file\nimport pickle\n# open file connection\npickle_file = open('model.pkl', 'ab')\n# save the model\npickle.dump(model_obj, pickle_file)\n# close file connectionÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â \npickle_file.close()\n\n# Open conn and save\ntest_dict = {\"Hello\": \"World!\"}\nwith open(\"test.pickle\", \"wb\") as outfile:\n# \"wb\" argument opens the file in binary mode\npickle.dump(test_dict, outfile)\n\n# open file connection\npickle_file = open('model.pkl', 'rb')\n# load saved model\nmodel = pickle.load(pickle_file)\n\n# open conn and load\n# Deserialization\nwith open(\"test.pickle\", \"rb\") as infile:\nÂ  Â  test_dict_reconstructed = pickle.load(infile)\n\nCan serialize almost everything including classes and functions\n\n\n\n\nEnvironment Variables\n\n{{os}}\n\nCheck existence\nenv_var_exists = 'ENV' in os.environ\n# or\nenv_var_exists = os.environ.has_key('ENV')\nList environment variables: print(os.environ)\nLoading\nimport os\n# Errors when not present\nenv_var = os.environ['ENV'] # where ENV is the name of the environment variable\n# Returns None when not present\nenv_var = os.environ.get('ENV', 'DEFAULT_VALUE') # using default value is optional\nSet/Export or overwrite\nos.environ['ENV'] = 'dev'\nLoad or create if not present\ntry:\nÂ  Â  env_var = os.environ['ENV']\nexcept KeyError:\nÂ  Â  os.environ['ENV'] = 'dev'\nDelete\nif 'ENV' in os.environ:\nÂ  Â  del os.environ['ENV']\n\n{{python-decouple}}\n\nAccess environment variables from whatever environment it is running in.\nCreate a .env file in the project root directory: touch .env\nOpen .env in nano text editor: nano .env\n\nNano text editor is pre-installed on macOS and most Linux distros\nCheck if installed/version: nano --version\nBasic usage tutorial\n\nAdd environment variables to file\nUSER=alex\nKEY=hfy92kadHgkk29fahjsu3j922v9sjwaucahf\n\nSave: Ctrl+o\nExit: Ctrl+x\n\n* Add .env to your .gitignore file *\nAccess\nfrom decouple import config\nAPI_USERNAME = config('USER')\nAPI_KEY = config('KEY')\n\n{{python-dotenv}}\n\nReads .env files\nProbably more popular than {{python-decouple}}\nHas a companion R package, {dotenv}, so .env files can be used in projects that use both R and Python."
  },
  {
    "objectID": "qmd/python-general.html#sec-py-gen-funs",
    "href": "qmd/python-general.html#sec-py-gen-funs",
    "title": "General",
    "section": "Functions",
    "text": "Functions\n\nMisc\n\nBenchmarking a function\n\nUsing IPython function\n%time dat['col1001'] = some_function(dat['col1'], dat['col2'], dat['col3'])\n\n%%time prints the wall time for the entire cell whereas %time gives you the time for first line only\n\nUsing a decorator\n\n\nAssigning functions based on arg type\ndef process_data(data):\nÂ  Â  if isinstance(data, dict):\nÂ  Â  Â  Â  process_dict(data)Â \nÂ  Â  else:\nÂ  Â  Â  Â  process_list(data)Â \ndef process_dict(data: dict):\nÂ  Â  print(\"Dict is processed\")\ndef process_list(data: list):\nÂ  Â  print(\"List is processed\")\n\nAssigns data to a particular function depending on whether itâ€™s a dict or a list\nisinstance checks that the passed argument is of the proper type or a subclass\n\nWrapping functions\nfrom functools import partial\nget_count_df = partial(get_count, df=df)\n\nWraps function to make df the default value for df arg\n\n\n\n\nDocumentation\n\nFunctions should at least include docstrings and type hinting\nDocstrings\n\nTypes: Google-style, Numpydoc, reStructured Text, EpyTex\nInformation to include\n\nFunction description, arg description, return value description, Description of errors, Optional extra notes or examples of usage.\n\nAccess functions docstring:\n\nprint(func_name.__doc__)\nFor large docstrings\nimport inspect\nprint(inspect.getdoc(func_name))\n\nExample: Google-style\ndef send_request(key: str, lat: float = 0, lon: float = 0):\nÂ  Â  \"\"\"Send a request to Climacell Weather API\nÂ  Â  to get weather info based on lat/lon.\n\nÂ  Â  Climacell API provides realtime weather\nÂ  Â  information which can be accessed using\nÂ  Â  their 'Realtime Endpoint'.\n\nÂ  Â  Args:\nÂ  Â  Â  key (str): an API key with length of 32 chars.\nÂ  Â  Â  lat (float, optional): value for latitude.\nÂ  Â  Â  Â  Default=0\nÂ  Â  Â  lon (float, optional): value for longitude.\nÂ  Â  Â  Â  Default=0\n\nÂ  Â  Returns:\nÂ  Â  Â  int: status code of the resultÂ \nÂ  Â  Â  dict: Result of the call as a dict\n\nÂ  Â  Notes:\nÂ  Â  Â  See https://www.climacell.co/weather-api/Â \nÂ  Â  Â  for more info on Weather API. You can get\nÂ  Â  Â  API key from there, too.\nÂ  Â  \"\"\"\n\nFirst sentence should contain the purpose of the function\n\nExample: Numpydoc\ndef send_request(key: str, lat: float = 0, lon: float = 0):\nÂ  Â  \"\"\"\nÂ  Â  Send a request to Climacell Weather API\nÂ  Â  to get weather info based on lat/lon.\n\nÂ  Â  Climacell API provides realtime weather\nÂ  Â  information which can be accessed using\nÂ  Â  their 'Realtime Endpoint'.\n\nÂ  Â  Parameters\nÂ  Â  ----------\nÂ  Â  Â  key (str): an API key with length of 32 chars.\nÂ  Â  Â  lat (float, optional): value for latitude.\nÂ  Â  Â  Â  Default=0\nÂ  Â  Â  lon (float, optional): value for longitude.\nÂ  Â  Â  Â  Default=0\n\nÂ  Â  Returns\nÂ  Â  -------\nÂ  Â  Â  int: status code of the resultÂ \nÂ  Â  Â  dict: Result of the call as a dict\n\nÂ  Â  Notes\nÂ  Â  -----\nÂ  Â  Â  See https://www.climacell.co/weather-api/Â \nÂ  Â  Â  for more info on Weather API. You can get\nÂ  Â  Â  API key from there, too.\nÂ  Â  \"\"\"\n\nType Hinting\n\nThis doesnâ€™t check the type; itâ€™s just metadata\n\nsee isinstance (see below), NotImplementedError (see below), or {{typecheck}} and {{mypy} (see bkmks) for type checking that will throw errors\n\nUsing type hints enables you to perform type checking. If you use an IDE like PyCharm or Visual Studio Code, youâ€™ll get visual feedback if youâ€™re using unexpected types:\nVariables: my_variable_name: tuple[int, ...]\n\nvariable should be a tuple that contains only integers. The ellipsis says the total quantity is unimportant.\n\nFunctions\ndef get_count(threshold: str, column: str, df: pd.DataFrame) -&gt; int:\nÂ  Â  return (df[column] &gt; threshold).sum()\n\nâ€œthresholdâ€, â€œcolumnâ€ should be strings (str)\nâ€œdfâ€ should be a pandas dataframe (pd.DataFrame)\nOutput should be an integer (int)\n\nFunction as an arg: Callable[[Arg1Type, Arg2Type], ReturnType]\n\nExample:\nfrom collections.abc import Callable\ndef foo(bar: Callable[[int, int], int], a: int, b: int) -&gt; int:\nÂ  Â  return bar(a, b)\n\nâ€œbarâ€ is a function arg for the function, â€œfooâ€\nâ€œbarâ€ is supposed to take: 2 integer args ([int, int]) and return an integer (int)\n\nExample:\ndef calculate(i: int, action: Callable[..., int], *args: int) -&gt; int:\nÂ  Â  return action(i, *args)\n\nâ€œactionâ€ takes any number and type of arguments but must return an integer.\nWith *args: int, you also allow a variable number of optional arguments, as long as theyâ€™re integers.\n\nExample: Lambda\nf: Callable[[int, int], int] = lambda x, y: 3*x + y\n\nMay not work\n\n\n\n\n\n\nArgs and Operators\n\nMisc\n\n** Args are not reset to default values after each call **\n\nExample:\n\ndef func(list1=[]):Â  Â  Â  # here l1 is a default argument set to []\nÂ  Â  list1.append(\"Temp\")\nÂ  Â  return list1\n\nâ€œNoneâ€ + conditional must be used to get the arg to reset back to the default value\n\ndef func(l1=None):Â  Â  Â \nÂ  Â  if l1 is None:Â \nÂ  Â  Â  Â  l1 = []\nÂ  Â  l1.append(\"Temp\")Â \nÂ  Â  return l1\n\n*\n\nUnpacks Lists\n\nnum_list = [1,2,3,4,5]\nnum_list_2 = [6,7,8,9,10]\n\nprint(*num_list)\n# 1 2 3 4 5\nnew_list = [*num_list, *num_list_2] # merge multiple lists\n# [1,2,3,4,5,6,7,8,9,10]\n*args\n\nFunctions that can accept a varying number of values\n\ndef names_tuple(*args):\nÂ  Â  return args\n\nnames_tuple('Michael', 'John', 'Nancy')\n# ('Michael', 'John', 'Nancy')\nnames_tuple('Jennifer', 'Nancy')\n# ('Jennifer', 'Nancy')\n**\n\nUnpacks Dictionaries\n\nnum_dict = {â€˜aâ€™: 1, â€˜bâ€™: 2, â€˜câ€™: 3}\nnum_dict_2 = {â€˜dâ€™: 4, â€˜eâ€™: 5, â€˜fâ€™: 6}\n\nprint(*num_dict) # only keys printed\n# a b c\nnew_dict = {**num_dict, **num_dict_2} # merge dictionaries\n# {â€˜aâ€™: 1, â€˜bâ€™: 2, â€˜câ€™: 3, â€˜dâ€™: 4, â€˜eâ€™: 5, â€˜fâ€™: 6}\n**kwargs\n\nFunctions that can accept a varying number of variable/value pairs (like a â€¦ in R)\n\ndef names_dict(**kwargs):\nÂ  Â  return kwargs\n\nnames_dict(Jane = 'Doe')\n# {'Jane': 'Doe'}\nnames_dict(Jane = 'Doe', John = 'Smith')\n# {'Jane': 'Doe', 'John': 'Smith'}\nFunction as an arg\ndef classic_boot(df, estimator, seed=1):\nÂ  Â  df_boot = df.sample(n=len(df), replace=True, random_state=seed)\nÂ  Â  estimate = estimator(df_boot)\nÂ  Â  return estimate\n\nBootstrap function with an â€œestimatorâ€ function (e.g.Â mean) as arg\nUsing a Callable\n\nClass as an arg\nfrom dataclasses import dataclass\n\n@dataclass\nclass Person:\n    name: str\n    age: int\n    address: str\n    phone: str\n    email: str\n\ndef process_data(person: Person):\n    print(f\"Processing data for {person.name}, {person.age}, living at {person.address}. Contact info: {person.phone}, {person.email}\")\n\nperson = Person(\"Alice\", 30, \"123 Main St\", \"555-1234\", \"alice@example.com\")\nprocess_data(person)\n\nMakes function more readable when the function requires a bunch of args\n\nClass as an arg (safer alternative)\nfrom typing import NamedTuple\n\nclass Person(NamedTuple):\n    name: str\n    age: int\n    address: str\n    phone: str\n    email: str\n\ndef process_data(person: Person):\n    print(f\"Processing data for {person.name}, {person.age}, living at {person.address}. Contact info: {person.phone}, {person.email}\")\n\nperson = Person(\"Alice\", 30, \"123 Main St\", \"555-1234\", \"alice@example.com\")\nprocess_data(person)\n\nUsing NamedTuple means that the attributes cannot be overridden\n\ne.g.Â Executing person.name = \"Bob\" will result in an error because tuples canâ€™t be modified.\n\n\nMake an arg optional\nlass Address:\nÂ  Â  def __init__(self, street, city, state, zipcode, street2=''):\nÂ  Â  Â  Â  self.street = street\nÂ  Â  Â  Â  self.street2 = street2\nÂ  Â  Â  Â  self.city = city\nÂ  Â  Â  Â  self.state = state\nÂ  Â  Â  Â  self.zipcode = zipcode\n\nâ€œstreet2â€ has default value of an empty string, so itâ€™s optional\n\n\n\n\nLambda\n\nUseful if you just have 1 expression that you need to execute.\nBest Practices\n\nlambda is an anonymous function, hence it is not a good idea to store it in a variable for future use\nDonâ€™t use lambdas for single functions (e.g.Â sqrt). Make sure itâ€™s an expression.\n\nExample\n# bad\nsqrt_list = list(map(lambda x: math.sqrt(x), mylist))\n# good\nsqrt_list = list(map(math.sqrt, mylist))\n\nAffects performance\n\n\nDonâ€™t use for complex expressions that require more than 1 line (meh)\n\nPer PEP8 guidelines, Limit all lines to a maximum of 79 characters\nExample\n# bad (118 characters)\ndf[\"FinalStatus\"] = df[\"Status\"].map(lambda x: 'Completed' if x ==\n'Delivered' or x == 'Shipped' else 'Not Completed')\n# instead\ndf[\"FinalStatus\"] = ''\ndf.loc[(df[\"Status\"] == 'Delivered') |\nÂ  Â  Â  (df[\"Status\"] == 'Shipped'),\nÂ  Â  Â  'FinalStatus'] = 'Completed'\ndf.loc[(df[\"Status\"] == 'Not Delivered') |\nÂ  Â  Â  (df[\"Status\"] == 'Not Shipped'),\nÂ  Â  Â  'FinalStatus'] = 'Not Completed'\n\n\nExample: 1 arg\n# py\nlambda x: np.sin(x / period * 2 * np.pi)\n# r\n~sin(.x / period * 2 * pi)\n# r\n\\(x) {sin(x / period * 2 * pi)}\nExample: 2 args\nGreater = lambda x, y : x if(x &gt; y) else y\nGreater(0.002, 0.5897)\nLambda-Filter\n\nFaster than a comprehension\n\nsee Loops &gt;&gt; Comprehensions\n\nFormat: filter(function, data_object)\n\nReturns a filter object, which needs to be converted into data structure such as list or set\n\nExample: Basic\nyourlist = list(np.arange(2,50,3))\nlist(filter(lambda x:x**2&lt;100, yourlist))\n# OutputÂ \n[2, 5, 8]\nExample: Filter w/logical\nimport pandas as pd\nimport datetime as dt\n# create a list of 10,000 dates\ndatlist = pd.date_range(dt.datetime.today(), periods=10000).tolist()Â \n# convert the dates to strings via list comprehension\ndatstrlist = [d.strftime(\"Day %d in %B of year %Y is a %A\") for d in datlist]\ndatstrlist[:4]\n['Day 21 in October of year 2021 is a Thursday', 'Day 22 in October of year 2021 is a Friday', 'Day 23 in October of year 2021 is a Saturday', 'Day 24 in October of year 2021 is a Sunday']\n\nstrLamb = filter(lambda d: ((d.endswith(\"urday\") or d.endswith(\"unday\")) and \"Oc\" in d), datstrlist)\n\nSearches for Saturdays and Sundays in the month of October of all years in list of strings\n\nExample: Nested Lists\ngroup1 = [1,2,3,43,23,42,8,3,7]\ngroup2 = [[3, 34, 23, 32, 42], [6, 11, 9], [1, 3,9,7,2,8]]\n[list(filter(lambda x: x in group1, sublist)) for sublist in group2]\n&gt;&gt; [[3, 23, 42], [], [1, 3, 7, 2, 8]]\n\nProbably useful for json\nfor-loop attached to the end of the list-filter combo\nEach sublist of group 2 is fed into the lambda-filter and compared to the group 1 list\n\n\nIterating over each element of a list\n\nExample: map\nlist(map(lambda x: x**2+x**3, yourlist))\n\nmap returns a map object that needs to be converted\n\nExample: 2 Lists\nmylist = list(np.arange(4,52,3))\nyourlist = list(np.arange(2,50,3))\nlist(map(lambda x,y: x**2+y**2, yourlist, mylist))\n\nLike a pmap\n\n\nNested lambdas\n\nExample: map\narr = [1,2,3,4,5]\nlist(map(lambda x: x*2, filter(lambda x: x%2 == 0, arr)))\n&gt;&gt; [4,8]\n\nWork inside out (locate where the data object, arr, appears)\nâ€œarrâ€ is filtered by the first lambda function for even numbers then iterated by map to be squared by the second lambda function\n\n\nIterate over rows of a column in a df\n\nExample: Using formula over rows\ngrade['evaluate']=grade['MathScore'].apply(lambda x: round((x**x)/2,2))\n\nâ€œgradeâ€ is the df; â€œMathScoreâ€ is a numeric column; â€œevaluateâ€ is the new column in the df\nFormula applied to each value of â€œMathScoreâ€ to generate each value of evaluate\n\nExample: Conditional over rows\ngrade['group']=grade['MathScore'].apply(lambda x: 'Excellent' if x&gt;=3.0 else 'Average')\n\nâ€œgradeâ€ is the df; â€œMathScoreâ€ is a numeric column; â€œgroupâ€ is the new column in the df\nConditional applied to each value of â€œMathScoreâ€ to generate each value of â€œgroupâ€\n\nUsing {{swifter}} for parallelization\nimport swifter\ndf['e'] = df.swifter.apply(lambda x: infer(x['a'], x['b'], x['c'], x['d']), axis = 1)\n\nIn a Pivot Table (like a crosstab)\n\nExample\n\ngrades_df\n\n2 names (â€œnameâ€)\n6 scores (â€œscoreâ€)\nOnly 2 letter grades associated with these scores (â€œletter gradeâ€)\n\nTask: drop lowest score for each letter grade, then calculate the average score for each letter grade\n\ngrades_df.pivot_table(index='name',\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  columns='letter grade',\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  values='score',\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  aggfunc = lambda series : (sorted(list(series))[-1] + sorted(list(series))[-2]) / 2)\n\nletter gradeÂ  Â  AÂ  Â  B\nname\nArifÂ  Â  Â  Â  Â  96.5Â  87.0\nKaylaÂ  Â  Â  Â  95.5Â  84.0\n\nindex: each row will be a â€œnameâ€\ncolumns: each column will be a â€œletter gradeâ€\nvalues: value in the cells will be from the â€œscoreâ€ column according to each combination columns in the index and columns args\naggfunc: uses a lambda to compute the aggregated values\n\nâ€œseriesâ€ is used a the variableÂ  in the lambda function\nsorts series (ascending), takes the top two values (using negative list indexing), and averages them\n\n\n\n\n\n\nScope\n\nPopulated objects within functions persist if you instantiate the object in the argument\n\n\nâ€œall_numbersâ€ retained itâ€™s previous value when the 2nd call to the function was made\n\n\n\n\nClosures\n\nInner functions that can access values in the outer function, even after the outer function has finished its execution\nExample\n\n# closure way\ndef balanceOwed(roomN,rate,nights):\nÂ  Â  def increaseByMeals(extra):\nÂ  Â  Â  Â  amountOwned=rate*nights+extra\nÂ  Â  Â  Â  print(f\"Dear Guest of Room [{roomN}]{style='color: #990000'}, you have\",Â \nÂ  Â  Â  Â  \"a due balance:\", \"${:.2f}\".format(amountOwned))\nÂ  Â  Â  Â  return amountOwned\nÂ  Â  return increaseByMeals\n\nba = balanceOwned(201,400,3)\nba(200)\nba(150)\nba(180)\nba(190)\nDear Guest of Room 201, you have a due balance: $1400.00\nDear Guest of Room 201, you have a due balance: $1350.00\nDear Guest of Room 201, you have a due balance: $1380.00\nDear Guest of Room 201, you have a due balance: $1390.00\n\nTedious way: For each value of â€œextraâ€ (e.g.Â meals), the function needs to be called even if the other values of the arguments donâ€™t change.\nClosure way:\n\nincreaseByMeals() is a closure function, because it remembers the values of the outer function balanceOwed(), even after the execution of the latter\nbalanceOwed() is called with its three arguments only once and then after its execution, we call it four times with the meal expenses (â€œextraâ€)."
  },
  {
    "objectID": "qmd/python-general.html#sec-py-gen-mods",
    "href": "qmd/python-general.html#sec-py-gen-mods",
    "title": "General",
    "section": "Modules",
    "text": "Modules\n\n.py files are called â€œmodules.â€\nA directory with .py files in which one of the files is an â€œ__init__.pyâ€ is called a package.\nMisc\n\nResource: Make your Python life easier by learning how imports find things\nsys.path contains the list of paths where Python is looking for things to import. Your virtual environment and the directory containing your entry point are automatically added to sys.path.\n\nsys.pathÂ is a list. Which means you canÂ .append(). Any directory you add there will have its content importable. Itâ€™s a useful hack, but use it as a last resort.\n\nWhen using -m flag to run a script, if you pass a package instead of a module, the package must contain a â€œ__main__.pyâ€ file for it to work. This __main__.py module will run.\nIf you have scripts in your projects, donâ€™t run them directly. Run them using â€œ-mâ€, and you can assume everything starts from the root.\n\nExample:\ntop_dir\nâ”œâ”€â”€ foo\nâ”‚   â”œâ”€â”€ bar.py\nâ”‚   â”œâ”€â”€ __init__.py\nâ”‚   â””â”€â”€ blabla.py\nâ””â”€â”€ blabla.py\n\nRunning python foo/bar.py, â€œtop_dirâ€ is the current working directory, but â€œfooâ€ is added to the sys.path.\nRunning python -m foo.bar, â€œtop_dirâ€ is the current working directory and added to sys.path.\n\nImports can all start from the root of the project and opened file paths as well.\n\n\n\n\nUsage\n\nProject Structure\nâ”œâ”€â”€ main.py\nâ”œâ”€â”€ packages\nâ”‚Â  â””â”€â”€ __init__.py\nâ”‚Â  â””â”€â”€ module_1.py\nâ”‚Â  â””â”€â”€ module_2.py\nâ”‚Â  â””â”€â”€ module_3.py\nâ””â”€â”€ â””â”€â”€ module_4.py\n\nâ€œ__init__.pyâ€ contains only 1 line which declares all the functions (or classes?) that are in the modules\n__all__ = [\"func1\", \"func2\"]\n\nIf the module files contained classes with multiple functions, I think youâ€™d just declare the classes and not every function in that class.\n\nIf using classes, each module should only have 1 class.\n\n\nScripts need to include â€œ_main_â€ in order to used in other scripts\n# test_function.py\ndef function1():Â \nÂ  Â  print(\"Hello world\")Â \nfunction1()\n\n# Define the __main__ script\nif __name__ == '__main__':Â  Â \nÂ  Â  # execute only if run as a script\nÂ  Â  function1()\n\nSays if this file is being run non-interactively (i.e.Â as a script), run this chunk\nAdd else: chunk, then that chunk will be run only if the file is imported as a module\nAllows you to allow or prevent parts of code from being run when the modules are imported\nImporting a module without _main_ in a jupyter notebook results in this\n\n\nLoading\n\nDO NOT USE from &lt;library&gt; import *\n\nThis will import anything and everything from that library and causes several problems:\n\nYou donâ€™t know what is in that package, so you have no idea what you just imported, or even if what you want is in there.\nYou just filled your local namespace with an unknown quantity of mysterious names, and you donâ€™t know what they will shadow.\nYour editor will have a hard time helping you since it doesnâ€™t know what you imported.\nYour colleague will hate you because they have no idea what variables come from where.\n\nException: In the shell, itâ€™s handy. Sometimes, you want to import all things in __init__.py and you have â€œ__all__â€ defined (see above)\n\nFrom the working directory, itâ€™s like importing from a library: from file1 import function1\nFrom a subdirectory, from subdirectory.file1 import function1\nFrom a directory outside the project, add the module to sys.path before importing it\nimport sys\nsys.path.append('/User/moduleDirectory')\n\nWhen a module is imported, it first searches for built-in modules, then the paths listed in sys.path\nThis appends the new path to the end of the sys.path\nimport sys\nsys.path.insert(1, '/User/moduleDirectory')\nPuts this path at the front of the sys.path directory list.\nimport sys\nsys.path.remove('/User/NewDirectory')\n\n*delete path from sys.path after you finish*\nPython will also search this path for future projects unless they are removed"
  },
  {
    "objectID": "qmd/python-general.html#sec-py-gen-cond",
    "href": "qmd/python-general.html#sec-py-gen-cond",
    "title": "General",
    "section": "Conditionals",
    "text": "Conditionals\n\nIf-Else\n\nSyntax\nif &lt;expression&gt;:\nÂ  Â  do something\nelse:\nÂ  Â  do something else\nExample\nregenerate = False\nif regenerate:\n    concepts_list = df2Graph(df, model='zephyr:latest')\n    dfg1 = graph2Df(concepts_list)\n    if not os.path.exists(outputdirectory):\n        os.makedirs(outputdirectory)\n\n    dfg1.to_csv(outputdirectory/\"graph.csv\", sep=\"|\", index=False)\n    df.to_csv(outputdirectory/\"chunks.csv\", sep=\"|\", index=False)\nelse:\n    dfg1 = pd.read_csv(outputdirectory/\"graph.csv\", sep=\"|\")\n\nTry-Except\n\nExample\nimport os\ntry:\nÂ  Â  env_var = os.environ['ENV']\nexcept KeyError:\nÂ  Â  # Do something\n\nIf â€œENVâ€ is not a present a KeyError is thrown. Then, except section executed.\n\n\nMatch (&gt; Python 3.10) (switch function)\nmatch object:\nÂ  Â  case &lt;pattern_1&gt;:\nÂ  Â  Â  Â  &lt;action_1&gt;\nÂ  Â  case &lt;pattern_2&gt;:\nÂ  Â  Â  Â  &lt;action_2&gt;\nÂ  Â  case &lt;pattern_3&gt;:\nÂ  Â  Â  Â  &lt;action_3&gt;\nÂ  Â  case _:\nÂ  Â  Â  Â  &lt;action_wildcard&gt;\n\nâ€œobjectâ€ is just a variable name; could be anything\nâ€œcase_â€ is the value used when none of the other cases are a match\nExample: function input inside user function\ndef http_error(status):\nÂ  Â  match status:\nÂ  Â  Â  Â  case 200:\nÂ  Â  Â  Â  Â  Â  return 'OK'\nÂ  Â  Â  Â  case 400:\nÂ  Â  Â  Â  Â  Â  return 'Bad request'\nÂ  Â  Â  Â  case 401 | 403 | 404:\nÂ  Â  Â  Â  Â  Â  return 'Not allowed'\nÂ  Â  Â  Â  case _:\nÂ  Â  Â  Â  Â  Â  return 'Something is wrong'\nExample: dict input inside a function\ndef get_service_level(user_data: dict):\nÂ  Â  match user_data:\nÂ  Â  Â  Â  case {'subscription': _, 'msg_type': 'info'}:\nÂ  Â  Â  Â  Â  Â  print('Service level = 0')\nÂ  Â  Â  Â  case {'subscription': 'free', 'msg_type': 'error'}:\nÂ  Â  Â  Â  Â  Â  print('Service level = 1')\nÂ  Â  Â  Â  case {'subscription': 'premium', 'msg_type': 'error'}:\nÂ  Â  Â  Â  Â  Â  print('Service level = 2')\nExample: inside a class\nclass ServiceLevel:\nÂ  Â  def __init__(self, subscription, msg_type):\nÂ  Â  Â  Â  self.subscription = subscription\nÂ  Â  Â  Â  self.msg_type = msg_type\n\nÂ  Â  def get_service_level(user_data):\nÂ  Â  Â  Â  match user_data:\nÂ  Â  Â  Â  Â  Â  case ServiceLevel(subscription=_, msg_type='info'):\nÂ  Â  Â  Â  Â  Â  Â  Â  print('Level = 0')\nÂ  Â  Â  Â  Â  Â  case ServiceLevel(subscription='free', msg_type='error'):\nÂ  Â  Â  Â  Â  Â  Â  Â  print('Level = 1')\nÂ  Â  Â  Â  Â  Â  case ServiceLevel(subscription='premium', msg_type='error'):\nÂ  Â  Â  Â  Â  Â  Â  Â  print('Level = 2')\nÂ  Â  Â  Â  Â  Â  case _:\nÂ  Â  Â  Â  Â  Â  Â  Â  print('Provide valid parameters')\n\nNote that inside the function, the change from â€œ:â€ to â€œ=â€Â  and â€œ()â€ following the class name in the â€œcaseâ€ portion of the match\n\n\nAssert\n\nUsed to confirm a condition\n\nIncorrect: assert condition, messageÂ \n\nCorrect method:Â \nif not condition:Â \nÂ  Â  raise AssertionError\n\nassert is useful for debugging code because it lets you test if a condition in your code returns True, if not, the program will raise an AssertionError.\n** Do not use in production, because when code is executed with the -O (optimize) flag, the assert statements are removed from the bytecode. **"
  },
  {
    "objectID": "qmd/python-general.html#sec-py-gen-loops",
    "href": "qmd/python-general.html#sec-py-gen-loops",
    "title": "General",
    "section": "Loops",
    "text": "Loops\n\nMisc\n\n{{tqdm}} - progress bar for loops\nfrom tqdm import tqdm\nfor i in tqdm(range(10000))\nÂ  Â  ...\nbreak terminates the loop containing it\n\nIf in a nested loop, it will terminate the inner-most loop containing it\n\ncontinue is used to skip the remaining code inside a loop for the current iteration only; forces the start of the next iteration of the loop\npass does nothing\n\nused when a statement or a condition is required to be present in the program but we do not want any command or code to execute\n\n\n\n\nIterators\n\nRemembers values\nExample\nD = {\"123\":\"Y\",\"111\":\"PT\",\"313\":\"Y\",\"112\":\"Y\",\"201\":\"PT\"}\nff = filter(lambda e:e[1]==\"Y\", D.items())\n\nprint(next(ff))\n&gt;&gt; ('123', 'Y')\nprint(next(ff))\n&gt;&gt; ('313', 'Y')\napply\n\naxis\n\n0 or â€˜indexâ€™: apply function to each column.\n1 or â€˜columnsâ€™: apply function to each row.\n\nExample: Function applied to rows of a column of a dataframe (i.e.Â cells)\ndef df2Graph(dataframe: pd.DataFrame, model=None) -&gt; list:\n  # dataframe.reset_index(inplace=True)\n  results = dataframe.apply(\n    lambda row: graphPrompt(row.text, {\"chunk_id\": row.chunk_id}, model), axis=1\n  )\n\ntext and chunk_id are column names of the dataframe\nrow is the row of the dataframe since axis=1, and from that row, the columns text and chunk_id are subsetted in the arguments of user-defined function.\n\nExample: Formula applied to rows of a column of a dataframe (i.e.Â cells)\ngrade['evaluate']=grade['MathScore'].apply(lambda x: round((x**x)/2,2))\n\ngrade is the df; MathScore is a numeric column; evaluate is the new column in the df\n\n\n\n\n\nGenerators\n\nGenerators are iterators, a kind of iterable you can only iterate over once. (normal iterators like lists, strings, etc. can be repeatedly iterated over)\nGenerators do not store all the values in memory, they generate the values on the fly\n\nyield - Pauses the function saving all its states and later continues from there on successive calls.\n\nAllows you to consume one element at a time and work with it without requiring you to have every element in memory.\nProduces a generator\n\n\nMisc\n\n{{itertools}} islice can slice a generator.\nAlso see APIs &gt;&gt; {{requests}} for an example\n\nExample: Using a comprehensionÂ \nmygenerator = (x*x for x in range(3))\nfor i in mygenerator:\n...Â  Â  print(i)\n\nProduce a list and ( ) produce a generatorÂ \n\nExample (using a function)\ndef create_generator():\nÂ  Â  mylist = range(3)\nÂ  Â  for i in mylist:\nÂ  Â  Â  Â  yield i*i\n\nfor i in mygenerator:\nÂ  Â  print(i)\n0\n1\n4\n\nThe first time the for calls the generator object created from your function, it will run the code in your function from the beginning until it hits yield, then itâ€™ll return the first value of the loop.\nThen, each subsequent call will run another iteration of the loop you have written in the function and return the next value.\nThis will continue until the generator is considered empty, which happens when the function runs without hitting yield.\n\nThat can be because the loop has come to an end, or because you no longer satisfy an â€œif/elseâ€\n\n\nExample: Sending values to (yield)\ndef grep(pattern):\nprint \"Looking for %s\" % pattern\nwhile True:\nÂ  Â  line = (yield)\nÂ  Â  if pattern in line:\nÂ  Â  Â  Â  print line,\ng = grep(\"python\")Â  # instantiate with \"python\" pattern to search for\n\ng.next() # Prime it\n&gt;&gt; Looking for python\n\ng.send(\"A series of tubes\") # \"python\" not present so returns nothing\ng.send(\"python generators rock!\") # \"python\" present so returns line\n&gt;&gt; python generators rock!\ng.close() # closes coroutine\n\n(yield) receives the input of the .send method and creates a generator object which is assigned to â€œlineâ€.\nAll coroutines must be â€œprimedâ€ by first calling .next() (or send(None))\n\nThis advances execution to the location of the first yield expression\n\n\nExample: (sending values to (yield))\ndef writer():\nÂ  Â  \"\"\"A coroutine that writes data *sent* to it to fd, socket, etc.\"\"\"\nÂ  Â  while True:\nÂ  Â  Â  Â  w = (yield)\nÂ  Â  Â  Â  print('&gt;&gt; ', w)\ndef writer_wrapper(coro):\nÂ  Â  # TBD\nÂ  Â  pass\nw = writer()\nwrap = writer_wrapper(w)\nwrap.send(None)Â  # \"prime\" the coroutine\nfor i in range(4):\nÂ  Â  wrap.send(i)\n&gt;&gt;Â  0\n&gt;&gt;Â  1\n&gt;&gt;Â  2\n&gt;&gt;Â  3\n\nA more complex framework if you want to break the workflow into multiple functions\n\n\n\nUsing yield from\n\nAllows for two-way usage (reading/sending) of generators\nExample (reading from a generator)\ndef reader():\nÂ  Â  \"\"\"A generator that fakes a read from a file, socket, etc.\"\"\"\nÂ  Â  for i in range(4):\nÂ  Â  Â  Â  yield '&lt;&lt; %s' % i\n\n# with yield\ndef reader_wrapper(g):\nÂ  Â  # Manually iterate over data produced by reader\nÂ  Â  for v in g:\nÂ  Â  Â  Â  yield v\n# OR with yield from\ndef reader_wrapper(g):\nÂ  Â  yield from g\nwrap = reader_wrapper(reader())\nfor i in wrap:\nÂ  Â  print(i)\n\nBasic; only eliminates 1 line of code\n\nExample (sending to a generator)\n# with (yield)\ndef writer_wrapper(coro):\nÂ  Â  coro.send(None)Â  # prime the coro\nÂ  Â  while True:\nÂ  Â  Â  Â  try:\nÂ  Â  Â  Â  Â  Â  x = (yield)Â  # Capture the value that's sent\nÂ  Â  Â  Â  Â  Â  coro.send(x)Â  # and pass it to the writer\nÂ  Â  Â  Â  except StopIteration:\nÂ  Â  Â  Â  Â  Â  pass\n# OR with yield from\ndef writer_wrapper(coro):\nÂ  Â  yield from coro\n\nNeed to see example 4 for the writer() code and the use case\nShows the other advantage of using â€œyield fromâ€: it automatically includes the code to stop prime and stop the loop.\n\nReusable generator\n\n\nreading example using â€œyield fromâ€\n\nSlicing a generator\nfrom itertools import islice\ndef gen():\nÂ  Â  yield from range(1,11)\ng = gen()\nmyslice = islice(g, 2)\n&gt;&gt; list(myslice)\n[1, 2]\n&gt;&gt; [i for i in g]\n[3,4,5,6,7,8,9,10]\n\n\n\n\nFor\n\n\nSyntax - for &lt;sequence&gt;: &lt;loop body&gt;\nNumeric Range\nfor i = 1 to 10\nÂ  Â  &lt;loop body&gt;\n\n# from 0 to 519\nfor i in range(520)\nÂ  Â  &lt;loop body&gt;\n\nres = 0\nfor idx in np.arange(0, 100000):\nÂ  res += df.loc[idx, 'int']\n\nnp.arange() ran 8000 times faster than the same chunk using range()\n\nList\nnumbers = [1, 2, 3, 4, 5, 6, 7, 8, 9]\neven_numbers = []\nfor item in numbers:\nÂ  Â  if item % 2 == 0:\nÂ  Â  Â  Â  even_numbers.append(item)\nprint(even_numbers)\n\n# results: [2, 4, 6, 8]\nList: index and value\nnumbers = [1, 2, 3, 4, 5, 6, 7, 8, 9]\nfor index, element in enumerate(numbers):\nÂ  Â  if element % 2 != 0:\nÂ  Â  Â  Â  numbers[index] = element * 2\nÂ  Â  else:\nÂ  Â  Â  Â  continue\nprint(numbers)\n# results: [2, 2, 6, 4, 10, 6, 14, 8, 18]\n\nenumerate also gets the index of the respective element at the same time\n\nWith three expressions\nfor (for i = 1; i &lt;= 10; i+=1)\nÂ  Â  &lt;loop body&gt;\nCollection-Based\n\nIf the collection is a dict, then this just iterates over the keys\n\nfor i in &lt;collection&gt;:\nÂ  Â  &lt;loop body&gt;\nIterate over a sliding window\n\nOver dictionary keys and values of a dict\nfor a,b in transaction_data.items():\nÂ  Â  print(a,â€™~â€™,b)\n\nThe .items method includes both key and value, so it iterates over the pairs.\n\nOver nested dictionaries\nfor k, v in transaction_data_n.items():\nÂ  Â  if type(v) is dict:\nÂ  Â  Â  Â  for nk, nv in v.items():Â \nÂ  Â  Â  Â  Â  Â  print(nk,â€™ â†’â€™, nv)\n\nIf the item of the dict is itself a dict then another loop iterates through its items.\nnk and nv stand for nested key and nested value\n\nSelecting a specific item in a nested dictionary\nfor k, v in transaction_data_n.items():\nÂ  Â  if type(v) is dict and k == 'transaction_2':\nÂ  Â  Â  Â  for sk, sv in v.items():\nÂ  Â  Â  Â  Â  Â  print(sk,'--&gt;', sv)\n\nOnly transaction_2â€™ s items are printed\n\nRows of a data.frame\nres = 0\nfor row in df.itertuples():\nÂ  res += getattr(row, 'int')\n\nitertuples()Â  is 30x faster than iterrows()\n\n\n\n\nzip\n\nCombine lists into 1 list of tuples\nacc_values = [1, 0.04, 0.9]\nacc_names = [\"RMSE\", \"MAPE\", \"R-sq\"]\nacc_list = list(zip(acc_names, acc_values))\nacc_list\n[('RMSE', 1), ('MAPE', 0.04), ('R-sq', 0.9)]\n\nzip does take lists of different lengths but will create shortest length list with corresponding elements\nCombine lists of unequal lengths but keep the non-paired elements\nfrom itertools import zip_longest\nacc_names3 = [\"RMSE\", \"MAPE\", \"R-sq\", \"MSE\"]\nacc_values3 = [rmse, mape, rsq]Â \nacc_list3 = list(zip_longest(acc_names3, acc_values3))\n\nUnzip list of tuples into separate lists\nnames, values = zip(*acc_list)\n\nasterisk is the â€œunzipping operatorâ€\n\nUnpack dict into a list of separate tuples for key:value pairs\nacc_tuples = list(zip(acc_dict.keys(), acc_dict.values()))\nacc_tuples\n[('RMSE', 1), ('MAPE', 0.04), ('R-sq', 0.9)]\n\n\n\nComprehensions\n\nMisc\n\nâ€˜for â€” inâ€™ construct within comprehensions is faster than the traditional for-loops\n\nnot faster than (all?) lambda-filters (see functions &gt;&gt; lambda)\n\nReturns lists or dicts (just change the bracket types)\n\nDicts\n\nSyntax: mydict = {key:val for key, val in zip(keys_list, vals_list)}\nCombine key:value lists into a dictionary\nacc_dict = {k:v for k,v in zip(acc_names, acc_values)}\nReturn value and output of expression\nmydict = {v: v**2 for v in numberslist}\nIf numberslist =[1,2,3], then mydict = {1:1, 2:4, 3:9}\n\nLists\n\nSyntax: newlist = [expression for item in iterable if condition == True]\nWith expression\nmylist = [x**2 for x in numberslist]\n\nif numberslist =[1,2,3], then mylist = [1,4,9]\n\nSet values in a list to uppercase\nnewlist = [x.upper() for x in fruits]\nWith conditional expression (if â€” else)\n\nappend to the comprehension to filter the dictionary or list\nSyntax: mylist = [expressionA if (condition2==True) else expressionB for item in list if (condition1==True)]\nExample: newlist = [x if x != \"banana\" else \"orange\" for x in fruits]\n\nReturn â€œorangeâ€ instead of â€œbananaâ€\n\nExample: new_list = [(x**2) if (x&gt;90) else (x**3) for x in old_list if (x%2==0)]\n\nSays\n\nSquare an argument if it exceeds 90, else cube itÂ \nReturn all the exponentiated results only if the argument was an even number\n\n\nExample: c = [d for d in datstrlist if ((d.endswith(\"urday\") or d.endswith(\"unday\")) and \"Oc\" in d)]\n\nString filter than looks for strings with saturdays and sundays in october\n*Slower than a lamda-filter* (See Functions &gt;&gt; lambda)\n\n\n\nNested\n\nSyntax: myset = {{expression(itemA, itemB) for itemA in setA} for itemB in setB}\nExample: {j for i in range(2, int(N**0.5)+1) for j in range(i**2, N, i)}\n\nN = 100000\ncreates a set of all the integers from 2 to 100,000.\npaces through all the integers i up to the square root of N\ndiscards from the set of 100,000 those numbers j which are equal or larger than the square of i"
  },
  {
    "objectID": "qmd/python-general.html#sec-py-gen-debug",
    "href": "qmd/python-general.html#sec-py-gen-debug",
    "title": "General",
    "section": "Debugging",
    "text": "Debugging\n\nMisc\nTerms\n\nException Errors - Raised when the syntax is correct but the program results in an error.\nSyntax Errors - Occur when the interpreter detects invalid syntax (relatively easier to fix)\n\ne.g.Â unmatched parenthesis\n\nTraceback - A report that helps us understand the reason for an exception.\n\nContains function calls made in the code along with their line numbers"
  },
  {
    "objectID": "qmd/python-general.html#sec-py-gen-errhand",
    "href": "qmd/python-general.html#sec-py-gen-errhand",
    "title": "General",
    "section": "Error Handling",
    "text": "Error Handling\n\ntry + except\n\nSays try the main code snippet, but if an exception (error) occurs, run the secondary code snippet, the workaround.\n\ndef pct_difference_error_handling(n1, n2):\nÂ  '''Function that takes two numbers and return the percentual difference\nÂ  between n1 and n2, being n1 the reference number'''\n\nÂ  # Try the main code\nÂ  try:\nÂ  Â  pct_diff = (n1-n2)/n1\nÂ  Â  return f'The difference between {n1} and {n2} is {n1-n2}, which is {pct_diff*100}% of {n1}'\n\nÂ  # If you find an error, use this code instead\nÂ  except:\nÂ  Â  pct_diff = (int(n1)-int(n2))/int(n1)\nÂ  Â  return f'The difference between {n1} and {n2} is {int(n1)-int(n2){style='color: #990000'}[}]{style='color: #990000'}, which is {pct_diff*100}% of {n1}'\n\nÂ  # Optional\nÂ  finally:\nÂ  Â  print(\"Code ended\")\n\nAssumes the error will be the user enters a string instead of a numeric. If errors, converts string to numeric and calcs.\nfinally: - This argument will always run, regardless if the try block raises an error or not. So it could be a completion message or a summary, for example."
  },
  {
    "objectID": "qmd/python-jupyter.html#sec-py-jupy-misc",
    "href": "qmd/python-jupyter.html#sec-py-jupy-misc",
    "title": "Jupyter",
    "section": "Misc",
    "text": "Misc\n\nDocs\nKernel Menu Commands (navbar)\n\nInterrupt: This command stops the processes that are currently running in a cell. This can be used, for example, to stop the training of a model, even if not all training epochs have been reached yet.\nRestart & Run All: With this command, all cells can be executed again and the previous variables were deleted. This can be useful if you want to read a newer data set into the existing program.\nRestart: The sole command â€œRestartâ€ leads to the same result, but not all cells are executed again.\nReconnect: When training large models, the kernel can â€œdieâ€ because the memory is full. Then a reconnect makes sense.\nShutdown: As long as a kernel is still running, it also ties up memory. If you run other programs in parallel for which you want to free memory, the â€œShutdownâ€ command can make sense.\n\nAdd HTML to markdown cells\n\nLimit Length of Cell Output\n\nRight click output cell &gt;&gt; Click â€œEnable Scrolling for Outputsâ€"
  },
  {
    "objectID": "qmd/python-jupyter.html#sec-py-jupy-shortcuts",
    "href": "qmd/python-jupyter.html#sec-py-jupy-shortcuts",
    "title": "Jupyter",
    "section": "Shortcuts",
    "text": "Shortcuts\n\nPreloaded\n\n\n\n\n\n\n\n\nDescription\nShortcut\n\n\n\n\nshift + Enter\nRun current cell and select the cell below\n\n\nctrl/cmd + Enter\nRun current cell\n\n\nalt/option + Enter\nRun current cell and insert another cell below\n\n\nctrl/cmd + s\nSave notebook\n\n\ni, i\nInterupt cell calculation\n\n\n0, 0\nRestart cell calculation\n\n\n\n\nCustom\n\nCreate/Edit\n\nNavbar &gt;&gt; Settings &gt;&gt; Advanced Settings Editor\nClick JSON Settings Editor (Top Right)\nCLick on User Preferences tab\n\nExample\n\n{\n    \"command\": \"notebook:move-cell-up\",\n    \"keys\": [\n        \"Ctrl Shift ArrowUp\"\n    ],\n    \"selector\": \".jp-Notebook:focus\"\n},\n{\n    \"command\": \"notebook:move-cell-down\",\n    \"keys\": [\n        \"Ctrl Shift ArrowDown\"\n    ],\n    \"selector\": \".jp-Notebook:focus\"\n},"
  },
  {
    "objectID": "qmd/python-jupyter.html#sec-py-jupy-ops",
    "href": "qmd/python-jupyter.html#sec-py-jupy-ops",
    "title": "Jupyter",
    "section": "Operations",
    "text": "Operations\n\nFirst click inside cell &gt;&gt; Press Esc to enter Command mode (Cursor should stop blinking)\n\n\nInsert/Delete Cells\n\na - Insert new cell above current cell\ndd (press d twice) - Delete current cell\nb - Insert new cell below current cell\n\n\n\nChange Cell Type\n\nm - Markdown mode (for writing comments and headers)\ny - Code mode\n\n\n\nSelect Multiple Cells\n\nWhile holding Shift + use the \\(\\uparrow\\) or \\(\\downarrow\\) to expand the selection"
  },
  {
    "objectID": "qmd/python-misc.html#sec-py-misc-misc",
    "href": "qmd/python-misc.html#sec-py-misc-misc",
    "title": "Misc",
    "section": "Misc",
    "text": "Misc\n\nEDA tools\n\nMisc\n\nNotes from - 3 Tools for Fast Data Profiling (overview)\n\nLux - Jupyter notebook widget that provides visual data profiling via existing pandas functions which makes this extremely easy to use if you are already a pandas user. It also provides recommendations to guide your analysis with the intent function. However, Lux does not give much indication as to the quality of the dataset such as providing a count of missing values for example.\n{{pandas_profiling}} - Produces a rich data profiling report with a single line of code and displays this in line in a Juypter notebook. The report provides most elements of data profiling including descriptive statistics and data quality metrics. Pandas-profiling also integrates with Lux.\n{{sweetviz}} - Provides a comprehensive and visually attractive dashboard covering the vast majority of data profiling analysis needed. This library also provides the ability to compare two versions of the same dataset which the other tools do not provide."
  },
  {
    "objectID": "qmd/python-misc.html#sec-py-misc-bd",
    "href": "qmd/python-misc.html#sec-py-misc-bd",
    "title": "Misc",
    "section": "Big Data",
    "text": "Big Data\n\n{{datatable}}\n\nMisc\n\nResources\n\nAn Overview of Pythonâ€™s DataTable\n\n\nfreadÂ  for fast loading of large datasets\nimport datatable as dtÂ  # pip install datatable\ntps_dt = dt.fread(\"data/tps_september_train.csv\").to_pandas()\n\nFor other options (e.g.Â Dask, Vaex, or cuDF) see this Kaggle notebook\n{{cuDF}}\n\nDocs, Beginnerâ€™s tutorial\nUseful for extreme data sizes (e.g.Â 100s of billions of rows)\nsimilar to pandas syntax\n\n{{vaex}}\n\nDocs\n\n{{dask}}\n\nSee MLOps &gt;&gt; Dask"
  },
  {
    "objectID": "qmd/python-misc.html#sec-py-misc-func",
    "href": "qmd/python-misc.html#sec-py-misc-func",
    "title": "Misc",
    "section": "Functions",
    "text": "Functions\n\nSuppress warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")"
  },
  {
    "objectID": "qmd/python-misc.html#sec-py-misc-mlset",
    "href": "qmd/python-misc.html#sec-py-misc-mlset",
    "title": "Misc",
    "section": "ML Set-Up",
    "text": "ML Set-Up\n# Suppress (annoying) warnings\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\nignore_warnings(category=ConvergenceWarning)\nif not sys.warnoptions:\nÂ  Â  warnings.simplefilter(\"ignore\")\nÂ  Â  os.environ[\"PYTHONWARNINGS\"] = ('ignore::UserWarning,ignore::RuntimeWarning')\n\n# Ensure logging\nlogging.basicConfig(\nÂ  Â  format='%(asctime)s:%(name)s:%(levelname)s - %(message)s',\nÂ  Â  level=logging.INFO,\nÂ  Â  handlers=[\nÂ  Â  Â  Â  logging.FileHandler(\"churn_benchmarking.log\"),\nÂ  Â  Â  Â  logging.StreamHandler()\nÂ  Â  ],\nÂ  Â  datefmt='%Y-%m-%d %H:%M:%S')\n\n# Determine number of cpus available\nn_cpus = mp.cpu_count()\nlogging.info(f\"{n_cpus} cpus available\")\n\n# Visualize pipeline when calling it\nset_config(display=\"diagram\")\n\n# Load prepared (pre-cleaned) files for benchmarking\nfile_paths = [f for f in glob.glob(\"00_data/*\") if f.endswith('_cleaned.csv')]\nfile_names = [re.search('[ \\w-]+?(?=\\_cleaned.)',f)[0] for f in file_paths]\ndfs = [pd.read_csv(df, low_memory=False) for df in file_paths]\ndata_sets = dict(zip(file_names, dfs))\nif not data_sets:\nÂ  Â  logging.error('No data sets have been loaded')\nÂ  Â  raise ValueError(\"No data sets have been loaded\")\nlogging.info(f\"{len(data_sets){style='color: #990000'}[}]{style='color: #990000'} data sets have been loaded.\")"
  },
  {
    "objectID": "qmd/python-misc.html#download-and-unzip-helper",
    "href": "qmd/python-misc.html#download-and-unzip-helper",
    "title": "Misc",
    "section": "Download and Unzip helper",
    "text": "Download and Unzip helper\nimport urllib.request\nfrom zipfile import ZipFile\nimport os\ndef extract(url: str, dest: str, target: str = '') -&gt; None:\nÂ  Â  \"\"\"\nÂ  Â  Retrieve online data sources from flat or zipped CSV.\nÂ  Â  Places data in data/raw subdirectory (first creating, as needed).\nÂ  Â  For zip file, automatically unzip target file.Â \nÂ  Â  Args:\nÂ  Â  Â  Â  url (str): URL path to the source file to be downloadedÂ \nÂ  Â  Â  Â  dest (str): FileÂ  for the destination file to land\nÂ  Â  Â  Â  target (str, optional): Name of file to extract (in case of zipfile). Defaults to ''.\nÂ  Â  \"\"\"\nÂ  Â  # set-up expected directory structure, if not exists\nÂ  Â  if not os.path.exists('data'):\nÂ  Â  Â  Â  os.mkdir('data')\nÂ  Â  if not os.path.exists('data/raw'):\nÂ  Â  Â  Â  os.mkdir('data/raw')\n\nÂ  Â  # download file to desired location\nÂ  Â  dest_path = os.path.join('data', 'raw', dest)\nÂ  Â  urllib.request.urlretrieve(url, dest_path)\nÂ  Â  # unzip and clean-up (remove zip) if needed\nÂ  Â  if target != '':\nÂ  Â  Â  Â  with ZipFile(dest_path, 'r') as zip_obj:\nÂ  Â  Â  Â  Â  Â  zip_obj.extract(target, path = \"data//raw\")\nÂ  Â  Â  Â  os.remove(dest_path)\n\nfrom helpers.extract import extract\nurl_cps_suppl = 'https://www2.census.gov/programs-surveys/cps/datasets/2020/supp/nov20pub.csv'\nextract(url_cps_suppl, 'cps_suppl.csv')\n\nFrom Riederer (github, article)"
  },
  {
    "objectID": "qmd/python-misc.html#sec-py-misc-easot",
    "href": "qmd/python-misc.html#sec-py-misc-easot",
    "title": "Misc",
    "section": "Extract a section of text",
    "text": "Extract a section of text\n\n\nDesired section of text is split between 2 â€œ~~~â€ strings\nProcess\n\nString is split into lines\nFind the start and stop indexes for the 2 â€œ~~~â€\nExtract lines between to the two indexes"
  },
  {
    "objectID": "qmd/python-misc.html#sec-py-misc-shstup",
    "href": "qmd/python-misc.html#sec-py-misc-shstup",
    "title": "Misc",
    "section": "Shell Start-Up",
    "text": "Shell Start-Up\n\nA start-up script automatically imports libraries, definines functions, or sets variables, etc. when the python interpreter is started.\n\nEvery time you start a shell, the first thing you usually do is import a bunch of stuff, or frenetically press the top arrow key to recall something from your history. This is aggravated by the fact Python has very limited support for reloading changed modules in a shell, so restarting it is a common thing.\n\nSteps\n\nChoose a location for your script which can be anywhere\nCreate python script at the location and fill in whatever you want to happen when you start a python REPL\n\nName can be pythonstartup.py or whatever\n\nSet the PYTHONSTARTUP environment variable to the path of the file\n\nWindows:\n\nCMD\nset PYTHONSTARTUP=C:\\path\\to\\pythonstartup.py\nPowershell\nSet-Item -Name PYTHONSTARTUP -Value C:\\path\\to\\pythonstartup.py\n\nMac/Linux:\nexport PYTHONSTARTUP=/path/to/pythonstartup.py\n\n\nExample: From Happiness is a good PYTHONSTARTUP script\n\nimport atexit\n\n# First, a lot of imports. I don't use all of them all the time, \n# but I like to have them available.\n\nimport csv\nimport datetime as dt\nimport hashlib\nimport json\nimport math\nimport os\nimport random\nimport re\nimport shelve\nimport subprocess\nimport sys\nimport tempfile\nfrom collections import *\nfrom functools import partial\nfrom inspect import getmembers, ismethod, stack\nfrom io import open\nfrom itertools import *\nfrom math import *\nfrom pprint import pprint as pretty_print\nfrom types import FunctionType\nfrom uuid import uuid4\nfrom unittest.mock import patch, Mock, MagicMock\nfrom datetime import datetime, date, timedelta\n\n\nimport pip\n\n# Set ipython prompt to \"&gt;&gt;&gt; \" for easier copying\ntry:\n    from IPython import get_ipython\n\n    get_ipython().run_line_magic(\"doctest_mode\", \"\")\n    get_ipython().run_line_magic(\"load_ext\", \"ipython_autoimport\")\nexcept:\n    pass\n\n\n\ntry:\n    import asyncio \n    # for easier pasting\n    from typing import * \n    from dataclasses import dataclass, field\nexcept ImportError:\n    pass\n\n# Mostly to parse strings to dates\ntry:\n    import pendulum\nexcept ImportError:\n    pass\n\n# I think you know why\ntry:\n    import requests\nexcept ImportError:\n    pass\n\n# If I'm in a regular Python shell, at least activate tab completion\ntry:\n    import readline\n\n    readline.parse_and_bind(\"tab: complete\")\nexcept ImportError:\n    pass\n\ntry:\n    # if rich is installed, set the repr() to be pretty printted\n\n    from rich import pretty \n    pretty.install() \n\nexcept ImportError:\n    pass\n\n# I wish Python had a Path literal but I can get pretty close with this:\n# Tiis let me to p/\"path/to/file\" to get a Path object\nfrom pathlib import Path\ntry:\n    class PathLiteral:\n        def __truediv__(self, other):\n            try:\n                return Path(other.format(**stack()[1][0].f_globals))\n            except KeyError as e:\n                raise NameError(\"name {e} is not defined\".format(e=e))\n\n        def __call__(self, string):\n            return self / string\n\n    p = PathLiteral()\nexcept ImportError:\n    pass\n\n\n# Force jupyter to print any lone variable, not just the last one in a cell\ntry:\n    from IPython.core.interactiveshell import InteractiveShell\n\n    InteractiveShell.ast_node_interactivity = \"all\"\n\nexcept ImportError:\n    pass\n\n\n# Check if I'm in a venv\nVENV = os.environ.get(\"VIRTUAL_ENV\")\n\n#  Make sure I always have a temp folder ready to go\nTEMP_DIR = Path(tempfile.gettempdir()) / \"pythontemp\"\ntry:\n    os.makedirs(TEMP_DIR)\nexcept Exception as e:\n    pass\n\n# I'm lazy\ndef now():\n    return datetime.now()\n\n\ndef today():\n    return date.today()\n\n\n# Since restarting a shell is common, I like to have a way to persit\n# calculations between sessions. This is a simple way to do it.\n# I can do store.foo = 'bar' and get store.foo in the next session.\nclass Store(object):\n    def __init__(self, filename):\n\n        object.__setattr__(self, \"DICT\", shelve.DbfilenameShelf(filename))\n        # cleaning the dict on the way out\n        atexit.register(self._clean)\n\n    def __getattribute__(self, name):\n        if name not in (\"DICT\", \"_clean\"):\n            try:\n                return self.DICT[name]\n            except:\n                return None\n        return object.__getattribute__(self, name)\n\n    def __setattr__(self, name, value):\n        if name in (\"DICT\", \"_clean\"):\n            raise ValueError(\"'%s' is a reserved name for this store\" % name)\n        self.DICT[name] = value\n\n    def _clean(self):\n        self.DICT.sync()\n        self.DICT.close()\n\n\npython_version = \"py%s\" % sys.version_info.major\ntry:\n    store = Store(os.path.join(TEMP_DIR, \"store.%s.db\") % python_version)\nexcept:\n    # This could be solved using diskcache but I never took the time\n    # to do it.\n    print(\n        \"\\n/!\\ A session using this store already exist.\"\n    )\n\n\n# Shorcurt to pip install packages without leaving the shell\ndef pip_install(*packages):\n    \"\"\" Install packages directly in the shell \"\"\"\n    for name in packages:\n        cmd = [\"install\", name]\n        if not hasattr(sys, \"real_prefix\"):\n            raise ValueError(\"Not in a virtualenv\")\n        pip.main(cmd)\n\n\ndef is_public_attribute(obj, name, methods=()):\n    return not name.startswith(\"_\") and name not in methods and hasattr(obj, name)\n\n\n# if rich is not installed\ndef attributes(obj):\n    members = getmembers(type(obj))\n    methods = {name for name, val in members if callable(val)}\n    is_allowed = partial(is_public_attribute, methods=methods)\n    return {name: getattr(obj, name) for name in dir(obj) if is_allowed(obj, name)}\n\n\nSTDLIB_COLLECTIONS = (\n    str,\n    bytes,\n    int,\n    float,\n    complex,\n    memoryview,\n    dict,\n    tuple,\n    set,\n    bool,\n    bytearray,\n    frozenset,\n    slice,\n    deque,\n    defaultdict,\n    OrderedDict,\n    Counter,\n)\n\ntry:\n    # rich a great pretty printer, but if it's not there, \n    # I have a decent fallback\n    from rich.pretty import print as pprint\nexcept ImportError:\n\n    def pprint(obj):\n        if isinstance(obj, STDLIB_COLLECTIONS):\n            pretty_print(obj)\n        else:\n            try:\n                name = \"class \" + obj.__name__\n            except AttributeError:\n                name = obj.__class__.__name__ + \"()\"\n            class_name = obj.__class__.__name__\n            print(name + \":\")\n            attrs = attributes(obj)\n            if not attrs:\n                print(\"    &lt;No attributes&gt;\")\n            for name, val in attributes(obj).items():\n                print(\"   \", name, \"=\", val)\n\n\n# pp/obj is a shortcut to pprint(obj), it work as a postfix operator as \n# well, which in the shell is handy\nclass Printer(float):\n    def __call__(self, *args, **kwargs):\n        pprint(*args, **kwargs)\n\n    def __truediv__(self, other):\n        pprint(other)\n\n    def __rtruediv__(self, other):\n        pprint(other)\n\n    def __repr__(self):\n        return repr(pprint)\n\n\npp = Printer()\npp.__doc__ = pprint.__doc__\n\n# Same as the printer, but for turning something into a list with l/obj\nclass ToList(list):\n    def __truediv__(self, other):\n        return list(other)\n\n    def __rtruediv__(self, other):\n        return list(other)\n\n    def __call__(self, *args, **kwargs):\n        return list(*args, **kwargs)\n\n\nl = ToList()\n\n# Those alias means JSON is now valid Python syntax that you can copy/paste \nnull = None\ntrue = True\nfalse = False\n\nAlso has a class for creating fake data. See article for the code."
  },
  {
    "objectID": "qmd/python-numpy.html#sec-py-numpy-misc",
    "href": "qmd/python-numpy.html#sec-py-numpy-misc",
    "title": "Numpy",
    "section": "Misc",
    "text": "Misc\n\nLinear algebra resources\n\nnumpy.linalg\nscipy.linalg documentation\n\nOptimization\n\n{{numba}} - JIT compiler that translates a subset of Python and NumPy code into fast machine code.\n\nTerms\n\nBroadcasting is a mechanism that allows Numpy to handle (nd)arrays of different shapes during arithmetic operations.\n\nSee article for details on how this works and when it fails (ValueErrors)\nA smaller (nd)array being â€œbroad-castedâ€ into the same shape as the larger (nd)array, before doing certain operations.\nThe smaller (nd)array will be copied multiple times, until it reaches the same shape as the larger (nd)array.\nFast, since it vectorizes array operations so that looping occurs in optimized C code\n\nMemory Views: Working with views can be highly desirable since it avoids making unnecessary copies of arrays to save memory resources\n\nnp.may_share_memory(new_array, old_array) - if the result is TRUE, then new_array is a memory view\n\nndarrays - multi-dimensional arrays of fixed-size items.\nPandas will typically outperform numpy ndarrays in cases that involve significantly larger volume of data (say &gt;500K rows) (not sure if this is true)\n\nInfo (no parentheses after method)\n\nNumber of dimensions: ary.ndim\nShape: ary.shape\nNumber of elements: ary.size\nNumber of rows (i.e.Â 1st dim): len(ary)\n\nRandom Number Generator\nrng2 = np.random.default_rng(seed=123)\nrng2.random(3)\n\narray([0.68235186, 0.05382102, 0.22035987])\nSample w/replacement\nnp.random.seed(3)\n# a parameter: generate a list of unique random numbers (from 0 to 11)\n# size parameter: how many samples we want (12)\n# replace = True: sample with replacement\nnp.random.choice(a=12, size=12, replace=True)\nCreate a grid of values\ngrid_q_low = np.linspace(number1,number2,num_vals).reshape(-1,1)\ngrid_q_high = np.linspace(number3,number4,num_vals).reshape(-1,1)\ngrid_q = np.concatenate((grid_q_low,grid_q_high),1)\n\nlinspace returns evenly spaced numbers over a specified interval.\n\nâ€œnumber1,2,3,4â€ are numeric values for args: start and stop\nreshape coerces the results into m x 1 column arrays (-1 is a placeholder)\n\nconcantenate axis = 1 says stack column-wise, so this results in a m x 2 array"
  },
  {
    "objectID": "qmd/python-numpy.html#sec-py-numpy-crecoe",
    "href": "qmd/python-numpy.html#sec-py-numpy-crecoe",
    "title": "Numpy",
    "section": "Create or Coerce",
    "text": "Create or Coerce\n\nComparison with R DataFrame\n&gt;&gt;&gt; X = np.arange(6).reshape(3, 2)\n&gt;&gt;&gt; X\narray([[0, 1],\nÂ  Â  Â  [2, 3],\nÂ  Â  Â  [4, 5]])\n# r\nX &lt;- data.frame(x1 = c(0,2,4), x2 = c(1,3,5))\n\nVariables are column in the array\n\nCreate column-wise array\n# example 1\na = np.array((1,2,3))\nb = np.array((2,3,4))\nnp.column_stack((a,b))\narray([[1, 2],\nÂ  Â  Â  [2, 3],\nÂ  Â  Â  [3, 4]])\n\n# example 2\nnp.column_stack([\nÂ  Â  model.predict(X_cal, quantile=(alpha/2)*100),Â \nÂ  Â  model.predict(X_cal, quantile=(1-alpha/2)*100)])\nCreate a constant array\nconstant_arr = np.full((other_array.shape), 5)\n# ** Don't really need this, since other_array + 5 works through broadcasting **\n\nâ€œother_arrayâ€ the array we want the constant array to do arithmetic with\n.shape method outputs other_arrayâ€™s dimensions\n\nCoerce from list\na = [1, 2, 3]\nnp.array(a)\n\na = [[1,2,3], [4,5,6]]\nnp.array(a, dtype = np.float32)\n\ndtype is optional\n\nConvert pandas df to ndarray\n\nnew_array = pandas_df.values\npandas_df.to_numpy()\nnp.array(df)"
  },
  {
    "objectID": "qmd/python-numpy.html#sec-py-numpy-manip",
    "href": "qmd/python-numpy.html#sec-py-numpy-manip",
    "title": "Numpy",
    "section": "Manipulation",
    "text": "Manipulation\n\nSubsetting a row\nary = np.array([[1, 2, 3],\nÂ  Â  Â  Â  Â  Â  Â  Â  [4, 5, 6]])\n\nfirst_row = ary[0]\nfirst_row = ary[1:3]\n\nAny changes to â€œfirst_rowâ€ also change â€œaryâ€\nProduces a â€œmemory viewâ€ which conserves memory and increases speed\nCan only subset contiguous indices\n\nSubsetting columns using Fancy Indexing\nary_copy = ary[:, [0, 2]] # first and and last column\n\nUses tuple or list objects of non-contiguous integer indices to return desired array elements\n** produces a copy of the array. So takes-up more memory**\n\nBoolean masking\nary_bool1 = (ary &gt; 3) & (ary % 2 == 0)\nary_bool2 = ary &gt; 3\nary_bool2\n\narray([[False, False, False],\nÂ  Â  Â  [ True,Â  True,Â  True]])\nSubsetting 1st elt of all dimensions using ellipsis\n# create an array with a random number of dimensions\ndimensions = np.random.randint(1,10)\nitems_per_dimension = 2\nmax_items = items_per_dimension**dimensions\naxes = np.repeat(items_per_dimension, dimensions)\narr = np.arange(max_items).reshape(axes)\n\narr[..., 0]\narray([[[[ 0,Â  2],\nÂ  Â  Â  Â  [ 4,Â  6]],\n\nÂ  Â  Â  Â  [[ 8, 10],\nÂ  Â  Â  Â  [12, 14]]],\n\nÂ  Â  Â  [[[16, 18],\nÂ  Â  Â  Â  [20, 22]],\n\nÂ  Â  Â  Â  [[24, 26],\nÂ  Â  Â  Â  [28, 30]]]])\n\nellipsis makes it so if you have a large (or unknown) number of dimensions, you donâ€™t have to use a ton of colons to subset the array\nHere, â€œarrâ€ has five dimensions\n\nFilter by boolean mask\nary[ary_bool2]\n\narray([4, 5, 6])\nReshaping\n\n1 dim to 2 dim\nary1d = np.array([1, 2, 3, 4, 5, 6])\nary2d_view = ary1d.reshape(2, 3)\nary2d_view\n\narray([[1, 2, 3],\nÂ  Â  Â  [4, 5, 6]])\n\n2 x 3 array\n\nNeed 2 columns\nary1d.reshape(-1, 2)\n\n-1 is a placeholder\nUseful if we donâ€™t know the number of rows, but we know we want 2 columns\n\nFlatten array\nary = np.array([[[1, 2, 3],\nÂ  Â  Â  Â  Â  Â  Â  Â  [4, 5, 6]]])\nary.reshape(-1)\nary.ravel()\nary.flatten()\n\narray([1, 2, 3, 4, 5, 6])\n\nreshape and ravel produce memory views; flatten produces a copy in memory\n-1 is a placeholder\n\n\nCombine arrays\nary = np.array([[1, 2, 3]])\n# stack along the first axis (here: rows)\nnp.concatenate((ary, ary), axis=0)\n\naxis=1 would be stack column-wise (i.e.Â side-by-side)\nComputationally ineffiicient, so should avoid if possible.\n\nSort vector (arrange)\n# asc\n&gt;&gt;&gt; boris = np.maximum(moose, squirrel) # see above\n&gt;&gt;&gt; np.sort(boris)\narray([-2, -1,Â  1,Â  4])\n\n# desc\n&gt;&gt;&gt; np.sort(boris,0)[::-1]\narray([ 4,Â  1, -1, -2])\nSort array (arrange)\n&gt;&gt;&gt; squirrel = np.array([-2,-2,-2,-2])\n&gt;&gt;&gt; moose = np.array([-3,-1,4,1])\n&gt;&gt;&gt; natascha = np.vstack((moose, squirrel))\narray([[-3, -1,Â  4,Â  1],\nÂ  Â  Â  [-2, -2, -2, -2]])\n\n# column-wise (default)\n&gt;&gt;&gt; np.sort(natascha)\narray([[-3, -1,Â  1,Â  4],\nÂ  Â  Â  [-2, -2, -2, -2]])\n# row-wise\n&gt;&gt;&gt; np.sort(natascha, 0)\narray([[-3, -2, -2, -2],\nÂ  Â  Â  [-2, -1,Â  4,Â  1]])\n# row-wise desc\n&gt;&gt;&gt; np.sort(natascha, 0)[::-1]\narray([[-2, -1,Â  4,Â  1],\nÂ  Â  Â  [-3, -2, -2, -2]])\nChange values by condition\nary = np.array([1, 2, 3, 4, 5])\nnp.where(ary &gt; 2, 1, 0)\n\nAny values &gt; 2 get changed to a 1 and the rest are changed to 0"
  },
  {
    "objectID": "qmd/python-numpy.html#sec-py-numpy-math",
    "href": "qmd/python-numpy.html#sec-py-numpy-math",
    "title": "Numpy",
    "section": "Mathematics",
    "text": "Mathematics\n\nIncrementing the values\nary_copy += 99\narray([[100, 102],Â \nÂ  Â  Â  [103, 105]])\nMatrix multiplication\nmatrix = np.array([[1, 2, 3],Â \nÂ  Â  Â  Â  Â  Â  Â  Â  Â  [4, 5, 6]])\ncolumn_vector = np.array([1, 2, 3]).reshape(-1, 1)\nnp.matmul(matrix, column_vector)\nDot product\nrow_vector = np.array([1, 2, 3])\nnp.matmul(row_vector, row_vector)\nnp.dot(row_vector, row_vector)\n\nOne or the other can be slightly faster on specific machines and versions of BLAS\n\nTranspose a matrix\nmatrix = np.array([[1, 2, 3],Â \nÂ  Â  Â  Â  Â  Â  Â  Â  Â  [4, 5, 6]])\nmatrix.transpose()\n\narray([[1, 4],\nÂ  Â  Â  [2, 5],\nÂ  Â  Â  [3, 6]])\nFind pairwise maximum (pmax)\n&gt;&gt;&gt; squirrel = np.array([-2,-2,-2,-2])\n&gt;&gt;&gt; moose = np.array([-3,-1,4,1])\n&gt;&gt;&gt; np.maximum(moose, squirrel)\narray([-2, -1,Â  4,Â  1])"
  },
  {
    "objectID": "qmd/python-pandas-recipes.html#sec-py-pandas-rec-misc",
    "href": "qmd/python-pandas-recipes.html#sec-py-pandas-rec-misc",
    "title": "Pandas, Recipes",
    "section": "Misc",
    "text": "Misc\n\n{{sketch}}: AI code-writing assistant for pandas users that understands the context of your data, greatly improving the relevance of suggestions. Sketch is usable in seconds and doesnâ€™t require adding a plugin to your IDE. Itâ€™s just a regular function + method."
  },
  {
    "objectID": "qmd/python-pandas-recipes.html#sec-py-pandas-rec-trans",
    "href": "qmd/python-pandas-recipes.html#sec-py-pandas-rec-trans",
    "title": "Pandas, Recipes",
    "section": "Transformations",
    "text": "Transformations\n\nBin a numeric\ndf = pd.DataFrame({\"value\": np.random.randint(0, 100, 20){style='color: #990000'}[}]{style='color: #990000'})\nlabels = [\"{0} - {1}\".format(i, i + 9) for i in range(0, 100, 10)]\ndf[\"group\"] = pd.cut(df.value, range(0, 105, 10), right=False, labels=labels)\ndf.head(10)\nÂ  valueÂ  Â  group\n0Â  Â  65Â  60 - 69\n1Â  Â  49Â  40 - 49\n2Â  Â  56Â  50 - 59\n3Â  Â  43Â  40 - 49\n4Â  Â  43Â  40 - 49\n5Â  Â  91Â  90 - 99\n6Â  Â  32Â  30 - 39\n7Â  Â  87Â  80 - 89\n8Â  Â  36Â  30 - 39\n9Â  Â  Â  8Â  Â  0 - 9\n\nqcut will create labels, so this probably isnâ€™t needed"
  },
  {
    "objectID": "qmd/python-pandas-recipes.html#sec-py-pandas-rec-strs",
    "href": "qmd/python-pandas-recipes.html#sec-py-pandas-rec-strs",
    "title": "Pandas, Recipes",
    "section": "Strings",
    "text": "Strings\n\nGenerate list of strings from a variable\ns = pd.Series([\"a\", \"b\", \"c\", \"a\"], dtype=\"category\")\nnew_categories = [\"Group %s\" % g for g in s.cat.categories]\nnew_categories\n['Group a', 'Group b', 'Group c']\nExtract city, state, and zip code from an address variable"
  },
  {
    "objectID": "qmd/python-pandas-recipes.html#sec-py-pandas-rec-eda",
    "href": "qmd/python-pandas-recipes.html#sec-py-pandas-rec-eda",
    "title": "Pandas, Recipes",
    "section": "Comparisons",
    "text": "Comparisons\n\nequals\n# series\nseries1 = pd.Series([1,2,3,4])\nseries2 = pd.Series([2,1,3,4])\nseries1.equals(series2)\n\n# dfs\ndf[\"device_id\"].equals(df1[\"device_id\"])\n#&gt; True\n\n# List of the columns having different values in the DataFrames df1 and df\nfor column in df.columns:\n  if df[column].equals(df1[column]):\n     pass\n  else:\n      print(column)\n\nFlags differences in order, dimensions, and of course, differences in data\n\ncompare\n\ndf4 = df.compare(df1)\ndf4\n\ndevice-temperature and device-status are the two common columns being compared\nself indicates the first DataFrame df and other indicates the other DataFrame df1.\nEssentially merges both the DataFrames and adds a MultiIndex to show both the DataFrames columns side by side, which helps you to see the columns and positions where the values have been changed."
  },
  {
    "objectID": "qmd/python-pandas.html#sec-py-pandas-misc",
    "href": "qmd/python-pandas.html#sec-py-pandas-misc",
    "title": "Pandas",
    "section": "Misc",
    "text": "Misc\n\nExamples of using numeric indexes to subset dfs\n\nindexes &gt;&gt; Syntax for using indexes\nselect\nfiltering &gt;&gt; .loc/.iloc\n\ndf.head(8) to see the first 8 rows\ndf.info() is like str in R\ndf.describe() is like summary in R\n\ninclude='all' to include string columns"
  },
  {
    "objectID": "qmd/python-pandas.html#sec-py-pandas-series",
    "href": "qmd/python-pandas.html#sec-py-pandas-series",
    "title": "Pandas",
    "section": "Series",
    "text": "Series\n\na vector with an index (ordered data object)\ns = pd.Series([20, 21, 12],\nÂ  Â  Â  Â  Â  Â  Â  index=['London', 'New York', 'Helsinki'])\n&gt;&gt; s\nLondonÂ  Â  Â  20\nNew YorkÂ  Â  21\nHelsinkiÂ  Â  12\ndtype: int64\nMisc\n\nTwo Pandas Series with the same elements in a different order are not the same object\nList vs Series: list can only use numeric indexes, while the Pandas Series also allows textual indexes.\n\nArgs\n\ndata: Different data structures can be used, such as a list, a dictionary, or even a single value.\nindex: A labeled index for the elements in the series can be defined.\n\nIf not set, the elements will be numbered automatically, starting at zero.\n\ndtype: Sets the data types of the series\n\nUseful if all data in the series are of the same data type\n\nname: Series can be named.\n\nUseful if the Series is to be part of a DataFrame. Then the name is the corresponding column name in the DataFrame.\n\ncopy: True or False; specifies whether the passed data should be saved as a copy or not\n\nSubset: series_1[\"A\"] or series_1[0]\nFind the index of a value: list(series_1).index(&lt;value&gt;)\nChange value of an index: series_1[\"A\"] = 1 (1 is now the value for index, A)\nAdd a value to a series: series_1[\"D\"] = \"fourth_element\" (D is the next index in the sequence)\nFilter by condition(s): series_1[series_1 &gt; 4] or series_1[series_1 &gt; 4][series_1 != 8]\ndict to Series: pd.Series(dict_1)\nSeries to df: pd.DataFrame([series_1, series_2, series_3])\n\nSeries objects should either all have the same index or no index. Otherwise, a separate column will be created for each different index, for which the other rows have no value"
  },
  {
    "objectID": "qmd/python-pandas.html#sec-py-pandas-cats",
    "href": "qmd/python-pandas.html#sec-py-pandas-cats",
    "title": "Pandas",
    "section": "Categoricals",
    "text": "Categoricals\n\nMisc - Also see - Conversions for converting between types - Optimizations &gt;&gt; Memory Optimizations &gt;&gt; Variable Type\nCategorical (docs)\n\npython version of factors in R\n\nRâ€™s levels are always of type string, while categories in pandas can be of any dtype.\nR allows for missing values to be included in its levels (pandasâ€™ categories). pandas does not allow NaN categories, but missing values can still be in the values.\n\nSee cat.codes below\n\n\nCreate a categorical series: s = pd.Series([\"a\", \"b\", \"c\", \"a\"], dtype=\"category\")\nCreate df of categoricals\ndf = pd.DataFrame({\"A\": list(\"abca\"), \"B\": list(\"bccd\"){style='color: #990000'}[}]{style='color: #990000'}, dtype=\"category\")\ndf[\"A\"]\n0Â  Â  a\n1Â  Â  b\n2Â  Â  c\n3Â  Â  a\n\nSpecify categories and add to dataframe (also see Set Categories below)\nraw_cat = pd.Categorical(\nÂ  Â  [\"a\", \"b\", \"c\", \"a\"], categories=[\"b\", \"c\", \"d\"], ordered=False   \n)\ndf[\"B\"] = raw_cat\ndfÂ \nÂ  AÂ  Â  B\n0Â  aÂ  NaN\n1Â  bÂ  Â  b\n2Â  cÂ  Â  c\n3Â  aÂ  NaN\n\nNote that categories not in the specification get NaNs\n\nSee categories, check if ordered: cat.categories, cat.ordered\ns.cat.categories\nOut[61]: Index(['c', 'b', 'a'], dtype='object')\ns.cat.ordered\nOut[62]: False\nSet categories for a categorical: s = s.cat.set_categories([\"one\", \"two\", \"three\", \"four\"])\nRename categories w/cat.rename_categories\n# with a list of new categories\nnew_categories = [\"Group %s\" % g for g in s.cat.categories]\ns = s.cat.rename_categories(new_categories)\n\n# with a dict\ns = s.cat.rename_categories({1: \"x\", 2: \"y\", 3: \"z\"})\ns\n0Â  Â  Group a\n1Â  Â  Group b\n2Â  Â  Group c\n3Â  Â  Group a\nAdd a category (doesnâ€™t have to be a string, e.g.Â 4): s = s.cat.add_categories([4])\nRemove categories\ns = s.cat.remove_categories([4])\ns.cat.remove_unused_categories()\nOrdered\n\nCreate ordered categoricals\nfrom pandas.api.types import CategoricalDtype\ns = pd.Series([\"a\", \"b\", \"c\", \"a\"])\ncat_type = CategoricalDtype(categories=[\"b\", \"c\", \"d\"], ordered=True)\ns_cat = s.astype(cat_type)\ns_catÂ \n0Â  Â  NaN\n1Â  Â  Â  b\n2Â  Â  Â  c\n3Â  Â  NaN\ndtype: category\nCategories (3, object): ['b' &lt; 'c' &lt; 'd']\n\n# alt\ns = pd.Series([\"a\", \"b\", \"c\", \"a\"]).astype(CategoricalDtype(ordered=True))\nReorder: cat.reorder_categories\ns = pd.Series([1, 2, 3, 1], dtype=\"category\")\ns = s.cat.reorder_categories([2, 3, 1], ordered=True)\ns\n0Â  Â  1\n1Â  Â  2\n2Â  Â  3\n3Â  Â  1\ndtype: category\nCategories (3, int64): [2 &lt; 3 &lt; 1]\n\nCategory codes: .cat.codes\ns = pd.Series([\"a\", \"b\", np.nan, \"a\"], dtype=\"category\")\ns\n0Â  Â  Â  a\n1Â  Â  Â  b\n2Â  Â  NaN\n3Â  Â  Â  a\ndtype: category\nCategories (2, object): ['a', 'b']\n\ns.cat.codes\n0Â  Â  0\n1Â  Â  1\n2Â  -1\n3Â  Â  0\ndtype: int8"
  },
  {
    "objectID": "qmd/python-pandas.html#sec-py-pandas-ops",
    "href": "qmd/python-pandas.html#sec-py-pandas-ops",
    "title": "Pandas",
    "section": "Operations",
    "text": "Operations\n\nRead\n\nMisc\n\nFor large datasets, better to use data.table::fread (see Python, Misc &gt;&gt; Misc)\n\nCSV df = pd.read_csv('wb_data.csv', header=0)\n\nâ€œusecols=[â€˜col1â€™, â€˜col8â€™]â€ for only reading certain columns\n\nRead and process data in chunks\nfor chunk in pd.read_csv(\"dummy_dataset.csv\", chunksize=50000):Â \nÂ  Â  print(type(chunk)) # process data\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\n&lt;class 'pandas.core.frame.DataFrame'&gt;\n\nIssue: Cannot perform operations that need the entire DataFrame. For instance, say you want to perform a groupby() operation on a column. Here, it is possible that rows corresponding to a group may lie in different chunks.\n\n\n\n\nWrite\n\nCSV: df.to_csv(\"file.csv\", sep = \"|\", index = False)\n\nâ€œsepâ€ - column delimiter (assume comma is default)\nâ€œindex=Falseâ€ -Â  instructs Pandas to NOT write the index of the DataFrame in the CSV file\n\n\n\n\nCreate/Copy\n\nDataframes\n\nSyntaxes\ndf = pd.DataFrame({\nÂ  Â  \"first_name\": [\"John\",\"jane\",\"emily\",\"Matt\",\"Alex\"],\nÂ  Â  \"last_name\": [\"Doe\",\"doe\",\"uth\",\"Dan\",\"mir\"],\nÂ  Â  \"group\": [\"A-12\",\"B-15\",\"A-18\",\"A-12\",\"C-15\"],\nÂ  Â  \"salary\": [\"$75000\",\"$72000\",\"Â£45000\",\"$77000\",\"Â£58,000\"]\n})\n\ndf = pd.DataFrame(\nÂ  Â  [\nÂ  Â  Â  Â  (1, 'A', 10.5, True),\nÂ  Â  Â  Â  (2, 'B', 10.0, False),\nÂ  Â  Â  Â  (3, 'A', 19.2, False)Â  Â  Â  Â \nÂ  Â  ],\nÂ  Â  columns=['colA', 'colB', 'colC', 'colD']\n)\n\ndata = pd.DataFrame([[\"A\", 1], [\"A\", 2], [\"B\", 1],Â \nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  [\"C\", 4], [\"A\", 10], [\"B\", 7]],Â \nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  columns = [\"col1\", \"col2\"])\n\ndf = pd.DataFrame([('bird', 389.0),\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  ('bird', 24.0),\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  ('mammal', 80.5),\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  ('mammal', np.nan)],\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  index=['falcon', 'parrot', 'lion', 'monkey'],\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  columns=('class', 'max_speed'))\ndf\nÂ  Â  Â  Â  classÂ  max_speed\nfalconÂ  Â  birdÂ  Â  Â  389.0\nparrotÂ  Â  birdÂ  Â  Â  24.0\nlionÂ  Â  mammalÂ  Â  Â  80.5\nmonkeyÂ  mammalÂ  Â  Â  Â  NaN\nCreate a copy of a df: df2 = df1.copy()\n\n\n\n\nIndexes\n\nMisc\n\nRow labels\nDataFrame indexes do NOT have to be unique\nThe time it takes to search a dataframe is shorter with unique index\nBy default, the index is a numeric row index\n\nSet\ndf = df.set_index('col1')\ndf.set_index(column, inplace=True)\n\nprevious index is discarded\ninplace=True says modify orginal df\n\nChange indexes but keep previous index\ndf.reset_index() # converts index to a column, index now the default numeric\ndf = df.set_index('col2')\n\nreset_index\ninplace=True says modify orginal df (default = False)\ndrop=True discards old index\n\nChange to new indices or expand indices: df.reindex\nSee indexes: df.index.names\nSort: df.sort_index(ascending=False)\nSyntax for using indexes\n\nMisc\n\nâ€œ:â€Â  is not mandatory as a placeholder for the column position\n\n.loc\n\ndf.loc[one_row_label] # Series\ndf.loc[list_of_row_labels] # DataFrame\ndf.loc[:, one_column_label] # Series\ndf.loc[:, list_of_column_labels] # DataFrame\ndf.loc[first_row:last_row]\n\n.iloc\n\ndf.iloc[one_row_position, :] # Series\ndf.iloc[list_of_row_positions, :] # DataFrame\n\n\nExample: df.iloc[0:3] or df.iloc[:3]\n\noutputs rows 0, 1, 2Â  (end point NOT included)\n\nExample: df.iloc[75:]\n\n(e.g.Â 78 rows) outputs rows 75, 76, 77 (end point, i.e., last row, NOT included)\n\nExample: df.loc['The Fool' : 'The High Priestess']\n\noutputs all rows from â€œThe Foolâ€ to â€œThe High Priestessâ€ (end point included)\n\nExample: Filtering cells\ndf.loc[\nÂ  Â  ['King of Wands', 'The Fool', 'The Devil'],\nÂ  Â  ['image', 'upright keywords']\n]\n\nindex values: â€˜King of Wandsâ€™, etc.\ncolumn names: â€˜imageâ€™, etc.\n\nMulti-indexes\n\nUse list of variables to create multi-index: df.set_index(['family', 'vowel_inventories'], inplace=True)\n\n\nâ€œfamilyâ€ is the 0th index; â€œvowel_inventoriesâ€ is the 1st index\n\nSee individual index values: df.index.get_level_values(0)\n\nGets the 0th index values (e.g.Â â€œfamilyâ€)\n\nSort by specific level: df.sort_index(level=1)\n\nNot specifying a level means df will be sorted first by level 0, then level 1, etc.\n\nReplace index values: df.rename(index={\"Arawakan\" : \"Maipurean\", \"old_value\" : \"new_value\"})\n\nReplaces instances in all levels of the multi-index\nFor only a specific level, use â€œlevels=â€˜level_nameâ€™â€\n\nSubsetting via multi-index\n\nBasic\n\n\nrow_label = ('Indo-European', '2 Average (5-6)')\ndf.loc[row_label, :].tail(3)\nNote: Using â€œ:â€ or specifying columns may be necessary using a multi-index\nWith multiple multi-index values\nrow_labels = [\nÂ  Â  ('Arawakan', '2 Average (5-6)'),\nÂ  Â  ('Uralic', '2 Average (5-6)'),\n]\nrow_labels = (['Arawakan', 'Uralic'], '2 Average (5-6)') # alternate method\ndf.loc[row_labels, :]\nWith only 1 level of the multi-index\nrow_labels = pd.IndexSlice[:, '1 Small (2-4)']\ndf.loc[row_labels, :]\n\n# drops other level of the multi-index\ndf.loc['1 Small (2-4)']\n\n# more verbose\ndf.xs('1 Small (2-4)', level = 'vowel_inventories', drop_level=False)\n\nIgnores level 0 of the multi-index and filters only by level 1\nCould also use an alias, e.g.Â idx = pd.IndexSlice if writing the whole thing gets annoying\n\n\n\n\n\nSelect\n\nAlso see Indexes\nSelect by name:\ndf[[\"name\",\"note\"]]\n\ndf.filter([\"Type\", \"Price\"])\n\ndat.loc[:, [\"f1\", \"f2\", \"f3\"]]\n\ntarget_columns = df.columns.str.contains('\\+') # boolean array: Trues, Falses\nX = df.iloc[:, ~target_columns]\nY = df.iloc[:, target_columns]\n\nâ€œ:â€ is a place holder in this case\n\nSubset\nts_df\nÂ  Â  t-3Â  t-2Â  t-1Â  t-0\n10Â  10.0Â  11.0Â  12.0Â  13.0\n11Â  11.0Â  12.0Â  13.0Â  14.0\n12Â  12.0Â  13.0Â  14.0Â  15.0\nAll but last column\nts_df.iloc[:, :-1]\nÂ  Â  t-3Â  t-2Â  t-1\n10Â  10.0Â  11.0Â  12.0\n11Â  11.0Â  12.0Â  13.0\n12Â  12.0Â  13.0Â  14.0\n\nFor [:, :-2], the last 2 columns would not be includedÂ \n\nAll but first column\nts_df.iloc[:, 1:]\nÂ  Â  t-2Â  t-1Â  t-0\n10Â  11.0Â  12.0Â  13.0\n11Â  12.0Â  13.0Â  14.0\n12Â  13.0Â  14.0Â  15.0\n\ni.e.Â the 0th indexed column is not included\nFor [:, 2:], the 0th and 1st indexed column would not be included\n\nAll columns between the 2nd col and 4th col\nts_df.iloc[:, 1:4]\nÂ  Â  t-2Â  t-1Â  t-0\n10Â  11.0Â  12.0Â  13.0\n11Â  12.0Â  13.0Â  14.0\n12Â  13.0Â  14.0Â  15.0\n\nleft endpoint (1st index) is included but the right endpoint (4th index) is not\ni.e.Â the 1st, 2nd, and 3rd indexed columns are included\n\nAll columns except the first 2 cols and the last col\nts_df.iloc[:, 2:-1]\nÂ  Â  t-1\n10Â  12.0\n11Â  13.0\n12Â  14.0\n\ni.e.Â 0th and 1st indexed columns not included and â€œ-1â€ says donâ€™t include the last column\n\n\n\n\nRename columns\ndf.rename({'variable': 'Year', 'value': 'GDP'}, axis=1, inplace=True)\n-   \"variable\" is renamed to \"Year\" and \"value\" is renamed to \"GDP\"\n\n\nDelete columns\n\ndf.drop(columns = [\"col1\"])\n\n\n\nFiltering\n\nAlso see Indexes\nOn value: df_filtered = data[data.col1 == \"A\"]\nVia query: df_filtered = data.query(\"col1 == 'A'\")\nOn multiple values: df_filtered = data[(data.col1 == \"A\") | (data.col1 == \"B\")]\nBy pattern: df[df.profession.str.contains(\"engineer\")]\n\nOnly rows of profession variable with values containing â€œengineerâ€\nAlso available\n\nstr.startswith: df_filtered = data[data.col1.str.startswith(\"Jo\")]\nstr.endswith: df_filtered = data[data.col1.str.endswith(\"n\")]\n\nIf column has NAs or NaNs, specify arg, â€œnan=Falseâ€ to ignore them, else youâ€™ll get a valueError\nMethods are case-sensitive unless you specify, â€œcase=False:â€\n\nBy negated pattern: df[~df.profession.str.contains(\"engineer\")]\nBy character type: df_filtered = data[data.col1.str.isnumeric()]\n\nAlso available\n\nupper-case: isupper()\nlower-case: islower()\nalphabetic: isalpha()\ndigits: isdigit()\ndecimal: isdecimal()\nwhitespace: isspace()\ntitlecase: istitle()\nalphanumeric: isalnum()\n\n\nBy month of a datetime variable: df[df.date_of_birth.dt.month==11]\n\ndatetime variable has yy-mm-dd format; dt.month accessor used to filter rows of â€œdate_of_birthâ€ variable wit\n\nConditional: df[df.note &gt; 90]\n\nBy string length: df_filtered = data[data.col1.str.len() &gt; 4]\n\nMulti-conditional\ndf[(df.date_of_birth.dt.year &gt; 2000) &Â \nÂ  (df.profession.str.contains(\"engineer\"))]\n\ndf.query(\"Distance &lt; 2 & Rooms &gt; 2\")\n%in%: df[df.group.isin([\"A\",\"C\"])]\nSmallest 2 values of note column: df.nsmallest(2, \"note\")\n\nnlargest also available\n\nOnly rows in a column with NA values: df[df.profession.isna()]\nOnly rows in a column that arenâ€™t NA: df[df.profession.notna()]\nFind rows by index value: df.loc['Tony']\n\nloc is for the value, iloc is for position (numerical or logical)\n\nSee Select columns Example for iloc logical case\n\nWith groupby\ndata_grp = data.groupby(\"Company Name\")\ndata_grp.get_group(\"Amazon\")\n\nReturn 1st 3 rows (and 2 columns): df.loc[:3, [\"name\",\"note\"]]\nReturn 1st 3 rows and 3rd column: df.iloc[:3, 2]\n\nh 11th month\n\nUsing index to filter single cell or groups of cells\ndf.loc[\nÂ  Â  ['King of Wands', 'The Fool', 'The Devil'],\nÂ  Â  ['image', 'upright keywords']\n]\n\nindex values: â€˜King of Wandsâ€™, etc. (aka row names)\ncolumn names: â€˜imageâ€™, etc.\n\n\n\n\nMutate\n\nLooks like all these functions do the same damn thing\nassign e.g.Â Add columns\ndf[\"col3\"] = df[\"col1\"] + df[\"col2\"]\ndf = df.assign(col3 = df[\"col1\"] + df[\"col2\"])\napplyÂ  a function to column(s)\ndf[['colA', 'colB']] = df[['colA', 'colB']].apply(np.sqrt)\ndf[\"col3\"] = df.apply(my_func, axis=1)\n\ndocs: Series, DataFrame\naxis - arg for applying a function across rowwise or columnwise (default is 0, which is for columnwise)\nThe fact that mutated columns are assigned to the columns of the df are what keeps it from being like summarize\n\nIf np.sum were the function, then the output would be two numbers and this probably wouldnâ€™t work.\n\n\nmap\ndf['colE'] = df['colE'].map({'Hello': 'Good Bye', 'Hey': 'Bye'})\n\nEach case of â€œHelloâ€ is changed to â€œGood Byâ€, etc.\nIf you give it a dict, it acts like a case_when or plyr::mapvalues or maybe recode\nValues in the column but arenâ€™t included in the dict get NaNs\nThis also can take a lambda function\nDocs (only applies to Series, so I guess that means only 1 column at a time(?))\n\napplymap\ndf[['colA', 'colD']] = df[['colA', 'colD']].applymap(lambda x: x**2)\n\nDocs\nSays it applies a function elementwise, so its probably performing a loop\n\nTherefore better to avoid if a vectorized version of the function is available\n\n\n\n\n\nPivot\n\npivot_longer\n\nExample\nyear_list=list(df.iloc[:, 4:].columns)\ndf = pd.melt(df, id_vars=['Country Name','Series Name','Series Code','Country Code'], value_vars=year_list)\n\nyear_list has the variable names of the columns you want to merge into 1 column\n\n\npivot_wider\n\nExample\n# Step 1: add a count column to able to summarize when grouping\nlong_df['count'] = 1\n\n# Step 2: group by date and type and sum\ngrouped_long_df = long_df.groupby(['date', 'type']).sum().reset_index()\n\n# Step 3: build wide format from long format\nwide_df = grouped_long_df.pivot(index='date', columns='type', values='count').reset_index()\n\nâ€œlong_dfâ€ has two columns: type and date\n\n\n\n\n\nBind_Rows\ndf1 = pd.DataFrame({'strata': 1, 'y': np.random.normal(loc=10, scale=1, size=size){style='color: #990000'}[}]{style='color: #990000'})\ndf2 = pd.DataFrame({'strata': 2, 'y': np.random.normal(loc=15, scale=2, size=size){style='color: #990000'}[}]{style='color: #990000'})\ndf3 = pd.DataFrame({'strata': 3, 'y': np.random.normal(loc=20, scale=3, size=size){style='color: #990000'}[}]{style='color: #990000'})\ndf4 = pd.DataFrame({'strata': 4, 'y': np.random.normal(loc=25, scale=4, size=size){style='color: #990000'}[}]{style='color: #990000'})\ndf = pd.concat([df1, df2, df3, df4])\n\ndf_ls = [df1, df2, df3, df4]\ndf_all = pd.concat([df_ls[i] for i in range(8)], axis=0)\n\n2 variables are created, â€œstrataâ€ and â€œyâ€, then merged into a df\nMake sure no rows are duplicates\n\ndf_loans = pd.concat([df, df_pdf], verify_integrity=True)\n\n\nBind_Cols\npd.concat([top_df, more_df], axis=1)\n\n\nCount\n\nvalue.counts: df[\"col_name\"].value_counts()\n\noutput arranged in descending order of frequencies\nfaster than groupby + size\nargs\n\nnormalize=False\ndropna=True\n\n\ngroupby + count\ndf.groupby(\"Product_Category\").size()\ndf.groupby(\"Product_Category\").count()\n\nâ€œsizeâ€ includes null values\nâ€œcountâ€ doesnâ€™t include null values\narranged by the index column\n\n\n\n\nArrange\n\ndf.sort_values(by = \"col_name\")\n\nargs\n\nascending=True\nignore_index=False (True will reset the index)\n\n\n\n\n\nGroup_By\n\nFind number of groups\ndf_group = df.groupby(\"Product_Category\")\ndf_group.ngroups\ngroupby + count\ndf.groupby(\"Product_Category\").size()\ndf.groupby(\"Product_Category\").count()\n\nâ€œsizeâ€ includes null values\nâ€œcountâ€ doesnâ€™t include null values\n\nOnly groupby observed categories\ndf.groupby(\"col1\", observed=True)[\"col2\"].sum()\ncol1\nAÂ  Â  49\nBÂ  Â  43\nName: col2, dtype: int64\n\ncol1 is a category type and has 3 levels specified (A, B, C) but the column only has As and Bs\nobserved=False (default) would include C and a count of 0.\n\nCount NAs\ndf[\"col1\"] = df[\"col1\"].astype(\"string\")\ndf.groupby(\"col1\", dropna=False)[\"col2\"].sum()\n# output\ncol1\nAÂ  Â  Â  49.0\nBÂ  Â  Â  43.0\n&lt;NA&gt;Â  Â  30.0\nName: col2, dtype: float64\n\n** Will not work with categorical types **\n\nSo categorical variables, you must convert to strings to be able to group and count NAs\n\n\nCombo\ndf.groupby(\"col3\").agg({\"col1\":sum, \"col2\":max})\n\nÂ  Â  Â  col1Â  col2\ncol3Â  Â  Â  Â  Â  Â \nAÂ  Â  Â  Â  1Â  Â  2\nBÂ  Â  Â  Â  8Â  Â  10\nAggregate with only 1 function\ndf.groupby(\"Product_Category\")[[\"UnitPrice(USD)\",\"Quantity\"]].mean()\n\nSee summarize for aggregate by more than 1 function\n\nExtract a group category\ndf_group = df.groupby(\"Product_Category\")\ndf_group.get_group('Healthcare')\n# or\ndf[df[\"Product_Category\"]=='Home']\nGroup objects are iterable\ndf_group = df.groupby(\"Product_Category\")\nfor name_of_group, contents_of_group in df_group:\nÂ  Â  print(name_of_group)\nÂ  Â  print(contents_of_group)\nGet summary stats on each category conditional on a column\ndf.groupby(\"Product_Category\")[[\"Quantity\"]].describe()\n\n\n\nSummarize\n\nWith groupby and agg\n\ndf.groupby(\"cat_col_name\").agg(new_col_name = (\"num_col_name\", \"func_name\"))\n\nTo reset the indexÂ  (i.e.Â ungroup)\n\nuse .groupby arg, as_index=False\nuse .reset_index() at the end of the chain\n\narg: drop=TRUE â€¦does something (maybe drops grouping columns)\n\n\n\n\nGroup aggregate with more than 1 function\ndf.groupby(\"Product_Category\")[[\"Quantity\"]].agg([min,max,sum,'mean'])\n\nwhen you mention â€˜meanâ€™ (with quotes), .aggregate() searches for a function mean belonging to pd.Series i.e.Â pd.Series.mean().\nWhereas, if you mention mean (without quotes), .aggregate() will search for function named mean in default Python, which is unavailable and will throw an NameError exception.\n\nUse a different aggregate function on specific columns\nfunction_dictionary = {'OrderID':'count','Quantity':'mean'}\ndf.groupby(\"Product_Category\").aggregate(function_dictionary)\n\n\n\nJoin\n\nUsing merge: pd.merge(df1,df2)\n\nMore versatile than join\nAutomatically detects a common column\nMethod: â€œhow = â€˜innerâ€™â€ (i.e.Â default is inner join)\n\nâ€˜outerâ€™ , â€˜leftâ€™ , â€˜rightâ€™ are available\n\nOn columns with different names\n\non = â€œcol_aâ€\nleft_on = â€˜left df col nameâ€™\nright_on = â€˜right df col nameâ€™\n\ncopy = True (default)\n\nUsing join\ndf1.set_index('Course', inplace=True)\ndf2.set_index('Course', inplace=True)\ndf3 = df1.join(df2, on = 'Course', how = 'left')\n\ninstance method that joins on the indexes of the dataframes\nThe column that we match on for the left dataframe doesnâ€™t have to be its index. But for the right dataframe, the join key must be its index\nCan use multiple columns as the index by passing a list, e.g.Â [â€œCourseâ€, â€œStudent_IDâ€]\n* indexes do NOT have to be unique *\nFaster than merge\n\n\n\n\nDistinct\n\nFind number of unique values: data.Country.nunique()\nDisplay unique values: df[\"col3\"].unique()\nCreate a boolean column to indicated duplicated rows: df.duplicated(keep=False)\nCheck for duplicate ids: df_loans[df_loans.duplicated(keep=False)].sort_index()\nCount duplicated ids: df_check.index.duplicated().sum()\nDrop duplicated rows: df.drop_duplicates()\n\n\n\nReplace values\n\nIn the whole df\n\n1 value\ndf.replace(to_replace = '?', value = np.nan, inplace=True)\n\nreplaces all values == ? with NaN\nfaster than loc method\n\nMultiple values\ndf.replace([\"Male\", \"Female\"], [\"M\", \"F\"], inplace=True) # list\ndf.replace({\"United States\": \"USA\", \"US\": \"USA\"}, inplace=True) # dict\n\nOnly in specific columns\ndf.replace(\nÂ  Â  {\nÂ  Â  Â  Â  \"education\": {\"HS-grad\": \"High school\", \"Some-college\": \"College\"},\nÂ  Â  Â  Â  \"income\": {\"&lt;=50K\": 0, \"&gt;50K\": 1},\nÂ  Â  },\nÂ  Â  inplace=True,\n)\n\nreplacement only occurs in â€œeducationâ€ and â€œincomeâ€ columns\n\nUsing Indexes\n\nExample (2 rows, 1 col): df.loc[['Four of Pentacles', 'Five of Pentacles'], 'suit'] = 'Pentacles'\n\nindex values: â€œFour of Pentaclesâ€, â€œFive of Pentaclesâ€\ncolumn name: â€œsuitâ€\nReplaces the values in those cells with the value â€œPentaclesâ€\n\nExample (1 row, 2 cols):\ndf.loc['King of Wands', ['suit', 'reversed_keywords']] = [\nÂ  Â  'Wands', 'impulsiveness, haste, ruthlessness'\n]\n\nindex value: â€œKing of Wandsâ€\ncolumn names: â€œsuitâ€, :reversed_keywordsâ€\nIn â€œsuit,â€ replaces value with â€œWandsâ€\nIn â€œreversed words,â€ replaces value with â€œimpulsiveness, haste, ruthlessnessâ€\n\n\nReplace Na/NaNs in a column with a constant value\ndf['col_name'].fillna(value = 0.85, inplace = True)\nReplaces Na/NaNs in a column with the value of a function/method\ndf['price'].fillna(value = df.price.median(), inplace = True)\nReplaces Na/NaNs in a column with a group value (e.g.Â group by fruit, then use price median)\n# median\ndf['price'].fillna(df.groupby('fruit')['price'].transform('median'), inplace = True)\nForward-Fill\ndf['price'].fillna(method = 'ffill', inplace = True)\ndf['price'] = df.groupby('fruit')['price'].ffill(limit = 1)\ndf['price'] = df.groupby('fruit')['price'].ffill()\n\nforward-fill: fills Na/NaN with previous non-Na/non-NaN value\nforward-fill, limited to 1: only fills with the previous value if thereâ€™s a non-Na/non-NaN 1 spot behind it.\n\nMay leave NAs/NaNs\n\nforward-fill by group\n\nBackward-Fill\ndf['price'].fillna(method = 'bfill', inplace = True)\n\nbackward-fill: fills Na/NaN with next non-Na/non-NaN value\n\nAlternating between backward-fill and forward-fill\ndf['price'] = df.groupby('fruit')['price'].ffill().bfill()\n\nalternating methods: for the 1st group a forward fill is performed; for the next group, a backward fill is performed; etc.\n\nInterpolation\n\ndf['price'].interpolate(method = 'linear', inplace = True)\ndf['price'] = df.groupby('fruit')['price'].apply(lambda x: x.interpolate(method='linear')) # by group\ndf['price'] = df.groupby('fruit')['price'].apply(lambda x: x.interpolate(method='linear')).bfill() # by group with backwards-fill\n\nInterpolation (e.g.Â linear)\n\nMay leave NAs/NaNs\n\nInterpolation by group\nInterpolation + backwards-fill\n\nApply a conditional: says fill na with mean_price where â€œweekdayâ€ column is TRUE; if FALSE, fill with mean_price*1.25\nmean_price = df.groupby('fruit')['price'].transform('mean')\ndf['price'].fillna((mean_price).where(cond = df.weekday, other = mean_price*1.25), inplace = True)\n\n\n\nStrings\n\nMisc\n\nregex is slow\nStored as â€œobjectâ€ type but â€œStringDtypeâ€ is available. This new Dtype is optional for now but it may be required to do so in the future\nRegex with Examples in python and pandas\n\nFilter (see Filter section)\nReplace pattern using regex\ndf['colB'] = df['colB'].str.replace(r'\\D', '')\ndf['colB'] = df['colB'].replace(r'\\D', r'', regex=True)\n\nâ€œrâ€ indicates youâ€™re using regex\nreplaces all non-numeric patterns (e.g.Â letters, symbols) with an empty string\nReplace pattern with list comprehension (more efficient than loops)\n\nWith {{re}}\nimport re\ndf['colB'] = [re.sub('[^0-9]', '', x) for x in df['colB']]\n\nre is the regular expressions library\nReplaces everything not a number with empty string (carrot inside is a negation)\n\nSplit w/list output\n&gt;&gt; df[\"group\"].str.split(\"-\")\n0Â  Â  [A, 1B]\n1Â  Â  [B, 1B]\n2Â  Â  [A, 1C]\n3Â  Â  [A, 1B]\n4Â  Â  [C, 1C]\n\nâ€œ-â€ is the delimiter. A-1B â€“&gt; [A, 1B]\n\nSplit into separate columns\ndf[\"group1\"] = df[\"group\"].str.split(\"-\", expand=True)[0]\ndf[\"group2\"] = df[\"group\"].str.split(\"-\", expand=True)[1]\n\nexpand = True splits into separate columns\nBUT you have to manually create the new columns in the old df by assigning each column to a new column name in the old df.\n\nConcatenate string columns\ndf['all'] = [p1 + ' ' + p2 + ' ' + p3 + ' ' + p4 for p1, p2, p3, p4 in zip(df['a'], df['b'], df['c'], df['d'])]\n\nList comprehension is fastest\nâ€œ+â€ operator also fast\nwith a space, â€ â€œ, as the delimiter\n\ndf['all'] = df['a'] + ' ' + df['b'] + ' ' + df['c'] + ' ' + df['d']\n\nHave read that this is most efficient for larger datasets\ndf['colE'] = df.colB.str.cat(df.colD) can be used for relatively small datasets (up to 100â€“150 rows)\n\narr1 = df['owner'].array\narr2 = df['gender'].array\narr3 = []\nfor i in range(len(arr1)):\n    if arr2[i] == 'M':\n        arr3.append('Mr. ' + arr1[i])\n    else:\n        arr3.append('Ms. ' + arr1[i])\ndf['name5'] = arr3\n\nVectorizes columns then concantenates with a +\nFor-loop w/vectorization was faster than apply + list comprehension or for-loop + itertuples or for-loop + iterrows\n\nConcatenate string and non-string columns\ndf['colE'] = df['colB'].astype(str) + '-' + df['colD']\nExtract pattern with list comprehension\nimport re\ndf['colB'] = [re.search('[0-9]', x)[0] for x in df['colB']]\n\nre is the regular expressions library\nextracts all numeric characters\n\nExtract all instances of pattern into a list\nresults_ls = re.findall(r'\\d+', s)\n\nFinds each number in string, â€œs,â€ and outputs into a list\nCan also use re.finditer\nfyi re.search only returns the first match\n\nExtract pattern using regex\ndf['colB'] = df['colB'].str.extract(r'(\\d+)', expand=False).astype(\"int\")\n\nâ€œrâ€ indicates youâ€™re using regex\nextracts all numeric patterns and changes type to integer\n\nRemove pattern by map loop\nfrom string import ascii_letters\ndf['colB'] = df['colB'].map(lambda x: x.lstrip('+-').rstrip(ascii_letters))\n\npluses or minuses are removed from if they are the leading character\nlstrip removes leading characters that match characters provided\nrstrip removes trailing characters that match characters provided\n\nDoes a string contain one or more digits\nany(c.isdigit() for c in s)\n\n\n\nConversions\n\nConvert dict to pandas df (**slow for large lists**)\ndf = pd.DataFrame.from_dict(acct_dict, orient=\"index\", columns=[\"metrics\"])\n\nresult_df = DataFrame.from_dict(search.cv_results_, orient='columns')Â \nprint(result_df.columns)\n\nkey of the dict is used as the index and value is column named â€œmetricsâ€\n\nConvert pandas df to ndarray\nndarray = df.to_numpy()\n# using numpy\nndarray = np.asarray(df)\nConvert df to list or dict\ndf\nÂ  ColA ColBÂ  ColC\n0Â  Â  1Â  Â  AÂ  Â  4\n1Â  Â  2Â  Â  BÂ  Â  5\n2Â  Â  3Â  Â  CÂ  Â  6\n\nresult = df.values.tolist()\n[[1, 'A', 4], [2, 'B', 5], [3, 'C', 6]]\n\ndf.to_dict()\n{'ColA': {0: 1, 1: 2, 2: 3},\n'ColB': {0: 'A', 1: 'B', 2: 'C'},\n'ColC': {0: 4, 1: 5, 2: 6}}\n\ndf.to_dict(\"list\")\n{'ColA': [1, 2, 3], 'ColB': ['A', 'B', 'C'], 'ColC': [4, 5, 6]}\n\ndf.to_dict(\"split\")\n{'index': [0, 1, 2],\n'columns': ['ColA', 'ColB', 'ColC'],\n'data': [[1, 'A', 4], [2, 'B', 5], [3, 'C', 6]]}\nConvert string variable to datetime\ndf.date_of_birth = df.date_of_birth.astype(\"datetime64[ns]\")\ndf['Year'] = pd.to_datetime(df['Year'])\nConvert all â€œobjectâ€ type columns to â€œcategoryâ€\nfor col in X.select_dtypes(include=['object']):\nÂ  X[col] = X[col].astype('category')\nConvert column to numeric\ndf['GDP'] = df['GDP'].astype(float)\n\n\n\nSample and Simulate\n\nSimulate\n\nRandom normal variable (and group variable named strata)\ndf1 = pd.DataFrame({'strata': 1, 'y': np.random.normal(loc=10, scale=1, size=size))\n\n2 columns â€œstrataâ€ (all 1s) and â€œyâ€\nloc = mean, scale = sd, size = number of observations to create\n\n\nSample\n\nRows\ndf.sample(n = 15, replace = True, random_state=2) # with replacement\n\nsample_df = df.sample(int(len(tps_df) * 0.2)) # sample 20% of the data\n\nThis method is faster than sampling using random indices with NumPy\n\nRows and columns\ntps.sample(5, axis=1).sample(7, axis=0)\n\n5 columns and 7 rows\n\n\n\n\n\nChaining\n\nNeed to encapsulate code in parentheses\n# to assign to an object\nnew_df = (\nÂ  Â  melb\nÂ  Â  .query(\"Distance &lt; 2 & Rooms &gt; 2\") # query equals filter in Pandas\nÂ  Â  .filter([\"Type\", \"Price\"]) # filter equals select in Pandas\nÂ  Â  .groupby(\"Type\")\nÂ  Â  .agg([\"mean\", \"count\"]) # calcs average price and row count for each Type; creates subcolumns mean and count under Price\nÂ  Â  .reset_index() # converts matrix to df\nÂ  Â  .set_axis([\"Type\", \"averagePrice\", \"numberOfHouses\"], # renames Price to averagePrice and count to numberOfHouses\nÂ  Â  Â  Â  Â  Â  Â  axis = 1,\nÂ  Â  Â  Â  Â  Â  Â  inplace = False)\nÂ  Â  .assign(averagePriceRounded = lambda x: x[\"averagePrice\"] # assign equals mutate in Pandas (?)\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  .round(1))\nÂ  Â  .sort_values(by = [\"numberOfHouses\"],\nÂ  Â  Â  Â  Â  Â  Â  Â  ascending = False)\n)\n\nif agg[\"mean\"] then there wouldnâ€™t be a subcolumn mean, just the values of Price would be the mean\n\nUsing pipe\n\nargs\n\nfunc: Function to apply to the Series/DataFrame\nargs: Positional arguments passed to func\nkwargs: Keyword arguments passed to func\n\nReturns: object, the return type of func\nSyntax\ndef f1(df, arg1):\n# do something return # a dataframe\n\ndef f2(df, arg2):\n# do something return # a dataframe\n\ndef f3(df, arg3):\n# do something return # a dataframe\n\ndf = pd.DataFrame(..) # some dataframe\n\ndf.pipe(f3, arg3 = arg3).pipe(f2, arg2 = arg2).pipe(f1, arg1 = arg1)\n\nfunction 3 (f3) is executed then function 2 then function 1\n\n\n\n\n\nCrosstab\n\nExample\ndf = pd.DataFrame([[\"A\", \"X\"],Â \nÂ  Â  Â  Â  Â  Â  Â  Â  Â  [\"B\", \"Y\"],Â \nÂ  Â  Â  Â  Â  Â  Â  Â  Â  [\"C\", \"X\"],\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  [\"A\", \"X\"]],Â \nÂ  Â  Â  Â  Â  Â  Â  Â  Â  columns = [\"col1\", \"col2\"])\nprint(pd.crosstab(df.col1, df.col2))\ncol2Â  XÂ  Y\ncol1Â  Â  Â \nAÂ  Â  2Â  0\nBÂ  Â  0Â  1\nCÂ  Â  1Â  0\n\n\n\nPivot Table\n\nExample\nprint(df)\nÂ  Â  NameÂ  SubjectÂ  Marks\n0Â  JohnÂ  Â  MathsÂ  Â  Â  6\n1Â  MarkÂ  Â  MathsÂ  Â  Â  5\n2Â  PeterÂ  Â  MathsÂ  Â  Â  3\n3Â  JohnÂ  ScienceÂ  Â  Â  5\n4Â  MarkÂ  ScienceÂ  Â  Â  8\n5Â  PeterÂ  ScienceÂ  Â  10\n6Â  JohnÂ  EnglishÂ  Â  10\n7Â  MarkÂ  EnglishÂ  Â  Â  6\n8Â  PeterÂ  EnglishÂ  Â  Â  4\n\npd.pivot_table(df,Â \nÂ  Â  Â  Â  Â  Â  Â  index = [\"Name\"],\nÂ  Â  Â  Â  Â  Â  Â  columns=[\"Subject\"],Â \nÂ  Â  Â  Â  Â  Â  Â  values='Marks',\nÂ  Â  Â  Â  Â  Â  Â  fill_value=0)\nSubjectÂ  EnglishÂ  MathsÂ  Science\nNameÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â \nJohnÂ  Â  Â  Â  Â  10Â  Â  Â  6Â  Â  Â  Â  5\nMarkÂ  Â  Â  Â  Â  6Â  Â  Â  5Â  Â  Â  Â  8\nPeterÂ  Â  Â  Â  Â  4Â  Â  Â  3Â  Â  Â  10\nExample: drop lowest score for each letter grade, then calculate the average score for each letter grade\ngrades_df.pivot_table(index='name',\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  columns='letter grade',\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  values='score',\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  aggfunc = lambda series : (sorted(list(series))[-1] + sorted(list(series))[-2]) / 2)\n\nletter gradeÂ  Â  AÂ  Â  B\nname\nArifÂ  Â  Â  Â  Â  96.5Â  87.0\nKaylaÂ  Â  Â  Â  95.5Â  84.0\n\ngrades_df\n\n2 names (â€œnameâ€)\n6 scores (â€œscoreâ€)\nOnly 2 letter grades associated with these scores (â€œletter gradeâ€)\n\nindex: each row will be a â€œnameâ€\ncolumns: each column will be a â€œletter gradeâ€\nvalues: value in the cells will be from the â€œscoreâ€ column according to each combination columns in the index and columns args\naggfunc: uses a lambda to compute the aggregated values\n\nâ€œseriesâ€ is used a the variableÂ  in the lambda function\nsorts series (ascending), takes the top two values (using negative list indexing), and averages them\n\nIterate over a df\n\nBetter to use a vectorized solution if possible\n\n\n\n\n\nIteration\n\niterrows\ndef salary_iterrows(df):\nÂ  Â  salary_sum = 0\n\nÂ  Â  for index, row in df.iterrows():\nÂ  Â  Â  Â  salary_sum += row['Employee Salary']\n\nÂ  Â  return salary_sum/df.shape[0]\n\nsalary_iterrows(data)\niteruples\ndef salary_itertuples(df):\nÂ  Â  salary_sum = 0\n\nÂ  Â  for row in df.itertuples():Â \nÂ  Â  Â  Â  salary_sum += row._4\n\nÂ  Â  return salary_sum/df.shape[0]\n\nsalary_itertuples(data)\n\nFaster than iterrows"
  },
  {
    "objectID": "qmd/python-pandas.html#sec-py-pandas-ts",
    "href": "qmd/python-pandas.html#sec-py-pandas-ts",
    "title": "Pandas",
    "section": "Time Series",
    "text": "Time Series\n\nMisc\n\nAlso see\n\nA Collection of Must-Know Techniques for Working with Time Series Data in Python\n\nCollection of preprocessing recipes\n\n{{datetime}} in bkmks\n\n\n\n\nOperations\n\nLoad and set index frequency\n\nExample\n#Load the PCE and UMCSENT datasets\ndf = pd.read_csv(\nÂ  Â  filepath_or_buffer='UMCSENT_PCE.csv',\nÂ  Â  header=0,\nÂ  Â  index_col=0,\nÂ  Â  infer_datetime_format=True,\nÂ  Â  parse_dates=['DATE']\n)\n#Set the index frequency to 'Month-Start'\ndf = df.asfreq('MS')\n\nâ€œheader=0â€ is default, says 1st row of file is the column names\nâ€œindex_col=0â€ says use the first column as the df index\nâ€œinfer_datetime_format=Trueâ€ says infer the format of the datetime strings in the columns\nâ€œparse_dates=[â€˜DATEâ€™]â€ says convert â€œDATEâ€ to datetime and format\n\n\nCreate date variable\n\nExample: w/date_range\n# DataFrame\ndate_range = pd.date_range('1/2/2022', periods=24, freq='H')\nsales = np.random.randint(100, 400, size=24)\nsales_data = pd.DataFrame(\nÂ  Â  sales,\nÂ  Â  index = date_range,\nÂ  Â  columns = ['Sales']\n)\n# Series\nrng = pd.date_range(\"1/1/2012\", periods=100, freq=\"S\")\nts = pd.Series(np.random.randint(0, 500, len(rng)), index=rng)\nMethods (article)\n\npandas.date_range â€” Return a fixed frequency DatetimeIndex.\n\nstart: the start date of the date range generated\nend: the end date of the date range generated\nperiods: the number of dates generated\nfreq: default to â€œDâ€ (daily), the interval between dates generated, it could be hourly, monthly or yearly\n\npandas.bdate_range â€” Return a fixed frequency DatetimeIndex, with the business day as the default frequency.\npandas.period_range â€” Return a fixed frequency PeriodIndex. The day (calendar) is the default frequency.\npandas.timedelta_range â€” Return a fixed frequency TimedeltaIndex, with the day as the default frequency.\n\n\nCoerce to datetime\n\nSource has month first or day first\n# month first\n# e.g 9/16/2015 --&gt; 2015-09-16\ndf['joining_date'] = pd.to_datetime(df['joining_date'])\n\n# day first\n# e.g 16/9/2015 --&gt; 2015-09-16\ndf['joining_date'] = pd.to_datetime(df['joining_date'], dayfirst=True)\n\ndefault is month first\n* If the first digit is a number that can NOT be a month (e.g.Â 25), then it will parse it as a day instead. *\n\nFormat conditional on sourceâ€™s delimiter\ndf['joining_date_clean'] = np.where(df['joining_date'].str.contains('/'),\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  pd.to_datetime(df['joining_date']),\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  pd.to_datetime(df['joining_date'], dayfirst=True)\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  )\n\nLike an ifelse. Source dates that have â€œ/â€ separating values are parsed as month-first and everything else as day-first\n\n\nTransform partial date columns\n\nExample: String 09/2007Â will be transformed to dateÂ 2007-09-01\nimport datetime\n\ndef parse_first_brewed(text: str) -&gt; datetime.date:\n    parts = text.split('/')\n    if len(parts) == 2:\n        return datetime.date(int(parts[1]), int(parts[0]), 1)\n    elif len(parts) == 1:\n        return datetime.date(int(parts[0]), 1, 1)\n    else:\n        assert False, 'Unknown date format'\n\n&gt;&gt;&gt; parse_first_brewed('09/2007')\ndatetime.date(2007, 9, 1)\n\n&gt;&gt;&gt; parse_first_brewed('2006')\ndatetime.date(2006, 1, 1)\n\nExtract time components\n\nCreate â€œyear-monthâ€ column: df[\"year_month\"] = df[\"created_at\"].dt.to_period(\"M\")\n\nâ€œMâ€ is the â€œoffset aliasâ€ string for month\n(Docs)\n\n\nFilter a range\ngallipoli_data.loc[\nÂ  Â  Â  Â  (gallipoli_data.DateTime &gt;= '2008-01-02')Â \nÂ  Â  Â  Â  &Â \nÂ  Â  Â  Â  (gallipoli_data.DateTime &lt;= '2008-01-03')\nÂ  Â  ]\nFill in gaps\n\nExample: hacky way to do it\npd.DataFrame(\nÂ  Â  sales_data.Sales.resample('h').mean()\n)\n\nfrequency is already hourly (â€˜hâ€™), so taking the mean doesnâ€™t change the values. But NaNs will be added for datetime values that donâ€™t exist.\n.fillna(0) can be added to .mean() if you want to fill the NaNs with something meaningful (e.g.Â 0)\n\nExplode interval between 2 date columns into a column with all the dates in that interval\n\nExample: 1 row\npd.date_range(calendar[\"checkin_date\"][0], calendar[\"checkout_date\"][0])\n# output\nDatetimeIndex(['2022-06-01', '2022-06-02', '2022-06-03',Â \nÂ  Â  Â  Â  Â  Â  Â  '2022-06-04', '2022-06-05', '2022-06-06',Â \nÂ  Â  Â  Â  Â  Â  Â  '2022-06-07'],\nÂ  Â  Â  Â  Â  Â  Â  dtype='datetime64[ns]', freq='D')\nExample: all rows\n\ncalendar.loc[:, \"booked_days\"] = calendar.apply(\n\nÂ  Â  lambda x: list(\nÂ  Â  Â  Â  pd.date_range(\nÂ  Â  Â  Â  Â  Â  x.checkin_date,Â \nÂ  Â  Â  Â  Â  Â  x.checkout_date + pd.DateOffset(days=1)\nÂ  Â  Â  Â  ).date\nÂ  Â  ),\nÂ  Â  axis = 1\n\n)\nExample: pivot_longer\n\n# explodeÂ \ncalendar = calendar.explode(\nÂ  Â  column=\"booked_days\", ignore_index=True\n)[[\"property\",\"booked_days\"]]\n# display the first 5 rows\ncalendar.head()\n\n\n\n\n\nCalculations\n\nGet the min/max dates of a dataset: print(df.Date.agg(['min', 'max'])) (â€œDateâ€ is the date variable)\nFind difference between to date columns\ndf[\"days_to_checkin\"] = (df[\"checkin_date\"] - df[\"created_at\"]).dt.days\n\nnumber of days between the check-in date and the date booking was created (i.e.Â number of days until the customer arrives)\n\nAdd 1 day to a subset of observations\ndf.loc[df[\"booking_id\"]==1001, \"checkout_date\"] = df.loc[df[\"booking_id\"]==1001, \"checkout_date\"] + pd.DateOffset(days=1)\n\nadds 1 day to the checkout date of the booking with id 1001\n\nAggregation\n\nMisc\n\nresample (docs) requires a datetime type column set as the index for the dataframe: df.index = df[â€˜DateTimeâ€™]\n\nbtw this function doesnâ€™t resample in the bootstrapping sense of the word. Just a function that allows you to do window calculations on time series\n\nCommon time strings (docs)\n\ns for seconds\nt for minutes\nh for hours\nw for weeks\nm for months\nq for quarter\n\n\nRolling-Window\n\nExample: 30-day rolling average\nwindow_mavg_short=30\nstock_df['mav_short'] = stock_df['Close'] \\\nÂ  Â  .rolling(window=window_mavg_short) \\\nÂ  Â  .mean()\n\nStep-Window\n\nExample: Mean temperature every 3 hours\n\ngallipoli_data.Temperature.resample(rule = â€˜3hâ€™).mean()\n\nâ€œ1.766667â€ is the average from 03:00:00 to 05:00:00\nâ€œ4.600000â€ is the average from 06:00:00 to 08:00:00\nâ€œhâ€ is the time string from hours\nBy default the calculation window starts on the left index (see next Example for right index) and doesnâ€™t include the right index\n\ne.g.Â index, â€œ09:00:00â€, calculation window includes â€œ09:00:00â€, â€œ10:00:00â€, and â€œ11:00:00â€\n\n\nExample: Max sunshine every 3 hrs\n\ngallipoli_data['Sunshine Duration'].resample(rule = '3h', closed= 'right').max()\n\nâ€œclosed=â€˜rightâ€™â€ says include the right index in the calculation but not the left\n\ne.g.Â index, â€œ09:00:00â€, calculation window includes â€œ10:00:00â€, â€œ11:00:00â€, and â€œ12:00:00â€\n\n\n\n\n\n\n\nSimulation\n\nExample: simulation, gaussian\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nnp.random.seed(987)\ntime_series = [np.random.normal()*0.1, np.random.normal()*0.1]\nsigs = [0.1, 0.1]\nfor t in range(2000):\nÂ  Â  sig_t = np.sqrt(0.1 + 0.24*time_series[-1]**2 + 0.24*time_series[-2]**2 + 0.24*sigs[-1]**2 + 0.24*sigs[-2]**2)\nÂ  Â  y_t = np.random.normal() * sig_t\nÂ  Â  time_series.append(y_t)\nÂ  Â  sigs.append(sig_t)\n\ny = np.array(time_series[2:])\nplt.figure(figsize = (16,8))\nplt.plot(y, label = \"Simulated Time-Series\")\nplt.grid(alpha = 0.5)\nplt.legend(fontsize = 18)\nStandard GARCH time-series thatâ€™s frequently encountered in econometrics\n\nExample: simulation, beta\n\nfrom scipy.stats import beta\nnp.random.seed(321)\ntime_series = [beta(0.5,10).rvs()]\nfor t in range(2000):\nÂ  Â  alpha_t = 0.5 + time_series[-1] * 0.025 * t\nÂ  Â  beta_t = alpha_t * 20\nÂ  Â  y_t = beta(alpha_t, beta_t).rvs()\nÂ  Â  time_series.append(y_t)\n\ny = np.array(time_series[1:])\nplt.figure(figsize = (16,8))\nplt.plot(y, label = \"Simulated Time-Series\")\nplt.grid(alpha = 0.5)\nplt.legend(fontsize = 18)"
  },
  {
    "objectID": "qmd/python-pandas.html#sec-py-pandas-opt",
    "href": "qmd/python-pandas.html#sec-py-pandas-opt",
    "title": "Pandas",
    "section": "Optimization",
    "text": "Optimization\n\nPerformance\n\nPandas will typically outperform numpy ndarrays in cases that involve significantly larger volume of data (say &gt;500K rows)\nBad performance by iteratively creating rows in a dataframe\n\nBetter to iteratively append lists then coerce to a dataframe at the end\nUse itertuples instead of iterrows in loops\n\nIterates through the data frame by converting each row of data as a list of tuples. Makes comparatively less number of function calls and hence carry less overhead.\ntqdm::tqdm is a progress bar for loops\n\n\nLibraries\n\n{{pandarallel}} - A simple and efficient tool to parallelize Pandas operations on all available CPUs.\n\narticle, article\n\n{{parallel_pandas}} -Â  A simple and efficient tool to parallelize Pandas operations on all available CPUs.\n\narticle\n\n{{modin}} - multi-processing package with identical APIs to Pandas, to speed up the Pandas workflow by changing 1 line of code. Modin offers accelerated performance for about 90+% of Pandas API. Modin uses Ray and Dask under the hood for distributed computing.\n\narticle\n\n{{numba}} - JIT compiler that translates a subset of Python and NumPy code into fast machine code.\n\nworks best with functions that involve many native Python loops, a lot of math, and even better, NumPy functions and arrays\nExample\n@numba.jit\ndef crazy_function(col1, col2, col3):\nÂ  Â  return (col1 ** 3 + col2 ** 2 + col3 * 10) ** 0.5\n\nmassive_df[\"f1001\"] = crazy_function(\nÂ  Â  massive_df[\"f1\"].values, massive_df[\"f56\"].values, massive_df[\"f44\"].values\n)\nWall time: 201 ms\n\n9GB dataset\nJIT stands for just in time, and it translates pure Python and NumPy code to native machine instructions\n\n\n\nUse numpy arrays\ndat['col1001'] = some_function(\nÂ  Â  Â  Â  Â  dat['col1'].values, dat['col2'].values, dat['col3'].values\nÂ  Â  Â  Â  )\n\nAdding the .values to the column vectors coerces to ndarrays\nNumPy arrays are faster because they donâ€™t perform additional calls for indexing, data type checking like Pandas Series\n\neval for non-mathematical operations (e.g.Â boolean indexing, comparisons, etc.)\n\nDocs\nExample\nmassive_df.eval(\"col1001 = (col1 ** 3 + col2 ** 2 + col3 * 10) ** 0.5\", inplace=True)\n\nUsed a mathematical operation for his example for some reason\n\n\niloc vs loc\n\niloc faster for filtering rows: dat.iloc[range(10000)]\nloc faster for selecting columns: dat.loc[:, [\"f1\", \"f2\", \"f3\"]]\nnoticeable as data size increases\n\n\n\n\nMemory Optimization\n\nSee memory size of an object\ndata = pd.read_csv(\"dummy_dataset.csv\")\ndata.info(memory_usage = \"deep\")\ndata.memory_usage(deep=True)Â  / 1024 ** 2 # displays col sizes in MBs\n\n# or for just 1 variable\ndata.Country.memory_usage()\n\n# a few columns\nmemory_usage = df.memory_usage(deep=True) / 1024 ** 2 # displays col sizes in MBs\nmemory_usage.head(7)\nmemory_usage.sum() # total\nUse inplace transformation (i.e.Â donâ€™t create new copies of the df) to reduce memory load\ndf.fillna(0, inplace = True)\n# instead of\ndf_copy = df.fillna(0)\nLoad only the columns you need from a file\ncol_list = [\"Employee_ID\", \"First_Name\", \"Salary\", \"Rating\", \"Company\"]\ndata = pd.read_csv(\"dummy_dataset.csv\", usecols=col_list)\nVariable datatypes\n\n\nConvert variables to smaller types when possible\n\nVariables always receive largest memory types\n\nPandas will always assign int64 as the datatype of the integer-valued column, irrespective of the range of current values in the column.\n\n\nByte ranges (same bit options for floats)\n\n\nuint refers to unsigned, only positive integers\nint8: 8-bit-integer that covers integers from [-2â·, 2â·].\nint16: 16-bit-integer that covers integers from [-2Â¹âµ, 2Â¹âµ].\nint32: 32-bit-integer that covers integers from [-2Â³Â¹, 2Â³Â¹].\nint64: 64-bit-integer that covers integers from [-2â¶Â³, 2â¶Â³].\n\nConvert integer column to smaller type: data[\"Employee_ID\"] = data.Employee_ID.astype(np.int32)\nConvert all â€œobjectâ€ type columns to â€œcategoryâ€\nfor col in X.select_dtypes(include=['object']):\nÂ  X[col] = X[col].astype('category')\n\nobject datatype consumes the most memory. Either use str or category if there are few unique values in the feature\npd.Categorical data type can speed things up to 10 times while using LightGBMâ€™s default categorical handler\n\nFor datetime or timedelta, use the native formats offered in pandas since they enable special manipulation functions\nFunction for checking and converting all numerical columns in dataframes to optimal types\ndef reduce_memory_usage(df, verbose=True):\nÂ  Â  numerics = [\"int8\", \"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\nÂ  Â  start_mem = df.memory_usage().sum() / 1024 ** 2\nÂ  Â  for col in df.columns:\nÂ  Â  Â  Â  col_type = df[col].dtypes\nÂ  Â  Â  Â  if col_type in numerics:\nÂ  Â  Â  Â  Â  Â  c_min = df[col].min()\nÂ  Â  Â  Â  Â  Â  c_max = df[col].max()\nÂ  Â  Â  Â  Â  Â  if str(col_type)[:3] == \"int\":\nÂ  Â  Â  Â  Â  Â  Â  Â  if c_min &gt; np.iinfo(np.int8).min and c_max &lt; np.iinfo(np.int8).max:\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  df[col] = df[col].astype(np.int8)\nÂ  Â  Â  Â  Â  Â  Â  Â  elif c_min &gt; np.iinfo(np.int16).min and c_max &lt; np.iinfo(np.int16).max:\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  df[col] = df[col].astype(np.int16)\nÂ  Â  Â  Â  Â  Â  Â  Â  elif c_min &gt; np.iinfo(np.int32).min and c_max &lt; np.iinfo(np.int32).max:\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  df[col] = df[col].astype(np.int32)\nÂ  Â  Â  Â  Â  Â  Â  Â  elif c_min &gt; np.iinfo(np.int64).min and c_max &lt; np.iinfo(np.int64).max:\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  df[col] = df[col].astype(np.int64)\nÂ  Â  Â  Â  Â  Â  else:\nÂ  Â  Â  Â  Â  Â  Â  Â  if (\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  c_min &gt; np.finfo(np.float16).min\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  and c_max &lt; np.finfo(np.float16).max\nÂ  Â  Â  Â  Â  Â  Â  Â  ):\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  df[col] = df[col].astype(np.float16)\nÂ  Â  Â  Â  Â  Â  Â  Â  elif (\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  c_min &gt; np.finfo(np.float32).min\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  and c_max &lt; np.finfo(np.float32).max\nÂ  Â  Â  Â  Â  Â  Â  Â  ):\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  df[col] = df[col].astype(np.float32)\nÂ  Â  Â  Â  Â  Â  Â  Â  else:\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  df[col] = df[col].astype(np.float64)\nÂ  Â  end_mem = df.memory_usage().sum() / 1024 ** 2\nÂ  Â  if verbose:\nÂ  Â  Â  Â  print(\nÂ  Â  Â  Â  Â  Â  \"Mem. usage decreased to {:.2f} Mb ({:.1f}% reduction)\".format(\nÂ  Â  Â  Â  Â  Â  Â  Â  end_mem, 100 * (start_mem - end_mem) / start_mem\nÂ  Â  Â  Â  Â  Â  )\nÂ  Â  Â  Â  )\nÂ  Â  return df\n\nBased on the minimum and maximum value of a numeric column and the above table, the function converts it to the smallest subtype possible\n\nCheck memory usage before and after conversion\nprint(\"Memory usage before changing the datatype:\", data.Country.memory_usage())\ndata[\"Country\"] = data.Country.astype(\"category\")\nprint(\"Memory usage after changing the datatype:\", data.Country.memory_usage())\nUse sparse types for variables with NaNs\n\n\nExample: data[\"Rating\"] = data.Rating.astype(\"Sparse[float32]\")\n\nSpecify datatype when loading data\ncol_list = [\"Employee_ID\", \"First_Name\", \"Salary\", \"Rating\", \"Country_Code\"]\ndata = pd.read_csv(\"dummy_dataset.csv\", usecols=col_list,Â \nÂ  Â  Â  Â  Â  Â  Â  Â  Â  dtype = {\"Employee_ID\":np.int32, \"Country_Code\":\"category\"})"
  },
  {
    "objectID": "qmd/python-reticulate.html#sec-py-retic-misc",
    "href": "qmd/python-reticulate.html#sec-py-retic-misc",
    "title": "reticulate",
    "section": "Misc",
    "text": "Misc\n\nBefore interactively running python in RStudio, start REPL\nreticulate::repl_python()"
  },
  {
    "objectID": "qmd/python-reticulate.html#sec-py-retic-rmark",
    "href": "qmd/python-reticulate.html#sec-py-retic-rmark",
    "title": "reticulate",
    "section": "RMarkdown",
    "text": "RMarkdown\n\nAlso see Quarto &gt;&gt; R and Python\nBasic set-up\n---\ntitle: \"R Notebook\"\noutput: html_notebook\n---\n\n\n```{r}\nknitr::opts_chunk$set(\nÂ  echo = TRUE,\nÂ  message = FALSE,\nÂ  warning = FALSE\n)\n```\n```{r}\nlibrary(reticulate) \n```\n```{python}\nimport pandas as pd \nimport numpy as np\n```"
  },
  {
    "objectID": "qmd/quarto-rmarkdown.html#sec-quarto-misc",
    "href": "qmd/quarto-rmarkdown.html#sec-quarto-misc",
    "title": "Quarto",
    "section": "Misc",
    "text": "Misc\n\nquarto --version - Must be in RStudio Terminal\nquarto check - Must be in RStudio Terminal - versions and engine checks\n$ quarto check\n[&gt;] Checking versions of quarto binary dependencies...\nÂ  Â  Â  Pandoc version 3.1.1: OK\nÂ  Â  Â  Dart Sass version 1.55.0: OK\n[&gt;] Checking versions of quarto dependencies......OK\n[&gt;] Checking Quarto installation......OK\nÂ  Â  Â  Version: 1.3.340\nÂ  Â  Â  Path: C:\\Users\\tbats\\AppData\\Local\\Programs\\Quarto\\bin\nÂ  Â  Â  CodePage: 1252\n[&gt;] Checking basic markdown render....OK\n[&gt;] Checking Python 3 installation....OK\nÂ  Â  Â  Version: 3.8.1 (Conda)\nÂ  Â  Â  Path: C:/Users/tbats/Miniconda3/python.exe\nÂ  Â  Â  Jupyter: 4.9.1\nÂ  Â  Â  Kernels: python3\n(\\) Checking Jupyter engine render....2023-04-28 10:18:15,018 - traitlets - WARNING - Kernel\nProvisioning: The 'local-provisioner' is not found.Â  This is likely due to the presence of multiple jupyter_client distributions and a        previous distribution is being used as the source for entrypoints - which does not include 'local-provisioner'.Â  That distribution should     be removed such that only the version-appropriate distribution remains (version &gt;= 7).Â  Until then, a 'local-provisioner' entrypoint will     be automatically constructed and used.\nThe candidate distribution locations are: ['C:\\\\Users\\\\tbats\\\\Miniconda3\\\\lib\\\\site-packages\\\\jupyter_client-5.3.4.dist-info',                'C:\\\\Users\\\\tbats\\\\Miniconda3\\\\lib\\\\site-packages\\\\jupyter_client-7.0.6.dist-info']\n[&gt;] Checking Jupyter engine render....OK\n[&gt;] Checking R installation...........OK\nÂ  Â  Â  Version: 4.2.3\nÂ  Â  Â  Path: C:/PROGRA~1/R/R-42~1.3\nÂ  Â  Â  LibPaths:\nÂ  Â  Â  Â  - C:/Users/tbats/AppData/Local/R/win-library/4.2\nÂ  Â  Â  Â  - C:/Program Files/R/R-4.2.3/library\nÂ  Â  Â  knitr: 1.42\nÂ  Â  Â  rmarkdown: 2.20\n[&gt;] Checking Knitr engine render......OK\nShortcuts\n\nNew R chunk: ctrl + alt + i\nBuild whole book: ctrl+shift b\nRender page and preview book: ctrl+shift k\n\nUsing yaml style for chunk options\n\nConvert Rmd chunk options to Quarto: knitr::convert_chunk_header(\"doc.rmd\", \"doc.qmd\")\nAnchor Link - A link, which allows the users to flow through a website page. It helps to scroll and skim-read easily. A named anchor can be used to link to a different part of the same page (like quickly navigating) or to a specific section of another page.\n\nThis is the â€œ#sec-mooseâ€ id that can be added to headers which it allows to be referenced within the document or in other documents."
  },
  {
    "objectID": "qmd/quarto-rmarkdown.html#sec-quarto-quarto",
    "href": "qmd/quarto-rmarkdown.html#sec-quarto-quarto",
    "title": "Quarto",
    "section": "Syntax",
    "text": "Syntax\n\nAlign code chunk under bullet and add indented comment below chunk\n-   [Example]{.ribbon-highlight} (using a SQL Query; method 1)\n\n    ``` r\n    # open dataset\n    ds &lt;- arrow::open_dataset(dir_out, partitioning = \"species\")\n    # open connection to DuckDB\n    con &lt;- dbConnect(duckdb::duckdb())\n    # register the dataset as a DuckDB table, and give it a name\n    duckdb::duckdb_register_arrow(con, \"my_table\", ds)\n    # query\n    dbGetQuery(con, \"\n      SELECT sepal_length, COUNT(*) AS n\n      FROM my_table\n      WHERE species = 'species=setosa'\n      GROUP BY sepal_length\n    \")\n\n    # clean up\n    duckdb_unregister(con, \"my_table\")\n    dbDisconnect(con)\n    ```\n\n    -   filtering using a partition, the WHERE format is '\\&lt;partition_variable\\&gt;=\\&lt;partition_value\\&gt;'\n\nSpace between bullet and top ticks\nSpace between bottom ticks and bullet\nNote alignment of text\n\nAdd Code Annotations\n-   [Partition a large file and write to arrow format]{.underline}\n\n    ``` r\n    lrg_file &lt;- open_dataset(&lt;file_path&gt;, format = \"csv\") # &lt;1&gt;\n    lrg_file %&gt;%\n        group_by(var) %&gt;% # &lt;2&gt;\n        write_dataset(&lt;output_dir&gt;, format = \"feather\") # &lt;3&gt;\n    ```\n\n    1.  Pass the file path to `open_dataset()`\n\n    2.  Use `group_by()` to partition the Dataset into manageable chunks\n\n    3.  Use `write_dataset()` to write each chunk to a separate Parquet file---all without needing to read the full CSV file into R\n\n    -   `open_dataset` is fast because it only reads the metadata of the file system to determine how it can construct queries"
  },
  {
    "objectID": "qmd/quarto-rmarkdown.html#chunk-options-and-yaml",
    "href": "qmd/quarto-rmarkdown.html#chunk-options-and-yaml",
    "title": "Quarto",
    "section": "Chunk Options and YAML",
    "text": "Chunk Options and YAML\n\nSet global chunk options in yaml\n\nEnable Margin Notes\n---\n# YAML front matter\nreference-location: margin\n---\n!expr to render code within chunk options\n\ne.g.Â figure caption: #| fig-cap: !expr glue::glue(\"The mean temperature was {mean(airquality$Temp) |&gt; round()}\")\n\nConditional Code Chunk Evaluation\n\nExample: document output type\n\nSet value in a code chunk\n```{{r setup}}\n# Include in first chunk of .qmd\n# Get output file type\nout_type &lt;- knitr::opts_knit$get(\"rmarkdown.pandoc.to\")\n```\nUse !expr sytax to determine evaluation status\n```{{r}}\n#| eval: !expr out_type == \"html\"\n\n# code to create interactive {plotly}\n```\n\n```{r}\n#| eval: !expr out_type == \"docx\"\n\n# code to create static {ggplot2}\n```\n\nExample: Use parameterization to set value\n---\ntitle: \"test\"\nformat: html\nparams:\n  my_value: false\n---\n\nmy_value can then be used throughout the document to determine chunk evaluation status\n\n\ncolumn: screen-inset yaml markup is used to show a very wide table\nCLI\n\nquarto render to compile a document\nquarto preview to render a live preview that automatically updates when the source files are saved\n\nGraphics\n\nCode Chunk\n```\n#| dpi: 300\n#| fig.height: 7.2\n#| fig.width: 3.6\n#| dev: \"png\"\n#| echo: false\n#| warning: false\n#| message: false\n```\n\nExample shows settings for a graph for mobile\nfig.height and fig.width are always given in inches\n\n\nIf you havenâ€™t set your Quarto document to be self-contained, then the images have also already been saved for you - probably in a folder called documentname_files/figure-html/\nformat: \n  html:\n    embed-resources: true\nYAML Example\n\nNested Tabs"
  },
  {
    "objectID": "qmd/quarto-rmarkdown.html#r-and-python",
    "href": "qmd/quarto-rmarkdown.html#r-and-python",
    "title": "Quarto",
    "section": "R and Python",
    "text": "R and Python\n\nIf only R or R and Python, the notebook is rendered by {knitr}\n\nIf only Python, the notebook is rendered by jupyter\n\nSet-up\n\n{reticulate} automatically comes loaded in Quarto and it knows to use it when it sees a python block, so you donâ€™t need to load the package\n\nR\n```{{r}}\n#| label: read-data\n#| echo: true\n#| message: false\n#| cache: true\nlemurs &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-08-24/lemur_data.csv')\n```\nPython\n```{{python}}\n#| label: modelling \n#| echo: true \n#| message: false\n\nlemur_data_py = r.lemur_data \nimport statsmodels.api as sm \ny = lemur_data_py[[\"Weight\"]] \nx = lemur_data_py[[\"Age\"]] \nx = sm.add_constant(x) \nmod = sm.OLS(y, x).fit() \nlemur_data_py[\"Predicted\"] = mod.predict(x) \nlemur_data_py[\"Residuals\"] = mod.resid`\n```\n\nUse r. to access the data in the R chunk\n\n(back to) R\n```{{r}}\n#| label: plotting \n#| echo: true \n#| output-location: slide \n#| message: false \n#| fig-align: center \n#| fig-alt: \"Scatter plot of predicted and residual values for the fitted linear model.\" \n\nlibrary(reticulate) \nlibrary(ggplot2) \nlemur_residuals &lt;- py$lemur_data_py \nggplot(data = lemur_residuals, aes(x = Predicted, y = Residuals)) +\n  geom_point(colour = \"#2F4F4F\") +\n  geom_hline(yintercept = 0,\n            colour = \"red\") +\n  theme(panel.background = element_rect(fill = \"#eaf2f2\", colour = \"#eaf2f2\"),\n        plot.background = element_rect(fill = \"#eaf2f2\", colour = \"#eaf2f2\"))\n```\n\nUse py$ to access the data in the Python chunk *\nMust call library(reticulate) in order for Quarto to recognize py$"
  },
  {
    "objectID": "qmd/quarto-rmarkdown.html#layouts",
    "href": "qmd/quarto-rmarkdown.html#layouts",
    "title": "Quarto",
    "section": "Layouts",
    "text": "Layouts\n\n2 cols (1 col: text, 1 col: image)\n\n::: {layout=\"[50,50]\"}\n\n::: column\nEvery Quarto project starts with a Quarto file that has the extension `.qmd`.\n\n\nThis particular one analyzes children's early words, but every `.qmd` includes the same three basic elements inside:\n\n\n- A block of metadata at the top, between two fences of `---`s. This is written in [YAML](https://learnxinyminutes.com/docs/yaml/). \n- Narrative text, written in [Markdown](https://commonmark.org/help/tutorial/). \n- Code chunks in gray between two fences of ```` ``` ````, written with R or another programming language.\n\n\nYou can use all three elements to develop your code and ideas in one reproducible document.\n:::\n\n![](img/01-source.png)\n:::\n2 figures, 2 columns (i.e.Â side-by-side) with captions at the top\n---\nfig-cap-location: top\n---\n\n-   Words\n    -   Predictions of Standard RF vs Oblique RF\n\n        ::: {layout-ncol=\"2\"}\n        ![Standard Random Forest](_resources/Regression,_Survival.resources/ml-rf-obl-vs-axis-axpred-1.png){fig-align=\"left\" width=\"432\"}\n\n        ![Oblique Random Forest](_resources/Regression,_Survival.resources/ml-rf-obl-vs-axis-oblpred-1.png){fig-align=\"left\" width=\"432\"}\n        :::\n\n        -   Words  \n\nfig-cap-location: bottom is default;\nfig-cap-location: margin is buggy, at least in for project type book. Captions are added to the margins but bullet points mysteriously disappear during rendering to html\n\n2 charts side-by-side extending past body margins\n```{{r}}\n#| label: my-figure\n#| layout-ncol: 2\n#| column: page\nggplot() + ...\nggplot() + ...\n```\n\nâ€œlayout-ncolâ€ says 2 side-by-side columns\nâ€œcolumn: pageâ€ says extend column width to the width of the page"
  },
  {
    "objectID": "qmd/regex.html#sec-regex-misc",
    "href": "qmd/regex.html#sec-regex-misc",
    "title": "Regex",
    "section": "Misc",
    "text": "Misc\n\nFor logical expressions, order matters. Therefore, more complicated patterns should precede simplier patterns.\n\nExample:\n\n(\\\\w+|\\\\w+\\\\s\\\\w+) says to extract single words then extract compound words or expressions, but since compound words are made up of single words, it will only extract the first halves and miss the whole compound word\n((\\\\w+\\\\s\\\\w+)|\\\\w+) will extract the compound word and then the single words.\n\n\nMatch anything once: (.*?)\nMatch empty lines: \"^$\"\nMatch punctuation and special characters: [^\\\\w\\\\s*]"
  },
  {
    "objectID": "qmd/regex.html#sec-regex-pats",
    "href": "qmd/regex.html#sec-regex-pats",
    "title": "Regex",
    "section": "Patterns",
    "text": "Patterns\n\nExtracting Words and Compound Words at the Beginning\nqmd_txt\n#&gt; [1] \"-   200 Status - An API serving an ML model returns a HTTP 200 OK success status response code indicates that the request has succeeded.\" \n#&gt; [2] \"-   AMI - amazon machine image. Thing that has R and the main packages you need to load onto the cloud server\"\n#&gt; [3] \"-   Anti-patterns - certain patterns in software development that are considered bad programming practices.\"\nqmd_txt |&gt; \n  str_extract(pattern = \"^\\\\-   ((\\\\w+\\\\s\\\\w+)|(\\\\w+[^\\\\w\\\\s*]+\\\\w+)|\\\\w+)\")\n\n#&gt; [1] \"-   200 Status\"                 \n#&gt; [2] \"-   AMI\"          \n#&gt; [3] \"-   Anti-patterns\"\n\n(\\\\w+\\\\s\\\\w+) matches patterns of â€œword + space + wordâ€\n(\\\\w+[^\\\\w\\\\s*]+\\\\w+) matches patterns of â€œword + (not word and not space) + wordâ€\n\n[^\\\\w\\\\s*] will match punctuation and special characters (e.g.Â hypens separating words)\n\n\\\\w+ matches word"
  },
  {
    "objectID": "qmd/regression-discrete.html#sec-reg-disc-misc",
    "href": "qmd/regression-discrete.html#sec-reg-disc-misc",
    "title": "Discrete",
    "section": "Misc",
    "text": "Misc\n\nAlso see Regression, Other\nPackages\n\n{glmnet}\n\nFor Diagnostics see:\n\n{DHARMa} - Built for Mixed Effects Models for count distributions but handles lm, glm (poisson) and MASS::glm.nb (neg.bin)\nDiagnostics, Probabilistic &gt;&gt; Visual Inspection &gt;&gt; Visual Inspection\nDiagnostics, GLM\n\nWith aggregated counts that are bound within a certain range, it can be better to turn the range of counts into percentages (see example) and model those as your outcome\n\nDistributions\n\nZero-One Inflated Beta\nmod_zoib &lt;-\n  brm(bf(outcome_pct ~ 1),\n      data = example_data,\n      family = zero_one_inflated_beta(),\n      cores = 4)\npp_check(mod_zoib)\n\nZero-One Inflated Binomial\n\nIn general, you can have a zero-N inflated binomial\n\n\nExample: Aggregated counts from 1 to 32 (Thread)\n\n\nIf something specific is generating 1 and 32 counts\n\nIdeally youâ€™d do this, but these require creating bespoke distribution families which is possible in STAN\n\nIf you cannot get zero, then a 0-31-inflated binomial works fine.\nIf 0 is possible but it didnâ€™t happen, then do a 1-32-inflated binomial.\n\nMore conveniently, youâ€™d transform the range (1-32) to percentages where 100% = 32, and use zero-one inflated beta (currently available in {brms} or zero-one inflated binomial\n\nIf there is NOT something specific generating the 1 and 32 counts(?)\n\nYou can keep the counts and treat them as an ordered factor\n\nCollapse the counts from 2-31 into a category, so you have 3 categories: 1, 2-31, 32.\nModel as an ordered logit\nmod_ologit &lt;- \n  brm(bf(outcome_factor ~ 1),\n      data = example_data,\n      family = cumulative,\n      cores = 4)\npp_check(mod_ologit)"
  },
  {
    "objectID": "qmd/regression-discrete.html#sec-reg-disc-terms",
    "href": "qmd/regression-discrete.html#sec-reg-disc-terms",
    "title": "Discrete",
    "section": "Terms",
    "text": "Terms\n\nA saturated model is a regression model that includes a discrete (indicator) variable for each set of values the explanatory variables can take.\n\nAnother case is when there are as many estimated parameters as data points.\n\ne.g.Â if you have 6 data points and fit a 5th-order polynomial to the data, you would have a saturated model (one parameter for each of the 5 powers of your independant variable plus one for the constant term).\n\nMulti-variable models require interactions to be able to cover each set of values that the explanatory variables can take (see 3rd example)\nSince saturated models, perfectly model the sample, they donâ€™t generalize to the population well.\n\nNo data left to estimate variance.\n\nExamples of Saturated Models\n\nWages ~ College Graduation (binary)\n\\[\n\\operatorname{Wages}_i = \\alpha + \\beta \\:\\mathbb{I}\\{\\operatorname{College Graduate}\\}_i + \\epsilon_i\n\\]\nWages ~ Schooling (discrete, yrs).\n\\[\n\\begin{align}\n\\operatorname{Wages} &= \\alpha + \\beta_1 \\:\\mathbb{I}\\{s_i = 1\\} + \\beta_2 \\:\\mathbb{I}\\{s_i = 2\\} + â‹¯ + \\beta_T \\:\\mathbb{I}\\{s_i = T\\}\n&\\text{where}\\quad s_i \\in \\{0, 1, 2,...T\\}\n\\end{align}\n\\]\n\n0 is the reference level; \\(\\beta\\) is the effect of j years of schooling.\n\nWages ~ College Graduation + Gender + Interaction.\n\\[\n\\operatorname{Wages} = \\alpha + \\beta_1 \\:\\mathbb{I}{\\operatorname{College Graduate}} + \\beta_2 \\:\\mathbb{I}\\{\\operatorname{Female}\\} + \\beta_3 \\:\\mathbb{I}\\{\\operatorname{College Graduate}\\} \\times \\:\\mathbb{I}\\{\\operatorname{Female}\\} + Îµ\n\\]\n\n\\(\\mathbb{E}[\\operatorname{Wages}_i | \\operatorname{College Graduate}_i = 0, \\operatorname{Female}_i = 0] = \\alpha\\)\n\nExpected value of Wages for individual i given theyâ€™re not a college graduate and are male\n\n\\(\\mathbb{E}[\\operatorname{Wages}_i | \\operatorname{College Graduate}_i = 1, \\operatorname{Female}_i = 0] = \\alpha + \\beta_1\\)\n\\(\\mathbb{E}[\\operatorname{Wages}_i | \\operatorname{College Graduate}_i = 0, \\operatorname{Female}_i = 1] = \\alpha + \\beta_2\\)\n\\(\\mathbb{E}[\\operatorname{Wages}_i | \\operatorname{College Graduate}_i = 1, \\operatorname{Female}_i = 1] = \\alpha + \\beta_1 + \\beta_2 + \\beta_3\\)\n\n\n\nNull Model has only one parameter, which is the intercept.\n\nThis is essentially the mean of all the data points.\nFor a bivariate model, this is a horizontal line with the same prediction for every point\n\nDeviance\n\\[\nD = 2(L_S - L_P) = 2(\\operatorname{loglik}(y\\;|\\;y) - \\operatorname{loglik}(\\mu\\;|\\;y))\n\\]\n\n\\(L_S\\) is the saturated model\n\\(L_P\\) is the â€œproposed modelâ€ (i.e.Â the model being fit)"
  },
  {
    "objectID": "qmd/regression-discrete.html#sec-reg-disc-binom",
    "href": "qmd/regression-discrete.html#sec-reg-disc-binom",
    "title": "Discrete",
    "section": "Binomial",
    "text": "Binomial\n\nExample: UCB Admissions\n# Array to tibble (see below for deaggregation this to 1/0)\nucb &lt;-Â \nÂ  Â  as_tibble(UCBAdmissions) %&gt;%Â \nÂ  Â  mutate(across(where(is.character), ~ as.factor(.))) %&gt;%Â \nÂ  Â  pivot_wider(\nÂ  Â  Â  Â  id_cols = c(Gender, Dept),\nÂ  Â  Â  Â  names_from = Admit,\nÂ  Â  Â  Â  values_from = n,\nÂ  Â  Â  Â  values_fill = 0L\nÂ  Â  Â  )\n\n## # A tibble: 12 Ã— 4\n##Â  Â  Gender DeptÂ     Admitted Rejected\n##Â  Â  &lt;fct&gt;Â  &lt;fct&gt;Â  Â    &lt;dbl&gt;Â  Â  &lt;dbl&gt;\n##Â  1 MaleÂ      AÂ  Â  Â  Â  Â  512Â  Â  Â  313\n##Â  2 Female    AÂ  Â  Â  Â  Â   89Â  Â  Â   19\n##Â  3 MaleÂ      BÂ  Â  Â  Â  Â  353Â  Â  Â  207\n##Â  4 Female    BÂ  Â  Â  Â  Â   17Â  Â  Â  Â  8\n##Â  5 MaleÂ      CÂ  Â  Â  Â  Â  120Â  Â  Â  205\n##Â  6 Female    CÂ  Â  Â  Â  Â  202Â  Â  Â  391\n##Â  7 MaleÂ      DÂ  Â  Â  Â  Â  138Â  Â  Â  279\n##Â  8 Female    DÂ  Â  Â  Â  Â  131Â  Â  Â  244\n##Â  9 MaleÂ      EÂ  Â  Â  Â  Â   53Â  Â  Â  138\n## 10 Female    EÂ  Â  Â  Â  Â   94Â  Â  Â  299\n## 11 MaleÂ      FÂ  Â  Â  Â  Â   22Â  Â  Â  351\n## 12 Female    FÂ  Â  Â  Â  Â   24Â  Â  Â  317\n\nglm(\nÂ  cbind(Rejected, Admitted) ~ Gender + Dept,\nÂ  data = ucb,\nÂ  family = binomial\n)\n## Coefficients:\n## (Intercept)Â  GenderMaleÂ  Â  Â  Â  DeptBÂ  Â  Â  Â  DeptCÂ  Â  Â  Â  DeptDÂ  Â  Â  Â  DeptEÂ \n##Â  Â  -0.68192Â  Â  Â 0.09987Â  Â  Â  0.04340Â  Â  Â  1.26260Â  Â  Â  1.29461Â  Â  Â  1.73931Â \n##Â  Â  Â  DeptFÂ \n##Â  Â  3.30648Â \n##Â \n## Degrees of Freedom: 11 Total (i.e. Null);Â  5 Residual\n## Null Deviance: Â  Â  877.1Â \n## Residual Deviance: 20.2 AIC: 103.1\n\ncbind(Rejected, Admitted) says that â€œRejectedâ€ is the response variable since it is listed first in the cbind function\nCan also use a logistic model, but need case-level data (e.g.Â 0/1)\n\nDeaggregate count data into 0/1 case-level data\ndata(UCBadmit, package = \"rethinking\")\nucb &lt;- UCBadmit %&gt;%\nÂ  mutate(applicant.gender = relevel(applicant.gender, ref = \"male\"))\n\n# deaggregate to 1/0\ndeagg_ucb &lt;- function(x, y) {\nÂ  UCBadmit %&gt;%\nÂ  Â  select(-applications) %&gt;%\nÂ  Â  group_by(dept, applicant.gender) %&gt;%\nÂ  Â  tidyr::uncount(weights = !!sym(x)) %&gt;%\nÂ  Â  mutate(admitted = y) %&gt;%\nÂ  Â  select(dept, gender = applicant.gender, admitted)\n}\nucb_01 &lt;- purrr::map2_dfr(c(\"admit\", \"reject\"),\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  c(1, 0),\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ~ disagg_ucb(.x, .y)\n)\n\n\nExample: Treatment/Control\nÂ  Â  Â  Â  Â  Â  DiseaseÂ  Â  Â  No Disease\nTreatmentÂ  Â  Â  Â  55Â  Â  Â  Â  Â  Â  Â  Â  67\nControlÂ  Â  Â  Â  Â  42Â  Â  Â  Â  Â  Â  Â  Â  34\n\ndf &lt;- tibble(treatment_status = c(\"treatment\", \"no_treatment\"),\nÂ  Â  Â  disease = c(55, 42),\nÂ  Â  Â  no_disease = c(67,34)) %&gt;%Â \nÂ  mutate(total = no_disease + disease,\nÂ  Â  Â  Â  proportion_disease = disease / total)Â \n\nmodel_weighted &lt;- glm(proportion_disease ~ treatment_status,\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  data = df,\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  family = binomial,\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  weights = total)\nmodel_cbinded &lt;- glm(cbind(disease, no_disease) ~ treatment_status,\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  data = df,\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  family = binomial)\n\n# Aggregated counts expanded into case-level data\ndf_expanded &lt;- tibble(disease_status = c(1, 1, 0, 0),Â \nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  treatment_status = rep(c(\"treatment\", \"control\"), 2)) %&gt;%\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  .[c(rep(1, 55), rep(2, 42), rep(3, 67), rep(4, 34)), ]\n# logistic\nmodel_expanded &lt;- glm(disease_status ~ treatment_status, data = df_expanded, family = binomial(\"logit\"))\n\nAll methods are equivalent\nâ€œdiseaseâ€ is listed first in the cbind function, therefore it is the response variable."
  },
  {
    "objectID": "qmd/regression-discrete.html#sec-reg-disc-pois",
    "href": "qmd/regression-discrete.html#sec-reg-disc-pois",
    "title": "Discrete",
    "section": "Poisson",
    "text": "Poisson\n\n\nMisc\nInterpretation\n\nEffect of a binary treatment\n\\[\ne^\\beta = \\mathbb{E}[Y(1)/\\mathbb{E}Y(0)] = \\theta_{\\text{ATE%}} + 1\n\\]\n\n\\(\\theta\\) is the effect interpreted as a percentage\n\\(\\mathbb{E}[Y(1)]\\) is the expected value of the outcome for a subject assigned to Treatment.\nTherefore, \\(e^\\beta - 1\\) is the average percent increase or decrease from baseline to treatment\nParameter may difficult to interpret in contexts where Y spans several order of magnitudes.\n\nExample: The econometrician may perceive a change in income from $5,000 to $6,000 very differently from a change in income from $100,000 to $101,000, yet both those changes are treatment effects in levels of $1,000 and thus contribute equally to Î¸ATE%."
  },
  {
    "objectID": "qmd/regression-interactions.html#sec-reg-inter-misc",
    "href": "qmd/regression-interactions.html#sec-reg-inter-misc",
    "title": "Interactions",
    "section": "Misc",
    "text": "Misc\n\nAlso see\n\nPost-Hoc Analysis, emmeans &gt;&gt; Interactions\nIntroduction: Adding Partial Residuals to Marginal Effects Plots for detecting interactions using residuals of a regression model.\nDiagnostics, Model Agnostic &gt;&gt; DALEX &gt;&gt; Instance Level &gt;&gt; Break-Down &gt;&gt; Example: Assume Interactions\n\nâ€œYou need 16 times the sample size to estimate an interaction than to estimate a main effectâ€ Gelman\nSelecting values of continuous predictors to use for calculating marginal effects\n\nMean/Median and quantiles are popular choices\nCheck if theyâ€™re multi-modal. If so, then the mean or median may not be a useful value to use for that variable when computing marginal effects.\n\nModes for the distribution would be a suitable alternative.\n\nMay also be useful to try to find intervals of values where there is no significant marginal effect.\nUsing all combinations of values in the observed data to compute individual marginal effects and then taking the average will result in the average interaction effect\n\nHelpful in generating inferences about the whole population of interest instead of scenarios of interest or groups of interest\n\n\nGelman and Hill: â€œInteractions can be important. In practice, inputs that have large main effects also tend to have large interactions with other inputs. (However, small main effects do not preclude the possibility of large interactions.)â€\nWe can probe or decompose interactions by asking the following research questions:\n\nWhat is the predicted Y given a particular X and W? (predicted value)\nWhat is relationship of X on Y at particular values of W? (simple slopes/effects)(marginal effects/predicted means)\nIs there a difference in the relationship of X on Y for different values of W? (comparing marginal effects)\n\nPredictors that are uncorrelated with each other still might have interaction effects on the response variable.\n\nExample: plant growth requires sunlight and rain. Without rain, sunlight will not increase growth and vice versa, but rain with sunlight does cause growth. This interaction effect exists even though amounts of sunlight and rain arenâ€™t (very) correlated with each other\n\nQuadratic interaction example from {marginaleffects} vignette\nDropping main effects\n\nFor the cont x cat interaction, as long as you donâ€™t drop a categorical main effect, I donâ€™t think it affects the model. It may or may not reparameterize the interaction coefficients so theyâ€™re the marginal effects instead contrasts. (see article)\n\ne.g.Â for admit ~ dept + gender:dept , the interpretation of an interaction coefficient might be something like male-female at dept A.\n\nWhether itâ€™s okay to drop the categorical variable, may depend on whether the main effects are significant or not\nDropping a continuous main effects changes the model, so best not to ever do that in cat x cont or cont x cont interactions.\nHarrell said in a SO post that you should never do it.\n\nHe had a link to blog post heâ€™d written but the link was dead and there wasnâ€™t anything in his RMS book.\n\n\nFalse linearity assumptions of main effects can inflate apparent interaction effects because interactions may be colinear with the omitted non-linear effect\nInteractions may also be non-linear, but shouldnâ€™t do this w/smaller sample sizes\n\nWith smaller sample sizes, make the main effect non-linear and the interaction component linear if the non-linear relationship is present between the interaction and outcome."
  },
  {
    "objectID": "qmd/regression-interactions.html#sec-reg-inter-terms",
    "href": "qmd/regression-interactions.html#sec-reg-inter-terms",
    "title": "Interactions",
    "section": "Terms",
    "text": "Terms\n\nDisordinal Interactions (aka crossover or antagonistic) - Interaction results whose lines do cross. (see fig in Ordinal Interactions)\n\nWhen an interaction is significant and â€œdisordinalâ€, interpretation of main effects is likely to be misleading.\n\nTo determine exactly which parts of the interaction are significant, the omnibus F test must be followed by more focused tests or comparisons.\n\n\nMarginal Effects (aka Simple Slope) - partial derivatives of the regression equation with respect to each variable in the model for each unit (i.e.Â observation) in the data; average marginal effects are simply the mean of these unit-specific partial derivatives over some sample\n\ni.e.Â the slope of the prediction function, measured at a specific value of the regressor\nIn OLS regression with no interactions or higher-order term, the estimated slope coefficients are average marginal effects\nIn other cases and for generalized linear models, the coefficients are NOT marginal effects at least not on the scale of the response variable\n\nModerator Variable (MV) - a predictor that changes the relationship of the IV on the DV, and it can be continuous or discrete. Used in an interaction to estimate its effect. (also see Causal Inference &gt;&gt; Moderation Analysis)\nOrdinal Interactions - Interaction results whose lines do not cross\n\nThis looks like an EDA plot but also see OLS &gt;&gt; continuous:continuous &gt;&gt; Plot Simple Slopes\n\nSimilar plot but for post-hoc analysis of a regression with an interaction\n\nExponential Interaction: If the lines are not parallel but one line is steeper than the other, the interaction effect will be significant (i.e.Â detected) only if thereâ€™s enough statistical power.\nIf the lines are exactly parallel, then there is no interaction effect\n\nSimple Effect - When a Independent Variable interacts with a moderator variable (MV), itâ€™s the differences in predicted values of the outcome variable at a particular levels or values of the MV\n\nThe article used as reference for OLS section uses Simple Effects and Slopes interchangeably and also confuses regression coefficients with predicted values when referring to those terms throughout the tutorial. Itâ€™s confusing, so Iâ€™ve seperated the terms â€” one for coefficients (slopes) and another for predicted values/means (effects).\nEach of these predicted values is a Marginal Mean (term used in {emmeans}). So, the simple effect would be the difference (aka contrast) of marginal means.\n\n{marginaleffects} calls these â€œadjusted predictionsâ€ or â€œmarginal meansâ€ but both seem to be the same thing.\n\nThe interaction coefficient is the difference of simple effects (aka the â€œdifference in differencesâ€)\nIMO this is the value is more useful than than a contrast of marginal effects\n\nFor the contrast of marginal effects, the pvals are unhelpful and the interpretation of the results is kind of janky (see interpretation in continuous:continuous &gt;&gt; Test pairwise differences (contrasts) of marginal effects section).\n\n\nSimple Slopes (aka Marginal Effects): when a Independent Variable interacts with a moderator variable (MV), its slope at a particular level or value of the MV\n\nThe main effect when an interaction is specified\ne.g.Â Y ~ b0 + b1X1 + b2X2 + b3X1X2\n\nInterpret marginal effect, b1, as the expected change in Y after 1 unit change in X1 when X2 = 0\n\nThe article used as reference for OLS section uses Simple Effects and Slopes interchangeably and also mixes regression coefficients with predicted values when referring to those terms throughout the tutorial. Itâ€™s confusing, so Iâ€™ve seperated the terms â€” one for coefficients (slopes) and another for predicted values/means (effects).\n\nSpotlight Analysis: Comparison of marginal effects to see analyze the effect of the variable of interest at different levels of the moderator\n\nWhen an interaction is significant, choose representative values for the moderator variable and plot the marginal effects of explanatory variable to decompose the relationship between the variables involved in the interaction.\nMade popular by Leona Aiken and Stephen Westâ€™s book Multiple Regression: Testing and Interpreting Interactions (1991)."
  },
  {
    "objectID": "qmd/regression-interactions.html#sec-reg-inter-preproc",
    "href": "qmd/regression-interactions.html#sec-reg-inter-preproc",
    "title": "Interactions",
    "section": "Preprocessing",
    "text": "Preprocessing\n\nCenter continuous variables\n\nCentering fixes collinearity issues when creating powers and interaction terms (CV post)\n\nCollinearity between the created terms and the main effects\n\nInterpretation of effects with be at the mean of the explanatory variable\n\ne.g.Â Since X = 0 implies , the intercept, marginal effects and simple effects are interpreted at X = mean(x).\nDiscourages unreasonable extrapolations"
  },
  {
    "objectID": "qmd/regression-interactions.html#sec-reg-inter-lin",
    "href": "qmd/regression-interactions.html#sec-reg-inter-lin",
    "title": "Interactions",
    "section": "Linear",
    "text": "Linear\n\nMisc\n\nNotes from Article\nIf the interaction term is not significant but the main effect is significant then the intercepts (i.e.Â response means) of at least one pair of the levels DO differ.\n\n{emmeans::emmeans} can explore and test these types of scenarios\n\nfactor(var) might be necessary for character variables with more than one level (to get the contrasts) but isnâ€™t necessary for 0/1 variables (same coefficient estimates with or without).\n{emtrends} takes all differences (aka contrasts) from the reference group. So signs may be different from the summary stats values\n\n\n\nContinuous:Continuous\n\nExample: Does the effect of workout duration (IV) on weight loss (Outcome) change due to the intensity of your workout (Moderator)?\ncontcont &lt;- lm(loss ~ hours + effort + hours:effort, data=dat)\n#&gt; Â  Â  Â  Â  Â  Â    Estimate Std. Error t value Pr(&gt;|t|)Â \n#&gt; (Intercept)Â   7.79864Â  11.60362Â   0.672Â   0.5017Â \n#&gt; hoursÂ  Â  Â  Â  -9.37568Â  Â 5.66392Â  -1.655Â   0.0982 .\n#&gt; effortÂ  Â  Â   -0.08028Â  Â 0.38465Â  -0.209Â   0.8347Â \n#&gt; hours:effortÂ  0.39335Â  Â 0.18750Â   2.098Â   0.0362 *\n\n(Intercept): the intercept, or the predicted outcome when Hours = 0 and Effort = 0.\nhours (marginal effect, Indepenedent Variable (IV)): for a one unit change in Hours, the predicted change in weight loss at Effort=0.\neffort (marginal effect, moderator): for a one unit change in Effort the predicted change in weight loss at Hours=0.\nhours:effort: the change in the slope of Hours for every one unit increase in Effort (or vice versa).\nCalculate representative values for the moderator variable\neff_high &lt;- round(mean(dat$effort) + sd(dat$effort), 1) # 34.8\neff &lt;- round(mean(dat$effort), 1) # 29.7\neff_low &lt;- round(mean(dat$effort) - sd(dat$effort), 1) # 24.5\n\nmean, +1 sd, -1Â  sd\n\nCalculate marginal effects for the IV at 3 representative values for the moderator variable\n# via {emmeans}\nmylist &lt;- list(effort = c(eff_low, eff, eff_high))Â \nemtrends(contcont, ~effort, var = \"hours\", at = mylist)\n#&gt; effort    hours.trendÂ  Â  SEÂ      df     lower.CL upper.CL\n#&gt; 24.5Â  Â  Â  0.261          1.352   896Â   -2.392Â  Â  2.91\n#&gt; 29.7Â  Â  Â  2.307          0.915   896Â  Â  0.511Â  Â  4.10\n#&gt; 34.8Â  Â  Â  4.313          1.308   896Â  Â  1.745Â  Â  6.88\n#&gt; Confidence level used: 0.95\n\n# via {marginaleffects}\nloss_grid &lt;- datagrid(effort = c(eff_high, eff, eff_low),\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  model = contcont)\nmarginaleffects(contcont, newdata = loss_grid)\n#&gt; Â  rowidÂ  Â  typeÂ      termÂ   dydxÂ      std.errorÂ  hours     effort\n#&gt; 1Â 1        responseÂ  hours  4.3127872 1.30838659 2.002403Â  34.8\n#&gt; 2Â 2        responseÂ  hours  2.3067185 0.91488236 2.002403Â  29.7\n#&gt; 3Â 3        responseÂ  hours  0.2613152 1.35205286 2.002403Â  24.5\n#&gt; 4Â 1        response  effort 0.7073625 0.08795998 2.002403Â  34.8\n#&gt; 5Â 2        response  effort 0.7073625 0.08795998 2.002403Â  29.7\n#&gt; 6Â 3        response  effort 0.7073625 0.08795999 2.002403Â  24.5\n\n{emmeans}\n\nâ€œcontcontâ€ is the model (e.g.Â lm)\nâ€œ~effortâ€ tells emtrends to stratify by the moderator variable â€œeffortâ€\ndf is 896 because we have a sample size of 900, 3 predictors and 1 intercept (df = 900 - 3 -1 = 896)\n\n{marginaleffects}\n\nShows how â€œhoursâ€ is held constant at its mean when itâ€™s marginal effect is calculated at the values specified for â€œeffortâ€\nAdditionally gives you the marginal effect for â€œeffortâ€ at the mean of â€œhoursâ€\n\nInterpretation:\n\nWhen effort = 24.5, a one unit increase in hours results in a 0.26 increase in weight loss on average\nAs we increase levels of Effort, the effect of hours on weight loss seems to increase\nmarginal effect of Hours is significant only for mean Effort levels and above (i.e.Â CIs for mean and high effort donâ€™t include 0)\n\n\nPlot marginal effects for the IV (hours)\n\n# via [{emmeans}]{style='color: #990000'}\nmylist &lt;- list(hours = seq(0,4, by = 0.4), effort = c(eff_low, eff, eff_high))\nemmip(contcont, effort ~ hours, at = mylist, CIs = TRUE)\n\n# via [{marginaleffects}]{style='color: #990000'}\nplot_cap(contcont, condition = c(\"hours\", \"effort\")) # creates it's own subset of values for effort\n\npredicted outcome vs IV (newdata) by moderator (new data)\n\nFor each value of effort, the slope of the hours line at the mean of hours (for that effort level) is the value of the hours marginal effect\n\nInterpretation:\n\nSuggests that hours spent exercising is only effective for weight loss if we put in more effort. (i.e.Â as effort increases so does the effect of hours)\nAt the highest levels of Effort, we achieve higher weight loss for a given time input.\n\nggplot\n\n# via {emmeans}\ncontcontdat &lt;- emmip(contcont,effort~hours,at=mylist, CIs=TRUE, plotit=FALSE)\ncontcontdat$feffort &lt;- factor(contcontdat$effort)\nlevels(contcontdat$feffort) &lt;- c(\"low\",\"med\",\"high\")\nggplot(data=contcontdat, aes(x=hours,y=yvar, color=feffort)) +\nÂ  Â  geom_line() +\nÂ  Â  geom_ribbon(aes(ymax=UCL, ymin=LCL, fill=feffort), alpha=0.4) +\nÂ  Â  labs(x=\"Hours\", y=\"Weight Loss\", color=\"Effort\", fill=\"Effort\")\n\n# via {marginaleffects}\nmoose &lt;- predictions(contcont, newdata = datagrid(hours = seq(0,4, by = 0.4),Â \nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  effort = c(eff_low, eff, eff_high),\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  grid.type = \"counterfactual\"))\nggplot(data=moose, aes(x=hours,y=predicted, color=factor(effort))) +\nÂ  geom_line() +\nÂ  geom_ribbon(aes(ymax=conf.high, ymin=conf.low, fill=factor(effort)), alpha=0.4) +\nÂ  labs(x=\"Hours\", y=\"Weight Loss\", color=\"Effort\", fill=\"Effort\")\n\nTest pairwise differences (contrasts) of marginal effects\nemtrends(contcont, pairwise ~effort, var=\"hours\",at=mylist)\n#&gt; $contrasts\n#&gt;    contrastÂ  Â  estimateÂ   Â  SEÂ   df t.ratio p.value\n#&gt; 24.5 - 29.7Â  Â     -2.05  0.975  896  -2.098Â  0.0362Â \n#&gt; 24.5 - 34.8Â  Â     -4.05  1.931  896  -2.098Â  0.0362Â \n#&gt; 29.7 - 34.8Â  Â     -2.01  0.956  896  -2.098Â  0.0362Â \n#&gt; Results are averaged over the levels of: hours\n\n{marginaleffects} doesnâ€™t do this from what I can tell and IMO looking at contrasts of simple effects (next section) is a better method\n{emmeans}\n\nâ€œpairwise ~effortâ€ tells the function that we want pairwise differences of the marginal effects of var=â€œhoursâ€ for each level of effort specified â€œat=mylistâ€\n\nhours not specified in mylist just the 3 values for effort\n\nThe results are with the arg, adjust=â€œnoneâ€, which removes the Tukey multiple test adjustment. Removed it from the code, because itâ€™s bad practice.\n\nInterpretation\n\nA one unit change in hours when effort is high (34.8) results is 4 lbs more weight loss than a one unit change in hours when effort is low (24.5)\nBased on the NHST, all the marginal effects are significantly different from each other\n\nFor continuous:continuous, p-values will all be the same and identical the interaction p-value of the model (has to do with the slope formula)\n\nTesting the simple effects contrasts (see below) may be the better option\n\n\nTest pairwise differences (contrasts) of simple effects\n# via {emmeans}\nmylist &lt;- list(hours = 4, effort = c(eff_low, eff_high))\nemmeans(contcont, pairwise ~ hours*effort, at=mylist)\n#&gt; $emmeans\n#&gt; hours  effort  emmeanÂ   SEÂ   df lower.CL upper.CL\n#&gt; Â  Â  4Â    24.5Â    6.88  2.79 896Â  Â  Â  1.4Â  Â   12.4\n#&gt; Â  Â  4Â    34.8Â   22.26  2.68 896Â  Â   17.0Â  Â   27.5\n#&gt; Confidence level used: 0.95Â \n#&gt; $contrasts\n#&gt;    contrastÂ  Â  Â  Â  estimateÂ    SEÂ   df t.ratio p.value\n#&gt; 4,24.5 - 4,34.8Â  Â     -15.4  3.97  896  -3.871Â  0.0001\n\n\n# via {marginaleffects}\ncomparisons(contcont,\nÂ  Â  Â  Â  Â  Â  variables = list(effort = c(eff_low, eff_high)),\nÂ  Â  Â  Â  Â  Â  newdata = datagrid(hours = 4)) %&gt;%Â \nÂ  tidy()\n#&gt; rowidÂ  Â  typeÂ  Â  Â  Â  term   contrast_effort comparison std.error statisticÂ  Â  Â  p.value conf.low conf.highÂ   effort hours\n#&gt; 1Â  Â  1 response interactionÂ  Â   34.8 - 24.5Â   15.37904Â   3.97295Â  3.870938 0.0001084175 7.592203Â  23.16588 29.65922Â  Â   4\n\n# alternate (difference in results could be due to rounding of eff_low, eff_high)\ncomparisons(contcont,\nÂ  Â  Â  Â  Â  Â  variables = list(effort = \"2sd\"),\nÂ  Â  Â  Â  Â  Â  newdata = datagrid(hours = 4,Â \nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  grid.type = \"counterfactual\")) %&gt;%Â \nÂ  tidy()\n#&gt; Â  Â  Â  typeÂ  Â  Â  Â  termÂ  Â   contrast_effort estimate std.error statisticÂ  Â  Â  p.value conf.low conf.high\n#&gt; 1 response interaction (x + sd) - (x - sd) 15.35743Â  3.967367  3.870938 0.0001084175 7.581535Â  23.13333\n\nOne reason to look at the tests for the predicted means contrast instead of the slope contrasts is the constant p-value across slope contrasts (see above)\nIf hours isnâ€™t specified emmeans holds hours at the sample mean.\n\nUsing {marginaleffects} and holding hours at sample mean (and probably every other variable in the model), comparisons(contcont, variables = list(effort = \"2sd\")) %&gt;% tidy()\n\nSpecification\n\nHours has a fixed value and the moderator is allowed to vary between high and low effort\nemmeans\n\nâ€œpairwise ~ hours*effort to tell the function that we want pairwise contrasts of each distinct combination of Hours and Effort specified at=mylist\n\nmarginaleffects\n\neffort = â€œ2dâ€ says compare (effort + 1 sd) to (effort - 1 sd)\ngrid.type = â€œcounterfactualâ€ because there is no observation in the data with hours = 4\n\n\nInterpretation\n\nBased on the NHST, the difference between the predicted means of high and low effort are significantly different from each other when hours = 4\nAfter 4 hours of exercise, high effort (34.8) will result in about 15 pounds more weight loss than low effort (24.5) on average.\n\n\n\n\n\n\nContinuous:Binary\n\nThe change in the slope of the continuous (IV) after a change from the reference level to the other level in the binary (Moderator)\n\nNote that this value is an additional change to the effect of the continuous (i.e when binary = 1, the effect = continuous effect + interaction effect)\n\nSignificant means that the factor variable does increase/decrease the effect of the continuous variable\nExample: Does the effect of workout duration (IV) on weight loss (Outcome) change because you are a male or female (Moderator)?\ndat &lt;- dat %&gt;%Â \nÂ  mutate(gender = factor(gender, labels = c(\"male\", \"female\")) # male = 1, female = 2,\nÂ  Â  Â  Â  gender = relevel(gender, ref = \"female\")\nÂ  )\ncontcat &lt;- lm(loss ~ hours * gender, data=dat)\n#&gt; Â  Â  Â  Â  Â  Â  Â  Â   Estimate Std. Error t value Pr(&gt;|t|)Â \n#&gt; (Intercept)Â  Â  Â  Â   3.335Â  Â  Â  2.731Â   1.221Â  Â  0.222Â \n#&gt; hoursÂ  Â  Â  Â  Â  Â  Â   3.315Â  Â  Â  1.332Â   2.489Â  Â  0.013 *\n#&gt; gendermaleÂ  Â  Â  Â  Â  3.571Â  Â  Â  3.915Â   0.912Â  Â  0.362Â \n#&gt; hours:gendermaleÂ   -1.724Â  Â  Â  1.898Â  -0.908Â  Â  0.364\n\nIntercept (b0): predicted weight loss when Hours = 0 in the reference group of Gender, which isÂ  females.\nhours (b1, Independent Variable): the marginal effect of Hours for the reference groupÂ  (e.g.Â females).\n\nhours marginal effect for males is b1 + b3 or emtrends(contcat, ~ gender, var=\"hours\") will give both marginal effects\n\ngendermale (b2, Moderator) (reference group: female): the difference in the effect of being male compared to female at hours = 0.\nhours:gendermale (b3): the difference in the marginal effects of Hours for males to Hours for females.\n\nemtrends can also give the summary stats for the interaction (sign different in this case, since difference is from the reference group, female)\nhours_slope &lt;- emtrends(contcat, pairwise ~ gender, var=\"hours\")\nhours_slope$contrasts\n#&gt; contrastÂ  Â  Â  estimateÂ   SEÂ   df t.ratio p.value\n#&gt; female - maleÂ  Â   1.72  1.9  896   0.908Â  0.3639\n\nMarginal effects of continuous (IV) by each level of the categorical (moderator)\n# via {emmeans}\nemtrends(contcat, ~ gender, var=\"hours\")\n#&gt; gender  hours.trendÂ    SEÂ    df  lower.CL  upper.CL\n#&gt; femaleÂ   Â  Â  Â  3.32  1.33  896Â  Â    0.702Â  Â    5.93\n#&gt; maleÂ  Â  Â   Â  Â  1.59  1.35  896Â     -1.063Â  Â    4.25\n\n# via {marginaleffects}\nmarginaleffects(contcat) %&gt;%\nÂ  Â  tidy(by = \"gender\")\n#&gt; Â  Â  Â  typeÂ  termÂ  Â  Â    contrast genderÂ   estimate std.error statisticÂ  Â  p.valueÂ   conf.low conf.high\n#&gt; 1 responseÂ  hoursÂ  Â  Â  Â    dY/dXÂ   male 1.59113665 1.3523002 1.1766150 0.23934921 -1.0593230Â  4.241596\n#&gt; 2 responseÂ  hoursÂ  Â  Â  Â    dY/dX female 3.31506746 1.3316491 2.4894453 0.01279426Â  0.7050833Â  5.925052\n#&gt; 3 response gender  male - femaleÂ   male 0.09616813 0.9382760 0.1024945 0.91836418 -1.7428191Â  1.935155\n#&gt; 4 response gender  male - female female 0.14194405 0.9382968 0.1512784 0.87975610 -1.6970840Â  1.980972\n\n{emmeans}\n\n~gender says obtain separate estimates for females and males\nvar=â€œhoursâ€ says obtain marginal effects (or trends) for hours\n\n{marginaleffects}\n\nCan also use summary(by = \"gender\")\n\nInterpretation\n\nhours by female is the same as the model coefficient above.\n\nSee model interpretation above\n\n\n\nSimple effects at values of the moderator\n# via {emmeans}\nmylist &lt;- list(hours=c(0,2,4),gender=c(\"male\",\"female\"))\nemcontcatÂ  &lt;- emmeans(contcat, ~ hours*gender, at=mylist)\ncontrast(emcontcat, \"pairwise\",by=\"hours\")\n#&gt; hours = 0:\n#&gt; contrastÂ  Â  Â  estimateÂ  Â  SEÂ  df t.ratio p.value\n#&gt; male - femaleÂ  Â  3.571 3.915 896Â   0.912Â  0.3619Â \n#&gt; hours = 2:\n#&gt; contrastÂ  Â  Â  estimateÂ  Â  SEÂ  df t.ratio p.value\n#&gt; male - femaleÂ  Â  0.123 0.938 896Â   0.131Â  0.8955Â \n#&gt; hours = 4:\n#&gt; contrastÂ  Â  Â  estimateÂ  Â  SEÂ  df t.ratio p.value\n#&gt; male - femaleÂ  -3.325 3.905 896   -0.851Â  0.3948\n\n# via {marginaleffects}\ncomparisons(contcat,\nÂ  Â  Â  Â  Â  Â  variables = \"gender\",\nÂ  Â  Â  Â  Â  Â  newdata = datagrid(hours = c(0, 2, 4)))\n#&gt; Â  rowidÂ  Â  typeÂ  Â  Â  Â  term contrast_gender comparison std.errorÂ  statisticÂ   p.valueÂ   conf.low conf.high gender hours\n#&gt; 1Â  Â  1 response interactionÂ   male - femaleÂ  3.5710607Â  3.914762Â  0.9122038 0.3616614Â  -4.101731 11.243853Â   maleÂ  Â   0\n#&gt; 2Â  Â  2 response interactionÂ   male - femaleÂ  0.1231991Â  0.937961Â  0.1313478 0.8955002Â  -1.715171Â  1.961569Â   maleÂ  Â   2\n#&gt; 3Â  Â  3 response interactionÂ   male - female -3.3246625Â  3.905153 -0.8513527 0.3945735 -10.978622Â  4.329297Â   maleÂ  Â   4\n\nThe moderator is now â€œhoursâ€\nNote that output for hours = 0, the effect 3.571 matches the b2 coefficient\nInterpretation\n\nas Hours increases, the male versus female difference becomes more negative (females are losing more weight than males)\n\n\nPlotting the continuous by binary interaction\n\n# via {emmeans}\nmylist &lt;- list(hours=seq(0,4,by=0.4),gender=c(\"female\",\"male\"))\nemmip(contcat, gender ~hours, at=mylist,CIs=TRUE)\n\nFor ggplot, see continuous:continuous &gt;&gt; Plot marginal effects for IV section (should be similar code)\nInterpretation\n\nAlthough it looks like thereâ€™s a cross-over interaction, the large overlap in confidence intervals suggests that the slope of Hours is not different between males and females\n\nConfirmed by looking at the standard error of the interaction coefficient (1.898) is large relative to the absolute value of the coefficient itself (1.724)\n\n\n\n\n\n\n\nCategorical:Categorical\n\nFrom {marginaleffects} Contrasts vignette, â€œSince derivatives are only properly defined for continuous variables, we cannot use them to interpret the effects of changes in categorical variables. For this, we turn to contrasts between adjusted predictions (aka Simple Effects).â€\n\ni.e.Â contrasts of marginal effects arenâ€™t possible\n\n2x3 cat:cat interaction\nExample: Do males and females (IV) lose weight (Outcome) differently depending on the type of exercise (Moderator) they engage in?\ndat &lt;- dat %&gt;%Â \nÂ  mutate(gender = factor(gender, labels = c(\"male\", \"female\")),\nÂ  Â  Â  Â  gender = relevel(gender, ref = \"female\"),\nÂ  Â  Â  Â  prog = factor(prog, labels = c(\"jog\",\"swim\",\"read\")),\nÂ  Â  Â  Â  prog = relevel(prog, ref=\"read\")\nÂ  )\ncatcat &lt;- lm(loss ~ gender * prog, data = dat)\n#&gt; Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Estimate Std. Error t value Pr(&gt;|t|)Â  Â \n#&gt; (Intercept)Â  Â  Â  Â  Â  -3.6201Â  Â   0.5322Â  -6.802 1.89e-11 ***\n#&gt; gendermaleÂ  Â  Â  Â  Â   -0.3355Â  Â   0.7527Â  -0.446Â  Â  0.656Â  Â \n#&gt; progjogÂ  Â  Â  Â  Â  Â  Â   7.9088Â  Â   0.7527Â  10.507Â  &lt; 2e-16 ***\n#&gt; progswimÂ  Â  Â  Â  Â  Â   32.7378Â  Â   0.7527Â  43.494Â  &lt; 2e-16 ***\n#&gt; gendermale:progjogÂ  Â  7.8188Â  Â   1.0645Â   7.345 4.63e-13 ***\n#&gt; gendermale:progswimÂ  -6.2599Â  Â   1.0645Â  -5.881 5.77e-09 ***\n\nDescription\n\nIntercept (b0): the intercept, or the predicted weight loss when gendermale = 0, progjog = 0, and progswim = 0\n\ni.e.Â the intercept for females in the reading program\n\ngendermale (b1): the simple effect of males when progjog = 0 and progswim = 0\n\ni.e.Â theÂ  male â€“ female weight loss in the reading group\n\nprogjog (b2): the simple effect of jogging when gendermale = 0\n\ni.e.Â the difference in weight loss between jogging versus reading for females\n\nprogswim (b3):Â  the simple effect of swimming when gendermale = 0\n\ni.e.Â the difference in weight loss between swimming versus reading for females\n\ngendermale:progjog (b4): the male effect (male â€“ female) in the jogging condition versus the male effect in the reading condition.\n\nAlso, the jogging effect (jogging â€“ reading) for males versus the jogging effect for females.\n\ngendermale:progswim (b5): the male effect (male â€“ female) in the swimming condition versus the male effect in the reading condition.\n\nAlso, the swimming effect (swimming- reading) for males versus the swimming effect for females.\n\nGender is the independent variable\nProg is the moderator\nreference groups\n\nprog == read\ngender == female\n\nb1 + b4: is the marginal effect of gender == males in the prog == jogging group.\nb1 + b5: is the marginal effect of gender == males in the prog == swimming group.\n\nMarginal Means for all combinations of the interaction variable levels\n# via {emmeans}\nemcatcat &lt;- emmeans(catcat, ~ gender*prog)\n#&gt; gender  prog  emmeanÂ  Â   SEÂ   df lower.CL upper.CL\n#&gt; female  readÂ   -3.62  0.532  894Â  Â  -4.66Â  Â  -2.58\n#&gt;   maleÂ  readÂ   -3.96  0.532  894Â  Â  -5.00Â  Â  -2.91\n#&gt; female   jogÂ  Â  4.29  0.532  894Â  Â   3.24Â  Â   5.33\n#&gt;   maleÂ   jogÂ   11.77  0.532  894Â  Â  10.73Â  Â  12.82\n#&gt; female  swimÂ   29.12  0.532  894Â  Â  28.07Â  Â  30.16\n#&gt;   maleÂ  swimÂ   22.52  0.532  894Â  Â  21.48Â  Â  23.57\n\n# via {marginaleffects}\npredictions(catcat, variables = c(\"gender\", \"prog\"))\n#&gt; Â  rowidÂ  Â  type predicted std.errorÂ  conf.low conf.high gender prog\n#&gt; 1Â  Â  1 response 11.772028 0.5322428 10.727437 12.816619Â   maleÂ  jog\n#&gt; 2Â  Â  2 response 22.522383 0.5322428 21.477792 23.566974Â   male swim\n#&gt; 3Â  Â  3 response -3.955606 0.5322428 -5.000197 -2.911016Â   male read\n#&gt; 4Â  Â  4 responseÂ  4.288681 0.5322428Â  3.244091Â  5.333272 femaleÂ  jog\n#&gt; 5Â  Â  5 response 29.117691 0.5322428 28.073100 30.162282 female swim\n#&gt; 6Â  Â  6 response -3.620149 0.5322428 -4.664740 -2.575559 female read\n\nMarginal means generated using the regression coefficients\n\n\nKinda weird. Since these are predicted means (i.e.Â outcome values), I wouldnâ€™t have thought they could be generated by the model coefficients. Â¯\\_(ãƒ„)_/Â¯\n\nInterpretation\n\nfemales in the reading program have an estimated weight gain of 3.62 pounds\nmales in the swimming program have an average weight loss of 22.52 pounds\n\n\nSimple Effects of the IV for all values of the moderator variable\n# via {emmeans}\ncontrast(emcatcat, \"revpairwise\", by=\"prog\") # see above code chunk for emcatcat\n#&gt; prog = read:\n#&gt; contrastÂ  Â  Â  estimateÂ  Â  SEÂ  df t.ratio  p.value\n#&gt; male - femaleÂ   -0.335 0.753 894  -0.446Â   0.6559Â \n#&gt; prog = jog:\n#&gt; contrastÂ  Â  Â  estimateÂ  Â  SEÂ  df t.ratio  p.value\n#&gt; male - femaleÂ  Â  7.483 0.753 894Â   9.942Â   &lt;.0001Â \n#&gt; prog = swim:\n#&gt; contrastÂ  Â  Â  estimateÂ  Â  SEÂ  df t.ratio  p.value\n#&gt; male - femaleÂ   -6.595 0.753 894  -8.762Â   &lt;.0001\n\n# via {marginaleffects}\ncomparisons(catcat,Â \nÂ  Â  Â  Â  Â  Â  variables = \"gender\",Â \nÂ  Â  Â  Â  Â  Â  newdata = datagrid(prog = c(\"read\", \"jog\", \"swim\")))\n#&gt; Â  rowidÂ  Â  typeÂ  Â  Â  Â  term contrast_gender comparison std.errorÂ  statisticÂ  Â  Â  p.valueÂ  conf.low conf.high gender prog\n#&gt; 1Â  Â  1 response interactionÂ   male - female -0.3354569 0.7527049 -0.4456686 6.558367e-01 -1.810731Â  1.139818Â   male read\n#&gt; 2Â  Â  2 response interactionÂ   male - femaleÂ  7.4833463 0.7527049Â  9.9419388 2.734522e-23Â  6.008072Â  8.958621Â   maleÂ  jog\n#&gt; 3Â  Â  3 response interactionÂ   male - female -6.5953078 0.7527049 -8.7621425 1.915739e-18 -8.070582 -5.120033Â   male swim\n\n{emmeans}\n\nâ€œrevpairwiseâ€ rather than â€œpairwiseâ€ because by default the reference group (female) would come first\nThe results are with the arg, adjust=â€œnoneâ€, which removes the Tukey multiple test adjustment. Removed it from the code, because itâ€™s bad practice.\n\nInterpretation\n\nmale effect for jogging and swimming are significant at the 0.05 level (with no adjustment of p-values), but not for the reading condition.\nmales lose more weight in the jogging condition (positive) but females lose more weight in the swimming condition (negative)\n\n\nPlotting the categorical by categorical interaction\n\n{emmeans} emmip(catcat, prog ~ gender,CIs=TRUE)\n\n{ggplot2}\n\ncatcatdat &lt;- emmip(catcat, gender ~ prog, CIs=TRUE, plotit=FALSE)\n#&gt; Â  gender progÂ  Â  Â  yvarÂ  Â  Â  Â  SEÂ  dfÂ  Â  Â   LCLÂ  Â  Â   UCLÂ   tvar xvar\n#&gt; 1 female read -3.620149 0.5322428 894 -4.664740 -2.575559 female read\n#&gt; 2Â   male read -3.955606 0.5322428 894 -5.000197 -2.911016Â   male read\n#&gt; 3 femaleÂ  jogÂ  4.288681 0.5322428 894Â  3.244091Â  5.333272 femaleÂ  jog\n#&gt; 4Â   maleÂ  jog 11.772028 0.5322428 894 10.727437 12.816619Â   maleÂ  jog\n#&gt; 5 female swim 29.117691 0.5322428 894 28.073100 30.162282 female swim\n#&gt; 6Â   male swim 22.522383 0.5322428 894 21.477792 23.566974Â   male swim\n\n# via {marginaleffects}\npredictions(catcat, variables = c(\"gender\", \"prog\"))\n\nggplot(data=catcatdat, aes(x=prog,y=yvar, fill=gender)) +\nÂ  Â  geom_bar(stat=\"identity\",position=\"dodge\") +\nÂ  Â  geom_errorbar(position=position_dodge(.9),width=.25, aes(ymax=UCL, ymin=LCL),alpha=0.3)\nÂ  Â  labs(x=\"Program\", y=\"Weight Loss\", fill=\"Gender\")\n\nI think I like the emmeans plot better\nValues come from the Marginal Means for all combinations of the interaction variable levels section above"
  },
  {
    "objectID": "qmd/regression-interactions.html#sec-reg-inter-logreg",
    "href": "qmd/regression-interactions.html#sec-reg-inter-logreg",
    "title": "Interactions",
    "section": "Logistic Regression",
    "text": "Logistic Regression\n\nMisc\n\n* categorical:categorical and continuous:continuous should be should be similar to analysis for binary:continuous and the code/analysis in the OLS section *\n\nFor more details on these other interaction types see the article linked below\n\nMisc\n\nNotes from\n\nHow Can I Understand A Categorical By Continuous Interaction In Logistic Regression\nDeciphering Interactions In Logistic Regression\n\nSee this article for continuous:continuous, cat:cat\n\n\nAlso see\n\nRegression, Logistic &gt;&gt; Marginal Effects\nPost-Hoc Analysis, emmeans &gt;&gt; Logistic Regression\n\nReminder: OR significance is being different from 1 and log odds ratios (logits) significance is being different from 0\nAlthough the interaction is quantified appropriately by the interaction coefficient when interpreting effects on the scale of log-odds or log-counts, the interaction effect will not reduce to the coefficient of the the interaction coefficient when describing effects on the natural response scale (i.e.Â probabilities, counts) because of the inverse-link transformation.\n\nIn order to get e.g.Â probabilities, the inverse-link function must be used, and this complicates the derivatives of the marginal effect calculation. The chain rule must be appied. This means the interaction effect is no longer equivalent to the interaction coefficient.\nTherefore, effects or contrasts are differences in marginal effects rather than coefficients\n\nInteraction effects represent change in a marginal effect of one predictor for a change in another predictoreraction effects represent change in a marg\n\n\nStandard errors of point estimate values of interaction effects (as well as their corresponding statistical significance) may also vary as a function of the predictors.\nNeed to decide whether the interaction is to be conceptualized in terms of log odds (logits) or odds ratios or probability\n\nAn interaction that is significant in log odds may not be significant in terms of probability or vice versa\n\nEven if they do agree, the p-values do not have the be the same. (See below, binary:continuous &gt;&gt; Simple Effects &gt;&gt; second example &gt;&gt; probabilities)\n\n\n\n\n\n\nBinary:Continuous\n\nExample:\n\nDescription\ndat_clean &lt;- dat %&gt;%Â \nÂ  mutate(f = factor(f, labels = c(\"female\", \"male\")))\nconcat &lt;- glm(y ~ f*s, family = binomial, data = dat_clean)\nsummary(concat)\n#&gt; Â  Â  Â  Â  Â  Â  Estimate  Std. Error  z value  Pr(&gt;|z|)Â  Â \n#&gt; (Intercept) -9.25380Â  Â   1.94189Â   -4.765  1.89e-06 ***\n#&gt; fmaleÂ  Â  Â  Â  5.78681Â  Â   2.30252Â    2.513Â    0.0120 *Â \n#&gt; sÂ  Â  Â  Â  Â  Â  0.17734Â  Â   0.03644Â    4.867  1.13e-06 ***\n#&gt; fmale:sÂ  Â   -0.08955Â  Â   0.04392Â   -2.039Â    0.0414 *\n\ny: binary outcome\ns (IV): continuous variable\nf (moderator): gender variable with female as the reference\ncv1: continuous adjustment variable\n\nMarginal effects of continuous (IV) by each level of the categorical (moderator)\n# via {marginaleffects}\nmarginaleffects(concat, variables = \"s\") %&gt;%\nÂ  Â  tidy(by = \"f\")\n#&gt; Â  Â  Â  type termÂ  Â  Â   fÂ   estimateÂ   std.error statisticÂ  Â   p.valueÂ   conf.lowÂ  conf.high\n#&gt; 1 responseÂ  Â  sÂ    male 0.01473352 0.003237072Â  4.551498 5.32654e-06 0.00838898 0.02107807\n#&gt; 2 responseÂ  Â  s  female 0.02569069 0.001750066 14.679842 0.00000e+00 0.02226062 0.02912075\n\nInterpetation\n\nIncreasing s by 1 unit given that the person is male will result in around a 1% increase on average in the probability of the event, y.\n\n\nSimple effects at values of the IV\n# via {marginaleffects}\ncomp &lt;- comparisons(concat2,Â \nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  variables = \"f\",Â \nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  newdata = datagrid(s = seq(20, 70, by = 2)))\nhead(comp)\n#&gt; Â  rowidÂ  Â  typeÂ  Â  Â  Â  termÂ  Â  contrast_f comparisonÂ  std.error statisticÂ  Â  p.valueÂ  Â   conf.low conf.highÂ  Â  fÂ  s\n#&gt; 1Â  Â  1 response interaction male - femaleÂ  0.1496878 0.09871431Â  1.516374 0.12942480 -0.043788674 0.3431643 male 20\n#&gt; 2Â  Â  2 response interaction male - femaleÂ  0.1724472 0.10430544Â  1.653291 0.09827172 -0.031987695 0.3768821 male 22\n#&gt; 3Â  Â  3 response interaction male - femaleÂ  0.1975119 0.10886095Â  1.814351 0.06962377 -0.015851616 0.4108755 male 24\n#&gt; 4Â  Â  4 response interaction male - femaleÂ  0.2246979 0.11209440Â  2.004542 0.04501204Â  0.004996938 0.4443989 male 26\n#&gt; 5Â  Â  5 response interaction male - femaleÂ  0.2536377 0.11376972Â  2.229395 0.02578761Â  0.030653136 0.4766222 male 28\n#&gt; 6Â  Â  6 response interaction male - femaleÂ  0.2837288 0.11375073Â  2.494303 0.01262048Â  0.060781436 0.5066761 male 30\n\nShows the predicted value contrasts for the moderator at different values of the independent variable.\n\nâ€œconcat2â€ model defined in â€œSimple Effects at values of the moderator and adjustment variableâ€ (see below)\n\nFor comparisons , the default is type = â€œresponseâ€ so the output is probabilities. (type = â€œlinkâ€ for log-ORs aka logits)\nInterpretation\n\nFor s = 26, 28, and 30, there is substantial evidence that there are differences in male and female probabilites of the event, y.\n\nExample: (article)\n\nDescription\n\nlowbwt: binary outcome, low birthweight = 1\nage (IV): continuous, age of the mother\nftv (moderator): binary, whether or not the mother made frequent physician visits during the first trimester of pregnancy\n\nSimple Effects: Odds Ratios\n\n\nORftv is the odds-ratio contrast of (ftv = 1) - (ftv = 0) at the specified age of the mother\nInterpretation\n\na mother at the age of 30 who visits the physician frequently has 0.174 times the odds of having a low birth weight baby as compared to those of the same age who donâ€™t visit the doctor frequently\nFor women whose ages are between 17 and 24, the 95% confidence intervals of the odds ratios include the null value of 1, so we do not have strong evidence of an association between frequent doctor visits and low birth weight for that age range.\nFor mothers aged 25 years and older, the odds of having a low birth weight baby significantly decrease if the mother frequently visits her physician.\n\n\nSimple Effects: Probabilities\n\n\nâ€œDifference in probabilityâ€ is the probability contrast of (ftv = 1) - (ftv = 0) at the specified age of the mother\nNote that the results agree with the OR results, but the pvals are different\n\nSo itâ€™s probably possible that in terms of â€œsignificance,â€ you could get different conclusions based on the metric used.\n\nInterpretation\n\nFor young mothers (less than 24 years old), we do not have strong evidence of an association between low birth weight and frequent physician visits.\nFor mothers aged 25 years and older, we reject the null hypothesis of no difference in probability between those with frequent physician visits and those without.\nOverall, the difference between the probability of low birth weight comparing those with frequent visits to those without increases as the motherâ€™s age increases.\n\n\n\n\nPlot the simple effects\n\nggplot(comp, aes(x = s, y = comparison)) +Â \nÂ  geom_line() +\nÂ  geom_ribbon(aes(ymax=conf.high, ymin=conf.low), alpha=0.4) +Â \nÂ  geom_hline(yintercept=0, linetype=\"dashed\") +\nÂ  labs(title = \"Male-Female Difference\", y=\"Probability Contrast\")\n\n{marginaleffects}\n\nYou change the comparisons (see above) type arg to â€œtype = linkâ€ to get log-ORs (aka logits) instead of probabilities and then plot\n\n\nInterpretation\n\nThe difference in probabilities for male and females is statistically significant between values of â€œsâ€ approximately between 28 to 55.\n\n\nSimple Effects at values of the moderator and adjustment variable\nconcat2 &lt;- glm(y ~ f*s + cv1, family = binomial, data = dat_clean)\ncomp2 &lt;- comparisons(concat2,Â \nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  variables = \"f\",Â \nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  newdata = datagrid(s = seq(20, 70, by = 2),\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  cv1 = c(40, 50, 60)))\nhead(comp2)\n#&gt; Â  rowidÂ  Â  typeÂ  Â  Â  Â  termÂ  Â  contrast_f comparisonÂ  std.error statisticÂ  Â  Â  p.valueÂ  Â  conf.low conf.highÂ  Â  fÂ  s cv1\n#&gt; 1Â  Â  1 response interaction male - femaleÂ  0.2307214 0.15001700Â  1.537968 1.240564e-01 -0.06330656 0.5247493 male 20Â  40\n#&gt; 2Â  Â  2 response interaction male - femaleÂ  0.6603841 0.20879886Â  3.162777 1.562722e-03Â  0.25114590 1.0696224 male 20Â  50\n#&gt; 3Â  Â  3 response interaction male - femaleÂ  0.9135206 0.07865839 11.613773 3.507873e-31Â  0.75935304 1.0676882 male 20Â  60\n#&gt; 4Â  Â  4 response interaction male - femaleÂ  0.2361502 0.14294726Â  1.652009 9.853270e-02 -0.04402131 0.5163217 male 22Â  40\n#&gt; 5Â  Â  5 response interaction male - femaleÂ  0.6663811 0.19447902Â  3.426494 6.114278e-04Â  0.28520927 1.0475530 male 22Â  50\n#&gt; 6Â  Â  6 response interaction male - femaleÂ  0.9097497 0.07571970 12.014703 2.974362e-33Â  0.76134178 1.0581575 male 22Â  60\nPlot simple effects and facet by values of the adjustment variable\n\nggplot(comp2, aes(x = s, y = comparison, group = factor(cv1))) +Â \nÂ  geom_line() +\nÂ  geom_ribbon(aes(ymax=conf.high, ymin=conf.low), alpha=0.4) +Â \nÂ  geom_hline(yintercept=0, linetype=\"dashed\") +\nÂ  facet_wrap(~ cv1) +\nÂ  labs(title = \"Male-Female Difference\", y=\"Probability Contrast\")\n\nInterpretation\n\nIt seems clear from looking at the three graphs that the male-female difference in probability increases as cv1 increases except for high values of s."
  },
  {
    "objectID": "qmd/regression-interactions.html#sec-reg-inter-poisnegbin",
    "href": "qmd/regression-interactions.html#sec-reg-inter-poisnegbin",
    "title": "Interactions",
    "section": "Poisson/Neg.Binomial",
    "text": "Poisson/Neg.Binomial\n\nMisc\n\nNotes from Paper Interpreting interaction effects in generalized linear models of nonlinear probabilities and counts\nAlthough the interaction is quantified appropriately by the interaction coefficient when interpreting effects on the scale of log-odds or log-counts, the interaction effect will not reduce to the coefficient of the the interaction coefficient when describing effects on the natural response scale (i.e.Â probabilities, counts) because of the inverse-link transformation.\n\nIn order to get e.g.Â probabilities, the inverse-link function must be used, and this complicates the derivatives of the marginal effect calculation. The chain rule must be appied. This means the interaction effect is no longer equivalent to the interaction coefficient.\nTherefore, effects or contrasts are differences in marginal effects rather than coefficients\n\nInteraction effects represent change in a marginal effect of one predictor for a change in another predictoreraction effects represent change in a marg\n\n\nStandard errors of point estimate values of interaction effects (as well as their corresponding statistical significance) may also vary as a function of the predictors.\n\nTypes\n\nContinuous:Continuous â€” the rate-of-change in the marginal effect of one predictor given a change in another predictor\n\nAlso, the amount the effect of x1 on E[Y|x] changes for every one-unit increase in x2 (and vice versa), holding all else constant.\n\nContinuous:Discrete â€” difference in the marginal effect of the continuous predictor between two selected values of the discrete predictor\n\nExample: Poisson outcome (population)\n\n\nx1 is continuous, x2 is continuous, x3 is binary (e.g.Â sex where female = 1).\nThe interaction effect, Î³213,Â  of x1x3 is the difference between the marginal effect of x1 for females versus males\nNote that x2 is included in the effect calculation for an interaction it isnâ€™t part of\nSpecific values for x1 and x2 must be chosen to the calculate the value of the effect\n\n\nFor x2, the 25th, 50th, 75th quartiles are used and x1 would be held at one value of substantive interest (e.g.Â mean)\nStandard errors via delta method for all 3 values can be computed along with p-values\n\nVisual\n\n\nx2 is held at its mean, and various values of x1 are used\nInterpretation: the expected population (count of Y) is larger among females than males across all values of x1 with x2 held at its mean.\n\n\n\nDiscrete : Discrete â€” the difference between the model evaluated at two categories of one variable (a, b) minus the difference in this model evaluated at two categories of another variable (c, d) (aka discrete double difference)"
  },
  {
    "objectID": "qmd/regression-linear.html#sec-reg-lin-misc",
    "href": "qmd/regression-linear.html#sec-reg-lin-misc",
    "title": "Linear",
    "section": "Misc",
    "text": "Misc\n\nGuide for suitable baseline models: link\nâ€œFor estimation of causal effects, it does typically not suffice to well control for confounders, we also need a sufficiently strong source of exogenous variation for our explanatory variable of interest.â€\n\nFrom Empirical Economics with R: Confounders, Proxies, and Sources of Exogenuous Variations\nEven if you have good proxy variables for the true confounder(s) and use them as controls, estimated effects of explanatory variables with low s.d. (with mean = 0, e.g.Â low 0.02, high 1) will still be inflated If the regression conditions arenâ€™t met - for instance, if heteroskedasticity is present - then the OLS estimator is still unbiased but it is no longer best. Instead, a variation called generalized least squares (GLS) will be Best Linear Unbiased Estimator (BLUE)\n\nA model is said to be hierarchical if it contains all the lesser degree terms in the hierarchy\n\nExample\n\nhierarchical: y = x + x2 + x3 + x4\nnot hierarchical: y = x + x4\nhierarchical: y = x1 + x2 + x1x2\nnot hierarchical: y = x1 + x1x2\n\nIt is expected that all polynomial models should have this property because only hierarchical models are invariant under linear transformation.\n\nRcppArmadillo::fastLmPure Not sure what this does but itâ€™s rcpp so maybe faster than lm for big data.\n.lm.fit is a base R lm function that is 30%-40% faster than lm."
  },
  {
    "objectID": "qmd/regression-linear.html#sec-reg-lin-ass",
    "href": "qmd/regression-linear.html#sec-reg-lin-ass",
    "title": "Linear",
    "section": "Assumptions",
    "text": "Assumptions\n\nAssumptions\n\nThe true relationship is linear\n\ni.e.Â a linear relationship between the independent variable, x, and the dependent variable, y.\n\nErrors are normallyÂ distributed\n\ni.e.Â random fluctuations around the true line\n\nHomoscedasticity of errors (or, equal variance around the line)\n\ni.e.Â variability in the response doesnâ€™t increase as the value of the predictor increases\n\nIndependence of the observations\n\ni.e.Â no autocorrelation\n\n\nCheck residuals\n\nhave a constant variance (Homoscedasticity of errors)\nbe approximately normally distributed (with a mean of zero) (Errors are normally distributed)\nbe independent of one another (independence, no autocorrelation)\n\nGelmanâ€™s order of importance (post)\n\nValidity. Most importantly, the data you are analyzing should map to the research question you are trying to answer. This sounds obvious but is often overlooked or ignored because it can be inconvenient\nAdditivity and linearity. The most important mathematical assumption of the regression model is that its deterministic component is a linear function of the separate predictors\nIndependence of errors\nEqual variance of errors\nNormality of errors - â€œNormality and equal variance are typically minor concerns, unless youâ€™re using the model to make predictions for individual data points.â€"
  },
  {
    "objectID": "qmd/regression-linear.html#sec-reg-lin-eda",
    "href": "qmd/regression-linear.html#sec-reg-lin-eda",
    "title": "Linear",
    "section": "EDA",
    "text": "EDA\n\nPreliminaryÂ \n\nSet seed\nCheck for missing values\nSee how many predictors youâ€™re allowed to use given the sample size. Check bkmk: statistics â€“&gt; sample size folder for Harrell articles\nremove predictors with zero variance recipes::step_zv(all_predictors()).\n\nCheck correlation matrix. greybox::assoc()\nCheck s.d. in predictors\n\nwithout enough variation in predictors (with mean = 0, e.g.Â low sd =Â  0.02, highÂ  = 1), effects will be inflated even after controlling for potential confounders\n\nLook at series of pairwise plots: see which independent variables are linearly related to your dependent variable as they would be good predictors. Also look forÂ  non-linearityÂ  relations. See which of your independent variables are linearly related to each other as this indicates collinearity. Use greybox::spread w/log = false for linear relationships and log = true for nonlinearÂ \nCollinearity can cause sign flips of coefficients as variables are added or taken away when we create our model. Use vif() and greybox::determ pg 121Â \nIn pairwise for prospective predictors, look for and make note of potential outlier points.\nLook at histograms of variables to check for skewness. We want normal distributions but distributions that are all similarly skewed isnâ€™t too bad either.\nLook at summary statistics and examine the difference between the median and the mean as this indicates skewness.\nSkewness metric?\nLook at box plots to check for skewness and outliers. Investigate at outliers to see if they can be removed as they can have an outsized effect that causes inaccuracy in your model.\nIs the outcome variable is multi-modal?\n\nRegression, Other\nSolutions:\n\nQuantile Regression (See Regression, Quantile)\nMixture models"
  },
  {
    "objectID": "qmd/regression-linear.html#sec-reg-lin-tran",
    "href": "qmd/regression-linear.html#sec-reg-lin-tran",
    "title": "Linear",
    "section": "Transformations",
    "text": "Transformations\n\nMisc\n\nSee Feature Engineering, General &gt;&gt; Continuous &gt;&gt; Transformations\n\nStandardization\n\nMethods\n\n{effectsize}\n\nCan also be used to standardize coefficients after the model has been fit\n\nCan also be done by multiplying the coefficient by the sd of the variable\n\nGood docs - applications, reliability, etc. of the many methods for mixed models, interaction terms are explained.\n\nCentering and Scaling\nmedian and MAD\n\nrobust\n\nMean and (2 â¨¯ std dev)\n\nGelman (2008)\nAllows for factor variables to also be compared to numeric variables in terms of importance\n\nMedian and IQR\nMean and Gini Mean Difference\n\n\nScaling\n\nvar/max(var)\n\nMaking the largest value = 1 and the theoretical minimum = 0\n\n\nSquaring (quadratic)\n\nPredictors should be standardized.\n\nSquaring variables can produce some very large numbers and the numerical estimations of the effects can be get screwed up.\nCentering fixes collinearity issues when creating powers and interaction terms (CV post)\n\nCollinearity between the created terms and the main effects\n\n\n\nLog\n\nTaking the log of a measure translates the measure into magnitudes. So by using the logarithm of a predictor, weâ€™re saying that we suspect that the magnitude of the predictor is related to outcome, in a linear fashion."
  },
  {
    "objectID": "qmd/regression-linear.html#sec-reg-lin-modsel",
    "href": "qmd/regression-linear.html#sec-reg-lin-modsel",
    "title": "Linear",
    "section": "Model Selection",
    "text": "Model Selection\n\nHarrell says use all variables unless number of variables , pÂ  &gt; (nrow(df) / 15), then use Regularized Regression\nSee Harrell sample size biostat paper (above). Has overview of approaches (I think).\nUse best subsets with Mallowâ€™s CP, AIC, and BIC as metrics for choosing best model. See R notebook, pg 121 and bkmk regressionÂ â€“&gt; multivariable folder for PSU stat 501\nCheck for partial correlations between predictor variables and outcome variable (see notebook and bookmarks) as a possible avenue of feature reduction and as a way of gauging true strength of the relationship between the predictor variables and outcome variable"
  },
  {
    "objectID": "qmd/regression-linear.html#sec-reg-lin-interp",
    "href": "qmd/regression-linear.html#sec-reg-lin-interp",
    "title": "Linear",
    "section": "Interpretation",
    "text": "Interpretation\n\nMarginal Effect - The effect of the predictor after taking into account the effects of all the other predictors in the model\nsummary\n\nStandard Errors: An estimate of how much estimates would â€˜bounce aroundâ€™ from sample to sample, if we were to repeat this process over and over and over again.\n\nMore specifically, it is an estimate of the standard deviation of the sampling distribution of the estimate.\n\nt-score: Ratio of parameter estimate and its SE\n\nused for hypothesis testing, specifically to test whether the parameter estimate is â€˜significantlyâ€™ different from 0.\n\np-value: The probability of finding an estimated value that is as far or further from 0, if the null hypothesis were true.\n\nNote that if the null hypothesis is not true, it is not clear that this value is telling us anything meaningful at all.\n\nF-statistic: This â€œsimultaneousâ€ test checks to see if the model as a whole predicts the response variable better than chance alone.\n\ni.e.Â whether or not all the estimates should be considered unable to be differentiated from 0\nIf this statisticâ€™s p-value &lt; 0.05, then this suggests that at least some of the parameter estimates are not equal to 0\nUnless you only have an intercept model, you have multiple tests (e.g.Â variables + intercept p-values). There is no protection from the problem of multiple comparisons without this.\n\nBear in mind that because p-values are random variablesâ€“whether something is significant would vary from experiment to experiment, if the experiment were re-runâ€“it is possible for these to be inconsistent with each other.\n\n\n\nIntercept\n\nIf variables are centered so that the predictors have mean 0, this is the expected value of Yi when the predictor values are set to their means.\nIf variables are not centered so that the predictors have mean 0, this is the expected value of Yi when the predictors are set to 0.\n\nmay not be a realistic or interpretable situation (e.g.Â what if the predictors were height and weight?)\n\n\nStandardized Predictors\n\nDimensions are in common units (standard deviations), and the\nCoefficients\n\nSimilar to Pearson Correlation Coefficients (r-scores)\nCanâ€™t be compared to other standardized coefficients to determine importance\n\nUnless the â€œtrueâ€ r-score value is zero, the estimate is driven in large part by the range of values of the predictor in the sample\nThe best that can ever be said is that variability in one explanatory variable within a specified range is more important to determining the level of the response than variability in another explanatory variable within another specified range\n\nâ€œvariabilityâ€ makes sense since the units are standard deviations\nTrying to interpret these coefficients in terms of â€œimportanceâ€ is probably not practically useful then.\n\nUnless the data is large and/or the range of the predictors is representative of the population\n\n\n\n\nDoes one coefficient have a larger effect than the other?\n\nExample: does extra hours work have a statistically larger effect on salary than does number_compliments_to_the_boss\n\nFrom Testing The Equality of Regression Coefficients\n\nlibrary(parameters)\nlibrary(effectsize)\ndata(\"hardlyworking\", package = \"effectsize\")Â \nhardlyworkingZ &lt;- standardize(hardlyworking)\nm &lt;- lm(salary ~ xtra_hours + n_comps, data = hardlyworkingZ)Â \nmodel_parameters(m)\n#&gt; ParameterÂ  | Coefficient |Â  SE |Â  Â  Â  Â  95% CI |Â  Â  t(497) |Â  Â  Â  p\n#&gt; ---------------------------------------------------------------------\n#&gt; (Intercept) |Â  -7.19e-17 | 0.02 | [-0.03, 0.03] | -4.14e-15 | &gt; .999\n#&gt; xtra_hoursÂ  |Â  Â  Â  Â  0.81 | 0.02 | [ 0.78, 0.84] |Â  Â  46.60 | &lt; .001\n#&gt; n_compsÂ  Â  |Â  Â  Â  Â  0.41 | 0.02 | [ 0.37, 0.44] |Â  Â  23.51 | &lt; .001\n\n# method 1\n# assume both variables have same coeff and test if the models are different\nm0 &lt;- lm(salary ~ I(xtra_hours + n_comps), data = hardlyworkingZ)Â \nmodel_parameters(m0)\nanova(m0, m) # Yes they are different and therefore xtra_hours does have a stronger effect\n#&gt; Analysis of Variance Table\n#&gt;Â \n#&gt; Model 1: salary ~ I(xtra_hours + n_comps)\n#&gt; Model 2: salary ~ xtra_hours + n_comps\n#&gt;Â  Res.DfÂ  Â  RSS Df Sum of SqÂ  Â  Â  FÂ  Â  Pr(&gt;F)Â  Â \n#&gt; 1Â  Â  498 113.67Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â \n#&gt; 2Â  Â  497Â  74.95Â  1Â  Â  38.716 256.73 &lt; 2.2e-16 ***\n\n# method 2\nlibrary(car)Â \nlinearHypothesis(m, c(\"xtra_hours - n_comps\"))\n#&gt; Linear hypothesis test\n#&gt;Â \n#&gt; Hypothesis:\n#&gt; xtra_hours - n_comps = 0\n#&gt;Â \n#&gt; Model 1: restricted model\n#&gt; Model 2: salary ~ xtra_hours + n_comps\n#&gt;Â \n#&gt;Â  Res.DfÂ  Â  RSS Df Sum of SqÂ  Â  Â  FÂ  Â  Pr(&gt;F)Â  Â \n#&gt; 1Â  Â  498 113.67Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â \n#&gt; 2Â  Â  497Â  74.95Â  1Â  Â  38.716 256.73 &lt; 2.2e-16 ***\n\n# method 3\nlibrary(emmeans)Â \ntrends &lt;- rbind(\nÂ  emtrends(m, ~1, \"xtra_hours\"),\nÂ  emtrends(m, ~1, \"n_comps\")\n)Â \n# clean up so it does not error later\ntrends@grid# Regression, Gaussian\n\n1` &lt;- c(\"xtra_hours\", \"n_comps\")\ntrends@misc$estName &lt;- \"trend\"\n\ntrends\n#&gt;Â  1Â  Â  Â  Â  Â  trendÂ  Â  SEÂ  df lower.CL upper.CL\n#&gt;Â  xtra_hours 0.811 0.0174 497Â  Â  0.772Â  Â  0.850\n#&gt;Â  n_compsÂ  Â  0.409 0.0174 497Â  Â  0.370Â  Â  0.448\n#&gt;Â \n#&gt; Confidence level used: 0.95Â \n#&gt; Conf-level adjustment: bonferroni method for 2 estimates\n\npairs(trends)\n#&gt;Â  contrastÂ  Â  Â  Â  Â  Â  estimateÂ  Â  SEÂ  df t.ratio p.value\n#&gt;Â  xtra_hours - n_compsÂ  Â  0.402 0.0251 497 16.023Â  &lt;.0001\n\n\nQuadratic\n\nÂµi = Î± + Î²1xi + Î²2xi2\n\nÎ± is the intercept. Tells us the expected value of Âµ when x is 0\n\nÎ²1 and Î²2 are linear and square components of the regression curve, respectively"
  },
  {
    "objectID": "qmd/regression-linear.html#sec-reg-lin-rep",
    "href": "qmd/regression-linear.html#sec-reg-lin-rep",
    "title": "Linear",
    "section": "Report",
    "text": "Report\n\nOutliers\n\nList any outliers that were removed and reasons why.\nReport results with and without outliers"
  },
  {
    "objectID": "qmd/regression-logistic.html#sec-reg-log-misc",
    "href": "qmd/regression-logistic.html#sec-reg-log-misc",
    "title": "Logistic",
    "section": "Misc",
    "text": "Misc\n\nAlso see\n\nClassification\nRegression, Regularized &gt;&gt; Misc, Firthâ€™s Estimator\n\nPackages\n\n{glmnet}\n\nglm needs the outcome to be numeric 0/1 (not factor)\nRegularized Logistic Regression is most necessary when the number of candidate predictors is large in relationship to the effective sample size 3np(1âˆ’p) where p is the proportion of Y=1 Harrell\nSample size requirements\n\nThese are conservative estimates. Sample size estimates assume an event probability of 0.50.\nLogistic Regression: (Harrell, link)\n\nFor just estimating the intercept and a margin of error for predicted probabilities of 0.1\n\nWith no covariates (i.e.Â population is homogeneous), n = 96\nWith 1 categorical covariate, n = 96 for each level of the covariate\n\ne.g.Â For gender, you need 96 males and 96 females\n\n\nFor just estimating the intercept and a margin of error for predicted probabilities of 0.05\n\nWith no covariates (i.e.Â population is homogeneous), n = 384\nIf true probabilities of event (and non-event) are known to be extreme, i.e.Â \\(p \\notin [0.2, 0.8]\\), n = 246\n\nFor estimating predicted probabilities with 1 continuous predictor\n\nFor a margin of error of 0.1, n = 150\nFor a margin of error of 0.07, n = 300\n\n\nRF: 200 events per candidate feature (Harrell, link)\n\nâ€œStableâ€ AUC requirements for 0/1 outcome:\n\npaper: Modern modelling techniques are data hungry: a simulation study for predicting dichotomous endpoints | BMC Medical Research Methodology | Full Text\nLogistic Regression: 20 to 50 events per predictor variable\nRandom Forest and SVM: greater than 200 to 500 events per predictor variable\n\nNon-Collapsibility: The conditional odds ratio (OR) or hazard ratio (HR) is different from the marginal (unadjusted) ratio even in the complete absence of confounding.\nDonâ€™t use percents to report probabilities (Harrell)\n\nExamples (Good)\n\nThe chance of stroke went from 0.02 to 0.03\nThe chance of stroke increased by 0.01 (or the absolute chance of stroke increased by 0.01)\nThe chance of stroke increased by a factor of 1.5\nIf 0.02 corresponded to treatment A and 0.03 corresponded to treatment B: treatment A multiplied the risk of stroke by 2/3 in comparison to treatment B.\nTreatment A modified the risk of stroke by a factor of 2/3\nThe treatment A: treatment B risk ratio is 2/3 or 0.667.\n\n\nRCTs (notes from Harrell)\n\noutcome ~ treatment\n\nThese simple models are for homogeneous patient populations, i.e.,Â  patients with no known risk factors\nWhen heterogeneity (patients have strong risk factors) is present, patients come from a mixture of distributions and this causes the treatment effect to shrink towards 0 in logistic and cox-ph models. (see Ch. 13 for in Harrellâ€™s biostats book details\n\nIn a linear model, this heterogeneity (i.e.Â risk factors) thatâ€™s unaccounted for gets absorbed into the error term (residual variance â†‘, power â†“), but logistic/cox-ph models donâ€™t have residuals so the treatment effect shrinks as a result.\n\n\noutcome ~ treatment + risk_factor_vars\n\nAdjusting for risk factors stops a loss of power but never increases power like it does for linear models.\n\nIn Cox and logistic models there are no residual terms, and unaccounted outcome heterogeneity has nowhere to go. So it goes into the regression coefficients that are in the model, attenuating them towards zero. Failure to adjust for easily accountable outcome heterogeneity in nonlinear models causes a loss of power as a result.\nModeling is a question of approximating the effects of baseline variables that explain outcome heterogeneity. The better the model the more complete the conditioning and the more accurate the patient-specific effects that are estimated from the model. Omitted covariates or under-fitting strong nonlinear relationships results in effectively conditioning on only part of what one would like to know. This partial conditioning still results in useful estimates, and the estimated treatment effect will be somewhere between a fully correctly adjusted effect and a non-covariate-adjusted effect."
  },
  {
    "objectID": "qmd/regression-logistic.html#sec-reg-log-interp",
    "href": "qmd/regression-logistic.html#sec-reg-log-interp",
    "title": "Logistic",
    "section": "Interpretation",
    "text": "Interpretation\n\nMisc\n\nAlso see Visualization &gt;&gt; Log Odds Ratios vs Odds Ratios\nâ€œDivide by 4â€ shortcut (Gelman)\n\nValid for coefficients and standard errors\nâ€œ1 unit increase in X results in a Î²/4 increase in probability that Y = 1â€\nAlways round down\nBetter appproximations when the probability is close to 0.50\n\n\nDefinitions\n\nEffect is non-linear in p\n\nProbability prediction:\n\nOdds Ratio - Describes the percent change in odds of the outcome based on a one unit increase in the input variable.\n\nOR significance is being different from 1 and log odds ratios (logits) significance is being different from 0\nlogit(p) = 0, p = 0.5, OR = 1\nlogit(p) = 6, p = always (close to 1)\nlogit(p) = -6, p = never (close to 0)\nGuidelines\n\nOR &gt; 1 means increased occurrence of an event\nOR &lt; 1 means decreased occurrence of an event\nOR = 1 â†”ï¸ log OR = 0 â‡’ No difference in odds\nOR &lt; 0.5 or OR &gt; 2 â‡’ Moderate effect\nOR &gt; 4 is pretty strong and unlikely to be explained by unmeasured variables\n\nCode\nprostate_model %&gt;%Â \nÂ  Â  Â  broom::tidy(exp = TRUE)\n##Â  termÂ  Â  Â  Â  estimate std.error statisticÂ  p.value\n##Â  &lt;chr&gt;Â  Â  Â  Â  Â  &lt;dbl&gt;Â  Â  &lt;dbl&gt;Â  Â  &lt;dbl&gt;Â  Â  &lt;dbl&gt;\n## 1 (Intercept)Â  0.0318Â  Â  0.435Â  Â  -7.93 2.15e-15\n## 2 fam_hxÂ  Â  Â  Â  0.407Â  Â  Â  0.478Â  Â  -1.88 6.05e- 2\n## 3 b_gsÂ  Â  Â  Â  Â  3.27Â  Â  Â  0.219Â  Â  Â  5.40 6.70e- 8\n\nExponentiates coefficients to get ORs\nInterpretation: the odds ratio estimate of 0.407 means that for someone with a positive family history (fam_hx) of prostate cancer, the odds of their having a recurrence are 59.3% ((1-0.407) x 100) lower than someone without a family history. Similarly, for each unit increase in the baseline Gleason score (b_gs), the odds of recurrence increase by 227% ((3.27-1) x 100).\n\n\nSummary\n## glm(formula = recurrence ~ fam_hx + b_gs, family = binomial,Â \n##Â  Â  data = .)\n##Â \n## Deviance Residuals:Â \n##Â  Â  MinÂ  Â  Â  1QÂ  MedianÂ  Â  Â  3QÂ  Â  Â  MaxÂ \n## -1.2216Â  -0.5089Â  -0.4446Â  -0.2879Â  2.5315Â \n##Â \n## Coefficients:\n##Â  Â  Â  Â  Â  Â  Estimate Std. Error z valueÂ  Â  Â  Â  Â  Â  Pr(&gt;|z|)\n## (Intercept)Â  -3.4485Â  Â  0.4347Â  -7.932 0.00000000000000215\n## fam_hxÂ  Â  Â  -0.8983Â  Â  0.4785Â  -1.877Â  Â  Â  Â  Â  Â  Â  0.0605\n## b_gsÂ  Â  Â  Â  Â  1.1839Â  Â  0.2193Â  5.399 0.00000006698872947\n##Â  Â  Â  Â  Â  Â  Â  Â \n## (Intercept) ***\n## fam_hxÂ  Â  Â  .Â \n## b_gsÂ  Â  Â  Â  ***\n## ---\n## Signif. codes:Â \n## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n##Â \n## (Dispersion parameter for binomial family taken to be 1)\n##Â \n##Â  Â  Null deviance: 281.88Â  on 313Â  degrees of freedom\n## Residual deviance: 246.81Â  on 311Â  degrees of freedom\n##Â  (2 observations deleted due to missingness)\n## AIC: 252.81\n##Â \n## Number of Fisher Scoring iterations: 5\n\nThe Deviance Residuals should have a Median near zero, and be roughly symmetric around zero. If the median is close to zero, the model is not biased in one direction (the outcome is not over- nor under-estimated).\nThe Coefficients estimate how much a change of one unit in each predictor will affect the outcome (in logit units).\n\nThe family history predictor (fam_hx) is not significant, but trends toward an association with a decreased odds of recurrence, while the baseline Gleason score (b_gs) is significant and is associated with an 18% increased log-odds of recurrence for each extra point in the Gleason score.\n\nNull Deviance and Residual Deviance. The null deviance is measured for the null model, with only an intercept. The residual deviance is measured for your model with predictors. Your residual deviance should be lower than the null deviance.\n\nYou can even measure whether your model is significantly better than the null model by calculating the difference between the Null Deviance and the Residual Deviance. This difference [281.9 - 246.8 = 35.1] has a chi-square distribution. You can look up the value for chi-square with 2 degrees (because you had 2 predictors) of freedom.\nOr you can calculate this in R with pchisq(q = 35.1, df=2, lower.tail = TRUE) which gives you a p value of 1.\n\nThe degrees of freedom are related to the number of observations, and how many predictors you have used. If you look at the mean value in the prostate dataset for recurrence, it is 0.1708861, which means that 17% of the participants experienced a recurrence of prostate cancer. If you are calculating the mean of 315 of the 316 observations, and you know the overall mean of all 315, you (mathematically) know the value of the last observation - recurrence or not - it has no degrees of freedom. So for 316 observations, you have n-1 or 315, degrees of freedom. For each predictor in your model you â€˜use upâ€™ one degree of freedom. The degrees of freedom affect the significance of the test statistic (T, or chi-squared, or F statistic).\nObservations deleted due to missingness - the logistic model will only work on complete cases, so if one of your predictors or the outcome is frequently missing, your effective dataset size will shrink rapidly. You want to know if this is an issue, as this might change which predictors you use (avoid frequently missing ones), or lead you to consider imputation of missing values.\n\nPredicted Risk - predicted probabilities from a logistic regression model thatâ€™s used to predict the risk of an event given a set of variables.\nstr(salespeople)\n## 'data.frame':Â  Â  350 obs. ofÂ  4 variables:\n##Â  $ promotedÂ  Â  : intÂ  0 0 1 0 1 1 0 0 0 0 ...\n##Â  $ salesÂ  Â  Â  Â  : intÂ  594 446 674 525 657 918 318 364 342 387 ...\n##Â  $ customer_rate: numÂ  3.94 4.06 3.83 3.62 4.4 4.54 3.09 4.89 3.74 3 ...\n##Â  $ performanceÂ  : intÂ  2 3 4 2 3 2 3 1 3 3 ...\n\nmodel &lt;- glm(formula = promoted ~ ., data = salespeople, family = \"binomial\")\nexp(model$coefficients) %&gt;%Â \nÂ  round(2)\n##Â  (Intercept)Â  Â  Â  Â  sales customer_rateÂ  performance2Â  performance3Â \n##Â  Â  Â  Â  Â  0.00Â  Â  Â  Â  Â  1.04Â  Â  Â  Â  Â  0.33Â  Â  Â  Â  Â  1.30Â  Â  Â  Â  Â  1.98Â \n##Â  performance4Â \n##Â  Â  Â  Â  Â  2.08\n\nInterpretation:\n\nFor two salespeople with the same customer rating and the same performance, each additional thousand dollars in sales increases the odds of promotion by 4%.\n\nSales in thousands of dollars\n\nFor two salespeople with the same sales and performance, each additional point in customer rating decreases the odds of promotion by 67%\nIncreasing performance4 by 1 unit and holding the rest of the variables constant increases the odds of getting a promotion by 108%.\n\nI think this is being treated as a factor variable, and therefore the estimate is relative to the reference level (performance1).\nShould be: â€œPerformance4 increases the odds of getting a promotion by 108% relative to having a performance1â€\n\nFor two salespeople of the same sales and customer rating, performance rating has no significant effect on the odds of promotion.\n\nNone of the levels of performance were statistically significant"
  },
  {
    "objectID": "qmd/regression-logistic.html#sec-reg-log-ass",
    "href": "qmd/regression-logistic.html#sec-reg-log-ass",
    "title": "Logistic",
    "section": "Assumptions",
    "text": "Assumptions\n\nperformance::model_check(mod) - provides a facet panel of diagnostic charts with subtitles to help interprete each chart.\n\nLooks more geared towards a regression model than a logistic model.\n\nLinear relationship between the logit of the binary outcome and each continuous explanatory variable\n\ncar::boxTidwell\n\nMay not work with factor response so might need to as.numeric(response) and use 1,2 values instead of 0,1\np &lt; 0.05 indicates non-linear relationship which is what you want\n\nCan also look a scatter plot of logit(response) vs numeric predictor\n\nNo outliers\n\nCookâ€™s Distance\n\nDifferent opinions regarding what cut-off values to use. One standard threshold is 4/N (where N = number of observations), meaning that observations with Cookâ€™s Distance &gt; 4/N are deemed as influential\n\nStandardized Residuals\n\nAbsolute standardized residual values greater than 3 represent possible extreme outliers\n\n\nAbsence of Multicollinearity\n\nVIF\nCorrelation Heatmap\n\niid\n\nDeviance residuals (y-axis) vs row index (x-axis) should show points randomly around zero (y-axis)\n\nJesperâ€™s response shows calculation and R code\nI think the dHARMA pkg also handles this\n\n\nSufficiently large sample size\n\nRules of thumb\n\n10 observations with the least frequent outcome for each independent variable\n&gt; 500 observations total\n\nIâ€™m sure Harrell has thoughts on this somewhere"
  },
  {
    "objectID": "qmd/regression-logistic.html#diagnostics",
    "href": "qmd/regression-logistic.html#diagnostics",
    "title": "Logistic",
    "section": "Diagnostics",
    "text": "Diagnostics\n\nMisc\n\n** The formulas for the deviances for a logistic regression model are slightly different from other GLMs since the deviance for the saturated logistic regression model is 0 **\nAlso see\n\nDiagnostics, Classification\nDiagnostics, GLM\n\n\nResidual Deviance (G2)\n\n-2 * LogLikelihood(proposed_mod)))\n\nNull Deviance\n\n-2 * LogLikelihood(null_mod)))\ni.e.Â deviance for the intercept-only model\n\nMcFaddenâ€™s Pseudo R2 = (LL(null_mod) - LL(proposed_mod)) / LL(null_mod))\n\nSee What are Pseudo-R Squareds? for formulas to various alternative R2s for logistic regression\nThe p-value for this R2 is the same as the p-value for:\n\n2 * (LL(proposed_mod) - LL(null_mod))\nNull Deviance - Residual Deviance\n\nFor the dof, use proposed_dof - null_dof\n\ndof for the null model is 1\n\n\n\nExample: Getting the p-value\nm1 &lt;- glm(outcome ~ treat)\nm2 &lt;- glm(outcome ~ 1)\n(ll_diff &lt;- logLik(m1) - logLik(m2))\n## 'log Lik.' 3.724533 (df=3)\n1 - pchisq(2*ll_diff, 3)\n\nCompare nested models\n\nModels\nmodel1 &lt;- glm(TenYearCHD ~ ageCent + currentSmoker + totChol,Â \nÂ  Â  Â  Â  Â  Â  Â  data = heart_data, family = binomial)\nmodel2 &lt;- glm(TenYearCHD ~ ageCent + currentSmoker + totChol +Â \nÂ  Â  Â  Â  Â  Â  Â  Â  as.factor(education),Â \nÂ  Â  Â  Â  Â  Â  Â  data = heart_data, family = binomial)\n\nAdd Education or not?\n\nExtract Deviances\n# Deviances\n(dev_model1 &lt;- glance(model1)$deviance)\n## [1] 2894.989\n(dev_model2 &lt;- glance(model2)$deviance)\n## [1] 2887.206\nCalculate difference and test significance\n# Drop-in-deviance test statistic\n(test_stat &lt;- dev_model1 - dev_model2)\n## [1] 7.783615\n\n# p-value\n1 - pchisq(test_stat, 3)Â  # 3 = number of new model terms in model2 (i.e. 3(?) levels of education)\n## [1] 0.05070196\n\n# Using anova\nanova(model1, model2, test = \"Chisq\")\n## Analysis of Deviance Table\n##Â \n## Model 1: TenYearCHD ~ ageCent + currentSmoker + totChol\n## Model 2: TenYearCHD ~ ageCent + currentSmoker + totChol + as.factor(education)\n##Â  Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi)Â \n## 1Â  Â  Â  3654Â  Â  2895.0Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â \n## 2Â  Â  Â  3651Â  Â  2887.2Â  3Â  7.7836Â  0.0507 .\n\n|Î²| &gt; 10\n\nExtreme\nImplies probabilities close to 0 or 1 which is suspect\nConsider removal of the variable or outlier(s) influencing the model\n\nIntercepts â‰ˆ -17\n\nIndicate a need for a simpler model (see bkmk - Troubleshooting glmmTMB)\n\nIf residuals are heteroskedastic, see {glmx}\n\nneg.bin, hurdle, logistic - Extended techniques for generalized linear models (GLMs), especially for binary responses, including parametric links and heteroskedastic latent variables\n\nBinned Residuals\n\nIt is not useful to plot the raw residuals, so examine binned residual plots\nMisc\n\n{arm} will mask some {tidyverse} functions, so donâ€™t load whole package\n\nLook for :\n\nPatterns\nNonlinear trend may be indication that squared term or log transformation of predictor variable required\nIf bins have average residuals with large magnitude\nLook at averages of other predictor variables across bins\nInteraction may be required if large magnitude residuals correspond to certain combinations of predictor variables\n\nProcess\n\nExtract raw residuals\n\nInclude type.residuals = \"response\" in the broom::augment function to get the raw residuals\n\nOrder observations either by the values of the predicted probabilities (or by numeric predictor variable)\nUse the ordered data to create g bins of approximately equal size.\n\nDefault value: g = sqrt(n)\n\nCalculate average residual value in each bin\nPlot average residuals vs.Â average predicted probability (or average predictor value)\n\nExample: vs Predicted Values\narm::binnedplot(x = risk_m_aug$.fitted, y = risk_m_aug$.resid,\nÂ  Â  Â  Â  Â  Â  Â  Â  xlab = \"Predicted Probabilities\",\nÂ  Â  Â  Â  Â  Â  Â  Â  main = \"Binned Residual vs. Predicted Values\",\nÂ  Â  Â  Â  Â  Â  Â  Â  col.int = FALSE)\n\nExample: vs Predictor\narm::binnedplot(x = risk_m_aug$ageCent,\nÂ  Â  Â  Â  Â  Â  Â  Â  y = risk_m_aug$.resid,\nÂ  Â  Â  Â  Â  Â  Â  Â  col.int = FALSE,\nÂ  Â  Â  Â  Â  Â  Â  Â  xlab = \"Age (Mean-Centered)\",\nÂ  Â  Â  Â  Â  Â  Â  Â  main = \"Binned Residual vs. Age\")\n\nCheck that residuals have mean zero: mean(resid(mod))\nCheck that residuals for each level of categorical have mean zero\nrisk_m_aug %&gt;%\nÂ  group_by(currentSmoker) %&gt;%\nÂ  summarize(mean_resid = mean(.resid))"
  },
  {
    "objectID": "qmd/regression-logistic.html#marginal-effects",
    "href": "qmd/regression-logistic.html#marginal-effects",
    "title": "Logistic",
    "section": "Marginal Effects",
    "text": "Marginal Effects\n\nMisc\n\nNotes from\n\nR &gt;&gt; Documents &gt;&gt; Regression &gt;&gt; glm-marginal-effects.pdf\n\nAlso see\n\nRegression, Interactions &gt;&gt; Logistic Regression\nPost-Hoc Analysis, emmeans &gt;&gt; Logistic Regression\n\nMarginal Effects and Elasticities are similar except elasticities are percent change.\n\ne.g.Â a percentage change in a regressor results in this much of a percentage change in the response level probability\n\n\nIn general\n\nA â€œmarginal effectâ€ is a measure of the association between an infinitely small change in a regressor and a change in the response variable\n\nâ€œinfinitely smallâ€ because weâ€™re using partial derivatives\nExample: If I change the cost (regressor) of taking the bus, how does that change the probability (not odds or log odds) of taking a bus to work instead of a car (response)\n\nAllows you to ask counterfactuals.\n\n\nIn OLS regression with no interactions or higher-order term, the estimated slope coefficients are marginal effects, but for glms, the coefficients are not marginal effects at least not on the scale of the response variable\nMarginal effects are partial derivatives of the regression equation with respect to each variable in the model for each unit in the data. (also see notebook, Regression &gt;&gt; logistic section)\n\nLogistic Regression\n\nThe partial derivative gives the slope of a tangent line at point on a nonlinear curve (e.g.Â logit) which is the linear change in probability at a single point on the nonlinear curve\n\\[\n\\frac {\\partial P}{\\partial X_{k}} = \\beta_{k} \\times P \\times (1 - P)\n\\]\n\nWhere P is the predicted probability and Î² is the model coefficient for the kth predictor\n\n\n3 types of marginal effects\n\nMarginal effect at the Means\n\nThe marginal effect that is â€œtypicalâ€ of the sample\nEach model coefficient is multiplied times their respective independent variables mean and summed along with the intercept.\n\nThis sum is transformed from a logit to a probability, P, is used in the partial derivative equation to calculate the marginal effect\n\nInterpretation\n\ncontinuous: an infinitely small change in  while all other predictors are held at their means results in a  change in \n\ne.g.Â an infinitely small change in this predictor for a hypothetical average person results in this amount of change in the dependent variable\n\n\nâ€œat the mediansâ€ can be easily calculated using marginaleffects::typicalÂ  (this is outdated) without any columns specified and then used to calculate marginal effects\n\nto get â€œat the meansâ€ youâ€™d have to supply each column with its mean value\n\n\nMarginal effect at Representative (â€œTypicalâ€) Values\n\nThe marginal effect that is â€œtypicalâ€ of a group represented in the sample\nA real â€œaverageâ€ person doesnâ€™t usually have the mean/median values for predictor values (e.gÂ  mean age of 54.68 years), so you might want to find the marginal effect for a â€œtypicalâ€ person of a certain demographic or group youâ€™re interested in by specifying values for predictors\nInterpretation\n\ncontinuous: an infinitely small change in  for a person with  results in a  change in \n\nCalculate using marginaleffects::typicalÂ  (this is outdated) to specify column values (this is outdated)\n\nAverage Marginal Effect (AME)\n\nThe marginal effect of a case chosen at random from the sample\n\nConsidered the best summary of an independent variable\n\nCalculate marginal effect for each observation (distribution of marginal effects) and then take the average\n\nMultiply the model coefficient times each value of an independent variable, repeat for each predictor, sum with intercept, use predicted probabilities to calculate marginal effect, and average all marginal effects across all obseravation for a predictor to get the AME\n\nInterpretation\n\ncontinuous: Holding all covariates constant, an infinitely small change in  results in a  change in  on average.\n\nUse marginaleffects::marginaleffects +summary or tidy\n\n\n{marginaleffects}\n\nExample: Palmer penguins data and logistic regression model\n\nCreate marginal effects object\nmfx &lt;- marginaleffects(mod)\nhead(mfx)\n#&gt;Â  rowidÂ  Â  Â  Â  Â  Â  Â  termÂ  Â  Â  Â  dydxÂ  std.error large_penguin bill_length_mm\n#&gt; 1Â  Â  1Â  Â  bill_length_mm 0.017622745 0.007837288Â  Â  Â  Â  Â  Â  0Â  Â  Â  Â  Â  39.1\n#&gt; 2Â  Â  1 flipper_length_mm 0.006763748 0.001561740Â  Â  Â  Â  Â  Â  0Â  Â  Â  Â  Â  39.1\n#&gt; 3Â  Â  2Â  Â  bill_length_mm 0.035846649 0.011917159Â  Â  Â  Â  Â  Â  0Â  Â  Â  Â  Â  39.5\n#&gt; 4Â  Â  2 flipper_length_mm 0.013758244 0.002880123Â  Â  Â  Â  Â  Â  0Â  Â  Â  Â  Â  39.5\n#&gt; 5Â  Â  3Â  Â  bill_length_mm 0.084433436 0.021119186Â  Â  Â  Â  Â  Â  0Â  Â  Â  Â  Â  40.3\n#&gt; 6Â  Â  3 flipper_length_mm 0.032406447 0.008159318Â  Â  Â  Â  Â  Â  0Â  Â  Â  Â  Â  40.3\n#&gt;Â  flipper_length_mm species\n#&gt; 1Â  Â  Â  Â  Â  Â  Â  181Â  Adelie\n#&gt; 2Â  Â  Â  Â  Â  Â  Â  181Â  Adelie\n#&gt; 3Â  Â  Â  Â  Â  Â  Â  186Â  Adelie\n#&gt; 4Â  Â  Â  Â  Â  Â  Â  186Â  Adelie\n#&gt; 5Â  Â  Â  Â  Â  Â  Â  195Â  Adelie\n#&gt; 6Â  Â  Â  Â  Â  Â  Â  195Â  Adelie\nAverage Marginal Effect (AME)\nsummary(mfx)\n#&gt; Average marginal effectsÂ \n#&gt;Â  Â  Â  typeÂ  Â  Â  Â  Â  Â  Â  TermÂ  Â  Â  Â  Â  ContrastÂ  Â  Effect Std. ErrorÂ  z value\n#&gt; 1 responseÂ  Â  bill_length_mmÂ  Â  Â  Â  Â  Â  Â  &lt;NA&gt;Â  0.02757Â  Â  0.00849Â  3.24819\n#&gt; 2 response flipper_length_mmÂ  Â  Â  Â  Â  Â  Â  &lt;NA&gt;Â  0.01058Â  Â  0.00332Â  3.18766\n#&gt; 3 responseÂ  Â  Â  Â  Â  species Chinstrap / AdelieÂ  0.00547Â  Â  0.00574 -4.96164\n#&gt; 4 responseÂ  Â  Â  Â  Â  speciesÂ  Â  Gentoo / AdelieÂ  2.19156Â  Â  2.75319Â  0.62456\n#&gt; 5 responseÂ  Â  Â  Â  Â  species Gentoo / Chinstrap 400.60647Â  522.34202Â  4.59627\n#&gt;Â  Â  Pr(&gt;|z|)Â  Â  Â  2.5 %Â  Â  97.5 %\n#&gt; 1Â  0.0011614Â  Â  0.01093Â  Â  0.04421\n#&gt; 2Â  0.0014343Â  Â  0.00408Â  Â  0.01709\n#&gt; 3 2.0906e-06Â  -0.00578Â  Â  0.01673\n#&gt; 4Â  0.8066373Â  -3.20459Â  Â  7.58770\n#&gt; 5 1.2828e-05 -623.16509 1424.37802\n\nInterpretation:\n\nHolding all covariates constant, for an infinitely small increase in bill length, the probability of being a large penguin increases on average by 2.757%\nSpecies contrasts are from {emmeans}(also see Post-Hoc Analysis, emmeans)\n\nContrasts from get_contrasts.R on package github using emmeans::contrast(emmeans_obj, method = \"revpairwise\")\nodds ratios\nYou can get response means per category on the probability scale using emmeans::emmeans(mod, \"species\", type = \"response\")\n\n\n\n\nOther features available for marginaleffects object\n\ntidy\n\ncoef stats with AME, similar to summary, just a different a object class I think\n\nglance\n\nmodel GOF stats\n\ntypical\n\ngenerate artificial predictor values and get marginal effects for them\nmedian (or mode depending on variable type) values used for columns that arenâ€™t provided\n\ncounterfactual\n\nuse observed data for all but a one or a few columns. Provide values for those column(s) (e.g.Â flipper_length_mm = c(160, 180))\ngenereates a new larger dataset where each observation has each of the provided values\n\nViz\n\n{modelsummary} tables\nplot (error bar plot for AME) and plot_cme (line plot for interaction term)\n\noutputs ggplot objects\n\n\n\n\nCategorical Variables\n\nThe 3 types of marginal effects can be modified for categoricals\nSteps (â€œat the meansâ€, binary)\n\nCalculate the predicted probability when the variable = 1 and the other predictors are equal to their means.\nCalculate the predicted probability when the variable = 0 and the other predictors are equal to their means.\nThe difference in predicted probabilities is the marginal effect for a change from the â€œbase levelâ€ (aka reference category)\n\nThis can extended to categorical variables with multiple categories by calculating the pairwise differences between each category and the reference category (contrasts)"
  },
  {
    "objectID": "qmd/regression-logistic.html#probit",
    "href": "qmd/regression-logistic.html#probit",
    "title": "Logistic",
    "section": "Probit",
    "text": "Probit\n\nâ€œFor some reason, econometricians have never really taken on the benefits of the generalized linear modelling framework. So you are more likely to see an econometrician use a probit model than a logistic regression, for example. Probit models tended to go out of fashion in statistics after the GLM revolution prompted by Nelder and Wedderburn (1972).â€ (Hyndman)\nVery similar to Logistic\n\nWhere probit and logistic curves differ\n\nlink function: Î¦-1(p) where Î¦ is the standard normal CDF:\n\nThe probability prediction, p:"
  },
  {
    "objectID": "qmd/regression-logistic.html#visualization",
    "href": "qmd/regression-logistic.html#visualization",
    "title": "Logistic",
    "section": "Visualization",
    "text": "Visualization\n\nMisc\n\nNotes from\n\nVisualizing logistic regression results for non-technical audiences (github, vignette, video)\n\nTables\n\n\nAppropriate for technical audiences who are familiar with logistic regression\nCan be taxing on the eyes making it difficult to absorb insights from your model\n\n\nLog Odds Ratio vs Odds Ratio\n\n\nUsing log odds ratio is recommended for visualizations for puposes of comparing effect magnitudes\n\nEach coefficient represents an additive change on the log odds scale; when we exponentiate to get odds, each coefficient represents a multiplicative change\nOdds ratios makes the chart asymmetric and squishes some bars (e.g.Â Pet: Fish)\nOdds ratios can be misleading in comparing negative vs postive variable effects\n\ne.g.Â Prior GPA looks like it has much bigger effect than Pet: Fish when using odds ratio\n\nThereâ€™s a danger that the percent change in odds might be misinterpreted as the absolute probability of the outcome (or the change in its probability)\n\ne.g.Â A 300% change in the odds ratio is a tripling of the odds ratio, not an 300% increase in probability. (see example below)\n\n\nAlthough numerically, changes in odds ratios may be a bit easier to describe for your audience than changes in log odds.\n\nExample: tripling the odds ratio is like going from 3-to-1 odds to 9-to-1 odds, or from 1-to-3 odds to an even chance.)\n\n\nError bar chart (caterpillar chart)\n\nChange in log odds\n\n\nThese are untransformed parameter estimates (See Misc &gt;&gt; Tables for estimate values)\n\nChange in probability from a baseline\n\nSince the effect is non-linear (See Interpetation &gt;&gt; Effect is non-linear in p), a baseline is needed in order to properly interpret the changes in probability due to the increases of 1 unit of a variable\nProcess:\n\nChoose an appropriate baseline\nCompute the marginal effect of a predictor given that baseline\n\nIntercept as a baseline\n\n\nThe x-axis is the problem. Values shown depict changes from the baseline\nBoth charts have the same values, but the chart on the left more clearly indicates the meaning while the chart on the right includes the CIs\nEstimates transformations\n\nIntercept: invlogit(intercept)\nOther parameters: invlogit(&lt;param&gt; + intercept)\n\nIncluding CI values\n\n\nVertical line is the output of the inverse logit of the intercept\n\nRepresenting the probability of passing for a student with average prior GPA, average height, and the baseline value of each categorical variable â€“ not a Mac user, doesnâ€™t wear glasses, has no pet, favorite color is blue, and didnâ€™t go to tutoring.\n\n\n\n\nOutcome vs Predictor with Logistic Curve\n\n# Plot of data with a logistic curve fit\nÂ  ggplot(data, aes(x = z_homr, y = as.numeric(alive) - 1)) +\nÂ  Â  geom_jitter(height = 0.1, size =1, alpha = 0.5) +\nÂ  Â  geom_smooth(method = \"glm\",\nÂ  Â  Â  Â  Â  Â  Â  Â  method.args = list(family = \"binomial\")) +\nÂ  Â  theme_minimal() +\nÂ  Â  scale_y_continuous(breaks = c(0, 1), labels = c(\"Alive\", \"Dead\")) +\nÂ  Â  ylab(\"\") +\nÂ  Â  xlab(\"HOMR Linear Predictor\")\n\nDead/Alive is the outcome and HOMR is the predictor\n\nPredictor by outcome (count)\n\ng1 &lt;- ggplot(data, aes(x = z_homr, fill = alive)) +\nÂ  Â  geom_histogram() +\nÂ  Â  theme_minimal() +\nÂ  Â  xlab(\"HOMR Linear Predictor\") +\nÂ  Â  ylab(\"Number of Participants\") +\nÂ  Â  scale_fill_brewer(\"Alive\", palette = \"Paired\")\nPredictor by outcome (proportion)\n\ng2 &lt;- ggplot(data, aes(x = z_homr, fill = alive)) +\nÂ  Â  geom_histogram(position = \"fill\") +\nÂ  Â  theme_minimal() +\nÂ  Â  xlab(\"HOMR Linear Predictor\") +\nÂ  Â  ylab(\"Proportion\") +\nÂ  Â  scale_fill_brewer(\"Alive\", palette = \"Paired\")\nPredictor vs Outcome (beeswarm)\n\nggplot(data, aes(y = z_homr, x = alive, fill = alive, color = alive)) +\nÂ  Â  geom_beeswarm() +\nÂ  Â  geom_boxplot(alpha = 0, color = \"black\") +\nÂ  Â  theme_minimal() +\nÂ  Â  ylab(\"HOMR Linear Predictor\") +\nÂ  Â  xlab(\"Alive at 1 year\") +\nÂ  Â  scale_fill_brewer(guide = FALSE, palette = \"Paired\") +\nÂ  Â  scale_color_brewer(guide = FALSE, palette = \"Paired\")"
  },
  {
    "objectID": "qmd/regression-multinomial.html#sec-reg-multin-misc",
    "href": "qmd/regression-multinomial.html#sec-reg-multin-misc",
    "title": "Multinomial",
    "section": "Misc",
    "text": "Misc\n\nAKA Random Utility or Choice Models\nGoal: Model the category probabilities for a polytomous (i.e.Â # of categoies &gt; 2) response\n\nLet \\(\\phi_{ij} \\equiv \\phi_j(\\vec{x_i})\\) be the probability of response category j for unit i given predictor \\(\\vec{x_i}\\) .\nSince the sum of the probabilities of all the categories equals 1, if you know m-1 probabilities for m categories, then you know the probability for the last category, \\(\\phi_{im}\\).\n\\[\n\\phi_{im} = 1 - \\sum_{j=1}^{m-1} \\phi_{ij}\n\\]\nThe essential idea is to construct a model for the polytomous response composed of mâˆ’1 logit comparisons among the response categories in a manner analogous to the treatment of factors in the predictor variables (i.e.Â creating dummy variables for the m-1 categories).\n\nNotes from ResEcon 703 Video Course\n\nWeeks 3, 4, 9, 10, 12, and 13\nVideo lectures talk through mathematics (interpretations, motivations, benefits, limitations, etc.) of the approaches. Each week ends with a coding session illustrating the approaches thatâ€™s not included in the videos but is included in the slides.\n\nThe slides are updated with new material while the videos have not.\n\ngithub with slides and problem sets/solutions with R code. Slides provide a deeper introduction to the application the algorithms and the econometrics. Problem set solutions have similar material but written out more clearly. {mlogit} used throughout.\n\nAlso see:\n\nEconometrics, Discrete Choice Models\nDiagnostics, Classification &gt;&gt; Multinomial\nClassification &gt;&gt; Discriminant Analysis &gt;&gt; Linear Discriminant Analysis (LDA)\nModel Building, brms &gt;&gt; Logistic Regression\n\nPackages\n\n{mlogit} - Enables the estimation of random utility models with choice situation and/or alternative specific variables. The main extensions of the basic multinomial model (heteroscedastic, nested and random parameter models) are implemented.\n\nVignette\n\n{apollo} - Both classical and Bayesian estimation is available, and multiple discrete continuous models are covered in addition to discrete choice. Multi-threading processing is supported for estimation and a large number of pre and post-estimation routines, including for computing posterior (individual-level) distributions.\n{mixl} - Fast (C++) estimation of complex mixed models for large datasets. Allows you to specify multiple utility functions, which is standard practice for more complicated models. (Paper)\n{nestedLogit} - Provides functions for fitting nested dichotomy logistic regression models for a polytomous response.\n\nNested dichotomies are statistically independent, and hence provide an additive decomposition of tests for the overall polytomous response. When the dichotomies make sense substantively, this method can be a simpler alternative to the standard multinomial logistic model which compares response categories to a reference level.\n\n{mnlogit} - (CRAN archived with no github) efficient estimation of MNL for large data sets\n\nOffers speedups of 10 - 50 times for modestly sized problems and more than 100 times for larger problems. Running in parallel mode on a multicore machine gives up to 4 times additional speedup on 8 processor cores. mnlogit achieves its computational efficiency by drastically speeding up computation of the log-likelihood functionâ€™s Hessian matrix through exploiting structure in matrices that arise in intermediate calculations.\n\n{{torch-choice}} - Choice modeling with PyTorch: logit model and nested logit model\n\nCan handle big data\n\n{{xlogit}} - A Python Package for GPU-Accelerated Estimation of Mixed Logit Models\n{{MODE.behave}} - Incorporates estimation routines for conventional multinomial logit models, as well as for mixed logit models with nonparametric distributions\n\nMakes use of latin hypercube sampling to increase the efficiency of the expectation maximization algorithm during the estimation process in order to decrease computation time.\nPre-estimated discrete choice simulation methods for transportation research are included\n\n{glmnet}\n{multgee}\n\nUse Cases:\n\nRespondents to a social survey are classified by their highest completed level of education, taking on the values (1) less than highschool, (2) highschool graduate, (3) some post-secondary, or (4) post-secondary degree.\nWomenâ€™s labor-force participation is classified as (1) not working outside the home, (2) working part-time, or (3) working full-time.\nVoters in Quebec in a Canadian national election choose one of the (1) Liberal Party, (2) Conservative Party, (3) New Democratic Party, or (4) Bloc Quebecois."
  },
  {
    "objectID": "qmd/regression-multinomial.html#sec-reg-multin-terms",
    "href": "qmd/regression-multinomial.html#sec-reg-multin-terms",
    "title": "Multinomial",
    "section": "Terms",
    "text": "Terms\n\nAlternative - The levels of a polytomous response in Random Utility Models.\nChoice Probability - The probability that a decision-maker will chose a particular alternative. The predicted response for a Random Utility Model.\nDiscounted Utility - The utility of some future event, such as consuming a certain amount of a good, as perceived at the present time as opposed to at the time of its occurrence. It is calculated as the present discounted value of future utility, and for people with time preference for sooner rather than later gratification, it is less than the future utility.\nMarginal Utility - Coefficients in the random choice model. The added satisfaction that a consumer gets from having one more unit of a good or service. The concept of marginal utility is used by economists to determine how much of an item consumers are willing to purchase.\nMarket Level - Environment or category where a class or brand of products share the same attributes\n\ne.g.Â All Cokes should cost the same in Indianapolis but that price may be different from the price of Cokes in Nashville. Therefore, Indianapolis and Nashville are separate markets.\n\nMarket Share - The percentage of total sales in an industry or product category that belong to a particular company during a discrete period of time. For a Random Utility Model, when the data is at the market level instead of the individual level, the predicted response is the Market Share.\nOutside Product (aka Outside Option) - Typically a â€œpurchase nothingâ€ indicator variable/variable category\n\nCan just be a vector of 0s\n\nRandom Utility Models - These models rely on the hypothesis that the decision maker is able to rank the different alternatives by an order of preference represented by a utility function, the chosen alternative being the one which is associated with the highest level of utility. They are called random utility models because part of the utility is unobserved and is modeled as the realization of a random deviate."
  },
  {
    "objectID": "qmd/regression-multinomial.html#sec-reg-multin-mnl",
    "href": "qmd/regression-multinomial.html#sec-reg-multin-mnl",
    "title": "Multinomial",
    "section": "Multinomial Logit (MNL)",
    "text": "Multinomial Logit (MNL)\n\nAKA Generalized Logit\nWhen the polytomous response has m levels (aka Alternatives), the multinomial logit model comprises mâˆ’1 log-odds comparisons with a reference level, typically the first or last.\n\nThe likelihood under the model and the fitted response probabilities that it produces are unaffected by choice of reference level, much as choice of reference level for dummy regressors created from a factor predictor doesnâ€™t affect the fit of a regression model.\n\nChoice Probability for alternative, i, and decision-maker, n:\n\\[\nP_{ni} = \\frac {e^{V_{ni}}}{\\sum_j e^{V_{nj}}} = \\frac {e^{\\hat{\\beta}x_{ni}}}{\\sum_j e^{{\\hat{\\beta}x_{nj}}}}\n\\]\n\nThe probability that decision-maker, \\(n\\), chooses alternative, \\(i\\)\nPredicted probability output from the model\n\nData sets used to estimate random utility models have therefore a specific structure that can be characterized by three indexes: the alternative, the choice situation, and the individual. The distinction between choice situation and individual indexes is only relevant if we have repeated observations for the same individual.\n\nExamples of variable types\n\nChoice Situation Specific\n\ndata1: Length of the vacation and the Season\ndata2: values of dist, income and urban are repeated four times.\n\ndist (the distance of the trip)\nincome (household income)\nurban (a dummy for trips which have a large city at he origin or the destination)\nnoalt the number of available alternatives\n\n\nIndividual Specific\n\ndata1: Income and Family Size\ndata2: None\n\nAlternative Specific\n\ndata1: Distance to Destination and Cost\ndata2:\n\ntransport modes (air, train, bus and car)\ncost for monetary cost\nivt for in vehicle time\novt for out of vehicle time\nfreq for frequency\n\n\n\nThe unit of observation is therefore the choice situation, and it is also the individual if thereis only one choice situation per individual observed, which is often the case\nData Descriptions\n\nEach individual has responded to several (up to 16) scenarios.\nFor every scenario, two train tickets A and B are proposed to the user, with different combinations of four attributes: price (the price in cents of guilders), time (travel time in minutes), change (the number of changes) and comfort (the class of comfort, 0, 1 or 2, 0 being the most comfortable class).\n\nDataset in wide format\n#&gt;    id choiceid choice price_A time_A change_A comfort_A price_B time_B\n#&gt; 1  1  1        A      2400    150    0        1         4000    150\n#&gt; 2  1  2        A      2400    150    0        1         3200    130\n#&gt; 3  1  3        A      2400    115    0        1         4000    115\n\n#&gt;    change_B comfort_B\n#&gt; 1  0        1\n#&gt; 2  0        1\n#&gt; 3  0        0\nTransform to long format and add index attribute with {dfidx}\nTr &lt;- \n  dfidx(Train, \n        shape = \"wide\", \n        choice = \"choice\",\n        varying = 4:11, \n        sep = \"_\", \n        idx = list(c(\"choiceid\", \"id\")),\n        idnames = c(\"chid\", \"alt\"),\n        opposite = c(\"price\", \"comfort\", \"time\", \"change\"))\n\nhead(Tr, 3)\n#&gt; ~~~~~~~\n#&gt; first 3 observations out of 5858\n#&gt; ~~~~~~~\n#&gt;   choice price     time  change comfort idx\n#&gt; 1 TRUE   -52.88904 -2.5  0      -1      1:A\n#&gt; 2 FALSE  -88.14840 -2.5  0      -1      1:B\n#&gt; 3 TRUE   -52.88904 -2.5  0      -1      2:A\n#&gt; ~~~ indexes ~~~~\n#&gt;   chid id alt\n#&gt; 1 1    1  A\n#&gt; 2 1    1  B\n#&gt; 3 2    1  A\n#&gt; indexes: 1, 1, 2\n\nIn â€œlongâ€ format (default) is a data.frame with a special column which containsthe corresponding indexes. A lot of these argument values are passed to reshape to produce the final format.\nâ€œvaryingâ€ - Indicates alternative specific variables which is a numeric vector that indicates their position in the data frame.\nâ€œsepâ€ - In wide format, variable names are of the form, &lt;var&gt;_choice (default â€œ.â€).\nâ€œchoiceâ€ - Indicates the response that has to be transformed in a logical value\nâ€œidxâ€ - Adds attribute which specifies the nesting structure of the id variables\n\nEach individual faces different choice situations, the â€œidâ€ variable, which contains the individual index, nests the choice situation variable called â€œchoiceid.â€\nContains the three relevant indexes: choiceid is the choice situation index, alt the alternative index and id is the individual index.\n\nâ€œoppositeâ€ - If you expect negative coefficients, taking the opposite of the covariates will lead to expected positive coefficients. (?)\n\nAdd idx attribute to an already long formatted df\nMC &lt;- \n  MC &lt;- dfidx(ModeCanada, \n              subset = noalt == 4,\n              idx = c(\"case\", \"alt\"))\n\nOrder of variables doesnâ€™t seem to matter for the â€œidxâ€ arg in order to specify index types\nThe primary dplyr verbs can be applied to dfidx objects\n\n\n\n\n{mlogit}\n\nIndexes: the alternative, the choice situation and the individual.\n\nThe distinction between choice situation id and individual id is only relevant if we have repeated observations for the same individual\nIn docs, the choice situation/individual id is indexed by \\(i\\) and the alternatives by \\(j\\)\n\nTypes of variables\n\n\n\n\n\n\n\n\nType\nVariable\nCoefficient\n\n\n\n\nAlternative and Choice Situation Specific\nAlternative Specific (only)\n\\(x_{ij}\\)\n\\(t_j\\)\n\\(\\beta\\) (generic)\n\\(\\nu\\) (generic)\n\n\nChoice Situation Specific\n\\(z_i\\)\n\\(\\gamma_j\\) (alternative specific)\n\n\nAlternative and Choice Situation Specific\n\\(w_j\\)\n\\(\\delta_j\\) (alternative specific)\n\n\nChoice Situation Specific\n\\(v_i\\)\nInfluence Variance of Errors\n\n\n\n\nLabelling your variables as one of these types isnâ€™t clear-cut, and seems a little closer to an art than a science.\n\n\nFormula Syntax: y ~ a | b | c | d\n\na: Variables with common parameters\nb: Individual-specific variables with alternative-specific parameters\nc: Alternative-specific variables with alternative-specific parameters\nd: Individual-specific variables that affect the scale parameter\n\ndfidx Args\n\ndata: data frame you wanted to be converted\nshape: â€˜wideâ€™ or â€˜longâ€™ for the format of the original data frame\nchoice:\n\nFor shape = \"wide\": categorical variable that contains the all alternative categories\nFor shape = \"long\": Indicator variable where TRUE is for 1 category and FALSE for everything else\n\nThe fourth argument depends on the format of the original data frame\n\nshape = \"wide\" - varying: numeric vector defining which variables by column index which contain alternative-specific data\nshape = \"long\"- idx: two-element character vector defining which which variables contain identifiers for the choice situation and alternative\n\n\nExample: Convert data to {dfidx} format\n\ndata(Heating, package = \"mlogit\")\nRaw Data (wide)\n\ndepvar: Choice set\n\ngc: gas central\ngr: gas room\nec: electric central\ner: electric room\nhp: heat pump\n\nAlternative-specific data (Each variable has 1 column for each alternative)\n\nic: installation cost\noc: annual operating cost\n\nHousehold demographic data\n\nincome: annual income\nagehed: age of household head\nrooms: number of rooms\nregion: home location\n\nOthers\n\nidcase: id variable (1 to 1)\n\n\nConvert Wide to Long using {dplyr}\nheating_long &lt;- heating %&gt;%\n  pivot_longer(contains('.')) %&gt;%\n  separate(name, c('name', 'alt')) %&gt;%\n  pivot_wider() %&gt;%\n  mutate(choice = (depvar == alt)) %&gt;%\n  select(-depvar)\n\nChoice: logical; TRUE if alternative (alt) == gc, otherwise FALSE\nalt: categorical with all alternatives\ndemographic variables\nic and oc: all wide variables pivoted into 2 variables\n\nAs there were 5 variables for each, the id var now has 5 of each id\n\n\nConvert from Wide\nheating_dfidx &lt;- \n  dfidx(heating, \n        shape = 'wide',\n        choice = 'depvar', \n        varying = 3:12)\n\nheating_dfidx\n#&gt;    idcase depvar income agehed rooms region      ic     oc  idx\n#&gt; 1       1  FALSE      7     25     6 ncostl  859.90 553.34 1:ec\n#&gt; 2       1  FALSE      7     25     6 ncostl  995.76 505.60 1:er\n#&gt; 3       1   TRUE      7     25     6 ncostl  866.00 199.69 1:gc\n#&gt; 4       1  FALSE      7     25     6 ncostl  962.64 151.72 1:gr\n#&gt; 5       1  FALSE      7     25     6 ncostl 1135.50 237.88 1:hp\n\n#&gt; ~~~ indexes ~~~~\n#&gt;    id1 id2\n#&gt; 1    1  ec\n#&gt; 2    1  er\n#&gt; 3    1  gc\n#&gt; 4    1  gr\n#&gt; 5    1  hp\n#&gt; indexes:  1, 2 \n\nprint(tibble(heating_dfidx), n = 10)\n#&gt; # A tibble: 4,500 Ã— 9\n#&gt;    idcase depvar income agehed rooms region    ic    oc idx$id1 $id2 \n#&gt;     &lt;dbl&gt; &lt;lgl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;int&gt; &lt;fct&gt;\n#&gt;  1      1 FALSE       7     25     6 ncostl  860.  553.       1 ec   \n#&gt;  2      1 FALSE       7     25     6 ncostl  996.  506.       1 er   \n#&gt;  3      1 TRUE        7     25     6 ncostl  866   200.       1 gc   \n#&gt;  4      1 FALSE       7     25     6 ncostl  963.  152.       1 gr   \n#&gt;  5      1 FALSE       7     25     6 ncostl 1136.  238.       1 hp \n\nâ€œdepvarâ€ is a column with all the choice categories (i.e.Â alternatives)\n\nItâ€™s a factor type but Iâ€™m not sure if thatâ€™s necessary. Character type might be sufficient.\n\nColumns 3:12 are the â€œwide variableâ€ columns. Thereâ€™s a column for each alternative category\n\nEssentially 2 variables and thereâ€™s 5 alternatives, therefore, 5 \\(\\times\\) 2 = 10 columns\n\n\nConvert from Long\nheating_long_dfidx &lt;- \n  dfidx(heating_long, \n        shape = 'long',\n        choice = 'choice', \n        idx = c('idcase', 'alt'))\n\nOutput is the same except instead of the index columns being named â€œid1, id2â€, theyâ€™re named â€œidcaseâ€, â€œaltâ€.\nidx: Since the df is already in long format, the id variabler (â€œidcaseâ€) has multiple observations for each id. Therefore, there needs be more than 1 variable to create a unique identifier (i.e.Â â€œaltâ€)."
  },
  {
    "objectID": "qmd/regression-multinomial.html#sec-reg-multin-nestlog",
    "href": "qmd/regression-multinomial.html#sec-reg-multin-nestlog",
    "title": "Multinomial",
    "section": "Nested Logit",
    "text": "Nested Logit\n\nFits separate models for each of a hierarchically nested set of binary comparisons among the response categories. The set of mâˆ’1 models comprises a complete model for the polytomous response, just as the multinomial logit model does.\nMisc\n\nIIA still holds for two alternatives in the same dichotomy, but doesnâ€™t hold for alternatives of different different dichotomies\nBoth MNL and Nested Logit methods have have pÃ—(mâˆ’1) parameters. The models are not equivalent, however, in that they generally produce different sets of fitted category probabilities and hence different likelihoods.\n\nMultinomial logit model treats the response categories symmetrically\n\nExtensions\n\nPaired Combinatorial Logit\n\nAllows alternatives to be in multiple dichotomies and multiple hierarchies and for them to have more complex correlation structures\n\nIn nested logits, only alternatives within the same hierarchy are modeled as being correlated with each other.\n\nCreates pairwise dichotomies for each combination of alternatives (i.e.Â each alternative appears in J-1 nests)\nBest for data with fewer alternatives since the parameter space can explode fairly quickly\n\nGeneralized Nested Logit\n\nAllow alternatives to be in any dichotomy in any hierarchy, but assign a weight to each alternative in each dichotomy.\nEstimate Parameters: \\(\\lambda_k\\) (See Choice Probability below), \\(\\alpha_{jk}\\) : â€œweightâ€ or proportion of alternative, \\(j\\), in dichotomy, \\(k\\)\nNeed to be careful about how many dichotomies that you place each alternative, since the parameter space can explode fairly quickly\n\nHeteroskedastic Logit\n\nHeteroskedacity in this sense means that the variance of the unobserved utility (aka residuals) can be different for each alternative\n\\[\n\\mbox{Var}(\\epsilon_{nj}) = \\frac {(\\theta_j \\pi)^2}{6}\n\\]\nSince thereâ€™s no closed form solution, simulation methods must be usded to get the choice probabilities and model parameters.\n\n\n\nConstruction of Dichotomies\n\nBy the construction of nested dichotomies, the submodels are statistically independent (because the likelihood for the polytomous response is the product of the likelihoods for the dichotomies), so test statistics, such as likelihood ratio (G2) and Wald chi-square tests for regression coefficients can be summed to give overall tests for the full polytomy.\nNested dichotomies are not unique and alternative sets of nested dichotomies are not equivalent: Different choices have different interpretations. Moreover, and more fundamentally, fitted probabilities and hence the likelihood for the nested-dichotomies model depend on how the nested dichotomies are defined.\nExample: 2 methods of splitting a 4-level response into dichotomies\n\n\nLeft: Y = {1, 2, 3, 4} â†’ {1,2} vs {3,4} â†’ {1} vs {2} and {3} vs {4}\nRight: (Continuous Logit) Y = {1, 2, 3, 4} â†’ {1} vs {2, 3, 4} â†’ {2} vs {3, 4} â†’ {3} vs {4}\n\n{1} vs.Â {2,3,4} could represent highschool graduation\n{2} vs.Â {3,4} could represesnt enrollment in post-secondary education\n{3} vs.Â {4} could represent completion of a post-secondary degree.\n\n\n\nExample: {nestedLogit}\n\nSee Vignette for additional methods including effects plotting, hypothesis testing, GoF tables, etc.\nData:\n\nâ€œparticâ€: labor force participation, the response, with levels:\n\nâ€œfulltimeâ€: working full-time\nâ€œnot.workâ€: not working outside the home\nâ€œparttimeâ€ : working part-time.\n\nâ€œhincomeâ€: Husbandâ€™s income, in $1,000s.\nâ€œchildrenâ€: Presence of children in the home, \"absent\" or \"present\".\n\nSet-up sets of dichotomies that you want analyze\nset1 &lt;- \n  logits(work = dichotomy(\"not.work\", \n                          working = c(\"parttime\", \"fulltime\")),\n         full = dichotomy(\"parttime\", \"fulltime\"))\n\nset2 &lt;-\n  logits(full = dichotomy(nonfulltime = c(\"not.work\", \"parttime\"),\n                          \"fulltime\"),\n         part = dichotomy(\"not.work\", \"parttime\"))\n\n# How set1 response variables would be coded in base-R\nWomenlf &lt;- within(Womenlf, {\n  work = ifelse(partic == \"not.work\", 0, 1)\n  full = ifelse(partic == \"fulltime\",  1,\n                ifelse(partic == \"parttime\", 0, NA))\n})\n\nset1:\n\nâ€œworkâ€: Women who are NOT working outside the home vs.Â those who are working (either part-time or full-time).\nâ€œfullâ€: Women who work full-time time vs.Â part-time, but among only those who work.\n\nset2:\n\nâ€œfullâ€: {full-time} vs.Â {not working, part-time}\n\nThe rationale is that the real hurdle for young married women to enter the paid labor force is to combine full-time work outside the home with housework.\n\nâ€œpartâ€: {not working} vs.Â {part-time}.\n\n\nFit 1st Set\nwlf.nested.one &lt;- \n    nestedLogit(partic ~ hincome + children,\n                dichotomies = set1,\n                data = Womenlf)\n\nFits 2 glm models\n\nglm(formula = work ~ hincome + children, family = binomial, data = Womenlf)\nglm(formula = full ~ hincome + children, family = binomial, data = Womenlf)\n\nA combined model is also produced using the Delta Method\nEstimates are calculated for all models and predicted probabilities for the category levels are from the combined model.\n\nResults\n# show as odds ratios\nexp(coef(wlf.nested))\n#&gt;                   work     full\n#&gt; (Intercept)     3.8032 32.38753\n#&gt; hincome         0.9586  0.89829\n#&gt; childrenpresent 0.2069  0.07055\n\n\\(\\hat{\\beta}_{\\text{work, hincome}}\\) gives the estimated change in the log-odds of working vs.Â not working associated with a $1,000 increase in husbandâ€™s income and with having children present, each holding the other constant.\n\\(\\hat{\\beta}_{\\text{full, hincome}}\\) same as above, but gives the estimated change the log-odds of working full-time vs.Â part-time among those who are working outside the home.\nInterpretation (after exponentiation): The odds of both working and working full-time decrease with husbandâ€™s income, by about 4% and 10% respectively per $1000. Having young children also decreases the odds of both working and working full-time, by about 79% and 93% respectively.\n\nFit 2nd Set\nwlf.nested.two &lt;- \n  nestedLogit(partic ~ hincome + children,\n              dictotomies = set2,\n              data=Womenlf)\nResults\nsummary(wlf.nested.two)\n\n# full\n#&gt;                 Estimate Std. Error z value Pr(&gt;|z|)    \n#&gt; (Intercept)       1.7696     0.4690    3.77  0.00016 ***\n#&gt; hincome          -0.0987     0.0277   -3.57  0.00036 ***\n#&gt; childrenpresent  -2.5631     0.3489   -7.35    2e-13 ***\n\n# part\n#&gt;                 Estimate Std. Error z value Pr(&gt;|z|)  \n#&gt; (Intercept)     -1.42758    0.58222   -2.45    0.014 *\n#&gt; hincome          0.00687    0.02343    0.29    0.769  \n#&gt; childrenpresent  0.01629    0.46762    0.03    0.972  \n\nsummary also shows all the typical model summary info, but only for each dichotomy and not the combined model.\nAnova will perform Chi-Square tests on each explanatory variable in each dichotomy and the combined model.\nThe predictors husbandâ€™s income and presence of children affect the decision to work full-time, but not the decision to work part-time among those who arenâ€™t engaged in full-time work. (See â€œSet-up sets of dichotomiesâ€ section describing this setâ€™s rationale)\n\nCompare Models\nfit1 &lt;- predict(wlf.nested.one)$p\nfit2 &lt;- predict(wlf.nested.two)$p\n\ndiag(cor(fit1, fit2))\n#&gt; not.work parttime fulltime \n#&gt;   0.9801   0.9185   0.9961\n\nmean(as.matrix(abs(fit1 - fit2)))\n#&gt; [1] 0.01712\n\nmax(abs(fit1 - fit2))\n#&gt; [1] 0.1484\n\n# GoF\nlogLik(wlf.nested.one)\n#&gt; 'log Lik.' -212.1 (df=6)\nlogLik(wlf.nested.two)\n#&gt; 'log Lik.' -211.4 (df=6)\nAIC(wlf.nested.one, wlf.nested.two)\n#&gt;                df   AIC\n#&gt; wlf.nested      6 436.2\n#&gt; wlf.nested.alt  6 434.9\n\nThe fitted probabilities are similar but far from the same. (See next section for what â€œsameâ€ looks like)\nSince these models arenâ€™t nested within each other, you canâ€™t compare with an LR test (i.e.Â use a p-value), but you can still look at the Loglikelihoods.\nThe GoF comparison slightly favors the alternative nested-dichotomies model (set2).\n\nCompare Model 2 with Multinomial Logit\nwlf.multinom &lt;- nnet::multinom(partic ~ hincome + children, data = Womenlf)\n\nsummary(wlf.multinom)\n#&gt; Coefficients:\n#&gt;          (Intercept)   hincome childrenpresent\n#&gt; parttime      -1.432  0.006894         0.02146\n#&gt; fulltime       1.983 -0.097232        -2.55861\n#&gt;  AIC: 434.9\n\nlogLik(wlf.multinom)\n#&gt; 'log Lik.' -211.4 (df=6)\n\nfit3 &lt;- predict(wlf.multinom, type=\"probs\")[, c(\"not.work\", \"parttime\", \"fulltime\")]\ndiag(cor(fit2, fit3))\n#&gt; not.work parttime fulltime \n#&gt;        1        1        1\nmean(as.matrix(abs(fit2 - fit3)))\n#&gt; [1] 0.0001251\nmax(abs(fit2 - fit3))\n#&gt; [1] 0.0006908 \n\nThe multinomial logit model and the alternative nested-dichotomies (i.e.Â set2) model produce nearly identical fits with similar simple interpretations."
  },
  {
    "objectID": "qmd/regression-multinomial.html#sec-reg-multin-mixlog",
    "href": "qmd/regression-multinomial.html#sec-reg-multin-mixlog",
    "title": "Multinomial",
    "section": "Mixed Logit",
    "text": "Mixed Logit\n\nIndividual heterogeneity can be introduced in the parameters associated with the covariates entering the observable part of the utility or in the variance of the errors.\n\nModel creates a distribution of \\(\\beta\\)s, so \\(\\beta\\) is allowed to vary throughout a population.\n\nThe \\(\\theta\\) distribution parameters are what get estimated by the model in order to calculate a distribution of \\(\\beta\\)s (i.e.Â Bayesian posterior)\n\nDoesnâ€™t exhibit IIA (See Econometrics, Discrete Choice Models &gt;&gt; Multinomial Logit &gt;&gt; Substitution Patterns) since correlations between alternatives are modeled.\n\nRandom Coefficients\n\\[\nU_{nj} = \\hat{\\alpha}\\boldsymbol{x}_{nj} + \\hat{\\mu}\\boldsymbol{z}_{nj} + \\epsilon_{nj}\n\\]\n\n\\(x_{nj}\\), \\(z_{nj}\\) - data for alternative j and decision maker n\n\\(\\hat{\\alpha}\\) - vector of fixed coefficients (i.e.Â same for all decision makers)\n\\(\\hat{\\mu}_n\\) - vector of random coefficients (i.e.Â a coefficient for each decision maker)\n\\(\\epsilon_{nj}\\) - residual from an extreme value distribution (e.g.Â Gumbel)\nCorrelated Random Utility\n\nLet \\(\\nu_{nj} = \\boldsymbol{\\hat{\\mu}}_n \\boldsymbol{z}_{nj} + \\epsilon_{nj}\\) be the random (unobserved) component of utility and \\(\\mbox{Cov}(\\nu_{ni}, \\nu_{nj}) = \\boldsymbol{z}_{ni} \\Sigma \\boldsymbol{z}_{nj}\\) the covariance between random utilities of different alternatives where \\(\\Sigma\\) is the variance/covariance matrix for \\(\\boldsymbol{\\hat{\\mu}}\\)\nThis is a structure to model the correlation between alternatives\n\n\nPanel Data\n\nData where each decision maker makes multiple choices over time periods\nModel allows for unobserved preference variation through random coefficients, which yields correlations in utility over time for the same decision maker\nDecision maker, n, chooses from a vector of alternatives over T time periods\nLag or lead predictors can be included\nLagged responses can be included\n\nUse cases:\n\nHabit Formation\nVariety-Seeking Behavior\nSwitching Costs\nBrand Loyalty\n\n\nOnly models a sequence of static choices\n\nLagged responses account for past choices affecting current choices, but not how current choices affect future choices\nA fully dynamic discrete choice model models how every choice affects subsequent choices.\n\n\nExample: {mlogit}\n\nâ€œrparâ€ specifies the random coefficients"
  },
  {
    "objectID": "qmd/regression-multinomial.html#multinomial-probit",
    "href": "qmd/regression-multinomial.html#multinomial-probit",
    "title": "Multinomial",
    "section": "Multinomial Probit",
    "text": "Multinomial Probit\n\nAssumes a Normal distribution of errors which can deal with heteroscedasticity and correlation of the errors."
  },
  {
    "objectID": "qmd/regression-ordinal.html#sec-reg-ord-misc",
    "href": "qmd/regression-ordinal.html#sec-reg-ord-misc",
    "title": "60Â  Ordinal",
    "section": "60.1 Misc",
    "text": "60.1 Misc\n\nNotes from\n\nAnalysis of ordinal data with cumulative link models â€” estimation with the R-package ordinal\n{ordinal} vignette, see below\n\nPackages\n\nCumulative Link Models (CLMs)\n\n{ordinal}\n\nCLMs and CLMMs with location, scale and nominal effects (aka partial proportional odds), structured thresholds and flexible link functions\nmethods for marginal means, tests of functions of the coefficients, goodness-of-fit tests\nvignette\n*response variable should be an ordered factor class\nCompatible with {marginaleffects}, {emmeans}, {ggeffects}\n\n{MASS::polr}\n\nstandard CLMs allowing for the 5 standard link functions but no further extensions\n\n{VGAM::vglm}\n\nCLMs using the cumulative link; allows for several link functions as well as partial effects.\n\n{rms::lrm, orm}\n\nCLMs with the 5 standard link functions but without scale effects, partial or structured thresholds.\n\n{mvord}:\n\nAn R Package for Fitting Multivariate Ordinal Regression Models\n\n{ordinalNet}:\n\nFits ordinal regression models with elastic net penalty. Supported model families include cumulative probability, stopping ratio, continuation ratio, and adjacent category\n\n{brms}\n\nCLMs that include structured thresholds in addition to random-effects.\n\n{{mord}}\n\nordinal classification and prediction focused at machine learning applications\n\n\n{partykit::ctree}\n\ntakes ordered factors as response vars and handles the rest (see vignette)\n\n\nxgboost\n\nobjective arg: â€œrank:pairwiseâ€, â€œrank:ndcpâ€, â€œrank:mapâ€\nsomething else needs to be done. xgboost::predict returned 0.5 for all values (sliced s1e6)\n\nPaired data: Use robust cluster sandwich covariance adjustment to allow ordinal regression to work on paired data. (Harrell)\nOrdered probit regression: This is very, very similar to running an ordered logistic regression. The main difference is in the interpretation of the coefficients.\nSample size: Both ordered logistic and ordered probit, using maximum likelihood estimates, require sufficient sample size. How big is big is a topic of some debate, but they almost always require more cases than OLS regression.\nHarrell summary and comparison of a PO model vs Logistic in the case of an ordinal outcome\n\n\nFormula is an ordinal outcome, Y, with binary treatment variable, Tx, and adjustment variables, X.\nSometimes researchers tend to collapse an ordinal outcome into a binary outcome. Harrell is showing how using a logistic model is inefficient and lower power than a PO model\nOriginal is interactive with additional information (link)\nincludes: efficiency, infomation used, assumptions, special cases, estimands, misc"
  },
  {
    "objectID": "qmd/regression-ordinal.html#sec-reg-ord-eda",
    "href": "qmd/regression-ordinal.html#sec-reg-ord-eda",
    "title": "60Â  Ordinal",
    "section": "60.2 EDA",
    "text": "60.2 EDA\n\nExamine distributions of the variables to get an idea of potential relationships\n\nCrosstabs for ordinal response and categorical explanatory variables\n\n\nftable(xtabs(~ public + apply + pared, data = dat))\n\n##Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  paredÂ  0Â  1\n## public applyÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â \n## 0Â  Â  Â  unlikelyÂ  Â  Â  Â  Â  Â  Â  175Â  14\n##Â  Â  Â  Â  somewhat likelyÂ  Â  Â  Â  98Â  26\n##Â  Â  Â  Â  very likelyÂ  Â  Â  Â  Â  Â  20Â  10\n## 1Â  Â  Â  unlikelyÂ  Â  Â  Â  Â  Â  Â  25Â  6\n##Â  Â  Â  Â  somewhat likelyÂ  Â  Â  Â  12Â  4\n##Â  Â  Â  Â  very likelyÂ  Â  Â  Â  Â  Â  7Â  3\n\napply is the response variable\nEmpty cells or small cells: If a cell has very few cases, the model may become unstable or it might not run at all.\nCan plot a histogram or compute percentages for each level of the variable\nExample: lvl_1% = lvl_1/N =Â  0.2219, lvl_2% = 0.2488, lvl_3% = 0.25, and lvl_4% = 0.2794\n\nSkewed and monotonicly increasing\n\nExample\n\nIf distributions of a pre-treatment/baseline (outcome) variable looks substantially different than the (post-treatment) outcome variable, then the treatment likely had an effect\n\nDescriptive stats for numeric explanatory variables\nBi-variate boxplots\n\nLook for trends of the median\nExample: ordinal vs ordinal\n\nNo idea what the red/blue splits mean\nY-Axis: ordinal outcome: 4 levels (y-xis is messed up)\nX-Axis: pre-treatment variable: 7 levels\nBoxes have the counts\nSteady increase in the median\n\nMakes sense because someone starting w/a higher score (pre-treatment) should mostly end up with a higher score (post-treatment)\n\n\nExample: numeric vs ordinal\n\n\nggplot(dat, aes(x = apply, y = gpa)) +\nÂ  geom_boxplot(size = .75) +\nÂ  geom_jitter(alpha = .5) +\nÂ  facet_grid(pared ~ public, margins = TRUE) +\nÂ  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1))\n\nIn the lower right hand corner, is the overall relationship between apply and gpa which appears slightly positive.\nPre-intervention means of the outcome by explanatory variables\n\nExample\n\n\nCC = 0, TV = 0 2.152\nCC = 0, TV = 1 2.087\nCC = 1, TV = 0 2.050\nCC = 1, TV = 1 1.979\n\nCC and TV are binary explanatory variables\nThe mean score of the pre-treatment outcome doesnâ€™t change much given these two explanatory variables.\nI think this shows that for the most part that assignment of the two different treatments was balanced in terms of the scores of the baseline variable."
  },
  {
    "objectID": "qmd/regression-ordinal.html#sec-reg-ord-diag",
    "href": "qmd/regression-ordinal.html#sec-reg-ord-diag",
    "title": "60Â  Ordinal",
    "section": "60.3 Diagnostics",
    "text": "60.3 Diagnostics\n\nDeviance: -2*(LogLik Reduced Model - LogLik Saturated model)\n\nlikelihood ratio statistic for the comparison of the full and reduced models\nReduced model is the model you just fit.\n\nCan usually be extracted from the model object (e.g.Â {ordinal}ll_reduced &lt;- mod$logLik)\n\nSaturated model also called the Full model (Also see Regression, Discrete &gt;&gt; Misc)\n\nThe full model has a parameter for each observation and describes the data perfectly while the reduced model provides a more concise description of the data with fewer parameters.\nUsually calculated from the data themselves\n\n\n\ndata(wine, package = \"ordinal\")\ntab &lt;- with(wine, table(temp:contact, rating))\n## Get full log-likelihood (aka saturated model log-likelihood)\npi.hat &lt;- tab / rowSums(tab)\n(ll.full &lt;- sum(tab * ifelse(pi.hat &gt; 0, log(pi.hat), 0))) ## -84.01558\n\nGOF: as a rule of thumb, if the deviance about the same size as the difference in the number of parameters (i.e.Â pfull - preduced), there is NOT evidence of lack of fit. ({ordinal} vignette, pg 14)\n\nExample (have doubts this is correct)\n\nLooking at the number of params (â€œno.parâ€) for fm1 in Example: {ordinal}, model selection with LR tests below and the model summary in Proportional Odds (PO) &gt;&gt; Example: {ordinal}, response = wine rating (1 to 5 = most bitter), the number of parameters for the reduced model is the number of regression parameters (2) + number of thresholds (4)\nFor the full model (aka saturated), the number of thresholds should be the same, and there should be one more regression parameter, an interaction between â€œtempâ€ and â€œcontactâ€. So, 7 should be the number of parameters for the full model\nTherefore, for a good-fitting model, the deviance should be close to pfull - preduced = 7 - 6 = 1\nThis example uses â€œnumber of parametersâ€ which is the phrase in the vignette but I think itâ€™s possible he might mean degrees of freedom (dof) which he immediatedly discusses afterwards. In the LR Test example below, under LR.Stat, which is essentially what deviance is, the number is around 11 which is quite aways from 1. Not exactly an apples to apples comparison, but the size after adding 1 parameter just makes me wonder if dof would match this scale of numbers for deviances better.\n\n\nModel selection: A difference in deviance between two nested models is identical to the likelihood ratio statistic for the comparison of these models\n\nsee below, Example: {ordinal}, model selection with LR test\n\nRequirement: Deviance tests are fine if the expected frequencies under the reduced model are not too small and as a general rule they should all be at least five.\n\nAlso see Discrete Analysis notebook\n\nResidual Deviance: Dresid = Dtotal - Dreduced\n\na concept similar to a residual sums of squares (RSS)\nTotal Deviance (Dtotal) is the Null Deviance: -2*(LogLik Null Model - LogLik Saturated model)\n\nanalogous to the total sums of squares for linear models\n\nDreduced is the calculation of â€œDevianceâ€ shown above\nSee example 7, pg 13 ({ordinal} vignette) for (manual) code\n\nExample: {ordinal}, model selection with LR tests\n\nfm2 &lt;- clm(rating ~ temp, data = wine)\nanova(fm2, fm1)\n\nLikelihood ratio tests of cumulative link models:\nÂ  Â  formula:Â  Â  Â  Â  Â  Â  Â  Â  link:Â  threshold:\nfm2 rating ~ tempÂ  Â  Â  Â  Â  logitÂ  flexible\nfm1 rating ~ temp + contact logitÂ  flexible\nÂ  Â  no.par AICÂ  Â  logLikÂ  LR.stat df Pr(&gt;Chisq)\nfm2 5Â  Â  Â  194.03 -92.013\nfm1 6Â  Â  Â  184.98 -86.492Â  11.043Â  1Â  0.0008902 ***\n\nFor â€œfm1â€ model, see Proportional Odds (PO) &gt;&gt; Example {ordinal}, response = wine rating (1 to 5 = most bitter)\nSpecial method for clm objects from the package; produces an Analysis of Deviance (ANODE) table\nAdding â€œcontactâ€ produces a better model (pval &lt; 0.05)\nExample: {ordinal}, variable LR tests\n\ndrop1(fm1, test = \"Chi\")\n\nrating ~ contact + temp\nÂ  Â  Â  DfÂ  AICÂ  Â  LRTÂ  Â  Pr(&gt;Chi)\n&lt;none&gt;Â  Â  184.98\ncontact 1Â  194.03 11.043 0.0008902 ***\ntempÂ  Â  1Â  209.91 26.928 2.112e-07 ***\n\nfm0 &lt;- clm(rating ~ 1, data = wine)\nadd1(fm0, scope = ~ temp + contact, test = \"Chi\")\n\nÂ  Â  Â  Â  DfÂ  AICÂ  Â  LRTÂ  Â  Â  Pr(&gt;Chi)\n&lt;none&gt;Â  Â  Â  215.44\ntempÂ  Â  1Â  Â  194.03Â  23.4113Â  1.308e-06 ***\ncontact 1Â  Â  209.91Â  7.5263Â  0.00608 **\n\nFor â€œfm1â€ model, see Proportional Odds (PO) &gt;&gt; Example {ordinal}, response = wine rating (1 to 5 = most bitter)\ndrop1\n\nTests the same thing as the Wald tests in the summary except with Ï‡2 instead of t-tests\n\ni.e.Â whether the estimates, while controlling for the other variables, differ from 0, except with LR tests.\n\nIn this case, LR tests slightly more significant than the Wald tests\n\nadd1\n\nTests variables where theyâ€™re the only explanatory variable in the model\nBoth variables still significant even without controlling for the other variable"
  },
  {
    "objectID": "qmd/regression-ordinal.html#sec-reg-ord-log",
    "href": "qmd/regression-ordinal.html#sec-reg-ord-log",
    "title": "60Â  Ordinal",
    "section": "60.4 Logistic Regression",
    "text": "60.4 Logistic Regression"
  },
  {
    "objectID": "qmd/regression-ordinal.html#sec-reg-ord-clm",
    "href": "qmd/regression-ordinal.html#sec-reg-ord-clm",
    "title": "60Â  Ordinal",
    "section": "60.5 Cumulative Link Models (CLM)",
    "text": "60.5 Cumulative Link Models (CLM)\n\nA general class of ordinal regression models that include many of the models in this note.\nTypes\n\nproportional odds model: CLM with a logit link\nproportional hazards model - CLM with a log-log link, for grouped survival times\n\n\nlink function: log[âˆ’ log(1 âˆ’ Î³)]\ninverse link: 1 âˆ’ exp[âˆ’ exp(Î·)]\ndistribution: Gumbel (min)b (?)\n1 âˆ’ Î³j(xi) is the probability or survival beyond category j given xi .\n\npartial proportional odds - also referred to as unequal slopes, partial effects, and nominal effects\ncumulative link mixed models (CLMM) - CLMs with normally distributed random effects\n\nLink functions\n\n\nNote the shape of the distributions of the response (see EDA) to help choose a link function\n\nSee Proportional Odds (PO) &gt;&gt; Example: {ordinal}, weighted complimentary log-log regression, comparing links\nThe probit link is often used when the model is interpreted with reference to a latent variable\nKurtotic, see Statistical Concepts &gt;&gt; Fundamentals &gt;&gt; Kurtosis (i.e.Â higher sharper peaks w/short tails, flatter peaks w/long tails)\n\nDefault parameter values fit a symmetric heavy tailed distribution (high, sharp peak)\n\n\n\nModel\n\n\nj is the jth ordinal category where j1 &lt; j2 &lt; â€¦\ni is the ith observation\nthe regression part xTi Î² is independent of j, so Î² has the same effect for each of the J âˆ’ 1 cumulative logits\nThe {Î¸j} parameters provide each cumulative logit (for each j, see below) with its own intercept\n\nÎ¸ is called a â€œthreshold.â€ See below, Latent Variable Concept\n\n\nResponse variable follows the Multinomial distribution\n\n\nThe output is the probability that the response is the jth category or lower.\nÏ€ij denotes the probability that the ith observation falls in response category j\n\nCumulative Logits (Logit Link)\n\n\nj = 1 , â€¦ , J - 1 so cumulative logits are defined for all but the last category\nIf x represent a treatment variable with two levels (e.g., placebo and treatment), then x2 âˆ’ x1 =Â  0 - 1 = -1 and the odds ratio is exp(âˆ’Î²treatment).\n\nSimilarly the odds ratio of the event Y â‰¥ j is exp(Î²treatment) (i.e.Â inverse of Y â‰¤ j).\n\n\nLatent Variable Concept\n\nNotes from article\nâ€œTo motivate the ordinal regression model, it is often assumed that there is an unobservable latent variable ( y* ) which is related to the actual response through theâ€threshold concept.â€ An example of this is when respondents are asked to rate their agreement with a given statement using the categories â€œDisagree,â€ â€œNeutral,â€ â€œAgree.â€ These three options leave no room for any other response, though one can argue that these are three possibilities along a continuous scale of agreement that would also make provision for â€œStrongly Agreeâ€ and â€œDisagree somewhat.â€ The ordinal responses captured in y and the latent continuous variable y* are linked through some fixed, but unknown, thresholds.â€\nA response occurs in category j (Y = j) if the latent response process y* exceeds the threshold value, Î¸j-1 , but does not exceed the threshold value, Î¸j .\n\nI think y* is continous on the scale of logits. Î¸js are also intercepts in the model equations and also on the scale of logits (see Proportional Odds (PO) &gt;&gt; Example: {MASS::polr})\n\nThe cumulative probabilities are given in terms of the cumulative logits with J âˆ’1 strictly increasing model thresholds Î¸1, Î¸2, â€¦ , Î¸J-1 .\n\nWith J = 4, we would have J âˆ’1 = 3 cumulative probabilities, given in terms of 3 thresholds Î¸1, Î¸2, and Î¸3 . The thresholds represent the marginal response probabilities in the J categories.\nEach cumulative logit is a model equation with a threshold for an intercept\n\nTo set the location of the latent variable, it is common to set a threshold to zero. Usually, the first of the threshold parameters (Î¸1) is set to zero.\n\nAlternatively, the model intercept (Î²0) is set to zero and J âˆ’1 thresholds are estimated. (Think this is the way {ordinal} does it.)\n\n\nStructured Thresholds\n\nIf ratings are the response variable, placing restrictions on thresholds and fitting a model allows us to test assumptions on how the judges are using the response scale\nAn advantage of applying additional restrictions on the thresholds is model has fewer parameters to estimate which increases model sensitivity.\nThreshold distances are affected by the shape of the latent variable distribution which is determined by the link function used\n\ni.e.Â if itâ€™s determined that threshold distances are equidistant under a normal distribution (logit link) assumption, then they will not be equidistant if a different link function is used.\n\nExample: is the reponse scale being treated by judges as equidistant between values.\n\ne.g.Â is the distance between a rating of 2 and a rating of 1 the same as the distance between a rating of 2 and a rating of 3?\nMathematically: Î¸j âˆ’ Î¸jâˆ’1 = constant for j = 2, â€¦, Jâˆ’1 where Î¸ are the thresholds and J is the number of levels in the response variable.\nFit equidistant model\n\n\n\nfm.equi &lt;- clm(rating ~ temp + contact, data = wine, threshold = \"equidistant\")\nsummary(fm.equi)\n\nlinkÂ  thresholdÂ  nobsÂ  logLikÂ  AICÂ  Â  niterÂ  max.gradÂ  cond.H\nlogit equidistant 72Â  -87.86Â  183.73 5(0)Â  4.80e-07Â  3.2e+01\n\nÂ  Â  Â  Â  Â  Estimate Std.Error z.value Pr(&gt;|z|)\ntempwarmÂ  2.4632Â  0.5164Â  Â  4.77Â  Â  1.84e-06 ***\ncontactyes 1.5080Â  0.4712Â  Â  3.20Â  Â  0.00137 **\n\nThreshold coefficients:\nÂ  Â  Â  Â  Â  Â  Estimate Std.Error z.value\nthreshold.1 -1.0010Â  0.3978Â  -2.517\nspacingÂ  Â  Â  2.1229Â  0.2455Â  Â  8.646\n\nspacing: average distance between consecutive thresholds\nCompare spacing parameter with your modelâ€™s average spacing\n\nmean(diff(coef(fm1)[1:4]))\n2.116929\n\nâ€œfm1â€ is from an example in the Proportional Odds (PO) section\nResult: â€œfm1â€ spacing is very close to â€œfm.equiâ€ spacing. Judges are likely applying the response scale as having equal distance between rating values.\nDoes applying threshold restrictions decrease the modelâ€™s GOF\n\nanova(fm1, fm.equi)\nÂ  Â  Â  Â  no.par AICÂ  Â  logLik LR.stat df Pr(&gt;Chisq)\nfm.equi 4Â  Â  Â  183.73 -87.865\nfm1Â  Â  6Â  Â  Â  184.98 -86.492 2.7454Â  2Â  0.2534\n\nNo statistical difference in log-likelihoods. Fewer parameters to estimate is better, so keep the equidistant thresholds"
  },
  {
    "objectID": "qmd/regression-ordinal.html#sec-reg-ord-po",
    "href": "qmd/regression-ordinal.html#sec-reg-ord-po",
    "title": "60Â  Ordinal",
    "section": "60.6 Proportional Odds (PO)",
    "text": "60.6 Proportional Odds (PO)\n\nMisc\n\nThe treatment effect for proportional odds model will be the average of the treatment effects of J-1 logistic regression models where each model is dichotmized at each, but not the last, ordinal level.\n\nThe intercepts in the proportional odds model will be similar to those intercepts from the J-1 logistic regression models\n\nThe intercepts of these logistic models have to be different, but the slopes could (in principle) be the same or same-ish. If they are the same, then the proportional odds assumption holds.\n\nThe proportional odds model has a smaller std.error for its treatment effect than any of the treatment effects of the J-1 logistic regression models (i.e.Â more accurately estimated in p.o. model)\n\nBenefits\n\nIt enforces stochastic ordering (?)\nLots of choices for link functions.\nReasonable model for analysing ordinal data\n\nÎ² will be some sort of appropriately-weighted average of what youâ€™d get for the separate logistic regressions\n\n\n\nAssumption: The independent variable effect is the same for all levels of the ordinal outcome variable\n\nÎ²s (i.e.Â treatment effect) are not allowed to vary with j (i.e.Â response variable levels) or equivalently that the threshold parameters {Î¸j} are not allowed to depend on regression variables\nExample:\n\noutcome = health_status (1 for poor, 2 for average, 3 for good and 4 for excellent)\nindependent variable = family_income (1 for above avg, 0 for below average)\nIf the proportional odds assumption holds then:\n\nThe log odds of being at average health from poor health is â€˜Î²1â€™ if family income increases to above average status.\nThe log odds of being at good heath from average health is â€˜Î²1â€™ if family income increases to above average status.\nThe log odds of being at excellent heath from good health is â€˜Î²1â€™ if family income increases to above average status.\n\n\nTesting the assumption\n\n**Even if the model fails the PO assumption, it can still be useful (see Misc &gt;&gt; Harrell summary and comparison of a PO model vs Logisticâ€¦)\nIssues with testing\n\nSmall Sample Sizes: For some variables, you might not have enough power to detect important violations of the PO assumption.\nLarge Sample Sizes: For some variables, you will detect small, unimportant violations of the PO assumption and reject a good model.\nOmnidirectional goodness-of-fit tests donâ€™t tell you which variables you should look at for improvements.\n\nYou can test this assumption by fitting a PO model and PPO model. Then, comparing both models via LR Test or using {ordinal::nominal_test} (see below, Partial Proportional Odds (PPO) examples)\nAssess graphically (article, Harrell RMS pg 316)\n\n1.) Calculate estimated effects for each explanatory variable in a univariate logistic regression model (e.g.Â glm(apply ~ pared, family = binomial))\n\n\n\n\nsf &lt;- function(y) {\nÂ  c('Y&gt;=1' = qlogis(mean(y &gt;= 1)),\nÂ  Â  'Y&gt;=2' = qlogis(mean(y &gt;= 2)),\nÂ  Â  'Y&gt;=3' = qlogis(mean(y &gt;= 3)))\n}\n(s &lt;- with(dat, summary(as.numeric(apply) ~ pared + public + gpa, fun=sf)))\n## +-------+-----------+---+----+--------+------+\n## |Â  Â  Â  |Â  Â  Â  Â  Â  |NÂ  |Y&gt;=1|Y&gt;=2Â  Â  |Y&gt;=3Â  |\n## +-------+-----------+---+----+--------+------+\n## |paredÂ  |NoÂ  Â  Â  Â  |337|Inf |-0.37834|-2.441|\n## |Â  Â  Â  |YesÂ  Â  Â  Â  | 63|Inf | 0.76547|-1.347|\n## +-------+-----------+---+----+--------+------+\n## |public |NoÂ  Â  Â  Â  |343|Inf |-0.20479|-2.345|\n## |Â  Â  Â  |YesÂ  Â  Â  Â  | 57|Inf |-0.17589|-1.548|\n## +-------+-----------+---+----+--------+------+\n## |gpaÂ  Â  |[1.90,2.73)|102|Inf |-0.39730|-2.773|\n## |Â  Â  Â  |[2.73,3.00)| 99|Inf |-0.26415|-2.303|\n## |Â  Â  Â  |[3.00,3.28)|100|Inf |-0.20067|-2.091|\n## |Â  Â  Â  |[3.28,4.00]| 99|Inf | 0.06062|-1.804|\n## +-------+-----------+---+----+--------+------+\n## |Overall|Â  Â  Â  Â  Â  |400|Inf |-0.20067|-2.197|\n## +-------+-----------+---+----+--------+------+\n\nFor Y â‰¥ 2 and â€œpared = noâ€, -0.37834 is the same as the intercept of the univariate model\nFor Y â‰¥ 2 and â€œpared = yesâ€, 0.76547 is the same as the intercept + coefficient of the univariate model\n2.) For each variable, calculate differences in estimates between levels of the ordinal response\n\ns[, 4] &lt;- s[, 4] - s[, 3]\ns[, 3] &lt;- s[, 3] - s[, 3]\ns\n## +-------+-----------+---+----+----+------+\n## |Â  Â  Â  |Â  Â  Â  Â  Â  |NÂ  |Y&gt;=1|Y&gt;=2|Y&gt;=3Â  |\n## +-------+-----------+---+----+----+------+\n## |paredÂ  |NoÂ  Â  Â  Â  |337|Inf |0Â  |-2.062|\n## |Â  Â  Â  |YesÂ  Â  Â  Â  | 63|Inf |0Â  |-2.113|\n## +-------+-----------+---+----+----+------+\n## |public |NoÂ  Â  Â  Â  |343|Inf |0Â  |-2.140|\n## |Â  Â  Â  |YesÂ  Â  Â  Â  | 57|Inf |0Â  |-1.372|\n## +-------+-----------+---+----+----+------+\n## |gpaÂ  Â  |[1.90,2.73)|102|Inf |0Â  |-2.375|\n## |Â  Â  Â  |[2.73,3.00)| 99|Inf |0Â  |-2.038|\n## |Â  Â  Â  |[3.00,3.28)|100|Inf |0Â  |-1.890|\n## |Â  Â  Â  |[3.28,4.00]| 99|Inf |0Â  |-1.864|\n## +-------+-----------+---+----+----+------+\n## |Overall|Â  Â  Â  Â  Â  |400|Inf |0Â  |-1.997|\n## +-------+-----------+---+----+----+------+\n\nFor pared, the difference in estimates between Y â‰¥ 2 and Y â‰¥ 3 is -2.062 and -2.113. Therefore, it would seem that the PO assumption holds up pretty well for pared\nFor public, the difference in estimates between Y â‰¥ 2 and Y â‰¥ 3 is -2.140 and -1.372. Therefore, it would seem that the PO assumption does NOT hold up for publicÂ \nIf there was a Y â‰¥ 4, then for pared, the difference in estimates between Y â‰¥ 3 and Y â‰¥ 4 should be near 2 as well in order for the PO assumption to hold for that variable.\n3.) Plot plot(s, which=1:3, pch=1:3, xlab='logit', main=' ', xlim=range(s[,3:4]))\n\n\nIn addition to the PO assumption not seemingly holding for public, there also seems to be some substantial differences between the quantiles of gpa.\n\nThe 3rd and 4th seem to be in agreement, but not the 1st and maybe not the 2nd.\n\n\nExample: {ordinal}, response = wine rating (1 to 5 = most bitter)\n\nfm1 &lt;- clm(rating ~ contact + temp, data = wine)\nsummary(fm1)\n\nformula: rating ~ contact + temp\ndata: wine\n\nlink threshold nobs logLikÂ  AICÂ  Â  niter max.gradÂ  cond.H\nlogit flexible 72Â  -86.49Â  184.98Â  6(0)Â  4.01e-12Â  2.7e+01\n\nCoefficients:\nÂ  Â  Â  Â  Â  EstimateÂ  Std.ErrorÂ  z valueÂ  Pr(&gt;|z|)\ncontactyes 1.5278Â  Â  0.4766Â  Â  3.205Â  Â  0.00135 **\ntempwarmÂ  2.5031Â  Â  0.5287Â  Â  4.735Â  Â  2.19e-06 ***\n---\nSignif. codes: 0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1\n\nThreshold coefficients:\nÂ  Â  Estimate Std.Error z value\n1|2 -1.3444Â  0.5171Â  -2.600\n2|3Â  1.2508Â  0.4379Â  Â  2.857\n3|4Â  3.4669Â  0.5978Â  Â  5.800\n4|5Â  5.0064Â  0.7309Â  Â  6.850\n\nconfint(fm1)\nÂ  Â  Â  Â  Â  2.5 %Â  Â  97.5 %\ntempwarmÂ  1.5097627 3.595225\ncontactyes 0.6157925 2.492404\n\nmodel: logit(P(Yi â‰¤ j)) = Î¸j âˆ’ Î²1(tempi ) âˆ’ Î²2(contacti)\n*response variable should be an ordered factor class\n\nIn this example, both explanatory variables were factor variables.\n\nâ€œcond.Hâ€ - condition number of the Hessian is a measure of how identifiable the model is\n\nValues larger than 1e4 indicate that the model may be ill defined\n\nRegression Coefficients\n\nÎ²1(warm âˆ’ cold) = 2.50\n\nThe reference level is cold, and this is the effect of moving from cold to warm\n\nÎ²2(yes âˆ’ no) = 1.53\n\nThe odds ratio of bitterness being rated in category j or above (OR(Y â‰¥ j)) is exp(Î²2(yes âˆ’ no)) = exp(1.53) = 4.61.\n\nNote: this is Pr(Y â‰¥ j) which is why weâ€™re using the positive Î²\n\nThe reference level is no, and this is the effect of moving from no to yes\n\nInterpretation:\n\ncontact and warm temperature both lead to higher probabilities of observations in the high categories\nMe: The odds of the rating being higher are 4.61 times greater when contact = yes than when contact = no.\n\n\nThresholds (aka intercepts)\n\n{ Î¸j} = {âˆ’1.34, 1.25, 3.47, 5.01}\nOften the thresholds are not of primary interest, but they are an integral part of the model. .\nIt is not relevant to test whether the thresholds are equal to zero, so no p-values are provided for this test.\n\nExample: {ordinal}, weighted complimentary log-log regression, comparing links\n\ndata\n\n\nÂ  yearÂ  pct income\n1Â  1960Â  6.5Â  Â  0-3\n2Â  1960Â  8.2Â  Â  3-5\n3Â  1960 11.3Â  Â  5-7\n4Â  1960 23.5Â  7-10\n5Â  1960 15.6Â  10-12\n6Â  1960 12.7Â  12-15\n7Â  1960 22.2Â  Â  15+\n8Â  1970Â  4.3Â  Â  0-3\n9Â  1970Â  6.0Â  Â  3-5\n10 1970Â  7.7Â  Â  5-7\n11 1970 13.2Â  7-10\n12 1970 10.5Â  10-12\n13 1970 16.3Â  12-15\n14 1970 42.1Â  Â  15+\n\nâ€œincomeâ€ (ordered factor) are intervals in thousands of 1973 US dollars\nâ€œpctâ€ (numeric) is percent of the population in that income bracket from that particular year\nâ€œyearâ€ (factor)\nComparing multiple links\n\n&gt; links &lt;- c(\"logit\", \"probit\", \"cloglog\", \"loglog\", \"cauchit\")\n&gt; sapply(links, function(link) {\nclm(income ~ year, data=income, weights=pct, link=link)$logLik })\nlogitÂ  Â  probitÂ  Â  cloglogÂ  loglogÂ  Â  cauchit\n-353.3589 -353.8036 -352.8980 -355.6028 -352.8434\n\nThe cauchy distribution has the highest log-likelihood and is therefore the best fit to the data, but is closely followed by the complementary log-log link\nFit the cloglog for interpretation convenience\n\nmod &lt;- clm(income ~ year, data=income, weights=pct, link=\"cloglog\")\nsummary(mod)\n\nlinkÂ  Â  threshold nobsÂ  logLikÂ  AICÂ  Â  niter max.grad cond.HÂ \ncloglog flexibleÂ  200.1 -352.90 719.80 6(0)Â  1.87e-11 7.8e+01\n\nÂ  Â  Â  Â  Estimate Std. Error z value Pr(&gt;|z|)Â  Â \nyear1970Â  0.5679Â  Â  0.1749Â  3.247Â  0.00116 **\n\nThreshold coefficients:\nÂ  Â  Â  Â  Â  Â  Estimate Std. Error z value\n0-3|3-5Â  Â  -2.645724Â  0.310948Â  -8.509\n3-5|5-7Â  Â  -1.765970Â  0.210267Â  -8.399\n5-7|7-10Â  Â  -1.141808Â  0.164710Â  -6.932\n7-10|10-12Â  -0.398434Â  0.132125Â  -3.016\n10-12|12-15Â  0.004931Â  0.123384Â  0.040\n12-15|15+Â  Â  0.418985Â  0.120193Â  3.486\n\n** The uncertainty of parameter estimates depends on the sample size, which is unknown here, so hypothesis tests should not be considered **\nInterpretation\n\nâ€œIf p1960(x) is proportion of the population with an income larger than $x in 1960 and p1970(x) is the equivalent in 1970, then approximatelyâ€\n\nSo for any income dollar amount, the proportion of the population in 1960 (or 1970 w/some algebra) with that income or greater can be estimated.\n\nExample: {MASS::polr}, response = apply (3 levels) (article)\n\nm &lt;- polr(apply ~ pared + public + gpa, data = dat, Hess=TRUE)\nsummary(m)\n\n## Coefficients:\n##Â  Â  Â  Â  Â  Value Std. Error t value\n## paredÂ  1.0477Â  Â  Â  0.266Â  3.942\n## public -0.0588Â  Â  Â  0.298Â  -0.197\n## gpaÂ  Â  0.6159Â  Â  Â  0.261Â  2.363\n##Â \n## Intercepts:\n##Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ValueÂ  Std. Error t value\n## unlikely|somewhat likelyÂ  Â  2.204Â  0.780Â  Â  Â  2.827Â \n## somewhat likely|very likelyÂ  4.299Â  0.804Â  Â  Â  5.345Â \n##Â \n## Residual Deviance: 717.02Â \n## AIC: 727.02\n\nResponse: â€œapplyâ€ to grad school with levels â€œunlikelyâ€, â€œsomewhat likelyâ€, and â€œvery likelyâ€ (coded 1, 2, and 3)\n\nThe researchers believe there is a greater distance between â€œsomewhat likelyâ€ and â€œvery likelyâ€ than â€œsomewhat likelyâ€ and â€œunlikelyâ€\n\nSee Cumulative Link Models (CLM) &gt;&gt; Structured Thresholds\n\n\nExplanatory:\n\nâ€œparedâ€ - parentâ€™s education (0/1),Â  graduate school or not\nâ€œpublicâ€ - public school or not (i.e.Â private) (0/1)\nâ€œgpaâ€ - numeric\n\nModel\n\n\nequation shows the thresholds (aka cutpoints) as intercepts and the minus sign propugated through the rest of the regression equation\n\nAdd p-values to the summary table\n\n## store table\n(ctable &lt;- coef(summary(m)))\n## calculate and store p values\np &lt;- pnorm(abs(ctable[, \"t value\"]), lower.tail = FALSE) * 2\n## combined table\n(ctable &lt;- cbind(ctable, \"p value\" = p))\n\nCIs:\n\nprofile CIs: confint(m)\nAssuming normality: confint.default(m)\n\nInterpretation:\n\nIn ordered log odds (aka ordered logits)\n\npared: â€œfor a one unit increase inâ€paredâ€ (i.e., going from 0 to 1), we expect a 1.05 increase in the expected value of â€œapplyâ€ on the log odds scale, given all of the other variables in the model are held constant.â€\ngpa: â€œfor one unit increase inâ€gpaâ€, we would expect a 0.62 increase in the expected value of â€œapplyâ€ in the log odds scale, given that all of the other variables in the model are held constant\n\nIn odds ratios (aka proportional odds ratios): (w/CIs exp(cbind(OR = coef(m), ci = confint(m))\n\nThis is a proportional odds model so the effect size is the same at all levels of the response variable\npared:\n\nâ€œFor students whose parents did have a grad degree, the odds of being more likely (i.e., very or somewhat likely versus unlikely) to apply is 2.85 times that of students whose parents did NOT have a grad degree, holding constant all other variables.â€\nâ€œFor students whose parents did have a grad degress, the odds of being less likely to apply (i.e., unlikely versus somewhat or very likely) is 2.85 times that of students whose parents did NOT have a grad degree, holding constant all other variables.â€\n\npublic\n\nFor students in public school, the odds of being more likelyÂ (i.e., very or somewhat likely versus unlikely) to apply is 5.71% lower [i.e., (1 -0.943) x 100%] than private school students, holding constant all other variables.\n\nFor students in private school, the odds of being more likely to apply is 1.06 times [i.e., 1/0.943] that of public school students, holding constant all other variables (positive odds ratio).\n\nFor students in private school, the odds of being less likely to applyÂ (i.e., unlikely versus somewhat or very likely) is 5.71% lower than public school students, holding constant all other variables.\n\nFor students in public school, the odds of beingÂ less likely to apply is 1.06 times that of private school students, holding constant all other variables (positive odds ratio).\n\n\ngpa\n\nFor every one unit increase in studentâ€™s GPA the odds of being more likely to apply (very or somewhat likely versus unlikely) is multiplied 1.85 times (i.e., increases 85%), holding constant all other variables.\nFor every one unit decrease in studentâ€™s GPA the odds of being less likely to apply (unlikely versus somewhat or very likely) is multiplied 1.85 times, holding constant all other variables.\n\n\nPredictions: predict(m, data = newdat, type = \"probs\")\nPlot Predictions\n\n\n\nnewdat &lt;- cbind(newdat, predict(m, newdat, type = \"probs\"))\nlnewdat &lt;- melt(newdat, id.vars = c(\"pared\", \"public\", \"gpa\"),\nÂ  variable.name = \"Level\", value.name=\"Probability\")\nggplot(lnewdat, aes(x = gpa, y = Probability, colour = Level)) +\nÂ  geom_line() + facet_grid(pared ~ public, labeller=\"label_both\")"
  },
  {
    "objectID": "qmd/regression-ordinal.html#sec-reg-ord-ppo",
    "href": "qmd/regression-ordinal.html#sec-reg-ord-ppo",
    "title": "60Â  Ordinal",
    "section": "60.7 Partial Proportional Odds (PPO)",
    "text": "60.7 Partial Proportional Odds (PPO)\n\nÎ²s are allowed to vary with j (ordinal outcome levels) or equivalently, the threshold parameters {Î¸j} are allowed to depend on regression variables.\nTwo sets of thresholds are applied at conditions with and without an explanatory variable\nExample: {ordinal}, PPO for â€œcontactâ€\n\nfm.nom &lt;- clm(rating ~ temp, nominal = ~ contact, data = wine)\nsummary(fm.nom)\n\nlink threshold nobs logLikÂ  AICÂ  Â  niterÂ  max.gradÂ  cond.H\nlogit flexible 72Â  -86.21Â  190.42Â  6(0)Â  1.64e-10Â  4.8e+01\n\nÂ  Â  Â  Â  EstimateÂ  Std.ErrorÂ  z.valueÂ  Pr(&gt;|z|)\ntempwarm 2.519Â  Â  0.535Â  Â  Â  4.708Â  Â  2.5e-06 ***\n\nThreshold coefficients:\nÂ  Â  Â  Â  Â  Â  Â  Â  Estimate Std.ErrorÂ  z.value\n1|2.(Intercept) -1.3230Â  0.5623Â  Â  -2.353\n2|3.(Intercept)Â  1.2464Â  0.4748Â  Â  2.625\n3|4.(Intercept)Â  3.5500Â  0.6560Â  Â  5.411\n4|5.(Intercept)Â  4.6602Â  0.8604Â  Â  5.416\n1|2.contactyesÂ  -1.6151Â  1.1618Â  Â  -1.390\n2|3.contactyesÂ  -1.5116Â  0.5906Â  Â  -2.559\n3|4.contactyesÂ  -1.6748Â  0.6488Â  Â  -2.581\n4|5.contactyesÂ  -1.0506Â  0.8965Â  Â  -1.172\n\nmodel: logit(P(Yi â‰¤ j)) = Î¸j + Î²Ëœj (contacti) âˆ’ Î²(tempi)\nTwo sets of thresholds are applied at conditions with (yes) and without (no) â€œcontactâ€\nResponse variable should be an ordered factor class\n\nIn this example, both explanatory variables were factor variables.\n\nIt is not possible to estimate both Î²2(contacti) and Î²Ëœj(contacti) in the same model. Consequently variables that appear in â€œnominalâ€ cannot enter in the formula as well\nResults\n\nÎ²Ë†(warm âˆ’ cold) = 2.52\n\nThe reference level is cold, and this is the effect of moving from cold to warm\n\n{ Ë†Î¸j} = {âˆ’1.32, 1.25, 3.55, 4.66}\n{Î²Ë†Ëœj (yes âˆ’ no){style=â€˜color: #990000â€™}} = {âˆ’1.62, âˆ’ 1.51, âˆ’ 1.67, âˆ’ 1.05}\n\nThe odds ratio of bitterness being rated in category j or above (OR(Y â‰¥ j)) now vary with j:\n\nORs = {exp(âˆ’Î² Ë†Ëœ j (yesâˆ’ no)){style=â€˜color: #990000â€™}} = exp{-(-1.6151), -(-1.5116), -(-1.6748), -(-1.0506){style=â€˜color: #990000â€™}} = {5.03, 4.53, 5.34, 2.86}\nMe: The odds of the rating being greater or equal to 2 are 4.53 times greater when contact = yes than when contact = no.\n\nExample: {ordinal}, Test the PO assumption for 1 variable w/LR Test\n\nanova(fm1, fm.nom)\n\nÂ  Â  Â  formula:Â  Â  Â  Â  Â  Â  Â  Â  nominal: link: threshold:\nfm1Â  Â  rating ~ temp + contactÂ  ~1Â  Â  Â  logit flexible\nfm.nom rating ~ tempÂ  Â  Â  Â  Â  Â  ~contact logit flexible\n\nÂ  Â  Â  no.par AICÂ  Â  logLikÂ  LR.stat df Pr(&gt;Chisq)\nfm1Â  Â  6Â  Â  Â  184.98 -86.492\nfm.nom 9Â  Â  Â  190.42 -86.209Â  0.5667Â  3Â  0.904\n\nâ€œfm1â€ model from example in Proportional Odds section, but the â€œformulaâ€ column in the summary tells you what variables are in it.\nThere is only little difference in the log-likelihoods of the two models and the test is insignificant (pval &lt; 0.05). Thus there is no evidence that the proportional odds assumption is violated for contact.\nExample: {ordinal}, Test the PO assumption for all variables w/LR Test\n\nnominal_test(fm1)\n\nÂ  Â  Â  Â  DfÂ  logLikÂ  AICÂ  Â  LRTÂ  Â  Pr(&gt;Chi)\n&lt;none&gt;Â  Â  -86.492Â  184.98\ntempÂ  Â  3Â  -84.904Â  187.81Â  3.1750Â  0.3654\ncontact 3Â  -86.209Â  190.42Â  0.5667Â  0.9040\n\nBehavior of this function is similar to the anova example above except the action is taken with every variable in the formula\n\nIf any variables are in the â€œscaleâ€ argument, they are also tested.\n\nNote that â€œcontactâ€ has the same LRT, pval as the previous example\nNo pvals &lt; 0.05, therefore no violations of the PO assumption for any variable."
  },
  {
    "objectID": "qmd/regression-ordinal.html#sec-reg-ord-se",
    "href": "qmd/regression-ordinal.html#sec-reg-ord-se",
    "title": "60Â  Ordinal",
    "section": "60.8 Scale Effects",
    "text": "60.8 Scale Effects\n\nAn alternative to nominal effects (partial proportional odds) when nonproportional odds structures are encountered in the data.\n\nUsing scale effects is often a better approach, and it uses fewer parameters which often lead to more sensitive tests than nominal effects\n\nSomething about allowing the scale parameter of the latent variable distribution to depend on explanatory variables.\n\nlatent distribution is determined by the choice of link function\n\nExample: {ordinal}, allow scale to vary according to a variable\n\nfm.sca &lt;- clm(rating ~ temp + contact, scale = ~ temp, data = wine)\nsummary(fm.sca)\n\nlink threshold nobs logLik AIC niter max.grad cond.H\nlogit flexible 72 -86.44 186.88 8(0) 5.25e-09 1.0e+02\n\nÂ  Â  Â  Â  Â  EstimateÂ  Std.Error z.valueÂ  Pr(&gt;|z|)\ntempwarmÂ  2.6294Â  Â  0.6860Â  Â  3.833Â  Â  0.000127 ***\ncontactyes 1.5878Â  Â  0.5301Â  Â  2.995Â  Â  0.002743 **\n\nlog-scale coefficients:\nÂ  Â  Â  Â  EstimateÂ  Std.Error z.valueÂ  Pr(&gt;|z|)\ntempwarm 0.09536Â  0.29414Â  0.324Â  Â  0.746\n\nThreshold coefficients:\nÂ  Â  Estimate Std.ErrorÂ  z value\n1|2 -1.3520Â  0.5223Â  Â  -2.588\n2|3Â  1.2730Â  0.4533Â  Â  2.808\n3|4Â  3.6170Â  0.7774Â  Â  4.653\n4|5Â  5.2982Â  1.2027Â  Â  4.405\n\nModel\n\n\nScale parameter of the latent variable distribution varies by â€œtempâ€\n\nThe scale term in the denominator. See Cumulative Link Models (CLM) &gt;&gt; model for more details on this term\n\nLocation parameter of the latent distribution is allowed to depend on both â€œtempâ€ and â€œcontactâ€\n\nThe location expression is the numerator\n\n\nResponse variable should be an ordered factor class\n\nIn this example, both explanatory variables were factor variables.\n\nResults\n\nLittle to no idea what any of this practically means\nThe location of the latent distribution is shifted 2.63Ïƒ (scale units) when temperature is warm as compared to cold conditions and 1.59Ïƒ when thereâ€™s presence (yes) of contact as compared to absence (no) of contact. (reference levels: temp: cold, contact: no)\nThe scale of the latent distribution is Ïƒ at cold conditions but Ïƒ âˆ— exp(Î¶(warmâˆ’cold)) = Ïƒ â¨¯ exp(0.095) = 1.10Ïƒ , i.e., 10% higher, at warm conditions.\nThe p value for the scale effect in the summary output shows that the ratio of scales is not significantly different from 1 (or equivalently that the difference on the log-scale is not different from 0).\n\nExample: Detect scale effects of variables\n\nscale_test(fm1)\n\nÂ  Â  Â  Â  Df logLikÂ  AICÂ  Â  LRTÂ  Â  Pr(&gt;Chi)\n&lt;none&gt;Â  Â  -86.492Â  184.98\ntempÂ  Â  1Â  -86.439Â  186.88 0.10492 0.7460\ncontact 1Â  -86.355Â  186.71 0.27330 0.6011\n\nâ€œfm1â€ comes from example in Proportional Odds (PO) section\nFunction has similar behavior to nominal_test (see example in Partial Proportional Odds (PPO) section)\nNo pvals &lt; 0.05, therefore no evidence of scale effects for any variable."
  },
  {
    "objectID": "qmd/regression-ordinal.html#sec-reg-ord-gor",
    "href": "qmd/regression-ordinal.html#sec-reg-ord-gor",
    "title": "60Â  Ordinal",
    "section": "60.9 Generalized Ordinal Regression",
    "text": "60.9 Generalized Ordinal Regression"
  },
  {
    "objectID": "qmd/regression-other.html#sec-reg-other-misc",
    "href": "qmd/regression-other.html#sec-reg-other-misc",
    "title": "61Â  Other",
    "section": "61.1 Misc",
    "text": "61.1 Misc\n\nHarrell: It is not appropriate to compute a mean or run parametric regression on â€œ% changeâ€ unless you first compute log((%_change/100) + 1)Â  to undo damage done by % change\nReaction time data can be modelled using several families of skewed distributions (LindelÃ¸v, 2019)"
  },
  {
    "objectID": "qmd/regression-other.html#sec-reg-other-other",
    "href": "qmd/regression-other.html#sec-reg-other-other",
    "title": "61Â  Other",
    "section": "61.2 Other",
    "text": "61.2 Other\n\nRates between 0 and 1\n\nOutcome without 0s and 1s\n\nBeta Regression\n\nOutcome with 0s and 1s\n\nOrdered Beta Regression\n\npaper, {ordbetareg}, {glmmTMB}\nHeiss overview of the model\nHeiss example using a 0,1 Inf Ordered Beta Regression\n\nZero-inflated or Zero-1-inflated beta regression (ZOIB) (see {brms})\nIf the zeros and ones are censored, use tobit.\n\n\nOutcome variable is greater than 0\n\nGamma Regression - Can handle some dispersion with a log link\nCan model multiplicative dgp\nIf zero-inflated, use Tweedie Regression\n\nBounded Outcome Variable\n\nOrdered Beta Regression\n\npaper, {ordbetareg}, {glmmTMB}\nHeiss overview of the model\nHeiss example using a 0,1 Inf Ordered Beta Regression to model an outcome with values between 1 and 32\n\n\nTweedie Regression - Where the frequency of events follows a Poisson distrbution and the amount associated with each event follows an Exponential distribution\n\ne.g.Â Insurance claims, Operational loss (banking)\nTweedie distribution is a Gamma distribution with a spike at zero.\n\nGeneralized Least Squares\n\nPackages\n\n{nlme::gls}\n\nMath - A Deep-Dive into Generalized Least Squares Estimation\nAlso See Weighted Least Squares and Weighted Least Squares &gt;&gt; Feasible Generalized Least Squares"
  },
  {
    "objectID": "qmd/regression-other.html#sec-reg-other-censtrunc",
    "href": "qmd/regression-other.html#sec-reg-other-censtrunc",
    "title": "61Â  Other",
    "section": "61.3 Censored and Truncated Data",
    "text": "61.3 Censored and Truncated Data\n\nCensored Data - Arise if exact values are only reported in a restricted range. Data may fall outside this range but those data are reported at the range limits (i.e.Â at the minimum or maximum of the range)\n\ni.e.Â instances outside the range are recorded in the data but the true values of those instances are not.\nTobit Regression - gaussian response, assumes constant (homoskedastic variance)\n\nLog-Likelihood function\n\n\ndáµ¢ = 0 if yi = 0, and 1 otherwise\n\n1st term (top line) is OLS likelihood\n2nd term (bottom line) accounts for the probability that observation i is censored.\n\n\nMarginal Effect\n\n\nÎ²â‚ is multiplied by the CDF (Î¦) of the Normal distribution\n\nÎ²â‚ is weighted by the probability of y occurring at x\nExample: work_completed ~ hourly rate\n\nÎ²â‚ is weighted by the probability that an individual is willing to work at the present hourly rate, as represented by the CDF.\n\n\nÏƒ is the standard deviation of the modelâ€™s residuals\n\nExample: Right-Censored at 800\nVGAM::vglm(resp ~ pred1 + pred2, \n           family = tobit(Upper = 800), \n           data = dat)\n\n2-Part Models (e.g.Â Hurdle Models) - A binary (e.g.Â Probit) regression model fits the exceedance probability of the lower limit and a truncated regression model fits the value given the lower limit is exceeded.\n\nTruncated Data - Arise if exact values are only reported in a restricted range. If data outside this range are omitted completely, we call it truncated\n\ni.e.Â instances outside the range are NOT recorded. No evidence of these instances are in the data.\nTruncated Regression - Also assumes constant (homoskedastic variance)\nA poisson model will try to predict 0s even if 0s are impossible. Therefore, you need a zero-truncated model.\nThe truncated normal model is different from a glm, because Î¼ and Ïƒ are not orthogonal and have to be estimated simultaneously. Misspecification of one parameter will lead to inconsistent estimation of the other. Thatâ€™s why for these models, not only is Î¼ often specified as a function of regressors but also Ïƒ, often in the framework of GAMLSS (generalized additive models of location, scale, and shape).\n\nExpectation: \\(E[y|x] = \\mu + \\sigma + \\frac {\\phi(\\mu / \\sigma)}{\\Phi(\\mu / \\sigma)}\\)\n\nWhere Ï•(â‹…) and Î¦(â‹…) are the probability density and cumulative distribution function of the standard normal distribution, respectively. This intrinsically depends on both Î¼ and Ïƒ.\n\n\n\nHeteroskadastic Variance - The variance of an underlying normal distribution does depend on covariates\n\n{crch}\n\nExamples\n\nInsurance: There is a claim on a policy that has a payout limit of u and a deductible of d,\n\nAny loss amount that is greater than u will be reported to the insurance company as a loss of u - dÂ  because that is the amount the insurance company has to pay.\nInsurance loss data is left-truncated because the insurance company doesnâ€™t know if there are values below the deductible d because policyholders wonâ€™t make a claim.\n\nâ€œtruncatedâ€ because the values (claims) are below d, so the instances arenâ€™t recorded in the data.\nâ€œleftâ€ because the values are below d and not above\n\nInsurance loss data is also right-censored if the loss is greater than u because u is the most the insurance company will pay. Thus, it only knows that your claim is greater than u, not the exact claim amount.\n\nâ€œcensoredâ€ because the values (claims) that are exactly u (policy limit) imply that claim is greater than u, so the instances are recorded but the true values are unknown.\nâ€œrightâ€ because the values are above u and not below\n\n\nMeasuring Wind Speed: The instrument needs a minimum wind speed, m, to start working.\n\nIf wind speeds below this minimum are recorded as the minimum value, m, the data is censored.\n\ni.e.Â some other instrument detects a wind instance occurred and that instance is recorded as m even though the true speed of the wind of that instance is unknown.\n\nIf wind speeds below this minimum are NOT recorded at all, the data is truncated.\n\ni.e.Â any wind instances (detected or not) are not recorded. No evidence in the data that these instances will have ever occurred."
  },
  {
    "objectID": "qmd/regression-other.html#sec-reg-other-fracreg",
    "href": "qmd/regression-other.html#sec-reg-other-fracreg",
    "title": "61Â  Other",
    "section": "61.4 Fractional Regression",
    "text": "61.4 Fractional Regression\n\nOutcome with values between 0 and 1\n** Use a fractional logit (aka quasi-binomial) only for big data situations **\n\nThe fractional logit model is not a statistical distribution, leading it to produce biased results.\nSee Kubinec article\n\nRecommends ordered beta regression, continuous bernoulli and provides examples\n\nIn a big data situation, it respects the bounds of proporitional/fractional outcomes, and is significantly easier to fit than the other alternatives.\nHaving a large dataset means that inefficiency or an incorrect form for the uncertainty of fractional logit estimates is unlikely to affect decision-making or inference.\n\nBeta: values lie between zero and one\n\nsee {betareg}, {DirichletReg}, {mgcv}, {brms}\n{ordbetareg}\n\nZero/One-Inflated Beta: larger percentage of the observations are at the boundaries (i.e.Â high amounts of 0s and 1s\n\nSee {brms}, {VGAM}, {gamlss}\n\nLogistic, Quasi-Binomial, or GAM w/robust std.errors: outcome, y, is 0 â‰¤ y â‰¤ 1 (i.e.Â 0s and 1s included)\n\nExample\nlibrary(lmtest)\nlibrary(sandwich)\n# logistic w/robust std.errors\nmodel_glm = glm(\nÂ  prate ~ mrate + ltotemp + age + sole,\nÂ  data = d,\nÂ  family = binomial\n)\nse_glm_robust &lt;- coeftest(model_glm, vcov = vcovHC(model_glm, type=\"HC\"))\n# quasi-binomial w/robust std.errors\nmodel_quasi = glm(\nÂ  prate ~ mrate + ltotemp + age + sole,\nÂ  data = d,\nÂ  family = quasibinomial\n)\nse_glm_robust_quasi = coeftest(model_quasi, vcov = vcovHC(model_quasi, type=\"HC\"))\n\n# Can also use a GAM to get the same results\n# Useful for more complicated model specifications\nmodel_gam_re = gam(\nÂ  prate ~ mrate + ltotemp + age + sole + s(id, bs = 're'),\nÂ  data = d,\nÂ  family = binomial,\nÂ  method = 'REML'\n)"
  },
  {
    "objectID": "qmd/regression-other.html#sec-reg-other-zeroinftrunc",
    "href": "qmd/regression-other.html#sec-reg-other-zeroinftrunc",
    "title": "61Â  Other",
    "section": "61.5 Zero-Inflated and Zero-Truncated",
    "text": "61.5 Zero-Inflated and Zero-Truncated\n\nContinuous\n\nSome economists will use log(1 + Y) or arcsinh(Y) to model a skewed, continous Y with 0s. In this case, the treatment effects (ATE) canâ€™t be interpreted as percents. The effect sizes will depend on the scale of Y. (see Thread)\nSolutions\n\nNormalize Y by a pretreatment baseline\n\nÃ = Y / Ypre\n\nwhere Ypre is the measured Y prior to treatment\n\nIn regression, average treatment effect (ATE) would then be\n\nWhere Y(1) is the value of Y for treated subjects\nInterpretation(e.g outcome = earnings): average treatment effect on earnings expressed as a percentage of pre-treatment earnings\n\n\nNormalizing Y by the expected outcome given observable covariates\n\nÃ = Y / E[Y(0) | X]\n\nY(0) are the observed outcome values for the control group\nThe â€œ|Xâ€ is kind of confusing but I donâ€™t think want the fitted values from a model where the outcome is the Y(0) values. I think theyâ€™d use a Y^ somewhere.\n\nSo I think E[Y(0) | X] just the mean of the Y(0) values\n\nInterpretation of this transformed variable (e.g.Â outcome = earnings)\n\nan individualâ€™s earnings as a percentage of the average control groupâ€™s earnings for people with the same observable characteristics X.\n\n\nAverage Treatment Effect (ATE) Interpretation (e.g.Â outcome = earnings, X = pretreatment earnings, education)\n\nThe average change in earnings as a percentage of the control groupâ€™s earnings for people with the same education and previous earnings.\n\n\n\n\nML 2-step Hurdle\n\nSteps\n\nTransform Target variable to 0/1 where 1 is any count that isnâ€™t a 0.\nUse a classifier to predict 0s (according to some probability threshold)\n\nRemember that models is predicting the probability of being a 1\n\nFilter rows that arenâ€™t predicted to be 0, and predict counts using a regressor model\n\nround-up or round-down predictions based on which results in lower error?\n\n\n\nStatistical\n\nOptions: Poission, Neg.Binomial, Zero-Inf Poisson/Neg.Binomial, Poisson/Neg.Binomial Hurdle\n\nZero-Inf Poisson/Neg.Binomial\n\n\nUses a second underlying process that determines whether a count is zero or non-zero. Once a count is determined to be non-zero, the regular Poisson process takes over to determine its actual non-zero value based on the Poisson processâ€™s PMF.\nÏ•i is the predicted probability from a logistic regression that yi is a 0. This vector of values is then plugged into both probability mass functions."
  },
  {
    "objectID": "qmd/regression-other.html#sec-reg-other-multmod",
    "href": "qmd/regression-other.html#sec-reg-other-multmod",
    "title": "61Â  Other",
    "section": "61.6 Multi-modal",
    "text": "61.6 Multi-modal\n\nAlso see EDA &gt;&gt; Interactions &gt;&gt; Categorical Outcome\n{upsetr} - Might be useful to examine bimodal structure and determine cutpoints based on categorical predictor values and not just outcome values\n{gghdr} - Visualization of Highest Density Regions in ggplot2\nTest for more than one mode: multimode::modetest(dat)\n\nPerforms Ameijeiras-Alonso excess mass test/dip statistic\nHa: More than 1 mode\n\nFind location of modes: multimode::locmodes(dat, mod0 = 2, display = TRUE)\n\n\nAnti-Mode location might be a good place for a cutpoint\n\nIdeas\n\nQuantile Regression\nMixture Model\nEstablish cutpoints and model each modal distribution separately\nML"
  },
  {
    "objectID": "qmd/regression-other.html#sec-reg-other-wls",
    "href": "qmd/regression-other.html#sec-reg-other-wls",
    "title": "61Â  Other",
    "section": "61.7 Weighted Least Squares (WLS)",
    "text": "61.7 Weighted Least Squares (WLS)\n\nOLS with Weighted Pbservations\nCommonly used to overcome binomial or â€œmegaphone-shapedâ€ types of heteroskedacity of OLS residuals\n\n\nMisc\n\nAlso see\n\nOther &gt;&gt; Generalized Least Squares\nReal Estate &gt;&gt; Appraisal Methods &gt;&gt; CMA &gt;&gt; Market Price &gt;&gt; Case-Shiller Method for an example\n\nResources\n\nR &gt;&gt; Documents &gt;&gt; Econometrics &gt;&gt; applied-econometrics-in-r-zeileis-kleiber &gt;&gt; pg 76\n\nFeasible Generalized Least Squares (FGLS) seems to have advantages over WLS Allows you to find the â€œform of the skedastic function to use andâ€¦ estimate it from the dataâ€ * See Zeileis applied econometrics book or another econometrics book for details\n\nThe residual error to be minimized becomes:\n\n\nWhere wi is the weight assigned to observation i\n\n\\(\\hat \\beta\\) becomes\n\n\nWhere W is a diagonal matrix containing the weights for each observation\n\nFor â€œmegaphone-shapedâ€ and binomial types of heteroskedacity, itâ€™s common to set the weights to equal to each observationâ€™s squared residual error"
  },
  {
    "objectID": "qmd/regression-other.html#sec-reg-other-gee",
    "href": "qmd/regression-other.html#sec-reg-other-gee",
    "title": "61Â  Other",
    "section": "61.8 Generalized Estimating Equations (GEE)",
    "text": "61.8 Generalized Estimating Equations (GEE)\n\nModels that are used when individual observations are correlated within groups. Often used when repeated measures (panel data) for an individual are collected over time.\n\nYou make a good guess on the within-subject covariance structure. The model averages over all subjects, and instead of assuming that data were generated from a certain distribution, it uses moment assumptions to iteratively choose the best Î² to describe the relationship between covariates and response.\nA semiparametric method: while we impose some structure on the data generating process (linearity), we do not fully specify its distribution. Estimating Î² is purely an exercise in optimization.\nLimitations\n\nLikelihood-based methods are not available for usual statistical inference. GEE is a quasi-likelihood method.\nUnclear on how to perform model selection, as GEE is just an estimating procedure. There is no goodness-of-fit measure readily available.\nNo subject-specific estimates; if that is the goal of your study, use a different method. (see below)\n\nOther option is Generalized Linear Mixed Model (GLMM), but GLMMs require some parametric assumptions (See Econometrics, Mixed Effects)\n\n*Note that the interpretations of the resulting estimates are different for GLMM and GEE*\n\nScenarios\n\nYou are a doctor. You want to know how much a statin drug will lower your patientâ€™s odds of getting a heart attack.\n\nGLMM answers this question\n\nYou are a state health official. You want to know how the number of people who die of heart attacks would change if everyone in the at-risk population took the stain drug.\n\nGEE answers this question. GEE estimates population-averaged model parameters and their standard errors\n\n\n\nMisc\n\nNotes from\n\nGeneralized Estimating Equations (GEE)\n\nPackages\n\n{gee}: traditional implementations (only has a manual)\n{geepack}: traditional implementations (1 vignette)\n\nCan also handle clustered categorical responses\n\n{multgee}: GEE solver for correlated nominal or ordinal multinomial responses using a local odds ratios parameterization\n\nThe traditional GEE implementation has severe computation challenges and may not be possible when the cluster sizes (large numbers of individuals per cluster) get too large (e.g.Â &gt;1000)\n\nUse One-Step Generalized Estimating Equations method (article with code)\n\nOperates under the assumption of exchangeable correlation (see below)\nCharacteristics\n\nMatches the asymptotic efficiency of the fully iterated GEE;\nUses a simpler formula to estimate the [intra-cluster correlation] ICC that avoids summing over all pairs;\nCompletely avoids matrix multiplications and inversions for computational efficiency\n\n\n\n\nAssumptions (similar to the assumptions for GLMs)\n\nThe responses Y1, Y2, â€¦ , Yn are correlated or clustered\nThere is a linear relationship between the covariates and a transformation of the response, described by the link function, g.\nWithin-cluster covariance has some structure (â€œworking covarianceâ€)\nIndividuals in different clusters are uncorrelated\n\nCovariance Structure\n\nNeed to pick one of these working covariance structures in order to fit the GEE\nTypes\n\nIndependence: observations over time are independent)\nExchangeable (aka Compound Symmetry): all observations over time have the same correlation\n\nCorrelation across individuals is constant within a cluster\nIntra-Cluster Correlation (ICC) is the measure of this correlation.\n\nAR(1): correlation decreases as a power of how many timepoints apart two observations are\n\nReasable if measurements taken closer together (i.e.Â probably more highly correlated)\n\nUnstructured: correlation between all timepoints may be different)\n\nIf the wrong covariance structure is chosen, Î² will be estimated consistently, even if the working covariance structure is wrong. However, the standard errors computed from this will be wrong.\n\nTo fix this, use Huber-White â€œsandwich estimatorâ€ (HC standard errors) for robustness. (See Econometrics, General &gt;&gt; Standard Errors)\n\nThe idea behind the sandwich variance estimator is to use the empirical residuals to approximate the underlying covariance.\nProblematic if:\n\nThe number of independent subjects is much smaller than the number of repeated measures\nThe design is unbalanced â€“ the number of repeated measures differs across individuals\n\n\n\n\nExample: geepack\n\nDescription: How does Vitamin E and copper level in the feeds affect the weights of pigs?\nData\nlibrary(\"geepack\")\ndata(dietox)\ndietox$Cu &lt;- as.factor(dietox$Cu)\ndietox$Evit &lt;- as.factor(dietox$Evit)\nhead(dietox)\n##Â  Â   WeightÂ  Â  Â  Feed TimeÂ  Pig Evit Cu Litter\n## 1 26.50000Â  Â  Â  Â  NAÂ  Â  1 4601Â  Â  1Â  1Â  Â  Â  1\n## 2 27.59999Â  5.200005Â  Â  2 4601Â  Â  1Â  1Â  Â  Â  1\n## 3 36.50000 17.600000Â  Â  3 4601Â  Â  1Â  1Â  Â  Â  1\n## 4 40.29999 28.500000Â  Â  4 4601Â  Â  1Â  1Â  Â  Â  1\n## 5 49.09998 45.200001Â  Â  5 4601Â  Â  1Â  1Â  Â  Â  1\n## 6 55.39999 56.900002Â  Â  6 4601Â  Â  1Â  1Â  Â  Â  1\n\nWeight of slaughter pigs measured weekly for 12 weeks\nStarting weight (i.e.Â the weight at week (Time) 1 and Feed = NA)\nCumulated Feed Intake (Feed)\nEvit is an indicator of Vitamin E treatment\nCu is an indicator of Copper treatment\n\nModel: Independence Working Covariance Structure\nmf &lt;- formula(Weight ~ Time + Evit + Cu)\ngeeInd &lt;- geeglm(mf, id=Pig, data=dietox, family=gaussian, corstr=\"ind\")\nsummary(geeInd)\n\n##Â  Coefficients:\n##Â  Â  Â  Â  Â  Â  EstimateÂ  Std.errÂ  Â  Wald Pr(&gt;|W|)Â  Â \n## (Intercept) 15.07283Â  1.42190Â  112.371Â  &lt;2e-16 ***\n## TimeÂ  Â  Â  Â  6.94829Â  0.07979 7582.549Â  &lt;2e-16 ***\n## Evit2Â  Â  Â  Â  2.08126Â  1.84178Â  Â  1.277Â  Â  0.258Â  Â \n## Evit3Â  Â  Â  -1.11327Â  1.84830Â  Â  0.363Â  Â  0.547Â  Â \n## Cu2Â  Â  Â  Â  -0.78865Â  1.53486Â  Â  0.264Â  Â  0.607Â  Â \n## Cu3Â  Â  Â  Â  Â  1.77672Â  1.82134Â  Â  0.952Â  Â  0.329Â  Â \n## ---\n## Signif. codes:Â  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n##Â \n## Estimated Scale Parameters:\n##Â  Â  Â  Â  Â  Â  Estimate Std.err\n## (Intercept)Â  Â  48.28Â  9.309\n##Â \n## Correlation: Structure = independenceNumber of clusters:Â  72Â  Maximum cluster size: 12\n\ncorstr=â€œindâ€ is the argument for the covariance structure - See article for examples of the other structures and how they affect estimates\n\nANOVA\nanova(geeInd)\n\n## Analysis of 'Wald statistic' Table\n## Model: gaussian, link: identity\n## Response: Weight\n## Terms added sequentially (first to last)\n##Â \n##Â  Â  Â  DfÂ  X2 P(&gt;|Chi|)Â  Â \n## TimeÂ  1 7507Â  Â  &lt;2e-16 ***\n## EvitÂ  2Â  Â  4Â  Â  Â  0.15Â  Â \n## CuÂ  Â  2Â  Â  2Â  Â  Â  0.41Â  Â \n## ---\n## Signif. codes:Â  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "qmd/regression-quantile.html#sec-reg-quant-misc",
    "href": "qmd/regression-quantile.html#sec-reg-quant-misc",
    "title": "Quantile",
    "section": "Misc",
    "text": "Misc\n\nResources\n\nHandbook of Quantile Regression - Koenker ({quantreg} book) (see R &gt;&gt; Documents &gt;&gt; Regression)\n\nPackages\n\n{quantregRanger} - uses Ranger to fit quantile RFs\n\nIn {tidymodels}, quantreg = TRUE tells ranger that youâ€™re estimating quantiles rather than averages. Also predict(airquality, type = 'quantiles')\n\n{grf} - generalized random forest\n{quantreg} - Estimation and inference methods for models for conditional quantile functions: Linear and nonlinear parametric and non-parametric (total variation penalized) models for conditional quantiles of a univariate response.\n{partykit} - conditional inference trees; model-based recursive partitioning trees\n\n{bonsai}: tidymodels partykit conditional trees, forests; successor to treesnip - Model Wrappers for Tree-Based Models\n\n\nFor quantiles &gt; 0.80, see quantile models in Extreme Value Theory (EVT))\nHarrell: To characterize an entire distribution or in other words, have a â€œhigh degree of confidence that no estimated quantile will be off by more than a probability of 0.01, n = 18,400 will achieve this.\n\nFor example with n = 18,400, the sample 0.25 quantile (first quartile) may correspond to population quantiles 0.24-0.26.\nTo achieve a $$0.1 MOE requires n = 180, and to have $$0.05 requires n = 730 (see table)\nÂ  Â  Â   nÂ   MOE\n1Â  Â   20 0.294\n2Â  Â   50 0.188\n3Â  Â  100 0.134\n4Â  Â  180 0.100\n5Â  Â  250 0.085\n6Â  Â  500 0.060\n7Â  Â  730 0.050\n8Â  Â  750 0.049\n9Â   1000 0.043\n10Â  2935 0.025\n11Â  5000 0.019\n12 10000 0.014\n13 18400 0.010\n\nHarrell has a pretty cool text effect to display quantile values in his {HMisc::describe} that uses {gt} under the hood (See EDA &gt;&gt; Packages &gt;&gt; HMisc)\n\n\nHistogram is a sparkline"
  },
  {
    "objectID": "qmd/regression-quantile.html#sec-reg-quant-diag",
    "href": "qmd/regression-quantile.html#sec-reg-quant-diag",
    "title": "Quantile",
    "section": "Diagnostics",
    "text": "Diagnostics\n\nMean Integrated Squared Error (MISE)"
  },
  {
    "objectID": "qmd/regression-regularized.html#sec-reg-reg-misc",
    "href": "qmd/regression-regularized.html#sec-reg-reg-misc",
    "title": "Regularized",
    "section": "Misc",
    "text": "Misc\n\nRegularized Logistic Regression is most necessary when the number of candidate predictors is large in relationship to the effective sample size 3np(1âˆ’p) where p is the proportion of Y=1 Harrell\nIf using sparse matrix, then you donâ€™t need to normalize predictors\nPreprocessing\n\nStandardize numerics\nDummy or factor categoricals\nRemove NAs, na.omit\n\nPackages\n\n{glmnet} - handles families: Gaussian, binomial, Poisson, probit, quasi-poisson, and negative binomial GLMs, along with a few other special cases: the Cox model, multinomial regression, and multi-response Gaussian.\nIn {{sklearn}} (see Model building, sklearn &gt;&gt; Algorithms &gt;&gt; Stochaistic Gradient Descent (SGD)), the hyperparameters are different than in R\n\nlambda (R) is alpha (py)\nalpha (R) is 1 - L1_ratio (py)\n\n{SLOPE} pkg - lasso regression that handles correlated predictors by clustering them\n\nVariable Selection\n\nFor Inference, only Adaptive LASSO is capable of handling block and time series dependence structures in data\n\nSee A Critical Review of LASSO and Its Derivatives for Variable Selection Under Dependence Among Covariates\n\nâ€œWe found that one version of the adaptive LASSO of Zou (2006) (AdapL.1se) and the distance correlation algorithm of Febrero-Bande et al.Â (2019) (DC.VS) are the only ones quite competent in all these scenarios, regarding to different types of dependence.â€\nThereâ€™s a deeper description of the model in the supplemental materials of the paper. I think the â€œ.1seâ€ means itâ€™s using the lambda.1se from cv.\n\nlambda.1se : largest value of Î» such that error is within 1 standard error of the cross-validated errors for lambda.min.\n\nsee lambda.min, lambda.1se and Cross Validation in Lasso : Binomial Response for code to access this value.\n\n\n\nRe the distance correlation algorithm (itâ€™s a feature selection alg used in this paper as benchmark vs LASSO variants)\n\nâ€œthe distance correlation algorithm for variable selection (DC.VS) of Febrero-Bande et al.Â (2019). This makes use of the correlation distance (SzÃ©kely et al., 2007; Szekely & Rizzo, 2017) to implement an iterative procedure (forward) deciding in each step which covariate enters the regression model.â€\nStarting from the null model, the distance correlation function, dcor.xy, in {fda.usc} is used to choose the next covariate\n\nguessing you want large distances and not sure what the stopping criteria is\n\nalgorithm discussed in this paper, Variable selection in Functional Additive Regression Models\n\nHarrell is skeptical. â€œIâ€™d be surprised if the probability that adaptive lasso selects theâ€rightâ€ variables is more than 0.1 for N &lt; 500,000.â€"
  },
  {
    "objectID": "qmd/regression-regularized.html#sec-reg-reg-conc",
    "href": "qmd/regression-regularized.html#sec-reg-reg-conc",
    "title": "Regularized",
    "section": "Concepts",
    "text": "Concepts\n\nShrinking effect estimates turns out to always be best\n\nOLS is the Best Linear Unbiased Estimator (BLUE), but being unbiased means the variance of the estimated effects is large from sample to sample and therefore outcome variable predictions using OLS donâ€™t generalize well.\nIf you predicted y using the sample mean times some coefficient, itâ€™s always(?) the case that youâ€™ll have a better generalization error with a coefficient less than 1 (shrinkage).\n\nRegularized Regression vs OLS\n\nAs N â†‘, standard errors â†“\n\nregularized regression and OLS regression produce similar predictions and coefficient estimates.\n\nAs the number of covariates â†‘ (relative to the sample size), variance of estimates â†‘\n\nregularized regression and OLS regression produce much different predictions and coefficient estimates\nTherefore OLS predictions are usually fine in a low dimension world (not usually the case)"
  },
  {
    "objectID": "qmd/regression-regularized.html#sec-reg-reg-ridge",
    "href": "qmd/regression-regularized.html#sec-reg-reg-ridge",
    "title": "Regularized",
    "section": "Ridge",
    "text": "Ridge\n\nThe regularization reduces the influence of correlated variables on the model because the weight is shared between the two predictive variables, so neither alone would have strong weights. This is unlike Lasso which just drops one of the variables (which one gets dropped isnâ€™t consistent).\nLinear transformations in the design matrix will affect the predictions made by ridge regression."
  },
  {
    "objectID": "qmd/regression-regularized.html#sec-reg-reg-lasso",
    "href": "qmd/regression-regularized.html#sec-reg-reg-lasso",
    "title": "Regularized",
    "section": "Lasso",
    "text": "Lasso\n\nWhen lasso drops a variable, it doesnâ€™t mean that the variable wasnâ€™t important.\n\nThe variable, x1, couldâ€™ve been correlated with another variable, x2, and lasso happens to drop x1 because in this sample, x2, predicted the outcome just a tad better.\n\n\n\nAdaptive LASSO\n\n\nPurple dot indicates that itâ€™s a weighted (wj) version of LASSO\nGreen checkmark indicates itâ€™s optimization is a convex problem\nBetter Selection, Bias Reduction are attributes that it has that are better than standard LASSO\nWeighted versions of the LASSO attach the particular importance of each covariate for a suitable selection of the weights. Joint with iteration, this modification allows for a reduction of the bias.\n\nZhou (2006) say that you should choose your weights so the adaptive Lasso estimates have the Oracle Property:\n\nYou will always identify the set of nonzero coefficientsâ€¦when the sample size is infinite\nThe estimates are unbiased, normally distributed, and the correct variance (Zhou (2006) has the technical definition)â€¦when the sample size is infinite.\n\nTo have these properties, wj = 1 / |Î²j_hat|Î³, where Î³ &gt; 0 and Î²j_hat is an unbiased estimate of the true parameter, Î²\n\nGenerally, people choose the Ordinary Least Squares (OLS) estimate of Î² because it will be unbiased. Ridge regression produces coefficient estimates that are biased, so you cannot guarantee the Oracle Property holds.\n\nIn practice, this probably doesnâ€™t matter. The Oracle Property is an asymptotic guarantee (when nâ†’âˆ), so it doesnâ€™t necessary apply to your data with a finite number of observations. There may be scenarios where using Ridge estimates for weights performs really well. Zhou (2006) recommends using Ridge regression over OLS when your variables are highly correlated.\n\n\n\nSee Adaptive LASSO for examples with a continuous, binary, and multinomial outcome"
  },
  {
    "objectID": "qmd/regression-regularized.html#sec-reg-reg-firth",
    "href": "qmd/regression-regularized.html#sec-reg-reg-firth",
    "title": "Regularized",
    "section": "Firthâ€™s Estimator",
    "text": "Firthâ€™s Estimator\n\nPenalized Logistic Regression estimator\nFor sample sizes less than around n = 1000 or sparse data, using Firth Estimator is recommended\nMisc\n\nNotes from\n\nThread\n\nPackages\n\n{brglm2} -\n{logistf} - Includes FLIC and FLAC extensions; uses profile penalized likelihood confidence intervals which outperform Wald intervals; includes a function that performs a penalized likelihood ratio test on some (or all) selected factors\n\nemmeans::emmeans is supported\n\n\nInvariant to linear transformations of the design matrix (i.e.Â predictor variables) unlike Ridge Regression\nWhile the standard Firth correction leads to shrinkage in all parameters, including the intercept, and hence produces predictions which are biased towards 0.5, FLIC and FLAC are able to exclude the intercept from shrinkage while maintaining the desirable properties of the Firth correction and ensure that the sum of the predicted probabilities equals the number of events.\n\nPenalized Likelihood\n\\[\nL^*(\\beta\\;|\\;y) = L(\\beta\\;|\\;y)\\;|I(\\beta)|^{\\frac{1}{2}}\n\\]\n\nEquivalent to penalization of the log-likelihood by the Jeffreys prior\n\\(I(\\beta)\\) is the Fisher information matrix, i. e. minus the second derivative of the log likelihood\n\nMaximum Likelihood vs Firthâ€™s Correction\n\nBias\n\nVariance\n\nCoefficient and CI bar comparison on a small dataset (n = 35, k = 7)\n\n\nLimitations\n\nRelies on maximum likelihood estimation, which can be sensitive to datasets with large random sampling variation. In such cases, Ridge Regression may be a better choice as it provides some shrinkage and can stabilize the estimates by pulling them towards the observed event rate.\nLess effective than ridge regression in datasets with highly correlated covariates\nFor the Firth Estimator, the Wald Test can perform poorly in data sets with extremely rare events."
  },
  {
    "objectID": "qmd/regression-survival.html#misc",
    "href": "qmd/regression-survival.html#misc",
    "title": "62Â  Survival",
    "section": "62.1 Misc",
    "text": "62.1 Misc\n\nModel for estimating the time until a particular event occurs\n\ne.g.Â death of a patient being treated for a disease, failure of an engine part in a vehicle\n\nPrediction models for survival outcomes are important for clinicians who wish to estimate a patientâ€™s risk (i.e.Â probability) of experiencing a future outcome. The term â€˜survivalâ€™ outcome is used to indicate any prognostic or time-to-event outcome, such as death, progression, or recurrence of disease. Such risk estimates for future events can support shared decision making for interventions in high-risk patients, help manage the expectations of patients, or stratify patients by disease severity for inclusion in trials.1 For example, a prediction model for persistent pain after breast cancer surgery might be used to identify high risk patients for intervention studies\nOutcome variable: Time until event occurs\npackages\n\n{survival}\n{censored} - tidymodels package for censored and survival modelling\n{quantreg} - quantile survival regression\n{msm} - multi-state models\n\nVignette for {msm}\nSee Multistate Models for Medical Applications\n\nTutorial using a heart transplant dataset\n\nStandard survival models only directly model two states: alive and dead. Multi-state models enable directly modeling disease progression where patients are observed to be in various states of health or disease at random intervals, but for which, except for death, the times of entering or leaving states are unknown.\nMulti-state models easily accommodate interval censored intermediate states while making the usual assumption that death times are known but may be right censored.\n\n{grf} - generalized random forest; causal forest with time-to-event data\n{partykit} - conditional inference trees; model-based recursive partitioning trees; can be used with {survival} to create random survival forests\n\n{bonsai}: tidymodels partykit conditional trees, forests; successor to treesnip - Model Wrappers for Tree-Based Models\n\n{aorsf} - optimized software to fit, interpret, and make predictions with oblique random survival forests (ORSFs) {{sklearn}} - Random Survival Forests, Survival Support Vector Machine\n\nNotes from\n\nWhat is Coxâ€™s proportional hazards model?\n\nWhy not use a standard regression model?\n\nUnits that â€œsurviveâ€ until the end of the study will have a censored survival time.\n\ni.e.Â we wonâ€™t have an observed survival time for these units because they survive for an unknown time after the study is completed.\nWe donâ€™t want to discard these units though, as they still have useful information.\n\n\nSample Size\nModels\n\nKaplan Meier model (i.e.Â K-M survival curve)\n\nOften used as a baseline in survival analysis\nCan not be used to compare risk between groups and compute metrics like the hazard ratio\n\nExponential model, the Weibull model, Cox Proportional-Hazards, Log-logistic and the Accelerated Failure Time (AFT)\nMulti-State Models\nHazard rates and Cumulative Hazard rates are typical quantities of interest\n\nLog-Rank Test (aka Mantel-Cox test) - tests if two groups survival curves are different\n\nnon-parametric; a special case with one binary X\nThe intuition behind the test is that if the two groups have different hazard rates, the two survival curves (so their slopes) will differ.\nCompares the observed number of events in each group to what would be expected if the survival curves were identical (i.e., if the null hypothesis were true).\nExample\nlibrary(survival)\ndat &lt;- data.frame(\nÂ  group = c(rep(1, 6), rep(2, 6)),\nÂ  time = c(4.1, 7.8, 10, 10, 12.3, 17.2, 9.7, 10, 11.1, 13.1, 19.7, 24.1),\nÂ  event = c(1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0)\n)\ndat\n##Â  Â  group time event\n## 1Â  Â  Â  1Â  4.1Â  Â  1\n## 2Â  Â  Â  1Â  7.8Â  Â  0\n## 3Â  Â  Â  1 10.0Â  Â  1\n## 4Â  Â  Â  1 10.0Â  Â  1\n## 5Â  Â  Â  1 12.3Â  Â  0\n## 6Â  Â  Â  1 17.2Â  Â  1\n## 7Â  Â  Â  2Â  9.7Â  Â  1\n## 8Â  Â  Â  2 10.0Â  Â  1\n## 9Â  Â  Â  2 11.1Â  Â  0\n## 10Â  Â  2 13.1Â  Â  0\n## 11Â  Â  2 19.7Â  Â  1\n## 12Â  Â  2 24.1Â  Â  0\nsurvdiff(Surv(time, event) ~ group,\nÂ  data = dat\n)\n##Â  Â  Â  Â  N Observed Expected (O-E)^2/E (O-E)^2/V\n## group=1 6Â  Â  Â  Â  4Â  Â  2.57Â  Â  0.800Â  Â  Â  1.62\n## group=2 6Â  Â  Â  Â  3Â  Â  4.43Â  Â  0.463Â  Â  Â  1.62\n##Â \n##Â  Chisq= 1.6Â  on 1 degrees of freedom, p= 0.2\n\n# plot curves with pval from test\nfit &lt;- survfit(Surv(time, event) ~ group, data = dat)\nggsurvplot(fit,\nÂ  pval = TRUE,\nÂ  pval.method = TRUE\n)\n\npval &gt; 0.05, so there isnâ€™t enough evidence to that theyâ€™re different."
  },
  {
    "objectID": "qmd/regression-survival.html#components",
    "href": "qmd/regression-survival.html#components",
    "title": "62Â  Survival",
    "section": "62.2 Components",
    "text": "62.2 Components\n\nSurvival Time (T) (aka death, failure time, event time): time at which the event occurs\nCensoring Time (C): time at which censoring occurs\n\nFor each unit, we observe T or C.\n\nY = min(T, C)\n\nright censoring: occurs when the event has happened after the enrollment (but the time is unknown).\n\nThe patient does not experience the event for the whole duration of the study.\nThe patient withdraws from the study.\nThe patient is lost to follow-up.\n\nleft censoring: occurs when the event has happened before the enrollment (but the time is unknown).\n\nCumulative hazard function (aka Cumulative Hazard Rates)\n\nShows the total accumulated risk of an event occurring at time t\nThe area under the hazard function\n\nHazard Rate (aka Risk Score), h(t | X)\n\nThe hazard rate is the probability that a unit with predictors, X, will experience an event at time, t, given that the unit has survived just before time, t.\nThe formula for the Hazard Rate is the Hazard function.\n\nHazard Ratio (aka Relative Risk of an event): Risk of an event given category / risk of an event given by reference category\n\nThe ratio of two instantaneous event rates\nCoefficient of the Cox Proportional Hazards model (e.g.Â paper)\n\neÎ² &gt; 1 (or Î² &gt; 0) for an increased risk of event (e.g.Â death).\neÎ² &lt; 1 (or Î² &lt; 0) for a reduced risk of event.\nHR of 2 is equivalent to raising the entire survival curve for a control subject to the second power to get the survival curve for an exposed subject\n\nExample: if a control subject has 5y survival probability of 0.7 and the exposed:control HR is 2, the exposed subject has a 5y survival probability of 0.49\nIf the HR is 1/2, the exposed subject has a survival curve that is the square root of the control, so S(5) would be âˆš0.7 = 0.837\n\n\n\nStatus indicator, Î´\n\nÎ´ = 1, if T â‰¤ C (e.g.Â unit fails before study ends)\n\nTrue survival time is observed\n\nÎ´ = 0, if T &gt; C (e.g.Â unit survives until end of study or has dropped out)\n\nCensoring time is observed\n\n\nSurvival function (aka Survival Rate), S(T &gt; t):\n\nOutputs the probability of a subject surviving (i.e., not experiencing the event) beyond time t\nMonotonically decreasing (i.e.Â level or decreasing)\nBaseline survival curve illustrates the survival function when all the covariates are set to their median value"
  },
  {
    "objectID": "qmd/regression-survival.html#kaplan-meir",
    "href": "qmd/regression-survival.html#kaplan-meir",
    "title": "62Â  Survival",
    "section": "62.3 Kaplan-Meir",
    "text": "62.3 Kaplan-Meir\n\nMisc\n\nUseful for validation of Proportional Hazards assumption. When lines cross the assumption, hazards are found to be non-proportional.\nHarrell RMS (Ch. 20.3):\n\nFor external validation: at least 200 events\nNeed 184 subjects with an event, or censored late, to estimateÂ  to within a margin of error of 0.1 everywhere, at the 0.95 confidence level\n\n\nOrder event times (T) of units from smallest to largest, t1 &lt; â€¦. &lt; tk\nCalculate probability that a unit survives past event time, ti, given that they survived up until event time, ti (i.e.Â past ti-1) (conditional probability)\n\ne.g.Â for t1, itâ€™s (n1 - d1) / n1\n\nn1 is the number of units that have survived at t1\nd1 is the number of units that have experienced the event (e.g.Â died) at t1\nSimilar for other t values\n\nMedian survival time is where the survival probability equals 0.5\n\nSurvival function[](./_resources/Regression,_Survival.resources/1-bp-FZyIs1WlqYgQMWn9Obw.gif]]\n\nThe survivalÂ  function computes the products of these probabilities resulting in the K-M survival curve\nThe product of these conditional probabilities reflects the fact that to survive past event time, t, a unit must have survived all previous event times and the current event time.\n\nExample: 50 patients\n\n\nDotted lines represent 95% CI\nRed dots indicate time when patients died (aka event times)\nMedian survival time is ~ 13yrs"
  },
  {
    "objectID": "qmd/regression-survival.html#exponential",
    "href": "qmd/regression-survival.html#exponential",
    "title": "62Â  Survival",
    "section": "62.4 Exponential",
    "text": "62.4 Exponential\n\nAssumes that the hazard rate is constant\n\ni.e.Â risk of the event of interest occurring remains the same throughout the period of observation\n\nSurvival function\n\nHazard function\n\n\nh(t) is the constand hazard rate\n\nEstimated parameter: Î»"
  },
  {
    "objectID": "qmd/regression-survival.html#weibull",
    "href": "qmd/regression-survival.html#weibull",
    "title": "62Â  Survival",
    "section": "62.5 Weibull",
    "text": "62.5 Weibull\n\nAssumes the change in hazard rate is linear.\nSurvival function\n\nHazard function\n\nEstimated parameters: Î» and Ï\n\nÎ» parameter indicates how long it takes for 63.2% of the subjects to experience the event.\nÏ parameter indicates whether the hazard rate is increasing, decreasing, or constant.\n\nIf Ï is greater than 1, the hazard rate is constantly increasing.\nIf Ï is less than 1, the hazard rate is constantly decreasing."
  },
  {
    "objectID": "qmd/regression-survival.html#coxs-proportional-hazards",
    "href": "qmd/regression-survival.html#coxs-proportional-hazards",
    "title": "62Â  Survival",
    "section": "62.6 Coxâ€™s Proportional Hazards",
    "text": "62.6 Coxâ€™s Proportional Hazards\n\nMultivariable regression model\nAllows the hazard rate to fluctuate\nHarrell: â€œunder PH [assumption] and absence of covariate interactions, HR is a good overall effect estimate for binary [treatment]â€\nMisc\n\nPackages\n\n{glmnet} - Regularized Cox Regression\n{coxphf} - Cox Regression with Firthâ€™s Penalized Likelihood\n\nSee Regression, Regularized &gt;&gt; Firthâ€™s Estimator\n\n\nSample Size\n\nHarrell RMS (Ch. 20.3):\n\nTo achieve a Multiplicative Margin of Error (MMOE ) of 1.2 (?) in estimating eÎ²^ with equal numbers of events in the two groups (balanced, binary treatment variable) and Î± = 0.05 â†’ requires a total of 462 events\n\n\n\nAssumes\n\nHazard ratios (ratio of hazard rates or exp(Î²) between groups/units remain constant\n\ni.e.Â no matter how the hazard rates of the subjects change during the period of observation, the hazard rate of one group relative to the other will always stay the same\n\nHazard Ratios are independent of time\nExample: Immunotherapy typically violates PH assumptions (post)\n\nThe survival probabilities between the treatment (blue) and the chemo (red) cross at around the 4.2 months\n\nThe distance between the lines should remain somewhat constant throughout the trial in order to adhere to the PH assumptions (1st assumption)\nAlso think the lines should be somewhat straight. (2nd assumption)\n\nPatients in immunotherapy drug trials often experience a period of toxicity, but if they survive this period, they have a much better outcome down the road.\n\nTests\n\nGrambsch and Therneau (G&T)\nSee Harrell RMS (Ch. 20.6.2)\n\nIf assumptions are violated,\n\nGelman says to try and â€œexpand the model, at the very least by adding an interaction.â€ (post)\nSee Harrell RMS (Ch. 20.7)\nUse a different model\n\nAccelerated Failure Time (AFT) model (See ML &gt;&gt; Gradient Boosting Survival Trees)\nAdjusted Cox PH model with Time-Varying Coefficients\n\nMust choose a functional form describing how the effect of the treatment changes over time\n\nRecommended to use AIC criteria to guide oneâ€™s choice among a large number of candidates\n\n\n\n\n\nModels event time (T) outcome variable and outputs parameter estimates for treatment (X) effects\n\nProvides a way to have time-dependent (i.e.Â repeated measures) explanatory variables (e.g.Â age, health status, biomarkers)\nCan handle other types of censoring such as left or interval censoring.\nHas extensions such as lasso to handle high dimensional data\nDL and ML models also have versions of this method\n\nHazard function\n\nh(t |X) = h0(t)exÎ²\nThe hazard rate for a unit with predictors, X, is the product of a baseline hazard, h0(t) (corresponding to X = 0) and a factor that depends on X and the regression parameters, Î²\nOptimized to yield partial maximum likelihood estimates, Î²^.\n\nDoesnâ€™t require the specification of h0(t) which makes the method flexible and robust\n\n\nHazard Ratio (aka relative risk) for a binary covariate (e.g.Â treatment)Â  = eá¶œáµ’áµ‰á¶ \nInterpretation:\n\nHazard Rate (risk of the event): Moving from the reference category to the other category changes the hazard rate by a factor of eá¶œáµ’áµ‰á¶ \n\ne.g.Â eá¶œáµ’áµ‰á¶  = 0.68 means the change in category results in a (1 - 0.68) = 0.32 = 32% decrease in the hazard rate on average.\n\nRelative Risk (of an event): Risk of an event given category / risk of an event given by reference category\nExample: Treatment = Smoking\n\nRisk Score (aka hazard rate) given by smoking: (xáµ¢=1): hâ‚€(t)exp(Î²â‹…xáµ¢) = hâ‚€(t)exp(Î²*1) = hâ‚€(t)exp(Î²)\nRisk Score (aka base hazard rate) given by not smoking: (xáµ¢=0): hâ‚€(t)exp(Î²â‹…xáµ¢) = hâ‚€(t)exp(Î²*0) = hâ‚€(t)\nRelative risk (aka hazard ratio) = risk given by smoking / risk given by not smoking: hâ‚€(t)exp(Î²) / hâ‚€(t) = exp(Î²)"
  },
  {
    "objectID": "qmd/regression-survival.html#ml",
    "href": "qmd/regression-survival.html#ml",
    "title": "62Â  Survival",
    "section": "62.7 ML",
    "text": "62.7 ML\n\n62.7.1 Misc\n\nSplit data so partitions have the same censoring distribution.\n\nThe censoring distribution might be obtained from a Kaplan-Meier estimator applied to the data.\n\nDynamic AUC is a recommended metric\n\n\n\n62.7.2 Random Survival Forests\n\nThe main difference from a standard RF lies in the metric used to assess the quality of a split: log-rank (see Misc) which is typically used when comparing survival curves among two or more groups.\nPackages\n\n{{sklearn}}\n{aorsf} - Optimized software to fit, interpret, and make predictions with oblique random survival forests (ORSFs)\n\nInstead of using one variable to split the data, use a weighted combination of variables, i.e.Â \\(\\text{instead of}\\;\\; x_1 &lt; \\text{cutpoint (left), use}\\;\\; c_1x_1 + c_2x_2 &lt; \\text{cutpoint (right)}\\)\n\nPredictions of Standard RF vs Oblique RF\n\n\n\n\n\nStandard Random Forest\n\n\n\n\n\n\n\nOblique Random Forest\n\n\n\n\n\n\n\nIn the standard rf, the decision boundaries are essentially perpendicular while the oblique rf boundaries are more angular. This should make the oblique model more flexible.\n\nKaplan-Meir Curves are fit in the leaves of the trees\n\n\nTime is on the x-axis and probability of survival on the y-axis\n\n\n\nExample: {aorsf}\n\nFrom Machine Learning for Risk Prediction using Oblique Random Survival Forests (Video; Slides & Code)\nVia package\n# equivalent syntaxes\nfit_orsf &lt;- orsf(data = pbc_orsf, \n                 formula = Surv(time + status) ~ . - id)\nfit_orsf &lt;- orsf(data = pbc_orsf, \n                 formula = time + status ~ . - id)\n\nTop model is fit with the typical survival::coxph syntax\ntime: time to event\nstatus: dummy variable indicating whether event occurred\nid: unit or patient id which is excluded\n\nVia {tidymodels}\nlibrary(parsnip)\nlibrary(censored) # must be version 0.2.0 or higher\nrf_spec &lt;- \n  rand_forest(trees = 200) %&gt;%\n  set_engine(\"aorsf\") %&gt;% \n  set_mode(\"censored regression\") \nfit_tidy &lt;- \n  rf_spec %&gt;% \n  parsnip::fit(data = pbc_orsf, \n               formula = Surv(time, status) ~ . - id)\nEstimated Expected Risk via Partial Dependence (PD)\n\nPD and importance rank for variables\norsf_summarize_uni(fit_orsf, n_variables = 1)\n## \n## -- bili (VI Rank: 1) ----------------------------\n## \n##         |---------------- risk ----------------|\n##   Value      Mean    Median     25th %    75th %\n##  &lt;char&gt;     &lt;num&gt;     &lt;num&gt;      &lt;num&gt;     &lt;num&gt;\n##    0.80 0.2343668 0.1116206 0.04509389 0.3729834\n##     1.4 0.2547884 0.1363122 0.05985486 0.4103148\n##     3.5 0.3698634 0.2862611 0.16196924 0.5533383\n## \n##  Predicted risk at time t = 1788 for top 1 predictors\n\nComputes expected risk (predicted probability) at different quantiles as a predictor variable varies.\n\nValue is 3 values of the predictor which are the 25th, 50th, and 75th quantile.\n\nn_variables says how many â€œimportantâ€ variables to look at\n\ne.g.Â n_variables = 2 would look at the top 2 variables in terms of variable importance.\nVI Rank: 1 indicates the bili is ranked first in variable importance\n\nbili: serum bilirubin (mg/dl); continuous predictor variable\nIt choses time = 1788 because thatâ€™s the median\nAlso see Diagnostics, Model Agnostic &gt;&gt; DALEX &gt;&gt; Dataset Level &gt;&gt; Partial Dependence Profiles\n\nPD at specified predictor values and time values\npd_by_gender &lt;- orsf_pd_oob(fit_orsf, \n                  pred_spec = list(sex = c(\"m\", \"f\")),\n                  pred_horizon = 365 * 1:5)\npd_by_gender %&gt;% \n  dplyr::select(pred_horizon, sex, mean) %&gt;% \n  tidyr::pivot_wider(names_from = sex, values_from = mean) %&gt;% \n  dplyr::mutate(ratio = m / f)\n\n## # A tibble: 5 x 4\n##   pred_horizon      m      f ratio\n##          &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n## 1          365 0.0768 0.0728  1.06\n## 2          730 0.125  0.111   1.13\n## 3         1095 0.230  0.195   1.18\n## 4         1460 0.298  0.251   1.19\n## 5         1825 0.355  0.296   1.20\n\norsf_pd_oob - Computes expected risk using out-of-bag only\n\nBoth values (m,f) for sex are specified\npred_horizon specifies the time values\n\nHere, time is in days, so these values specify expected risk (predicted probabilities) at years 1 through 5.\n\n\nratio is the risk ratio of males compared to females.\nOthers\n\norsf_pd_inb - Computes expected risk using all training data\norsf_pd_new - Computes expected risk using new data\n\n\n\n\n\n\n\n62.7.3 Gradient Boosting Survival Trees\n\nLoss Functions\n\nPartial likelihood loss of Coxâ€™s proportional hazards model\nSquared regression loss\nInverse probability of censoring weighted least squares error.\n\nAllows the model to accelerate or decelerate the time to an event by a constant factor. It is known as the Accelerated Failure Time (AFT). It contrasts with the Cox proportional hazards model where only the features influence the hazard function.\n\n\nPackages\n\n{{sklearn}}\n\n\n\n\n62.7.4 Survival Support Vector Machine\n\nPredictions cannot be easily related to the standard quantities of survival analysis, that is, the survival function and the cumulative hazard function.\nPackages\n\n{{sklearn}}"
  },
  {
    "objectID": "qmd/regression-survival.html#diagnostics",
    "href": "qmd/regression-survival.html#diagnostics",
    "title": "62Â  Survival",
    "section": "62.8 Diagnostics",
    "text": "62.8 Diagnostics\n\nMisc\n\nNotes from How to Evaluate Survival Analysis Models\nPackages\n\n{survex} - Explainable Machine Learning in Survival Analysis\n\nFrom Dalex group\n\n\n\nConcordance Index (C-Index, Harrellâ€™s C)\n\nConsider a pair of patients (i, j). Intuitively, a higher risk should result in a shorter time to the adverse event. Therefore, if a model predicts a higher risk score for the first patient (Î·áµ¢ &gt; Î·â±¼), we also expect a shorter survival time in comparison with the other patient (Táµ¢ &lt; Tâ±¼).\nEach pair (i, j) that fulfills this expectation (Î·áµ¢ &gt; Î·â±¼ : Táµ¢ &lt; Tj or Î·áµ¢ &lt; Î·â±¼ : Táµ¢ &gt; Tâ±¼) as concordant pair, and discordant otherwise.\n\nA high number of concordant pairs is an evidence of the quality of the model, as the predicted higher risks correspond to an effectively shorter survival time compared to lower risks\n\nFormula\n\n\nIf both patients i and j are censored, we have no information on Táµ¢ and Tâ±¼, hence the pair is discarded.\nIf only one patient is censored, we keep the pair only if the other patient experienced the event prior to the censoring time. Otherwise, we have no information on which patient might have experienced the event first, and the pair is discarded\n\nProgrammatic Formula\n\n\nWhere the variable Î”â±¼ indicates whether Tâ±¼ has been fully observed (Î”â±¼ = 1) or not (Î”â±¼ = 0). Therefore, the multiplication by Î”â±¼ allows to discard noncomparable pairs because the smaller survival time is censored (Î”â±¼ = 0).\n\nGuidelines\n\nC = 1: perfect concordance between risks and event times.\nC = 0: perfect anti-concordance between risks and event times.\nC = 0.5: random assignment. The model predicts the relationship between risk and survival time as well as a coin toss.\nDesirable values range between 0.5 and 1.\n\nThe closer to 1, the more the model differentiates between early events (higher risk) and later occurrences (lower risk).\n\n\nIssues\n\nThe C-index maintains an implicit dependency on time.\nThe C-index becomes more biased (upwards) the more the amount of censoring (see Unoâ€™s C below)\n\n\nUnoâ€™s C\n\n** Preferable to Harrellâ€™s C in the presence of a higher amount of censoring. **\nVariation of Harrellâ€™s C that includes the inverse probability of censoring weighting\n\nWeights based on the estimated censoring cumulative distribution\nUses the Kaplan-Meier estimator for the censoring distribution\n\nSo the disribution of censored units should be independent of the covariate variables\n\nIn the paper, Uno showed through simulation this measure is still pretty robust even when the censoring is dependent on the covariates\n\n\n\n\nDynamic AUC\n\nAUC where the False Positive Rates (FPR) and True Positive Rates (TPR) are time-dependent\n\nSince a unit is a True Negative until the event then becomes a True Positive\nRecommended for tasks when you want to measure performance over a specific period of time (e.g.Â predicting churn in the first year of subscription).\n\nFormula\n\n\nf^s are predicted risk scores\ná½½ is the inverse probability of censoring weight (see Unoâ€™s C)\n\ndisregard the freaking dash above the omega, microsoft deleted the regular one for some reason\n\nI(yi,j &gt; t) indicates whether the unit pairâ€™s, i and j, event time is greater or less the time, t. (I think)"
  },
  {
    "objectID": "qmd/renv.html#sec-renv-misc",
    "href": "qmd/renv.html#sec-renv-misc",
    "title": "Renv",
    "section": "Misc",
    "text": "Misc\n\nBruno Rodrigues posts on reproducibility\n\nMRAN is getting shutdown - what else is there for reproducibility with R, or why reproducibility is on a continuum?\n\ntl;dr:\n\nI want to start a project and make it reproducible.\n\n{renv} and Docker\n\nThereâ€™s an old script laying around that I want to run.\n\n{groundhog} and Docker\n\nI want to work inside an environment that enables me to run code in a reproducible way.\n\nDocker and the Posit CRAN mirror.\n\n\n\nCode longevity of the R programming language\n\nAlternatives\n\nGNU Guix\n\nSee Brunoâ€™s â€œMRAN is getting shutdown - what else is there for reproducibility with R, or why reproducibility is on a continuum?â€ post for his thoughts\n\n\nInstall from github\nrenv::install(\"eddelbuettel/digest\")"
  },
  {
    "objectID": "qmd/renv.html#sec-renv-actrest",
    "href": "qmd/renv.html#sec-renv-actrest",
    "title": "Renv",
    "section": "Activating/Restoring a Project",
    "text": "Activating/Restoring a Project\n\nMisc\n\n** Restoring an environment comes with a few caveats: **\n\nFirst of all, renv does not install a different version of R if the recorded and current version disagree. This is a manual step and up to the user.\nThe same is true for packages with external dependencies. Those libraries, their headers and binaries also need to be installed by the user in the correct version, which is not recorded in the lockfile.\nFurthermore renv supports restoring packages installed from git repositories, but fails if the user did not install git beforehand.\n\nhttps://rstudio.github.io/renv/articles/faq.html#im-returning-to-an-older-renv-project-what-do-i-do\n\nInit steps (updates lockfile to either latest pkg versions or what you have locally)\n\nRun renv::init()\nChoose â€œDiscard the lockfile and re-initialize the projectâ€\n\nRestore steps (looking to reproduce the original project results)\n\nRun renv::restore()\nUpdate any packages with outside dependencies\n\nRMarkdown depends on pandoc which is usually installed by installing RStudio. So if you updated RStudio, then the rmarkdown version in the projectâ€™s lockfile may not work with your current pandoc version. Youâ€™ll have to update rmarkdown (or revert your pandoc versionâ€¦ shhhyea, as if)\n\n\nIssues\n\nA pkg installation fails (and therefore restore fails) because some pkg requires another pkg to be a more up-to-date version or some other reason\n\n**If itâ€™s something with a shit ton of dependencies like rstanarm, you might as well use renv::init( ) method**\nExamples:\n\n{pkgload} failed to install because I had rlang v.4.0.6 instead of &gt; v4.09\n{RCurl} failed because it couldnâ€™t find some obsure file\n\nSolution (update the package):\n\nupdate(\"pkg\") (or go to the github and search for the latest stable version) to see what the latest stable version is.\nrenv::modify() allows you edit the lockfile. Change pkg version in lockfile to that latest version\nrerun renv::restore()\nRepeat as necessary"
  },
  {
    "objectID": "qmd/renv.html#sec-renv-font",
    "href": "qmd/renv.html#sec-renv-font",
    "title": "Renv",
    "section": "Installing Fonts",
    "text": "Installing Fonts\n\nUsing a package like extrafont, it wonâ€™t find any fonts installed so you have to point it to the system path\nSteps\n\nInstall {systemfonts}\nRun systemfonts::match_font(\"&lt;name of font you have installed&gt;\") and copy path (donâ€™t include file (i.e.Â .ttf))\nRun extrafont::"
  },
  {
    "objectID": "qmd/renv.html#using-local-directory-with-package-tarbell-to-install-a-package",
    "href": "qmd/renv.html#using-local-directory-with-package-tarbell-to-install-a-package",
    "title": "Renv",
    "section": "Using Local directory with Package Tarbell to Install a Package",
    "text": "Using Local directory with Package Tarbell to Install a Package\n\nNeeded to install an old XML package version that wasnâ€™t available through CRAN mirror. Errored looking libxml parser. Discussion says you need to use libxml2.\nResources\n\nrenv using local package directory\n\nhttps://rstudio.github.io/renv/articles/local-sources.html\n\nInstructions on compiling old XML package with rtools40\n\nhttps://github.com/r-windows/rtools-installer/issues/3\nhttps://github.com/r-windows/checks/issues/5#issue-335598042\n\nInstall libxml2 library\n\nhttps://github.com/r-windows/docs/blob/master/packages.md#xml\n\nFinding pacman package manager to install libxml2\n\nhttps://github.com/r-windows/docs/blob/master/rtools40.md#readme\n\n\nIn the project directory, create renv/local directory. Then, download/move the package versionâ€™s tar.gz file to that â€œlocalâ€ folder\n&gt; Sys.setenv(LIB_XML = \"$(MINGW_PREFIX)\")\n&gt; Sys.setenv(LOCAL_CPPFLAGS = \"-I/mingw$(WIN)/include/libxml2\")\n&gt; install.packages('renv/local/XML_3.99-0.3.tar.gz', repos = NULL, type = 'source')"
  },
  {
    "objectID": "qmd/renv.html#errors",
    "href": "qmd/renv.html#errors",
    "title": "Renv",
    "section": "Errors",
    "text": "Errors\n\nFails to retrieve package\n\nSolutions:\n\nInstall from github\nrenv::install(\"eddelbuettel/digest\")\nRevert to previous version\nremotes::install_version(\"cachem\", version = \"1.0.3\", repos = \"http://cran.us.r-project.org\")\nrenv::install(\"cachem@1.0.3\")"
  },
  {
    "objectID": "qmd/scraping.html#sec-scrap-misc",
    "href": "qmd/scraping.html#sec-scrap-misc",
    "title": "Scraping",
    "section": "Misc",
    "text": "Misc\n\nIn loops, use Sys.sleep (probably) after EVERY selenium function. Sys.sleep(1) might be all thatâ€™s required.\n\nsee Projects &gt; foe &gt; gb-level-1_9-thread &gt; scrape-gb-levels.R\nMight not always be needed, but absolutely need if youâ€™re filling out a form and submitting it.\nMight even need one at the top of the loop\nIf a Selenium function stops working, adding Sys.sleeps are worth a try.\n\nSometimes clickElement( ) stops working for no apparent reason.Â When this happens used sendKeysToElement(list(\"laptops\",key=\"enter\"))\nIn batch scripts (.bat), sometimes after a major windows update, the Java that selenium uses will trigger Windows Defender (WD) and cause the scraping script to fail (if you have it scheduled). If you run the .bat script manually and then when the WD box rears its ugly head, just click ignore. WD should remember after that and not to mess with it.\nRSelenium findElement(using = \"\") options â€œclass nameâ€ : Returns an element whose class name contains the search value; compound class names are not permitted.\n\nâ€œcss selectorâ€ : Returns an element matching a CSS selector.\nâ€œidâ€ : Returns an element whose ID attribute matches the search value.\nâ€œnameâ€ : Returns an element whose NAME attribute matches the search value.\nâ€œlink textâ€ : Returns an anchor element whose visible text matches the search value.\nâ€œpartial link textâ€ : Returns an anchor element whose visible text partially matches the search value.\nâ€œtag nameâ€ : Returns an element whose tag name matches the search value.\nâ€œxpathâ€ : Returns an element matching an XPath expression."
  },
  {
    "objectID": "qmd/scraping.html#sec-scrap-terms",
    "href": "qmd/scraping.html#sec-scrap-terms",
    "title": "Scraping",
    "section": "Terms",
    "text": "Terms\n\nStatic Web Page: A web page (HTML page) that contains the same information for all users. Although it may be periodically updated, it does not change with each user retrieval.\nDynamic Web Page: A web page that provides custom content for the user based on the results of a search or some other request. Also known as â€œdynamic HTMLâ€ or â€œdynamic contentâ€, the â€œdynamicâ€ term is used when referring to interactive Web pages created for each user."
  },
  {
    "objectID": "qmd/scraping.html#sec-scrap-rvest",
    "href": "qmd/scraping.html#sec-scrap-rvest",
    "title": "Scraping",
    "section": "rvest",
    "text": "rvest\n\nMisc\n\nNotes from: Pluralsight.Advanced.Web.Scraping.Tactics.R.Playbook\n\nUses css selectors or xpath to find html nodes\nlibrary(rvest)\npage &lt;- read_html(\"&lt;url&gt;\")\nnode &lt;- html_element(page, xpath = \"&lt;xpath&gt;\"\n\nFind css selectors\n\nselector gadget\n\nclick selector gadget app icon in Chrome in upper right assuming youâ€™ve installed it already\nclick item on webpage you want to scrape\n\nit will highlight other items as well\n\nclick each item you DONâ€™T want to deselect it\ncopy the selector name in box at the bottom of webpage\nUse html_text to pull text or html_attr to pull a link or something\n\ninspect\n\nright-click item on webpage\nclick inspect\nhtml element should be highlighted in elements tab of right side pan\nright-click element â€“&gt; copy â€“&gt; copy selector or copy xpath\n\n\n\nExample: Access data that needs authenticationÂ (also see RSelenium version)\n\nnavigate to login page\nsession &lt;- session(\"&lt;login page url&gt;\")\nFind â€œformsâ€ for username and password\nform &lt;- html_form(session)[[1]]\nform\n\nEvidently there are multiple forms on a webpage. He didnâ€™t give a good explanation for why he chose the first one\nâ€œsession_keyâ€ and â€œsession_passwordâ€ are the ones needed\n\nFill out the necessary parts of the form and send it\nfilled_form &lt;- html_form_set(form, session_key = \"&lt;username&gt;\", session_password = \"&lt;password&gt;\")\nfilled_form # shows values that inputed next the form sections\nlog_in &lt;- session_submit(session, filled_form)\nConfirm that your logged in\nlog_in # prints url status = 200, type = text/html, size = 757813 (number of lines of html on page?)\nbrowseURL(log_in$url) # think this maybe opens browser\n\nExample: Filter a football stats table by selecting values from a dropdown menu on a webpage (also see RSelenium version)\n\nAfter set-up and navigating to url, get the forms from the webpage\nforms &lt;- html_form(session)\nforms # prints all the forms\n\nThe fourth has all the filtering menu categories (team, week, position, year), so that one is chosen\n\nFill out the form to enter the values you want to use to filter the table and submit that form to filter the table\nfilled_form &lt;- html_form_set(forms[[4]], \"team\" = \"DAL\", \"week\" = \"all\", \"position\" = \"QB\", \"year\" = \"2017\")\nsubmitted_session &lt;- session_submit(session = session, form = filled_form)\nLook for the newly filtered table\ntables &lt;- html_elements(submitted_session, \"table\")\ntables\n\nUsing inspect, you can see the 2nd one has &lt;table class = â€œsortable stats-tableâ€¦etc\n\nSelect the second table and convert it to a dataframe\nfootball_df &lt;- html_table(tables[[2]], header = TRUE)"
  },
  {
    "objectID": "qmd/scraping.html#sec-scrap-rsel",
    "href": "qmd/scraping.html#sec-scrap-rsel",
    "title": "Scraping",
    "section": "RSelenium",
    "text": "RSelenium\n\nAlong with installing package you have to know the version of the browser driver of the browser youâ€™re going to use\n\nhttps://chromedriver.chromium.org/downloads\nFind Chrome browser version\n\nThrough console\nsystem2(command = \"wmic\",\nÂ  Â  Â  Â  args = 'datafile where name=\"C:\\\\\\\\Program Files         (x86)\\\\\\\\Google\\\\\\\\Chrome\\\\\\\\Application\\\\\\\\chrome.exe\" get Version /value')\n\nList available Chrome drivers\nbinman::list_versions(appname = \"chromedriver\")\n\nIf no exact driver version matches your browser version,\n\nEach version of the Chrome driver supports Chrome with matching major, minor, and build version numbers.\n\nExample: Chrome driver 73.0.3683.20Â  supports all Chrome versions that start with 73.0.3683\n\n\n\n\nStart server and create remote driver\n\na browser will pop up and say â€œChrome is being controlled by automated test softwareâ€\n\nlibrary(RSelenium)\ndriver &lt;- rsDriver(browser = c(\"chrome\"), chromever = \"&lt;driver version&gt;\", port = 4571L) # assume the port number is specified by chrome driver ppl.\nremDr &lt;- driver[['client']] # can also use $client\nNavigate to a webpage\nremDr$navigate(\"&lt;url&gt;\")\nremDR$maxWindowSize(): Set the size of the browser window to maximum.\n\nBy default, the browser window size is small, and some elements of the website you navigate to might not be available right away\n\nGrab the url of the webpage youâ€™re on\nremDr$getCurrentUrl()\nGo back and forth between urls\nremDr$goBack()\nremDr$goForward()\nFind html element (name, id, class name, etc.)\nwebpage_element &lt;- remDr$findElement(using = \"name\", value = \"q\") \n\nSee Misc section for selector options\nWhere â€œnameâ€ is the element class and â€œqâ€ is the value e.g.Â name=â€œqâ€ if you used the inspect method in chrome\nAlso see Other Stuff &gt;&gt; Shadow DOM elements &gt;&gt; Use {shadowr} for alternate syntax to search for web elements\n\nHighlight element in pop-up browser to make sure you have the right thing\nwebpage_element$highlightElement()\nExample: you picked a search bar for your html element and now you want to use the search bar from inside R\n\nEnter text into search bar\nwebpage_element$sendKeysToElement(list(\"Scraping the web with R\"))\nHit enter to execute search\nwebpage_element$sendKeysToElement(list(key = \"enter\"))\n\nYou are now on the page with the results of the google search\n\nScrape all the links and titles on that page\nwebelm_linkTitles &lt;- remDr$findElement(using = \"css selector\", \".r\") \n\nInspect showed â€\n\n\n. Notice he used â€œ.râ€. Says it will pick-up all elements with â€œrâ€ as the class.\n\nGet titles\n# first title\nwebelm_linkTitles[[1]]$getElementText()\n\n# put them all into a list\ntitles &lt;- purrr::map_chr(webelm_linkTitles, ~.x$getElementText())\ntitles &lt;- unlist(lapply(\n    webelm_linkTitles, \n    function(x) {x$getElementText()}\n\nExample: Access data that needs user authenticationÂ (also see rvest version)\n\nAfter set-up and navigating to webpage, find elements where you type in your username and password\nwebelm_username &lt;- remDr$findElement(using = \"id\", \"Username\")\nwebelm_pass &lt;- remDr$findElement(using = \"id, \"Password\")\nEnter username and password\nwebpage_username$sendKeysToElement(list(\"&lt;username&gt;\"))\nwebpage_pass$sendKeysToElement(list(\"&lt;password&gt;\"))\nClick sign-in button and click it\nwebelm_sbutt &lt;- remDr$findElement(using = \"class\", \"psds-button\")\nwebelm_sbutt$clickElement()\n\nExample: Filter a football stats table by selecting values from a dropdown menu on a webpage (also see rvest version)\n\nThis is tedious â€” use rvest to scrape this if possible (have to use rvest at the end anyways). html forms are the stuff.\nAfter set-up and navigated to url, find drop down â€œteamâ€ menu element locator using inspect in the browser and use findElement\nwebelem_team &lt;- remDr$findElement(using = \"name\", value = \"team\") # conveniently has name=\"team\" in the html\n\nAlso see Other Stuff &gt;&gt; Shadow DOM elements &gt;&gt; Use {shadowr} for alternate syntax to search for web elements\n\nclick team dropdown\nwebelem_team$clickElement()\nGo back to inspect in the browser, you should be able to expand the team menu element. Left click value that you want to filter team by to highlight it. Then right click the element and select â€œcopyâ€ â€“&gt; â€œcopy selectorâ€. Paste selector into value arg\nwebelem_DAL &lt;- remDr$findElement(using = \"css\", value = \"edit-filters-0-team &gt; option:nth-child(22)\")\nwebelem_DAL$clickElement()\n\nAlso see Other Stuff &gt;&gt; Shadow DOM elements &gt;&gt; Use {shadowr} for alternate syntax to search for web elements\nRepeat process for week, position, and year drop down menu filters\n\nAfter youâ€™ve selected all the values in the dropdown, click the submit button to filter the table\nwebelem_submit &lt;- remDr$findElement(using = \"css\", value =     \"edit-filters-0-actions-submit\") \nwebelem_submit$clickElement()\n\nFinds element by using inspect on the submit button and copying the selector\n\nGet the url of the html code of the page with the filtered table. Read html code into R with rvest.\nurl &lt;- remDr$getPageSource()[[1]]\nhtml_page &lt;- rvest::read_html(url)\n\nIf you want the header, getPageSource(header = TRUE)\n\nUse rvest to scrape the table. Find the table with the stats\nall_tables &lt;- rvest::html_elements(html_page, \"table\")\nall_tables\n\nUsed the â€œhtml_elementsâ€ version instead of â€œelementâ€\nThird one has â€œ&lt;table class =â€sortable stats-table full-width blah blahâ€\n\nSave to table to dataframe\nfootball_df &lt;- rvest::html_table(all_tables[[3]], header = TRUE)"
  },
  {
    "objectID": "qmd/scraping.html#sec-scrap-ostuff",
    "href": "qmd/scraping.html#sec-scrap-ostuff",
    "title": "Scraping",
    "section": "Other Stuff",
    "text": "Other Stuff\n\nClicking a semi-infinite scroll button (e.g.Â â€œSee moreâ€)\n\nExample: For-Loop\n# Find Page Element for Body\nwebElem &lt;- remDr$findElement(\"css\", \"body\")\n\n# Page to the End\nfor (i in 1:50) {\nÂ  message(paste(\"Iteration\",i))\nÂ  webElem$sendKeysToElement(list(key = \"end\"))\n\nÂ  # Check for the Show More Button\nÂ  element&lt;- try(unlist(\nÂ      remDr$findElement(\nÂ        \"class name\",\nÂ        \"RveJvd\")$getElementAttribute('class')), silent = TRUE)\n\nÂ  #If Button Is There Then Click It\nÂ  Sys.sleep(2)\nÂ  if(str_detect(element, \"RveJvd\") == TRUE){\nÂ  Â  buttonElem &lt;- remDr$findElement(\"class name\", \"RveJvd\")\nÂ  Â  buttonElem$clickElement()\nÂ  }\n\nÂ  # Sleep to Let Things Load\nÂ  Sys.sleep(3)\n}\n\narticle\nAfter scrolling to the â€œendâ€ of the page, thereâ€™s a â€œshow me more buttonâ€ that loads more data on the page\n\nExample: Recursive\nload_more &lt;- function(rd) {\n  # scroll to end of page\n  rd$executeScript(\"window.scrollTo(0, document.body.scrollHeight);\", args = list())\n\n  # Find the \"Load more\" button by its CSS selector and ...\n  load_more_button &lt;- rd$findElement(using = \"css selector\", \"button.btn-load.more\")\n\n  # ... click it\n  load_more_button$clickElement()\n\n  # give the website a moment to respond\n  Sys.sleep(5)\n}\n\nload_page_completely &lt;- function(rd) {\n  # load more content even if it throws an error\n  tryCatch({\n    # call load_more()\n    load_more(rd)\n    # if no error is thrown, call the load_page_completely() function again\n    Recall(rd)\n  }, error = function(e) {\n    # if an error is thrown return nothing / NULL\n  })\n}\n\nload_page_completely(remote_driver)\n\narticle\nRecall is a base R function that calls the same function itâ€™s in.\n\n\nShadow DOM elements\n\n#shadow-root and shadow dom button elements\nMisc\n\nTwo options: {shadowr} or JS script\n\nExample: Use {shadowr}\n\nMy stackoverflow post\nSet-up\npacman::p_load(RSelenium, shadowr)\ndriver &lt;- rsDriver(browser = c(\"chrome\"), chromever = chrome_driver_version)\n# chrome browser\nchrome &lt;- driver$client\nshadow_rd &lt;- shadow(chrome)\nFind web element\n\nSearch for element using html tag\n\n\nwisc_dl_panel_button4 &lt;- shadowr::find_elements(shadow_rd, 'calcite-button')\nwisc_dl_panel_button4[[1]]$clickElement()\n\nShows web element located in #shadow-root\nSince there might be more than one element with the â€œcalcite-buttonâ€ html tag, we use the plural, find_elements, instead of find_element\nThereâ€™s only 1 element returned, so we use [[1]] index to subset the list before clicking it\n\nSearch for web element by html tag and attribute\nwisc_dl_panel_button3 &lt;- find_elements(shadow_rd, 'button[aria-describedby*=\"tooltip\"]')\nwisc_dl_panel_button3[[3]]$clickElement()\n\nâ€œbuttonâ€ is the html tag which is subsetted by the brackets, and â€œaria-describedbyâ€ is the attribute\nOnly part of the attributeâ€™s value is used, â€œtooltip,â€ so I think thatâ€™s why â€œ*=â€ instead of just â€œ=â€ is used. I believe the â€œ*â€ may indicate partial-matching.\nSince there might be more than one element with thisÂ  html tag + attribute combo, we use the plural, find_elements, instead of find_element\nThere are 3 elements returned, so we use [[3]] index to subset the list to element we want before clicking it\n\n\nExample: Use a JS script and some webelement hacks to get a clickable element\n\nMisc\n\nâ€œ.class_nameâ€\n\nfill in spaces with periods\n\nâ€œ.btn btn-default hidden-xsâ€ becomes â€œ.btn.btn-default.hidden-xsâ€\n\n\n\nYou can find the element path to use in your JS script by going step by step with JS commands in the Chrome console (bottom window)\n\nSteps\n\nWrite JS script to get clickable elementâ€™s elementId\n\nStart with element right above first shadow-root element and use querySelector\nMove to the next element inside the next shadow-root element using shadowRoot.querySelector\nContinue to desired clickable element\n\nIf thereâ€™s isnâ€™t another shadow-root that you have to open, then the next element can be selected usingquerySelector\nIf you do have to click on another shadow-root element to open another branch, then used shadowRoot.querySelector\nExample\n\n\nâ€œhub-download-cardâ€ is just above shadow-root so it needs querySelector\nâ€œcalcite-cardâ€ is an element thatâ€™s one-step removed from shadow-root, so it needs shadowRoot.querySelector\nâ€œcalcite-dropdownâ€ (type = â€œclickâ€) is not directly (see div) next to shadow-root , so it can selected using querySelector\n\n\nWrite and execute JS script\nwisc_dlopts_elt_id &lt;- chrome$executeScript(\"return document.querySelector('hub-download-card').shadowRoot.querySelector('calcite-card').querySelector('calcite-dropdown');\")\n\nMake a clickable element or just click the damn thing\n\nclickable element (sometimes this doesnâ€™t work; needs to be a button or type=click)\n\nUse findElement to find a generic element class object that you can manipulate\nUse â€œ@â€ ninja-magic to force elementId into the generic webElement to coerce it into your button element\nUse clickElement to click the button\n\n# think this is a generic element that can always be used\nmoose &lt;- chrome$findElement(\"css\", \"html\")\nmoose@.xData$elementId &lt;- as.character(wisc_dlopts_elt_id)\nmoose$clickElement()\n\nClick the button\nchrome$executeScript(\"document.querySelector('hub-download-card').shadowRoot.querySelector('calcite-card').querySelector('calcite-dropdown').querySelector('calcite-dropdown-group').querySelector('calcite-dropdown-item:nth-child(2)').click()\")\n\n\n\nGet data from a hidden input\n\narticle\nHTML Element\n&lt;input type=\"hidden\" id=\"overview-about-text\" value=\"%3Cp%3E100%25%20Plant-Derived%20Squalane%20hydrates%20your%20skin%20while%20supporting%20its%20natural%20moisture%20barrier.%20Squalane%20is%20an%20exceptional%20hydrator%20found%20naturally%20in%20the%20skin,%20and%20this%20formula%20uses%20100%25%20plant-derived%20squalane%20derived%20from%20sugar%20cane%20for%20a%20non-comedogenic%20solution%20that%20enhances%20surface-level%20hydration.%3Cbr%3E%3Cbr%3EOur%20100%25%20Plant-Derived%20Squalane%20formula%20can%20also%20be%20used%20in%20hair%20to%20increase%20heat%20protection,%20add%20shine,%20and%20reduce%20breakage.%3C/p%3E\"&gt;\nExtract value and decode the text\noverview_text &lt;- webpage |&gt;\n  html_element(\"#overview-about-text\") |&gt;\n  html_attr(\"value\") |&gt;\n  URLdecode() |&gt;\n  read_html() |&gt;\n  html_text()\n\noverview_text\n#&gt; [1] \"100% Plant-Derived Squalane hydrates your skin while supporting its natural moisture barrier."
  },
  {
    "objectID": "qmd/shiny-general.html#resources",
    "href": "qmd/shiny-general.html#resources",
    "title": "59Â  General",
    "section": "59.1 Resources",
    "text": "59.1 Resources\n\nRStudio https://shiny.rstudio.com/tutorial/\n\nmodules https://shiny.rstudio.com/articles/modules.html\nbeginner https://rstudio-education.github.io/shiny-course/\n\nHadley ebook https://mastering-shiny.org/\nEngineering Production-Grade Shiny Apps - {golem} book\nAdv. UI ebook https://unleash-shiny.rinterface.com/\nSimple app packaged, modules, etc.\n\nhttps://github.com/2DegreesInvesting/scenarioSelector#learning-shiny\n\nExamples and templates for various usecases (link)"
  },
  {
    "objectID": "qmd/shiny-general.html#misc",
    "href": "qmd/shiny-general.html#misc",
    "title": "59Â  General",
    "section": "59.2 Misc",
    "text": "59.2 Misc\n\nA common source of poor Shiny performance and maintainability is placing a large amount of logic within a single reactive statement such as anÂ observe()\n\nInstead of adding a bunch of renderPlots to an observe(), liberally use reactive variables and your code becomes much cleaner, faster, and more maintainable\n\nGeneral Workflow (Dancho)\n\nSimple App\n\nSomething bare bones that has ui and server sections\nCould be a company or personal template or the â€œHello, Worldâ€ app with slider and text output\n\nBetter App\n\nAdd images (e.g.Â navbar logo, splash page background)\nAdjust layout (add sidebar, main panel, etc. to UI)\nAdd blank cards (pill, tab, etc.) which will later be filled with charts, tables, etc.\nTitles (navbar, side bar, main panel, etc.)\nUse Bootstrap 5 and style.css files (style navbar, titles, etc.)\n\nPlaceholder App\n\nIntegrate analysis into the app\nMake cards look professional\nUse widgets to make interactive visualizations (charts, tables)\n\nReactive App\n\nMake all calculations and events reactive\n\ni.e user input changes output (e.g.Â data values/calculations that go into the charts, tables, etc.)\n\nThis is the basic working application\n\nFinal Product\n\nFeature examples\n\nAdd full data ingestion\nAdd search\nAdd additional inputs and calculations (i.e.Â more analysis)\n\nCreate modular code (utils directory)\n\nCode gets long for more complicated apps. Modularizing it makes it more readable and easier to maintain\n\nMake output downloadable into a report"
  },
  {
    "objectID": "qmd/shiny-general.html#design",
    "href": "qmd/shiny-general.html#design",
    "title": "59Â  General",
    "section": "59.3 Design",
    "text": "59.3 Design\n\nMisc\n\npackages\n\n{designer} - makes the initial generation of a UI wireframe of a shiny application as quick and simple as possible. Good for PoCs. Allows you to drag and drop components.\n{shinyuieditor} - seems like itâ€™s similar to {designer}\n\nAlso see BizSci video\n\n\nSemantic Layer - AÂ  business representation of corporate data that helps end users access data autonomously using common business terms. By using common business terms, rather than data language, to access, manipulate, and organize information, a semantic layer simplifies the complexity of business data. Business terms are stored as objects in a semantic layer, which are accessed through business views\nask for feedback on each stage and implement changes accordingly\nInfinite Scrolling vs Pagination (article with code)\n\nInfinite scrolling is easier to use on mobile devices and requires fewer â€œclicksâ€ compared to pagination.\n\nMight increase user engagement on your page as viewers stay on your website longer; continuously searching for relevant content when they have no particular goal in mind.\n\n\n\nMap the userâ€™s workflow\n\nWhy?\n\nSeeing how the users accomplish the task now will give you a better understanding of the whole process or even point to a competitive advantage.\nMaybe the existing tools have some inefficiencies that your app can address?\nOr maybe some parts of the current solution can be reused to speed up user adoption and onboarding?\n\nQuestions\n\nWhat is the main reason for building the app?\nWho are the users?\nWhat will the users accomplish with your app? What are their business goals?\nHow have they done it so far? Are they already used to any particular tools or workflows?\n\n\nFor Analysis apps\n\ninclude at least a few toy datasets so people know what the data is supposed to look like and be able to play around with the app\n\nadd a description card for each toy data set\n\nadd tool-tips for input ui text\nadd descriptions/instructions to the top of each page\n\nApps are for users (not you)\n\nWhat decision does the user make by visiting this app?\nWhat does the user learn?\nHow do they interact with your app to do this?\n\nData visualizations\n\nWhat do you want to tell with the data?\nWhat is the context?\nWho is your audience?"
  },
  {
    "objectID": "qmd/shiny-general.html#testing",
    "href": "qmd/shiny-general.html#testing",
    "title": "59Â  General",
    "section": "59.4 Testing",
    "text": "59.4 Testing\n\nCan test at any stage of the development process, even before it starts.\n\nYou can use the wireframes or mockups and manually change the â€œscreensâ€ as the user â€œperforms an actionâ€ in the app.\n\n**Do not to leave testing for the last moment. When the app development is finished, rebuilding the UI will be costly.**\nTypes\n\nunit tests\nperformance tests\nin-depth user interviews\n\nSessions with the user where you ask the person to perform several tasks within the tool. This way you can see if there are any recurring problems with navigation or the general ease of use.\n\nhallway tests\n\nShort version of an in-depth user interview\nJust ask your colleagues to use the app for 5-10 minutes and note their impressions.\n\n\nFinal Checklist\n\nIs the app design responsive?\nDoes the user know whatâ€™s wrong when receiving an error message?\nIs the user well informed about the state of the app, e.g.Â when waiting for the calculation, is it clear when it will be finished?"
  },
  {
    "objectID": "qmd/shiny-general.html#js",
    "href": "qmd/shiny-general.html#js",
    "title": "59Â  General",
    "section": "59.5 JS",
    "text": "59.5 JS\n\nwww - directiory for images, styles, or JS script. Same directory as app.R\nUse a javascript file + library\n\nExample stylish pop-up notification\n# Shiny AppÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  // JS script; basic toastify notification\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Shiny.addCustomMessageHandler('notify', function(msg){\nui &lt;- fluidPage(Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Toastify({\nÂ  tags$head(Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  text: msg\nÂ  Â  # toastify css dependencyÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  }).showToast();\nÂ  Â  tags$link(Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  });\nÂ  Â  Â  rel = \"stylesheet\",\nÂ  Â  Â  type = \"text/css\",\nÂ  Â  Â  href = \"https://cdn.jsdelivr.net/npm/toastify-js/src/toastify.min.css\"\nÂ  Â  ),\nÂ  Â  # our script\nÂ  Â  tags$script(\nÂ  Â  Â  type = \"text/javascript\",\nÂ  Â  Â  src = \"script.js\"\nÂ  Â  )\nÂ  ),\nÂ  h1(\"Notifications\"),\nÂ  textInput(\"text\", \"Text\"),\nÂ  actionButton(\"show\", \"Show\"),\nÂ  # toastify js dependency\nÂ  tags$script(\nÂ  Â  type = \"text/javascript\",\nÂ  Â  src = \"https://cdn.jsdelivr.net/npm/toastify-js\"\nÂ  )\n)\nserver &lt;- function(input, output, session){\nÂ  observeEvent(input$show, {\nÂ  Â  session$sendCustomMessage(\nÂ  Â  Â  \"notify\",\nÂ  Â  Â  input$text\nÂ  })\n}\nshinyApp(ui, server)\n\ntoastify.js is a library for some snazzy notifications you can use that pop-up in your browser when a event (e.g.Â button pushed) has happened.\nButton is pressed in ui, sends â€œmessageâ€ (text, data.frame, etc.) to server, server sends â€œmessageâ€ to js script in ui which triggers a notification in the browser.\nUI\n\ntags\\(head includes: * tags\\)link links to the toastify CSS stylesheet\n\ntags$script points to the path of the js script and provides the identifier (â€œtypeâ€), â€œtext/javascriptâ€\n\ntags$script links to the toastify js dependency and also has the identifier, â€œtext/javascriptâ€\n\nServer\n\nObserveEventÂ  listens for ui input â€œshowâ€ button to be pressed\nsendCustomMessageÂ  is a js function to send a message from the server to the browser\n\nidentifier (â€œtypeâ€) = â€œnotifyâ€ says which custom handler to use in the js script\n\n\njs script\n\naddCustomMessageHandlerÂ  is a js function that receives the message from the server\n\nâ€œnotifyâ€ is the identifer that it listens for\nToastify()Â  takes a JSON formatted list of args\nshowToastÂ  method sends snazzy notification to the userâ€™s browser."
  },
  {
    "objectID": "qmd/shiny-hosting.html#sec-shiny-hosting-misc",
    "href": "qmd/shiny-hosting.html#sec-shiny-hosting-misc",
    "title": "Hosting",
    "section": "Misc",
    "text": "Misc\n\nExamples\n\nshiny server, linux, local and Digital Ocean - Deploy your own Shiny app server with debian | R with White Dwarf\n\nBenchmarks\n\nOverview\n\n\nPosit Connect &gt;&gt; ShinyProxy for scaling up even though its free (article)\nFree options\n\nshinyapps.io, heroku, Fly.io\nFly.ioâ€™s free tier is your best option if you want the apps running 24/7 and under a secured custom domain (none of this is offered for Shinyapp.io and Heroku).\nOther options\n\nIf you run out of free builds or usage limits, use multiple emails to create multiple free-tier accounts\nBuild mulitple apps on the same docker image\n\nShinyProxy\n\nBenefits\n\nOpen source â€“ no subscription fee\nA wide range of Authorization options available (LDAP / Kerberos / SSO / SAML / Open ID / Keycloak / Social Media / Simple (flat file of users & passwords))\n\nCosts\n\nHigher implementation cost\nNo product support, therefore there is no possibility of SLA guarantees\nThe additional cost of post-implementation support\nThe additional layer of complexity â€“ Docker/Kubernetes\nLong-term maintenance requiring a large workforce\n\n\nShiny Server\n\nPros - open source\nCons\n\nMust load it on the server\nKeep track of security patches\nManage dependencies and harmonize them if you have multiple apps on the same server\n\n\n\nDigital Ocean\n\ndigitalocean droplet - $4/month and host the apps with shinyproxy or shinyserver\n\nFor light, intermittent usage\n\n\nShinyapps.io\n\nAs of 2023, $13 for lowest paid tier.\n\nPosit Connect\n\nBenefits\n\nComes with great integration with R and Python and comes with a lot more than just hosting Shiny apps\nAbility to deploy R/Python applications, R/Python API, RMarkdown reports that regenerate automatically, etc.\nWorth it if youâ€™re working with a team and delivering more than shiny apps (e.g.Â reports, cron jobs, etc.)\nStability with Software licensing and Standard Software Support from Posit included\nLower implementation costs\nEasier configuration and deployment of dashboard versions\nAuthorization is included in the Posit Connect Subscription, with a wide range of options to choose from: (LDAP and Active Directory / SAML / OAuth 2.0 using Google Apps accounts / PAM / Proxied Authentication / SLA)\nEasier maintenance in production\nOption to add Posit Workbench and Package Manager\nAutomatic scaling â€“ no need to manually trigger new processes\nAn industry standard by Fortune 500 companies working with R\nAdmin panel to manage users and monitor the logs and machine\n\nCosts\n\nA yearly Posit Connect subscription starts at $14,995/year (USD)"
  },
  {
    "objectID": "qmd/shiny-modules.html",
    "href": "qmd/shiny-modules.html",
    "title": "64Â  Shiny, Modules",
    "section": "",
    "text": "TOC\n\nMisc\nBasics\nBest Practices\n\nMisc\n\nResources\n\nRStudio\nMastering Shiny, Ch. 19\nEngineering Production-Grade Shiny Apps - {golem} book\nSimple app packaged, modules, etc.\n\nhttps://github.com/2DegreesInvesting/scenarioSelector#learning-shiny\n\n\nPackages\n\n{golem} - also has many example apps and resources\n{rhino} - appsilon package\n\n\nBasics\n\nDecompose Requirements to help determine modules\n\nPick a subset of the data\nThe metrics of interest are:\n\ne.g.Â average departure delay, average arrival delay, proportion of flights with an arrival delay &gt; 5min\n\nFor each metric of interest, users should:\n\nExample:\n\nSee a time series plot of the average daily value of the metric\nBe able to click a download button to download a PNG of the plot\nRead a text summary that reports the number of days with breaches\n\n\n\nDemo function for testing modules\n\nmodule_demo &lt;- function () {\nÂ  Â  # define test data\nÂ  Â  df &lt;- data.frame(x = 1:30, y = 1:30)\n\nÂ  Â  # call module components\nÂ  Â  ui &lt;- fluidPage(module_ui(\"x\"))\nÂ  Â  server &lt;- function(input, output, session) {\nÂ  Â  Â  Â  module_server(\"x\", reactive([{df}]{style='color: #990000'}))\nÂ  Â  }\nÂ  Â  shinyApp(ui, server)\n}\n\nExample: Workflow\n\nHandle one project requirement at a time\nModule for text summary of a metric\n\n\ntext_ui &lt;- function(id) {\n\nÂ  fluidRow(textOutput(NS(id, \"text\")))\n\n}\n\ntext_server &lt;- function(id, df, vbl, threshhold) {\n\nÂ  moduleServer(id, function(input, output, session) {\n\nÂ  Â  n &lt;- reactive({sum(df()[[vbl]] &gt; threshhold){style='color: #990000'}[}]{style='color: #990000'})\n\nÂ  Â  output$text &lt;- renderText({\nÂ  Â  Â  paste(\"In this month\",\nÂ  Â  Â  Â  Â  Â  vbl,\nÂ  Â  Â  Â  Â  Â  \"exceeded the average daily threshhold of\",\nÂ  Â  Â  Â  Â  Â  threshhold,\nÂ  Â  Â  Â  Â  Â  \"a total of\", n(), \"days\")\nÂ  Â  })\nÂ  })\n}\n\ntext_demo &lt;- function() {\nÂ  df &lt;- data.frame(day = 1:30, arr_delay = 1:30)\nÂ  ui &lt;- fluidPage(text_ui(\"x\"))\nÂ  server &lt;- function(input, output, session) {\nÂ  Â  text_server(\"x\", reactive([{df}]{style='color: #990000'}), \"arr_delay\", 15)\nÂ  }\nÂ  shinyApp(ui, server)\n}\n\nModule for plotting\n\nplot_ui &lt;- function(id) {\n\nÂ  fluidRow(\nÂ  Â  column(11, plotOutput(NS(id, \"plot\"))),\nÂ  Â  column( 1, downloadButton(NS(id, \"dnld\"), label = \"\"))\n)\n\n}\n\nplot_server &lt;- function(id, df, vbl, threshhold = NULL) {\n\nÂ  moduleServer(id, function(input, output, session) {\nÂ  Â  plot &lt;- reactive({viz_monthly(df(), vbl, threshhold){style='color: #990000'}[}]{style='color: #990000'})\nÂ  Â  output$plot &lt;- renderPlot({plot(){style='color: #990000'}[}]{style='color: #990000'})\nÂ  Â  output$dnld &lt;- downloadHandler(\nÂ  Â  Â  filename = function() {paste0(vbl, '.png'){style='color: #990000'}[}]{style='color: #990000'},\nÂ  Â  Â  content = function(file) {ggsave(file, plot()){style='color: #990000'}[}]{style='color: #990000'}\nÂ  Â  )\nÂ  })\n\n}\n\nplot_demo &lt;- function() {\nÂ  Â  df &lt;- data.frame(day = 1:30, arr_delay = 1:30)\nÂ  Â  ui &lt;- fluidPage(plot_ui(\"x\"))\nÂ  Â  server &lt;- function(input, output, session) {\nÂ  Â  Â  Â  plot_server(\"x\", reactive([{df}]{style='color: #990000'}), \"arr_delay\")\nÂ  Â  }\nÂ  Â  shinyApp(ui, server)\n}\n\nCompose Modules\n\nmetric_ui &lt;- function(id) {\n\nÂ  Â  fluidRow(\nÂ  Â  Â  text_ui(NS(id, \"metric\")),\nÂ  Â  Â  plot_ui(NS(id, \"metric\"))\nÂ  Â  )\n\n}\n\nmetric_server &lt;- function(id, df, vbl, threshhold) {\n\nÂ  Â  moduleServer(id, function(input, output, session) {\nÂ  Â  Â  text_server(\"metric\", df, vbl, threshhold)\nÂ  Â  Â  plot_server(\"metric\", df, vbl, threshhold)\nÂ  Â  })\n\n}\n\nmetric_demo &lt;- function() {\n\nÂ  Â  df &lt;- data.frame(day = 1:30, arr_delay = 1:30)\nÂ  Â  ui &lt;- fluidPage(metric_ui(\"x\"))\nÂ  Â  server &lt;- function(input, output, session) {\nÂ  Â  metric_server(\"x\", reactive([{df}]{style='color: #990000'}), \"arr_delay\", 15)\nÂ  Â  }\nÂ  Â  shinyApp(ui, server)\n\n}\n\nBuild-out app\n\nui &lt;- fluidPage(\n\nÂ  Â  titlePanel(\"Flight Delay Report\"),\n\nÂ  Â  sidebarLayout(\nÂ  Â  Â  sidebarPanel = sidebarPanel(\nÂ  Â  Â  Â  selectInput(\"month\", \"Month\",\nÂ  Â  Â  Â  choices = setNames(1:12, month.abb),\nÂ  Â  Â  Â  selected = 1\nÂ  Â  Â  )\nÂ  Â  ),\nÂ  Â  mainPanel = mainPanel(\nÂ  Â  Â  h2(textOutput(\"title\")),\nÂ  Â  Â  h3(\"Average Departure Delay\"),\nÂ  Â  Â  metric_ui(\"dep_delay\"),\nÂ  Â  Â  h3(\"Average Arrival Delay\"),\nÂ  Â  Â  metric_ui(\"arr_delay\"),Â \nÂ  Â  Â  h3(\"Proportion Flights with &gt;5 Min Arrival Delay\"),\nÂ  Â  Â  metric_ui(\"ind_arr_delay\")\nÂ  Â  )\n)\nserver &lt;- function(input, output, session) {\n\nÂ  Â  output$title &lt;- renderText({paste(month.abb[as.integer(input$month)], \"Report\"){style='color: #990000'}[}]{style='color: #990000'})\nÂ  Â  df_month &lt;- reactive({filter(ua_data, month == input$month){style='color: #990000'}[}]{style='color: #990000'})\nÂ  Â  metric_server(\"dep_delay\", df_month, vbl = \"dep_delay\", threshhold = 10)\nÂ  Â  metric_server(\"arr_delay\", df_month, vbl = \"arr_delay\", threshhold = 10)\nÂ  Â  metric_server(\"ind_arr_delay\", df_month, vbl = \"ind_arr_delay\", threshhold = 0.5)\n\n}\nBest Practices\n\nPass reactive variable to modules. Donâ€™t call modules inside of some other reactive statement likeÂ observe()\n\nBad module calling example\n\n\nÂ  Â  observe({\nÂ  Â  Â  # Process data before sending it into the module\nÂ  Â  Â  if (input$filterTo != \"special\") {\nÂ  Â  Â  Â  myModuleServer(\nÂ  Â  Â  Â  Â  data %&gt;%Â \nÂ  Â  Â  Â  Â  Â  filter(val == input$filterTo)\nÂ  Â  Â  Â  )\nÂ  Â  Â  } else {\nÂ  Â  Â  Â  # Handle special caseÂ \nÂ  Â  Â  Â  myModuleServer(\nÂ  Â  Â  Â  Â  data %&gt;%Â \nÂ  Â  Â  Â  Â  Â  ...\nÂ  Â  Â  Â  )\nÂ  Â  Â  }\nÂ  Â  })\n\nGood module calling example\n\nÂ  Â  server &lt;- function(input, output, session) {\n\nÂ  Â  Â  # initialise the app state...\nÂ  Â  Â  app_state &lt;- reactiveValues(...)\n\nÂ  Â  Â  ...Â \n\nÂ  Â  Â  # add server logic\nÂ  Â  Â  mod_commute_mode(\"mode\", app_state)\nÂ  Â  Â  mod_commute_map(\"map\", app_state)\nÂ  Â  Â  mod_commute_table(\"table\", app_state)\nÂ  Â  Â  mod_commute_filter(\"filter\", app_state)\nÂ  Â  }\n\nLiberally use reactive variables and your code becomes much cleaner, faster, and more maintainable\n\nThere shouldnâ€™t be any deeply nested bracketing in your code\n\n\ndatasheet_df &lt;- reactive({\nÂ  sample_data %&gt;%\nÂ  filter(site %in% input$selectSiteDatasheets) %&gt;%Â \nÂ  ...\n})\n# Download button\noutput$download_datasheet &lt;- downloadHandler(\nÂ  filename = function() {\nÂ  Â  paste(\"spreadsheet_\", input$selectSiteDatasheets, \".csv\", sep = \"\")\nÂ  },\nÂ  content = function(file) {\nÂ  Â  write.csv(datasheet_df(), file, row.names = FALSE)\nÂ  }\n)\n\nHereÂ datasheet_dfÂ is a reactive variable that Shiny will always keep up to date. Therefore the download button only needs to describe that it uses whatever the current value of that reactive is.\n\nThis separation keeps the code easy to reason about and allows easy use ofÂ datasheet_dfÂ in other contexts than just the download button."
  },
  {
    "objectID": "qmd/shiny-production.html",
    "href": "qmd/shiny-production.html",
    "title": "65Â  Shiny, Production",
    "section": "",
    "text": "TOC\n\nMisc\nDocker\nData Strategies\nOptimization\nDevelopment\nSecurity\n\nMisc\n\n{cicerone} - provides guided tours to your shiny apps\n\nDocker\n\nMisc\n\nImages have a 2GB limit\n\nBase Image Build Times\n\nSmaller images take longer to load because all the packages/libraries have to be compiled\nrstudio/base, rocker/shiny, rocker/r-ubuntu use binary packages\nrocker/r-bspm and eddelbuettel/r2u uses binary packages and apt-get\n\nOrder of Image Layers\n\nOrder is bottom to top when writing your dockerfile. (see example below)\nImportant for the bottom layers to be things that you might change most often. This way you donâ€™t have to reinstall R everytime you change something in your app code.\n\nExample\nMethods for installing dependencies (article)\n\nlittler::install.r (littler is installed on all Rocker images)\nExample: image size comparison\n\n\nREPOSITORYÂ  Â  Â  Â  Â  Â  Â  Â  Â  TAGÂ  Â  Â  Â  Â  Â  Â  Â  SIZE\nanalythium/covidapp-shinyÂ  renvÂ  Â  Â  Â  Â  Â  Â  Â  1.7GB\nanalythium/covidapp-shinyÂ  depsÂ  Â  Â  Â  Â  Â  Â  Â  1.18GB\nanalythium/covidapp-shinyÂ  basicÂ  Â  Â  Â  Â  Â  Â  1.24GB\n\nbasic image (aka â€œexplicitâ€ method above): 105 packages installed\ndeps image (aka â€œDESCRIPTIONâ€ method above): has remotes added on top of these; remotes::install_deps() to install packages from the DESCRIPTION file\nrenv image has remotes, renv and BH as extras.\n\nBH seems to be responsible for the size difference, this package provides Boost C++ header files.\nYour app will probably work perfectly fine without BH.\nYou can use renv::remove(\"BH\") to remove BH from the project or use the â€œcustomâ€ model and list all the packages to be added to the lockfile\n\n{deps} (article)\n\nlightweight method that installs packages via a DESCRIPTION json file\n\nBlend between package and renv approaches to dependency management\n\nProject scripts contain roxygen-like, decorator code about packages and those are used to build the DESCRIPTION json file\nImage size should be similar to the â€œdeps/DESCRIPTIONâ€ method above\n\nSecurity\n\n(taken from above dockerfile example)\nBest to create user groups and not run app as a root sudo\nchown, an abbreviation of change owner, is used on Unix and Unix-like operating systems to change the owner of file system files, directories. Unprivileged users who wish to change the group membership of a file that they own may use chgrp\n\nCI/CD\n\nUse github action for docker caching which builds the image and pushes your image to Docker Hub\nThen your compute instance (PaaS) pulls the image from that registry\n\n\nData strategies\n\nDo as little processing as possible in the app\nOptions\n\nBundle datasets alongside the source code, such that wherever the app is deployed, those datasets are available.\n\nDrawback: data would need to be kept in version control along with your source code, and a new deployment of the app would be required whenever the data is updated.\n\nFor frequently updated datasets, this is impractical, but may be valuable if those datasets are unlikely to change during the lifetime of a project.\n\n\nKeep data in cloud storage\n\nAllows collaborators to upload new data on an ad-hoc basis without touching the app itself. The app would then download data from the cloud for presentation during each user session\nBetter for frequently updated datasets\nOptimization (loading in secs instead of mins)\n\nUse parquet file format\nCaching the data for the appâ€™s landing page or use asynchronous computing to initiate downloading the data while presenting a less data-intensive landing page\n\n\nPartition data:\n\nRaw data that is not computationally expensive or needs no processing\nProcessed data that is more computationally expensive to process.\n\nThe data processing pipeline is outside of the app. (e.g.Â GitHub Actions workflow)\nData storage and app server should be in the same region to reduce latency\n\nCreate Pipeline Triggers\n\nWhen new raw data is uploaded, then data gets processed and into the app in a timely manner.\nWhen the source code for the app or the data processing pipeline change, the data processing pipeline should run afresh.\nIf changes to the structure of the raw dataset mean that the data processing pipeline produces malformed processed data, there should be a way to log that.\n\n\n\n\n\nOptimization\n\nMisc\n\nResources\n\nOffload Shinyâ€™s Workload: COVID-19 processing for the WHO/Europe\n\nOverview of how they reduced loading times of a World Health Organization app from minutes to seconds\n\nShiny docs: caching and async programming\nChapter 15 of Engineering Production-Grade Shiny Apps covers, in detail, some common performance pitfalls and how to solve them.\n\n\nReduce the amount of computation occuring inside the app (by caching plots and tables, or by precomputing your data),\nAnalyze whether the app could be using too much reactivity or regenerating UI elements unnecessarily\n\nDevelopment\n\nAWS deployment\n\nNotes from\n\nCreating a Dashboard Framework with AWS (Part 1)\n\nFeatures\n\nSecure, end-to-end encrypted (SSL, TLS) access to dashboards.\nSecure authentication through E-mail and Single-Sign-On (SSO).\nHorizontal scalability of dashboards according to usage, fail-safe.\nEasy adaptability by analysts through automation and continuous integration (CI/CD).\nEasy maintenance and extensibility for system operators.\n\nComponents\n\nApplication Load Balancer (ALB) to handle secure end-to-end (SSL) encrypted access to the dashboards based on different host names (host-based-routing).\nAWS Cognito for user authentication based on E-mail and SSO through Ping Federate.\nAWS Fargate for horizontal scalability, fail-safe operations and easy maintenance.\nAWS Codepipeline and Codebuild for automated build of dashboard Docker containers.\nExtensive usage of managed services requiring low maintenance (Fargate, Cognito, ALB) and Amazon Cloud Development Kit (CDK) to define and manage infrastructure-as-code managed in Git and deployed via Code Pipelines.\n\n\nAetna Insurance Notes from Q&A Shatrunjai Singh | R in Insurance | RStudio\n\nRStudio Connect\n\nproduction & dev â€œversionâ€ (branches?)\n\nSteps\n\nBuild minimum viable product (MVP)\nUpload to dev branch\nPerform Test & Learn for about a month\n\nGive it to a small number of ppl\nHave them use it and get feedback\n\nCreate â€œdev2â€ app\n\nApply fixes\nAdd most critical elements according to the feedback\n\nGive to a different, slightly larger group of people\n\nLooking for issues, bugs, etc.\n\nOnce satisfied that all major kinks are worked out, move app to production branch\n\nLaunched to entire company\n\nFor the 1st 6 months, â€œrecalibrate analysisâ€ every 1.5 months\nRecalibrate every 6 months afterwards\n\nDo packages need updated?\nDo the models need retrained\nIs the app still working as intended?\nIf itâ€™s all good, do some code optimization, refactoring, etc.\n\n\n\n\nSecurity\n\n{shinyauthr}\n\nfor user authentication. App doesnâ€™t get rendered until user is authenticated\n\n{backendlessr}\n\nremotes::install_gitlab(\"rresults/backendlessr\")\nProvides user registration, login, logout, profiles\n\nSome small amounts of data can be sent to backendless (not log files)\n\nThere are some user counting functions in the package for keeping track of API calls (I think)\n\n\nAPI wrapper for backendless platform\nfree for up to 60 API calls per minute 1M API calls per month\n\nIf you need more, use invite code p6tvk3 when you create a new app to get 25% off for the first 6 months\n\nDemo login: backendlessr::shiny_demo_loginÂ  (http://0.0.0.0:3838)\n\nClicking the â€œRegisterâ€ button calls the backendless API\nDisplays â€œSuccessfulâ€ if user registration worked\n\nSteps\n\nRegister at backendless and get a free account\nRegister you app\n\nGet the Application ID and API key\n\nAdd ID and key to .Renviron (for testing)\n\n\n\nÂ  Â  Â  Â  Â  Â  Â  BACKENDLESS_APPID = \"&lt;app id&gt;\"\nÂ  Â  Â  Â  Â  Â  Â  BACKENDLESS_KEY = \"&lt;api_key&gt;\"\n\nAdd ID and key to Docker Swarm secrets (for production)\nInstall package and run functions in your shiny app\n\nExample (basic)\nApp listens for actionLink open_login which is the user loggin into the app\nThen showElement unhides all the hidden things in the ui (e.g.Â logout, loginmain module, plot) and actionLink log in becomes hidden (I think)\n\nBunch of user credential functions available but hereâ€™s a list that currently arenâ€™t:\n\nSocial Network logins (e.g.Â to allow a user to use their Facebook account to log in to our service);\nAsync calls;\nGeolocation;\nLogging (send log messages of your app to Backendless platform).\nEnhanced Security\nUser password reset"
  },
  {
    "objectID": "qmd/shiny-python.html#sec-shiny-py-misc",
    "href": "qmd/shiny-python.html#sec-shiny-py-misc",
    "title": "Python",
    "section": "Misc",
    "text": "Misc\n\nResources\n\nBizSci Video: basic, plotly, finance app\n\nBuilt off of same technology as {{fastapi}} which is uvicorn"
  },
  {
    "objectID": "qmd/shiny-ui.html",
    "href": "qmd/shiny-ui.html",
    "title": "66Â  Shiny, UI",
    "section": "",
    "text": "TOC\n\nMisc\nDesign Principles\nStyle\nResponsiveness\n\nMisc\n\n\n\nDesign Principles\n\nMisc\n\nNotes from\n\nErik Kennedy thread\n\n\nDe-emphasize Dividing lines\nUse fonts that subtly convey brand\nContent cards should be lighter than their bg (in dark mode too)\nDonâ€™t resize icons\n\nTheir level of detail and stroke weights are meant to work best at a certain size.\nInstead, try adding a border or container around them for some extra visual pop\n\nREMOVE-HIDE-LIGHTEN for cleaner designs\nBe consistent until itâ€™s time not to be consistent\n\nBreak consistency example: when youâ€™re trying to catch the userâ€™s eye\n\nGood imagery\nRemove Congestion See Designing Accessible Research with R/Shiny UI â€“ Part 2 for an example of an iterable workflow\n  Congestion is when an app shows everything, all at once and thatâ€™s stressful for the user. It doesnâ€™t elicit the behavior we want for an engaging, learning experience. \n\n  This stress triggers two subconscious actions â€“ run or freeze. Both lead to cognitive load and negative perception, and ultimately, failed adoption.\n      Running is expressed in bounce rate. A user will enter the app, become frustrated, and leave.\n\n      Freezing means that a user pauses with a delayed time to understand. This can be expressed in a number of ways, but most likely a combination of longer session times with aimless user behavior.\n\nMinimize options, legends, etc. and move to the edges of the app (e.g.Â header, sides)\n\nExample\n\nPrototype\n\nIssues\n\nThe tree selector consumes space on the dashboard\nSelecting via images doesnâ€™t add a lot of value as not all tree species are easily identified by image\nThe UI is too dark\nThe visual accessibility feature was lost as some colors donâ€™t work on a dark background\n\n\nFinal\n\nDashboard\n\nThe legend moved to the card (see mobile for more details\n\nTree card gives details about species and a summary\nClicking upper right icon flips the card to display the bar chart.\n\nSwitching languages â€‹â€‹and information about the application moved to the header (right side of blue nav bar)\nClicking text in header opens pop-up with options. One for type of tree and one for type of scenario (beteen blue nav bar and map)\nAll other interactive elements that were scattered on the screen are now organized into the right vertical panel on the map. Here is also the color blindness option for people with visual disabilities, as the main focus of the application is on the color ratio on the map\n\nMobile\n\nâ€œlanguagesâ€ and â€œaboutâ€ moved to the top\n\n\n\n\n\n\nStyle\n\nMisc\n\nImages and style.css files should go into the â€œwwwâ€ folder\nDancho full page map UI from app in learning lab 28\n\nCode only shown partially in learning lab 83\nControl panel had a transparent background\nClickable logos\n{shiny}, {fresh}, {shinyWidgets}\n\n\n\nmy_theme &lt;- create_theme(\nÂ  Â  theme = \"paper\",\nÂ  Â  bs_vars_global(\nÂ  Â  Â  Â  body_bg = \"black\",\nÂ  Â  Â  Â  text_color = \"#fff\"\nÂ  Â  ),\nÂ  Â  bs_vars_navbar(\nÂ  Â  Â  Â  default_bg = \"#75h8d1\",\nÂ  Â  Â  Â  default_color = \"#ffffff\",\nÂ  Â  Â  Â  default_link_color = \"#ffffff\",\nÂ  Â  Â  Â  default_link_active_color = \"#75b8d1\",\nÂ  Â  Â  Â  default_link_active_bg = \"#ffffff\",\nÂ  Â  Â  Â  default_link_hover_color = \"#2c3e50\"\nÂ  Â  ),\nÂ  Â  bs_vars_dropdown(\nÂ  Â  Â  Â  bg = \"#0006\",\nÂ  Â  ),\nÂ  Â  bs_vars_modal(\nÂ  Â  Â  Â  content_bg = \"#0006\"\nÂ  Â  )\nÂ  Â  bs_vars_wells(\nÂ  Â  Â  Â  bg = \"#75b8d1\nÂ  Â  )\nÂ  Â  bs_vars_input(\nÂ  Â  Â  Â  color = \"#FFF\",\nÂ  Â  Â  Â  color_placeholder = \"bdbdbd\"\nÂ  Â  ),\nÂ  Â  bas_vars_button(\nÂ  Â  Â  Â  default_color = \"black\",\nÂ  Â  Â  Â  primary_bg = \"black\",\nÂ  Â  Â  Â  success_bg = \"#188C9C\",\nÂ  Â  Â  Â  info_bq = \"#A6CEE3\",\nÂ  Â  Â  Â  info_color = \"#2c3e50\",\nÂ  Â  Â  Â  warning_bg = \"#CCBE93\",\nÂ  Â  Â  Â  danger_bg = \"#E31A1C\"\nÂ  Â  )\nÂ  Â  bs_vars_panel(\nÂ  Â  Â  Â  bg = \"#0006\",\nÂ  Â  Â  Â  default_heading_bg = \"#0006\",\nÂ  Â  Â  Â  default_text = \"white\"\nÂ  Â  )\n)\nui &lt;- bootstrapPage(\n\nÂ  Â  tags$style(type = \"text/css\", \"html, body {width: ... not seen\nÂ  Â  tags$head(\nÂ  Â  Â  Â  HTML(\"&lt;style&gt;\nÂ  Â  Â  Â  Â  Â  h1,h2,h3,h4,h5,h6(color:#FFF !important;} .... possibly not seen\nÂ  Â  Â  Â  Â  Â  /form-control{color:#FFF;}\nÂ  Â  Â  Â  Â  Â  .dataTables_filter{color:white;}\nÂ  Â  Â  Â  Â  Â  thead{color:#FFF;}\nÂ  Â  Â  Â  Â  Â  &lt;/style&gt;\")\nÂ  Â  ),\n\nÂ  Â  use_theme(my_theme),\nÂ  Â  leafletOutput(\"map\", width = \"100%\", height = \"100%\" .... not seen\n\nÂ  Â  absolutePanel(\nÂ  Â  Â  Â  id = \"logos\",\nÂ  Â  Â  Â  style = \"z-index:300;bottom:50px;right:50px;\",\nÂ  Â  Â  Â  h2(\"Pharmacy Finder\")\nÂ  Â  ),\n\nÂ  Â  absolutePanel(\nÂ  Â  Â  Â  id = \"business-science\",\nÂ  Â  Â  Â  style = \"z-index:300;bottom:50px;left:50px;\", .... not seen\nÂ  Â  Â  Â  h4(\"Learn Shiny\", class = \"text_primary\"),\nÂ  Â  Â  Â  h5(\nÂ  Â  Â  Â  Â  Â  tags$img(src = \"https://www.business.scien .... not seen\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  style = \"width:48px;-webkit-filte .... not seen\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  \"Business Science\"\nÂ  Â  Â  Â  ) %&gt;%\nÂ  Â  Â  Â  Â  Â  tags$a(class = \"btn btn-primary btn-sm\", .... not seen\nÂ  Â  ),\n\nÂ  Â  absolutePanel(\nÂ  Â  Â  Â  id = \"controls\",\nÂ  Â  Â  Â  style = \"zindex:5000;top:10px;left:50px;\",Â  .... not seen\nÂ  Â  Â  Â  draggable = FALSE,\nÂ  Â  Â  Â  div(\nÂ  Â  Â  Â  Â  Â  class=\"panel panel-default\",\nÂ  Â  Â  Â  Â  Â  style = \"width:300px;\",\nÂ  Â  Â  Â  Â  Â  div(\nÂ  Â  Â  Â  Â  Â  Â  Â  class=\"panel-body\",\nÂ  Â  Â  Â  Â  Â  Â  Â  textInput(\"city\", \"City\", \"Pittsburgh\" .... not seen\nÂ  Â  Â  Â  Â  Â  Â  Â  selectInput(\"amenity\", \"Amenity Type\", .... not seen\nÂ  Â  Â  Â  Â  Â  Â  Â  shiny::actionButton(\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  inputId = \"submit\",\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  label = \"Search\",a\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  class = \"btn-default\"\nÂ  Â  Â  Â  Â  Â  Â  Â  ),\nÂ  Â  Â  Â  Â  Â  Â  Â  downloadButton(\"download_csv\", \"Downlo .... not seen\nÂ  Â  Â  Â  Â  Â  )\nÂ  Â  Â  Â  )\nÂ  Â  )\n)\n\n\nImplementing a css file\n\nExample: basic\n\nmain.css file\n\n\n\n@import url('https://fonts.googleapis.com/css2?family=Poppins&display=swap');\n* {\nÂ  margin: 0;\nÂ  padding: 0;\nÂ  box-sizing: border-box;\nÂ  font-family: 'Poppins', sans-serif;\n}\nbody {\nÂ  padding: 1rem;\n}\n#map {\nÂ  height: 98vh !important;\nÂ  border-radius: 0.5rem !important;\n}\n\napp.R\n\nui &lt;- fluidPage(\nÂ  tags$head(tags$link(rel = \"stylesheet\", type = \"text/css\", href = \"main.css\")),\nÂ  sidebarLayout(\nÂ  Â  ...\nÂ  )\n)\nResponsiveness\n\nResponsiveness isnâ€™t the same as performance. Performance is about completing an operation in the minimal amount of time, while responsiveness is about meeting human needs for feedback when executing an action\nMisc\n\nNotes from Improving the responsiveness of Shiny applications\n\nButton Click Registered\n\n(left) default arrow for your app\n(middle) hand indicates to the user that the app has registered their hovering over a button\n(right) arrow+pie indicates to the user that the app has registered their clicking the button\n(generic) entry into CSS file\n\n\nhtml.shiny-busy .container-fluid {\nÂ  cursor: wait;\n}\n\nLooks like this code is to produce something like the right-side image\nFor touch devices (i.e.Â without cursor), you might want to take a look at {shinycssloaders} and/or {waiter}\nIf the user might have to wait longer than a few seconds for the process theyâ€™ve just set in motion to complete, you should consider a progress indicator.\n\n{shiny} has a progress indicator, while {waiter} also offers a nice built-in-to-button option (below)"
  },
  {
    "objectID": "qmd/shiny-ui-examples.html",
    "href": "qmd/shiny-ui-examples.html",
    "title": "67Â  Shiny, UI Examples",
    "section": "",
    "text": "TOC\nMisc\nMap Apps\n\nDancho full page map UI from app in learning lab 28\n\nCode only shown partially in learning lab 83\n\nserver part also shown\n\nControl panel had a transparent background\nClickable logos\n{shiny}, {fresh}, {shinyWidgets}\n\n\nmy_theme &lt;- create_theme(\nÂ  Â  theme = \"paper\",\nÂ  Â  bs_vars_global(\nÂ  Â  Â  Â  body_bg = \"black\",\nÂ  Â  Â  Â  text_color = \"#fff\"\nÂ  Â  ),\nÂ  Â  bs_vars_navbar(\nÂ  Â  Â  Â  default_bg = \"#75h8d1\",\nÂ  Â  Â  Â  default_color = \"#ffffff\",\nÂ  Â  Â  Â  default_link_color = \"#ffffff\",\nÂ  Â  Â  Â  default_link_active_color = \"#75b8d1\",\nÂ  Â  Â  Â  default_link_active_bg = \"#ffffff\",\nÂ  Â  Â  Â  default_link_hover_color = \"#2c3e50\"\nÂ  Â  ),\nÂ  Â  bs_vars_dropdown(\nÂ  Â  Â  Â  bg = \"#0006\",\nÂ  Â  ),\nÂ  Â  bs_vars_modal(\nÂ  Â  Â  Â  content_bg = \"#0006\"\nÂ  Â  )\nÂ  Â  bs_vars_wells(\nÂ  Â  Â  Â  bg = \"#75b8d1\nÂ  Â  )\nÂ  Â  bs_vars_input(\nÂ  Â  Â  Â  color = \"#FFF\",\nÂ  Â  Â  Â  color_placeholder = \"bdbdbd\"\nÂ  Â  ),\nÂ  Â  bas_vars_button(\nÂ  Â  Â  Â  default_color = \"black\",\nÂ  Â  Â  Â  primary_bg = \"black\",\nÂ  Â  Â  Â  success_bg = \"#188C9C\",\nÂ  Â  Â  Â  info_bq = \"#A6CEE3\",\nÂ  Â  Â  Â  info_color = \"#2c3e50\",\nÂ  Â  Â  Â  warning_bg = \"#CCBE93\",\nÂ  Â  Â  Â  danger_bg = \"#E31A1C\"\nÂ  Â  )\nÂ  Â  bs_vars_panel(\nÂ  Â  Â  Â  bg = \"#0006\",\nÂ  Â  Â  Â  default_heading_bg = \"#0006\",\nÂ  Â  Â  Â  default_text = \"white\"\nÂ  Â  )\n)\nui &lt;- bootstrapPage(\n\nÂ  Â  tags$style(type = \"text/css\", \"html, body {width: ... not seen\nÂ  Â  tags$head(\nÂ  Â  Â  Â  HTML(\"&lt;style&gt;\nÂ  Â  Â  Â  Â  Â  h1,h2,h3,h4,h5,h6(color:#FFF !important;} .... possibly not seen\nÂ  Â  Â  Â  Â  Â  /form-control{color:#FFF;}\nÂ  Â  Â  Â  Â  Â  .dataTables_filter{color:white;}\nÂ  Â  Â  Â  Â  Â  thead{color:#FFF;}\nÂ  Â  Â  Â  Â  Â  &lt;/style&gt;\")\nÂ  Â  ),\n\nÂ  Â  use_theme(my_theme),\nÂ  Â  leafletOutput(\"map\", width = \"100%\", height = \"100%\" .... not seen\n\nÂ  Â  absolutePanel(\nÂ  Â  Â  Â  id = \"logos\",\nÂ  Â  Â  Â  style = \"z-index:300;bottom:50px;right:50px;\",\nÂ  Â  Â  Â  h2(\"Pharmacy Finder\")\nÂ  Â  ),\n\nÂ  Â  absolutePanel(\nÂ  Â  Â  Â  id = \"business-science\",\nÂ  Â  Â  Â  style = \"z-index:300;bottom:50px;left:50px;\", .... not seen\nÂ  Â  Â  Â  h4(\"Learn Shiny\", class = \"text_primary\"),\nÂ  Â  Â  Â  h5(\nÂ  Â  Â  Â  Â  Â  tags$img(src = \"https://www.business.scien .... not seen\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  style = \"width:48px;-webkit-filte .... not seen\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  \"Business Science\"\nÂ  Â  Â  Â  ) %&gt;%\nÂ  Â  Â  Â  Â  Â  tags$a(class = \"btn btn-primary btn-sm\", .... not seen\nÂ  Â  ),\n\nÂ  Â  absolutePanel(\nÂ  Â  Â  Â  id = \"controls\",\nÂ  Â  Â  Â  style = \"zindex:5000;top:10px;left:50px;\",Â  .... not seen\nÂ  Â  Â  Â  draggable = FALSE,\nÂ  Â  Â  Â  div(\nÂ  Â  Â  Â  Â  Â  class=\"panel panel-default\",\nÂ  Â  Â  Â  Â  Â  style = \"width:300px;\",\nÂ  Â  Â  Â  Â  Â  div(\nÂ  Â  Â  Â  Â  Â  Â  Â  class=\"panel-body\",\nÂ  Â  Â  Â  Â  Â  Â  Â  textInput(\"city\", \"City\", \"Pittsburgh\" .... not seen\nÂ  Â  Â  Â  Â  Â  Â  Â  selectInput(\"amenity\", \"Amenity Type\", .... not seen\nÂ  Â  Â  Â  Â  Â  Â  Â  shiny::actionButton(\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  inputId = \"submit\",\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  label = \"Search\",a\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  class = \"btn-default\"\nÂ  Â  Â  Â  Â  Â  Â  Â  ),\nÂ  Â  Â  Â  Â  Â  Â  Â  downloadButton(\"download_csv\", \"Downlo .... not seen\nÂ  Â  Â  Â  Â  Â  )\nÂ  Â  Â  Â  )\nÂ  Â  )\n)"
  },
  {
    "objectID": "qmd/simulation-data.html",
    "href": "qmd/simulation-data.html",
    "title": "68Â  Simulation, Data",
    "section": "",
    "text": "TOC\n\nMisc\nSimStudy\n\nMisc\n\nTodo - for the â€œtrtAssignâ€ mess with ratio and the number of ratios\nAlso see\n\nPandas-Time Series-Simulation\n\nA Gaussian and Standard GARCH time-series thatâ€™s frequently encountered in econometrics {style=â€œcolor: greenâ€} SimStudy\n\n\nMisc\n\n{simstudy}\nReference\n\nAvailable distributions (link)\n\nProbability Distributions\nâ€œnonrandomâ€: For constants; can be a numeric or a string with a formula that defines a dependency on another variable\nâ€œclusterSizeâ€: For variable cluster sizes but a constant total sample size\n\nâ€œformulaâ€: the (fixed) total sample size\nâ€œvarianceâ€: a (non-negative) dispersion measure that represents the variability of size across clusters\n\nIf the dispersion is set to 0, then cluster sizes are constant\n\n\nâ€œtrtAssignâ€: For treatment assignment\n\nâ€œformulaâ€: ratio which is separated by semicolons and number of treatments\n\ne.g.Â 2 values = 2 groups and â€œ1;2â€ says group 2 has twice as many units and group 1\n\nâ€œvarianceâ€: stratification; ratio in formula is used as the stratification ratio (e.g.Â unbalanced treatment groups â†’ unbalanced stratification)\nExample\n\n\n\n\n\ndef &lt;- defData(def, varname = \"rx\", dist = \"trtAssign\",\nÂ  Â  Â  Â  Â  Â  Â  formula = \"1;1;2\", variance = \"male;over65\")\n&gt; count(studytbl, rx)\n# A tibble: 3 Ã— 2\nÂ  Â  rxÂ  Â  n\nÂ  &lt;int&gt; &lt;int&gt;\n1Â  Â  1Â  Â  84\n2Â  Â  2Â  Â  82\n3Â  Â  3Â  164\n&gt; count(studytbl, male, rx)\n# A tibble: 6 Ã— 3\nÂ  maleÂ  Â  rxÂ  Â  n\nÂ  &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1Â  Â  0Â  Â  1Â  Â  40\n2Â  Â  0Â  Â  2Â  Â  39\n3Â  Â  0Â  Â  3Â  Â  78\n4Â  Â  1Â  Â  1Â  Â  44\n5Â  Â  1Â  Â  2Â  Â  43\n6Â  Â  1Â  Â  3Â  Â  86\n&gt; count(studytbl, over65, rx)\n# A tibble: 6 Ã— 3\nÂ  over65Â  Â  rxÂ  Â  n\nÂ  &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1Â  Â  Â  0Â  Â  1Â  Â  66\n2Â  Â  Â  0Â  Â  2Â  Â  65\n3Â  Â  Â  0Â  Â  3Â  130\n4Â  Â  Â  1Â  Â  1Â  Â  18\n5Â  Â  Â  1Â  Â  2Â  Â  17\n6Â  Â  Â  1Â  Â  3Â  Â  34\n\nFunctions\n\ndefData(dtDefs = NULL,Â  varname,Â  formula,Â  variance = 0,Â  dist = \"normal\",Â  link = \"identity\",Â  id = \"id\") - Initially creates a data.table or adds a column to a data.table with instructions about creating a variable\n\nâ€œformulaâ€: numeric constant or string formula for the mean, probability of event (binary), probability of success (binomial), etc.\n\ndefDataAdd(dtDefs = NULL,Â  varname,Â  formula,Â  variance = 0,Â  dist = \"normal\",Â  link = \"identity\") -Â  Creates a variable definition like defData but is used to augment a already generated dataset. Used as input to addColumns which will generate the variable data from the instructions in this object and add it as a column to the already generated dataset.\ngenCluster(dtClust, cLevelVar, numIndsVar, level1ID, allLevel2 = TRUE) - After generating cluster-level data, this function takes the number of clusters and the sizes of each cluster from that data, and does something like expand.grid to generate an individual-level dataset. Also, adds an id variable.\n\nâ€œdtClustâ€ - cluster-level data\nâ€œcLevelVarâ€ - cluster variable from the cluster-level data\nâ€œnumIndsvarâ€ - variable with the number of units per cluster from the cluster-level data\nâ€œlevel1IDâ€ - name you want for your individual-level ID variable\n\n\nVariable Dependence\n\nBinary depends on a Binary\n\nDefinitions\n\n\n\ndef &lt;- defData(varname = \"male\", dist = \"binary\",Â \nÂ  Â  Â  Â  Â  Â  Â  formula = .5 , id=\"cid\")\ndef &lt;- defData(def, varname = \"over65\", dist = \"binary\",Â \nÂ  Â  Â  Â  Â  Â  Â  formula = \"-1.7 + .8*male\", link=\"logit\")\n\nWhatâ€™s happening\n\nmale &lt;- c(1,1,0,1,0,0,0,1,0,1)\nlogits &lt;- -1.7 + 0.8 * male\nprobabilities &lt;- boot::inv.logit(logits)\nover65 &lt;- rbinom(n = 10, size = 1, prob = probabilities)\n\nThe formula in the â€œlogitsâ€™ line defines the relationship between being male and being over 65yrs old.\nMales in this sample will have a higher probability (0.2890505) of being over 65yrs old than females (0.1544653)\nTo sample from a Bernoulli distribution, set â€œsizeâ€ = 1\nâ€œover65â€ is an indicator where each value is determined by a separate probability parameter for a Bernoulli distribution\nClustered with Cluster-Level Random Effect\n\nExample: Fixed Cluster sizes; Balanced\n\nCluster Definitions\n\n\n\nd0 &lt;- defData(varname = \"n\", formula = 20, dist = \"nonrandom\")\nd0 &lt;- defData(d0, varname = \"a\", formula = 0, variance = 0.33)\nd0 &lt;- defData(d0, varname = \"rx\", formula = \"1;1\", dist = \"trtAssign\")\nd1 &lt;- defDataAdd(varname = \"y\", formula = \"18 + 1.6 * rx + a\",Â \nÂ  Â  Â  Â  Â  variance = 16, dist = \"normal\")\n\nâ€œnâ€: sample size for the cluster\n\ndist: â€œnonrandomâ€ and formula: 20 says use a constant for the cluster sizer\n\nâ€œrxâ€: treatment indicator\n\ndist: â€œtrtAssignâ€; formula: â€œ1;1â€ says 2 treatment groups and theyâ€™re balanced\n\nâ€œyâ€: the individual-level outcome is a function of the treatment assignment and the cluster effect, as well as random individual-level variation\nâ€œaâ€: random individual-level variation (i.e.Â random effect)\n\nRandom Effects are sampled from Normal(0, Ïƒ) where the variance is typically estimated in a Mixed Effects model.\n\nGenerate Cluster-Level Data\n\nset.seed(2761)\ndc &lt;- genData(10, d0, \"site\")\ndc\n##Â  Â  siteÂ  nÂ  Â  Â  a rx\n##Â  1:Â  Â  1 20 -0.3548Â  1\n##Â  2:Â  Â  2 20 -1.1232Â  1\n##Â  3:Â  Â  3 20 -0.5963Â  0\n##Â  4:Â  Â  4 20 -0.0503Â  1\n##Â  5:Â  Â  5 20Â  0.0894Â  0\n##Â  6:Â  Â  6 20Â  0.5294Â  1\n##Â  7:Â  Â  7 20Â  1.2302Â  0\n##Â  8:Â  Â  8 20Â  0.9663Â  1\n##Â  9:Â  Â  9 20Â  0.0993Â  0\n## 10:Â  10 20Â  0.6508Â  0\n\nGenerates 10 clusters labelled as â€œsiteâ€ according to the instructions in â€œd0â€\nGenerate Individual Level Data\n\ndd &lt;- genCluster(dc, \"site\", \"n\", \"id\")\ndd &lt;- addColumns(d1, dd)\ndd\n##Â  Â  Â  siteÂ  nÂ  Â  Â  a rxÂ  idÂ  Â  y\n##Â  1:Â  Â  1 20 -0.355Â  1Â  1 17.7\n##Â  2:Â  Â  1 20 -0.355Â  1Â  2 16.2\n##Â  3:Â  Â  1 20 -0.355Â  1Â  3 19.2\n##Â  4:Â  Â  1 20 -0.355Â  1Â  4 20.6\n##Â  5:Â  Â  1 20 -0.355Â  1Â  5 14.7\n##Â  ---Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â \n## 196:Â  10 20Â  0.651Â  0 196 25.3\n## 197:Â  10 20Â  0.651Â  0 197 22.1\n## 198:Â  10 20Â  0.651Â  0 198 13.2\n## 199:Â  10 20Â  0.651Â  0 199 15.6\n## 200:Â  10 20Â  0.651Â  0 200 13.8\n\ngenCluster performs an expand.grid to generate an individual-level dataset along with adding an ID variable\naddColumns uses individual-level data and outcome variable definition to generate the outcome variable and add it to the dataset.\nExample: Varying Cluster Sizes and therefore Varying Sample Size\n\nd0 &lt;- defData(varname = \"n\", formula = 20, dist = \"poisson\")\ngenData(10, d0, \"site\")\n##Â  Â  siteÂ  n\n##Â  1:Â  Â  1 13\n##Â  2:Â  Â  2 18\n##Â  3:Â  Â  3 21\n##Â  4:Â  Â  4 26\n##Â  5:Â  Â  5 25\n##Â  6:Â  Â  6 27\n##Â  7:Â  Â  7 23\n##Â  8:Â  Â  8 30\n##Â  9:Â  Â  9 23\n## 10:Â  10 20\n\nFormula sets the poisson distribution parameter, Î» = 20. So sizes are sampled from poisson distribution with that mean/variance\nTo increase the variability between clusters, use the negative binomial distribution\nMost likely leads to an unbalanced design\nExample: Varying Cluster Sizes but Constant Sample Size\n\n# moderately varying cluster sizes\nd0 &lt;- defData(varname = \"n\", formula = 200, variance = 0.2, dist = \"clusterSize\")\ngenData(10, d0, \"site\")\n\n##Â  Â  siteÂ  n\n##Â  1:Â  Â  1 20\n##Â  2:Â  Â  2 28\n##Â  3:Â  Â  3 25\n##Â  4:Â  Â  4 24\n##Â  5:Â  Â  5 28\n##Â  6:Â  Â  6 22\n##Â  7:Â  Â  7Â  7\n##Â  8:Â  Â  8 13\n##Â  9:Â  Â  9 22\n## 10:Â  10 11\n\n# Very highly varying cluster sizes\nd0 &lt;- defData(varname = \"n\", formula = 200, variance = 5, dist = \"clusterSize\")\ngenData(10, d0, \"site\")\n##Â  Â  siteÂ  n\n##Â  1:Â  Â  1Â  10\n##Â  2:Â  Â  2Â  2\n##Â  3:Â  Â  3Â  17\n##Â  4:Â  Â  4Â  2\n##Â  5:Â  Â  5Â  49\n##Â  6:Â  Â  6 110\n##Â  7:Â  Â  7Â  1\n##Â  8:Â  Â  8Â  4\n##Â  9:Â  Â  9Â  1\n## 10:Â  10Â  4\n\nTotal sample size is fixed at 200 (formula), but individual cluster sizes are allowed to vary.\nâ€œvarianceâ€ is a dispersion parameter that controls the amount of varying of the cluster sizes"
  },
  {
    "objectID": "qmd/spreadsheets.html#misc",
    "href": "qmd/spreadsheets.html#misc",
    "title": "Spreadsheets",
    "section": "Misc",
    "text": "Misc\n\nSome Excel files are binaries and in order to use download.file, you must set mode = â€œwbâ€\ndownload.file(url, \n              destfile = glue(\"{rprojroot::find_rstudio_root_file()}/data/cases-age.xlsx\"), \n              mode = \"wb\")"
  },
  {
    "objectID": "qmd/spreadsheets.html#sec-spdsht-bprac",
    "href": "qmd/spreadsheets.html#sec-spdsht-bprac",
    "title": "Spreadsheets",
    "section": "Best Practices",
    "text": "Best Practices\n\nNotes from Data organization in spreadsheets\n\nBe consistent\nWrite dates like YYYY-MM-DD\nDonâ€™t leave any cells empty\nPut just one thing in a cell\nOrganize the data as a single rectangle (with subjects as rows and variables as columns, and with a single header row)\nCreate a data dictionary\nDonâ€™t include calculations in the raw data files\nDonâ€™t use font color or highlighting as data\nChoose good names for things\nMake backups\nUse data validation to avoid data entry errors\nSave the data in plain text files."
  },
  {
    "objectID": "qmd/sql.html#sec-sql-misc",
    "href": "qmd/sql.html#sec-sql-misc",
    "title": "SQL",
    "section": "Misc",
    "text": "Misc\n\nResources\n\nPublicly Available SQL Databases: Need to email administrator to gain access\n\ndplyr::show_query can convert a dplyr expression to SQL for db object (e.g.Â dbplyr,Â  duckdb, arrow)\nQueries in examples\n\nWindow Functions\n\nAverage Salary by Job Title\nAverage Unit Price for each CustomerId\nRank customers by amount spent\nCreate a new column that ranks Unit Price in descending order for each CustomerId\nCreate a new column that provides the previous order dateâ€™s Quantity for each ProductId\nCreate a new column that provides the very first Quantity ever ordered for each ProductId\nCalculate a cumulative moving average UnitPrice for each CustomerId\nRank customers for each department by amount spent\nFind the model and year of the car that been on the lot the longest\nCreate a subset (CTE)\n\nCalculate a running monthly total (aka cumsum)\n\nAlso running average\n\nCalculate a running monthly total for each account id\nCalculate a 3 months rolling running total using a window that includes the current month.\nCalculate a 7 months rolling running total using a window where the current month is always the middle month\nCalculate the number of consecutive days spent in each country\n\n\nCTE\n\nAverage monthly cost per campaign for the companyâ€™s marketing efforts\nCount the number of interactions of new users\nThe average top Math test score for students in California\n\nBusiness Queries\n\n7-day Simple Moving Average (SMA)\nRank product categories by shipping cost for each shipping address\nDaily counts of open jobs (where â€œopenâ€ is an untracked daily status)\nGet the latest order from each customer\nOverall median price\nMedian price for each product\nOverall median price and quantity\n\nProcessing Expressions\n\nProvide subtotals for a hierarchical group of fields (e.g.Â family, category, subcategory)\n\nSee NULLs &gt;&gt; COALESCE\n\n\n\nOrder of Operations\n\n\nHigher ranked functions can be inserted inside lower ranked functions\n\ne.g a window function can be inside a SELECT function but not inside a WHERE clause\nThere are exceptions and hacks around this in some cases\n\n\nTypes of Commands\n\nData Query Language (DQL) - used to find and view data without making any permanent changes to the database.\nData Manipulation Language (DML) - used to make permanent changes to the data, such as updating values or deleting them.\nData Definition Language (DDL) - used to make permanent changes to the table, such as creating or deleting a table.\nData Control Language (DCL) - used for administrative commands, such as adding or removing users of different tables and databases.\nTransact Control Language (TCL) - advanced SQL that deals with transaction level statements.\n\nMicrosoft SQL Server format for referencing a table:\n\n[database].[schema].[tablename]\nAlternative\nUSE my_data_base\nGO\n\nCheck if a table is updatable\nSELECT table_name, is_updatable\nFROM information_schema.views\n\nUseful if some of the tables you are working with are missing values that you need to add\nIf not updatable, then youâ€™ll need to contact the database administrator to request permission to update that specific table\nShow all tables\nSHOW FULL TABLES -- mysql"
  },
  {
    "objectID": "qmd/sql.html#sec-sql-setup",
    "href": "qmd/sql.html#sec-sql-setup",
    "title": "SQL",
    "section": "Set-up",
    "text": "Set-up\n\npostgres\n\nDownload postgres\npgAdmin is an IDE commonly used with postgres\n\nOpen pgAdmin and click on â€œAdd new server.â€\n\nSets up connection to existing server so make sure postgres is installed beforehand\n\nCreate Tables\n\nhome &gt;&gt; Databases (1) &gt;&gt; postgres &gt;&gt; Query Tool\n\nIf needed, give permission to pgAdmin to access data from a folder\n\nMight be necessary to upload csv files\n\nImport csv file\n\nright-click the table name &gt;&gt; Import/Export\nOptions tab\n\nSelect import, add file path to File Name, choose csv for format, select Yes for Header, add , for Delimiter\n\nColumns tab\n\nuncheck columns not in the csv (probably the primary key)\n\nWonder if NULLs will be automatically inserted for columns in the table that arenâ€™t in the file."
  },
  {
    "objectID": "qmd/sql.html#sec-sql-terms",
    "href": "qmd/sql.html#sec-sql-terms",
    "title": "SQL",
    "section": "Terms",
    "text": "Terms\n\nBatch - a set of sql statements e.g.Â statements between BEGIN and END\nCompiled object - you can create a function written in C/C++ and load into the database (at least in postgres) to achieve high performance.\nCorrelated Columns - tells how good the match between logical and physical ordering is.\nCorrelated/Uncorrelated Subqueries\n\ncorrelated- a type of query, where inner query depends upon the outcome of the outer query in order to perform its execution\n\nA correlated subquery can be thought of as a filter on the table that it refers to, as if the subquery were evaluated on each row of the table in the outer query\n\n\nuncorrelated - a type of sub-query where inner query doesnâ€™t depend upon the outer query for its execution.\n\nIt is an independent query, the results of which are returned to and used by the outer query once (not per row).\n\n-- Uncorrelated subquery:\n-- inner query, c1, only depends on table2\nselect c1, c2\nÂ  from table1 where c1 = (select max(x) from table2);\n\n-- Correlated subquery:\n-- inner query, c1, depends on table1 and table2\nselect c1, c2\nÂ  from table1 where c1 = (select x from table2 where y = table1.c2);\n\nFunctions execute at a different level of priority and are handled differently than Views. You will likely see better performance.\nIndex - a quick lookup table (e.g.Â field or set of fields) for finding records users need to search frequently. An index is small, fast, and optimized for quick lookups. It is very useful for connecting the relational tables and searching large tables. (also see DB, Engineering &gt;&gt; Cost Optimizations)\nMigrations (schema) - version control system for your database schema. Management of incremental, reversible changes and version control to relational database schemas. A schema migration is performed on a database whenever it is necessary to update or revert that databaseâ€™s schema to some newer or older version.\nPhysical Ordering - A PostgreSQL table consists of one or more files of 8KB blocks (or â€œpagesâ€). The order in which the rows are stored in the file is the physical ordering.\nPredicate - defines a logical condition being applied to rows in a table. (e.g.Â IN, EXISTS, BETWEEEN, LIKE, ALL, ANY)\nScalar/Non-Scalar Subqueries\n\nA scalar subquery returns a single value (one column of one row). If no rows qualify to be returned, the subquery returns NULL.\nA non-scalar subquery returns 0, 1, or multiple rows, each of which may contain 1 or multiple columns. For each column, if there is no value to return, the subquery returns NULL. If no rows qualify to be returned, the subquery returns 0 rows (not NULLs).\n\nSelectivity - the fraction of rows in a table or partition that is chosen by the predicate\n\nRefers to the quality of a filter in its ability to reduce the number of rows that will need to be examined and ultimately returned\n\nWith a high selectivity, using the primary key or indexes to get right to the rows of interest\nWith a low selectivity, a full table scan would likely be needed to get the rows of interest.\n\nHigher selectivity means: more unique data; fewer duplicates; fewer number of rows for each key value\nUsed to estimate the cost of a particular access method; it is also used to determine the optimal join order. A poor choice of join order by the optimizer could result in a very expensive execution plan.\n\nSoft-deleted - An operation in which a flag is used to mark data as unusable, without erasing the data itself from the database\nSurrogate key - very similar to a primary key in that it is a unique value for an object in a table. However, rather than being derived from actual data present in the table, it is a field generated by the object itself. It has no business value like a primary key does, but is rather only used for data analysis purposes. Can be generated using different columns that already exist in your table or more often from two or more tables. dbt function definition\n\nExamples: PostgreSQL serial column, Oracle sequence column, or MySQL auto_increment column, Snowflake _file + _line columns\nExample: Each employee id is concatenated with a department id (e.g.Â marketing or finance)\n\n\nTransaction - a set of queries tied together such that if one query fails, the entire set of queries are rolled back to a pre-query state if the situation dictates.\n\nA database transaction, by definition, must be atomic, consistent, isolated and durable. These are popularly known as ACID properties.Â  These properties can ensure the concurrent execution of multiple transactions without conflict.\n\nViews - database objects that represent saved SELECT queries in â€œvirtualâ€ tables.\n\nContains a query plan. Â For each query executed, the planner has to evaluate whatâ€™s being asked and calculate an optimal path. Views already have this plan calculated so it allows subsequent queries to be returned with almost no friction in processing aside from data retrieval.\nSome views are updateable but under certain conditions (1-1 mapping of rows in view to underlying table, no group_by, etc.)"
  },
  {
    "objectID": "qmd/sql.html#sec-sql-basics",
    "href": "qmd/sql.html#sec-sql-basics",
    "title": "SQL",
    "section": "Basics",
    "text": "Basics\n\nCreate Tables\n\nIf you donâ€™t include the schema as part of the table name (e.g.Â schema_name.table_name), pgadmin automatically places it into the â€œpublicâ€ schema directory\nField Syntax: name, data type, constraints\nExample: Create table as select (CTAS)\nCREATE TABLE new_table ASÂ \nSELECT *Â \nFROM old_tableÂ \nWHERE condition;\nExample: Table 1 (w/primary key)\nDROP TABLE IF EXISTS classrooms CASCADE;\nCREATE TABLE classrooms (\nÂ  Â  id INT PRIMARY KEY GENERATED ALWAYS AS IDENTITY,\nÂ  Â  teacher VARCHAR(100)\nÂ  Â  );\n -- OR\nCREATE TABLE classrooms (Â \nÂ  Â  id INT GENERATED ALWAYS AS IDENTITY,Â \nÂ  Â  teacher VARCHAR(100)\nÂ  Â  PRIMARY KEY(id)Â \nÂ  Â  );Â  Â \n\nâ€œclassroomsâ€ is the name of the table; â€œidâ€ and â€œteacherâ€ are the fields\nCASCADE - postgres wonâ€™t delete the table if other tables point to it, so cascade will override measure.\nGENERATED ALWAYS AS IDENTITY - makes it so you donâ€™t have to keep track of which â€œidâ€ values have been used when adding rows. You can ommit the value for â€œidâ€ and just add the values for the other fields\nSee tutorial for options, usage, removing, adding, etc. this constraint\nINSERT INTO classrooms\nÂ  Â  (teacher)\nVALUES\nÂ  Â  ('Mary'),\nÂ  Â  ('Jonah');\n\nAlso see Add Data &gt;&gt; Example: chatGPT\n\n\nExample: Table 2 (w/foreign key)\nDROP TABLE IF EXISTS students CASCADE;\nCREATE TABLE students (\nÂ  Â  id INT PRIMARY KEY GENERATED ALWAYS AS IDENTITY,\nÂ  Â  name VARCHAR(100),\nÂ  Â  classroom_id INT,\nÂ  Â  CONSTRAINT fk_classrooms\n    Â  Â  FOREIGN KEY(classroom_id)\nÂ  Â  Â  Â  REFERENCES classrooms(id)\n);\n\nâ€œstudentsâ€ is the name of the table; â€œidâ€, â€œnameâ€, and â€œclassroom_idâ€ are the fields\nCreate a foreign key that points to the â€œclassroomsâ€ table\n\nExpression\n\nfk_classrooms is the name of the CONSTRAINT\nâ€œclassroom_idâ€ is the field that will be the FOREIGN KEY\nREFERENCES points the foreign key to the classrooms tableâ€™s primary key, â€œidâ€\n\nforeign keys can point to any table\n\n\nPostgres wonâ€™t allow you to insert a row into students with a â€œclassroom_idâ€ that doesnâ€™t exist in the â€œidâ€ field of classrooms but will allow you to use a NULL placeholder\n-- Explicitly specify NULL\nINSERT INTO students\nÂ  Â  (name, classroom_id)\nVALUES\nÂ  Â  ('Dina', NULL);Â \n\n-- Implicitly specify NULL\nINSERT INTO students\nÂ  Â  (name)\nVALUES\nÂ  Â  ('Evan');\n\nAlso see Add Data &gt;&gt; Example: chatGPT\n\n\n\nExample\nCREATE TABLE members (\nÂ  Â  id serial primary key,\nÂ  Â  second_name character varying(200) NOT NULL,\nÂ  Â  date_joined date NOT NULL DEFAULT current_date,\nÂ  Â  member_id integer references members(id),\nÂ  Â  booking_start_time timestamp without timezone NOT NULL\n\nThe â€œserialâ€ data type does the same thing as GENERATED ALWAYS AS IDENTITY (see first example), but is NOT compliant with the SQL standard. Use GENERATED ALWAYS AS IDENTITY\nâ€œreferencesâ€ seems to be another old way to create foreign keys (see 2nd example for proper way)\nâ€œcharacter varyingâ€ - variable-length with limit (e.g limit of 200 characters)\n\ncharacter(n), char(n) are for fixed character lengths; text is for unlimited character lengths\n\nâ€œcurrent_dateâ€ is a function that will insert the current date as a value\nâ€œtimestamp without timezoneâ€ is literally that\n\nalso available: time with/without timezone, date, interval (see Docs for details)\n\n\nExample: MySQL\nCREATE DATABASE products;\n\nCREATE TABLE `products`.`prices` (\n  `pid` int(11) NOT NULL AUTO_INCREMENT,\n  `category` varchar(100) NOT NULL,\n  `price` float NOT NULL,\n  PRIMARY KEY (`pid`)\n);\n\nINSERT INTO products.prices\n    (pid, category, price)\nVALUES\n    (1, 'A', 2),\n    (2, 'A', 1),\n    (3, 'A', 5),\n    (4, 'A', 4),\n    (5, 'A', 3),\n    (6, 'B', 6),\n    (7, 'B', 4),\n    (8, 'B', 3),\n    (9, 'B', 5),\n    (10, 'B', 2),\n    (11, 'B', 1)\n;\n\n\n\nAdd Data\n\nExample: Copy/Paste table values into chatGPT to get the query\n\nExample: Add data via .csv\nCOPY assignments(category, name, due_date, weight)\nFROM 'C:/Users/mgsosna/Desktop/db_data/assignments.csv'\nDELIMITER ','\nCSV HEADER;\n\nâ€œassignmentsâ€ is the table; â€œcategoryâ€, â€œnameâ€, â€œdue_dateâ€, â€œweightâ€ are fields that you want to import from the csv file\n** The order of the columns must be the same as the ones in the CSV file **\nHEADER keyword to indicate that the CSV file contains a header\nMight need to have superuser access in order to execute the COPY statement successfully\n\n\n\n\nSubqueries\n\n**Using CTEs instead of subqueries make code more readable**\n\nSubqueries make it difficult to understand their context in the larger query\nThe only way to debug a subquery is by turning it into a CTE or pulling it out of the query entirely.\nCTEs and subqueries have a similar runtime, but subqueries make your code more complex for no reason.\n\nNotes from How to Use SubQueries in SQL\n\nAlso shows the alt method of creating a temporary table to compute the queries\n\nUse cases\n\nFiltering rows from a table with the context of another.\nPerforming double-layer aggregations such as average of averages or an average of sums.\nAccessing aggregations with a subquery.\n\nTables used in examples\n\nStore A (store_a)\n\n\nStore B is similar\n\n\nExample: Filtering rows\nselect * \nfrom sandbox.store_b\nwhere product_id IN (\nÂ  Â  select product_id\nÂ  Â  from sandbox.store_b\nÂ  Â  group by product_idÂ \nÂ  Â  having count(product_id) &gt;= 3\n);\n\nfilters the rows with products that have been bought at least three times in store_b\n\nExample: Multi-Layer Aggregation\nselect avg(average_price.total_value) as average_transaction from (\nÂ  select transaction_id, sum(price_paid) as total_value\nÂ  from sandbox.store_a\nÂ  group by transaction_id\nÂ  ) as average_price\n;\n\ncomputes the average of all transactions\ncanâ€™t apply an average directly, as our table is oriented to product_ids and not to transaction_ids\n\nExample: Filtering the table based on an Aggregation\nselect @avg_transaction:= avg(agg_table.total_value)\nfrom (\n  select transaction_id, sum(price_paid) as total_value\n  from sandbox.store_a\n  group by transaction_id\n) as agg_table;\n\nselect *Â \nfrom sandbox.store_a\nwhere transaction_id in (\n  select transaction_id\n  from sandbox.store_a\n  group by transaction_id\nÂ  having sum(price_paid) &gt; @avg_transaction\n)\n\nfilters transactions that have a value higher than the average (where the output must retain the original product-oriented row)\n\n\n\n\nJoins\n\n\n\nCross Join -Â  acts like an expand_grid; where each value in the join key column gets all combinations of rows in both tables (also see above pic)\n\n\nEfficient join\nWhen you add the where clause, the cross join acts similarly to an inner join, except you arenâ€™t joining it on any specified column\nExample:\nSELECT\nÂ  schedule.event,\nÂ  calendar.number_of_days\nFROM schedule\nCROSS JOIN calendar\nWHERE schedule.total_number &lt; calendar.number_of_days\n\nOnly join the row in the â€œscheduleâ€ table with the rows in the â€œcalendarâ€ table that meet the specified condition\n\n\nNatural Join - donâ€™t need to specify join columns; need to have two columns in each table with the same name\n\nUse cases\n\nThere are a lot of common columns with the same name across multiple tables\n\nThey will all be used as joining keys.\n\nYou donâ€™t want to type out all of the common columns in select just to avoid outputting the same columns multiple times.\n\n\nselect *\nfrom table_a\nnatural join table_b\n;\n\n-- natural + outer\nselect *\nfrom table_a\nnatural outer join table_b\n;"
  },
  {
    "objectID": "qmd/sql.html#sec-sql-r",
    "href": "qmd/sql.html#sec-sql-r",
    "title": "SQL",
    "section": "R",
    "text": "R\n\nGet query from dplyr code\ntbl_to_sql &lt;- function(tbl) {\nÂ  dplyr::show_query(tbl) |&gt;Â \nÂ  Â  capture.output() |&gt;Â \nÂ  Â  purrr::discard_at(1) |&gt;Â \nÂ  Â  paste(collapse = \" \")\n}\n\nTransforms query into a string\nAlso see Generating SQL with {dbplyr} and sqlfluff\n\nConnect to or Create a SQLite database\ncon &lt;- DBI::dbConnect(drv = RSQLite::SQLite(),\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  here::here(\"db_name.db\"),\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  timeout = 10)\nConnect to Microsoft SQL Server\ncon &lt;- DBI::dbConnect(odbc::odbc(),Â \nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Driver = \"SQL Server\",Â \nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Server = \"SERVER\",Â \nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Database = \"DB_NAME\",Â \nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Trusted_Connection = \"True\",Â \nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Port = 1433)\nClose connection: dbDisconnect(con)\nCreate a table from a data source: df &lt;- dbplyr::tbl(con, \"&lt;table name&gt;\")\n\nAllows you to use dplyr verbs with a remote database table then collect\n\nCancel a running query (postgres)\n# Store PID\npid &lt;- DBI::dbGetInfo(conn)$pid\n\n# Cancel query and get control of IDE back\n# SQL command\nSELECT pg_cancel_backend(&lt;PID&gt;)\n\nUseful if query is running too long and you want control of your IDE back\n\nCreate single tables from a list of tibbles to a database\npurrr::map2(table_names, list_of_tbls, ~ dbWriteTable(con, .x, .y))\nLoad all tables from a database into a list\ntables &lt;- dbListTables(con)Â \nall_data &lt;- map(tables, dbReadTable, conn = con)\nCan use map_dfr if all the tables have the same columns\nDynamic queries with {glue}\n\nExample: MS SQL Server\nvars &lt;- c(\"columns\", \"you\", \"want\", \"to\", \"select\")\ndate_var &lt;- 'date_col'\nstart_date &lt;- as.Date('2022-01-01')\ntoday &lt;- Sys.Date()\ntablename &lt;- \"yourtablename\"\nschema_name &lt;- \"yourschema\"\nquery &lt;- glue_sql(.con = con, \"SELECT TOP(10) {`vars`*} FROM {`schema_name`}.{`tablename`} \")\nDBI::dbGetQuery(con, query)\n\nvars format collapses the vars vector, separated by commas, so that it resembles a SELECT statement"
  },
  {
    "objectID": "qmd/sql.html#sec-sql-bestp",
    "href": "qmd/sql.html#sec-sql-bestp",
    "title": "SQL",
    "section": "Best Practices",
    "text": "Best Practices\n\nMisc\n\nResources\n\nSQL Style Guide\n\nUse aliases only when table names are long enough so that using them improves readability (but choose meaningful aliases)\nDo not useÂ SELECT *. Explicitly list columns instead\nUse comments to document business logic\nA comment at the top should provide a high-level description\nUse an auto-formatter\nGeneral Optimizations\n\nRemoving duplicates or filtering out null values at the beginning of your model will speed up queries\nReplace complex code with window functions\n\nExample: Replace GROUP_BY + TOP with a partition + FIRST_VALUE()\nFIRST_VALUE(test_score) OVER(PARTITION BY student_name ORDER BY test_score DESC)\nExample: AVG(test_score) OVER(PARTITION BY student_name)\n\n\n\nCTEs\n\nBreak down logic in CTEs usingÂ WITH â€¦ AS\nTheÂ SELECTÂ statement inside each CTE must do a single thing (join, filter or aggregate)\nThe CTE name should provide a high-level explanation\nThe last statement should be aÂ SELECTÂ statement querying the last CTE\n\nJoins\n\nUseÂ WHEREÂ for filtering, not for joining\nFavorÂ LEFT JOINÂ overÂ INNER JOIN; in most cases, itâ€™s essential to know the distribution ofÂ NULLs\nAvoid usingâ€Self-Joins.â€ Use window functions instead (see Google, BigQuery &gt;&gt; Optimization for details on self-joins)\nWhen doing equijoins (i.e., joins where all conditions have theÂ something=anotherÂ form), use theÂ USINGÂ keyword\nBreak-up joins using OR into UNION because SQL uses nested operations for JOIN + OR queries which slow things.\n\nBad\n\nGood\n\nUNION simply joins the outputs of two separate SELECT statements and retains only one occurrence of duplicated rows if there are any.\n\n\nStyle Guide"
  },
  {
    "objectID": "qmd/sql.html#sec-sql-index",
    "href": "qmd/sql.html#sec-sql-index",
    "title": "SQL",
    "section": "Indexes",
    "text": "Indexes\n\nMisc\n\nAn index may consist of up to 16 columns\nThe first column of the index must always be present in the queryâ€™s filter, order , join or group operations to be used\n\nCreate Index on an existing table (postgres)\nCREATE INDEX\nÂ  Â  score_index ON grades(score, student);\n\nâ€œscore_indexâ€ is the name of the index\nâ€œgradesâ€ is the name of the table\nâ€œscoreâ€ and â€œstudentâ€ are fields to be used as the indexes\n\nCreate Index that only uses a specific character length\n/* mysql */\nCREATE TABLE test (blob_col BLOB, INDEX(blob_col(10)));\n\nindex only uses the first 10 characters of the column value of a BLOB column type\n\nCreate index with multiple columns\n/* mysql */\nCREATE TABLE test (\nÂ  Â  idÂ  Â  Â  Â  INT NOT NULL,\nÂ  Â  last_nameÂ  CHAR(30) NOT NULL,\nÂ  Â  first_name CHAR(30) NOT NULL,\nÂ  Â  PRIMARY KEY (id),\nÂ  Â  INDEX name (last_name,first_name)\n)\nUsage of multiple column index (** order of columns is important **)\nSELECT * FROM test WHERE last_name='Jones';\nSELECT * FROM test\nÂ  WHERE last_name='Jones' AND first_name='John';\nSELECT * FROM test\nÂ  WHERE last_name='Jones'\nÂ  AND (first_name='John' OR first_name='Jon');\nSELECT * FROM test\nÂ  WHERE last_name='Jones'\nÂ  AND first_name &gt;='M' AND first_name &lt; 'N';\n\nIndex is used when both columns are used as part of filtering criteria or when only the left-most column is used\nif you have a three-column index on (col1, col2, col3), you have indexed search capabilities on (col1), (col1, col2), and (col1, col2, col3).\n\nInvalid usage of multiple column index\nSELECT * FROM test WHERE first_name='John';\nSELECT * FROM test\nÂ  WHERE last_name='Jones' OR first_name='John';\n\nThe â€œnameâ€ index wonâ€™t be used in these queries since\n\nfirst_name is NOT the left-most column specified in the index\nOR is used instead of AND\n\n\nCreate index with DESC, ASC\n/* mysql */\nCREATE TABLE t (\nÂ  c1 INT, c2 INT,\nÂ  INDEX idx1 (c1 ASC, c2 ASC),\nÂ  INDEX idx2 (c1 ASC, c2 DESC),\nÂ  INDEX idx3 (c1 DESC, c2 ASC),\nÂ  INDEX idx4 (c1 DESC, c2 DESC)\n);\n\nUsed by ORDER BY\n\nSee Docs to see what operations and index types support Descending Indexes\n\nNote: idx_a on column_p, column_q desc is not the same as an * Index idx_a on column_q desc, column p or, * Index idx_b on column_p desc, column q\n\nUsage of Descending Indexes\nORDER BY c1 ASC, c2 ASCÂ  Â  -- optimizer can use idx1\nORDER BY c1 DESC, c2 DESCÂ  -- optimizer can use idx4\nORDER BY c1 ASC, c2 DESCÂ  -- optimizer can use idx2\nORDER BY c1 DESC, c2 ASCÂ  -- optimizer can use idx3\n\nSee previous example for definition of idx* names\nSee Docs to see what operations and index types support Descending Indexes"
  },
  {
    "objectID": "qmd/sql.html#sec-sql-part",
    "href": "qmd/sql.html#sec-sql-part",
    "title": "SQL",
    "section": "Partitioning",
    "text": "Partitioning\n\nMisc\n\nAlso see\n\nMySQL Docs\nGoogle, BigQuery &gt;&gt; Optimization &gt;&gt; Partitioning and Clustering\nDB, Engineering &gt;&gt; Cost Optimization &gt;&gt; Partitioning\n\nall of your queries to the partitioned table must contain the partition_key in the WHERE clause"
  },
  {
    "objectID": "qmd/sql.html#sec-sql-views",
    "href": "qmd/sql.html#sec-sql-views",
    "title": "SQL",
    "section": "Views",
    "text": "Views\n\nA smaller data object that contains the subset of data resulting from a specific query\nWhereas a query happens after data is loaded, a materialized view is a precomputation\nThe computation is done once, and changes to the data are incorporated as they occur, making subsequent updates to the view much cheaper and more efficient than querying the entire database from scratch.\nCreate a View\n\nExample: Create view as select (CVAS)\nCREATE VIEW high_earner ASÂ \nSELECT p.id AS person_id, j.salary\nFROM People p\nJOIN Job jÂ \nON p.job = j.title\nWHERE j.salary &gt;= 200000;\n\nQuery a view (same as a table): SELECT * FROM high_earner\nUpdate view\nCREATE OR REPLACE VIEW high_earner ASÂ \nSELECT p.id AS person_id, j.salary\nFROM People p\nJOIN Job jÂ \nON p.job = j.title\nWHERE j.salary &gt;= 150000;\n\nExpects the query output to retain the same number of columns, column names, and column data types. Thus, any modification that results in a change in the data structure will raise an error.\n\nList views\n\nSELECT *Â \nFROM information_schema.views\nWHERE table_schema NOT IN ('pg_catalog', 'information_schema');\n\nâ€œtable_nameâ€ has the names of the views\nâ€œview_definitionâ€ shows the query stored in the view\nWHERE command is included to omit built-in views from PostgreSQL.\n\nDelete view: DROP VIEW high_earner;"
  },
  {
    "objectID": "qmd/sql.html#sec-sql-vars",
    "href": "qmd/sql.html#sec-sql-vars",
    "title": "SQL",
    "section": "Variables",
    "text": "Variables\n\nMisc\n\nAlso see Business Queries &gt;&gt; Medians\n\nUser-defined\n\nDECLARE and SET\n-- Declare your variables\nDECLARE @start date\nDECLARE @stop date\n--â€ŠSET the relevant values for each variable\nSET @start = '2021-06-01'\nSET @stop = GETDATE()\n\nDECLARE sets the variable type (e.g.Â date)\nSET assigns a value\n\nOr just use DECLARE\nDECLARE @Iteration Integer = 0;\nExamples\n\nExample: Exclude 3 months of data from the query\nSELECT t1.[DATETIME], COUNT(*) AS vol\nFROM Medium.dbo.Earthquakes t1\nWHERE t1.[DATETIME] BETWEEN @start AND DATEADD(MONTH, -3, @stop)\nGROUP BY t1.[DATETIME]\nORDER BY t1.[DATETIME] DESC;\n\nSee above for the definitions of @start and @stop\n\nExample: Apply a counter\n-- Declare the variable (a SQL Command, the var name, the datatype)\nDECLARE @counter INT;\n--â€ŠSet the counter to 20\nSET @counter = 20;\n-- Print the initial value\nSELECT @counter AS _COUNT;\n--â€ŠSelect and increment the counter by one\nSELECT @counter = @counter + 1;\n-- Print variable\nSELECT @counter AS _COUNT;\n--â€ŠSelect and increment the counter by one\nSELECT @counter += 1;\n--â€ŠPrint the variable\nSELECT @counter AS _COUNT;\n\n\nSystem\n\nROWCOUNT - returns the number of rows affected by the last previous statement\n\nExample\nBEGIN\nÂ  Â  SELECT\nÂ  Â  Â  Â  product_id,\nÂ  Â  Â  Â  product_name\nÂ  Â  FROM\nÂ  Â  Â  Â  production.products\nÂ  Â  WHERE\nÂ  Â  Â  Â  list_price &gt; 100000;\nÂ  Â  IF @@ROWCOUNT = 0\nÂ  Â  Â  Â  PRINT 'No product with price greater than 100000 found';\nEND"
  },
  {
    "objectID": "qmd/sql.html#sec-sql-funs",
    "href": "qmd/sql.html#sec-sql-funs",
    "title": "SQL",
    "section": "Functions",
    "text": "Functions\n\nâ€œ||â€ Concantenate strings. e.g â€˜Postâ€™ || â€˜greSQLâ€™ â€“&gt; PostgreSQL\nBEGINâ€¦END - defines a compound statement or statement block. A compound statement consists of a set of SQL statements that execute together. A statement block is also known as a batch\n\nA compound statement can have a local declaration for a variable, a cursor, a temporary table, or an exception\n\nLocal declarations can be referenced by any statement in that compound statement, or in any compound statement nested within it.\nLocal declarations are invisible to other procedures that are called from within a compound statement\n\n\nCOMMIT - a transaction control language that is used to permanently save the changes done in the transaction in tables/databases. The database cannot regain its previous state after its execution of commit.\nDATEADD - adds units of time to a variable or value\n\ne.g.Â DATEADD(month, -3, '2021-06-01')\n\nsubtracts 3 months from 2021-06-01\n\n\nDATE_TRUNC - pulls a component of a date object.\n\ne.g.Â date_trunc('month', date_var) as month\n\nDENSE_RANK- similar to the RANK , but it does not skip any numbers even if there is a tie between the rows.\n\nValues are ranked by the column specified in ORDER BY expression of the window function\n\nEXPLAIN - a means of running your query as a what-if to see what the planner thinks about it. It will show the process the system goes through to get to the data and return it.\nEXPLAIN\nSELECT\nÂ  Â  s.id AS student_id,\nÂ  Â  g.score\nFROM\nÂ  Â  students AS s\nLEFT JOIN\nÂ  Â  grades AS g\nÂ  Â  ON s.id = g.student_id\nWHERE\nÂ  Â  g.score &gt; 90\nORDER BY\nÂ  Â  g.score DESC;\n/*\nQUERY PLAN\n----------\nSort (cost=80.34..81.88 rows=617 width=8)\n[...] Sort Key: g.score DESC\n[...] -&gt; Hash Join (cost=16.98..51.74 rows=617 width=8)\n[...] Hash Cond: (g.student_id = s.id)\n[...] -&gt; Seq Scan on grades g (cost=0.00..33.13 rows=617 width=8)\n[...] Filter: (score &gt; 90)\n[...] -&gt; Hash (cost=13.10..13.10 rows=310 width=4)\n[...] -&gt; Seq Scan on students s (cost=0.00..13.20 rows=320 width=4)\n*/\n\nSequentially scanning (â€œSeq Scanâ€) the grades and students tables because the tables arenâ€™t indexed\n\nAny Seq Scan, parallel or not, is sub-optimal\n\nEXPLAIN (BUFFERS) also shows how may data pages the database had to fetch using slow disk read operations (â€œreadâ€), and how many of them were cached in memory (â€œshared hitâ€)\n\nEXPLAIN ANALYZE - tells the planner to not only hypothesize on what it would do, but actually run the query and show the results.\n\nshows where indexes are being hit â€” or not hit as it may be. You can step through and re-optimize your basic and complex queries.\n\nGETDATE() - Gets the current date\nGO - Not a sql function. Used by some interpreters as a reset.\n\ni.e.Â any variables set before the GO statement will now not be recognized by the interpreter.\nHelps to separate code into different sections\n\nISDATE - boolean - checks that a variable is a date type\nQUALIFY - clause filters the results of window functions.\n\nuseful when answering questions like fetching the most XXX value of each category\nQUALIFY does with window functions as what HAVING does with GROUP BY. As a result, in the order of execution, QUALIFY is evaluated after window functions.\nSee Business Queries &gt;&gt; Get the latest order from each customer\n\nUNNEST - BigQuery - takes an ARRAY and returns a table with a row for each element in the ARRAY (docs)\n\nGoogle Analytics, Analysis &gt;&gt; Example 17"
  },
  {
    "objectID": "qmd/sql.html#sec-sql-udfs",
    "href": "qmd/sql.html#sec-sql-udfs",
    "title": "SQL",
    "section": "User Defined Functions (UDF)",
    "text": "User Defined Functions (UDF)\n\nMisc\n\nAvailable in SQL Server (Docs1, Docs2), Postgres (Docs), BigQuery (docs), etc.\nKeep a dictionary with the UDFs youâ€™ve created and make sure to share it with any collaborators.\nCan be persistent or temporary\n\nPersistent UDFs can be used across multiple queries, while temporary UDFs only exist in the scope of a single query\n\n\nCreate\n\nExample: temporary udf (BQ)\nCREATE TEMP FUNCTION AddFourAndDivide(x INT64, y INT64)\nRETURNS FLOAT64\nAS (\nÂ  (x + 4) / y\n);\nSELECT\nÂ  val, AddFourAndDivide(val, 2)\nFROM\nÂ  UNNEST([2,3,5,8]) AS val;\nExample: persistent udf (BQ)\nCREATE FUNCTION mydataset.AddFourAndDivide(x INT64, y INT64)\nRETURNS FLOAT64\nAS (\nÂ  (x + 4) / y\n);\n\nSELECT\nÂ  val, mydataset.AddFourAndDivide(val, 2)\nFROM\nÂ  UNNEST([2,3,5,8,12]) AS val;\n\nDelete persistent udf: DROP FUNCTION &lt;udf_name&gt;\nWith Scalar subquery (BQ)\nCREATE TEMP FUNCTION countUserByAge(userAge INT64)\nAS (\nÂ  (SELECT COUNT(1) FROM users WHERE age = userAge)\n);\nSELECT\nÂ  countUserByAge(10) AS count_user_age_10,\nÂ  countUserByAge(20) AS count_user_age_20,\nÂ  countUserByAge(30) AS count_user_age_30;"
  },
  {
    "objectID": "qmd/sql.html#sec-sql-loops",
    "href": "qmd/sql.html#sec-sql-loops",
    "title": "SQL",
    "section": "Loops",
    "text": "Loops\n\nMisc\n\nposgres docs for loops\n\nWHILE\n\nExample: Incrementally add to a counter variable\n-- Declare the initial value\nDECLARE @counter INT;\nSET @counter = 20;\n-- Print initial value\nSELECT @counter AS _COUNT;\n--â€ŠCreate a loop\nBEGIN;\n--â€ŠLoop code starting point\nWHILE @counter &lt; 30\nSELECT @counter = @counter + 1;\n--â€ŠLoop finish\nEND;\n--â€ŠCheck the value of the variable\nSELECT @counter AS _COUNT;\n\nCursors (Docs)\n\nRather than executing a whole query at once, it is possible to set up a cursor that encapsulates the query, and then read the query result a few rows at a time.\n\nOne reason for doing this is to avoid memory overrun when the result contains a large number of rows. (However, PL/pgSQL users do not normally need to worry about that, since FOR loops automatically use a cursor internally to avoid memory problems.)\nA more interesting usage is to return a reference to a cursor that a function has created, allowing the caller to read the rows. This provides an efficient way to return large row sets from functions.\n\nExample (article (do not pay attention dynamic sql. itâ€™s for embedding sql in C programs))\nDECLARE\nÂ  Â  cur_orders CURSOR FORÂ \nÂ  Â  Â  Â  SELECT order_id, product_id, quantity\nÂ  Â  Â  Â  FROM order_details\nÂ  Â  Â  Â  WHERE product_id = 456;\nÂ  Â  product_inventory INTEGER;\nBEGIN\nÂ  Â  OPEN cur_orders;\nÂ  Â  LOOP\nÂ  Â  Â  Â  FETCH cur_orders INTO order_id, product_id, quantity;\nÂ  Â  Â  Â  EXIT WHEN NOT FOUND;\nÂ  Â  Â  Â  SELECT inventory INTO product_inventory FROM products WHERE product_id = 456;\nÂ  Â  Â  Â  product_inventory := product_inventory - quantity;\nÂ  Â  Â  Â  UPDATE products SET inventory = product_inventory WHERE product_id = 456;\nÂ  Â  END LOOP;\nÂ  Â  CLOSE cur_orders;\nÂ  Â  -- do something after updating the inventory, such as logging the changes\nEND;\n\nA table called â€œproductsâ€ that contains information about all products, including the product ID, product name, and current inventory. You can use a cursor to iterate through all orders that contain a specific product and update its inventory.\nA cursor called â€œcur_ordersâ€ that selects all order details that contain a specific product ID. We then define a variable called â€œproduct_inventoryâ€ to store the current inventory of the product.\nInside the loop, we fetch each order ID, product ID, and quantity from the cursor, subtract the quantity from the current inventory and update the products table with the new inventory value.\nFinally, we close the cursor and do something after updating the inventory, such as logging the changes."
  },
  {
    "objectID": "qmd/sql.html#sec-sql-winfun",
    "href": "qmd/sql.html#sec-sql-winfun",
    "title": "SQL",
    "section": "Window Functions",
    "text": "Window Functions\n\nUnlike GROUP BY, keeps original columns after an aggregation\n\n\nAllows you to work with both aggregate and non-aggregate values all at once\n\nBetter performance than using GROUP BY + JOIN to get the same result\n\n\nDespite the order of operations, if you really need to have a window function inside a WHERE clause or GROUP BY clause, you may get around this limitation by using a subquery or a WITH query\n\nExample: Remove duplicate rows\nWITH temporary_employees as\n(SELECTÂ \nÂ  employee_id,\nÂ  employee_name,\nÂ  department,\nÂ  ROW_NUMBER() OVER(PARTITION BY employee_name,\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  department,\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  employee_id) as row_count\nFROM Dummy_employees)\n\nSELECT *\nFROM temporary_employees\nWHERE row_count = 1\n\n3 Types of Window Functions\n\n\nLEAD() will give you the row AFTER the row you are finding a value for.\nLAG() will give you the row BEFORE the row you are finding a value for.\nFIRST_VALUE() returns the first value in an ordered, partitioned data output.\n\nGeneral Syntax\n\n\nwindow_function is the name of the window function we want to use (e.g.Â see above)\n\nexpression is the name of the column that we want the window function operated on.\n\nMay not be necessary depending on what window_function is used\n\nOVER is just to signify that this is a window function\n\nPARTITION BY divides the rows into partitions so we can specify which rows to use to compute the window function\n\npartition_list is the name of the column(s) we want to partition by (i.e.Â group_by)\n\nORDER BY is used so that we can order the rows within each partition. This is optional and does not have to be specified\n\norder_list is the name of the column(s) we want to order by\n\nROWS (optional; typically not used) used to subset the rows within each partition.\n\nframe_clause defines how much to offset from our current row\nSyntax: ROWS BETWEEN &lt;starting_row&gt; AND &lt;ending_row&gt;\n\nOptions for starting and ending row\n\nUNBOUNDED PRECEDING â€” all rows before the current row in the partition, i.e.Â the first row of the partition\n[some #] PRECEDING â€” # of rows before the current row\nCURRENT ROW â€” the current row\n[some #] FOLLOWING â€” # of rows after the current row\nUNBOUNDED FOLLOWING â€” all rows after the current row in the partition, i.e.Â the last row of the partition\n\nExamples\n\nROWS BETWEEN 3 PRECEDING AND CURRENT ROW â€” this means look back the previous 3 rows up to the current row.\nROWS BETWEEN UNBOUNDED PRECEDING AND 1 FOLLOWING â€” this means look from the first row of the partition to 1 row after the current row\nROWS BETWEEN 5 PRECEDING AND 1 PRECEDING â€” this means look back the previous 5 rows up to 1 row before the current row\nROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING â€” this means look from the first row of the partition to the last row of the partition\n\n\n\n\nExample: Average Salary by Job Title\n\n\nTables for Examples\n\nExample: Average Unit Price for each CustomerId\n\nSELECT CustomerId,Â \nÂ  Â  Â  UnitPrice,Â \nÂ  Â  Â  AVG(UnitPrice) OVER (PARTITION BY CustomerId) AS â€œAvgUnitPriceâ€\nFROM [Order]Â \nINNER JOIN OrderDetail ON [Order].Id = OrderDetail.OrderId\nExample: Average Unit Price for each group of CustomerId AND EmployeeId\n\nSELECT CustomerId,Â \nÂ  Â  Â  EmployeeId,Â \nÂ  Â  Â  AVG(UnitPrice) OVER (PARTITION BY CustomerId, EmployeeId) AS â€œAvgUnitPriceâ€\nFROM [Order]Â \nINNER JOIN OrderDetail ON [Order].Id = OrderDetail.OrderId\nExample: Create a new column that ranks Unit Price in descending order for each CustomerId\n\nSELECT CustomerId,Â \nÂ  Â  Â  OrderDate,Â \nÂ  Â  Â  UnitPrice,Â \nÂ  Â  Â  ROW_NUMBER() OVER (PARTITION BY CustomerId ORDER BY UnitPrice DESC) AS â€œUnitRankâ€\nFROM [Order]Â \nINNER JOIN OrderDetailÂ \nON [Order].Id = OrderDetail.OrderId\n\nSubstituting RANK in place of ROW_NUMBER should produce the same results\nNote that ranks are skipped (e.g.Â rank 3 for ALFKI) when there are rows with the same rank\n\nIf you donâ€™t want ranks skipped, use DENSE_RANK for the window function\n\n\nExample: Create a new column that provides the previous order dateâ€™s Quantity for each ProductId\n\nSELECT ProductId,Â \nÂ  Â  Â  OrderDate,Â \nÂ  Â  Â  Quantity,Â \nÂ  Â  Â  LAG(Quantity) OVER (PARTITION BY ProductId ORDER BY OrderDate) AS \"LAG\"\nFROM [Order]Â \nINNER JOIN OrderDetail ON [Order].Id = OrderDetail.OrderId\n\nUse LEAD for the following quantity\n\nExample: Create a new column that provides the very first Quantity ever ordered for each ProductId\n\nSELECT ProductId,Â \nÂ  Â  Â  OrderDate,Â \nÂ  Â  Â  Quantity,Â \nÂ  Â  Â  FIRST_VALUE(Quantity) OVER (PARTITION BY ProductId ORDER BY OrderDate) AS \"FirstValue\"\nFROM [Order]Â \nINNER JOIN OrderDetail ON [Order].Id = OrderDetail.OrderId\nExample: Calculate a cumulative moving average UnitPrice for each CustomerId\n\nSELECT CustomerId,Â \nÂ  Â  Â  UnitPrice,Â \nÂ  Â  Â  AVG(UnitPrice) OVER (PARTITION BY CustomerIdÂ \nÂ  Â  Â  ORDER BY CustomerIdÂ \nÂ  Â  Â  ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS â€œCumAvgâ€\nFROM [Order]\nINNER JOIN OrderDetail ON [Order].Id = OrderDetail.OrderId\nExample: Rank customers for each department by amount spent\nSELECT\nÂ  Â  customer_name,\nÂ  Â  customer_id,\nÂ  Â  amount_spent,\nÂ  Â  department_id,\nÂ  Â  RANK(amount_spent) OVER(ORDER BY amount_spent DESC PARTITION BY department_id) AS spend_rank\nFROM employees\nExample: Find the model and year of car that been on lot the longest\nSELECTÂ \nFIRST_VALUE(name) OVER(PARTITION BY model, year ORDER BY date_at_lot ASC) AS oldest_car_name\nmodel,\nyear\nFROM cars\nRunning Totals/Averages (Cumulative Sums)\n\nUses SUM as the window function\n\nJust replace SUM with AVG to get running averages\n\nExample\n\n\nGenerate a new dataset grouped by month, instead of timestamp. (CTE)\n\nOnly include three fields: account_id, occurred_month and total_amount_usd\nOnly computed for the following accounts: 1041 , 1051, 1061, 10141.\n\nCompute a running total ordered by occurred_month, without collapsing the rows in the result set.\n\nDisplay 2 columns: occurred_month and cum_amnt_usd_by_month\n\n\nBecause no partition was specified, the running total is applied on the full dataset and ordered by (ascending) occurred_month\n\nExample running total by grouping variable\n\nUsing previous CTE\n\nCompute a running total by account_id, ordered by occurred_month, and account_id (i.e.Â a separate running total for each account_id.)\n\nDisplay 3 columns: account_id, occurred_month, and cum_mon_amnt_usd_by_account\n\n\n\nSame as previous example except a partition column (account_id) is added\n\nExample Running total over various window lengths\n\nUsing previous CTE\n\nCompute a 3 months rolling running total using a window that includes the current month.\nCompute a 7 months rolling running total using a window where the current month is always the middle month.\n\n\nFirst case uses 2 PRECEDING rows and the CURRENT_ROW\nSecond case uses 3 PRECEDING rows and 3 FOLLOWING rows and the CURRENT_ROW\n\nExample: Calculate the number consecutive days spent in each country (sqlite)\nwith ordered as (\n  select \n    created,\n    country,\n    lag(country) over (order by created desc)\n      as previous_country\n  from \n    raw\n),\ngrouped as (\n  select \n    country, \n    created, \n    count(*) filter (\n      where previous_country is null\n      or previous_country != country\n    ) over (\n      order by created desc\n      rows between unbounded preceding\n      and current row\n    ) as grp\n  from \n    ordered\n)\nselect\n  country,\n  date(min(created)) as start,\n  date(max(created)) as end,\n  cast(\n    julianday(date(max(created))) -\n    julianday(date(min(created))) as integer\n  ) as days\nfrom \n  grouped\ngroup by\n  country, grp\norder by\n  start desc;\n\nPost\n\nGoes over the code and thought process step-by-step with shows original data and results during intermediate steps\n\nThread\n\nEvidently only sqlite and postgres support filter. Someone in the thread suggest an alternate method.\n\nOutput:\ncountry         start         end           days\nUnited Kingdom  2023-06-08  2023-06-08  0\nUnited States   2019-09-02  2023-05-11  1347\nFrance          2019-08-25  2019-08-31  6\nMadagascar      2019-07-31  2019-08-07  7\nFrance          2019-07-25  2019-07-25  0\nUnited States   2019-05-04  2019-06-30  57\nUnited Kingdom  2018-08-29  2018-09-10  12\nUnited States   2018-08-05  2018-08-10  5"
  },
  {
    "objectID": "qmd/sql.html#sec-sql-ctes",
    "href": "qmd/sql.html#sec-sql-ctes",
    "title": "SQL",
    "section": "Common Table Expressions (CTE)",
    "text": "Common Table Expressions (CTE)\n\nThe result set of a query which exists temporarily and for use only within the context of a larger query. Much like a derived table, the result of a CTE is not stored and exists only for the duration of the query.\nAlso see\n\nWindow Functions &gt;&gt; Running Totals &gt;&gt; Examples\nGoogle, Google Analytics, Analysis &gt;&gt; Examples 12-15, 18, 19\n\nUse Cases\n\nNeeding to reference a derived table multiple times in a single query\nAn alternative to creating a view in the database\nPerforming the same calculation multiple times over across multiple query components\n\nImproves readability and usually no performance difference\n\nPrior to PostgreSQL 12, https://hakibenita.com/be-careful-with-cte-in-postgre-sql , something with the caching mechanism created a bottleneck. Currently, version 13 is the latest, so hopefully not a common problem anymore.\n\nSteps\n\nInitiate a CTE using â€œWITHâ€\nProvide a name for the result soon-to-be defined query\nAfter assigning a name, follow with â€œASâ€\nSpecify column names (optional step)\nDefine the query to produce the desired result set\nIf multiple CTEs are required, initiate each subsequent expression with a comma and repeat steps 2-4.\nReference the above-defined CTE(s) in a subsequent query\n\nSyntax\nWITH\nexpression_name_1 AS\n(CTE query definition 1)\n[, expression_name_X AS\nÂ  (CTE query definition X)\n, etc ]\nSELECT expression_A, expression_B, ...\nFROM expression_name_1\nExample\n\nComparison with a â€œderivedâ€ query\nâ€œWhat is the average monthly cost per campaign for the companyâ€™s marketing efforts?â€\nUsing CTE workflow\n-- define CTE:\nWITH Cost_by_Month AS\n(SELECT campaign_id AS campaign,\nÂ  Â  Â  TO_CHAR(created_date, 'YYYY-MM') AS month,\nÂ  Â  Â  SUM(cost) AS monthly_cost\nFROM marketing\nWHERE created_date BETWEEN NOW() - INTERVAL '3 MONTH' AND NOW()\nGROUP BY 1, 2\nORDER BY 1, 2)\n\n-- use CTE in subsequent query:\nSELECT campaign, avg(monthly_cost) as \"Avg Monthly Cost\"\nFROM Cost_by_Month\nGROUP BY campaign\nORDER BY campaign\nUsed derived query\n-- Derived\nSELECT campaign, avg(monthly_cost) as \"Avg Monthly Cost\"\nFROM\nÂ  Â  -- this is where the derived query is used\nÂ  Â  (SELECT campaign_id AS campaign,\nÂ  Â  Â  TO_CHAR(created_date, 'YYYY-MM') AS month,\nÂ  Â  Â  SUM(cost) AS monthly_cost\nÂ  Â  FROM marketing\nÂ  Â  WHERE created_date BETWEEN NOW() - INTERVAL '3 MONTH' AND NOW()\nÂ  Â  GROUP BY 1, 2\nÂ  Â  ORDER BY 1, 2) as Cost_By_Month\nGROUP BY campaign\nORDER BY campaign\n\nExample\n\nCount the number of interactions of new users\nSteps\n\nGet new users\nCount interactions\nGet interactions of new users\n\n\nWITH new_users AS (\nÂ  Â  SELECT id\nÂ  Â  FROM users\nÂ  Â  WHERE created &gt;= '2021-01-01'\n),\ncount_interactions AS (\nÂ  Â  SELECT id,\nÂ  Â  Â  Â  COUNT(*) n_interactions\nÂ  Â  FROM interactions\nÂ  Â  GROUP BY id\n),\ninteractions_by_new_users AS (\nÂ  Â  SELECT id,\nÂ  Â  Â  Â  n_interactions\nÂ  Â  FROM new_users\nÂ  Â  Â  Â  LEFT JOIN count_interactions USING (id)\n)\n\nSELECT *\nFROM interactions_by_new_users\nExample\n\nFind the average top Math test score for students in California\nSteps\n\nGet a subset of students (California)\nGet a subset of test scores (Math)\nJoin them together to get all Math test scores from California students\nGet the top score per student\nTake the overall average\n\nDerived Query (i.e.Â w/o CTE)\nSELECT AVG(score)\nFROMÂ \nÂ  (SELECT students.id, MAX(test_results.score) as score\nÂ  FROM studentsÂ \nÂ  JOIN schools ON (\nÂ  Â  students.school_id = schools.id AND schools.state = 'CA'\nÂ  )\nÂ  JOIN test_results ON (\nÂ  Â  students.id = test_results.student_id\nÂ  Â  AND test_results.subject = 'math'\nÂ  )\nÂ  GROUP BY students.id) as tmp\nUsing CTE\nWITH\nÂ  student_subset as (\nÂ  Â  SELECT students.idÂ \nÂ  Â  FROM studentsÂ \nÂ  Â  JOIN schools ON (\nÂ  Â  Â  students.school_id = schools.id AND schools.state = 'CA'\nÂ  Â  )\nÂ  ),\nÂ  score_subset as (\nÂ  Â  SELECT student_id, scoreÂ \nÂ  Â  FROM test_resultsÂ \nÂ  Â  WHERE subject = 'math'\nÂ  ),\nÂ  student_scores as (\nÂ  Â  SELECT student_subset.id, score_subset.score\nÂ  Â  FROM student_subsetÂ \nÂ  Â  JOIN score_subset ON (\nÂ  Â  Â  Â  student_subset.id = score_subset.student_id\nÂ  Â  )\nÂ  ),\nÂ  top_score_per_student as (\nÂ  Â  SELECT id, MAX(score) as scoreÂ \nÂ  Â  FROM student_scoresÂ \nÂ  Â  GROUP BY id\nÂ  )\n\nSELECT AVG(score)Â \nFROM top_score_per_student"
  },
  {
    "objectID": "qmd/sql.html#sec-sql-str",
    "href": "qmd/sql.html#sec-sql-str",
    "title": "SQL",
    "section": "Strings",
    "text": "Strings\n\nConcatenate\n\nAlso see Processing Expressions &gt;&gt; NULLs\nâ€œ||â€\nSELECT 'PostgreSQL' || ' ' || 'Databases' AS result;\n\nÂ  Â  result\n--------------\nPostgreSQL Databases\nCONCAT\nSELECT CONCAT('PostgreSQL', ' ', 'Databases') AS result;\n\nÂ  Â  result\n--------------\nPostgreSQL Databases\nWith NULL values\nSELECT CONCAT('Harry', NULL, 'Peter');\n\n--------------\nHarryPeter\n\nâ€œ||â€ wonâ€™t work with NULLs\n\nColumns\nSELECT first_name, last_name,Â \nCONCAT(first_name,' ' , last_name) \"Full Name\"Â \nFROM candidates;\n\nNew column, â€œFull Nameâ€, is created with concatenated columns\n\n\nSplitting (BQ)\nSELECT\n*,\nCASE WHEN ARRAY_LENGTH(SPLIT(page_location, '/')) &gt;= 5Â \nÂ  Â  Â  Â  Â  AND\nÂ  Â  Â  Â  Â  CONTAINS_SUBSTR(ARRAY_REVERSE(SPLIT(page_location, '/'))[SAFE_OFFSET(0)], '+')\nÂ  Â  Â  Â  Â  AND (LOWER(SPLIT(page_location, '/')[SAFE_OFFSET(4)]) INÂ \nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ('accessories','apparel','brands','campus+collection','drinkware',\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'electronics','google+redesign',\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'lifestyle','nest','new+2015+logo','notebooks+journals',\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'office','shop+by+brand','small+goods','stationery','wearables'\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  )\nÂ  Â  Â  Â  Â  Â  Â  Â  OR\nÂ  Â  Â  Â  Â  Â  Â  Â  LOWER(SPLIT(page_location, '/')[SAFE_OFFSET(3)]) INÂ \nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ('accessories','apparel','brands','campus+collection','drinkware',\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'electronics','google+redesign',\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'lifestyle','nest','new+2015+logo','notebooks+journals',\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'office','shop+by+brand','small+goods','stationery','wearables'\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  )\nÂ  Â  Â  Â  Â  )\nÂ  Â  Â  Â  Â  THEN 'PDP'\nÂ  Â  Â  Â  Â  WHEN NOT(CONTAINS_SUBSTR(ARRAY_REVERSE(SPLIT(page_location, '/'))[SAFE_OFFSET(0)], '+'))\nÂ  Â  Â  Â  Â  AND (LOWER(SPLIT(page_location, '/')[SAFE_OFFSET(4)]) INÂ \nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â    ('accessories','apparel','brands','campus+collection','drinkware',\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'electronics','google+redesign',\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'lifestyle','nest','new+2015+logo','notebooks+journals',\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'office','shop+by+brand','small+goods','stationery','wearables'\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  )\nÂ  Â  Â  Â  Â  Â  Â  Â  ORÂ \nÂ  Â  Â  Â  Â  Â  Â  Â  LOWER(SPLIT(page_location, '/')[SAFE_OFFSET(3)]) INÂ \nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ('accessories','apparel','brands','campus+collection','drinkware',\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'electronics','google+redesign',\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'lifestyle','nest','new+2015+logo','notebooks+journals',\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'office','shop+by+brand','small+goods','stationery','wearables'\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  )\nÂ  Â  Â  Â  Â  )\nÂ  Â  Â  Â  Â  THEN 'PLP'\nÂ  Â  Â  ELSE page_title\nÂ  Â  Â  END AS page_title_adjustedÂ \nFROMÂ \nÂ  unnested_events\n\nFrom article, gist\nQuery is creating a new categorical column, â€œpage_title_adjusted,â€ that is â€œPDPâ€ when a substring in â€œpage_locationâ€ is one of a set of words, and â€œPLPâ€ when itâ€™s not, and the value of page_title otherwise.\nSPLIT splits the string by separator, â€˜/â€™\nCONTAINS_SUBSTR is looking for substring with a â€œ+â€\n[SAFE_OFFSET(3)] pulls the 4th substring (think this indexes by 0?)\nAfter itâ€™s been reversed via ARRAY_REVERSE (?)\nELSE says use the value for page_title when length of the substrings after splitting page_location is 5 or less\nâ€œunnested_eventsâ€ is a CTE"
  },
  {
    "objectID": "qmd/sql.html#sec-sql-arr",
    "href": "qmd/sql.html#sec-sql-arr",
    "title": "SQL",
    "section": "Arrays",
    "text": "Arrays\n\nMisc\n\nPostGres\n\nIndexing Arrays starts at 1, not at 0\n\n\nCreate Array (BQ)\nSELECT ARRAY\nÂ  (SELECT 1 UNION ALL\nÂ  SELECT 2 UNION ALL\nÂ  SELECT 3) AS new_array;\n+-----------+\n| new_array |\n+-----------+\n| [1, 2, 3] |\n+-----------+\n\nSELECT\nÂ  ARRAY\nÂ  Â  (SELECT AS STRUCT 1, 2, 3\nÂ  Â  UNION ALL SELECT AS STRUCT 4, 5, 6) AS new_array;\n+------------------------+\n| new_arrayÂ  Â  Â  Â  Â  Â  Â  |\n+------------------------+\n| [{1, 2, 3}, {4, 5, 6}] |\n+------------------------+\n\nSELECT ARRAY\nÂ  (SELECT AS STRUCT [1, 2, 3] UNION ALL\nÂ  SELECT AS STRUCT [4, 5, 6]) AS new_array;\n+----------------------------+\n| new_arrayÂ  Â  Â  Â  Â  Â  Â  Â  Â  |\n+----------------------------+\n| [{[1, 2, 3]}, {[4, 5, 6]}] |\n+----------------------------+\nCreate a table with Arrays (Postgres)\n\nCREATE TEMP TABLE shopping_cart (\nÂ  cart_id serial PRIMARY KEY,\nÂ  products text ARRAY\nÂ  );\nINSERT INTO\nÂ  shopping_cart(products)\nVALUES\nÂ  (ARRAY['product_a', 'product_b']),\nÂ  (ARRAY['product_c', 'product_d']),\nÂ  (ARRAY['product_a', 'product_b', 'product_c']),\nÂ  (ARRAY['product_a', 'product_b', 'product_d']),\nÂ  (ARRAY['product_b', 'product_d']);\n\n-- alt syntax w/o ARRAY\nINSERT INTO\nÂ  shopping_cart(products)\nVALUES\nÂ  ('{\"product_a\", \"product_d\"}');\n\nAlso see Basics &gt;&gt; Add Data &gt;&gt; Example: chatGPT\n\nSubset an array (postgres)\n\nSELECT\nÂ  cart_id,\nÂ  products[1] AS first_product -- indexing starts at 1\nFROM\nÂ  shopping_cart;\nSlice an array (postgres)\n\nSELECT\nÂ  cart_id,\nÂ  products [1:2] AS first_two_products\nFROM\nÂ  shopping_cart\nWHERE\nÂ  CARDINALITY(products) &gt; 2;\nUnnest an array (postgres)\n\nSELECT\nÂ  cart_id,\nÂ  UNNEST(products) AS products\nFROM\nÂ  shopping_cart\nWHERE\nÂ  cart_id IN (3, 4);\n\nUseful if you want to perform a join\n\nFilter according to items in arrays (postgres)\nSELECT\nÂ  cart_id,\nÂ  products\nFROM\nÂ  shopping_cart\nWHERE\nÂ  'product_c' = ANY (products);\n\nOnly rows with arrays that have â€œproduct_câ€ will be returned\n\nChange array values using UPDATE, SET\n-- update arraysÂ \nUPDATE\nÂ  shopping_cart\nSET\nÂ  products = ARRAY['product_a','product_b','product_e']\nWHERE\nÂ  cart_id = 1;\n\nUPDATEÂ \nÂ  shopping_cart\nSET\nÂ  products[1] = 'product_f'\nWHERE\nÂ  cart_id = 2;\nSELECT\nÂ  *\nFROM\nÂ  shopping_cart\nORDER BY cart_id;\n\nFirst update: all arrays where cart_id == 1 are set to [â€˜product_aâ€™,â€˜product_bâ€™,â€˜product_eâ€™]\nSecond update: all array first values where cart_id == 2 are set to â€˜product_fâ€™\n\nInsert array values\n\nARRAY_APPEND - puts value at the end of the array\nUPDATE\nÂ  shopping_cart\nSET\nÂ  products = ARRAY_APPEND(products, 'product_x')\nWHERE\nÂ  cart_id = 1;\n\narrays in product column where cart_id == 1 get â€œproduct_xâ€ appended to the end of their arrays\n\nARRAY_PREPEND - puts value at the beginning of the array\nUPDATEÂ \nÂ  shopping_cart\nSET\nÂ  products = ARRAY_PREPEND('product_x', products)\nWHERE\nÂ  cart_id = 2;\n\narrays in product column where cart_id == 2 get â€œproduct_xâ€ prepended to the beginning of their arrays\n\n\nARRAY_REMOVE - remove array item\nUPDATE\nÂ  shopping_cart\nSET\nÂ  products = array_remove(products, 'product_e')\nWHERE cart_id = 1;\n\narrays in product column where cart_id == 1 get â€œproduct_eâ€ removed from their arrays\n\nARRAY_CONCAT(BQ), ARRAY_CAT(postgres) - Concantenate\nSELECT ARRAY_CONCAT([1, 2], [3, 4], [5, 6]) as count_to_six;\n+--------------------------------------------------+\n| count_to_sixÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  |\n+--------------------------------------------------+\n| [1, 2, 3, 4, 5, 6]Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  |\n+--------------------------------------------------+\n\n-- postgres\nSELECT\nÂ  cart_id,\nÂ  ARRAY_CAT(products, ARRAY['promo_product_1', 'promo_product_2'])\nFROM shopping_cart\nORDER BY cart_id;\nARRAY_TO_STRING - Coerce to string (BQ)\nWITH items AS\nÂ  (SELECT ['coffee', 'tea', 'milk' ] as list\nÂ  UNION ALL\nÂ  SELECT ['cake', 'pie', NULL] as list)\nSELECT ARRAY_TO_STRING(list, '--') AS text\nFROM items;\n+--------------------------------+\n| textÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  |\n+--------------------------------+\n| coffee--tea--milkÂ  Â  Â  Â  Â  Â  Â  |\n| cake--pieÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  |\n+--------------------------------+\n\nWITH items AS\nÂ  (SELECT ['coffee', 'tea', 'milk' ] as list\nÂ  UNION ALL\nÂ  SELECT ['cake', 'pie', NULL] as list)\nSELECT ARRAY_TO_STRING(list, '--', 'MISSING') AS text\nFROM items;\n+--------------------------------+\n| textÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  |\n+--------------------------------+\n| coffee--tea--milkÂ  Â  Â  Â  Â  Â  Â  |\n| cake--pie--MISSINGÂ  Â  Â  Â  Â  Â  |\n+--------------------------------+\nARRAY_AGG - gather values of a group by variable into an array (doc)\n\nMakes the output more readable\nExample: Get categories for each brand\n-- without array_agg\nselect\nÂ  Â  brand,\nÂ  Â  category\nfrom order_item\ngroup by brand, category\norder by brand, category\n;\nResults:\n| brandÂ  | categoryÂ  |Â \n| ------ | ---------- |Â \n| ArketÂ  | jacketÂ  Â  |\n| COSÂ  Â  | shirtsÂ  Â  |\n| COSÂ  Â  | trousersÂ  |Â \n| COSÂ  Â  | vestÂ  Â  Â  |\n| Levi's | jacketÂ  Â  |\n| Levi's | jeansÂ  Â  Â  |\n\n-- with array_agg\nselect\nÂ  brand,\nÂ  array_agg(distinct category) as all_categories\nfrom order_item\ngroup by brand\norder by brand\n;\nResults:\n| brandÂ  | all_categoriesÂ  Â  Â  Â  Â  Â  Â  |Â \n| ------ | ---------------------------- |Â \n| ArketÂ  | ['jacket']Â  Â  Â  Â  Â  Â  Â  Â  Â  |\n| COSÂ  Â  | ['shirts','trousers','vest'] |\n| Levi's | ['jacket','jeans']Â  Â  Â  Â  Â  |\n| Uniqlo | ['shirts','t-shirts','vest'] |\n\nARRAY_SIZE - function takes an array or a variant as input and returns the number of items within the array/variant (doc)\n\nExample: How many categories does each brand have?\nselect\nÂ  brand,\nÂ  array_agg(distinct category) as all_categories,\nÂ  array_size(all_categories) as no_of_cat\nfrom order_item\ngroup by brand\norder by brand\n;\nResults:\n| brandÂ  | all_categoriesÂ  Â  Â  Â  Â  Â  Â  | no_of_cat |\n| ------ | ---------------------------Â  | --------- |\n| ArketÂ  | ['jacket']Â  Â  Â  Â  Â  Â  Â  Â  Â  | 1Â  Â  Â  Â  |\n| COSÂ  Â  | ['shirts','trousers','vest'] | 3Â  Â  Â  Â  |\n| Levi's | ['jacket','jeans']Â  Â  Â  Â  Â  | 2Â  Â  Â  Â  |\n| Uniqlo | ['shirts','t-shirts','vest'] | 3Â  Â  Â  Â  |\n\n-- postgres using CARDINALITY to get array_size\nSELECT\nÂ  cart_id,\nÂ  CARDINALITY(products) AS num_products\nFROM\nÂ  shopping_cart;\n\nARRAY_CONTAINS checks if a variant is included in an array and returns a boolean value. (doc)\n\nVariant is just a specific category\nNeed to cast the item youâ€™d like to check as a variant first\nSyntax: ARRAY_CONTAINS(variant, array)\nExample: What brands have jackets?\nselect\nÂ  brand,\nÂ  array_agg(distinct category) as all_categories,\nÂ  array_size(all_categories) as no_of_cat,\nÂ  array_contains('jacket'::variant,all_categories) as has_jacket\nfrom order_item\ngroup by brand\norder by brand\n;\nResults:\n| brandÂ  | all_categoriesÂ  Â  Â  Â  Â  Â  Â  | no_of_cat | has_jacket |\n| ------ | ---------------------------Â  | --------- | ---------- |\n| ArketÂ  | ['jacket']Â  Â  Â  Â  Â  Â  Â  Â  Â  | 1Â  Â  Â  Â  | trueÂ  Â  Â  |\n| COSÂ  Â  | ['shirts','trousers','vest'] | 3Â  Â  Â  Â  | falseÂ  Â  Â  |\n| Levi's | ['jacket','jeans']Â  Â  Â  Â  Â  | 2Â  Â  Â  Â  | trueÂ  Â  Â  |\n| Uniqlo | ['shirts','t-shirts','vest'] | 3Â  Â  Â  Â  | falseÂ  Â  Â  |\n\n-- postgres contains_operator, @&gt;\nSELECT\nÂ  cart_id,\nÂ  products\nFROM\nÂ  shopping_cart\nWHERE\nÂ  productsÂ  @&gt; ARRAY['product_a', 'product_b'];\n\nâ€œ@&gt;â€ example returns all rows with arrays containing product_a and product_b"
  },
  {
    "objectID": "qmd/sql.html#sec-sql-bizq",
    "href": "qmd/sql.html#sec-sql-bizq",
    "title": "SQL",
    "section": "Business Queries",
    "text": "Business Queries\n\nSimple Moving Average (SMA)\n\nExample: 7-day SMA including today\nSELECT\nÂ  Date, Conversions,\nÂ  AVG(Conversions) OVER (ORDER BY Date ROWS BETWEEN 6 PRECEDING AND\nÂ  CURRENT ROW) as SMA\nFROM daily_sales\n\nExample: 3-day SMA not including today\nselect\nÂ  date,\nÂ  sales,\nÂ  avg(sales) over (order by date\nÂ  Â  Â  Â  rows between 3 preceding and current row - 1) as moving_avg\nfrom table_daily_sales\nExample: Rank product categories by shipping cost for each shipping address\n\nSELECT Product_Category,\nÂ  Shipping_Address,\nÂ  Shipping_Cost,\nÂ  ROW_NUMBER() OVER\nÂ  Â  Â  Â  Â  Â  Â  (PARTITION BY Product_Category,\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Shipping_Address\nÂ  Â  Â  Â  Â  Â  Â  ORDER BY Shipping_Cost DESC) as RowNumber,\nÂ  RANK() OVERÂ \nÂ  Â  Â  Â  (PARTITION BY Product_Category,\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Shipping_Address\nÂ  Â  Â  Â  ORDER BY Shipping_Cost DESC) as RankValues,\nÂ  DENSE_RANK() OVERÂ \nÂ  Â  Â  Â  Â  Â  Â  (PARTITION BY Product_Category,\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Shipping_AddressÂ \nÂ  Â  Â  Â  Â  Â  Â  ORDER BY Shipping_Cost DESC) as DenseRankValues\nFROM Dummy_Sales_Data_v1\nWHERE Product_Category IS NOT NULL\nAND Shipping_Address IN ('Germany','India')\nAND Status IN ('Delivered')\n\nRANK() retrieves ranked rows based on the condition of ORDER BY clause. As you can see there is a tie between 1st two rows i.e.Â first two rows have same value in Shipping_Cost column (which is mentioned in ORDER BY clause).\nDENSE_RANK is similar to the RANK , but it does not skip any numbers even if there is a tie between the rows. This you can see in Blue box in the above picture.\nRank resets to 1 when â€œShipping_Addressâ€ changes location\n\nExample: Total order quantity for each month\nSELECT strftime('%m', OrderDate) as Month,\nÂ  Â  Â  SUM(Quantity) as Total_Quantity\nfrom Dummy_Sales_Data_v1\nGROUP BY strftime('%m', OrderDate)\n\nstrftime extracts the month (%m) from the datetime column, â€œOrderDateâ€\n\nExample: Daily counts of open jobs\n\nThe issue is that there arenâ€™t rows for transactions that remain in a type of holding status\n\ne.g.Â Job Postings website has date columns for the date the job posting was created, the date the job posting went live on the website, and the date the job posting was taken down (action based timestamps), but no dates for the status between â€œwent liveâ€ and â€œtaken downâ€.\n\n\n-- create a calendar column\nSELECT parse_datetime('2020â€“01â€“01 08:00:00', 'yyyy-MM-dd H:m:s') + (interval '1' day * d) as cal_date fromÂ \nFROM ( SELECT\nROW_NUMBER() OVER () -1 as d\nFROM\n(SELECT 0 as n UNION SELECT 1) p0,\n(SELECT 0 as n UNION SELECT 1) p1,\n(SELECT 0 as n UNION SELECT 1) p2,\n(SELECT 0 as n UNION SELECT 1) p3,\n(SELECT 0 as n UNION SELECT 1) p4,\n(SELECT 0 as n UNION SELECT 1) p5,\n(SELECT 0 as n UNION SELECT 1) p6,\n(SELECT 0 as n UNION SELECT 1) p7,\n(SELECT 0 as n UNION SELECT 1) p8,\n(SELECT 0 as n UNION SELECT 1) p9,\n(SELECT 0 as n UNION SELECT 1) p10\n)\n\n-- left-join your table to the calendar column\nSelect\nÂ  Â  c.cal_date,\nÂ  Â  count(distinct opp_id) as \"historical_prospects\"\nFrom calendar c\nLeft Join\nÂ  Â  opportunities o\nÂ  Â  on\nÂ  Â  Â  Â  o.stage_entered â‰¤ c.cal_dateÂ \nÂ  Â  Â  Â  and (o.stage_exited is null or o.stage_exited &gt; c.cal_date)\n\nCalendar column should probably be a CTE\nNotes from Using SQL to calculate trends based on historical status\nSome flavours of SQL have a generate_series function, which will create this calendar column for you\nFor one particular month, then create an indicator column with â€œif posting_publish_date â‰¤ 2022â€“01â€“01 and (posting_closed_date is null or posting_closed_date &gt; 2022â€“01â€“31) then Trueâ€ and then filter for True and count.\n\nExample: Get the latest order from each customer\n-- Using QUALIFY\nselect\nÂ  Â  date,\nÂ  Â  customer_id,\nÂ  Â  order_id,\nÂ  Â  price\nfrom customer_order_table\nqualify row_number() over (partition by customer_id order by date desc) = 1\n;\n\n-- CTE w/window function\nwith order_order as\n(\nselect\nÂ  Â  date,\nÂ  Â  customer_id,\nÂ  Â  order_id,\nÂ  Â  price,\nÂ  Â  row_number() over (partition by customer_id order by date desc)Â  Â \nÂ  Â  as order_of_orders\nfrom customer_order_tableÂ \n)\n\nselect\nÂ  Â  *\nfrom order_order\nwhere order_of_orders = 1\n;\nResults:\n| dateÂ  Â  Â  | customer_id | order_id | price |\n|------------|-------------|----------|-------|\n| 2022-01-03 | 002Â  Â  Â  Â  | 212Â  Â  Â  | 350Â  |\n| 2022-01-06 | 005Â  Â  Â  Â  | 982Â  Â  Â  | 300Â  |\n| 2022-01-07 | 001Â  Â  Â  Â  | 109Â  Â  Â  | 120Â  |\nMedians\n\nNotes from How to Calculate Medians with Grouping in MySQL\n\nVariables:\n\npid: unique id variable\ncategory: A or B\nprice: random value between 1 and 6\n\n\nExample: Overall median price\nSELECT AVG(sub.price) AS median\nFROM ( \n    SELECT @row_index := @row_index + 1 AS row_index, p.price\n    FROM products.prices p, (SELECT @row_index := -1) r\n    WHERE p.category = 'A'\n    ORDER BY p.price \n) AS sub\nWHERE sub.row_index IN (FLOOR(@row_index / 2), CEIL(@row_index / 2))\n;\n\nmedian|\n------+\n   3.0|\n\n@row_index is a SQL variable that is initiated in the FROM statement and updated for each row in the SELECT statement.\nThe column whose median will be calculated (the price column in this example) should be sorted. It doesnâ€™t matter if itâ€™s sorted in ascending or descending order.\nAccording to the definition of median, the median is the value of the middle element (total count is odd) or the average value of the two middle elements (total count is even). In this example, category A has 5 rows and thus the median is the value of the third row after sorting. The values of both FLOOR(@row_index / 2) and CEIL(@row_index / 2) are 2 which is the third row. On the other hand, for category B which has 6 rows, the median is the average value of the third and fourth rows.\n\nExample: Median price for each product\nSELECT\n    sub2.category,\n    CASE WHEN MOD(sub2.total, 2) = 1 THEN sub2.mid_prices\n         WHEN MOD(sub2.total, 2) = 0 THEN (SUBSTRING_INDEX(sub2.mid_prices, ',', 1) + SUBSTRING_INDEX(sub2.mid_prices, ',', -1)) / 2\n    END AS median    \nFROM \n    (\n        SELECT \n            sub1.category,\n            sub1.total,\n            CASE WHEN MOD(sub1.total, 2) = 1 THEN SUBSTRING_INDEX(SUBSTRING_INDEX(sub1.prices, ',', CEIL(sub1.total/2)), ',', '-1')\n                 WHEN MOD(sub1.total, 2) = 0 THEN SUBSTRING_INDEX(SUBSTRING_INDEX(sub1.prices, ',', sub1.total/2 + 1), ',', '-2')\n            END AS mid_prices\n        FROM \n            (\n                SELECT\n                    p.category,\n                    GROUP_CONCAT(p.price ORDER BY p.price) AS prices,\n                    COUNT(*) AS total\n                FROM products.prices p\n                GROUP BY p.category\n            ) sub1\n    ) sub2\n;\n\ncategory|median|\n--------+------+\nA       |3     |\nB       |3.5   |\n\nBreaking down the subqueries\n\nSort prices per category\nSELECT\n    category,\n    GROUP_CONCAT(price ORDER BY p.price) AS prices,\n    COUNT(*) AS total\nFROM products.prices p\nGROUP BY p.category\n;\n\ncategory|prices     |total|\n--------+-----------+-----+\nA       |1,2,3,4,5  |    5|\nB       |1,2,3,4,5,6|    6|\n\nIf your table has a lot of data, GROUP_CONCAT would not contain all the data. In this case, you increase the limit for GROUP_CONCAT by: SET GROUP_CONCAT_MAX_LEN = 100000;\n\nGet middle prices according to whether the total count is an odd or even number\nSELECT \n    sub1.category,\n    sub1.total,\n    CASE WHEN MOD(sub1.total, 2) = 1 THEN SUBSTRING_INDEX(SUBSTRING_INDEX(sub1.prices, ',', CEIL(sub1.total/2)), ',', '-1')\n         WHEN MOD(sub1.total, 2) = 0 THEN SUBSTRING_INDEX(SUBSTRING_INDEX(sub1.prices, ',', sub1.total/2 + 1), ',', '-2')\n    END AS mid_prices\nFROM \n    (\n        SELECT\n            p.category,\n            GROUP_CONCAT(p.price ORDER BY p.price) AS prices,\n            COUNT(*) AS total\n        FROM products.prices p\n        GROUP BY p.category\n    ) sub1\n;\n\ncategory|total|mid_prices|\n--------+-----+----------+\nA       |    5|3         |\nB       |    6|3,4       |\n\nWe use theÂ MODÂ function (modulo) to check if the total count is an odd or even number.\nTheÂ SUBSTRING_INDEXÂ function is used twice to extract the middle elements.\n\n\n\nExample: Overall median of price and quantity\nSELECT\n    CASE WHEN MOD(sub2.total, 2) = 1 THEN sub2.mid_prices\n         WHEN MOD(sub2.total, 2) = 0 THEN (SUBSTRING_INDEX(sub2.mid_prices, ',', 1) + SUBSTRING_INDEX(sub2.mid_prices, ',', -1)) / 2\n    END AS median_of_price,\n    CASE WHEN MOD(sub2.total, 2) = 1 THEN sub2.mid_quantities\n         WHEN MOD(sub2.total, 2) = 0 THEN (SUBSTRING_INDEX(sub2.mid_quantities, ',', 1) + SUBSTRING_INDEX(sub2.mid_prices, ',', -1)) / 2\n    END AS median_of_quantity\nFROM \n    (\n        SELECT \n            sub1.total,\n            CASE WHEN MOD(sub1.total, 2) = 1 THEN SUBSTRING_INDEX(SUBSTRING_INDEX(sub1.prices, ',', CEIL(sub1.total/2)), ',', '-1')\n                 WHEN MOD(sub1.total, 2) = 0 THEN SUBSTRING_INDEX(SUBSTRING_INDEX(sub1.prices, ',', sub1.total/2 + 1), ',', '-2')\n            END AS mid_prices,\n            CASE WHEN MOD(sub1.total, 2) = 1 THEN SUBSTRING_INDEX(SUBSTRING_INDEX(sub1.quantities, ',', CEIL(sub1.total/2)), ',', '-1')\n                 WHEN MOD(sub1.total, 2) = 0 THEN SUBSTRING_INDEX(SUBSTRING_INDEX(sub1.quantities, ',', sub1.total/2 + 1), ',', '-2')                 \n            END AS mid_quantities\n        FROM \n            (\n                SELECT\n                    COUNT(*) AS total,\n                    GROUP_CONCAT(o.price ORDER BY o.price) AS prices,\n                    GROUP_CONCAT(o.quantity ORDER BY o.quantity) AS quantities\n                FROM products.orders o\n            ) sub1\n    ) sub2\n;\n\n\nmedian_of_price|median_of_quantity|\n---------------+------------------+\n3              |30                |\n\nSimilar to previous example\nVariables: order_id, price, quantity"
  },
  {
    "objectID": "qmd/sql.html#sec-sql-trans",
    "href": "qmd/sql.html#sec-sql-trans",
    "title": "SQL",
    "section": "Transactions",
    "text": "Transactions\n\nMisc\n\nAlso see\n\nTerms &gt;&gt; Transaction\nDatabase, Warehouses &gt;&gt; Database Triggers - Shows how to efficiently transfer data from a transactional database to a warehouse/relational database by setting up event triggers and staging tables.\n\nWhen the transaction is successful, COMMIT is applied. When the transaction is aborted, incorrect execution, system failure ROLLBACK occurs.\n\nOnly used with INSERT, UPDATE and DELETE\nBEGIN TRANSACTION: It indicates the start point of an explicit or local transaction.\n\nRepresents a point ast which the data referenced by a connection is logically and physically consistent.\nIf errors are encountered, all data modifications made after the BEGIN TRANSACTION can be rolled back to return the data to this known state of consistency\nSyntax: BEGIN TRANSACTION transaction_name ;\n\nSET TRANSACTION: Places a name on a transaction.\n\nSyntax: SET TRANSACTION [ READ WRITE | READ ONLY ];\n\nCOMMIT: used to permanently save the changes done in the transaction in tables/databases. The database cannot regain its previous state after its execution of commit.\n\nIf everything is in order with all statements within a single transaction, all changes are recorded together in the database is called committed. The COMMIT command saves all the transactions to the database since the last COMMIT or ROLLBACK command\nExample: Delete records\nDELETE FROM Student WHERE AGE = 20;\nCOMMIT;\n\nDeletes those records from the table which have age = 20 and then commits the changes in the database.\n\n\nROLLBACK: used to undo the transactions that have not been saved in the database. The command is only been used to undo changes since the last commit\n\nIf any error occurs with any of the SQL grouped statements, all changes need to be aborted. The process of reversing changes is called rollback. This command can only be used to undo transactions since the last COMMIT or ROLLBACK command was issued.\nSyntax: ROLLBACK;\n\nSAVEPOINT: creates points within the groups of transactions in which to ROLLBACK.\n\nSyntax: SAVEPOINT &lt;savepoint_name&gt;;\nA savepoint is a point in a transaction in which you can roll the transaction back to a certain point without rolling back the entire transaction.\nRemove a savepoint: RELEASE SAVEPOINT &lt;savepoint_name&gt;\nExample: Rollback a deletion\nSAVEPOINT SP1;\n//Savepoint created.\nDELETE FROM Student WHERE AGE = 20;\n//deleted\nSAVEPOINT SP2;\n//Savepoint created.\nROLLBACK TO SP1;\n//Rollback completed."
  },
  {
    "objectID": "qmd/sql.html#sec-sql-procexp",
    "href": "qmd/sql.html#sec-sql-procexp",
    "title": "SQL",
    "section": "Processing Expressions",
    "text": "Processing Expressions\n\nUse multiple conditions in a WHERE expression\nselect\nÂ  Â  *\nfrom XXX_table\nwhere 1=1\nÂ  Â  (if condition A) and clause 1Â \nÂ  Â  (if condition B) and clause 2Â \nÂ  Â  (if condition C) and clause 3\n;\n\nThe â€œ1=1â€ prevents errors that would occur when the first condition doesnâ€™t apply to any rows.\n\nCan also use â€œtrueâ€\n\n\nSelect unique rows without using DISTINCT\n\nUsing UNION\nSELECT employee_id,\nÂ  Â  Â  employee_name,\nÂ  Â  Â  department\nFROM Dummy_employees\nUNION\nSELECT employee_id,\nÂ  Â  Â  employee_name,\nÂ  Â  Â  department\nFROM Dummy_employees\n\nthere must be same number and order of columns in both the SELECT statements\n\nUsing INTERSECT\nSELECT employee_id,\nÂ  Â  Â  employee_name,\nÂ  Â  Â  department\nFROM Dummy_employees\nINTERSECT\nSELECT employee_id,\nÂ  Â  Â  employee_name,\nÂ  Â  Â  department\nFROM Dummy_employees\n\nThere must be same number and order of columns in both the SELECT statements\n\nUsing ROW_NUMBER\nWITH temporary_employees as (\n  SELECT\n  Â  employee_id,\n  Â  employee_name,\n  Â  department,\n  Â  ROW_NUMBER() OVER(PARTITION BY employee_name,\n  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  department,\n  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  employee_id) as row_count\n  FROM Dummy_employees\n)\n\nSELECT *\nFROM temporary_employees\nWHERE row_count = 1\nUsing GROUP BY\nSELECT employee_id,\nÂ  Â  Â  employee_name,\nÂ  Â  Â  department\nFROM Dummy_employees\nGROUP BY employee_id,\nÂ  Â  Â  Â  employee_name,\nÂ  Â  Â  Â  department\n\nJust need to group by all the columns. Useful to use in conjunction with aggregate functions.\n\n\nCASE WHEN\nSELECT OrderID,\nÂ  Â  Â  OrderDate,\nÂ  Â  Â  Sales_Manager,\nÂ  Â  Â  Quantity,\nÂ  Â  Â  CASE WHEN Quantity &gt; 51 THEN 'High'\nÂ  Â  Â  Â  Â  Â  WHEN Quantity &lt; 51 THEN 'Low'\nÂ  Â  Â  Â  Â  Â  ELSE 'Medium'Â \nÂ  Â  Â  END AS OrderVolume\nFROM Dummy_Sales_Data_v1\n\nEND AS specifies the name of the new column, â€œOrderVolumeâ€\nELSE specifies the value when none of the conditions are met\n\nIf you did not mention ELSE clause and no condition is satisfied, the query will return NULL for that specific record\n\n\nPivot Wider\n\nSELECT Sales_Manager,\nÂ  Â  Â  COUNT(CASE WHEN Shipping_Address = 'Singapore' THEN OrderID\nÂ  Â  Â  Â  Â  Â  END) AS Singapore_Orders,\n\nÂ  Â  Â  COUNT(CASE WHEN Shipping_Address = 'UK' THEN OrderID\nÂ  Â  Â  Â  Â  Â  END) AS UK_Orders,\n\nÂ  Â  Â  COUNT(CASE WHEN Shipping_Address = 'Kenya' THEN OrderID\nÂ  Â  Â  Â  Â  Â  END) AS Kenya_Orders,\n\nÂ  Â  Â  COUNT(CASE WHEN Shipping_Address = 'India' THEN OrderID\nÂ  Â  Â  Â  Â  Â  END) AS India_Orders\nFROM Dummy_Sales_Data_v1\nGROUP BY Sales_Manager\n\nDepending on your use-case you can also use different aggregation such as SUM, AVG, MAX, MIN with CASE statement.\n\n\n\nNULLs\n\nDivision and NULLS\n\nAny division with NULL values with have a result of NULL.\nisNull allows to get a different resulting value\nSELECT IsNull(&lt;column&gt;, 0) / 45\n\nAll NULL values in the column will replaced with 0s during the division operation.\n\n\nCOALESCE\n\nSubstitute a default value in place of NULLs\nSELECT COALESCE(column_name, 'Default Value') AS processed_column\nFROM table_name;\n\nSELECT COALESCE(order_date, current_date) AS processed_date\nFROM orders;\n\nSELECT\n  product ||' - '||\n  COALESCE(subcategory, category, family, 'no product description ')\n    AS product_and_subcategory\nFROM stock\n\n3rd Expression: If there is a NULL in subcategory, then it looks in category, then into family, and finally if all those fields have NULLs, it uses â€œno product descriptionâ€ as the value.\n\nConcantenating Strings where NULLs are present\nSELECT COALESCE(first_name, '') || ' ' || COALESCE(last_name, '') AS full_name\nFROM employees;\n\nNULLs are replaced with an empty string so transformation doesnâ€™t break\n\nPerforming calculations involving numeric columns where there are NULLs\nSELECT COALESCE(quantity, 0) * COALESCE(unit_price, 0) AS total_cost\nFROM products;\n\nSELECT product,\n  quantity_available,\n  minimum_to_have,\n  COALESCE(minimum_to_have, quantity_available * 0.5) AS threshold\nFROM stock\n\nNULLs are substituted with 0s so the calcuation doesnâ€™t break\n\nAs part of a join in case keys have missing values\nSELECT *\nFROM employees e\nLEFT JOIN departments d ON COALESCE(e.department_id, 0) = COALESCE(d.id, 0);\nWith Aggregate Functions\nSELECT department_id, COALESCE(SUM(salary), 0) AS total_salary\nFROM employees\nGROUP BY department_id;\nMake hierarchical subtotals output more readable\n\nSELECT COALESCE(family,'All Families') AS family,\n COALESCE(category,'All Categories') AS category,\n COALESCE(subcategory,'All Subcategories') AS subcategory,\n SUM(quantity_available) as quantity_in_stock\nFROM stock\nGROUP BY ROLLUP(family, category, subcategory)\nORDER BY family, category, subcategory\n\nROLLUPÂ clause assumes a hierarchy among the columnsÂ family,Â category, andÂ subcategory. Thus, it generates all the grouping sets that make sense considering the hierarchy:Â GROUP BY family,Â GROUP BY family, categoryÂ andÂ GROUP BY family, category, subcategory.\n\nThis is the reason whyÂ ROLLUPÂ is often used to generate subtotals and grand totals for reports.\n\nWithout COALESCE , the text in the unused columns for the subtotals would be NULLs.\n\n\n\n\n\nDuplicated Rows\n\nRemove duplicated rows with window function\nWITH temporary_employees asÂ \n(SELECTÂ \nÂ  employee_id,Â \nÂ  employee_name,Â \nÂ  department,Â \nÂ  ROW_NUMBER() OVER(PARTITION BY employee_name,Â \nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  department,Â \nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  employee_id) as row_countÂ \nFROM Dummy_employees)\n\nSELECT *Â \nFROM temporary_employeesÂ \nWHERE row_count = 1\nUse a hash column as id column, then test for duplicates, remove them or investigate them (BigQuery)\n\nWITH\nÂ  Â  inbound_zoo_elephants AS (\nÂ  Â  Â  Â  SELECT *\nÂ  Â  Â  Â  FROM flowfunctions.examples.zoo_elephants\nÂ  Â  ),\nÂ  Â  add_row_hash AS (\nÂ  Â  Â  Â  SELECT\nÂ  Â  Â  Â  Â  Â  *,\nÂ  Â  Â  Â  Â  Â  TO_HEX(MD5(TO_JSON_STRING(inbound_zoo_elephants))) AS hex_row_hash\nÂ  Â  Â  Â  FROM inbound_zoo_elephants\nÂ  Â  )\n\nSELECT\nÂ  Â  COUNT(*) AS records,\nÂ  Â  COUNT(DISTINCT hex_row_hash) AS unique_records\nFROM add_row_hash\n\nNo duplicate records found, since â€œrecordsâ€ = 9 and â€œunique_recordsâ€ = 9\n\nif records &gt; unique_records, duplicates exist\n\nCan select distinct hex_row_hash if you want to remove duplicates\nCan count hex_row_hash then filter where hex_row_hash &gt; 1 to find which rows are duplicates\nNotes from link\nDescription\n\nflowfunctions is the project name\nexamples is a directory (?)\nzoo_elephants is the dataset\n\nSteps\n\nTO_JSON_STRING - creates column with json string for each row\nMD5 hashes that string\nTO_HEX makes it alpha-numeric and gets rid of the symbols in the hash\n\nEasier to deal with in BigQuery\nAssume this is still unique (?)\n\n\nNote: By adding â€œtrueâ€ value, TO_JSON_STRING(inbound_zoo_elephants, true) , TO_JSON_STRING adds line breaks to the json string for easier readability.\nHashing function options\n\nMD5 -Â  shortest one (16 characters), fine for this use case\n\ncryptographically broken, returns 16 characters and suffices for our use-case. Other options are\n\nFARM_FINGERPRINT - returns a signed integer of variable length\nSHA1, SHA256 and SHA512, which return 20, 32 and 64 bytes respectively and are more secure for cryptographic use cases.\n\n\n\n\n\nNested Data\n\nRecursive CTE\n\nRecursive CTEs are used primarily when you want to query hierarchical data or graphs. This could be a companyâ€™s organizational structure, a family tree, a restaurant menu, or various routes between cities\nAlso see\n\nWhat Is a Recursive CTE in SQL?\n\nTutorial, 3 examples, and links to other articles\n\n\nSyntax\nWITH RECURSIVE cte_name AS (\nÂ  Â  cte_query_definition (the anchor member)\nÂ  Â  UNION ALL\nÂ  Â  cte_query_definition (the recursive member)\n)\n\nSELECT *\nFROMÂ  cte_name;\nExample: : postgres\nWITH RECURSIVE category_tree(id, name, parent_id, depth, path) AS (\nÂ  SELECT id, name, parent_id, 1, ARRAY[id]\nÂ  FROM categories\nÂ  WHERE parent_id IS NULL\nÂ  UNION ALL\nÂ  SELECT categories.id, categories.name, categories.parent_id, category_tree.depth + 1, path || categories.id\nÂ  FROM categories\nÂ  JOIN category_tree ON categories.parent_id = category_tree.id\n)\n\nSELECT id, name, parent_id, depth, path\nFROM category_tree;\n\nCTE (WITH) + RECURSIVE says itâ€™s a recursive query.\nUNION ALLcombines the results of both statements.\n\nExample is defined by 2 Select statements\n\nAnchor Member: First SELECT statement selects the root nodes of the category tree (nodes with no parent)\n\nRoot node is indicated by â€œparent_idâ€ = NULL\n\nRecursive member: Second SELECT statement selects the child nodes recursively\n\nAlso see Arrays for further examples of the use of UNION ALL\n\nThe â€œdepthâ€ column is used to keep track of the depth of each category node in the tree.\n\nâ€œ1â€ in the first statement\nâ€œcategory_tree.depth + 1â€ in the second statement\n\nWith every recursion, the CTE will add 1 to the previous depth level, and it will do that until it reaches the end of the hierarchy\n\n\nThe â€œpathâ€ column is an array that stores the path from the root to the current node.\n\nâ€œARRAY[id]â€ in the first statement\nâ€œpath || categories.idâ€ in the second statement\n\nâ€œ||â€ concatenates â€œpathâ€ and â€œidâ€ columns (See Strings)\n\n\n\n\n\n\n\nBinning\n\nCASE WHEN\n\nSELECT\n Name, \n Grade,\n CASE\n  WHEN Grade &lt; 10 THEN '0-9'\n  WHEN Grade BETWEEN 10 and 19 THEN '10-19'\n  WHEN Grade BETWEEN 20 and 29 THEN '20-29'\n  WHEN Grade BETWEEN 30 and 39 THEN '30-39'\n  WHEN Grade BETWEEN 40 and 49 THEN '40-49'\n  WHEN Grade BETWEEN 50 and 59 THEN '50-59'\n  WHEN Grade BETWEEN 60 and 69 THEN '60-69'\n  WHEN Grade BETWEEN 70 and 79 THEN '70-79'\n  WHEN Grade BETWEEN 80 and 89 THEN '80-89'\n  WHEN Grade BETWEEN 90 and 99 THEN '90-99'\n  END AS Grade_Bucket\n FROM students\n\nBETWEEN is inclusive of the end points\nFlexible for any size of bin you need\n\nFLOOR\nSELECT\n Name,\n Grade,\n FLOOR(Grade / 10) * 10 AS Grade_Bucket\nFROM students\n\nCan easily scale up the number of bins without having to increase the lines of code\nOnly useful for evenly spaced bins\n\nLEFT JOIN on preformatted table\nCREATE OR REPLACE TABLE bins (\n    Lower_Bound INT64,\n    Upper_Bound INT64,\n    Grade_Bucket STRING\n);\n\nINSERT bins (Lower_Bound, Upper_Bound, Grade_Bucket)\nVALUES\n (0, 9, '0-9')\n (10, 19, '10-19')\n (20, 29, '20-29')\n (30, 39, '30-39')\n (40, 49, '40-49')\n (50, 59, '50-59')\n (60, 69, '60-69')\n (70, 79, '70-79')\n (80, 89, '80-89')\n (90, 99, '90-99');\n\nSELECT\n A.Name, \n A.Grade,\n B.Grade_Bucket\nFROM students AS A\nLEFT JOIN bins AS B\nON A.Grade BETWEEN B.Lower_Bound AND B.Upper_Bound\n\nâ€œbinsâ€ table acts a template that funnels the values from your table into the correct bins\n\n\n\n\nTime Series\n\nExtract components from date-time columns\n/* MySQL */\nEXTRACT(part_of_date FROM date_time_column_name)\nYEAR(date_time_column_name)\nMONTH(date_time_column_name)\nMONTHNAME(date_time_column_name)\nDATE_FORMAT(date_time_column_name)\n\n/* SQLte */\nSELECT strftime('%m', OrderDate) as Month\n\nstrftime codes\n\n\nPreprocess Time Series with 4 Lags (article)\nWITH top_customers as (\nÂ  Â  --- select the customter ids you want to track\n),\ntransactions as (\nÂ  Â  SELECTÂ \nÂ  Â  Â  cust_id,Â \nÂ  Â  Â  dt,Â \nÂ  Â  Â  date_trunc('hour', cast(event_time as timestamp)) as event_hour,Â \nÂ  Â  Â  count(*) as transactions\nÂ  Â  FROM ourTable\nÂ  Â  WHERE\nÂ  Â  Â  Â  dt between cast(date_add('day', -7, current_date) as varchar)Â \nÂ  Â  Â  Â  and cast(current_date as varchar)\nÂ  Â  GROUP BY 1,2,3 Order By event_hour asc\n)\n\nSELECT transactions.cust_id,\nÂ  Â  Â  transactions.event_hour,\nÂ  Â  Â  day_of_week(transactions.event_hour) day_of_week,\nÂ  Â  Â  Â  hour(transactions.event_hour) hour_of_day,\nÂ  Â  Â  Â  transactions.transactions as transactions,\nÂ  Â  Â  Â  LAG(transactions,1) OVERÂ \nÂ  Â  Â  Â  Â  (PARTITION BY transactions.cust_id ORDER BY event_hour) AS lag1,\nÂ  Â  Â  Â  LAG(transactions,2) OVERÂ \nÂ  Â  Â  Â  Â  (PARTITION BY transactions.cust_id ORDER BY event_hour) AS lag2,\nÂ  Â  Â  Â  LAG(transactions,3) OVERÂ \nÂ  Â  Â  Â  Â  (PARTITION BY transactions.cust_id ORDER BY event_hour) AS lag3,\nÂ  Â  Â  Â  LAG(transactions,4) OVERÂ \nÂ  Â  Â  Â  Â  (PARTITION BY transactions.cust_id ORDER BY event_hour) AS lag4\nFROM transactionsÂ \nÂ  Â  join top_customersÂ \nÂ  Â  Â  on transactions.cust_id = top_customers.cust_id\n\n/* output */\n\"cust_id\", \"event_hour\", \"day_of_week\", \"hour_of_day\", \"transactions\", \"lag1\", \"lag2\", \"lag3\", \"lag4\"\n\"Customer-123\",\"2023-01-14 00:00:00.000\",\"6\",\"0\",\"4093\",,,,,,\n\"Customer-123\",\"2023-01-14 01:00:00.000\",\"6\",\"1\",\"4628\",\"4093\",,,,,\n\"Customer-123\",\"2023-01-14 02:00:00.000\",\"6\",\"2\",\"5138\",\"4628\",\"4093\",,,,\n\"Customer-123\",\"2023-01-14 03:00:00.000\",\"6\",\"3\",\"5412\",\"5138\",\"4628\",\"4093\",,,\n\"Customer-123\",\"2023-01-14 04:00:00.000\",\"6\",\"4\",\"5645\",\"5412\",\"5138\",\"4628\",\"4093\",\n\"Customer-123\",\"2023-01-14 05:00:00.000\",\"6\",\"5\",\"5676\",\"5645\",\"5412\",\"5138\",\"4628\",\n\"Customer-123\",\"2023-01-14 06:00:00.000\",\"6\",\"6\",\"6045\",\"5676\",\"5645\",\"5412\",\"5138\",\n\"Customer-123\",\"2023-01-14 07:00:00.000\",\"6\",\"7\",\"6558\",\"6045\",\"5676\",\"5645\",\"5412\",\n\nDataset contains number of transactions made per customer per hour.\n2 WITH clauses: the first just extracts a list of customers we are interested in. Here you can add any condition that is supposed to filter in or out specific customers (perhaps you want to filter new customers or only include customers with sufficient traffic). The second WITH clause simply creates the first data set â€” Dataset A, which pulls a week of data for these customers and selects the customer id, date, hour, and number of transactions.\nFinally, the last and most important SELECT clause generates Dataset B, by using SQL lag() function on each row in order to capture the number of transactions in each of the hours that preceded the hour in the row."
  },
  {
    "objectID": "qmd/sql.html#sec-sql-tools",
    "href": "qmd/sql.html#sec-sql-tools",
    "title": "SQL",
    "section": "Tools",
    "text": "Tools\n\nChatSQL: Convert plain text to MySQL query by ChatGPT\n{{sqlglot}} - no dependency Python SQL parser, transpiler, optimizer, and engine\n\nFormat SQL or translate between nearly twenty different SQL dialects.\n\nIt doesnâ€™t just transpile active SQL code, too. Moves comments from one dialect to another.\n\nThe parser itself can be customized\nCan also help you analyze queries, traverse parsed expression trees, and incrementally (and, programmatically) build SQL queries.\nsupport for optimizing SQL queries, and performing semantic diffs.\nCan be used to unit test queries through mocks based on Python dictionaries.\nExample: : translate duckdb to hive\nimport sqlglot\nsqlglot.transpile(\nÂ  \"SELECT EPOCH_MS(1618088028295)\",Â \nÂ  read = \"duckdb\",Â \nÂ  write = \"hive\"\n)[0]\n---\n'SELECT FROM_UNIXTIME(1618088028295 / 1000)'"
  },
  {
    "objectID": "qmd/stochastic-processes.html",
    "href": "qmd/stochastic-processes.html",
    "title": "69Â  Stochastic Processes",
    "section": "",
    "text": "TOC\n\nMisc\nTerms\nTransitions\n\n1-step\n2-step\nmatrixpower and n-step\n\nCommunication\n\nCommunication\nClasses\nReducibility\n\nRandom Walk\n\nTransient, Recurrent, and Absorbing States\n\nHidden Markov Models (HMM)\n\nMarkov Switching Dynamic Regression (MSDR)\nMarkov Switching Auto Regression (MSAR)\n\n\nMisc\n\nNotes from\n\nStochastic Processes: A Beginners Guide\n\n\nTerms\n\nBrownian Motion - a continuous process such that its increments for any time scale are drawn from a normal distribution\nCommunication Class - consists a set of states that all communicate with one another (e.g.Â A must be reachable from B and B reachable from A)\nIrreducible - A Markov chain is irreducible if it consists of a single communication class otherwise itâ€™s Reducible\nLimiting distribution (aka invariant distribution) describes the long-term probability of being in any state, as well as the proportion of time spent in that state.\n\nThis is like a asymptotic State Probability Distribution or a âˆ-step State Probability Distribution\nCan be calcâ€™d by raising a transition matrix to a very large power\n\n\nmatrixpower(printers, 100)\nÂ  Â  [,0]Â  Â  Â  [,1]Â  Â  Â  [,2]\n[0,] 0.5882353 0.2941176 0.1176471\n[1,] 0.5882353 0.2941176 0.1176471\n[2,] 0.5882353 0.2941176 0.1176471\n\nMarkov property:Â  A system or variable is said to be Markov when knowing the current state of the process is just as good as knowing the entire history.\n\nMarkov processes are said to be â€˜memorylessâ€™ because the history of their states is irrelevant.\n\nRecurrent states - states that have an infinite number of expected visits\n\nAbsorbing state (boundary) - (special recurrent case) states that only communicate with themselves. A state in which once itâ€™s entered, you can never leave\n\nState Probability Distribution - Distribution of the Markov variable Xt at time, t, and it is denoted by Ï€t\n\nNotation for n-states at time, t (or â€œstepâ€ t is the same thing)\nEach element of Ï€t can be referenced using the notation Ï€jt\n\nThe sum of Ï€jt=1, so that says that the sum of the probabilities for all the states at a given time step must be 1.\n\nNote that each row in a transition matrix sums to 1.\n\n\n\nTransient states - states that have a finite number of expected visits\n\nexamples:\n\nstate 0 (above) - if state 0 is the starting point, then after the 1st transition, it is no longer a potential destination\nstates 1,2,3 (drunkard below) - small values or zeros in the limiting distribution\n\n\nTransition matrix\n\nRow index represents the initial or source state at time t\nColumn index represents the destination state at time (t+n)\nEach Cell is the probability that by starting in source state, i, you end-up in destinations state, j after n-steps/time\n\n\nTransitions\n\nâ€œstepsâ€ and â€œtime stepsâ€ are interchangable in terms of transition matrices, e.g.Â step n+2 and time, t+2, state probability distributions, etc.\n1-Step transition matrix\n\nExample:\n\nThis transition matrix describes\n\ny-axis - probabilities of starting with an initial state (e.g.Â # of printers broken at the beginning of the day)\nx-axis - probabilities of transitioning to the final (for a 1-step) state (e.g.Â # of printers broken at the end of that day which will be the initial state of the following day)\n\n(0,0) says thereâ€™sÂ  a 70% probability if you start the day with 0 printers broken, then youâ€™ll end the day with 0 printers broken.\n(1,2) says thereâ€™s a 40% probability if you start the day with 1 printer broken, then youâ€™ll end the day with 2 printers broken.\n(2,1) says thereâ€™s a 100% probability if you start the day with 2 printers broken, then youâ€™ll end the day with 0 printers broken.\n\ne.g.Â a repairman is contractually obligated to show up and fix or replace both printers by the end of the day if they break on the same day.\n\n\n\n\n\n2-Step Transition\n\nExample: Same scenario as above but there are two steps (2-day interval) in which the transition is from state 0 to state 1, (0,1).\n\nTo do that we would need to sum all possible two-step transitions that start at state 0 and end-up in state 1.\nFirst potential 2-step transition says:\n\n1st step -Â  the probability of moving to state 0 (n-1) given you were in state 0 (n-2) then\n2nd step - the probability of moving to state 1 (n) given you were in state 0 (n-1)\n\nFor this example, each 2 step transition will be of the form p(x|0) * p(1|x)\nIn general, the probability that the Markov chain will be in state j at step n, given that it was in state i at (n-1)\n\nPij(Xn = j | Xn-1 = i)\n\n\n\nN-step\n\nIn general,\n\nWhere\n\nP is the trasition matrix and t is the step (t is for time step instead of N)\nÏ€0 is like a prior probability vector for each state at t = 0\n\nThis prior gives us the probabilities for which state weâ€™ll likely start out at.\n\n\nA little different from other the calculations in the Transitions section. Here t-step transition matrix is multiplied times a prior\n\nmatrixpower (user-defined) performs this calculation for all n-step transitions\n\n\n# this is just matrix exponentiation and multiplying the square (based on row dim) identity matrix at the end for some reason (probably makes a diff for non-square matrices)\nmatrixpower &lt;- function(mat,k) {\nÂ  if (k == 0) return (diag(dim(mat)[1]))Â  # 3x3 identity matrix\nÂ  if (k == 1) return(mat)\nÂ  if (k &gt; 1) return( mat %*% matrixpower(mat, k-1))\n} # matrix power function from Introduction to Stochastic Processes with R by Robert Dobrow\n\nprinters &lt;- matrix(c(.7, .2, 1, .3, .4, 0, 0, .4, 0), nrow = 3, ncol = 3) # make the matrix\nprinters\nÂ  Â  [,0] [,1] [,2]\n[0,]Â  0.7Â  0.3Â  0.0\n[1,]Â  0.2Â  0.4Â  0.4\n[2,]Â  1.0Â  0.0Â  0.0\n\n# 2-step\nmatrixpower(printers, 2)\nÂ  Â  [,0] [,1] [,2]\n[0,] 0.55 0.33 0.12\n[1,] 0.62 0.22 0.16\n[2,] 0.70 0.30 0.00\n\n# also 3-step\nmatrixpower(printers, 3)Â \nÂ  Â  Â  [,0]Â  [,1]Â  [,2]Â \n[0,] 0.571 0.297 0.132Â \n[1,] 0.638 0.274 0.088Â \n[2,] 0.550 0.330 0.120\n\nIn the 2-step transition output, 0.33 at (0,1) matches the manual calculation above\nThereâ€™s a manual change to start indexing from 0, as thatâ€™s the typical way of indexing matrices for stochastic processes. R indexing typically starts from 1.\n\n\nCommunication\n\nCommunication\n\ndefinition\n\nClasses\n\nstates in class X can communicate (i.e.Â transition to one another) but Class X members do not communicate with members of Class Y\n\nReducibility\n\nIrreducible - A Markov chain is irreducible if it consists of a single communication class otherwise itâ€™s reducible.\n\nExample\n\n\n\nreducible &lt;-Â  matrix(c(0, 0, 0, 0, 0, .4, .9, .7, 0, 0, 0, .1, .3, 0, 0, .6, 0, 0, .5, .5, 0, 0, 0, .5, .5), nrow = 5, ncol = 5)\nreducible\nÂ  [,0] [,1] [,2] [,3] [,4]\n[0,]Â  Â  0Â  0.4Â  0.0Â  0.6Â  0.0\n[1,]Â  Â  0Â  0.9Â  0.1Â  0.0Â  0.0\n[2,]Â  Â  0Â  0.7Â  0.3Â  0.0Â  0.0\n[3,]Â  Â  0Â  0.0Â  0.0Â  0.5Â  0.5\n[4,]Â  Â  0Â  0.0Â  0.0Â  0.5Â  0.5\n\nmatrixpower(reducible, 100)\nÂ  [,0]Â  [,1]Â  [,2] [,3] [,4]\n[0,]Â  Â  0 0.350 0.050Â  0.3Â  0.3\n[1,]Â  Â  0 0.875 0.125Â  0.0Â  0.0\n[2,]Â  Â  0 0.875 0.125Â  0.0Â  0.0\n[3,]Â  Â  0 0.000 0.000Â  0.5Â  0.5\n[4,]Â  Â  0 0.000 0.000Â  0.5Â  0.5\n\n2 Communication Classes\n\nloop with states 1 and 2 (see rows 1 and 2)\nloop with states 3 and 4 (see rows 3 and 4)\n\nstarting at state 0,\n\nYou can transition to either of the 2 classes through state 1 or state 3 but never return to state 0 (i.e.Â state 0 isnâ€™t part of a class).\n\nstarting in states 1 or 2, you remain in that class indefinitely\nstarting in states 3 or 4, you remain in that class indefinitely\nLimiting Distributions (see definition in Terms)\n\nFor a Reducible Chain\n\nSome or all the values for a column will be different\nSome or all the rows will be different\nThe values are different because the starting state matters\n\nFor an Irreducible Chain:\n\nEach column will have the (essentially) the same value in each row\nEach row will be identical\nThe values are the same because the starting state doesnâ€™t matter\n\nNote that in the example above that each state can (eventually) be reached from state 0.\n\n\nRandom Walk\n\nTransient, Recurrent, and Absorbing states\n\nExample: Drunkard that either makes it home (state 4) or the ocean (state 0)\n\n\nrandom_walk &lt;- matrix(c(1, .5, 0, 0, 0, 0, 0, .5, 0, 0, 0, .5, 0, .5, 0, 0, 0, .5, 0, 0, 0, 0, 0, .5, 1),\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  nrow = 5,\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ncol = 5)\nÂ  Â  [,0] [,1] [,2] [,3] [,4]\n[0,]Â  1.0Â  0.0Â  0.0Â  0.0Â  0.0\n[1,]Â  0.5Â  0.0Â  0.5Â  0.0Â  0.0\n[2,]Â  0.0Â  0.5Â  0.0Â  0.5Â  0.0\n[3,]Â  0.0Â  0.0Â  0.5Â  0.0Â  0.5\n[4,]Â  0.0Â  0.0Â  0.0Â  0.0Â  1.0\nmatrixpower(random_walk, 100)\nÂ  Â  [,0]Â  Â  Â  Â  [,1]Â  Â  Â  Â  [,2]Â  Â  Â  Â  [,3] [,4]\n[0,] 1.00 0.000000e+00 0.000000e+00 0.000000e+00 0.00\n[1,] 0.75 4.440892e-16 0.000000e+00 4.440892e-16 0.25\n[2,] 0.50 0.000000e+00 8.881784e-16 0.000000e+00 0.50\n[3,] 0.25 4.440892e-16 0.000000e+00 4.440892e-16 0.75\n[4,] 0.00 0.000000e+00 0.000000e+00 0.000000e+00 1.00\n\nStates 0, 4 are absorbing states and states 1,2,3 are transient\nEventually through probability, there must be a transition to states 0 or 4 where itâ€™s impossible to leave. Therefore the number of visits to 1,2,3 will be finite and by definition, they are transient.\nLimiting distribution - notice the middle states have either very small numbers or zero values, this is a property of transient states.\n\nHidden Markov Models (HMM)\n\nNotes from A Math Lovers Guide to Hidden Markov Models\nUsed to study phenomena in which only a portion of the phenomenon can be directly observed while the rest of it cannot be directly observed, although its effect can be felt on what is observed. The effect of the unobserved portion (â€œhiddenâ€) can only be estimated.\n\nObserved portion is modeled using arima, glm, rf, etc.\nUnobserved portion is modeled using Markov process\n\nSort of like a latent variable thatâ€™s values are states (aka regimes)\n\nThe two models produce a final prediction by calculating an expected value at each step that uses predictions from observed model and probabilities from the unobserved model.\n\nUse Cases\n\nStudying human workflows using computer event logs. Part of that is taking a userâ€™s raw event log (keystrokes, URLs visited, etc.) and determining what they were doing at each time (background research, document review, etc.). Sometimes you can guess from one event in isolation, but usually you also need to look at the events before and after to have confidence.\nYou have a photograph of a page from a novel, and you want to digitize the text. The photograph is already segmented into images of words. Most of them can be decoded with OCR, but some words are ambiguous because of dirt on the original page (is that â€œquickâ€ or â€œquackâ€?). You want to use the surrounding words to pick the best option.\nYou have a sequence of brain scans, taken every second while a person was playing a video game. During the game their avatar moves in and out of buildings, and your goal is to guess whether it is inside or outside using only the brain scans. Any given scan is very ambiguous, but the avatar usually stays inside/outside for many seconds at a time. You expect large spans of time where the avatar is in(out)side, and where the brain scans look slightly more in(out)side on average.\n\nComponents\n\nAn underlying markov chain that describes how likely you are to transition between different states (or stay in the same state). Typically this underlying state is the thing that youâ€™re really interested in. If there are k states in the HMM then the markov chain consists of:\n\nk*k transition matrix saying how likely you are to transition from a state Sa to a state Sb\nk-length vector saying how likely you are to start off in each of the states.\n\nA probability model that lets you compute Pr[O|S] â€” the probability of seeing observation O if we assume that the underlying state is S.\n\nUnlike the markov chain, which has a fixed format, the model for Pr[O|S] can be arbitrarily complex. In many HMMs though Pr[O|S] is pretty simple: each state S is a different loaded die, and the Pr[O|S] are its probabilities of landing on each side.\n\n\nBaum-Welch algorithm\n\nSpecific case of the EM algorithm\n\nAs with the EM algorithm, solutions are only locally optimal\n\nMeaning youâ€™re results will vary for each model you run (i.e.Â a non-convex problem)\n\n\nStep summary\n\nGuess at what the state labels are and train an HMM using those guesses\n\nThe guess is usually about the parameters of the HMM and not actually the states.\n\nTypically random values from a dirichlet distribution\n\n\nUse the trained HMM to make better guesses at the states, and re-train the HMM on those better guesses\n\nUses the Forward-Backward Algorithm\n\nContinue process until the trained HMM stabilizes.\n\nForward-Backward Algorithm\n\nFind the most likely state for each timestep\n\nUnlike the Viterbi algorithm which computes the mostly likely sequence of states\n\nIn practice, both algorithms only disagree about 5% of the time\n\n\n\n\nMitigating the local optimum issue\n\nYou can simplify your HMM.\n\nThe number of local optima can grow exponentially with the number of parameters in your HMM. If you reduce the parameter size you can reduce the number of local optima there are to fall into.\n\nExamples\n\nUsing a 2-state rather than a 3-state HMM.\nIf your target is multinomial, collapse categories so that the cardinality is smaller.\n\n\n\nUse business logic or domain expertise to have an initial HMM that is similar to what you expect the ultimate outcome to be.\n\nDifferent local optima often have large qualitative differences between each other, so on a gross level the HMM you ultimately converge on is likely to resemble the one you started with.\nDo make sure to have your guess be partly random though â€” there could be several local optima that are consistent with your business intuitions, and if so you want to know about it.\n(Sounds like formulating a prior)\n\n\nMarkov Switching Dynamic Regression (MSDR)\n\nHMM for times series which uses dynamic regression as the observed model.\nNotes from A Wormâ€™s Eye-View of the Markov Switching Dynamic Regression Model\nBasic formula for each time step, t\n\nÎ¼^tj: final prediction for time step, t\n\nThe j subscript is just saying that the hidden state variable st is part of the final prediction\n\nst: hidden random variable thatâ€™s made-up of states (aka regimes) and helps determine the final prediction, Î¼^\n\nTheoretically, a change in value of st impacts the distributional parameters (e.g.Â mean, variance) of the observed variable, yt\nst is â€œhiddenâ€ because we do not know exactly when it changes its regime.\n\nIf we knew which regime was in effect at each time step, we could simply make st a predictor variable.\n\n\nPoisson\n\nProcess\n\nFit and get predictions from a dynamic regression model\n\nm predictor variables and k states (i.e.Â a complete set of parameters is estimated for each state at each time step)\n\nGet the final prediction for time step, t, by calculating the expected value\n\n^y are the predictions from the observed model\nP(st = j) are the state probabilities\n\ne.g.Â 2 -states\nWhere each Ï€tj is a row of the transition matrix\n\n\nUses Maximum Likelihood Estimation (MLE) or Expectation Maximization (EM) to maximize the joint probability density.\n\nEstimated parameters:\n\ntransition matrix (for each time step)\n\nDuring estimation, cells are allowed to range from -âˆ to âˆ, but after itâ€™s completed, theyâ€™re normalized to [0,1] (see link)\n\nregression coefficients (for each state)\n\nUnclear to me how you estimate multiple coefficients with the same data. Maybe the likelihood for a Î² is maximized for a given (estimated?) transition matrix.\n\n(residual) variance (for each state)\n\nExample: Using MLE for a gaussian distribution\n\nFor a Poisson DGP\n\n\n\nExample\n\nDescription\n\nDependent variable: Personal Consumption Expenditures % Change Month over Month (M-o-M)\nIndependent variable: Consumer Sentiment index % Change Month over Month (M-o-M)\n\nModel Equation\n\n2-states\nObserved model is a linear regression\n\nModel Summary (python)\n\nTransition Matrix probabilities have pvals and CIs\np[0-&gt;0] is cell p00 and p[1-&gt;0] is cell p10 in the transition matrix\n\nunknown_cell_values = (1 - known_cell_values) for each row\n\n\nInterpretation\n\nVariance of States vs Transition Probabilities\n\nWhen Personal Consumption Expenditures are in a low variance regime, they tend to switch to a high variance regime less than 2% of the time.\nWhen Personal Consumption Expenditures are in a high variance regime, they tend to switch to a low variance regime with roughly 20% probability.\nThis variance is â€œsigma2â€ in the model summary in the Regime Parameters sections.\n\nMe: This would be the residual variance which is usually a GOF thing (i.e.Â variance of the points around the regression line). In finance, variance is a volatility measure and I guess this can be thought of in that way too. I guess it would the volatility around a trend line.\n\n\nSmoothed State Probabilities for each time step\n\nBottom two chartsâ€™ y-axes are probabilities, so chart 2â€™s dips in proability are when the hmm is in stage 1 (confirmed by looking at chart 3)\nAccessed via msdr_model_results.smoothed_marginal_probabilities[0] ([1] for state 1)\nAfter 2000, we see that often the Markov state model is in the high variance state towards the end a recession. (bottom chart)\n\n\n\n\nMarkov Switching Auto Regression (MSAR)\n\nAdds a fraction of the residual from the previous step to the MSDR model"
  },
  {
    "objectID": "qmd/surveys-analysis.html#sec-surveys-anal-misc",
    "href": "qmd/surveys-analysis.html#sec-surveys-anal-misc",
    "title": "70Â  Analysis",
    "section": "70.1 Misc",
    "text": "70.1 Misc\n\nQuestionnaire data can be modelled using ordinal regression (Liddell & Kruschke, 2018)\nPairwise Likelihood (original, mathy paper, factor analysis w/ordinal data paper, usage on customer survey data ($) paper)\n\na special case of composite likelihood methods that uses lower-order conditional or marginal log-likelihoods instead of the full log-likelihood\n\nWhen the number of items is greater than five (p &gt; 5), Full Information Likelihood (FIML) is only feasible when the Item Response Theory (IRT) framework is used. However, even in IRT, FIML becomes very computationally heavy as the number of factors increases. Using Pairwise likelihood is a suitable alternative\n\nIgnoring the survey design features (such as stratification, clustering and unequal selection probabilities) can lead to erroneous inferences on model parameters because of sample selection bias caused by informative sampling.\nIt is tempting to expand the models by including among the auxiliary variables all the design variables that define the selection process at the various levels and then ignore the design and apply standard methods to the expanded model. The main difficulties with this approach are the following:\n\nNot all design variables may be known or accessible to the analyst\nToo many design variables can lead to difficulties in making inference from the expanded model\nThe expanded model may no longer be of scientific interest to the analyst\n\ndesign-based approach can provide asymptotically valid repeated sampling inferences without changing the analystâ€™s model.\nresampling methods, such as the jackknife and the bootstrap for survey data, can provide valid variance estimators and associated inferences on the census parameters\n\nIn other cases, it is necessary to estimate the model variance of the census parameters from the sample. The estimator of the total variance is then given by the sum of this estimator and the re-sampling variance estimator.\n\nExample: in an education study of students, schools (first stage sampling units) may be selected with probabilities proportional to school size and students (second stage units) within selected schools by stratified random sampling.\n\nsee Surveys, Sampling Methods &gt;&gt; Probabilistic Sampling Methods &gt;&gt; Multi-Stage Sampling\nAgain, ignoring the survey design and using traditional methods for multi-level models can lead to erroneous inferences in the presence of sample selection bias\n\nIn the design-based approach, estimation of variance component parameters of the model is more difficult than that of regression parameters.\n\n\nasymptotically valid even when the sample sizes within sampled clusters (level 1 units) are small, unlike some of the existing methods, but knowledge of the joint inclusion probabilities within sampled clusters is required.\n\nLarge variations in cluster sizes may cause an issue, see Lumley\n\n\nExample of debiasing a dataset by other means than by weighting by population\n\nThe economist created a death-by-covid risk probability model. They had a bunch of medical records with patient comorbidities, age, gender, positive test, hospitalized, death/no death, etc. (people with other illnesses already) and were worried that the people who tested positive but just stayed at home (i.e.Â no medical records like younger people). Not correcting for this bias of undetected cases would bias their risk probabilities.\n\nFailing to correct this bias would lead to underestimating the risks associated with comorbidities, and to overestimating the risks among those without listed conditions.\n\nThey used an estimated metric, national cfr per age group per gender per week (separate dataset from CDC which has stats on groups with and without medical records). When a weekâ€™s sample cfr didnâ€™t match that weekâ€™s national cfr, they would randomly sample people in the dataset who didnâ€™t meet the selection criteria (i.e.Â positive covid test) and assign them a positive test. They continued to add these reclassified people to that weekâ€™s sample until the sample cfr matched the national cfr. Thus, debiasing theyâ€™re data set.\nThought this was an interesting case because it used a estimated metric to â€œweightâ€ subgroups within their sample to make it more representative of the â€œtrueâ€ population.\n\nAlso see Projects &gt;&gt; Rolling COVID-19 CFR\n\nhttps://www.economist.com/graphic-detail/2021/03/11/how-we-built-our-covid-19-risk-estimator\n\nCeiling or floor effects occur when the tests or scales are relatively easy or difficult such that substantial proportions of individuals obtain either maximum or minimum scores and that the true extent of their abilities cannot be determined.\n\nSounds like censoring (See Regression, Other &gt;&gt; Censored and Truncated Data)\nCeiling or floor effects alone would induce, respectively, attenuation or inflation in mean estimates. And both ceiling and floor effects would result in attenuation in variance estimates.\n{DACF}\n\nRecovers mean and variance given data with ceiling/floor effects\nAllows for mean comparison tests such as t-test and ANOVA for data with ceiling/floor effects"
  },
  {
    "objectID": "qmd/surveys-analysis.html#sec-surveys-anal-wts",
    "href": "qmd/surveys-analysis.html#sec-surveys-anal-wts",
    "title": "70Â  Analysis",
    "section": "70.2 Weights",
    "text": "70.2 Weights\n\nMisc\n\nSurveys responses are often biased due to coverage error, sampling error and non-response bias. Weighting is often an important step when analyzing survey data. For each unit in the sample (e.g.Â respondent to a survey), we attach a weight that can be understood as the approximate number of people from the target population that this respondent represents. Weights adjust the sample distribution more towards the population distribution\n\nThe green bars show the sample with weights applied.\nThe weighted average will also be less biased to the extent the response is correlated with respondentâ€™s age.\nThe weighted distribution is not fully corrected, mainly because of bias-variance considerations\n\nPackages\n\n{{balance}} - see section below\n{CBPS} - Covariate Balancing Propensity Scores (CBPS)\n\nAlso see\n\nTypes &gt;&gt; Covariate Balancing Propensity Scores (CBPS)\n{{balance}} &gt;&gt; Steps &gt;&gt; Calculate Weights &gt;&gt; Methods\n\n\n\n\nTypes\n\nFrequency Weights\n\nSteps\n\nRemove the duplicate observations\n\nDuplicates donâ€™t add any additional information\n\nWeight each observation by the square root of number of times it appeared in the original dataset, \nSSE needs to be divided by n - k + 1\n\nwhere n is the number of observations in the original dataset and k is the number of predictors in the regression\n\n\n\nImportance Weights - focus on how much each row of the data set should influence model estimation. These can be based on data or arbitrarily set to achieve some goal.\nAnalytic Weights - If a data point has an associated precision, analytic weighting helps a model focus on the data points with less uncertainty (such as in meta-analysis).\n(Inverse) Probability Weights (wiki) - {{balance}} refers to this type as â€œinverse propensity weightsâ€\n\nAlso see below, {{balance}} &gt;&gt; Steps &gt;&gt; Adjust &gt;&gt; Options\nUsed to reduce bias when respondents have different probabilities of selection Adjusts a non-random sample to represent a population by weighting the sample units. It assumes two samples: A sample of respondents to a survey (or in a more general framework, a biased panel).\n  A sample of a target population, often referred to as \"reference sample\" or \"reference survey.\"\n      This sample includes a larger coverage of the population or a better sampling properties in a way that represents the population better.\n\n      It often includes only a limited number of covariates and doesn't include the outcome variables (the survey responses).\n\n      In different cases it can be the whole target apopulation (in case it is available), a census data (based on a survey) or an existing survey.\nPropensity Score - the probability to be included in the sample (the respondents group) conditioned on the characteristics of the unit Let pi = Pr{i âˆˆ S | xi} with i = 1 â€¦ n.\n  i is the unit (aka respondent), n is the total number of respondents, S is the sample of respondents\n      X is a set of covariates that are available for the sample and the target population\n\n  piâ€‹ is the estimated probabilities of being in the sample using logistic regression\n      Data includes both sample and target population\n\n      outcome is a binary variable (1/0): 1 = Sample, 0 = Target\n\n      covariates are X\n\n  Also see [Econometrics, General](Econometrics, General) &gt;&gt; Propensity Score Matching\nCalculate Weights \n\ndi is â€¦?\n\n\nCovariate Balancing Propensity Scores (CBPS)\n\nWhen estimating propensity score, there is often a process of adjusting the model and choosing the covariates for better covariate balancing. The goal of CBPS is to allow the researcher to avoid this iterative process and suggest an estimator that is optimizing both the propensity score and the balance of the covariates together.\nMain advantage is in cases when the researcher wants better balance on the covariates than traditional propensity score methods - because one believes the assignment model might be misspecified and would like to avoid the need to fit followup models to improve the balance of the covariates.\nAlso see\n\nMisc &gt;&gt; packages &gt;&gt; {CBPS}\n{{balance}} &gt;&gt; Steps &gt;&gt; Adjust &gt;&gt; Options\n\n\n\n{{balance}}\n\nA Python package for adjusting biased data samples.\n\nProvides eda, weights calculation, comparison of variables before and after weighting\n\nSteps:\n\nEDA:\n\nUnderstanding the initial bias in the sample data relative to a target population we would like to infer\nSummary Statistics\n\nThe limitation of using the mean is that it is not easily comparable between different variables since they may have different variances.\nASMD (Absolute Standardized Mean Deviation) measures the difference between the sample and target for each covariate.\n\nIt uses weighted average and std.dev for the calculations (e.g.: to take design weights into account).\nThis measure is the same as taking the absolute value of Cohenâ€™s d statistic (also related to SSMD), when using the (weighted) standard deviation of the population.\n\nNot sure why it says â€œ(weighted)â€ when itâ€™s the std.dev of the population since weights are applied to sample data. Maybe the population estimate is itself a weighted calculation.\nGuidelines on effect size for Cohenâ€™s D should apply here, too.\nFor categorical variables, the ASMD can be calculated as the average of the ASMD applied to each of the one-hot encoding of the categories of the variable\n\nAlso see\n\nPost-Hoc Analysis, general &gt;&gt; Bayesian &gt;&gt; Cohenâ€™s D, SSMD\nPost-Hoc Analysis, Multilevel &gt;&gt; Cohenâ€™s D\n\n\n\nVisualizations\n\nQ-Q plot (continuous)\n\nThe closer the line is to the 45-degree-line the better (i.e.: the less bias is observed in the sample as compared to the target population).\n\nBar Plots (categorical)\n\n\nCalculate Weights:\n\nAdjust the data to correct for the bias by producing weights for each unit in the sample based on propensity scores\nPreprocessing (â€œusing best practices in the fieldâ€):\n\nTransformations are done on both the sample dataframe and the target dataframe together\nMissing valuesÂ  - adds a column â€˜_is_naâ€™ to any variable that contains missing values\n\nConsidered as a separate category for the adjustment\n\nFeature Engineering\n\nContinuous - bucketed into 10 quantiles buckets.\nCategorical - rare categories (with less than 5% prevalence) are grouped together so to avoid overfitting rare events\n\n\nMethods\n\nInverse Propensity Weighting (IPW)\n\nCoefficients, parameters of the fitted models are available\nSee above, Types &gt;&gt; (Inverse) Probability Weights Using LASSO logistic regression keeps the inflation of the variance as minimal as possible while still addressing the meaningful differences in the covariates between the sample and the target\nDesign Effect (max_de) for tuning penalty factor, Î», and the trimming ratio parameter\n\nA measure of the expected impact of a sampling design on the variance of an estimator for some parameter\nmax_de=X - the regularization parameter and the trimming ratio parameter are chosen by a grid search over the 10 models with the max design effect value\n\nDefault is 1.5\nAssumption: larger design effect often implies better covariate balancing.\nWithin these 10 models, the model with the smallest ASMD is chosen.\n\nmax_de=None - optimization is performed by cross-validation of the logistic model\n\npenalty factor, Î», is chosen when the MSE is at most 1 standard error from the minimal MSE\nthe trimming ratio parameter is set by the user, and default to 20\n\n\n\nCovariate Balancing Propensity Scores (CBPS)\n\nEstimates the propensity score in a way that optimizes prediction of the probability of sample inclusion as well as the covariates balance.\nAlso see\n\nTypes &gt;&gt; Covariate Balancing Propensity Scores (CBPS)\nMisc &gt;&gt; packages &gt;&gt; {CBPS}\n\nDesign Effect (max_de)\n\na measure of the expected impact of a sampling design on the variance of an estimator for some parameter\ndefault is 1.5; If â€œNoneâ€, then optimization is unconstrained\n\n\nPost-Stratification\n\nPost-processing of the weights:\n\nTrims - trims the weights in order to avoid overfitting of the model and unnecessary variance inflation.\n\nOptions\n\nMean-Ratio - ratio from above according to which the weights are trimmed by mean(weights) * ratio. Default is 20.\nPercentile - winsorization is applied\n\n\nNormalizing to population size - weights can be described as approximating the number of units in the population this unit of the sample represents.\n\n\nCompare data with and without weights\n\nEvaluate the final bias and the variance inflation after applying the fitted weights.\nCompares ASMD score (See EDA), Design Effect, Model proportion deviance explained (if inverese propensity weighting method was used)\n\nASMD: since categorical variables are hot-encoded, a comparison (with/without weights) is made for each level\n\nComparison of means is available\nSimilar charts used in EDA are available that show a comparison between weighted/not weighted\nResponse Rates with/without weights\nEffects on outcome variable"
  },
  {
    "objectID": "qmd/surveys-analysis.html#sec-surveys-anal-modeling",
    "href": "qmd/surveys-analysis.html#sec-surveys-anal-modeling",
    "title": "70Â  Analysis",
    "section": "70.3 Modeling",
    "text": "70.3 Modeling\n\nTidymodels\n\nTidymodels\n\nFrequency weights are used for all parts of the preprocessing, model fitting, and performance estimation operations.\n\nThis includes v-fold CV splits for now (see Using case weights with tidymodels for details)\n\nImportance weights only affect the model estimation and supervised recipes steps (i.e.Â depend on the outcome variable).\n\nNot used with yardstick functions for calculating measures of model performance.\n\n\nExample: Importance weights\n\n\ntraining_sim &lt;-\nÂ  training_sim %&gt;%Â \nÂ  mutate(\nÂ  Â  case_wts = ifelse(class == \"class_1\", 60, 1),\nÂ  Â  case_wts = parsnip::importance_weights(case_wts)\nÂ  )\n\nset.seed(2)\nsim_folds &lt;- vfold_cv(training_sim, strata = class)\n\nsim_rec &lt;-Â \nÂ  recipe(class ~ ., data = training_sim) %&gt;%Â \nÂ  step_ns(starts_with(\"non_linear\"), deg_free = 10) %&gt;%Â \nÂ  step_normalize(all_numeric_predictors())\n\nlr_spec &lt;-\nÂ  logistic_reg(penalty = tune(), mixture = 1) %&gt;%\nÂ  set_engine(\"glmnet\")\n\nlr_wflow &lt;-\nÂ  workflow() %&gt;%\nÂ  add_model(lr_spec) %&gt;%\nÂ  add_recipe(sim_rec) %&gt;%Â  Â  Â  Â \nÂ  add_case_weights(case_wts)\n\ncls_metrics &lt;- metric_set(sensitivity, specificity)\ngrid &lt;- tibble(penalty = 10^seq(-3, 0, length.out = 20))\nset.seed(3)\nlr_res &lt;-\nÂ  lr_wflow %&gt;%\nÂ  tune_grid(resamples = sim_folds, grid = grid, metrics = cls_metrics)\nautoplot(lr_res) # calibration curves\n\nDescription\n\nBinary outcome; lasso\nâ€œclass_1â€ (80 obs) is severely imbalanced with â€œclass_2â€ (4920)\n\nclass_1 observations get a weight of 60 since 4920/80 = 61.5 which is ~ 60\n\n\nrecipe will automatically detect the weights (pretty sure it doesnâ€™t matter whether on no â€œcase_wtsâ€ is included in formula, e.g.Â class ~ .)\n\nSince these are performance weights and step_ns and step_normalize donâ€™t depend on the outcome variable (i.e.Â supervised), case weights are not used in these transformations.\n\nSteps\n\nadd â€œcase_wtsâ€ variable to df\nuse add_case_weights function in workflow code\n\nRemove the case weights from a workflow\n\nlr_unwt_wflow &lt;-\nÂ  lr_wflow %&gt;%\nÂ  remove_case_weights()\n\nUseful if you want to make a comparison between models"
  },
  {
    "objectID": "qmd/surveys-census-data.html",
    "href": "qmd/surveys-census-data.html",
    "title": "71Â  Surveys, Census Data",
    "section": "",
    "text": "TOC\n\nMisc\nGeographies\nAmerican Community Survey (ACS)\n\nMisc\n\nFIPS GEOID\npopular variable calculations from variables in ACS\nCensus Geocoder (link)\n\nEnter an address and codes for various geographies are returned\nBatch geocoding available for up to 10K records\n\nCodes for geographies returned in a .csv file\n\n\nTIGERweb (link)\n\nAllows you to get geography codes by searching for an area on a map\nOnce zoomed-in on your desired area, you turn on geography layers to find the geography code for your area.\n\nUS Census Regions\n\nGeographies \n\nMisc\n\nACS Geography Boundaries by Year (link)\n\nTypes\n\nLegal/Administrative\n\nCensus gets boundaries from outside party (state, county, city, etc.)\ne.g.Â election areas, school districts, counties, county subdivisions\n\nStatistical\n\nCensus creates these boundaries\ne.g.Â regions, census tracts, ZCTAs, block groups, MSAs, urban areas\n\n\nNested Areas\n\nCensus Tracts\n\nAreas within a county\nAround 1200 to 8000 people\nSmall towns, rural areas, neighborhoods\n** Census tracts may cross city boundaries **\n\nBlock Groups\n\nAreas within a census tract\nAround 600 to 3000 people\n\nCensus Blocks\n\nAreas within a block group\nNot for ACS, only for the 10-yr census\n\n\nPlaces\n\nMisc\n\nOne place cannot overlap another place\nExpand and contract as population or commercial activity increases or decreases\nMust represent an organized settlement of people living in close proximity.\n\nIncorporated Places\n\ncities, towns, villages\nUpdated through Boundary and Annexation Survey (BAS) yearly\n\nCensus Designated Places (CDPs)\n\nAreas that canâ€™t become Incorporated Places because of state or city regulations\nConcentrations of population, housing, commericial structures\nUpdated through Boundary and Annexation Survey (BAS) yearly\n\n\nCounty Subdivisions\n\nMinor Civil Divisions (MCDs)\n\nLegally defined by the state or county, stable entity. May have elected government\ne.g.Â townships, charter townships, or districts\n\nCensus County Divisions (CCDs)\n\nno population requirment\nSubcounty units with stable boundaries and recognizable names\n\n\nZip Code Tabulation Areas (ZCTAs)\n\nMisc\n\n{crosswalkZCTA} - Contains the US Census Bureauâ€™s 2020 ZCTA to County Relationship File, as well as convenience functions to translate between States, Counties and ZIP Code Tabulation Areas (ZCTAs)\n\nApproximate USPS Code distribution for housing units\n\nThe most frequently occurring zip code within an census block is assigned to a census block\nThen blocks are aggregated into areas (ZCTAs)\n\nZCTAs do NOT nest within any other geographies\n\nI guess the aggregated ZCTA blocks can overlap block groups\n\n2010 ZCTAs exclude large bodies of water and unpopulated areas\n\n\nAmerican Community Survey\n\nAbout\n\nYearly estimates based on samples of the population\n\nTherefore a Margin of Error (MoE) is included with the estimates.\n\nDetailed social, economic, housing, and demographic characteristics\ncensus.gov/acs\n\nACS Release ScheduleÂ  (releases)\n\nSeptember - 1-Year Estimates (from previous yearâ€™s collection)\n\nEstimates for areas with populations of &gt;65K\n\nOctober - 1-Year Supplemental Estimates\n\nEstimates for areas with populations between 20K-64999\n\nDecember - 5-Year Estimates\n\nEstimates for areas including census tract and block groups\n\n\nData Collected\n\nPopulation\n\nSocial\n\nAncestry, Citizenship, Citizen Voting AgeÂ  Population, Disability, Education Attainment, Fertility, Grandparents, Language, Marital Status, Migration, School Enrollment, Veterans\n\nDemographic\n\nAge, Hispanic Origin, Race, Relationship, Sex\n\nEconomic\n\nClass of worker, Commuting, Employment Status, Food Stamps (SNAP), Health Insurance, Hours/Week, Weeks/Year, Income, Industry & Occupation\n\n\nHousing\n\nComputer & Internet Use, Costs (Mortgage, Taxes, Insurance), Heating Fuel, Home Value, Occupancy, Plumbing/Kitchen Facilities, Structure, Tenure (Own/Rent), Utilities, Vehicles, Year Built/Year Movied In"
  },
  {
    "objectID": "qmd/surveys-design.html#sec-surv-design-misc",
    "href": "qmd/surveys-design.html#sec-surv-design-misc",
    "title": "68Â  Design",
    "section": "68.1 Misc",
    "text": "68.1 Misc\n\nFor sampling methods, see Surveys, Sampling Methods\nItem-specific surveys perform better/more reliable than agree/disagree surveys. The reason behind this is that participants are more certain while choosing a position in item-specific surveys compared to agree/disagree surveys\nFor self-report surveys, ceiling/floor effects on score distributions disaappear when response options are greater than 2 or 3 (Thread, Paper)\nSurvey Monkey sample size calculator\nIf asking the Age or Income of a respondant:\n\nIf calculating mean or median, exact numbers are best but respondants are usually hesitant to give out exact numbers for such data.\nRanges give less accurate averages but respondants are more likely to answer.\nGroup based on population your studying\n\nIf itâ€™s the general population, $20k or less is a good first rung; $21â€“39k is next; from there: $40â€“69k, $70â€“99k; $100â€“150k; and $150+\n\nIf itâ€™s college students, the ranges will be lower.\n\nBreaks between groups should line-up with known charcteristics of the population youâ€™re studying\n\n\nUse the phrases, jargon, and emotions that your customers are familiar with.\n\nGet a sense of this by getting on the phone and talking to your customers. Or run focus groups. Or run some on-site surveys.\n\nBest practices for the format of common demographic questions, link"
  },
  {
    "objectID": "qmd/surveys-design.html#sec-surv-design-terms",
    "href": "qmd/surveys-design.html#sec-surv-design-terms",
    "title": "68Â  Design",
    "section": "68.2 Terms",
    "text": "68.2 Terms\n\nSelf-Report Study - Type of survey, questionnaire, or poll in which respondents read the question and select a response by themselves without any outside interference. A self-report is any method which involves asking a participant about their feelings, attitudes, beliefs and so on.\nCeiling and Floor Effects - An artificial lower limit on the value that a variable can attain, causing the distribution of scores to be skewed.\n\nExample: The distribution of scores on an ability test will be skewed by a floor effect if the test is much too difficult for many of the respondents and many of them obtain zero scores.\nExample: The distribution of scores on an ability test will be skewed by a ceiling effect if the test is much too easy for many of the respondents and many of them obtain perfect scores"
  },
  {
    "objectID": "qmd/surveys-design.html#sec-surv-design-soe",
    "href": "qmd/surveys-design.html#sec-surv-design-soe",
    "title": "68Â  Design",
    "section": "68.3 Sources of Error",
    "text": "68.3 Sources of Error\n\n\nMisc\n\nNotes from Total Survey Error: Design, Implementation, and Evaluation (d/l pdf to see figures)\n\nsurvey error is defined as the deviation of a survey response from its underlying true value\nsurvey accuracy is defined as the deviation of a survey estimate from its underlying true parameter value\nTotal Survey error - accumulation of all errors that may arise in the design, collection, processing, and analysis of survey data. It includes sampling variability, interviewer effects, frame errors, response bias, and non-response bias\n\nNon-Sampling Error\n\nSpecification error occurs when the concept implied by the survey question differs from the concept meant to be measured in the survey.\n\nOften caused by poor communication between the researcher, data analyst, or survey sponsor and the questionnaire designer.\n\nCoverage or Frame error typically results from the frame construction process. Discrepancies between the (theoretical) target population of a survey and the frame which is used to draw a sample are equivalent to statistical errors.\n\nSee Surveys, Sampling Methods &gt;&gt; Terms&gt;&gt; sampling frame\nIn practice, frames are not error-free:\n\nthey can never encompass the whole target population (because it always takes time to administrativally record an individual in a register),\nthey also contain individuals which are no longer eligible (e.g.Â individuals who left the country and may keep recorded in the Register several months after they left).\ncoverage errors or frame errors.\n\nExamples\n\nSome units may be omitted or duplicated an unknown number of times\nSome ineligible units may be included on the frame, such as businesses that are not farms in a farm survey.\n\n\nNonresponse error - When the reason for nonresponse is related to the missing value, parameter estimates can be biased when nonresponse is not accounted for. Includes:\n\nunit nonresponse - sampling unit does not respond to any part of the questionnaire\n\ne.g.Â calling a person and them choosing not to answer or participate in the the survey\n\nitem nonresponse - the questionnaire is partially completed.\n\nMeasurement error occurs when the method of obtaining the measurement affects the recorded value, often involving simultaneously the respondent, the interviewer, and the survey questionnaire.\nProcessing error refers to errors that arise during the data processing stage, including errors in the editing of the data, data encoding, the assignment of survey weights, and tabulation of the survey data.\n\nSampling Error - - caused by collecting partial information over a fraction of the population rather than the whole population itself.\n\nSampling scheme (e.g., multistage or multiple-phase sample)\nSample size\nChoice of estimator (e.g., a ratio or regression estimator, levels of post-stratification)\n\n\nKeeping sampling errors under control\n\nSurvey questionnaires must be prepared with utmost care, intensively pre-tested and field-tested in order to detect issues in question wording, routing problems or any other inconsistency\nModes of data collection must be chosen and combined judiciously in order to get most people to cooperate\nInterviewers must be carefully recruited and properly trained\nCommunication and contact strategies towards participants must be designed and adapted in order to reach highest participation."
  },
  {
    "objectID": "qmd/surveys-design.html#sec-surv-design-respfor",
    "href": "qmd/surveys-design.html#sec-surv-design-respfor",
    "title": "68Â  Design",
    "section": "68.4 Response Formats",
    "text": "68.4 Response Formats\n\nitem-specific (IS) - multiple choice response format\n\nAgree/Disagree (A/D) - response format where the response are degrees of strength of agreement or disagreement"
  },
  {
    "objectID": "qmd/surveys-design.html#sec-surv-design-respsc",
    "href": "qmd/surveys-design.html#sec-surv-design-respsc",
    "title": "68Â  Design",
    "section": "68.5 Response Scales",
    "text": "68.5 Response Scales\n\n68.5.1 Misc\n\nThe default values and ranges you use\nExamples:\n\nage: 5 yr default range (e.g.Â 20â€“25) instead of 10 yrs ranges\n\n\n\n\n68.5.2 Types\n\nDichotomous scales\n\nprecise data, but they donâ€™t allow for nuance in respondentsâ€™ answers.\nExamples: â€œYesâ€ or â€œNoâ€; â€œTrueâ€ or â€œFalseâ€; â€œFairâ€ or â€œUnfairâ€\n\nQuantitative Scales\n\nRating Scales\n\nProvides more range than Dichotmous scales. Too generic for attitudes\n1â€“10; 1â€“7; 1â€“5 (or Likert scale, also see below)\n\nlabel all number or none\n\na 1â€“5 scale presented verbal descriptions for only the 1 and 5 endpoints and this led more people to choose the endpoints.\nA â€œschool gradeâ€ scale is more reliable than other types of word labels\n\n\n\nOrdinal and interval scales\n\nWith ordinal, the numbers just have an intrinsic order\nWith interval, the distance between the numbers must also be equal in terms of context\n\ne.g.Â rating something a 2 vs 1 doesnâ€™t mean that being a 2 is â€œtwice as goodâ€ as being a 1\n\nunless being 5 is also twice as good as being a 4, etc.\n\n\nThere is no practical difference between ordinal or interval scales\n\nRatio Scales\n\nwhere there is a true zero and equal intervals between neighboring points. Unlike on an interval scale, a zero on a ratio scale means there is a total absence of the variable you are measuring. Length, area, and population are examples of ratio scales\n\n\nSemantic differential scales\n\n\nGather data and â€œinterpret based on the connotative meaning of the respondentâ€™s answer.â€\nUsually have dichotomous words at either end of the spectrum\nThe more quantifiable the information is (behavior questions, for instance), the smaller the range should be.\n\nWhen you want to measure attitudes or feelings, using a 5- or 7-point semantic differential scale is a good strategy.\n\n\n\n\n\n68.5.3 Likert Scale\n\nNotes from: Likert Scales: Friend or Foe?\nquantitative, rating scale\nAvoid using agree/disagree wording (see biases section below)\nExamples:\n\n5-point:\n\n[1] Strongly Disagree, [2] Disagree\n[3] (4pt removes this response) Neither Disagree Nor Agree\n[4] Agree, [5] Strongly Agree\n\n7-point:\n\n[1] Strongly disagree, [2] Disagree, [3] Somewhat disagree\n[4] Neither agree nor disagree\n[5] Somewhat agree, [6] Agree, [7] Strongly agree\n\nMore examples, Link\n\nIssues\n\nOrdinal and not Interval\n\nAnswers are not all equidistant\n\nRespondents may perceive (4) Strongly Agree and (3) Agree very similarly and thus the difference between these two options might be much smaller than the difference between (3) Agree and (2) Disagree, despite having the same distance.\n\n\nNumerical values assigned to the response options cannot be treated as interval data\n\nParametric statistics (e.g., mean, standard deviation) and parametric statistical methods (e.g., summing up individual questions to find a total survey score, running a regression with survey scores) would NOT yield valid results\n\nAcceptable Analysis Methods:\n\nMedian and Mode\nOrdinal regression\nChi-Square Test of Independence\nItem Response Theory (IRT) modeling\nGraphical Tools (e.g.Â bar charts and correlation matrix plots)\n\n\nAdding a neutral option (i.e.Â Neither Disagree Nor Agree)\n\nMay increase the accuracy of survey data because respondents who do not have a strong preference may prefer to select the neutral response option, instead of randomly selecting a response option or skipping the question\nMay produce a bias: research shows that respondents often see the visual midpoint of a scale as representing the middle response option\nSolutions:\n\nuse a Likert scale that consists of an even number of response options without a neutral option\nselect survey questions for which respondents would not select the neutral option very easily.\n\n\nChoosing the number of responses\n\nResearch shows that Likert scales with 2 to 5 response options often yield precise results, although smaller numbers of response options may reduce the measurement precision of a survey\nResearchers found that there are no clear advantages of using beyond 6 response options on a Likert scale\n\nPositively and negatively worded questions together\n\nTypically used to prevent response bias\nPositively and negatively worded questions are not necessarily mirror images of each other.\n\nTherefore, when analyzing survey data, reverse-coding the Likert scale for negatively worded questions (e.g., 1-Strongly agree; 2-Agree; 3-Disagree; 4-Strongly disagree) may not necessarily put these questions in the same direction as positively worded questions\n\nResearch shows that negative wording may confuse respondents, leading to less accurate responses to the survey questions\n\nMay be increasing response bias instead of reducing it.\n\nStudies indicate that respondents are more likely to disagree with negatively worded questions than to agree with positive ones\n\nExample\n\nA respondent who would select Agree for â€œMy room was cleanâ€ might prefer to select Strongly Disagree for â€œMy room was dirtyâ€.\n\n\nSolutions:\n\nKeep the number of negatively worded questions minimal, while taking the impact of negatively worded questions on responses into account.\n\n\nSliding Scales\n\nlink\n\n\n\n\n\n68.5.4 Guttman Scale\n\n\nDichotomous or Likert\nGradually increases in specificity. The intent of the scale is that the person will agree with all statements up to a point and then will stop agreeing.\nThe scale may be used to determine how extreme a view is, with successive statements showing increasingly extremist positions.\nAlso, a useful tool for measuring satisfaction\nIf needed, the escalation can be concealed by using intermediate questions.\n\n\n\n68.5.5 Net Promoter Score (NPS)\n\nCustomer loyalty metric, 0-9 (or 1-10)\nâ€œHow likely is it that you would recommend our company/product/service to a friend or colleague?â€\nNPS = percent_promoters - percent_detractors\nResponses get broken up into 3 groups\n\nPromoters (9â€“10). These are your happiest and most loyal customers who are most likely to refer you to others. Use them for testimonials, affiliates, etc. These customers are key to business growth and thereby sustaining their customer experience is critical to the brand.\nPassives or Neutrals (7â€“8). These customers are happy but are unlikely to refer you to friends. They may be swayed to a competitor fairly easily. A business should look at ways and means of upgrading neutrals to promoters by understanding their requirements.\nDetractors (0â€“6). Detractors are unhappy customers who can be dangerous for your brand, spreading negative messages and reviews. Figure out their problems and fix them.\n\nAny NPS that is positive is usually perceived as good, and an NPS score of 50+ is considered excellent. The range of NPS is from -100 (all detractors) to +100 (all promoters).\n**Donâ€™t use as a single predictor of customer loyalty**\n\nA customer might actually be very enthusiastic about the product, but they just might not ever feel the urge to recommend hemorrhoid cream to their pals\n\nCombine with other measures\n\nAsk follow-up questions.\n\nPromoters. Whatâ€™s your favorite part about our product/service?\nPassives. What would make you love us?\nDetractors. What could we do to improve your experience?\n\nCombine it with user research.\n\nusability testing and other common conversion research techniques\nUser testing\nCustomer surveys\nLive chat\nHeat maps (e.g.Â buttons clicked on websites, scrolling actions)\n\nFind and fix issues.\n\nuse info to prioritize projects that enhance user experience\n\nMarket to promoters. Use Promoters, Passives, and Detractors as a segmentation tool.\n\nGive free stuff to â€œpromotersâ€ to incentize more buying or advocates to others by sharing on FB or twitter or write a review.\nFind correlations between certain product actions (heatmaps) and a higher NPS. This can help deduce what your productâ€™s â€œmagic momentâ€ is when your users are truly activated and likely to derive delight from your product. Then you can focus on product optimizations to get more of your customer base to this point\n\n\nIssues\n\nresponse rate for surveys is relatively low\ntime-consuming and costly affair to collect a sizeable amount of survey data\nrelatively high-cost outlay associated with sending out surveys\nresponses are typically quantitative. There is rarely qualitative information explaining the reason for the response\n\nAlternative: Use sentiment analysis on customer reviews and social media posts to generate a proxy for NPS based on sentiment scores.\n\nSee Algorithms, Product &gt;&gt; Net Promoter Score"
  },
  {
    "objectID": "qmd/surveys-design.html#sec-surv-design-respsc-biases",
    "href": "qmd/surveys-design.html#sec-surv-design-respsc-biases",
    "title": "68Â  Design",
    "section": "68.6 Biases",
    "text": "68.6 Biases\n\nAcquiescence Bias\n\nA tendency to agree with statements rather than disagree\nOccurs with Agree/disagree (A/D) or Yes/No (Y/N) questions\nEffects can be stronger when the survey is administered by a person compared to self-administered surveys\n\nSome peopleâ€™s personal inclination can lead them to be polite and avoid conflict, ultimately being aggregable.\nSome participants may consider themselves in lower social status than the interviewer/researcher. Therefore, they may believe what is offered in questions and unintentionally accept the â€˜agreeâ€™ choice.\nIn many cultures, while interacting with another person, agreeing is more well-suited than disagreeing\n\nCan cause a correlational relation between similarly worded questions, thus, eliminates some important constructs\nEven a small survey error stemming from acquiescence bias decreases the quality of the inferences\nSolution:\n\nconvert Agree/Disagree (A/D) response format to item-specific (IS) response format\nExample\n\n\n\nResponse Order Effect\n\nChoices presented earlier are more probable to be selected\nIf the answer options are categorical, respondents tend to conceive them consecutively\nEffect is relatively small in rating scales\nSome respondents stop when they come to an acceptable answer and never see the rest of the options.\nIf respondents spend more time on the first half of the response scale, they are more likely to choose one of the answers here.\nSolution:\n\nIf using Agree/Disagree (A/D), convert to item-specific (IS) response format\n\nRespondants spend more time on IS format thus more likely to read all the anwsers\n\nChange the order of the responses randomly for different participants\n\nSome platforms like Survey Monkey and Qualdtrics websites have a randomize response order option\n\nEffect is smaller in vertical arrangement of responses\n\n\nCognitive Errors\n\nUnderstanding the question\n\nProvide definitions for key terms in the questions\n\nRetrieval of information (Remembering)\n\nUnless itâ€™s something really memorable, donâ€™t ask about something that happened 6 months ago\nUse memory cues\n\nlike sequencing the questions as the events wouldâ€™ve been sequenced\neliciting life events in the question\n\nSomethings are encoded into memories\n\nWhen we paid cash, we paid more attention to prices of certain items, because we had to get out the cash and count it. Being able to just swipe a card doesnâ€™t encode such information as well.\n\n\nIntegration of that information into an estimate or judgment\n\nParticipants may underreport or overreport a behavior when the frequency is asked\n\nReporting of that judgment (picking a response)\n\nThereâ€™s a cognitive burden when a respondant tries to bin there answer into one of the provided responses\nGreater burden with Agree/Disagree (A/D) format\n\nitem-specific questions are less sequential compared to A/D questions. Thus, respondents may experience less burden on the cognitive process of reporting an answer"
  },
  {
    "objectID": "qmd/surveys-sampling-methods.html#sec-surv-sampmeth-misc",
    "href": "qmd/surveys-sampling-methods.html#sec-surv-sampmeth-misc",
    "title": "Sampling Methods",
    "section": "Misc",
    "text": "Misc\n\nNotes from:\n\nSurvey data in the field of economy and finance (ebook)"
  },
  {
    "objectID": "qmd/surveys-sampling-methods.html#sec-surv-sampmeth-terms",
    "href": "qmd/surveys-sampling-methods.html#sec-surv-sampmeth-terms",
    "title": "Sampling Methods",
    "section": "Terms",
    "text": "Terms\n\nIn a survey setting,\n\nU denotes a finite population (i.e.Â [target population) of N units\nA sample s of n units (nâ‰¤N) is taken from U\n\nDesign Weights - The average number of units in the population that each sampled unit represents. This weight is determined by the sampling method and is an important part of the estimation process.\nEmpirical Design - When the inclusion probabilities (see below) are unknown\n\nSee Non-Probabilistic Sampling Methods\nExamples\n\nQuota Sampling - Units are selected so to reflect known structures for the population\nExpert Sampling - Units are selected according to expert advice\nNetwork Sampling - Existing sample units recruit future units from among their â€˜networkâ€™.\n\n\nEstimator of the parameter, Î¸, is a function of sample observations\n\nExample: Sample Mean\n\\[\n\\hat{\\theta} = \\frac {\\sum_{i \\in S}y_i}{S} = \\bar{y}_S\n\\]\n\nPopulation mean of the study variable can be estimated by the mean value over the sample observations\n\n\nInclusion Probability - The probability for a unit to appear in the sample\nProbabilistic Design - When every element in the population has a fixed, known-in-advance inclusion probabilities\nSampling Bias - The probability distribution in the collected dataset deviates from its true natural distribution one would actually observe in the wilderness.\nSampling Frame - An exhaustive list of all the individuals which comprise the target population (Also see Surveys, Design &gt;&gt; Sources of Error &gt;&gt; Coverage or Frame Error)\n\nStudy Parameter (Î¸) - Linear parameter of the study variable, such as a mean, a total or a proportion, or a more complex one such as a ratio between two population means, a correlation or a regression coefficient, a quantile (e.g.Â median, quartile, quintile or decile) or an inequality measure such as the Gini or the Theil coefficient. (also see estimator)\nStudy Variable (y)\n\nQuantitative - Numerical information (e.g.Â the total disposable income or the total food consumption)\nQualitative - Categorical information (e.g.Â gender, citizenship, country of birth, marital status, occupation or activity status)"
  },
  {
    "objectID": "qmd/surveys-sampling-methods.html#sec-surv-sampmeth-probsamp",
    "href": "qmd/surveys-sampling-methods.html#sec-surv-sampmeth-probsamp",
    "title": "Sampling Methods",
    "section": "Probabilistic Sampling Methods",
    "text": "Probabilistic Sampling Methods\n\nSimple Random Sampling (SRS)\n\nA method of selecting n units out of N such that every sample s of size n has the same probability of selection\nSimple Inclusion Probability - the probability for a unit to appear in the sample\n\\[\n\\pi_i = \\mbox{Pr}(i \\in S) = \\sum \\limits_{i \\in s \\in S} \\mbox{Pr}(S = s) = \\frac {\\binom{N-1}{n-1}}{\\binom{N}{n}} = \\frac {n}{N}\n\\]\n\n\\(n\\) is the size of sample, \\(s\\), and \\(N\\) is the target population size\n\nDouble Inclusion Probability - the probability for 2 units to appear in the sample\n\\[\n\\pi_{ij} = \\mbox{Pr}(i,j \\in S) = \\sum \\limits_{i,j \\in s \\in S} \\mbox{Pr}(S = s) = \\frac {\\binom{N-2}{n-2}}{\\binom{N}{n}} = \\frac {n}{N} \\frac {n-1}{N-1}\n\\]\n\nWhere \\(i \\neq j\\)\n\\(n\\) is the size of sample, \\(s\\), and \\(N\\) is the target population size\n\nWithout Replacement (most common)\n\nAt the first extraction, each one of the population units will have an equal probability of selection, \\(1/N\\).\nAt the second extraction, the remaining N-1 units will have a selection probability equal to \\(1/(N-1)\\). Etc.\n\nWith Replacement - all the units of the population will have all the same probability of being selected \\(1/N\\) Advantages:\n\nItâ€™s simple and doesnâ€™t use auxiliary information on the population\nThe selection is random and, then, any unit is favoured\nThe sample is representative Disadvantages:\nThe choice of the element is completely random\nA complete list of the population units is necessary\nItâ€™s time and cost consuming\n\nEstimated Total of the study variable, \\(\\hat{Y}\\)\n\\[\n\\hat{Y}_{SRS} = N \\bar{y}\n\\]\n\nWhere \\(N\\) is the target population size\n\nEstimated Mean of the study variable, \\(\\bar{Y}\\)\n\\[\n\\hat{\\bar{Y}}_{SRS} = \\bar{y}\n\\]\n\nWhere \\(\\bar{y}\\) is the sample mean\n\nVariance for Estimated Total\n\\[\nV(\\hat{Y}_{SRS}) = N^2 (1-f) \\frac {S^2_y}{n}\n\\]\n\n\\(S^2_y\\) is the dispersion of the study variable, \\(y\\), over the population \\(U\\)\n\\[\nS^2_y = \\frac {1}{N-1} \\sum_{i \\in U} (y_i - \\bar{Y})^2\n\\]\nSampling Rate or Sampling Fraction: \\(\\mbox{f} = n/N\\)\nFinite Population Correction Factor: \\(1-\\mbox{f}\\)\n\nVariance for Estimated Mean\n\\[\n\\hat{V}(\\bar{y}) = (1-\\mbox{f}) \\frac {s^2_y}{n}\n\\]\n\nSample Dispersion\n\\[\ns^2_y = \\frac{1}{n-1}\\sum_{i \\in s} (y_i - \\bar{y})^2\n\\]\n\nEstimated size of subpopulation, \\(A\\)\n\\[\n\\hat{N}_A = Np_A\n\\]\n\n\\(p_A\\) is the sample proportion of units from target subpopulation, \\(U_A\\)\n\ni.e.Â (I think) \\(n_A / N_A\\)\n\nExamples: Subpopulations\n\nTotal number of males or females in the population\nTotal number of elderly people aged more than 65 in the population\nTotal number of establishments having more than 50 employees in a certain geographical region or in a sector of activity.\n\n\nVariance of sample proportion of subpopulation, \\(A\\)\n\\[\n\\hat{V}(p_A) = \\frac{p_A(1-p_A)}{n}\n\\]\nDomain Parameter Estimation\n\nRefers to estimating population parameters for sub-populations of interest, called domains. For instance, one may wish to estimate the mean household disposable income broken down by personal characteristics such as age, gender or citizenship\n\nI think this is different from â€œEstimated size of subpopulation, Aâ€ (above) because weâ€™re estimating a study variable of subpopulation vs the size of the subpopulation\n\nEstimated Total of the study variable\n\\[\n\\hat{Y}_D = \\frac{N \\cdot n_D}{n} \\; \\bar{y}_D\n\\]\n\n\\(\\bar{y}_D\\) - The sample mean of study variable, \\(y\\), within the domain, \\(D\\)\n\\(n_D\\) - The total number of sample units from the sample \\(s\\) which fall into domain, \\(D\\)\n\nSample size \\(n_D\\) is a random variable of mean \\(\\bar{n}D = nP_D\\) where \\(P_D = N_D / N\\)\n\nI guess this is a random variable because this is strictly SRS, so you arenâ€™t stratifying by \\(D\\) when you sample the target population. Therefore, the number of samples from \\(D\\) you happen to get will be random and have a distribution.\n\n\nAlternative: When the size of the domain, \\(N_D\\), of \\(U_D\\) is known\n\n\\(\\hat{Y}_{D,\\mbox{alt}} = N_D \\cdot \\bar{y}D\\)\nThis formula has a provably (see ebook in Misc) lower variance than the original formula\n\n\nVariance for Estimated Total\n\\[\nV(\\hat{Y}_D) \\approx N^2_D \\left(\\frac{1}{\\bar{n}_D} - \\frac{1}{N_D}\\right)S^2_D \\left(1 + \\frac{1-P_D}{CV^2_D} \\right)\n\\]\n\nWhere\n\\[\n\\begin{align}\n&S^2_D = \\sum_{k \\in U_D} \\frac{(y_k - \\bar{Y}_D)^2}{N_D - 1}\\\\\n&CV_D = \\frac{S_D}{\\bar{Y}_D}\n\\end{align}\n\\]\nAssumes the population sizes, \\(N\\) and \\(N_D\\), are â€œlarge enough.â€\nFor the Alternative Estimated Total formula (see above)\n\\[\nV(\\hat{Y}_{D,alt}) \\approx N^2_D \\left(\\frac{1}{\\bar{n}_D} - \\frac{1}{N_D} \\right)S^2_D\n\\]\n\nAssumes the sample size, \\(n_D\\), is â€œlarge enough.â€\nA provably lower variance (see ebook in Misc)\n\n\n\n\n\n\nUnequal Probability Sampling\n\nDifferent units in the population will have different probabilities of being included in a sample.\n\nUnlike SRS, where each unit has an equal probability of being included in the sample\n\nUnequal probability sampling can result in estimators having higher precision than when simple random sampling or other equal probability designs are used.\n\nEmphasizes the importance of utilizing so-called â€œauxiliaryâ€ information as a way to boost sampling precision. (see Ï€k below)\n\nHorvitz-Thompson estimator (without replacement selection)\n\nEstimated Total, \\(\\hat{Y}\\), for the study variable\n\\[\n\\hat{Y}_{HT}= \\sum_{k \\in S} \\frac{y_k}{\\pi_k} = \\sum_{k \\in s} d_ky_k\n\\]\n\\(d_k = 1/\\pi_k\\) is the design weight of unit, \\(k\\), of sample, \\(s\\)\n\\(\\pi_k\\) is the inclusion probability for unit, \\(k\\), of sample, \\(s\\)\n\nIn practice, as the study variable \\(y\\) is unknown, the inclusion probabilities should be taken proportional to an auxiliary variable \\(x\\) assumed to have a linear relationship with \\(y: Ï€ \\propto x\\) (probability proportional to size sampling)\nAn inclusion probability that is optimal with respect to one study variable may be far from optimal with other study variables. In case of multi-purpose surveys, this is a major problem which generally prevents from using unequal probability sampling.\n\nAlternatively, survey statisticians use stratification as we know it always make accuracy better no matter the study variable.\n\n\n\nHansen-Hurwitz estimator has been proposed in case of sampling with replacement.\n\n\n\nCluster Sampling\n\nAssumes population has natural clusters (e.g.Â family unit). Different from Stratified Sampling in that the clustering characteristic(s) is the same for all clusters (between cluster variation = 0) and the within cluster variation is heterogeneous (i.e.Â within cluster variation != 0).\n\nExample: Family units in NYC are chosen randomly chosen. The variation between family members is whats studied.\n\nAdvantages:\n\nitâ€™s efficient when the clusters constitute naturally formed subgroups, for which we donâ€™t possess the list of the population\nStudying clusters can be less expensive than simple random sampling.\n\nDisadvantages:\n\nThe conditions of the clusters arenâ€™t always respected. The clusters may contain similar elements.\n\n\n\n\nStratified Sampling\n\n\nMisc\n\nNotes from Chapter 3 Stratification\nThe population is classified into subpopulations, called strata, based on some categorical characteristics, such as age, gender, education\nStratified sampling buckets the population into k strata (e.g., countries), and then the experiment random samples individuals from each stratum independently.\nAssumes between group variation is not 0 (i.e.Â heterogeneous) and within-group variation is 0 (i.e.Â homogeneous)\nReasons for stratification\n\nBaseline for group A different from group B\nReason to believe the effect for group A will be different from group B\n\nAdvantages\n\nIt can be more efficient than simple random sampling\nThere is less risk of obtaining non-representative samples\n\nDisadvantages\n\nIt needs the availability of auxiliary information on the population.\nThere are strict conditions for the strata\n\n\nEstimated Total, \\(\\hat{Y}\\), for the study variable and the\nEstimated Mean of the study variable, \\(\\bar{Y}\\) (respectively)\n\\[\n\\begin{align}\n&\\hat{Y}_\\mbox{STSRS}=\\sum_{h=1}^H N_h \\bar{y}_h \\\\\n&\\hat{\\bar{Y}}_\\mbox{STSRS} = \\sum_{h=1}^H W_h \\bar{y}_h\n\\end{align}\n\\]\n\nAssumes SRS within each strata\n\\(N\\) is the population size\n\\(N_h\\) is the population strata size for strata, \\(h\\)\n\\(W_h\\) is the frequency weight where \\(W_h = N_h / N\\)\n\\(\\bar{y}_h\\) is the sample mean of strata, \\(h\\)\n\nVariance for Estimated Total (assuming SRS within strata)\n\\[\nV(\\hat{Y}_\\mbox{STSRS}) = \\sum_{h=1}^H N_h^2 (1-\\mbox{f}_h)\\frac{S_h^2}{n_h}\n\\]\n\n\\(n_h\\) is the sample size for stratum, \\(h\\)\nStratum Sampling Fraction: \\(\\mbox{f}_h = n_h / N_h = n/N\\) (which is just \\(\\mbox{f}\\))\n\nAssumes \\(\\mbox{f}_h\\) is the same for each strata\n\nStratum Dispersion: \\(S_h^2\\) should be similar to the sample dispersion for SRS below, except the domain of the variables is within stratum, \\(h\\) (e.g.Â \\(n â†’ n_h,\\) \\(È³ â†’ È³_h\\), etc.)\n\nSample Dispersion for SRS\n\\[\ns_y^2 = \\frac{1}{n-1}\\sum_{i \\in s} (y_i - \\bar{y})^2\n\\]\n\n\nVariance for Estimated Mean (assuming SRS within strata)\n\\[\nV(\\bar{Y}_\\mbox{STSRS}) = (1 - \\mbox{f}) \\frac{s^2_h}{n_h}\n\\]\n\n\\(n_h\\) will be the same for all \\(h\\), so itâ€™s constant in this case\nSampling Fraction: \\(\\mbox{f} = N / n\\)\n\n\\(n\\) is the overall sample size\n\nWithin-Stratum Dispersion\n\\[\nS_w^2 = \\sum_{h=1}^H W_hS_h^2\n\\]\n\n\\(S^2_h\\): See above\n\\(N\\) is the population size and \\(N_h\\) is the population strata size for strata, \\(h\\)\n\\(W_h\\) is the frequency weight where \\(W_h = N_h / N\\)\n\n\nDesign Weights: \\(d_i = N_h / n_h \\;\\;\\forall \\in s_h\\)\n\nFor SRS, design weights are equal within each stratum\n\\(s_h\\) is the set of samples within stratum, \\(h\\)\n\nStratum sample size allocation methods\n\nLet assume the overall sample size, \\(n\\), has been fixed (generally out of budgetary considerations). We seek to determine which sample size, \\(n_h\\), is to be drawn out of each stratum in order to achieve statistical optimality under cost considerations.\nEqual Allocation\n\n\\(n^\\mbox{eq}_h = n / H\\)\n\\(H\\) is the number of strata\nPerforms poorly when the dispersions, \\(S^2_h\\), are different from one stratum to another\n\nProportional Allocation\n\nConsists of selecting samples in each stratum in proportion to the size, \\(N_h\\), of the stratum population\n\\(n^\\mbox{prop}_h = (n \\cdot N_h) / N = n \\cdot W_h\\)\nVariance\n\\[\n\\begin{align}\nV(\\hat{\\bar{Y}}_\\mbox{prop}) &= (1-\\mbox{f})\\frac{S^2_w}{n} \\\\\n&=\\frac{\\sum_{h=1}^h W_h S^2_h}{n} - \\frac{\\sum_{h=1}^h W_h S^2_h}{N}\n\\end{align}\n\\]\n\nOptimal or Neyman Allocation\n\nSeeks to minimize the variance under the cost constraint\n\\[\n\\sum_{h=1}^H c_h n_h = C_0\n\\]\n\n\\(C_0\\) is the overall budget available and \\(c_h\\) the average survey cost for an individual in stratum \\(h\\).\n\nStrata Sample Size with Cost Constraint\n\\[\n\\forall h \\;\\; n_h^\\mbox{opt} = \\frac{N_hS_h}{\\sqrt{c_h}} \\frac{C_0}{\\sum_{h=1}^H N_h S_h \\sqrt{c_h}}\n\\]\nStrata Sample Size without Cost Constraint\n\\[\nn_h^\\mbox{opt} = n \\frac{N_hS_h}{\\sum_{h=1}^H N_hS_h}\n\\]\nVariance\n\\[\nV(\\hat{\\bar{Y}}_\\mbox{SRS}) = \\frac{1}{n}\\sum_h W_h(\\bar{Y}_h - \\bar{Y})^2 - \\frac{1}{n}\\sum_h W_h(S_h - \\bar{S})^2\n\\]\n\n\\(\\bar{S}\\) must be the mean sqrt dispersion across all stratum\n\nContrary to proportional allocation, the Neyman allocation is variable-specific: optimality is defined with respect to one study variable, and what is optimal with respect to one variable may be far from optimal with respect to another.\nThe gain in accuracy as compared to proportional allocation is pretty small. Thatâ€™s why in practice proportional allocation is often preferred to optimal allocation.\n\nBalanced Allocation\n\\[\n\\forall h \\;\\; n_h^\\mbox{bal} = \\frac{\\tilde{n}}{H} + (n - \\tilde{n})W_h\n\\]\n\n\\(\\tilde{n}\\) is a subsample of \\(n\\) that is equally allocated (see above) among the strata which insures minimal precision within the strata (i.e.Â locally)\nThe rest of the sample (\\(n-\\tilde{n}\\)) can be allocated using either proportional or optimal allocations (see above) in order to optimize accuracy for the overall sample (i.e.Â globally)\nBoth proportional and Neyman allocations increase sample accuracy at global level, but may happen to perform very poorly when it comes to strata (e.g.Â regional) level estimates.\n\n\n\n\n\nMulti-Stage Sampling\n\nMisc\n\nUseful when no sampling frame is available\nStages\n\nAt first-stage sampling, a sample of Primary Sampling Units (PSU) is selected using a probabilistic design (e.g.Â simple random sampling or other, with or without stratification)\nAt second-stage sampling, a sub-sample of Secondary Sampling Units (SSU) is selected within each PSU selected at first-stage. The selection of SSU is supposed to be independent from one PSU to another.\nAt third-stage sampling a sample of Tertiary Sampling Units can be selected with each of the SSU selected at second stage.\netc.\n\nExample: (given an absence of any frame of individuals)\n\nSelect a sample of municipalities (first-stage sampling),\nSelect a sample of neighbourhoods (second-stage sampling) within each selected municipality,\nSelect a sample of households (third-stage sampling) within each of the neighbourhoods selected a second stage\nSelect a sample of individuals (fourth-stage sampling) within each household.\n\nAdvantages:\n\nCan be more efficient than using only 1 of the sampling strategies\nCan decrease sample size if there are numerous units within strata or clusters\n\nDisadvantages:\n\nIf sampling assumptions arenâ€™t valid, multi-stage sampling results to be less efficient than simple random sampling.\n\n\nExample: 2-stage cluster sampling\n\nAdds a second stage to cluster sampling. After clusters are chosen, units within those clusters are randomly sampled.\n\nExample: 2-Stage Stratified Sampling\n\nNotes from Two Stage Stratified Random Sampling â€” Clearly Explained\nUseful for when you have hierarchical strata (e.g.Â towns/blocks and households)\nExample: An education study of students where:\n\nSchools (first stage sampling units) may be selected with probabilities proportional to school size\nStudents (second stage units) within selected schools may be selected by stratified random sampling\n\nStage 1\n\n\n(Random?) Sample from group of First Stage Units (FSU)\n\nEach FSU usually has a population within a range\ne.g.Â census geographies (census block, metropolitan statistical area, etc.)\n\n\nStage 2\n\n\nAll Second Stage Units (SSU) within each FSU are pooled together to create a population\n\nSSUs are the base geography unit you want to measure\ne.g.Â households\n\nThen each SSU is binned into Second Stage Strata (SSS) according to a characteristic or set of characteristics\n\ne.g.Â race, age, income level, education, etc.\nThe SSS are stratified sampled\n\n\n\n\n\n\nSystematic Sampling\n\n\nSteps\n\nAfter choosing a sample size, n, calculate the sampling interval k = N/n, where N is the population size\n\nIn the example, we have 9 smiles and we want to obtain a sample of 3 units, then N = 9, n = 3 and k = 9/3 =3.\n\nSelect a random starting point, r, which is a random integer between 1 and k: 1â‰¤râ‰¤k.\n\nIn the example, r = 2, where 1â‰¤râ‰¤3.\n\nOnce the first unit is selected, we take every following kth item to build the sample: r, r+k, r+2k , â€¦ , r+(n-1)k.\n\nAdvantages:\n\nThe random selection is applied only on the first item, while the rest of the items selected depend on the position of the first item and a fixed interval at which items are picked.\n\nDisadvantages:\n\nIf the list of the population elements presents a determined order, there is the risk of obtaining a non-representative sample"
  },
  {
    "objectID": "qmd/surveys-sampling-methods.html#sec-surv-sampmeth-probsamp-nonprob",
    "href": "qmd/surveys-sampling-methods.html#sec-surv-sampmeth-probsamp-nonprob",
    "title": "Sampling Methods",
    "section": "Non-Probabilistic Sampling Methods",
    "text": "Non-Probabilistic Sampling Methods\n\nMisc\n\nMostly used when probabilistic methods arenâ€™t possible due to rarity or difficulty in obtaining a representative sample of the population being studied or cost constraints of the experiment\n\nQuota Sampling\n\nSimilar to Stratified Sampling (see Probabilistic Sampling Methods) except:\n\nEach stratumâ€™s sample size is called its quota\nEach stratumâ€™s sample size takes into account its distribution in the whole population.\n\nExample: If 80% of the population are males, then 80% of the sample should be males.\n\nWithin each stratumâ€™s quota, the interviewer is free to choose the participants to interview.\n\nThis seems to be the main difference\n\n\nAdvantages:\n\nItâ€™s time and cost-effective, in particular with respect to the stratified sampling.\n\nDisadvantages:\n\nThe results can be distorted due to the discretion of the interviewers or the non-response bias\nThe quota sample can produce a selection bias\n\n\nJudgemental Sampling (aka Purposive Sampling)\n\nThe researcher selects the participants because he believes they are representative of the population\n\nUseful when there is only a limited number of people with specific traits\n\nAdvantages:\n\nItâ€™s time and cost-effective\nItâ€™s suitable to study a certain cultural domain, where the knowledge of an expert is needed\n\nDisadvantages:\n\nIt can lead to a high selection bias the bigger is the gap between the researcherâ€™s knowledge and the actual situation of the population\n\n\nConvenience Sampling\n\nThe researcher chooses anyone that is â€œconvenientâ€ to him, i.e.Â people that are immediately available to answer the questions, without any specific criteria\n\nUsually volunteers\n\nAdvantages:\n\nItâ€™s very cheap and fast\n\nDisadvantages:\n\nIt leads to a non-representative sample\n\n\nSnowball Sampling\n\nThe researcher asks already recruited people to identify other potential participants, and so on\n\nUseful for rare populations, for which itâ€™s not possible to have the list of the population or itâ€™s difficult to locate the population.\n\ne.g.Â illegal immigrants\n\n\nAdvantages:\n\nItâ€™s useful for market studies or researches about delicate topics.\n\nDisadvantages:\n\nThe sample may be non-representative since itâ€™s not random, but depends on the people contacted directly or indirectly by the researcher\nItâ€™s time-consuming"
  },
  {
    "objectID": "qmd/teaching.html#sec-teach-misc",
    "href": "qmd/teaching.html#sec-teach-misc",
    "title": "Teaching",
    "section": "Misc",
    "text": "Misc\n\ntl;dr for writingÂ article or preparing a talk Audience\n\nKnow their level of knowledge\nWhy they should want to know this\nHow does that reason get them closer to their goal\n(Maybe those last 2 are the same?)\n\nBooks\n\nTeaching what you dont know\nThe discussion book -Â 50 great ways to get people talking\n\nOn average, 7 \\(\\pm\\) 2 things can be kept in short term memory\nTeach chunks of your concept map. Each new chunk should be adjacent to the previous chunk.\nSacrifice truth for clarity to give learner actionable concept\nExplaining an unclear concept from the homework or reading (Gelman)\n\nBetter to work through an example than to try to clarify a definition or restate it, etc.\nThen, ask the students to get into pairs and explain to each other the meaning of each of the concepts in question\nThen, if students want to ask questions on the concept, we could do it in the context of this example that theyâ€™ve just been talking about. We could also loop back to their homework assignment."
  },
  {
    "objectID": "qmd/teaching.html#sec-teach-steps",
    "href": "qmd/teaching.html#sec-teach-steps",
    "title": "Teaching",
    "section": "Steps",
    "text": "Steps\n\nState goalpost\nSplit goalpost into concepts\nConnect concepts to form map\nIf more than 7 \\(\\pm\\) 2 concepts, then group in chunks\nThe next chunk of concepts you present should be adjacent to previous chunk\nSummarize"
  },
  {
    "objectID": "qmd/teaching.html#sec-teach-chtaud",
    "href": "qmd/teaching.html#sec-teach-chtaud",
    "title": "Teaching",
    "section": "Characterize the Audience",
    "text": "Characterize the Audience\n\nGeneral background\nrelevant experience\nperceived needs\nspecial consideration"
  },
  {
    "objectID": "qmd/teaching.html#sec-teach-mentmod",
    "href": "qmd/teaching.html#sec-teach-mentmod",
    "title": "Teaching",
    "section": "Mental Model",
    "text": "Mental Model\n\nDraw a concept map. Concepts and connections between them\nExamples:Â VennÂ diagrams, flow charts"
  },
  {
    "objectID": "qmd/teaching.html#sec-teach-ass",
    "href": "qmd/teaching.html#sec-teach-ass",
    "title": "Teaching",
    "section": "Assessments",
    "text": "Assessments\n\nSummative\n\nSummary of what you want to be learned. Goalpost\nGuide to creatingÂ formative assessments by working backwards from the endpoint\n\nFormative\n\nIs the learning working?Â \nTypes of mistakes or questions are clues to the types of misconceptions that learners are thinking and what you should say next.\nDiagnosing misconceptions by checking in every few minutes with questions. Think about what those answers might be and they mean.\nQuestions should have diagnostic power\nTells you if its okay to move on to the next lesson."
  },
  {
    "objectID": "qmd/teaching.html#sec-teach-forces",
    "href": "qmd/teaching.html#sec-teach-forces",
    "title": "Teaching",
    "section": "Forces for Learning",
    "text": "Forces for Learning\n\nPositive\n\nIntrinsic Motivation - Learner isnt being made to learn something, theyre choosing to learn\nUtility - Moves them towards their goals\nCommunity - Not alone in learning, connection to peers, more comfortable about not knowing someting\n\nNegative\n\nUnpredictability - â€œWhat i do doesnt seem to affect the outcomeâ€, learned helplessness\nUnfairness - Teacher Bias\nIndifference - Feeling that the teacher doesnt care about your problem"
  },
  {
    "objectID": "qmd/tidyeval.html#misc",
    "href": "qmd/tidyeval.html#misc",
    "title": "73Â  Tidyeval",
    "section": "73.1 Misc",
    "text": "73.1 Misc\n\n{rlang}\nNotes from\n\nProgramming with dplyr\nBrad Cannellâ€™s notes\n\nExamples\n\nadd-mase-scale\nadd-tsfeatures\ndescale-by-mase\nfable-resids\nprewhitened-ccf\nscale-by-mase\nto-js-array"
  },
  {
    "objectID": "qmd/tidyeval.html#terms",
    "href": "qmd/tidyeval.html#terms",
    "title": "73Â  Tidyeval",
    "section": "73.2 Terms",
    "text": "73.2 Terms\n\ndata masking allows you to use data variables as if they were variables in the environment (i.e.Â you write my_variable not df$myvariable).\n\ne.g.Â arrange(), count(), filter(), group_by(), mutate(), and summarise()\n\ntidy selection - helper functions that allow you to easily choose variables based on their position, name, or type (e.g.Â starts_with(â€œxâ€) or is.numeric).\n\ne.g.Â across(), relocate(), rename(), select(), and pull()\nWhen you have an env-variable that is a character vector, you need to use all_of() or any_of() depending on whether you want the function to error if a variable is not found.\n\nenv-variables are â€œprogrammingâ€ variables that live in an environment. They are usually created with &lt;-. data-variables are â€œstatisticalâ€ variables that live in a data frame. They usually come from data files (e.g.Â .csv, .xls), or are created manipulating existing variables. When you have the data-variable in a function argument (i.e.Â an env-variable that holds a promise), you need to embrace the argument by surrounding it in doubled braces, like filter(df, {{ var }}).\n\ndf &lt;- data.frame(x = runif(3), y = runif(3))\n\nIt creates a env-variable, df, that contains two data-variables, x and y. Then you can extract the data-variable x out of the env-variable df using $."
  },
  {
    "objectID": "qmd/tidyeval.html#for-string-input-or-character-vector",
    "href": "qmd/tidyeval.html#for-string-input-or-character-vector",
    "title": "73Â  Tidyeval",
    "section": "73.3 For string input or character vector",
    "text": "73.3 For string input or character vector\n\ne.g.Â compute_mpg(model = \"ford\")\n.data is not a data frame; itâ€™s a special construct, a pronoun, that allows you to access the current variables either directly, with .data$x or indirectly with .data[[var]]. Donâ€™t expect other functions to work with it.\n\nnever uses data masking or tidy select\n\nFilter(.data[input] &gt; 0)\nFilter(.data[input] == .env$value)\n\ninput and value are both string function inputs\nit is consistent with other tidyeval where handling lhs is a little different than rhs\n\nGroup_by (.data[input])\nSummarize(var2 = mean(.data[input])\nExamples\n\n#' @importFrom rlang .data\n# when data is the only thing that's user-supplied\nmy_summary_fun &lt;- function(data) {\nÂ  data %&gt;%Â \nÂ  Â  filter(.data$x &gt; 0) %&gt;%Â \nÂ  Â  group_by(.data$grp) %&gt;%Â \nÂ  Â  summarize(y = mean(.data$y), n = n())\n}\n\n# character vector needs tidy selection (also see all_of and any_of in Terms section)\nsummarize_mean &lt;- function(data, vars) {\nÂ  data %&gt;% summarize(n = n(), across({{ vars }}, mean))\n}\nmtcars %&gt;%Â \nÂ  group_by(cyl) %&gt;%Â \nÂ  summarize_mean(where(is.numeric))"
  },
  {
    "objectID": "qmd/tidyeval.html#for-nonstring-input",
    "href": "qmd/tidyeval.html#for-nonstring-input",
    "title": "73Â  Tidyeval",
    "section": "73.4 For nonstring input",
    "text": "73.4 For nonstring input\n\ne.g.Â compute_mpg(model = ford)\nFilter({{input}} &gt; 0)\nSummarize({{col_name}} = mean({{col_name}}, na.rm = T)\nggplot(df, aes(x = {{x}})\n\nmy_summarise2 &lt;- function(data, expr) {\nÂ  data %&gt;% summarise(\nÂ  Â  mean = mean({{ expr }}),\nÂ  Â  sum = sum({{ expr }}),\nÂ  Â  n = n()\nÂ  )\n}"
  },
  {
    "objectID": "qmd/tidyeval.html#creating-variable-names-from-user-input",
    "href": "qmd/tidyeval.html#creating-variable-names-from-user-input",
    "title": "73Â  Tidyeval",
    "section": "73.5 Creating variable names from user input",
    "text": "73.5 Creating variable names from user input\n\nrequires glue syntax and â€œ:=â€\n\nmy_summarise4 &lt;- function(data, expr) {\nÂ  data %&gt;% summarise(\nÂ  Â  \"mean_[{{expr}}]{style='color: goldenrod'}\" := mean({{ expr }}),\nÂ  Â  \"sum_[{{expr}}]{style='color: goldenrod'}\" := sum({{ expr }}),\nÂ  Â  \"n_[{{expr}}]{style='color: goldenrod'}\" := n()\nÂ  )\n}\nmy_summarise5 &lt;- function(data, mean_var, sd_var) {\nÂ  data %&gt;%Â \nÂ  Â  summarise(\nÂ  Â  Â  \"mean_{{mean_var}}\" := mean({{ mean_var }}),Â \nÂ  Â  Â  \"sd_{{sd_var}}\" := mean({{ sd_var }})\nÂ  Â  )\n}"
  },
  {
    "objectID": "qmd/tidyeval.html#for-undetermined-number-of-user-supplied-inputs",
    "href": "qmd/tidyeval.html#for-undetermined-number-of-user-supplied-inputs",
    "title": "73Â  Tidyeval",
    "section": "73.6 For undetermined number of user supplied inputs",
    "text": "73.6 For undetermined number of user supplied inputs\n\nUsing â€œâ€¦â€\nmake sure that any other arguments start with . to reduce the chances of argument clashes\n\ne.g.Â the .data in the example isnâ€™t the â€œ.dataâ€ construct for string inputs\n\n\nmy_summarise &lt;- function(.data, ...) {\nÂ  .data %&gt;%\nÂ  Â  group_by(...) %&gt;%\nÂ  Â  summarise(mass = mean(mass, na.rm = TRUE), height = mean(height, na.rm = TRUE))\n}\nstarwars %&gt;% my_summarise(sex, gender)"
  },
  {
    "objectID": "qmd/tidyeval.html#loops-with-purrr",
    "href": "qmd/tidyeval.html#loops-with-purrr",
    "title": "73Â  Tidyeval",
    "section": "73.7 Loops with purrr",
    "text": "73.7 Loops with purrr\n\nlist item input uses .data construct\n\nmtcars %&gt;%Â \nÂ  names() %&gt;%Â \nÂ  purrr::map(~ count(mtcars, .data[[.x]]))\n\n# quoted input\ndisagg_ucb &lt;- function(x, y) {\nÂ  UCBadmit %&gt;%\nÂ  Â  select(-applications) %&gt;%\nÂ  Â  group_by(dept, applicant.gender) %&gt;%\nÂ  Â  tidyr::uncount(weights = !!sym(x)) %&gt;%\nÂ  Â  mutate(admitted = y) %&gt;%\nÂ  Â  select(dept, gender = applicant.gender, admitted)\n}\n\nucb_01 &lt;- purrr::map2_dfr(c(\"admit\", \"reject\"),\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  c(1, 0),\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ~ disagg_ucb(.x, .y)\n)\n\nThis same technique works with for loop alternatives like the base R apply()"
  },
  {
    "objectID": "qmd/tidyeval.html#shiny",
    "href": "qmd/tidyeval.html#shiny",
    "title": "73Â  Tidyeval",
    "section": "73.8 Shiny",
    "text": "73.8 Shiny\nlibrary(shiny)\nui &lt;- fluidPage(\nÂ  selectInput(\"var\", \"Variable\", choices = names(diamonds)),\nÂ  tableOutput(\"output\")\n)\nserver &lt;- function(input, output, session) {\nÂ  data &lt;- reactive(filter(diamonds, .data[[input$var]] &gt; 0))\nÂ  output$output &lt;- renderTable(head(data()))\n}"
  },
  {
    "objectID": "qmd/tidyeval.html#tidy-evaluation-ebook-deprecated",
    "href": "qmd/tidyeval.html#tidy-evaluation-ebook-deprecated",
    "title": "73Â  Tidyeval",
    "section": "73.9 Tidy Evaluation ebook (deprecated)",
    "text": "73.9 Tidy Evaluation ebook (deprecated)\n\nhttps://tidyeval.tidyverse.org/sec-why-how.html\nThe technical term for delaying code evaluation is quoting, because evaluation must first be suspended before being resumed in a different environment.\nA blueprint contains both the expression and the environment. The evaluation of an expression captured as a blueprint can be resumed at any time, possibly in a different context\n\nvars(\nÂ  ends_with(\"color\"),\nÂ  height:mass\n)\n# a blueprint\n#&gt; &lt;list_of&lt;quosure&gt;&gt;\n#&gt;Â \n#&gt; [[1]]\n#&gt; &lt;quosure&gt;\n#&gt; expr: ^ends_with(\"color\")\n#&gt; env:Â  global\n#&gt;Â \n#&gt; [[2]]\n#&gt; &lt;quosure&gt;\n#&gt; expr: ^height:mass\n#&gt; env:Â  global\n\nvars( ) performs external quoting , which is when a helper function quotes args and outputs blueprints.\n\n\n73.9.1 !! (bang, bang)\n\nmodifies the quoted code by inlining the value of its operand right into the blueprint\nit is evaluated before any R code is evaluated. In other words, before the code is parsed by dplyr, rlang is able to evaluate !!var, replacing it with (quoted) value. Sometimes referred to as partial evaluation.\nvariable or argument value is inserted as is (i.e.Â numeric is a numeric, string is a string)\n\nx &lt;- 1\n\nrlang::qq_show(\nÂ  starwars %&gt;% summarise(out = x)\n)\n#&gt; starwars %&gt;% summarise(out = x)\n\nrlang::qq_show(\nÂ  starwars %&gt;% summarise(out = !!x)\n)\n#&gt; starwars %&gt;% summarise(out = 1)\n\n---------------------------------------------------------\n\ncol &lt;- \"height\"\nrlang::qq_show(\nÂ  starwars %&gt;% summarise(out = sum(!!col, na.rm = TRUE))\n)\n#&gt; starwars %&gt;% summarise(out = sum(\"height\", na.rm = TRUE))\n\nTo refer to column names inside a blueprint, we need to inline blueprint material. We need symbols (references to R objects):\n\nsym(col)\n#&gt; height\nrlang::qq_show(\nÂ  starwars %&gt;% summarise(out = sum(!!sym(col), na.rm = TRUE))\n)\n#&gt; starwars %&gt;% summarise(out = sum(height, na.rm = TRUE))\n\nvalue no longer as is\nsym transforms arg or var value (aka an indirect reference) into a â€œpiece of blueprintâ€ and !! inserts transformed value of variable into the dplyr verb blueprint\n\naka the â€œquote and unquote patternâ€\nThis pattern is the heart of programming with tidy eval functions. We quote an expression and unquote it in another quoted expression. In other words, we create or capture a piece of blueprint, and insert it in another blueprint just before itâ€™s captured by a data masking function (i.e.Â dplyr function). This process is also called interpolation.\npackage up the variable value to transport it inside a quoting function and then unquote it to open up the variable and us its value in a regular way\n\nTwo types of functions:\n\nevaluating - indirect references (i.e.Â stored value in a variable) resolved in the ordinary way and also support direct references (e.g.Â df$y or df[[â€œyâ€]])\nquoting - args are auto-quoted and evaluated in a special way\n\n\n# evaluating function since this works\ntemp &lt;- mtcars$am\nsum(temp)\n\n# quoting function, this doesn't work - tries to find package named \"temp\"\ntemp &lt;- \"dplyr\"\nlibrary(temp)\n# But has unquoting option, this works\nlibrary(temp, character.only = TRUE)\n# another example\ntemp &lt;- \"mtcars\"\nrm(list = temp)\n\nFor tidyverse, !! is the unquoting operator.\n\nYou can useÂ !!Â to cancel the automatic quotation and supply indirect references everywhere an argument is automatically quoted. In other words, unquoting lets you open a variable and use whatâ€™s inside instead.\n\n\n\n\n73.9.2 Using non-strings for user input classes\ngrouped_mean &lt;- function(data, group_var, summary_var) {\nÂ  # quote function args to essentially transform your function into a quoting function\nÂ  group_var &lt;- enquo(group_var)Â  Â  Â  Â \nÂ  summary_var &lt;- enquo(summary_var)\nÂ  data %&gt;%\nÂ  Â  # open quoted variables inside other quoting functions\nÂ  Â  group_by(!!group_var) %&gt;%Â  Â  Â  Â  Â  Â  Â \nÂ  Â  summarise(mean = mean(!!summary_var))Â \n}\ngrouped_mean(mtcars, am, disp)\nquote and unquote pattern\n1. UseÂ enquo()Â to make a (user-defined) function automatically quote its argument.\n2. UseÂ !!Â to unquote the argument.\n\nThis quote - unquote pattern creates a quoting function (user-defined function) wrapped around other quoting functions (dplyr functions)\n\n\n\n73.9.3 Using strings for user input classes\ngrouped_mean2 &lt;- function(data, group_var, summary_var) {\nÂ  group_var &lt;- sym(group_var)\nÂ  summary_var &lt;- sym(summary_var)\nÂ  data %&gt;%\nÂ  Â  group_by(!!group_var) %&gt;%\nÂ  Â  summarise(mean = mean(!!summary_var))\n}\ngrouped_mean2(starwars, \"gender\", \"mass\")\n\n# covid cfr project\ncases_deaths_ccf &lt;- function(input_col = \"sev_day_cases\",Â \nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  output_col = \"sev_day_deaths\")Â \n{\nÂ  Â  Â  Â  ccf_vals &lt;- whitened_tsb %&gt;%Â \nÂ  Â  Â  Â  Â  Â  Â  Â  feasts::CCF(!!sym(input_col), !!sym(output_col),Â \nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  lag_max = 40, type = \"correlation\")\n}\n\nTransforming strings to symbols makes them suitable for unquoting\n\n\n\n73.9.4 Using character vectors as user input classes\n# transformed to a LIST of symbols\ncols &lt;- syms(c(\"species\", \"gender\"))Â \ncols\n#&gt; [[1]]\n#&gt; species\n#&gt;Â \n#&gt; [[2]]\n#&gt; gender\n\n# subset the list with double bangs\ngroup_by(starwars, !!cols[[1]], !!cols[[2]])\n# or use triple bang\ngroup_by(starwars, !!!cols)\n\n\n73.9.5 Using a function as an argument\n# creating a S3 class method (not necessary but this was the context of the example)\nmutate_with &lt;- function(data, fn, col_in, col_out = \"output\") {\nÂ  UseMethod(generic = \"mutate_with\", object = data)\n}\n\n# with a dataframe obj class\nmutate_with.data.frame &lt;- function(data, fn, col_in, col_out = \"output\") {\nÂ  dplyr::mutate(.data = data, !!col_out := fn(!!rlang::sym(col_in)))\n}\n\n# with a spark obj class\nmutate_with.tbl_spark &lt;- function(data, fn, col_in, col_out = \"output\") {\nÂ  fn &lt;- rlang::enquo(fn)\nÂ  dplyr::mutate(\nÂ  Â  .data = data, !!col_out := rlang::call2(.fn = !!fn, rlang::sym(col_in))\nÂ  )\n}\nmutate_with(\nÂ  data = mtcars_spark, fn = mean, col_in = \"mpg\", col_out = \"mean_mpg\"\n)\n# # Source: spark&lt;!--?--&gt; [?? x 12]\n#Â  Â  Â  mpgÂ  cylÂ  dispÂ  Â  hpÂ  dratÂ  Â  wtÂ  qsecÂ  Â  vsÂ  Â  amÂ  gearÂ  carb mean_mpg\n#Â  Â  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;Â  Â  &lt;dbl&gt;\n#Â  1Â  21Â  Â  Â  6Â  160Â  Â  110Â  3.9Â  2.62Â  16.5Â  Â  0Â  Â  1Â  Â  4Â  Â  4Â  Â  20.1\n#Â  2Â  21Â  Â  Â  6Â  160Â  Â  110Â  3.9Â  2.88Â  17.0Â  Â  0Â  Â  1Â  Â  4Â  Â  4Â  Â  20.1\n#Â  3Â  22.8Â  Â  4Â  108Â  Â  93Â  3.85Â  2.32Â  18.6Â  Â  1Â  Â  1Â  Â  4Â  Â  1Â  Â  20.1\n#Â  4Â  21.4Â  Â  6Â  258Â  Â  110Â  3.08Â  3.22Â  19.4Â  Â  1Â  Â  0Â  Â  3Â  Â  1Â  Â  20.1\n#Â  5Â  18.7Â  Â  8Â  360Â  Â  175Â  3.15Â  3.44Â  17.0Â  Â  0Â  Â  0Â  Â  3Â  Â  2Â  Â  20.1\n#Â  6Â  18.1Â  Â  6Â  225Â  Â  105Â  2.76Â  3.46Â  20.2Â  Â  1Â  Â  0Â  Â  3Â  Â  1Â  Â  20.1\n\n# using a quoted function name\n# build the expression as a string which we then parse ourselves using rlang::parse_expr()\n# also works with dataframe class\nmutate_with.tbl_spark &lt;- function(data, fn, col_in, col_out = \"output\") {\nÂ  condition &lt;- paste0(fn, \"(\", col_in, \")\")\nÂ  dplyr::mutate(.data = data, !!col_out := !!rlang::parse_expr(condition))\n}\nmutate_with(\nÂ  data = mtcars_spark, fn = \"mean\", col_in = \"mpg\", col_out = \"mean_mpg\"\n)\n# # Source: spark&lt;!--?--&gt; [?? x 12]\n#Â  Â  Â  mpgÂ  cylÂ  dispÂ  Â  hpÂ  dratÂ  Â  wtÂ  qsecÂ  Â  vsÂ  Â  amÂ  gearÂ  carb mean_mpg\n#Â  Â  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;Â  Â  &lt;dbl&gt;\n#Â  1Â  21Â  Â  Â  6Â  160Â  Â  110Â  3.9Â  2.62Â  16.5Â  Â  0Â  Â  1Â  Â  4Â  Â  4Â  Â  20.1\n#Â  2Â  21Â  Â  Â  6Â  160Â  Â  110Â  3.9Â  2.88Â  17.0Â  Â  0Â  Â  1Â  Â  4Â  Â  4Â  Â  20.1\n#Â  3Â  22.8Â  Â  4Â  108Â  Â  93Â  3.85Â  2.32Â  18.6Â  Â  1Â  Â  1Â  Â  4Â  Â  1Â  Â  20.1\n#Â  4Â  21.4Â  Â  6Â  258Â  Â  110Â  3.08Â  3.22Â  19.4Â  Â  1Â  Â  0Â  Â  3Â  Â  1Â  Â  20.1\n#Â  5Â  18.7Â  Â  8Â  360Â  Â  175Â  3.15Â  3.44Â  17.0Â  Â  0Â  Â  0Â  Â  3Â  Â  2Â  Â  20.1\n#Â  6Â  18.1Â  Â  6Â  225Â  Â  105Â  2.76Â  3.46Â  20.2Â  Â  1Â  Â  0Â  Â  3Â  Â  1Â  Â  20.1\n\n# using base r (bquote + eval)\nmutate_with_bquote &lt;- function(data, fn, col_in, col_out = \"output\") {\nÂ  eval(bquote(\nÂ  Â  dplyr::mutate(.data = data, !!col_out := .(fn)(rlang::sym(col_in)))\nÂ  ))\n}\nmutate_with_bquote(\nÂ  data = mtcars_spark, fn = mean, col_in = \"mpg\", col_out = \"mean_mpg\"\n)\n# # Source: spark&lt;!--?--&gt; [?? x 12]\n#Â  Â  Â  mpgÂ  cylÂ  dispÂ  Â  hpÂ  dratÂ  Â  wtÂ  qsecÂ  Â  vsÂ  Â  amÂ  gearÂ  carb mean_mpg\n#Â  Â  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;Â  Â  &lt;dbl&gt;\n#Â  1Â  21Â  Â  Â  6Â  160Â  Â  110Â  3.9Â  2.62Â  16.5Â  Â  0Â  Â  1Â  Â  4Â  Â  4Â  Â  20.1\n#Â  2Â  21Â  Â  Â  6Â  160Â  Â  110Â  3.9Â  2.88Â  17.0Â  Â  0Â  Â  1Â  Â  4Â  Â  4Â  Â  20.1\n#Â  3Â  22.8Â  Â  4Â  108Â  Â  93Â  3.85Â  2.32Â  18.6Â  Â  1Â  Â  1Â  Â  4Â  Â  1Â  Â  20.1\n#Â  4Â  21.4Â  Â  6Â  258Â  Â  110Â  3.08Â  3.22Â  19.4Â  Â  1Â  Â  0Â  Â  3Â  Â  1Â  Â  20.1\n#Â  5Â  18.7Â  Â  8Â  360Â  Â  175Â  3.15Â  3.44Â  17.0Â  Â  0Â  Â  0Â  Â  3Â  Â  2Â  Â  20.1\n#Â  6Â  18.1Â  Â  6Â  225Â  Â  105Â  2.76Â  3.46Â  20.2Â  Â  1Â  Â  0Â  Â  3Â  Â  1Â  Â  20.1"
  },
  {
    "objectID": "qmd/tuning.html#sec-tuning-misc",
    "href": "qmd/tuning.html#sec-tuning-misc",
    "title": "Tuning",
    "section": "Misc",
    "text": "Misc\n\nTree Parameter Categories (many overlap)\n\nTree Structure and Learning\nTraining Speed\nAccuracy\nOverfitting\n\nWhen the search space is quite large, try the particle swarm method or genetic algorithm for optimization.\nEarly Stopping can lower computational costs and decrease practitioner downtime"
  },
  {
    "objectID": "qmd/tuning.html#sec-tuning-pairwise",
    "href": "qmd/tuning.html#sec-tuning-pairwise",
    "title": "Tuning",
    "section": "Pairwise Tuning",
    "text": "Pairwise Tuning\n\nTunes a pair of parameters at a time. Once the first pair of parameters is tuned, those values replace the default parameter values, and the next pair of parameters is tuned, etc.\nLimits the computational cost of performing a full grid search jointly with all parameters at once supposedly without sacrificing much in terms of predictive performance.\n\nA full grid search or other tuning method can be applied to each pair.\n\nMight be beneficial to create pairs that affect the same tuning area of the model fit (e.g.Â subsampling, regularization, tree complexity) so that the tuning process might capture any interaction effects between the parameters.\nExample: XGBoost (article)\n\nParameter Pairs\n\n(max_depth, eta)\n(subsample, colsample_bytree)\n(min_child_weight, gamma), and\n(reg_alpha, reg_lambda)\n\nProcess\n\nmax_depth and eta are tuned\nTuned values for max_depth and eta replace their default values\nsubsample and colsample_bytree are tuning using the model with the tuned values for max_depth and eta\netc.\n\n\nViz\n\n\n\nEach pair of parameters along with the loss metric are plotted in a 3D chart\nmatplotlib::plot_trisurf() uses Surface Triangulation is used to interpolate the gaps between the tested parameter values\nIf the chart has multiple pronounced dips and bumps (left chart):\n\nMay indicate that thereâ€™s a minima in one of the dips that might be better than the chosen parameter values as the interpolation process might have smoothed over that area a bit.\n\nMight want to play with the smoothing parameter a bit to try and get a clearer idea of the range of values to further investigate.\n\nIndicates the model is sensitive to this pair of parameters which might translate into a model instability, when we pass a new type of dataset into the tuned model in the deployment domain."
  },
  {
    "objectID": "qmd/tuning.html#sec-tuning-bayesopt",
    "href": "qmd/tuning.html#sec-tuning-bayesopt",
    "title": "Tuning",
    "section": "Bayesian Optimization",
    "text": "Bayesian Optimization\n\ntl;dr\n\nBuilds a surrogate model using Gaussian Processes that estimates model score\nThis surrogate is then provided with configurations picked randomly, and the one that gives the best score is kept for training\nEach new training updates the posterior knowledge of the surrogate model.\n\nComponents:\n\nThe black box function to optimize: f(x).\n\nWe want to find the value of x which globally optimizes f(x) (aka objective function, the target function, or the loss function)\n\nThe acquisition function: a(x)\n\nUsed to generate new values of x for evaluation with f(x).\na(x) internally relies on a Gaussian process model m(X, y) to generate new values of x.\n\n\nSteps:\n\nDefine the black box function f(x), the acquisition function a(x) and the search space of the parameter x.\nGenerate some initial values of x randomly, and measure the corresponding outputs from f(x).\nFit a Gaussian process model m(X, y) onto X = x and y = f(x) (i.e.Â surrogate model for f(x))\nThe acquisition function a(x) then uses m(X, y) to generate new values of x as follows:\n\nUse m(X, y) to predict how f(x) varies with x.\nThe value of x which leads to the largest predicted value in m(X, y) is then suggested as the next sample of x to evaluate with f(x).\n\nRepeat the optimization process in steps 3 and 4 until we finally get a value of x that leads to the global optimum of f(x).\n\nAll historical values of x and f(x) should be used to train the Gaussian process model m(X, y) in the next iteration â€” as the number of data points increases, m(X, y) becomes better at predicting the optimum of f(x)."
  },
  {
    "objectID": "qmd/tuning.html#sec-tuning-tpe",
    "href": "qmd/tuning.html#sec-tuning-tpe",
    "title": "Tuning",
    "section": "Tree-based Parzen Estimators (TPE)",
    "text": "Tree-based Parzen Estimators (TPE)\n\nMisc\n\nNotes from HyperOpt Demystified\nLibraries\n\n{{hyperopt}}\n\nTypically outperforms basic bayesian optimization, but the main selling point is it handles complex hyperparameter relationships via a tree structure.\nSupports categorical variables (cat hyperparams?) which traditional Bayesian optimization does not.\n\nProcess\n\nTrain a model with several sets of randomly-selected hyperparameters, returning objective function values.\nSplit our observed objective function values into â€œgoodâ€ and â€œbadâ€ groups, according to some threshold gamma (Î³).\nCalculate the â€œpromisingnessâ€ score, which is just P(x|good) / P(x|bad).\nDetermine the hyperparameters that maximize promisingness via mixture models.\nFit our model using the hyperparameters from step 4.\nRepeat steps 2â€“5 until a stopping criteria.\n\nTips/Tricks\n\nHyperOpt is parallelizable via both Apache Spark and MongoDB. If youâ€™re working with multiple cores, wether it be in the cloud or on your local machine, this can dramatically reduce runtime.\nIf youâ€™re parallelizing the tuning process via Apache Spark, use a SparkTrialsobject for single node ML models (sklearn) and a Trails object for parallelized ML models (MLlib).\nMLflow easily integrates with HyperOpt.\nDonâ€™t narrow down the search space too early. Some combinations of hyperparameters may be surprisingly effective.\nDefining the search space can be tricky, especially if you donâ€™t know the functional form of your hyperparameters. However, from personal experience TPE is pretty robust to misspecification of those functional forms.\nChoosing a good objective function goes a long way. In most cases, error is not created equal. If a certain type of error is more problematic, make sure to build that logic into to your function."
  },
  {
    "objectID": "qmd/tuning.html#sec-tuning-dectr",
    "href": "qmd/tuning.html#sec-tuning-dectr",
    "title": "Tuning",
    "section": "Decision Trees",
    "text": "Decision Trees\n\nNotes\nHyperparameters\n\nMaximum Depth (aka Tree Depth): maximum level a tree can â€œdescendâ€ during the training process\n\ntoo high may lead to overfit\ntoo low may lead to underfit\n\nMinimum Samples Split: control how many observations a node must contain to be available for a split\n\ntoo low may lead to overfit\ntoo high may lead to underfit\n\nMinimum Samples Leaf (aka Min N): number of observations in a node after the split has â€œpotentiallyâ€ happened\n\ntoo low may lead to overfit\ntoo high may lead to underfit\n\nMinimum Impurity Decrease: sets the threshold for the amount of impurity decrease that must occur in order for there to be another split\n\ntoo low may lead to overfit\ntoo high may lead to underfit\n\nMaximum Features: randomly choosing a set of features for each split\n\nUseful for high dimension datasets; adds some randomness\ntoo high can lead to long training times\ntoo low may lead to underfit"
  },
  {
    "objectID": "qmd/tuning.html#sec-tuning-rf",
    "href": "qmd/tuning.html#sec-tuning-rf",
    "title": "Tuning",
    "section": "Random Forest",
    "text": "Random Forest\n\nMisc\n\nImplicit features selection is performed by splitting, but performance decreases substantially if over 100 noise-like features are added & drastically if over 500 noise-like features\n\n\nHyperparameters\n\nmtry: the number of trees at each node\n\nMost influential hyperparameter for random forests.\nIncreasing it improves performance in the presence of noise\n\n\nDefault: square root of the number of features\nSimilar improvements can be had with (explicit) feature selection (e.g.Â recursive feature elimation)"
  },
  {
    "objectID": "qmd/tuning.html#sec-tuning-lgbm",
    "href": "qmd/tuning.html#sec-tuning-lgbm",
    "title": "Tuning",
    "section": "LightGBM",
    "text": "LightGBM\n\nNotes\n\nParameters are listed from most to least important\nSeems like the strategy should be to tune structure first, then move to accuracy or overfitting parameters based on results\nMissing values should be encoded as NA_integer_\nProcessing: it is recommended to rescale data before training so that features have similar mean and standard deviation\n\nHyperparameters\n\nStructure\n\nnum_leaves: the number of decision nodes in a tree\n\nkaggle recommendation: 2^(max_depth)\n\ntranslates to a range of 8 - 4096\n\n\nmax_depth: The complexity of each tree\n\nkaggle recommendation: 3 - 12\n\nmin_data_in_leaf: the minimum number of observations that fit the decision criteria in a leaf\n\nValue depends on sample size and num_leaves\nlightgbm doc recommendation: 100 - 10000 for large datasets\n\nlinear_tree (docs): fits piecewise linear gradient boosting tree\n\nTree splits are chosen in the usual way, but the model at each leaf is linear instead of constant\n\nThe first tree has constant leaf values\n\nHelps to extrapolate linear trends in forecasting\nCategorical features are used for splits as normal but are not used in the linear models\nIncreases memory use; no L1 regularization\n\n\nAccuracy\n\nn_estimators: controls the number of decision trees\nlearning_rate: step size parameter of the gradient descent\n\nKaggle recommendation: 0.01 - 0.3\n\nMoving outside this range is usually towards zero\n\n\nmax_bin: controls the maximum number of bins that continuous features will bucketed into\n\ndefault = 255\n\n\nOverfitting\n\nlambda_l1,Â lambda_l2: regularization\n\nDefault: 0\nKaggle recommendation: 0 - 100\n\nmin_gain_to_split: the reduction in training loss that results from adding a split point\n\nDefault: 0\nExtra regularization in large parameter grids\nReduces training time\n\nbagging_fraction: randomly select this percentage of data without resampling\n\nDefault: 1\n* must set bagging_freq to an integer *\n\nfeature_fraction: specifies the percentage of features to sample from when training each tree\n\nDefault: 1\n* Must set bagging_freq to an integer *\n\nbagging_freq: frequency for bagging\n\nDefault: 0 (disabled)\n(Integer) e.g.Â Setting to 2 means perform bagging at every 2nd iteration\n\nstopping_rounds: early stopping\n\n\nIssues (from docs)\n\nPoor Accuracy\n\nUse large max_binÂ (may be slower)\nUse small learning_rateÂ with largeÂ num_iterations\nUse large num_leaves (may cause over-fitting)\nUse bigger training data\n\nOverfitting\n\nUse small max_bin\nUse small num_leaves\nUse min_data_in_leaf andÂ min_sum_hessian_in_leaf\nUse bagging by set bagging_fraction andÂ bagging_freq\nUse feature sub-sampling by set feature_fraction\nUse bigger training data\nTry lambda_l1,Â lambda_l2 andÂ min_gain_to_split for regularization\nTry max_depthÂ to avoid growing deep tree\nTry extra_trees\nTry increasing path_smooth"
  },
  {
    "objectID": "qmd/tuning.html#sec-tuning-xgb",
    "href": "qmd/tuning.html#sec-tuning-xgb",
    "title": "Tuning",
    "section": "XGBoost",
    "text": "XGBoost\n\nNotes\n\nDrob starts with learning rate = 0.01 and tune other parameters before coming back to tune learning rate\nKuhn suggests setting trees to about 500 and tune stop_iter\n\nstop_iter: early stopping; stops if no improvement has been made after this many iterations\n\nUber found that the most important hyperparameters were:\n\ntree_depth, trees, learning_rate, and min_n\n\ntree_method (more details about exact, approx)- specify which tree construction algorithm you want to use. Trade-offs between accuracy and speed\n\nâ€œexactâ€ - accurate algorithm, but it is not very scalable as during each split find procedure it iterates over all entries of input data.\n\nInefficient when the data does not completely fit into memory\nDoesnâ€™t support distributed training\n\nâ€œapproxâ€ - uses quantile sketch and gradient histograms\nâ€œhistâ€ - method used in lightgbm with slight changes (binning continuous features)\n\nApplies some of approx performance enhancements (e.g.Â bin caching)\nTypically faster than approx\n\nâ€œgpu_histâ€ - gpu implementation of â€œhistâ€\n\nMuch faster than â€œhistâ€ and usually requires less memory\n\nGuessing because it uses the gpu memory\n\nNot available for some OSs (link)\n\nâ€œautoâ€ - Default. Chooses fastest method (exact or approx) based on data size\n\nFor larger datasets, approx will be used\n\n\n\nHyperparameters\n\ntrees: The number of trees in your forest\n\nGrid Value Examples: seq(50, 700, 50), seq(250, 1500, 25)\n\nmin_n: minimum number of data points in a node that is required for the node to be split further\n\naka minimum child weight\n\nmtry: The number of predictors to randomly sample for each split in a tree\n\nGrid Value Examples: c(3,5,7), c(5, 7, 9)\nMore Predictors â€“&gt; higher mtry\n\nShrinkage\n\nlearning_rate (aka eta) aka step size; explainer\n\nShrinks the feature weights to make the boosting process more conservative default: 0.3\nRange: [0,1]\nGrid value examples: 0.2, 0.01, 0.008, 0.005\nMore trees \\(\\rightarrow\\) lower learning rate\n\n\nTree-Booster Constraints\n\ntree_depth (aka max_depth): The complexity of each tree\n\nGrid value examples: 4 - 8 (also see lightgbm recommendations for max_depth)\nIncreasing makes the model more likely to overfit\nConsumes memory when training a deep tree.\nâ€œexactâ€ tree method requires non-zero value.\nDefault: 6\nRange: [0,âˆ]\n\nmin_child_weight\n\nMinimum sum of weights needed in a node in order for the algorithm to make another split\nIncreasing makes the model less likely to overfit\nDefault: 1\nRange: [0,âˆ]\n\n\nRegularization\n\nreg_lambda (aka lambda), reg_alpha (aka alpha), and gamma:\n\nThese are parameters of the regularization function\nAlso see\n\nAlgorithms, ML &gt;&gt; Boosting &gt;&gt; XGBoost &gt;&gt; Regularization\nArticle\n\nHigh Alpha: generates a sparses model (i.e.Â has many null leaf weights)\n\nL1 regularization term on weights\nSimplifies the model\nReduces the model size as itâ€™s not necessary to store null values.\n\nHigh Lambda: more stringently penalizes weights of less influential samples\n\nL2 regularization term on weights\nSimplifies the model\nHas largest effect when data is smaller\n\nHigh Gamma: fewer nodes\n\nAffects the amount of gain needed to add another split\nDefault: 0\nRange: [0,âˆ]\n\n\n\nRandom Subsampling\n\nsubsample:\n\nSetting it to 0.5 means that XGBoost would randomly sample half of the training data prior to growing trees. and this will prevent overfitting.\nSubsampling will occur once in every boosting iteration\nDefault: 1\nRange: (0,1]\n\ncolsample_bytree\n\nSubsample ratio of columns when constructing each tree.\nDefault: 1\n\n\nIteration Control\n\nnum_boost_round - Total number of boosting iterations\nearly_stopping_round - Validation metric needs to improve at least once in every early_stopping_rounds round(s) to continue training"
  },
  {
    "objectID": "qmd/tuning.html#sec-tuning-svm",
    "href": "qmd/tuning.html#sec-tuning-svm",
    "title": "Tuning",
    "section": "SVM",
    "text": "SVM\n\nMisc\n\nPackages: {e1071}, {kernlab}, {LiblineaR}, {{sklearn}}\nAlso see\n\nModel Building, tidymodels &gt;&gt; Model Specification &gt;&gt; Support Vector Machines\nAlgorithms, ML &gt;&gt; Support Vector Machines (SVM)\n\n\nHyperparameters\n\ngamma â€“ All the kernels except the linear one require the gamma parameter. ({e1071} default: 1/(data dimension)\ncoef0 â€“ Parameter needed for kernels of type polynomial and sigmoid ({e1071} default: 0).\ncost â€“ The cost of constraints violation ({e1071} default: 1)â€”it is the â€˜Câ€™-constant of the regularization term in the Lagrange formulation.\n\nC = 1/Î» (R) or 1/Î± (sklearn)\nWhen C is small, the regularization is strong, so the slope will be small\n\ndegree - Degree of the polynomial kernel function ({e1071} default: 3)\nepsilon - Needed for insensitive loss function (see Regression below) ({e1071} default: 0.1)\n\nWhen the value of epsilon is small, the model is robust to the outliers.\nWhen the value of epsilon is large, it will take outliers into account.\n\nnu - For {e1071}, needed for types: nu-classification, nu-regression, and one-classification"
  },
  {
    "objectID": "qmd/tuning.html#sec-tuning-ensemb",
    "href": "qmd/tuning.html#sec-tuning-ensemb",
    "title": "Tuning",
    "section": "Ensembles",
    "text": "Ensembles\n\nMisc\n\nNotes from\n\nHyperparameters Tuning for Machine Learning Model Ensembles\n\nSee article for\n\nDetails on performance, durations, etc. between sequential and simultaneous tuning methods\nLinks to repo, experimental paper\n\ntldr;\n\nThey like the simultaneous tuning because had the best metric performance with the lowest variance (more stable results)\nI think the sequential tuning method was comparable (and better in some cases) to the simultaneous tuning method but is probably faster and less costly.\n\n\n\nComposite Structures\n\nW/sequential model pipelines (bottom to top)\n\n\nâ€œdtrâ€ = decision tree regression\n\nW/feature engineering models\n\n\nOn the left side are typical structures with individual predictive models being fed into an ensemble model (e.g.Â Random Forest)\nOn the right side, some kind of feature engineering process or modeling happens prior to the predictive model/ensemble model.\n\n\n\nSequential Tuning\n\nOnly one model is tuned at a time\nThe scoring, during the tuning process, happens on the ensemble model\nExample\n\n\nStructure\n\n(top pipe) KNN feeds its predictions to the ridge regression which feeds itâ€™s predictions to the lasso regression (ensemble model)\n(bottom pipe) Ridge regression feeds its predictions to the random forest which feeds itâ€™s predictions to the lasso regression (ensemble model)\nRed arrows indicate how far the data/predictions have travelled through the structure. Here itâ€™s all the way to the ensemble model\nRed circle indictates which model is currently being tuned\nModels without hyperparameter values are models with default values for their hyperparameters\n\nFigure shows that the KNN and the RR (bottom pipe) models have already been tuned and the RR model (top pipe) is the model being tuned.\nThe red Yâ€™ indicates that the prediction scoring, while ridge regression is being tuned, is happening at the ensemble model.\nRF gets tuned next then finally the ensemble model\n\n\nSimultaneous Tuning\n\nAll models, including the ensemble model, are tuned at the same time\nComputationally expensive\n\nIâ€™d think it would be more monetarily expensive as well given that you likely have to provision more machines in any realistic scenario to get a decent training time (one for each pipe?)\n\nExample\n\n\nSee Sequential tuning example for details on the structure and what items in the figure represent."
  },
  {
    "objectID": "qmd/tuning.html#sec-tuning-dl",
    "href": "qmd/tuning.html#sec-tuning-dl",
    "title": "Tuning",
    "section": "DL",
    "text": "DL\n\nWeight Decay\n\n\nImproves data efficiency by &gt; 50%\nFrequently found in the best hyperparam configs\nAmong the most important hparams to tune\nTricky to tune\n\nLearning Rate Scheduling (article): The schedule reduces the learning rate as training progresses, so take smaller step sizes near and around the optimum\n\nTime-based Decay\n\\[\n\\alpha = \\frac{\\alpha_0}{1+\\text{decay} + \\text{epoch}}\n\\]\n\n\\(\\alpha\\): Learning Rate\n\\(\\alpha_0\\): Initial Learning Rate\n\\(\\text{decay}\\): Decay Rate\n\\(\\text{epoch}\\): Number of interations\n\nStep Decay\n\n\\[\n\\alpha = \\alpha_0 \\times \\operatorname{factor}^{\\frac{\\text{epoch}}{\\text{step}}}\n\\]\n\n\\(\\alpha\\): Learning Rate\n\\(\\alpha_0\\): Initial Learning Rate\n\\(\\text{factor}\\): Factor that the learning rate will be reduced by\n\\(\\text{epoch}\\): Number of interations\n\\(\\text{step}\\): Number of epochs after which the learning rate should be reduced\n\nExponential Decay\n\\[\n\\alpha = \\alpha_0 \\times e^{-\\text{decay} \\times \\text{epoch}}\n\\]\n\n\\(\\alpha\\): Learning Rate\n\\(\\alpha_0\\): Initial Learning Rate\n\\(\\text{epoch}\\): Number of interations\n\\(\\text{decay}\\): Decay Rate\n\nOthers: Performance Scheduling, 1Cycle Scheduling, and Power Scheduling."
  },
  {
    "objectID": "qmd/visualization.html#sec-vis-misc",
    "href": "qmd/visualization.html#sec-vis-misc",
    "title": "Visualization",
    "section": "Misc",
    "text": "Misc\n\nMicrosoft Paint 3D\n\nLocation: Start &gt;&gt; All Programs &gt;&gt; Paint 3D\nHightlight Text\n\nClick 2D Shapes (navbar) &gt;&gt; Select square (side panel)\nLeft click and hold &gt;&gt; Extend area around text you want to highlight &gt;&gt; Release\nChoose Line Type color and Sticker Opacity level (37%)\nOn area surrrounding text\n\nIf needed, make area size adjustment dragging little box-shaped icons that are along the outside\nOn the right side, click the check mark icon to finalize\n\nClick Menu (left-side on navbar) &gt;&gt; save as &gt;&gt; Image\n\nIt adds a png extension, but you just need to type the name.\n\n\n\nAlt Text\n\nThe guiding principle is to write alt text that gives disabled readers as close to the same experience as nondisabled readers as possible.\n\nBar Graphs\n\nDonâ€™t use bar graphs for anything except counts. Audiences have trouble with the abstraction.\nFor averages, used errorbar charts or use median + raincloud.\nGuide\n\n\nBox Plots\n\n\nSmall data - emphasize the points\nLarge data - emphasize the box\n\nLine Charts\n\nSometimes itâ€™s appropriate not to use zero as the baseline\nHaving the y-axis not intersect the x-axis can minimize the risk of confusing the readers with a non-zero baseline chart\nTime Series of ordinal discrete data by category\n\nordinal data has 3 levels\n\n\nggplot2: Donâ€™t use stat calculating geoms and set axis limits with scale_y_continuous\n\n\nSee examples of the behavior in this thread\n\nDefaults for any {ggplot2} geom using the default_aes field (i.e.Â GeomBlah$default_aes )"
  },
  {
    "objectID": "qmd/visualization.html#sec-vis-concepts",
    "href": "qmd/visualization.html#sec-vis-concepts",
    "title": "Visualization",
    "section": "Concepts",
    "text": "Concepts\n\nExploration and Analysis\n\nGoal: explore a new dataset, gertan overview, find answers to specific questions\nFast iteration of many generic charts, donâ€™t customize or worry about color schemes, etc.\n\nExplanation\n\nGoal: help others understand a relationship in the data\nUse as few charts as possible, carefully chosen\nSequence so that they are easy to understand\nAdd interaction to help people get a better understanding\n\nPresentation\n\nGoal: walk your audience through an argument, help them come to a decision\nFocus on polishing charts: colors, legends, titles, etc.\nHighlighting of key elements (which might be considered biasing in Exploration)\nPossibly use of unusual charts for memorability\nSequence to make a specific point"
  },
  {
    "objectID": "qmd/visualization.html#sec-vis-svg",
    "href": "qmd/visualization.html#sec-vis-svg",
    "title": "Visualization",
    "section": "SVG",
    "text": "SVG\n\nBetter for doing post-processing in Inkscape and gimp\nSVGs wonâ€™t be pixelated when you zoom in like PNGs are\nD3 outputs SVG\nsvglite PKG\n\nusing svglite instead of base::svg( ) allows you alter text in Inkscape or Illustrator\nrequires the used fonts to be present on the system it is viewed on.\n\nThe vast majority of interactive data visualizations on the web are now based on D3.js which often renders to SVG and it all seems to behave. Still, this is something to be mindful of, and a reason to use svg() if exactness of the rendered text is of prime importance\n\nFile size will be dramatically smaller"
  },
  {
    "objectID": "qmd/visualization.html#sec-vis-layout",
    "href": "qmd/visualization.html#sec-vis-layout",
    "title": "Visualization",
    "section": "Layout",
    "text": "Layout\n\nFacetting vs Single Graph\n\nLayout based on experiement design\n\nalign title ALL the way to the left (ggplot: plot.title.position = â€œplotâ€)\nremove legends\n\nuse colored text in title (ggtext)\nlabel points or lines\nlast resort: place legend underneath title/subtitle\n\ngrid lines\n\nremove if possible\nsparse and faint if needed\n\naxis labels\n\nremove if obvious (e.g brands of cars)\ncreate a title that informs about the axis labels\nshould always be horizontal\n\nflip axis, donâ€™t angle them 45 degrees\n\n\ntext\n\nleft-align most text\ncan center a subtitle if it helps with making the graph more symmetrical\nsome labels can be right-aligned\n\nRemove all borders\nMaximize white space\n\ndonâ€™t cram visuals together\n\nWorking memory. A cognitive limitation that affects plot comprehension is the limit on working memory. Typically, working memory is limited to approximately seven (plus or minus two) items, or chunks. In practice, this means that categorical scales with more than seven categories decrease readability, increase comprehension time, and require significant attentional resources, because it is not possible to hold the legend mapping in working memory.\nThe use of redundant aesthetics that activate the same gestalt principles (such as color and shape in a scatter plot, which both activate similarity) results in higher identification of corresponding data features. In addition, dual encoding increases the accessibility of a chart to individuals who have impaired color vision or perceptual processing (e.g., dyslexia, dysgraphia). This experimental evidence directly contradicts the guidelines popularized by Tufte (1991), which suggest the elimination of any feature that is not dedicated to representing the core data, including redundant encoding and other unnecessary graphical elements.\nggplot themes\n\nCedric Sherer (article)\ntheme_set(theme_minimal(base_size = 15, base_family = \"Anybody\"))\ntheme_update(\n  axis.title.x = element_text(margin = margin(12, 0, 0, 0), color = \"grey30\"),\n  axis.title.y = element_text(margin = margin(0, 12, 0, 0), color = \"grey30\"),\n  panel.grid.minor = element_blank(),\n  panel.border = element_rect(color = \"grey45\", fill = NA, linewidth = 1.5),\n  panel.spacing = unit(.9, \"lines\"),\n  strip.text = element_text(size = rel(1)),\n  plot.title = element_text(size = rel(1.4), face = \"bold\", hjust = .5),\n  plot.title.position = \"plot\"\n)"
  },
  {
    "objectID": "qmd/visualization.html#sec-vis-ar",
    "href": "qmd/visualization.html#sec-vis-ar",
    "title": "Visualization",
    "section": "Aspect Ratio",
    "text": "Aspect Ratio\n\nMisc\n\nGolden Rectangle\n```{{{r, fig.width = 6, fig.asp = 1.618}}}\n```\nGet consistent outputs\n\nRStudio pane displays in 72dpi which can mislead you on what your output looks like.\nThink {ragg} is supposed to have taken care of the inconsistency in terms of printing on different OSes\nUsing {camcorder}\n\nStart â€œrecordingâ€ plots\ncamcorder::gg_record(\n  dir = \"imgs\",\n  width = 12,\n  height = 12*9/16,\n  dpi = 300,\n  bg = \"white\"  # Makes sure background is actually white an not transparent\n)\n\nAll plots will immediately be exported as a .png-file to the directory specified\nAll plots will be displayed in the viewer with dimensions and resolution that you specified and not in the plots pane in RStudio\n300 dpi is pretty standard and default of ggsave\n\nDo work. Export final png file in directory when done and delete the rest\nRegarding Fonts\n\nIf using {ragg}, then all is fine.\nIf using {showtext}, then you have to set resolution in options, showtext_opts(dpi = 300)\n\n\n\n\nTwitter\n\nVideo: 1105 x 1920\n\nLine Charts\n\nMatters most if two different line charts are being compared\n\nThe core idea of â€œbankingâ€ is that the slopes in a line chart are most readable if they average to 45Â°.\nUse ggthemes::bank_slopes(x, y, method = c(\"ms\", \"as\"))\n\n2 methods (that req. no optimization) from Jeer, Maneesh who followed Clevelandâ€™s 45Â° guideline\ndocs\n\nâ€œThe problem with banking is that sometimes you need the chart in a certain aspect ratio to fit into a page layout. Especially if banking produces portrait sized charts. But why not let the optimal chart ratio define your layout? For instance, you can put the additional information to the side of the chart. Remember that the main goal of banking is to increase the readability of the line slopes. In the following example, the slopes for Nuclear and Renewables would have been much more difficult to see, if the chart would have been â€˜squeezedâ€™ to a landscape aspect.â€ (article)"
  },
  {
    "objectID": "qmd/visualization.html#sec-vis-typo",
    "href": "qmd/visualization.html#sec-vis-typo",
    "title": "Visualization",
    "section": "Typography",
    "text": "Typography\n\nCSS Length Units\n\nAbsolute Lengths\n\n* Pixels (px) are relative to the viewing device. For low-dpi devices, 1px is one device pixel (dot) of the display. For printers and high resolution screens 1px implies multiple device pixels.\n\n\ncm\ncentimeters\n\n\nmm\nmillimeters\n\n\nin\ninches (1in = 96px = 2.54cm)\n\n\npx*\npixels (1px = 1/96th of 1in)\n\n\npt\npoints (1pt = 1/72 of 1in)\n\n\npc\npicas (1pc = 12 pt)\n\n\n\nRelative Lengths\n\nThe em and rem units are practical in creating perfectly scalable layout! * Viewport = the browser window size. If the viewport is 50cm wide, 1vw = 0.5cm.\n\n\n\n\n\n\nem\nRelative to the font-size of the element (2em means 2 times the size of the current font)\n\n\nex\nRelative to the x-height of the current font (rarely used)\n\n\nch\nRelative to the width of the â€œ0â€ (zero)\n\n\nrem\nRelative to font-size of the root element\n\n\nvw\nRelative to 1% of the width of the viewport*\n\n\nvh\nRelative to 1% of the height of the viewport*\n\n\nvmin\nRelative to 1% of viewportâ€™s* smaller dimension\n\n\nvmax\nRelative to 1% of viewportâ€™s* larger dimension\n\n\n%\nRelative to the parent element\n\n\n\n\nFont Weight\n\n400 is the same as normal, and 700 is the same as bold\n\nFonts\n\nAdelle\n\nA serif font that doesnâ€™t go overboard. Good for short paragraphs.\n\nAlegreya\nBarlow\n\nSlender font\n\nFira Code Retina\n\ncode syntax highlighting\n@import url(â€œhttps://cdn.rawgit.com/tonsky/FiraCode/1.205/distr/fira_code.cssâ€);\n\nLora\n\nbody\nUsed in COVID-19 project &gt;&gt; Static Charts, Hospitals\n@import url(â€˜https://fonts.googleapis.com/css2?family=Lora&display=swapâ€™);\n\nMerriweather\n\nSimilar to Adelle, but has a bit more pronounced hooks\n\nMontserrat\n\nSimple design that can handle long lines of text. I like it for minimal plots.\n\nPrata\n\nheader\nUsed in ericbook-distill\n@import url(â€˜https://fonts.googleapis.com/css2?family=Cinzel&display=swapâ€™);\n\nReforma family\n\nonly one I have is Roboto, need to import and load the rest using extrafont pkg\n\nRoboto family\n\nDancho shiny apps\n\np, body: 100 wt\nHeaders, (h1, h2, etc.): 400 wt\n\nRoboto Slab\n\nNot sure if this is exact font used but itâ€™s very similar. Only difference I spotted was the â€œ3.â€\n\n\nTitillium Web Bold\n\nheaders\nUsed in ebtools\n@import url(â€˜https://fonts.googleapis.com/css?family=Titillium+Web&display=swapâ€™);\n\n\nNumbers\n\nshould all have the same height (Lining)\nshould all have the same width (Tabular)\n\nUsing {showtext}\nlibrary(showtext)\n#load font\nfont_add_google(name = \"Metal Mania\", family = \"metal\")\nfont_add_google(name = \"Montserrat\", family = \"montserrat\")\nshowtext_auto()"
  },
  {
    "objectID": "qmd/visualization.html#sec-vis-annot",
    "href": "qmd/visualization.html#sec-vis-annot",
    "title": "Visualization",
    "section": "Annotation",
    "text": "Annotation\n\nPeople love annotations (thread, paper). More text, the better.\n\nTheir takeaway from the chart is more likely to resemble the annotation if it takes the form of L2 and/or L4 and is close to the data\n\nExample: Financial Times\n\n\nTitle (L2) is used for part of the takaway message\n\nSubtitle used to describe the Y-Axis\n\nChart annotation paragraph (L4) gives contextual information\n\n\nWhen to annotate\n\na design element in your visualization that needs explaining\na data point or series that you want readers to see, like an outlier\nreaders should know something to better understand why certain data points look the way they do\n\nRemove the color key/legend and directly label your categories\n\nIf the screen is small (e.g.Â mobile), then itâ€™s better to keep the legend\n\nMake it obvious which units your data uses.\n\nDonâ€™t just put units in the description, but also in axis labels, tooltips, and annotations\n\nFor large numbers (e.g.Â 20 million), try to use B, M, K instead of an annotation somewhere that says something like â€œin thousandâ€\nTooltips\n\nConsider not just stating the numbers in tooltips, but also the category\n\ne.g.Â â€œ3.4% unemployedâ€ instead of â€œ3.4%,â€ or â€œ+16% revenueâ€ instead of â€œ+16%â€\n\nUse a transparent background by setting the alpha channel of CSS background-colorto a number less than 1\n\ne.g.Â 0.3 using rgba(255, 255, 255, 0.3)\n\nWith a transparent background, text behind the tooltip can interfere with the text in the tooltip, so also apply backdrop-filter\n\nExample:\n.tooltip {\nÂ  background-color: rgba(255, 255, 255, 0.3);\nÂ  -webkit-backdrop-filter: blur(2px);\nÂ  backdrop-filter: blur(2px);\n}\n@media (prefers-contrast: more) {\nÂ  .tooltip {\nÂ  Â  background-color: white;\nÂ  Â  -webkit-backdrop-filter: none;\nÂ  Â  backdrop-filter: none;\nÂ  }\n}\n\nExample shows a tooltip that has an HTML class of â€œtooltipâ€.\nblur is measured in pixels and the image size varies with screen width, so the optimal blur size here may vary for you depending on the dimensions of your browser window.\n\nApplies a Gaussian blur to the target elementâ€™s background with the standard deviation specified as the argument (e.g.Â two pixels).\n\nAs of Mar 2023, doesnâ€™t work on Safari, so adding -webkit-backdrop-filter allows it to work on Safari\n@media (prefers-contrast: more)checks if your user has informed their operating system or browser that they prefer increased contrast. When they do, this chunk then overrides the applied styles.\n\n\n\nTransparent backgrounds might work better with thematic maps and less with scatter plots\nDonâ€™t center-align your text\nUse straightforward phrasings\nMove axis labels nearest the most important chart objects (e.g.Â bars)\n\n\nIf the higher bars are whatâ€™s most important and theyâ€™re on the right, then usea right-side axis\n\nFonts for annotation\n\n\nUse what readers are most used to (e.g.Â sans-serif regular, &gt;12px, (almost) black text\nIf you need to need a lot of words and they donâ€™t fit, donâ€™t use smaller font, use a tooltip instead\n\nOn mobile screens you can also hide the least important annotations, or move them below the visualization\n\n\nLead the eye with font sizes, styles, and colors\n\n\nThe biggest and boldest text with the highest contrast against the background should be reserved for the most important information.\n\nDonâ€™t overdo it though\n\nUse only two levels of hierarchy that are clearly different from each other â€” like a 12px gray and a 14px black\nEmphasize within the annotations using boldness\n\nKeep labels horizontal\n\n\nUse a text outline\n\n\nSet the stroke around your letters, using the background color of your chart.\n\nBe conversational first and precise later"
  },
  {
    "objectID": "qmd/visualization.html#sec-vis-color",
    "href": "qmd/visualization.html#sec-vis-color",
    "title": "Visualization",
    "section": "Color",
    "text": "Color\n\nMisc\n\nWhen choosing bg and fg colors, keep in mind that itâ€™s generally a good idea to pick colors with a similar hue but a large difference in their luminance.\nDatawrapper guide\nWhen using several subplots together to tell a story and they each have their own color scheme. Blend a color into each color scheme to produce a more unified look\n\nExample: Blending blue into a plot with green color scheme.\n\n\nBreakpoints for scales\n\nHow to choose an interpolation for your color scale\n\nCharts (see prismatic PKG to do this manipulation within ggplot)\nPalette composition methods\n\nComplimentary\n\nopposite sides of the color wheel (2 colors)\ncontrast\n\nAnalogous\n\nsame side of the color wheel (multiple)\ngradient\n\nTriadic\n\nforms triangle on the color wheel\nvibrant, contrast\n\nOthers\n\nsplit complimentary (popular)\n\nComprised of one color and two colors symmetrically placed around it. This strategy adds more variety than complementary color schemes by including three hues without being too jarring or bold. Using this method, we end up with combinations that include warm and cool hues that are more easily balanced than the complementary color schemes\n\nquadratic\n\n\nAdjustments once you chosen a color (hue) to create variations\n\nMove brightness up for lighter variations and down for darker variations\nThen, move saturation in the opposite way you moved brightness\n\nSave colors you find attractive\n\ninstant eyedropper (windows)\nThen use HSL (hue, saturation, lightness) slider for adjustments\n\nBackgrounds\n\nWhite\n\nbright, used a lot\ntry ivory or a light gray\nshades of eggshell, link\n\nAvoid black (or REALLY dark) unless situation calls for it\n\ndark is fine\n\n\nWhat to do when you have a lot of categories\n\nSimply donâ€™t show different colors Does your chart work without colors?\n\n1 color and a discrete axis with the categories\n\nShow shades, not hues Can you make the chart less confetti-like?\n\nAlthough, consider not using shades when the parts are as or more important than the totals\n\nEmphasize Can you only use color for your most important categories?\nLabel directly Can you use the same or similar colors but label them?\nMerge categories Can you put categories together?\nGroup categories, but keep showing them Can strokes help to tell categories apart?\n\nChange the chart type Will another chart type rely less on colors?\nâ€œSmall multiplyâ€ it Can you split the categories into multiple charts? (i.e.Â facet by category)\nAdd other indicators Can you add symbols, patterns, line widths, or dashes?\n\n\nDoesnâ€™t use any color â€” just opacity, thickness, and dotted lines.\n\nUse tooltips and hover effects Can smaller categories be hidden with them?\n\nColor scales should be chosen to best match the data values and plot type: If the goal is to show magnitude, a univariate color scheme is typically preferable, while a double-ended color scale is typically more effective when showing data that differ in sign and magnitude. Where possible, color scales should use a minimal number of hues, varying intensity or lightness of the color to show magnitude, and transitioning through neutral colors (white, light yellow) when utilizing a gradient. Cognitive load can also be reduced by selecting colors with cultural associations that match the data display, such as the use of blue for men and red (or pink) for women, or the use of blue for cold temperatures and red/orange for warm temperatures.\n\nIt is also important to consider the human perceptual system, which does not perceive hues uniformly: We can distinguish more shades of green than any other hue, and fewer shades of yellow, so green univariate color schemes will provide finer discriminability than other colors because the human perceptual system evolved to work in the natural world, where shades of green are plentiful.\n\n\nFigure above shows the International Commission on Illumination (CIE) 1931 color space, which maps the wavelength of a color to a physiologically based perceptual space; a significant portion of the color space is dedicated to greens and blues, while much smaller regions are dedicated to violet, red, orange, and yellow colors. This unevenness in mapping color is one reason that the multi-hued rainbow color scheme is suboptimalâ€”the distance between points in a given color space may not be the same as the distance between points in perceptual space. As a result of the uneven mapping between color space and perceptual space, multi-hued color schemes are not recommended."
  },
  {
    "objectID": "qmd/visualization.html#sec-vis-maps",
    "href": "qmd/visualization.html#sec-vis-maps",
    "title": "Visualization",
    "section": "Maps",
    "text": "Maps\n\nabove rules also apply\nRemove as many extraneous elements as possible\n\nhard because maps have so many necessary elements\n\nborders, labels, etc.\n\nIn cloropleths, remove unnecessary borders (e.g.Â along coastlines)\n\n\nâ€œBorders as linesâ€ is much less cluttered\narticle, rmapshaper::ms_innerlines() keeps only the necessary inner borders in the â€œgeometryâ€ column of the spatial dataset.\n\n\nPay close attention to typography hierarchy\n\nBold, Font size, etc\n\nUse iconography to help users identify what you want them to see\nNumeric values (thread)\n\nPalettes: use a sequential (top row) or diverging (bottom row)\n\n\nFor diverging palettes\n\n\nThe middle value should be light on a light background (top left) or dark on a dark background (bottom left)\n\n\nBackgrounds:\n\n\nLight background: darker color on the value of interest (usually the higher value) (top left)\nDark background: lighter color on the value of interest (usually the higher value) (bottom left)\n\n\nTry not to use Rainbow palettes, because they are misleading\n\n(acceptable) rainbow called â€œTurboâ€ if you need one (article)\n\nCode - see comments for links to R scripts and improved versions of Turbo"
  },
  {
    "objectID": "qmd/visualization.html#sec-vis-area",
    "href": "qmd/visualization.html#sec-vis-area",
    "title": "Visualization",
    "section": "Area",
    "text": "Area\n\nIn general, these charts arenâ€™t good for noisy data and data with many categories\n\nHave issues when values increase sharply (see video. around 50:13)\n\nExperiment with the order of the groups\n\nEvents that youâ€™re looking for are probably only visable when thereâ€™s a particular order\nMost of the time, putting the most stable groups at the bottom produces the best results"
  },
  {
    "objectID": "qmd/visualization.html#sec-vis-ts",
    "href": "qmd/visualization.html#sec-vis-ts",
    "title": "Visualization",
    "section": "Time Series",
    "text": "Time Series\n\nHorizon Charts\n\nSee Anomaly Detection &gt;&gt; Charts\nEspecially useful for showing data with large amplitudes in a short vertical space"
  },
  {
    "objectID": "qmd/visualization.html#sec-vis-uncert",
    "href": "qmd/visualization.html#sec-vis-uncert",
    "title": "Visualization",
    "section": "Uncertainty",
    "text": "Uncertainty\n\nVisualizing only inferential uncertainty can lead to significant overestimates of treatment effects\n\n\nWhen possible, plot individual data points alongside statistical estimates\n\nTranslate percentages into counts (e.g.Â â€œa 1 out of 5 chanceâ€ rather than â€œa 20% chanceâ€)\n\n{riskyr} - icon arrays and less sophisticated viz for the above chart\nicon arrays\n\nExamples\n\nbase rates and error rates (paper)\nrelative risks (paper)\n\n\nWaffle plots are similar to icon arrays\n\nquantile dotplots\n\n{ggdist} (many examples and flavors)\n\nhypothetical outcome plots\n\nConsists of multiple individual plots (frames), each of which depicts one draw from a distribution (use case for animation)\nBest suited for multivariate judgments like how reliable a perceived difference between two random variables is\nIllustration of the process\n\nYou create a distribution to sample from or using known distribution and parameters or bootstrapping the sample and sample from each bootstrap.\nEach sample/draw is presented on the right side of the distribution plot (fig 1) (final product)\n\nI think it would be better if after each draw the previous draw remained but was de-emphasized (i.e.Â turned light gray)\nAnother example would McElreathâ€™s lecture video on posterior prediction distribution.\n\nFigs 2 and 3 show a sequence of draws from a joint distribution of uncorrelated variables (fig 2) and correlated variables (fig 3)\n\nExample: NYT on interpreting jobs reports\n\n2 facets: accelerating job growth (left), steady job growth (right)\nFor each facet,\n\nthe left plot is static, and the right plot is animated showing different noisy samples of the same underlying dgp\nthe left plot shows what normals perceive the distribution to look like for the given interpretation (e.g.Â accelerating job growth), and the right plot shows what real (i.e.Â noisy) data with the same interpretion looks like.\n\n\n\n\nFan charts\n\n\nshows a 90% interval broken divided into 30% increments (left) or 10% increments (right)\n\nShow previous forecasts\n\n\nTruth is in dark blue with light blue branches showing previous forecasts"
  },
  {
    "objectID": "qmd/visualization.html#sec-vis-mob",
    "href": "qmd/visualization.html#sec-vis-mob",
    "title": "Visualization",
    "section": "Mobile",
    "text": "Mobile\n\nMisc\n\nRStudio plots are displayed in 96 dpi and ggsave uses 300 dpi as default\n\ni.e.Â viewed plots wonâ€™t look the same as the saved plots using default settings\n\n\nUse sharp color contrasts when highlighting\nMinimal readable size is 16, but 22 is recommended\nAspect ratio of 4:3 or 1024 x 768 pixels\n\nAnother article say 1:2\n\nBar Charts should be horizontal to make charts with many categories readable\n\nMobile screens are more tall than wide so labels on the y-axis makes more sense than on the x-axis\n\nR\n\nSet-up external window with aspect ratio (e.g.Â 1:2)\ndev.new(width=1080, height=2160, unit=\"px\", noRStudioGD = TRUE)\n\nnoRStudioGD = TRUE says any new plots appear in the new graphics window rather than the RStudio graphics device\nCan also use windows(), x11(), or png() from {ragg}\n\n\nUse Quarto (or Rmd) for developement\n#| dpi: 300     \n#| fig.height: 7.2     \n#| fig.width: 3.6     \n#| dev: \"png\"     \n#| echo: false     \n#| warning: false     \n#| message: false`\n\nThis way your dpi and aspect ratio are set and you can view the final output without having to save the png and viewing it separately to see how it looks\nfig.height and fig.width are always given in inches\n\nIf you havenâ€™t set your Quarto document to be self-contained, then the images have also already been saved for you - probably in a folder called documentname_files/figure-html/"
  },
  {
    "objectID": "qmd/vs-code.html#sec-vsc-misc",
    "href": "qmd/vs-code.html#sec-vsc-misc",
    "title": "VS Code",
    "section": "Misc",
    "text": "Misc\n\nPress â€œ.â€ inside any page in github to create a vscode instance that opens that file."
  },
  {
    "objectID": "qmd/vs-code.html#sec-vsc-shcts",
    "href": "qmd/vs-code.html#sec-vsc-shcts",
    "title": "VS Code",
    "section": "Shortcuts",
    "text": "Shortcuts\n\n\n\n\n\n\n\nShortcut\nDescription\n\n\n\n\nCTRL/Command + Enter\nInsert a new line directly below, regardless of where you are in the current line\n\n\nALT/Option +Shift + Up/Down\nDuplicate your current row up or down\n\n\nALT/Option + Up/Down\nMove the current row up or down\n\n\nALT/Option + Shift + Right\nHit this twice to select everything within a current bracket(this option is called smartSelect.grow , if it needs to be re-mapped)\n\n\nCTRL/Command + /\nComment out the current line\n\n\nCTRL/Command + [ or ]\nIndent lines inward or outward"
  },
  {
    "objectID": "qmd/web-design.html#sec-webdes-misc",
    "href": "qmd/web-design.html#sec-webdes-misc",
    "title": "Web Design",
    "section": "Misc",
    "text": "Misc\n\nWebsites should be under 14kb (article)\n\nMost web servers TCP slow start algorithm starts by sending 10 TCP packets which works out to 14kb\n\n404 pages\n\nGuidelines\n\nbe brief: the message on the 404 page should be straightforward and easy to understand, informing the user that the page they were trying to access is not available.\nbe contrite: the tone of the 404 page should be friendly and apologetic, acknowledging the userâ€™s inconvenience and expressing empathy.\nbe helpful: provide links to other areas of the website or a search box that can help users find what theyâ€™re looking for quickly and easily.\nbe informative: include contact information, such as a feedback form, social media account, or email address to give users an alternative way to reach out to you for assistance.\nbe you: incorporate your brandâ€™s visual identity, including logos and colors, to help reinforce brand recognition and create a cohesive user experience.\n\nExamples\n\n404 Page SVG Animations That Maximize Visitor Retention\n21 Stunning 404 Pages to Convert Lost Visitors 2023\nguinslym/awesome-404: A curated list of awesome 404 web pages greynoiseâ€™s â€˜404â€™ equivalent and hrbmstrâ€™s."
  },
  {
    "objectID": "LICENSE.html#attribution-noncommercial-4.0-international",
    "href": "LICENSE.html#attribution-noncommercial-4.0-international",
    "title": "Creative Commons Attribution-NonCommercial 4.0 International License",
    "section": "Attribution-NonCommercial 4.0 International",
    "text": "Attribution-NonCommercial 4.0 International\n\nCreative Commons Corporation (â€œCreative Commonsâ€) is not a law firm and does not provide legal services or legal advice. Distribution of Creative Commons public licenses does not create a lawyer-client or other relationship. Creative Commons makes its licenses and related information available on an â€œas-isâ€ basis. Creative Commons gives no warranties regarding its licenses, any material licensed under their terms and conditions, or any related information. Creative Commons disclaims all liability for damages resulting from their use to the fullest extent possible.\nUsing Creative Commons Public Licenses\nCreative Commons public licenses provide a standard set of terms and conditions that creators and other rights holders may use to share original works of authorship and other material subject to copyright and certain other rights specified in the public license below. The following considerations are for informational purposes only, are not exhaustive, and do not form part of our licenses.\n\nConsiderations for licensors: Our public licenses are intended for use by those authorized to give the public permission to use material in ways otherwise restricted by copyright and certain other rights. Our licenses are irrevocable. Licensors should read and understand the terms and conditions of the license they choose before applying it. Licensors should also secure all rights necessary before applying our licenses so that the public can reuse the material as expected. Licensors should clearly mark any material not subject to the license. This includes other CC-licensed material, or material used under an exception or limitation to copyright. More considerations for licensors.\nConsiderations for the public: By using one of our public licenses, a licensor grants the public permission to use the licensed material under specified terms and conditions. If the licensorâ€™s permission is not necessary for any reasonâ€“for example, because of any applicable exception or limitation to copyrightâ€“then that use is not regulated by the license. Our licenses grant only permissions under copyright and certain other rights that a licensor has authority to grant. Use of the licensed material may still be restricted for other reasons, including because others have copyright or other rights in the material. A licensor may make special requests, such as asking that all changes be marked or described. Although not required by our licenses, you are encouraged to respect those requests where reasonable. More considerations for the public."
  },
  {
    "objectID": "LICENSE.html#creative-commons-attribution-noncommercial-4.0-international-public-license",
    "href": "LICENSE.html#creative-commons-attribution-noncommercial-4.0-international-public-license",
    "title": "Creative Commons Attribution-NonCommercial 4.0 International License",
    "section": "Creative Commons Attribution-NonCommercial 4.0 International Public License",
    "text": "Creative Commons Attribution-NonCommercial 4.0 International Public License\nBy exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution-NonCommercial 4.0 International Public License (â€œPublic Licenseâ€). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.\n\nSection 1 â€“ Definitions.\n\nAdapted Material means material subject to Copyright and Similar Rights that is derived from or based upon the Licensed Material and in which the Licensed Material is translated, altered, arranged, transformed, or otherwise modified in a manner requiring permission under the Copyright and Similar Rights held by the Licensor. For purposes of this Public License, where the Licensed Material is a musical work, performance, or sound recording, Adapted Material is always produced where the Licensed Material is synched in timed relation with a moving image.\nAdapterâ€™s License means the license You apply to Your Copyright and Similar Rights in Your contributions to Adapted Material in accordance with the terms and conditions of this Public License.\nCopyright and Similar Rights means copyright and/or similar rights closely related to copyright including, without limitation, performance, broadcast, sound recording, and Sui Generis Database Rights, without regard to how the rights are labeled or categorized. For purposes of this Public License, the rights specified in Section 2(b)(1)-(2) are not Copyright and Similar Rights.\nEffective Technological Measures means those measures that, in the absence of proper authority, may not be circumvented under laws fulfilling obligations under Article 11 of the WIPO Copyright Treaty adopted on December 20, 1996, and/or similar international agreements.\nExceptions and Limitations means fair use, fair dealing, and/or any other exception or limitation to Copyright and Similar Rights that applies to Your use of the Licensed Material.\nLicensed Material means the artistic or literary work, database, or other material to which the Licensor applied this Public License.\nLicensed Rights means the rights granted to You subject to the terms and conditions of this Public License, which are limited to all Copyright and Similar Rights that apply to Your use of the Licensed Material and that the Licensor has authority to license.\nLicensor means the individual(s) or entity(ies) granting rights under this Public License.\nNonCommercial means not primarily intended for or directed towards commercial advantage or monetary compensation. For purposes of this Public License, the exchange of the Licensed Material for other material subject to Copyright and Similar Rights by digital file-sharing or similar means is NonCommercial provided there is no payment of monetary compensation in connection with the exchange.\nShare means to provide material to the public by any means or process that requires permission under the Licensed Rights, such as reproduction, public display, public performance, distribution, dissemination, communication, or importation, and to make material available to the public including in ways that members of the public may access the material from a place and at a time individually chosen by them.\nSui Generis Database Rights means rights other than copyright resulting from Directive 96/9/EC of the European Parliament and of the Council of 11 March 1996 on the legal protection of databases, as amended and/or succeeded, as well as other essentially equivalent rights anywhere in the world.\nYou means the individual or entity exercising the Licensed Rights under this Public License. Your has a corresponding meaning.\n\n\n\nSection 2 â€“ Scope.\n\nLicense grant.\n\nSubject to the terms and conditions of this Public License, the Licensor hereby grants You a worldwide, royalty-free, non-sublicensable, non-exclusive, irrevocable license to exercise the Licensed Rights in the Licensed Material to:\nA. reproduce and Share the Licensed Material, in whole or in part, for NonCommercial purposes only; and\nB. produce, reproduce, and Share Adapted Material for NonCommercial purposes only.\nExceptions and Limitations. For the avoidance of doubt, where Exceptions and Limitations apply to Your use, this Public License does not apply, and You do not need to comply with its terms and conditions.\nTerm. The term of this Public License is specified in Section 6(a).\nMedia and formats; technical modifications allowed. The Licensor authorizes You to exercise the Licensed Rights in all media and formats whether now known or hereafter created, and to make technical modifications necessary to do so. The Licensor waives and/or agrees not to assert any right or authority to forbid You from making technical modifications necessary to exercise the Licensed Rights, including technical modifications necessary to circumvent Effective Technological Measures. For purposes of this Public License, simply making modifications authorized by this Section 2(a)(4) never produces Adapted Material.\nDownstream recipients.\nA. Offer from the Licensor â€“ Licensed Material. Every recipient of the Licensed Material automatically receives an offer from the Licensor to exercise the Licensed Rights under the terms and conditions of this Public License.\nB. No downstream restrictions. You may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, the Licensed Material if doing so restricts exercise of the Licensed Rights by any recipient of the Licensed Material.\nNo endorsement. Nothing in this Public License constitutes or may be construed as permission to assert or imply that You are, or that Your use of the Licensed Material is, connected with, or sponsored, endorsed, or granted official status by, the Licensor or others designated to receive attribution as provided in Section 3(a)(1)(A)(i).\n\nOther rights.\n\nMoral rights, such as the right of integrity, are not licensed under this Public License, nor are publicity, privacy, and/or other similar personality rights; however, to the extent possible, the Licensor waives and/or agrees not to assert any such rights held by the Licensor to the limited extent necessary to allow You to exercise the Licensed Rights, but not otherwise.\nPatent and trademark rights are not licensed under this Public License.\nTo the extent possible, the Licensor waives any right to collect royalties from You for the exercise of the Licensed Rights, whether directly or through a collecting society under any voluntary or waivable statutory or compulsory licensing scheme. In all other cases the Licensor expressly reserves any right to collect such royalties, including when the Licensed Material is used other than for NonCommercial purposes.\n\n\n\n\nSection 3 â€“ License Conditions.\nYour exercise of the Licensed Rights is expressly made subject to the following conditions.\n\nAttribution.\n\nIf You Share the Licensed Material (including in modified form), You must:\nA. retain the following if it is supplied by the Licensor with the Licensed Material:\n\nidentification of the creator(s) of the Licensed Material and any others designated to receive attribution, in any reasonable manner requested by the Licensor (including by pseudonym if designated);\na copyright notice;\na notice that refers to this Public License;\na notice that refers to the disclaimer of warranties;\na URI or hyperlink to the Licensed Material to the extent reasonably practicable;\n\nB. indicate if You modified the Licensed Material and retain an indication of any previous modifications; and\nC. indicate the Licensed Material is licensed under this Public License, and include the text of, or the URI or hyperlink to, this Public License.\nYou may satisfy the conditions in Section 3(a)(1) in any reasonable manner based on the medium, means, and context in which You Share the Licensed Material. For example, it may be reasonable to satisfy the conditions by providing a URI or hyperlink to a resource that includes the required information.\nIf requested by the Licensor, You must remove any of the information required by Section 3(a)(1)(A) to the extent reasonably practicable.\nIf You Share Adapted Material You produce, the Adapterâ€™s License You apply must not prevent recipients of the Adapted Material from complying with this Public License.\n\n\n\n\nSection 4 â€“ Sui Generis Database Rights.\nWhere the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material:\n\nfor the avoidance of doubt, Section 2(a)(1) grants You the right to extract, reuse, reproduce, and Share all or a substantial portion of the contents of the database for NonCommercial purposes only;\nif You include all or a substantial portion of the database contents in a database in which You have Sui Generis Database Rights, then the database in which You have Sui Generis Database Rights (but not its individual contents) is Adapted Material; and\nYou must comply with the conditions in Section 3(a) if You Share all or a substantial portion of the contents of the database.\n\nFor the avoidance of doubt, this Section 4 supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.\n\n\nSection 5 â€“ Disclaimer of Warranties and Limitation of Liability.\n\nUnless otherwise separately undertaken by the Licensor, to the extent possible, the Licensor offers the Licensed Material as-is and as-available, and makes no representations or warranties of any kind concerning the Licensed Material, whether express, implied, statutory, or other. This includes, without limitation, warranties of title, merchantability, fitness for a particular purpose, non-infringement, absence of latent or other defects, accuracy, or the presence or absence of errors, whether or not known or discoverable. Where disclaimers of warranties are not allowed in full or in part, this disclaimer may not apply to You.\nTo the extent possible, in no event will the Licensor be liable to You on any legal theory (including, without limitation, negligence) or otherwise for any direct, special, indirect, incidental, consequential, punitive, exemplary, or other losses, costs, expenses, or damages arising out of this Public License or use of the Licensed Material, even if the Licensor has been advised of the possibility of such losses, costs, expenses, or damages. Where a limitation of liability is not allowed in full or in part, this limitation may not apply to You.\nThe disclaimer of warranties and limitation of liability provided above shall be interpreted in a manner that, to the extent possible, most closely approximates an absolute disclaimer and waiver of all liability.\n\n\n\nSection 6 â€“ Term and Termination.\n\nThis Public License applies for the term of the Copyright and Similar Rights licensed here. However, if You fail to comply with this Public License, then Your rights under this Public License terminate automatically.\nWhere Your right to use the Licensed Material has terminated under Section 6(a), it reinstates:\n\nautomatically as of the date the violation is cured, provided it is cured within 30 days of Your discovery of the violation; or\nupon express reinstatement by the Licensor.\n\nFor the avoidance of doubt, this Section 6(b) does not affect any right the Licensor may have to seek remedies for Your violations of this Public License.\nFor the avoidance of doubt, the Licensor may also offer the Licensed Material under separate terms or conditions or stop distributing the Licensed Material at any time; however, doing so will not terminate this Public License.\nSections 1, 5, 6, 7, and 8 survive termination of this Public License.\n\n\n\nSection 7 â€“ Other Terms and Conditions.\n\nThe Licensor shall not be bound by any additional or different terms or conditions communicated by You unless expressly agreed.\nAny arrangements, understandings, or agreements regarding the Licensed Material not stated herein are separate from and independent of the terms and conditions of this Public License.\n\n\n\nSection 8 â€“ Interpretation.\n\nFor the avoidance of doubt, this Public License does not, and shall not be interpreted to, reduce, limit, restrict, or impose conditions on any use of the Licensed Material that could lawfully be made without permission under this Public License.\nTo the extent possible, if any provision of this Public License is deemed unenforceable, it shall be automatically reformed to the minimum extent necessary to make it enforceable. If the provision cannot be reformed, it shall be severed from this Public License without affecting the enforceability of the remaining terms and conditions.\nNo term or condition of this Public License will be waived and no failure to comply consented to unless expressly agreed to by the Licensor.\nNothing in this Public License constitutes or may be interpreted as a limitation upon, or waiver of, any privileges and immunities that apply to the Licensor or You, including from the legal processes of any jurisdiction or authority.\n\n\nCreative Commons is not a party to its public licenses. Notwithstanding, Creative Commons may elect to apply one of its public licenses to material it publishes and in those instances will be considered the â€œLicensor.â€ Except for the limited purpose of indicating that material is shared under a Creative Commons public license or as otherwise permitted by the Creative Commons policies published at creativecommons.org/policies, Creative Commons does not authorize the use of the trademark â€œCreative Commonsâ€ or any other trademark or logo of Creative Commons without its prior written consent including, without limitation, in connection with any unauthorized modifications to any of its public licenses or any other arrangements, understandings, or agreements concerning use of licensed material. For the avoidance of doubt, this paragraph does not form part of the public licenses.\nCreative Commons may be contacted at creativecommons.org"
  },
  {
    "objectID": "qmd/db-duckdb.html#remote-connections",
    "href": "qmd/db-duckdb.html#remote-connections",
    "title": "DuckDB",
    "section": "Remote Connections",
    "text": "Remote Connections\n\nMisc\n\nNotes from\n\nQuery Remote Parquet Files with DuckDB\n\n\nhttpfs Extension\n\nCreate a db in memory since the data is stored remotely.\nconn &lt;- \n  DBI::dbConnect(\n    duckdb::duckdb(),\n    dbdir = \":memory:\"\n  )\nInstall and Load httpfs extension\nDBI::dbExecute(conn, \"INSTALL httpfs;\")\nDBI::dbExecute(conn, \"LOAD httpfs;\")\n\nCurrently not available for Windows\n\nQuery\nparquet_url &lt;- \"url_to_parquet_files\"\nres &lt;- DBI::dbGetQuery(\n  conn, \n  glue::glue(\"SELECT carrier, flight, tailnum, year FROM '{parquet_url}' WHERE year = 2013 LIMIT 100\")\n)\n\nQueries that needs more data and return more rows takes longer to run, especially transmitting data over the Internet. Craft carefully your queries with this in mind.\n\nTo use {dplyr}, a View must first be created\nDBI::dbExecute(conn, \n               glue::glue(\"CREATE VIEW flights AS SELECT * FROM PARQUET_SCAN('{parquet_url}')\"))\nDBI::dbListTables(conn)\n#&gt; [1] \"flights\"\n\ntbl(conn, \"flights\") %&gt;%\n  group_by(month) %&gt;%\n  summarise(freq = n()) %&gt;%\n  ungroup() %&gt;%\n  collect()\nClose connection: DBI::dbDisconnect(conn, shutdown = TRUE)\n\n{duckdbfs}\n\nCreate dataset object\nparquet_url &lt;- \"url_to_parquet_files\" #e.g. AWS S3\nds &lt;- duckdbfs::open_dataset(parquet_url)\nQuery\nds %&gt;%\n  group_by(month) %&gt;%\n  summarise(freq = n()) %&gt;%\n  ungroup() %&gt;%\n  collect()"
  },
  {
    "objectID": "qmd/geospatial-remote-sensing.html#pattern-based",
    "href": "qmd/geospatial-remote-sensing.html#pattern-based",
    "title": "38Â  Remote Sensing",
    "section": "38.3 Pattern-based",
    "text": "38.3 Pattern-based\n\nEnables spatial analyses such as searching, change detection, clustering, or segmentation\nMisc\n\nNotes from Analysis of Spatial Patterns: Current State and Future Challenges (Slides)\n\nSteps\n\nDivide data into a large number of smaller areas (local landscapes)\nRepresent each area using a statistical description of the spatial pattern - a spatial signature.\n\nMost landscape metrics are single numbers representing specific features of a local landscape. Spatial signatures, on the other hand, are multi-element representations of landscape composition and configuration.\nThe basic signature is the co-occurrence matrix:\n\n\n\n\nagriculture\nforest\ngrassland\nwater\n\n\n\n\nagriculture\n272\n218\n4\n0\n\n\nforest\n218\n38778\n32\n12\n\n\ngrassland\n4\n32\n16\n0\n\n\nwater\n0\n12\n0\n2\n\n\n\n\nI believe this is a comparison of two local landscapes where, for example, ag vs grass = 4 indicates there are 4 grid cells that coincide to a grassland in one local area and an agricultural area in the other local area.\n\nA spatial signature should allow simplification to the form of a normalized vector\nNormalized Co-Occurence Vector:\n\nCo-Occurence Vector (cove) - c(272, 218, 4, 0, 218, 38778, 32, 12, 4, 32, 16, 0, 0, 12, 0, 2)\n\nNumbers are taken from the co-occurrence matrix (See above) where the rows are combined end-to-end to create a vector.\n\nSimplified Co-Occurence Vector (cove) - c(136 , 218, 19389, 4, 32, 8, 0, 12, 0, 1)\n\nThe process name wasnâ€™t mentioned in the slide, but here, the cove has been simplified by creating a vector with the halved diagonal values and unique values of off-diagonal cells in the original cove (Doubt order matters).\n\nNormalized Co-Occurence Vector\nsimple_cove &lt;- c(136, 218, 19389, 4, 32, 8, 0, 12, 0, 1)\nmoose &lt;- as.matrix(simple_cove)\nround(simple_cove/norm(moose), 4)\n#&gt; [1] 0.0069 0.0110 0.9792 0.0002 0.0016 0.0004 0.0000 0.0006 0.0000 0.0001\n\n\nSpatial signatures can be compared using a large number of existing distance or dissimilarity measures\n\nDissimilarity\n\nMeasuring the distance between two signatures in the form of normalized vectors allows determining dissimilarity between spatial structures.\nExample: Jensen-Shannon Divergence - Lower\n\n\n\n\n\nReference (cove_ref)\n\n\n\n\n\n\n\nArea of Interest (cove_x1)\n\n\n\n\n\n\ncove_ref &lt;- c(0.0069, 0.011, 0.9792, 0.0002, 0.0016, 0.0004, 0, 0.0006, 0, 0.0001)\ncove_x1 &lt;- c(0.1282, 0.0609, 0.8105, 0.0002, 0.0002, 0.0001, 0, 0, 0, 0)\ncove_mat1 &lt;- rbind(cove_ref, cove_x1)\n\nphilentropy::JSD(cove_mat)\n#&gt; jensen-shannon \n#&gt;     0.06826663\n\nA lower JSD means the two images are less dissimilar (i.e more similar)\n\nExample: Jensen-Shannon Divergence - Higher\n\n\n\n\n\nReference (cove_ref_ext)\n\n\n\n\n\n\n\nArea of Interest (cove_x1)\n\n\n\n\n\n\n# zeros added to match length of cov_x2\ncove_ref_ext &lt;- c(0.0069, 0.011, 0.9792, 0.0002, 0.0016, 0.0004, 0, 0.0006, 0, 0.0001, 0, 0, 0, 0, 0)\ncove_x2 &lt;- c(0.2033, 0.1335, 0.2944, 0.1747, 0.0562, 0.1307, 0.0035, 0.0002, 0.0004, 0.0015, 0.0007, 0.0005, 0, 0, 0.0005)\ncove_mat2 &lt;- rbind(cove_ref_ext, cove_x2)\n\nphilentropy::JSD(cove_mat2)\n#&gt; jensen-shannon \n#&gt;      0.4444198 \n\nA higher JSD means the two images are more dissimilar."
  },
  {
    "objectID": "qmd/geospatial-remote-sensing.html#sec-geo-rs-misc",
    "href": "qmd/geospatial-remote-sensing.html#sec-geo-rs-misc",
    "title": "Remote Sensing",
    "section": "Misc",
    "text": "Misc\n\nResources\n\nGLCM Texture: A Tutorial\n\nTypes of Measures\n\nTexture - Descriptive statistic that measures spatial relationships\n\nValues cannot be transferred from one situation to another\n\ne.g.Â you canâ€™t say, â€œforests always have Contrast values between .5 and .7â€\n\nPrimarily useful in comparing one part of an image to another part\n\nFor multi-image comparison (e.g.Â mosaic):\n\nThe images analysed must be equivalent radiometrically, in regards to sun angle, and phenologically with regards to cyclically variable ground phenomena\n\n\n\nSpectral - Descriptive statistics that essentially measure chemical properties of the ground objects\n\nSpectral and spatial are very likely to be independent data and so complement one another\nGrey Level Co-occurrence Matrix (GCLM) - Used for texture measurements. A tabulation of how often different combinations of pixel brightness values (grey levels) occur in an image.\nPCA Issues\n\nEach new dataset requires recalculation of both, landscape metrics and principal components analysis (PCA)\nHighly correlated landscape metrics are used\nPCA results interpretation is not straightforward\n\nInformation Theory (IT) Based Metrics\n\nMarginal entropy [H(x)] - Diversity (composition) of spatial categories - from monothematic patterns to multithematic patterns\nRelative Mutual Information [U] - Clumpiness (configuration) of spatial categories from fragmented patterns to consolidated patterns)\nH(x) and U are uncorrelated\nIssues\n\nRelative mutual information is a result of dividing mutual information by entropy. What to do when the entropy is zero?\nHow to incorporate the meaning of categories into the analysis?"
  },
  {
    "objectID": "qmd/geospatial-remote-sensing.html#sec-geo-rs-terms",
    "href": "qmd/geospatial-remote-sensing.html#sec-geo-rs-terms",
    "title": "Remote Sensing",
    "section": "Terms",
    "text": "Terms\n\nNormalized Difference Vegetation Index (NDVI) - A widely-used metric for quantifying the health and density of vegetation using sensor data. It is calculated from spectrometric data at two specific bands: red and near-infrared. The spectrometric data is usually sourced from remote sensors, such as satellites.\n\nRange: -1 and 1\nInterpretation\n\n0: Area has nothing growing (e.g.Â Deserts)\n1: Arean has dense, healthy vegetation\n&lt;0: Suggest lack of dry land (e.g.Â oceans have NDVI = -1)\n\n\nSemantic Segmentation - The process of labelling pixels or regions of the image\n\nEssential in many applications including infrastructure planning, land cover, humanitarian crisis maps and environmental assessments."
  },
  {
    "objectID": "qmd/geospatial-remote-sensing.html#sec-geo-rs-patt",
    "href": "qmd/geospatial-remote-sensing.html#sec-geo-rs-patt",
    "title": "Remote Sensing",
    "section": "Pattern-based",
    "text": "Pattern-based\n\nEnables spatial analyses of raster data such as searching, change detection, clustering, or segmentation\nMisc\n\nNotes from Analysis of Spatial Patterns: Current State and Future Challenges (Slides)\n\nUse-Cases\n\nFinding similar spatial structures (one-to-many)\n\n\nTake the normalized cove of the mountain (or other structure) and compare other local areas with it. (i.e.Â which coves are least dissimilar the mountain cove)\n\nQuantitative assessment of changes in spatial structures (one-to-one)\n\n\nThis pic represents the change in land coverage in the Amazon from two different time periods\nTake the normalized coves from the earlier time period and make a one-to-one comparison (i.e.Â Calculate the difference in JSDs) with the coves of the currrent time period\nAreas with the greatest change have the highest JSD values.\n\nClustering similar spatial structure (many-to-many)\n\n\nCluster the normalized coves\nMetrics\n\nIntra-cluster heterogeneity - determines distances between all landscapes within a group\nInter-cluster isolation - determines distances between a given group and all others\n\n\n\nSteps\n\nDivide data into a large number of smaller areas (local landscapes)\nRepresent each area using a statistical description of the spatial pattern - a spatial signature.\n\nMost landscape metrics are single numbers representing specific features of a local landscape. Spatial signatures, on the other hand, are multi-element representations of landscape composition and configuration.\nThe basic signature is the co-occurrence matrix:\n\n\n\n\nagriculture\nforest\ngrassland\nwater\n\n\n\n\nagriculture\n272\n218\n4\n0\n\n\nforest\n218\n38778\n32\n12\n\n\ngrassland\n4\n32\n16\n0\n\n\nwater\n0\n12\n0\n2\n\n\n\n\nLand Coverage Categories: agriculture, forest, grassland, water, wetland, settlement, shrubland, sparse vegetation, bare area.\nLandform Categories: flat or nearly flat plains, smooth plains with some local relief, irregular plains with moderate relief, irregular plains with low hills, scattered moderate hills, moderate hills, scattered high hills, high hills, scattered low mountains, low mountains, scattered high mountains, high mountains, tablelands with moderate relief, tablelands with considerable relief, tablelands with high relief, tablelands with very high relief, surface water.\nI believe this is a comparison of two local landscapes where, for example, ag vs grass = 4 indicates there are 4 grid cells that coincide to a grassland in one local area and an agricultural area in the other local area.\n\nA spatial signature should allow simplification to the form of a normalized vector\nNormalized Co-Occurence Vector:\n\nCo-Occurence Vector (cove) - c(272, 218, 4, 0, 218, 38778, 32, 12, 4, 32, 16, 0, 0, 12, 0, 2)\n\nNumbers are taken from the co-occurrence matrix (See above) where the rows are combined end-to-end to create a vector.\n\nSimplified Co-Occurence Vector (cove) - c(136 , 218, 19389, 4, 32, 8, 0, 12, 0, 1)\n\nThe process name wasnâ€™t mentioned in the slide, but here, the cove has been simplified by creating a vector with the halved diagonal values and unique values of off-diagonal cells in the original cove (Doubt order matters).\n\nNormalized Co-Occurence Vector\nsimple_cove &lt;- c(136, 218, 19389, 4, 32, 8, 0, 12, 0, 1)\nmoose &lt;- as.matrix(simple_cove)\nround(simple_cove/norm(moose), 4)\n#&gt; [1] 0.0069 0.0110 0.9792 0.0002 0.0016 0.0004 0.0000 0.0006 0.0000 0.0001\n\n\nSpatial signatures can be compared using a large number of existing distance or dissimilarity measures\n\nDissimilarity\n\nMeasuring the distance between two signatures in the form of normalized vectors allows determining dissimilarity between spatial structures.\nExample: Jensen-Shannon Divergence - Lower\n\n\n\n\n\nReference (cove_ref)\n\n\n\n\n\n\n\nArea of Interest (cove_x1)\n\n\n\n\n\n\ncove_ref &lt;- c(0.0069, 0.011, 0.9792, 0.0002, 0.0016, 0.0004, 0, 0.0006, 0, 0.0001)\ncove_x1 &lt;- c(0.1282, 0.0609, 0.8105, 0.0002, 0.0002, 0.0001, 0, 0, 0, 0)\ncove_mat1 &lt;- rbind(cove_ref, cove_x1)\n\nphilentropy::JSD(cove_mat)\n#&gt; jensen-shannon \n#&gt;     0.06826663\n\nA lower JSD means the two images are less dissimilar (i.e more similar)\n\nExample: Jensen-Shannon Divergence - Higher\n\n\n\n\n\nReference (cove_ref_ext)\n\n\n\n\n\n\n\nArea of Interest (cove_x1)\n\n\n\n\n\n\n# zeros added to match length of cov_x2\ncove_ref_ext &lt;- c(0.0069, 0.011, 0.9792, 0.0002, 0.0016, 0.0004, 0, 0.0006, 0, 0.0001, 0, 0, 0, 0, 0)\ncove_x2 &lt;- c(0.2033, 0.1335, 0.2944, 0.1747, 0.0562, 0.1307, 0.0035, 0.0002, 0.0004, 0.0015, 0.0007, 0.0005, 0, 0, 0.0005)\ncove_mat2 &lt;- rbind(cove_ref_ext, cove_x2)\n\nphilentropy::JSD(cove_mat2)\n#&gt; jensen-shannon \n#&gt;      0.4444198 \n\nA higher JSD means the two images are more dissimilar."
  },
  {
    "objectID": "qmd/geospatial-general.html#sec-geo-gen-misc",
    "href": "qmd/geospatial-general.html#sec-geo-gen-misc",
    "title": "General",
    "section": "Misc",
    "text": "Misc\n\nQGIS - free and open source\nArcGIS - expensive and industry-standard\nspatiotemporal data â€” data cubes with spatial and regular temporal dimensions â€” such as\n\ne.g.Â gridded temperature values (raster time series) and vector data with temporal records at regular temporal instances (e.g.Â election results in states).\n\n{stars} - regular intervals\n{sftime} - irregular intervals\n\n\nSpatial Resampling\n\nCreates cross-validation folds by k-means clustering coordinate variables\nlibrary(tidymodels)\nlibrary(spatialsample)\nset.seed(123)\nspatial_splits &lt;- spatial_clustering_cv(landslides, coords = c(\"x\", \"y\"), v = 5)\n\n# fit a logistic model\nglm_spec &lt;- logistic_reg()\nlsl_form &lt;- lslpts ~ slope + cplan + cprof + elev + log10_careaÂ \nlsl_wf &lt;- workflow(lsl_form, glm_spec)\ndoParallel::registerDoParallel()Â \nregular_rs &lt;- fit_resamples(lsl_wf, bad_folds)"
  },
  {
    "objectID": "qmd/geospatial-general.html#sec-geo-gen-terms",
    "href": "qmd/geospatial-general.html#sec-geo-gen-terms",
    "title": "General",
    "section": "Terms",
    "text": "Terms\n\nCensus Block Groups - ~600â€“3,000 population; the smallest geography reported; Wiki\nCensus Tract - ~4,000 average population; Docs\n\nAlso see Survey, Census Data &gt;&gt; Geographies\n\nGraticules - a network of lines on a map that delineate the geographic coordinates (degrees of latitude and longitude.)\n\nUse of graticules is not advised, unless the graphical output will be used for measurement or navigation, or the direction of North is important for the interpretation of the content, or the content is intended to display distortions and artifacts created by projection. Unnecessary use of graticules only adds visual clutter but little relevant information. Use of coastlines, administrative boundaries or place names permits most viewers of the output to orient themselves better than a graticule\n{sf::st_graticule}\n\nVRT - File format that allows a virtual GDAL dataset to be composed from other GDAL datasets with repositioning, and algorithms potentially applied as well as various kinds of metadata altered or added. VRT descriptions of datasets can be saved in an XML format normally given the extension .vrt.\n\nBasically a metadata XML file describing various properties of the actual raster file, like pixel dimensions, geolocation, etc.."
  },
  {
    "objectID": "qmd/geospatial-general.html#sec-geo-gen-opt",
    "href": "qmd/geospatial-general.html#sec-geo-gen-opt",
    "title": "General",
    "section": "Optimization",
    "text": "Optimization\n\nMisc\nVector Tiles\n\nMisc\n\nNotes from Push the limits of interactive mapping in R with vector tiles\n\nMcBain goes through a complete example with plenty of tips on simplification strategies and hosting mbtiles files\n\nIssues (solution: Vector Tiles)\n\nLimited number of features with DOM canvas\n\nThereâ€™s a limit to how many features leaflet maps can handle, because at some point the DOM gets too full and your browser stops being able to parse it.\n\nLimited number of maps on same webpage\n\nOnce you start rendering spatial data on WebGL canvasses instead of the DOM youâ€™ll find there is a low number of WebGL contexts that can co-exist on any one web page, typically limiting you to only around 8 maps.\n\nFile sizes blow up to hundreds of MB\n\nTrying to reuse WebGL maps by toggling on and off different layers of data for the user at opportune times. This is an improvement, but data for all those layers piles up, and your toolchain wants to embed this in your page as reams of base64 encoded text. Page file sizes are completely blowing out.\n\n\n\nUse Cases\n\nSimplification of geometry is not desirable, e.g.Â because of alignment issues\n\ne.g.Â The zoomed-in road network has to align with the road network on the basemap, so that viewers can see features that lie along sections of road.\n\nSimplification of geometry doesnâ€™t really help, you still have too many features\nCumulatively your datasets are too large to handle.\n\nVector TilesÂ  - contain arrays of annotated spatial coordinate data which is combined with a separately transmitted stylesheet to produce the tile image.\n\ni.e.Â The edges of the roads, the boundaries of buildings etc. Not an image, but the building blocks for one\nDifferent stylesheets can use the same vector data to produce radically different looking maps that either highlight or omit data with certain attributes\nMapbox Vector Tiles (MVT) - specification; the de-facto standard for vector tile files\n\nstored as a Google protocol buffer - a tightly packed binary format.\n\n\nMBTiles - by Mapbox; describe a method of storing an entire MVT tileset inside a single file.\n\nInternally .mbtiles files are SQLlite databases containing two tables: metadata and tiles.\n\ntiles table\n\nindexed by z,x,y\ncontains a tile_data column for the vector tile protocol buffers, which are compressed using gzip\n\n\nSQLite format and gzip compression help with efficient retrieval and transmission\n\nUsing vector tiles we can have unlimited reference layers. Each one contributes nothing to the report file size since it is only streamed on demand when required.\nWorkflow to convert data to .tbtiles\n\nIn R, read source data as an sf, and wrangle\n\nTippecanoe expects by epsg 4326 by default\n\nWrite data out to geojson\nOn the command line, convert geojson to .mbtiles using the tippecanoe command line utility.\n\nTippecanoe sources\n\nMapbox version - repo\n\nMcBain says, he uses this version and hasnâ€™t had any problems\nREADME has helpful cookbook section\n\nActively maintained community forked version - repo\nMay be a headache to get dependencies if using Windows\n\nAlternatively it can output a folder structure full of protocol buffer files.\n\nExample\ntippecanoe -zg \\\nÂ  Â  Â  Â  Â  -o abs_mesh_blocks.mbtiles \\\nÂ  Â  Â  Â  Â  --coalesce-densest-as-needed \\\nÂ  Â  Â  Â  Â  --extend-zooms-if-still-dropping \\\nÂ  Â  Â  Â  Â  mb_shapes.geojson\n\nMapping\n\nExample\nlibrary(mvtview)\nlibrary(rdeck)\n\n# Fire up the server\nserve_mvt(\"abs_mesh_blocks.mbtiles\", port = 8765)\n# Serving your tile data from http://0.0.0.0:8765/abs_mesh_blocks.json.\n# Run clean_mvt() to remove all server sessions.\n\nmesh_blocks &lt;- jsonlite::fromJSON(\"http://0.0.0.0:8765/abs_mesh_blocks.json\")\n\n# Map the data\nrdeck(\nÂ  Â  initial_bounds = structure(meshblocks$bounds, crs = 4326, class = \"bbox\") # set map limits using the tilejson\n) |&gt;\nÂ  add_mvt_layer(\nÂ  Â  data = rdeck::tile_json(\"http://0.0.0.0:8765/abs_mesh_blocks.json\"),\nÂ  Â  get_fill_color = scale_color_linear(\nÂ  Â  Â  random_attribute\nÂ  Â  ),\nÂ  Â  opacity = 0.6\nÂ  )\n\nSee McBain article for options on hosting .mbtiles files\nRegarding â€œabs_mesh_blocksâ€: {mvtview} provides a way to fetch the metadata table from .mbtiles as json by querying a json file with the same name as the .mbitles file.\nThe structure of â€˜tilejsonâ€™ is yet another specification created by Mapbox, and is supported in deck.gl (and therefore {rdeck}) to describe tile endpoints."
  },
  {
    "objectID": "qmd/geospatial-general.html#sec-geo-gen-gridsys",
    "href": "qmd/geospatial-general.html#sec-geo-gen-gridsys",
    "title": "General",
    "section": "Grid Systems",
    "text": "Grid Systems\n\nMisc\n\nExplainer: Why using hexbins to visualize Australian electoral map is better than a typical provincial map.\n\ntl;dr: Geographical size distorts what the value is trying to measure. The value is the party that wins the parliamentary seat\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe bar graph shows the values the map is trying to visualize geographically. The hexabins better represent the close race by removing the distorting element which is the geographical size of the provinces.\nEach voting district (hexabin) is voting for 1 representative and has the same number of voters, but districts can have vastly different areas depending on population density.\n\nKeep unit at constant size (like hexabins) but alter hex shape to keep state shape.\n\n\nA better U.S. house election results map?\nResults\n\nstate size depends on number of districts which depends on population and therefore correctly conveys voting results visually across the whole country\nDistricts get distorted but the states retain their shape and so distortion of the overall visualization is minimized\n\n\n\n\nUberâ€™s H3 grid system -\n\nMisc\n\nPackages: {h3r}, {h3-r}\ndocs\nAdd census data to H3 hexagons, calculate overlaps (article)\nFor large areas, you can reduce the number of hexagons by merging some hexagons into larger hexagons.\n\nReduces storage size\nIssue: leaves small gaps between hexagons\n\nmight not matter for your use case\n\nSolution: use Microsoftâ€™s Quadkeys approach (see article)\n\n\nEach hexagon has a series of smaller hexagons that sit (mostly) inside of another, which creates a hierarchy that can be used for consistent referencing and analysis, all the way down to lengths of 2 feet for the edges.\nâ€œHexagons were an important choice because people in a city are often in motion, and hexagons minimize the quantization error introduced when users move through a city. Hexagons also allow us to approximate radiuses easily.â€\nRe other shapes: â€œWe could use postal code areas, but such areas have unusual shapes and sizes which are not helpful for analysis, and are subject to change for reasons entirely unrelated to what we would use them for. Zones could also be drawn by Uber operations teams based on their knowledge of the city, but such zones require frequent updating as cities change and often define the edges of areas arbitrarilyâ€\nGrid systems can have comparable shapes and sizes across the cities that Uber operates in and are not subject to arbitrary changes. While grid systems do not align to streets and neighborhoods in cities, they can be used to efficiently represent neighborhoods by clustering grid cells. Clustering can be done using objective functions, producing shapes much more useful for analysis. Determining membership of a cluster is as efficient as a set lookup operation.\n16 Resolutions\n\n0 - 15 (0 being coarsest and 15 being finest)\nEach finer resolution has cells with one seventh the area of the coarser resolution. Hexagons cannot be perfectly subdivided into seven hexagons, so the finer cells are only approximately contained within a parent cell.\nThe identifiers for these child cells can be easily truncated to find their ancestor cell at a coarser resolution, enabling efficient indexing. Because the children cells are only approximately contained, the truncation process produces a fixed amount of shape distortion. This distortion is only present when performing truncation of a cell identifier; when indexing locations at a specific resolution, the cell boundaries are exact.\nWant a resolution granular enough to introduce variability and wide enough to capture the effects of an area\nExample of resolution 6 in Iowa"
  },
  {
    "objectID": "qmd/geospatial-general.html#sec-geo-gen-feats",
    "href": "qmd/geospatial-general.html#sec-geo-gen-feats",
    "title": "General",
    "section": "Features",
    "text": "Features\n\nCarto Spatial Features dataset ($) - https://carto.com/spatial-data-catalog/browser/?country=usa&category=derived&provider=carto\n\nResolution: Quadgrid level 15 (with cells of approximately 1x1km) and Quadgrid level 18 (with cells of approximately 100x100m).\n\nGuessing if the areas youâ€™re interested in have high population density, then maybe 100 x 100 m cells would be more useful\n\nFeatures\n\nTotal population\nPopulation by gender\nPopulation by age and gender (e.g.Â female_0_to_19)\nPOIs by category\n\nRetail Stores\nEducation\n\nNumber of education related POIs, incuding schools, universities, academies, etc.\n\nFinancial\n\nNumber of financial sector POIs, including ATMs and banks.\n\nFood, Drink\n\nNumber of sustenance related POIs, including restaurants, bars, cafes and pubs.\n\nHealthcare\n\nNumber of healthcare related POIs, including hospitals\n\nLeisure\n\nNumber of POIs related to leisure activities, such as theaters, stadiums and sport centers.\n\nTourism\n\nNumber of POIs related to tourism attractions\n\nTransportation\n\nNumber of transportation related POIs, including parking lots, car rentals, train stations and public transport stations.\n\n\n\n\nCarto Data Observatory ($) - https://carto.com/spatial-data-catalog/browser/dataset/mc_geographic\\_\\_4a11e98c/\n\nFeatures\n\nGeo id\nRegion id\nIndustry\nTotal Transactions Amount Index\nTransaction Count Index\nAccount Count Index\nAverage Ticket Size Index\nAverage Frequency of Transaction per Card Index\nAverage Spend Amount by Account Index"
  },
  {
    "objectID": "qmd/geospatial-general.html#sec-geo-gen-interx",
    "href": "qmd/geospatial-general.html#sec-geo-gen-interx",
    "title": "General",
    "section": "Interactions",
    "text": "Interactions\n\nSimilar to interpolation but keeps the original spatial units as interpretive framework. Hence, the map reader can still rely on a known territorial division to develop its analyses\n\nThey produce understandable maps by smoothing complex spatial patterns\nThey enrich variables with contextual spatial information.\n\nMisc\n\nResources\n\nGetting Started with Potential - nice little mathematical summary, some background\n\nPackages\n\n{potential}: spatial interaction modeling via Stewart Potentials. Also capable of interpolation\n\n\nThere are two main ways of modeling spatial interactions: the first one focuses on links between places (flows), the second one focuses on places and their influence at a distance (potentials).\nComparisons ({potential}article)\n\nGDP per capita (cloropleth)\n\n\nTypical cloropleth at the municipality level\nValues have been binned\n\nPotential GDP per Capita (interaction)\n\n\nStewart Potentials have smoothed the values\nMunicipality boundaries still intact, so you could perform an analysis based on these GDP regions\n\nSmoothed GDP per Capita (interpolation)\n\n\nSimilar results as the interaction model except there are no boundaries"
  },
  {
    "objectID": "qmd/geospatial-general.html#sec-geo-gen-interp",
    "href": "qmd/geospatial-general.html#sec-geo-gen-interp",
    "title": "General",
    "section": "Interpolation",
    "text": "Interpolation\n\nThe process of using points with known values to estimate values at other points. In GIS applications, spatial interpolation is typically applied to a raster with estimates made for all cells. Spatial interpolation is therefore a means of creating surface data from sample points.\nKriging\n\n\nMisc\n\n{gstat}\n\nUses a correlated Gaussian process to guess at values between data points\n\n\nUncorrelated - white noise\nCorrelated - smooth\n\nThe closer in distance two points are to each other, the more likely they are to have a similar value (i.e.Â geospatially correlated)\nExample: temperature\n\n\nFewer known points means greater unceretainty\n Inputs:\n\nThe measured values at the sampling points,\nThe geometric coordinates of the sampling points,\nThe geometric coordinates of the target points to interpolate,\nThe â€œcalibratedâ€ probabilistic model, with the spatial correlation obtained by data outputs:\nThe estimated values at the target points,\nThe estimated uncertainty (variance) at the target points."
  },
  {
    "objectID": "qmd/feature-selection.html#why",
    "href": "qmd/feature-selection.html#why",
    "title": "30Â  Feature Selection",
    "section": "30.1 Why?",
    "text": "30.1 Why?\n\nReduces chances of overfitting\nMulticollinearity among predictors blows up std.errors in regression\nLowers computer resources requirements and increase speed"
  },
  {
    "objectID": "qmd/feature-selection.html#basic",
    "href": "qmd/feature-selection.html#basic",
    "title": "30Â  Feature Selection",
    "section": "30.2 Basic",
    "text": "30.2 Basic\n\nHarrell: Use the full model unless p &gt; m/15 â†’ number_of_columns &gt; number_of_rows / 15\n\nIf p &gt; m/15, then use penalyzed regression (see below, Shrinkage Methods)\n\nVIF - eliminate low variance predictorsÂ \n\n{collinear} - Allows you to set thresholds for correlation between predictors and VIF. The algorithm removes predictors based on those thresholds. For selecting between highly correlated variables, you can manually list the variables in order of preference or select from a few GOF functions that will be used to choose between collinear variables. Categoricals are target encoded as a preprocessing step.\n\nRemove noisy features - compare correlation between a var in train set and same var in test set\nBest subset\n\n{lmSubsets}: for linear regression; computes best set of predictors by fitting all subsets; chooses based on metric (AIC, BIC, etc.)\n{leaps::regsubsets} performs best subset selection for regression models using RSS. summary(obj) gives best subset of variables for different sized subsets. nvmax arg can be used set the max size of the subsets.\nLikelihood Ratio Test (LR Test) - For a pair of nested models, the difference in âˆ’2ln L values has a Ï‡2 distribution, with degrees of freedom equal to the difference in number of parameters estimated in the models being compared.\n\n-2 * Log Likelihood is called the residual deviance of the model\nExample:\n\nÏ‡2 = (-2)*log(model1_likelihood) - (-2)*log(model2_likelihood) = 4239.49 â€“ 4234.02 = 5.47\n\n-2*log can probably be factored out\n\ndegrees of freedom = model1_dof - model2_dof = 12 â€“ 8 = 4\npval &gt; 0.05 therefore the likelihoods of these models are not signficantly different and the variable isnâ€™t worth adding.\n\n\n\nReduction methods (if multicollinearity is a problem) - PCA\nShrinkage methodsÂ (if multicollinearity is a problem) - lasso, ridge, elastic net,Â Least Angle Regression (LAR)"
  },
  {
    "objectID": "qmd/feature-selection.html#complex",
    "href": "qmd/feature-selection.html#complex",
    "title": "30Â  Feature Selection",
    "section": "30.3 Complex",
    "text": "30.3 Complex\n\n{projpred} (Vehtari): projection predictive variable selection for various regression models, and also allows for valid post-selection inference\n\nPapers and other articles in bkmks &gt;&gt; Features &gt;&gt; Selection &gt;&gt; projpred\nCurrently requires {rstanarm} or {brms} models\nFamilies: gaussian, binomial, poisson. Also categorical and ordinal\nTerms: linear main effects, interactions, multilevel, other additive terms\nComponents\n\nSearch: determines the solution path, i.e., the best submodel for each submodel size (number of predictor terms).\nEvaluation: determines the predictive performance of the submodels along the solution path\n\nAfter model selection, a matrix of projected posterior draws is produced using the reference model and the selected number of features. Then, typical posterior stats for variable effects can be calculated, visualized via {posterior}, {bayesplot}, etc.\n\nprojected posterior distribution - the distribution arising from the deterministic projection of the reference modelâ€™s posterior distribution onto the parameter space of the final submodel\n\nAlso see Horseshoe model\n\nRegularized Horseshoe prior\nGelman - link\nPaper\n\n\n{MLGL} - approach combines variables aggregation and selection in order to improve interpretability and performance\n\nGoal is to remove redundant variables from a high dim dataset.\nSteps\n\nFirst, hierarchical clustering procedure provides at each level a partition of the variables into groups.\nThen, the set of groups of variables from the different levels of the hierarchy is given as input to group-Lasso, with weights adapted to the structure of the hierarchy.\n\nAt this step, group-Lasso outputs sets of candidate groups of variables for each value of the regularization parameter\n\n\n\n{Rdimtools} - has many algorithms for feature selection\nEnsemble Ranking: apply multiple feature selection methods then create an ensemble ranking\n\npaper, article\n\nCompared 12 individual feature selection methods and the 6 ensemble methods described above in 8 datasets for a classification task\n\nBest performance is Ensemble Reciprocal Ranking\n\nwhere r(f) is the final rank of feature f\n\nj is the the index for the feature selection methods\n\nEquivalent to the harmonic mean rank\naka Inverse Rank Position\nLower is better (I think)\n\nBest performance by single method is SHAP\n\nBoruta - built around random forest algorithm,Â can identify variables involved in non-linear interactions, slow if dealing with thousands of variables\nMultivariable Fractional Polynomials\n\nfits polynomial transformations of features with exponents in a range of numbers including fractions\nincludes a pretty intensive statistical model testing procedure\n{mfp}, explainer\n\nbounceR pkg - ?\nnimble pkg - Reversible Jump MCMC (RJMCMC) is a general framework for MCMC simulation in which the dimension of the parameter space (i.e., the number of parameters) can vary between iterations of the Markov chain. (article)\nRandom forest with shallow trees - see overview kdnuggets 7 methods article and notebook, pg 153 for proper var importance flavor\n\nFuzzy Forests: Extending Random Forest Feature Selection for Correlated, High-Dimensional Data\nGain Penalyzed Forests: Uses gain penalization to regularize the random forest algorithm\n\nThink this also works for numeric target variables\nNotes from\n\nFeature Selection via Gain Penalization in Random Forests\n\nIncludes coded example\n\nPaper\n\nWhen determining the next child node to be added to a decision tree, the gain (or the error reduction) of each feature is multiplied by a penalization parameter\n\nU is the set of indices of the features previously used in the tree\nXi is the candidate feature\nt is the candidate splitting point and Î»âˆˆ(0,1]\nSo at each split point, variables that have NOT been chosen at prior split points receive a penalty\n\nPenalty for each variable (the Î» shown in the Gain equation above)\n\nÎ»i âˆˆ [0,1)\nÎ»0 âˆˆ [0,1) is interpreted as the baseline regularization\nÎ³ âˆˆ [0,1) is the mixture parameter\ng(xi) is a function of the ith feature\n\nshould represent relevant information about the feature, based on some characteristic of interest (correlation to the target, for example)\nintroduces â€œprior knowledgeâ€ regarding the importance of each feature into the model\nthe data will tell us how strong our assumptions about the penalization are, since even if we try to penalize a truly important feature, its gain will be high enough to overcome the penalization and the feature will get selected by the algorithm\nExamples\n\nThe Mutual Information between each feature and the target variable y\n\nnormalized to be between 0 and 1\n\nThe variable importance values obtained from a previously run standard Random Forest, which is what I call a Boosted g(xi)\n\nnormalized to be between 0 and 1\n\nother options, see the paper\n\n\n\nSteps\n\nWe run a bunch of penalized random forests models with different hyperparameters and record their accuracies and final set of features\n\nÎ³, Î»0 and mtry are hyperparameters that should be tuned\n\nFor each training dataset, select the top-n (e.g.Â n = 3) fitted models in terms of the accuracies, and run a â€œnewâ€ random forest for each of the feature sets used by them. This is done using all of the training sets so we can evaluate how these features perform in slightly different scenarios\nFinally, get the top-m set of models (here m = 30) from these new ones, check which features were the most used between them and run a final random forest model with this feature set.\n\ne.g.Â select only the 15 most used features from the top 30 models, but both numbers can be changed depending on the situation\n\n\n\ndistance correlation algorithm (also see Regression, Regularized &gt;&gt; Misc\n\nâ€œthe distance correlation algorithm for variable selection (DC.VS) of Febrero-Bande et al.Â (2019). This makes use of the correlation distance (SzÃ©kely et al., 2007; Szekely & Rizzo, 2017) to implement an iterative procedure (forward) deciding in each step which covariate enters the regression model.â€\nStarting from the null model, the distance correlation function, dcor.xy,Â  in {fda.usc} is used to choose the next covariate\n\nguessing you want large distances between an already chosen variable and the next variable\nMaybe for the first explanatory variable you want a short distance between the it and the outcome variable\ndunno what the stopping criteria is\n\nalgorithm discussed in this paper, Variable selection in Functional Additive Regression Models"
  },
  {
    "objectID": "qmd/feature-selection.html#multidimensional-feature-selection-mfds",
    "href": "qmd/feature-selection.html#multidimensional-feature-selection-mfds",
    "title": "30Â  Feature Selection",
    "section": "30.4 Multidimensional Feature Selection (MFDS)",
    "text": "30.4 Multidimensional Feature Selection (MFDS)\n\nCompares tuples of variables using entropy/information gain (see below for details on the algorithm)\nA filter method which means it identifies informative variables before modelling is performed.\n\nIn general, filter methods are usually simplistic and therefore fast, but simplicity can sometimes lead to errors\n\nAs of 10-13-2019, implementation only for use with binary outcomes\n\nFor multi-categorical outcomes, vignette suggests either performing the analyses with all pairs of outcome categories, then any variable deemed relevant for one analysis is considered relevant. Instead of doing all-pairs, you can dichotomize variable into pairs of category-other which would be less expensive\nContinuous outcomes have to be discretizedÂ \n\nthe algorithm explores all k-tuples of variables, Â , for k-dimensional analysis., whereÂ   Â is one of the explanatory variables\n\n*** max k is 5\n\nFor larger values than 5, it becomes computationally expensive and detecting significant differences in conditional entropy becomes less likely.\n\nFor example, 2-dimensional analysis would explore the set of all possible pairs (2-tuples) of variables.\n\nFor each variable,  , check whether adding it to the set with another variable,Â   ,Â adds information to the system. If there exists such a  , then we declareÂ   as 2-weakly relevant.\n\n\nSteps\n\nDiscretize all variables\n\nChoose the number of classes, c\n\nAll variables are coerced into having the same number of classes (even binary? So if you have one binary, then all variables are coerced into binaries?)\nUnless thereâ€™s domain knowledge, multiple cs should be tried.\n\nRandomly sample (c-1) integers from a uniform distribution on the interval, (2, N) where N is the number of observations.\n\nThe integers will be indexes where the variable is split into c classes\nAt first glance it looks like the interval starts at 2 so that each class has minimum of 2 or 3 values (depending if the split happens before or after the integer), but unless thereâ€™s some kind of check, two consecutive integers, like 19,20, could still be sampled and then you have a problem. With a large data set, it seems unlikely to happen, but itâ€™d still be possible. So I donâ€™t know why it starts at 2. The vignette isnâ€™t clear about this IMO.\n\nSort the variable\n\nI guess smallest value to largest? Probably doesnâ€™t matter as long as itâ€™s consistent for all variables)\n\nSplit the sorted variable at the indexes given by the randomly sampled integers\nrepeat for each variable\n\noutcome variable might have to be done manually by the user prior to starting analysis/\n\n\nCompute conditional entropy for Y given the k-dimension variable set that includes a specific variable and the conditional entropy for the (k-1) set that excludes that specific variable.\n\nConditional Entropy forÂ   Â  is\n\n\n\nd is for the outcome variable category which much be binary\ni_1 is the ith category of the 1st variable\ni_k is the ith category of the kth variable\nThis sequence of summations amounts to taking every permutation of outcome category and every category of every explanatory variable and summing them together.\nSee the decision tree section in the â€œAlgorithm descriptionsâ€ note for further discussion\n\nThe conditional entropy for the (k-1) set of explanatory variable is calculated similarily\n\nIf k is less than total number of variables, then this process could include many permutations\n\nCompute the mutual information gain between these two entropies. Then out of all the mutual information values from all the different permutations of the k-tuple, choose the maximum value\n\nmutual information is the difference between two conditional entropies. Very similar to the information gain used in trees\n\n\nm is mth permutation of the k-tuple\nN is the number of observations. The reason for it being there is that multiplying the max-IG by N makes it a distribution parameter that can be hypothesis tested.\n\n\nHypothesis test this difference (IG_max) to see if itâ€™s significant\n\nIf the difference is significant, then that variable is â€œk-weakly relevantâ€\nTest statistic for a specific Î±-level is from an empirically calculated distribution\nÂ Adjust p-values to control FWER or FDR\n\nIf performing a more complicated model selection process, take the top n variables whose IG_max was significant and repeat steps\n\nGuessing this is backwards elimination\nThis is a method thatâ€™s built for dealing with thousands of variables, so this taking top-n-then-repeat seems to be a way of controlling the amount of time needed to complete the analysis."
  },
  {
    "objectID": "qmd/networks-knowledge-graphs.html#sec-net-kg-misc",
    "href": "qmd/networks-knowledge-graphs.html#sec-net-kg-misc",
    "title": "Knowledge Graphs",
    "section": "Misc",
    "text": "Misc\n\nNotes from:\n\nWhat is a knowledge graph?\nHow to Convert Any Text Into a Graph of Concepts\n\nUse Cases\n\nCalculate the centralities for any node, to understand how important a concept (node) is to the body of work\nAnalyze connected and disconnected sets of concepts, or calculate communities of concepts for a deep understanding of the subject matter.\nUsed to implement Graph Retrieval Augmented Generation (GRAG or GAG). Can give much better results than RAG when querying an LLM about documents.\n\nRetrieving the context that is the most relevant for the query with a simple semantic similarity search is not always effective. Especially, when the query does not provide enough context about its true intent, or when the context is fragments across a large corpus of text.\n\nRetail: Knowledge graphs have been for up-sell and cross-sell strategies, recommending products based on individual purchase behavior and popular purchase trends across demographic groups.\nEntertainment: Knowledge graphs are also leveraged for artificial intelligence (AI) based recommendation engines for content platforms, like Netflix, SEO, or social media. Based on click and other online engagement behaviors, these providers recommend new content for users to read or watch.\nFinance: This technology has also been used for know-your-customer (KYC) and anti-money laundering initiatives within the finance industry. They assist in financial crime prevention and investigation, allowing banking institutions to understand the flow of money across their clientele and identify noncompliant customers.\nHealthcare: Knowledge graphs are also benefiting the healthcare industry by organizing and categorizing relationships within medical research. This information assists providers by validating diagnoses and identifying treatment plans based on individual needs."
  },
  {
    "objectID": "qmd/networks-knowledge-graphs.html#sec-net-kg-terms",
    "href": "qmd/networks-knowledge-graphs.html#sec-net-kg-terms",
    "title": "Knowledge Graphs",
    "section": "Terms",
    "text": "Terms\nKnowledge Graph - Also known as a semantic network, represents a network of real-world entities â€” i.e.Â objects, events, situations, or concepts â€” and illustrates the relationship between them. Each node represents a concept and each edge is a relationship between a pair of such concepts. This information is usually stored in a graph database and visualized as a graph structure, prompting the term knowledge â€œgraph.â€"
  },
  {
    "objectID": "qmd/networks-knowledge-graphs.html#sec-net-kg-proc",
    "href": "qmd/networks-knowledge-graphs.html#sec-net-kg-proc",
    "title": "Knowledge Graphs",
    "section": "Process",
    "text": "Process\n\n\nCorpus Example:\nMary had a little lamb,\nYouâ€™ve heard this tale before;\nBut did you know she passed her plate,\nAnd had a little more!\nSteps\n\nSplit the corpus of text into chunks. Assign a chunk_id to each of these chunks.\nFor every text chunk, extract concepts and their semantic relationships using a LLM. This relation is assigned a weight of W1. There can be multiple relationships between the same pair of concepts. Every such relation is an edge between a pair of concepts.\nConsider that the concepts that occur in the same text chunk are also related by their contextual proximity. This relation is assigned a weight of W2. Note that the same pair of concepts may occur in multiple chunks.\nGroup similar pairs, sum their weights, and concatenate their relationships. So now we have only one edge between any distinct pair of concepts. The edge has a certain weight and a list of relations as its name.\nPopulate nodes (concepts) and edges (relations) in a graph data structure or a graph database.\nVisualize"
  },
  {
    "objectID": "qmd/networks-analysis.html#sec-net-anal-misc",
    "href": "qmd/networks-analysis.html#sec-net-anal-misc",
    "title": "Analysis",
    "section": "Misc",
    "text": "Misc\n\nPackages\n\n{statnet} - statnet is a collection of software packages for statistical network analysis that are designed to work together, with a common data structure and API, to provide seamless access to a broad range of network analytic and graphical methodology.\n\nList of individual package tutorials\nModels fit with MCMC, so can be slow.\n\n\nResources\n\nNetwork Analysis: Integrating Social Network Theory, Method, and Application with R\n\nAnalysis Questions\n\nAt a given moment in time:\n\nWho is connected to whom? Who is not connected?\nWhere, and who are the hubs?\nWhere and about what are the clusters? Are there silos?\n\nChanges over time:\n\nAre new connection forming?\nAre new patterns of connectivity forming?\nHow was our network before and after the introduction of an activity?\n\n\nIssues with Statistics\n\nThey are unable to leverage node features at all. All nodes with the same values for these summary statistics are indistinguishable from each other.\nThere is no learnable component in the production of these features. We cannot fit a custom objective or train them jointly with a downstream task."
  },
  {
    "objectID": "qmd/networks-analysis.html#sec-net-anal-terms",
    "href": "qmd/networks-analysis.html#sec-net-anal-terms",
    "title": "Analysis",
    "section": "Terms",
    "text": "Terms\n\nCentrality\n\nMeasures, abstractly, how important a given graph is to the connectivity of the overall graph\nHigher for nodes which lie in paths that efficiently connect many nodes to each other.\nTypes:\n\nBetweenness - Nodes with high betweenness centrality tend to be the â€œcrossroadsâ€ between nodes, i.e.Â when seeking to connect with another node that isnâ€™t immediately adjacent, it will typically involve a node with high betweeness centrality.\n\nThese nodes are important in keeping the network connected. Likely important intermediaries or bridges\nCalculated by counting the number of shortest paths that pass through a node and dividing by the total number of shortest paths in the network.\n\nCloseness - Nodes with high closeness centrality have quick access to many other nodes. These nodes have the shortest distance, in network terms, to all other nodes.\n\nThese nodes are important in spreading information to all other nodes as quickly and efficiently as possible.\nCloseness Centrality = 1 / (Sum of SPD from the node to all other nodes)\n\nWhere SPD is the shortest path distance. In practice, this would be done with a shortest path algorithm like Breadth-First SearchÂ orÂ Dijkstraâ€™s algorithm.\n\n\n\n\nClusters\n\nCluster Clique - a cluster that has at least one node thatâ€™s connected to another node outside of the cluster\nCluster Silo - a cluster that has no node connected to any other node outside of the cluster\n\nClustering coefficient\n\nMeasures the density of a nodeâ€™s local portion of the graph.\nNodes who have neighbors that are all connected to each other will have a higher clustering coefficient\n\nDegeneracy - A network model is degenerate when the space that an MCMC sampler can explore is so constrained that the only way to get the observed g(y) is essentially to flicker between full and empty graphs in the right proportion.\n\nA good indication that you have a degenerate model is that you have NA values for standard errors on your model parameter estimates. You canâ€™t calculate a variance â€“ and, therefore, a standard error â€“ if you simply flicker between full and empty graphs.\n\nDegree aka Degree Centrality - Total edges a given node has.\nDensity - the number of edges in the observed network divided by the number of possible edges\nEdges (aka Dyads)- connection between two nodes. Depending on the type of connection, the edge can have a direction.\nEdge Bundling - visually bundle together similar edges to reduce the visual clutter within a graph\nMultiplexity - The number of connections between two nodes\n\nCould be represented by the thickness, darkness, etc. of an edge between 2 nodes\nNodes that have high multiplicity with each other typically form clusters\n\nRandom Graph - A network with n nodes where the edges between nodes occur randomly with probability P (each potential edge is one Bernoulli trial). Network density is typically used for P.\nTransitivity of a relation means that when there is a tie from i to j, and also from j to h, then there is also a tie from i to h: friends of my friends are my friends\n\n\nTransitivity Index (aka Clustering Index) = # Transitive Triads / # Potentially Transitive Triads\n\nHas a range between 0 and 1 where 1 is a transitive graph.\nFor random graphs, the expected value of the transitivity index is close to the density of the graph."
  },
  {
    "objectID": "qmd/networks-analysis.html#sec-net-anal-layalg",
    "href": "qmd/networks-analysis.html#sec-net-anal-layalg",
    "title": "Analysis",
    "section": "Layouts",
    "text": "Layouts\n\nSpring\n\nFruchterman-Reingold force-directed algorithm\n\narranges the nodes so the edges have similar length and minimum crossing edges\n\n\nRandom - nodes positioned uniformly at random in the unit square\nCircular - nodes on a circle\nBipartite - nodes in two straight lines\nSpectral - nodes positioned using the eigenvectors of the graph Laplacian"
  },
  {
    "objectID": "qmd/networks-analysis.html#sec-net-anal-ergm",
    "href": "qmd/networks-analysis.html#sec-net-anal-ergm",
    "title": "Analysis",
    "section": "Exponential Random Graph Models (ERGM)",
    "text": "Exponential Random Graph Models (ERGM)\n\nAnalogous to logistic regression: ERGMs predict the probability that a pair of nodes in a network will have a tie between them.\nMisc\n\nNotes from Introduction to ERGMs\nPackages\n\n{statnet} - See Misc &gt;&gt; Packages\n{ergmito} - Simulation and estimation of Exponential Random Graph Models (ERGMs) for small networks (up to 5 vertices for directed graphs and 7 for undirected networks) using exact statistics\n\nIn the case of small networks, the calculation of the likelihood of ERGMs becomes computationally feasible, which allows us avoiding approximations and do exact calculation, ultimately obtaining MLEs directly.\n\n\nCan be used for directed, undirected, valued, unvalued, and bipartite networks.\nFitting a model with just edges is kind of like an intercept-only regression model.\nLess informative for dense networks.\n\nExamples: From Introduction to ERGMs\n\nDense\n\n\n\nNot Dense\n\n\nTriads aka triangles (i.e.Â transitive relationships) cause problems in ERGMs (more triads â†’ denser graph). They often lead to degeneracy.\n\nNAs for standard error estimates are a good indication of degeneracy.\nSince ERGMs donâ€™t handle triads well, it is NOT recommended using â€œtriangleâ€ as an adjustment variable in your model\n\n\n\nGoal: Describe the local selection forces that shape the global structure of a network\nExamples of networks examined using ERGM include knowledge networks, organizational networks, colleague networks, social media networks, and networks of scientific development.\nThe basic principle underlying the method is comparison of an observed network to Exponential Random Graphs.\n\nThe Null Hypothesis is a Erdos-Renyi graph\n\nA random graph where the degree of any node is binomially distributed (with n-1 Bernoulli trials per node, for a directed graph). (n is the number of nodes)\n\n\nEquation:\n\\[\n\\text{logit}(Y_{ij} = 1 \\; | \\; y_{ij}^c) = \\theta^T \\delta (y_{ij})\n\\]\n\n\\(\\theta\\) is a vector of coefficients\n\\(y_{ij}\\) denotes ijth dyad in graph \\(y\\)\n\nIf \\(y_{ij} = 1\\), then i and j are connected by an edge.\nIf \\(y_{ij} = 0\\), then i and j are NOT connected by an edge.\n\\(y_{ij}^c\\) is the complement (i.e.Â all other pairs of vertices in \\(y\\) other than (i, j)).\n\n\\(\\delta(y_{ij})\\) is the change statistic. A measure of how the graph statistic \\(g(y)\\) changes if the ijth vertex is toggled on or off.\n\n\\(\\delta(y_{ij}) = g(y_{ij}^+) - g(y_{ij}^-)\\)\n\\(y_{ij}^+\\) is the same network as \\(y\\) except that \\(y_{ij} = 1\\).\n\\(y_{ij}^-\\) is the same network as \\(y\\) except that \\(y_{ij} = 0\\).\n\n\nExample:{statnet} Edges model\nbottmodel.01 &lt;- ergm(bott[[4]] ~ edges)\n\n## Evaluating log-likelihood at the estimate.\nsummary(bottmodel.01)\n## \n## ==========================\n## Summary of model fit\n## ==========================\n## \n## Formula:   bott[[4]] ~ edges\n## \n## Iterations:  4 out of 20 \n## \n## Monte Carlo MLE Results:\n##       Estimate Std. Error MCMC %  p-value    \n## edges  -0.7621     0.2047      0 0.000313 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n##      Null Deviance: 152.5  on 110  degrees of freedom\n##  Residual Deviance: 137.6  on 109  degrees of freedom\n##  \n## AIC: 139.6    BIC: 142.3    (Smaller is better.)\nExample:{statnet} Edges and Triads model\nsummary(bott[[4]]~edges+triangle)\n##    edges triangle \n##       35       40\n\nbottmodel.02 &lt;- ergm(bott[[4]]~edges+triangle)\nsummary(bottmodel.02)\n## \n## ==========================\n## Summary of model fit\n## ==========================\n## \n## Formula:   bott[[4]] ~ edges + triangle\n## \n## Iterations:  2 out of 20 \n## \n## Monte Carlo MLE Results:\n##          Estimate Std. Error MCMC % p-value\n## edges    -0.55772    0.58201      0   0.340\n## triangle -0.05674    0.15330      0   0.712\n## \n##      Null Deviance: 152.5  on 110  degrees of freedom\n##  Residual Deviance: 137.5  on 108  degrees of freedom\n##  \n## AIC: 141.5    BIC: 146.9    (Smaller is better.)\nattributes of the individuals who make up our graph vertices may affect their propensity to form (or receive) ties\ntest this hypothesis, we can employ nodal covariates using the nodecov() term.\nbottmodel.03 &lt;- ergm(bott[[4]]~edges+nodecov('age.month'))\nsummary(bottmodel.03)\n## \n## ==========================\n## Summary of model fit\n## ==========================\n## \n## Formula:   bott[[4]] ~ edges + nodecov(\"age.month\")\n## \n## Iterations:  4 out of 20 \n## \n## Monte Carlo MLE Results:\n##                    Estimate Std. Error MCMC % p-value\n## edges             -1.526483   1.335799      0   0.256\n## nodecov.age.month  0.009501   0.016352      0   0.562\n## \n##      Null Deviance: 152.5  on 110  degrees of freedom\n##  Residual Deviance: 137.3  on 108  degrees of freedom\n##  \n## AIC: 141.3    BIC: 146.7    (Smaller is better.)\n\nresult suggests that in this dataset, the combined age of the children involved in a dyad as no effect on the probability of a tie between them, in either direction.\n\nimitation is a directed relationship. We might expect that older children are more likely to be role models to others. To test this hypothesis, we can use the directed variants of nodecov(): nodeocov() (effect of an attribute on out-degree) and nodeicov() (effect of an attribute on in-degree). We note that the nodecov() group of terms are for numeric attributes; nodefactor() terms are available for categorical attributes.\nbottmodel.03b &lt;- ergm(bott[[4]]~edges+nodeicov('age.month'))\n\nsummary(bottmodel.03b)\n## \n## ==========================\n## Summary of model fit\n## ==========================\n## \n## Formula:   bott[[4]] ~ edges + nodeicov(\"age.month\")\n## \n## Iterations:  4 out of 20 \n## \n## Monte Carlo MLE Results:\n##                    Estimate Std. Error MCMC % p-value   \n## edges              -2.89853    0.96939      0 0.00345 **\n## nodeicov.age.month  0.05225    0.02272      0 0.02340 * \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n##      Null Deviance: 152.5  on 110  degrees of freedom\n##  Residual Deviance: 132.1  on 108  degrees of freedom\n##  \n## AIC: 136.1    BIC: 141.5    (Smaller is better.)\n\nThe number of other children who imitated a child increase with the childâ€™s age"
  },
  {
    "objectID": "qmd/networks-analysis.html#sec-net-anal-ndembd",
    "href": "qmd/networks-analysis.html#sec-net-anal-ndembd",
    "title": "Analysis",
    "section": "Node Embeddings",
    "text": "Node Embeddings\n\nLearnable vectors of numbers that can be mapped to each node in the graph, allowing us to learn a unique representation for each node.\n\nUse as features in a downstream model.\n\nMethods\n\nDeepWalk and Node2vec papers\n\nUse the concept of a random walk, which involves beginning at a given node and randomly traversing edges, to produce pairs of nodes that are nearby each other.\nTrained by maximizing the cosine similarity between nodes that co-occurred in random walks.\n\nThis training objective leverages the homophily assumption, which states that nodes that are connected to each other tend to be similar to each other.\n\n\n\nIssues with Embeddings\n\nThey do not use node features at all. They assume that close-by nodes are similar without actually using the node features to confirm this assumption.\n\nThey rely on a fixed mapping from node to embedding (i.e.Â this is a transductive method).\n\nFor dynamic graphs, where new nodes and edges may be added, the algorithm must be re-ran from scratch, and all node embeddings need to be recalculated. In real-world problems, this is quite a big issue, as most online platforms have new users signing up every day, and new edges being created constantly."
  },
  {
    "objectID": "qmd/networks-analysis.html#sec-net-anal-gcn",
    "href": "qmd/networks-analysis.html#sec-net-anal-gcn",
    "title": "Analysis",
    "section": "Graph Convolutional Networks (GCN)",
    "text": "Graph Convolutional Networks (GCN)\n\nLearns representations of nodes by learning a function that aggregates a nodeâ€™s neighborhood (the set of nodes connected to the original node), using both graph structure and node features.\n\nThese representations are a function of a nodeâ€™s neighborhood and are not hardcoded per node (i.e.Â this is an inductive method), so changes in graph structure do not require re-training the model.\n\nFor unsupervised learning tasks, the method is similar to Node2vec/DeepWalk\nLayers\n\nA layer takes a weighted average of the node features in the original nodeâ€™s neighborhood, and the weights are learned by training the network\nAdding layers produces aggregations that use more of the graph.\n\nThe span of the subgraph used to produce a nodeâ€™s embedding is expanded by 1 hop."
  },
  {
    "objectID": "qmd/networks-knowledge-graphs.html#examples",
    "href": "qmd/networks-knowledge-graphs.html#examples",
    "title": "Knowledge Graphs",
    "section": "Examples",
    "text": "Examples"
  },
  {
    "objectID": "qmd/python-snippets.html#refactor-a-group-of-functions-into-a-class",
    "href": "qmd/python-snippets.html#refactor-a-group-of-functions-into-a-class",
    "title": "Snippets",
    "section": "Refactor a group of functions into a class",
    "text": "Refactor a group of functions into a class\n\nFrom pybites video\nGroup of functions before refactoring\napi_config = {\n    \"api_url\": \"https://example.com/api\",\n    \"api_key\": \"1234567890abcdef\",\n}\n\ndef setup_connection(api_url, api_key, user_id, session_token):\n    print(f\"Setting up connection to {api_url} with API key {api_key}, for user {user_id} with session {session_token}\")\n\ndef fetch_data(user_id, session_token):\n    setup_connection(api_config['api_url'], api_config['api_key'], user_id, session_token)\n    print(f\"Fetching data for user {user_id} with session {session_token}\")\n\ndef process_data(user_id, session_token, data):\n    setup_connection(api_config['api_url'], api_config['api_key'], user_id, session_token)\n    print(f\"Processing data {data} for user {user_id} with session {session_token}\")\n\ndef save_data(user_id, session_token, data):\n    setup_connection(api_config['api_url'], api_config['api_key'], user_id, session_token)\n    print(f\"Saving data {data} for user {user_id} with session {session_token}\")\n\nMany of the functions have the same arguments\nMany of the functions call the same function\n\nThe created class after refactoring\nclass ApiClient:\n\n    def __init__(self, config, user_id, session_token):\n        self.api_url = config['api_url']\n        self.api_key = config['api_key']\n        self.user_id = user_id\n        self.session_token = session_token\n        self._setup_connection()\n\n    def _setup_connection(self):\n        print(f\"Setting up connection to {self.api_url} with API key {self.api_key}, for user {self.user_id} with session {self.session_token}\")\n\n    def fetch_data(self):\n        print(f\"Fetching data for user {self.user_id} with session {self.session_token}\")\n\n    def process_data(self, data):\n        print(f\"Processing data {data} for user {self.user_id} with session {self.session_token}\")\n\n    def save_data(self, data):\n        print(f\"Saving data {data} for user {self.user_id} with session {self.session_token}\")\n\napi_config = {\n    \"api_url\": \"https://example.com/api\",\n    \"api_key\": \"1234567890abcdef\",\n}\nclient = ApiClient(api_config, 123, \"abc\")\nclient.fetch_data()\nclient.process_data(\"some data\")\nclient.save_data(\"some other data\")"
  },
  {
    "objectID": "qmd/python-snippets.html#sec-py-snip-mlset",
    "href": "qmd/python-snippets.html#sec-py-snip-mlset",
    "title": "Snippets",
    "section": "ML Set-Up",
    "text": "ML Set-Up\n# Suppress (annoying) warnings\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\nignore_warnings(category=ConvergenceWarning)\nif not sys.warnoptions:\nÂ  Â  warnings.simplefilter(\"ignore\")\nÂ  Â  os.environ[\"PYTHONWARNINGS\"] = ('ignore::UserWarning,ignore::RuntimeWarning')\n\n# Ensure logging\nlogging.basicConfig(\nÂ  Â  format='%(asctime)s:%(name)s:%(levelname)s - %(message)s',\nÂ  Â  level=logging.INFO,\nÂ  Â  handlers=[\nÂ  Â  Â  Â  logging.FileHandler(\"churn_benchmarking.log\"),\nÂ  Â  Â  Â  logging.StreamHandler()\nÂ  Â  ],\nÂ  Â  datefmt='%Y-%m-%d %H:%M:%S')\n\n# Determine number of cpus available\nn_cpus = mp.cpu_count()\nlogging.info(f\"{n_cpus} cpus available\")\n\n# Visualize pipeline when calling it\nset_config(display=\"diagram\")\n\n# Load prepared (pre-cleaned) files for benchmarking\nfile_paths = [f for f in glob.glob(\"00_data/*\") if f.endswith('_cleaned.csv')]\nfile_names = [re.search('[ \\w-]+?(?=\\_cleaned.)',f)[0] for f in file_paths]\ndfs = [pd.read_csv(df, low_memory=False) for df in file_paths]\ndata_sets = dict(zip(file_names, dfs))\nif not data_sets:\nÂ  Â  logging.error('No data sets have been loaded')\nÂ  Â  raise ValueError(\"No data sets have been loaded\")\nlogging.info(f\"{len(data_sets){style='color: #990000'}[}]{style='color: #990000'} data sets have been loaded.\")"
  },
  {
    "objectID": "qmd/python-snippets.html#sec-py-snip-dlunzip",
    "href": "qmd/python-snippets.html#sec-py-snip-dlunzip",
    "title": "Snippets",
    "section": "Download and Unzip helper",
    "text": "Download and Unzip helper\nimport urllib.request\nfrom zipfile import ZipFile\nimport os\ndef extract(url: str, dest: str, target: str = '') -&gt; None:\nÂ  Â  \"\"\"\nÂ  Â  Retrieve online data sources from flat or zipped CSV.\nÂ  Â  Places data in data/raw subdirectory (first creating, as needed).\nÂ  Â  For zip file, automatically unzip target file.Â \nÂ  Â  Args:\nÂ  Â  Â  Â  url (str): URL path to the source file to be downloadedÂ \nÂ  Â  Â  Â  dest (str): FileÂ  for the destination file to land\nÂ  Â  Â  Â  target (str, optional): Name of file to extract (in case of zipfile). Defaults to ''.\nÂ  Â  \"\"\"\nÂ  Â  # set-up expected directory structure, if not exists\nÂ  Â  if not os.path.exists('data'):\nÂ  Â  Â  Â  os.mkdir('data')\nÂ  Â  if not os.path.exists('data/raw'):\nÂ  Â  Â  Â  os.mkdir('data/raw')\n\nÂ  Â  # download file to desired location\nÂ  Â  dest_path = os.path.join('data', 'raw', dest)\nÂ  Â  urllib.request.urlretrieve(url, dest_path)\nÂ  Â  # unzip and clean-up (remove zip) if needed\nÂ  Â  if target != '':\nÂ  Â  Â  Â  with ZipFile(dest_path, 'r') as zip_obj:\nÂ  Â  Â  Â  Â  Â  zip_obj.extract(target, path = \"data//raw\")\nÂ  Â  Â  Â  os.remove(dest_path)\n\nfrom helpers.extract import extract\nurl_cps_suppl = 'https://www2.census.gov/programs-surveys/cps/datasets/2020/supp/nov20pub.csv'\nextract(url_cps_suppl, 'cps_suppl.csv')\n\nFrom Riederer (github, article)"
  },
  {
    "objectID": "qmd/python-snippets.html#sec-py-snip-easot",
    "href": "qmd/python-snippets.html#sec-py-snip-easot",
    "title": "Snippets",
    "section": "Extract a section of text",
    "text": "Extract a section of text\n\n\nDesired section of text is split between 2 â€œ~~~â€ strings\nProcess\n\nString is split into lines\nFind the start and stop indexes for the 2 â€œ~~~â€\nExtract lines between to the two indexes"
  },
  {
    "objectID": "qmd/python-snippets.html#sec-py-snip-shstup",
    "href": "qmd/python-snippets.html#sec-py-snip-shstup",
    "title": "Snippets",
    "section": "Shell Start-Up",
    "text": "Shell Start-Up\n\nA start-up script automatically imports libraries, definines functions, or sets variables, etc. when the python interpreter is started.\n\nEvery time you start a shell, the first thing you usually do is import a bunch of stuff, or frenetically press the top arrow key to recall something from your history. This is aggravated by the fact Python has very limited support for reloading changed modules in a shell, so restarting it is a common thing.\n\nSteps\n\nChoose a location for your script which can be anywhere\nCreate python script at the location and fill in whatever you want to happen when you start a python REPL\n\nName can be pythonstartup.py or whatever\n\nSet the PYTHONSTARTUP environment variable to the path of the file\n\nWindows:\n\nCMD\nset PYTHONSTARTUP=C:\\path\\to\\pythonstartup.py\nPowershell\nSet-Item -Name PYTHONSTARTUP -Value C:\\path\\to\\pythonstartup.py\n\nMac/Linux:\nexport PYTHONSTARTUP=/path/to/pythonstartup.py\n\n\nExample: From Happiness is a good PYTHONSTARTUP script\n\nimport atexit\n\n# First, a lot of imports. I don't use all of them all the time, \n# but I like to have them available.\n\nimport csv\nimport datetime as dt\nimport hashlib\nimport json\nimport math\nimport os\nimport random\nimport re\nimport shelve\nimport subprocess\nimport sys\nimport tempfile\nfrom collections import *\nfrom functools import partial\nfrom inspect import getmembers, ismethod, stack\nfrom io import open\nfrom itertools import *\nfrom math import *\nfrom pprint import pprint as pretty_print\nfrom types import FunctionType\nfrom uuid import uuid4\nfrom unittest.mock import patch, Mock, MagicMock\nfrom datetime import datetime, date, timedelta\n\n\nimport pip\n\n# Set ipython prompt to \"&gt;&gt;&gt; \" for easier copying\ntry:\n    from IPython import get_ipython\n\n    get_ipython().run_line_magic(\"doctest_mode\", \"\")\n    get_ipython().run_line_magic(\"load_ext\", \"ipython_autoimport\")\nexcept:\n    pass\n\n\n\ntry:\n    import asyncio \n    # for easier pasting\n    from typing import * \n    from dataclasses import dataclass, field\nexcept ImportError:\n    pass\n\n# Mostly to parse strings to dates\ntry:\n    import pendulum\nexcept ImportError:\n    pass\n\n# I think you know why\ntry:\n    import requests\nexcept ImportError:\n    pass\n\n# If I'm in a regular Python shell, at least activate tab completion\ntry:\n    import readline\n\n    readline.parse_and_bind(\"tab: complete\")\nexcept ImportError:\n    pass\n\ntry:\n    # if rich is installed, set the repr() to be pretty printted\n\n    from rich import pretty \n    pretty.install() \n\nexcept ImportError:\n    pass\n\n# I wish Python had a Path literal but I can get pretty close with this:\n# Tiis let me to p/\"path/to/file\" to get a Path object\nfrom pathlib import Path\ntry:\n    class PathLiteral:\n        def __truediv__(self, other):\n            try:\n                return Path(other.format(**stack()[1][0].f_globals))\n            except KeyError as e:\n                raise NameError(\"name {e} is not defined\".format(e=e))\n\n        def __call__(self, string):\n            return self / string\n\n    p = PathLiteral()\nexcept ImportError:\n    pass\n\n\n# Force jupyter to print any lone variable, not just the last one in a cell\ntry:\n    from IPython.core.interactiveshell import InteractiveShell\n\n    InteractiveShell.ast_node_interactivity = \"all\"\n\nexcept ImportError:\n    pass\n\n\n# Check if I'm in a venv\nVENV = os.environ.get(\"VIRTUAL_ENV\")\n\n#  Make sure I always have a temp folder ready to go\nTEMP_DIR = Path(tempfile.gettempdir()) / \"pythontemp\"\ntry:\n    os.makedirs(TEMP_DIR)\nexcept Exception as e:\n    pass\n\n# I'm lazy\ndef now():\n    return datetime.now()\n\n\ndef today():\n    return date.today()\n\n\n# Since restarting a shell is common, I like to have a way to persit\n# calculations between sessions. This is a simple way to do it.\n# I can do store.foo = 'bar' and get store.foo in the next session.\nclass Store(object):\n    def __init__(self, filename):\n\n        object.__setattr__(self, \"DICT\", shelve.DbfilenameShelf(filename))\n        # cleaning the dict on the way out\n        atexit.register(self._clean)\n\n    def __getattribute__(self, name):\n        if name not in (\"DICT\", \"_clean\"):\n            try:\n                return self.DICT[name]\n            except:\n                return None\n        return object.__getattribute__(self, name)\n\n    def __setattr__(self, name, value):\n        if name in (\"DICT\", \"_clean\"):\n            raise ValueError(\"'%s' is a reserved name for this store\" % name)\n        self.DICT[name] = value\n\n    def _clean(self):\n        self.DICT.sync()\n        self.DICT.close()\n\n\npython_version = \"py%s\" % sys.version_info.major\ntry:\n    store = Store(os.path.join(TEMP_DIR, \"store.%s.db\") % python_version)\nexcept:\n    # This could be solved using diskcache but I never took the time\n    # to do it.\n    print(\n        \"\\n/!\\ A session using this store already exist.\"\n    )\n\n\n# Shorcurt to pip install packages without leaving the shell\ndef pip_install(*packages):\n    \"\"\" Install packages directly in the shell \"\"\"\n    for name in packages:\n        cmd = [\"install\", name]\n        if not hasattr(sys, \"real_prefix\"):\n            raise ValueError(\"Not in a virtualenv\")\n        pip.main(cmd)\n\n\ndef is_public_attribute(obj, name, methods=()):\n    return not name.startswith(\"_\") and name not in methods and hasattr(obj, name)\n\n\n# if rich is not installed\ndef attributes(obj):\n    members = getmembers(type(obj))\n    methods = {name for name, val in members if callable(val)}\n    is_allowed = partial(is_public_attribute, methods=methods)\n    return {name: getattr(obj, name) for name in dir(obj) if is_allowed(obj, name)}\n\n\nSTDLIB_COLLECTIONS = (\n    str,\n    bytes,\n    int,\n    float,\n    complex,\n    memoryview,\n    dict,\n    tuple,\n    set,\n    bool,\n    bytearray,\n    frozenset,\n    slice,\n    deque,\n    defaultdict,\n    OrderedDict,\n    Counter,\n)\n\ntry:\n    # rich a great pretty printer, but if it's not there, \n    # I have a decent fallback\n    from rich.pretty import print as pprint\nexcept ImportError:\n\n    def pprint(obj):\n        if isinstance(obj, STDLIB_COLLECTIONS):\n            pretty_print(obj)\n        else:\n            try:\n                name = \"class \" + obj.__name__\n            except AttributeError:\n                name = obj.__class__.__name__ + \"()\"\n            class_name = obj.__class__.__name__\n            print(name + \":\")\n            attrs = attributes(obj)\n            if not attrs:\n                print(\"    &lt;No attributes&gt;\")\n            for name, val in attributes(obj).items():\n                print(\"   \", name, \"=\", val)\n\n\n# pp/obj is a shortcut to pprint(obj), it work as a postfix operator as \n# well, which in the shell is handy\nclass Printer(float):\n    def __call__(self, *args, **kwargs):\n        pprint(*args, **kwargs)\n\n    def __truediv__(self, other):\n        pprint(other)\n\n    def __rtruediv__(self, other):\n        pprint(other)\n\n    def __repr__(self):\n        return repr(pprint)\n\n\npp = Printer()\npp.__doc__ = pprint.__doc__\n\n# Same as the printer, but for turning something into a list with l/obj\nclass ToList(list):\n    def __truediv__(self, other):\n        return list(other)\n\n    def __rtruediv__(self, other):\n        return list(other)\n\n    def __call__(self, *args, **kwargs):\n        return list(*args, **kwargs)\n\n\nl = ToList()\n\n# Those alias means JSON is now valid Python syntax that you can copy/paste \nnull = None\ntrue = True\nfalse = False\n\nAlso has a class for creating fake data. See article for the code."
  },
  {
    "objectID": "qmd/python-snippets.html#sec-py-snip-refclfun",
    "href": "qmd/python-snippets.html#sec-py-snip-refclfun",
    "title": "Snippets",
    "section": "Refactoring",
    "text": "Refactoring\n\nConvert group of functions into a class\n\nFrom pybites video\nGroup of functions before refactoring\napi_config = {\n    \"api_url\": \"https://example.com/api\",\n    \"api_key\": \"1234567890abcdef\",\n}\n\ndef setup_connection(api_url, api_key, user_id, session_token):\n    print(f\"Setting up connection to {api_url} with API key {api_key}, for user {user_id} with session {session_token}\")\n\ndef fetch_data(user_id, session_token):\n    setup_connection(api_config['api_url'], api_config['api_key'], user_id, session_token)\n    print(f\"Fetching data for user {user_id} with session {session_token}\")\n\ndef process_data(user_id, session_token, data):\n    setup_connection(api_config['api_url'], api_config['api_key'], user_id, session_token)\n    print(f\"Processing data {data} for user {user_id} with session {session_token}\")\n\ndef save_data(user_id, session_token, data):\n    setup_connection(api_config['api_url'], api_config['api_key'], user_id, session_token)\n    print(f\"Saving data {data} for user {user_id} with session {session_token}\")\n\nMany of the functions have the same arguments\nMany of the functions call the same function\n\nThe created class after refactoring\nclass ApiClient:\n\n    def __init__(self, config, user_id, session_token):\n        self.api_url = config['api_url']\n        self.api_key = config['api_key']\n        self.user_id = user_id\n        self.session_token = session_token\n        self._setup_connection()\n\n    def _setup_connection(self):\n        print(f\"Setting up connection to {self.api_url} with API key {self.api_key}, for user {self.user_id} with session {self.session_token}\")\n\n    def fetch_data(self):\n        print(f\"Fetching data for user {self.user_id} with session {self.session_token}\")\n\n    def process_data(self, data):\n        print(f\"Processing data {data} for user {self.user_id} with session {self.session_token}\")\n\n    def save_data(self, data):\n        print(f\"Saving data {data} for user {self.user_id} with session {self.session_token}\")\n\napi_config = {\n    \"api_url\": \"https://example.com/api\",\n    \"api_key\": \"1234567890abcdef\",\n}\nclient = ApiClient(api_config, 123, \"abc\")\nclient.fetch_data()\nclient.process_data(\"some data\")\nclient.save_data(\"some other data\")\n\n\n\nGather a group of constants into an enum classs\n\nFrom pybites video\nGroup of constants all related to a common concept (e.g.Â user status)\nMakes code more organized and readable by grouping constants with common concepts into classes\nConstants before refactoring\nSTATUS_ACTIVE = 1\nSTATUS_INACTIVE = 2\nSTATUS_PENDING = 3\nSTATUS_CANCELLED = 4\nSTATUS_COMPLETED = 5\n\ndef update_user_status(user_id: int, status: int):\n    if status == STATUS_ACTIVE:\n        print(\"Activating user\")\n    elif status == STATUS_INACTIVE:\n        print(\"Deactivating user\")\n    # etc\n\nupdate_user_status(123, STATUS_ACTIVE)\n#&gt; Activating user\nConstants after refactoring into an enum\nfrom enum import Enum\n\nclass Status(Enum):\n    ACTIVE = 1\n    INACTIVE = 2\n    PENDING = 3\n    CANCELLED = 4\n    COMPLETED = 5\n\ndef update_user_status(user_id: int, status: Status):\n    if status is Status.ACTIVE:\n        print(\"Activating user\")\n    elif status is Status.INACTIVE:\n        print(\"Deactivating user\")\n    # etc\n\nupdate_user_status(123, Status.INACTIVE)\n#&gt; Deactivating user\nStatus.ACTIVE.name\n#&gt; 'ACTIVE'\nStatus.INACTIVE.value\n#&gt; 2\nStatus.__members__\n#&gt; mappingproxy({'ACTIVE': &lt;Status.ACTIVE: 1&gt;,\n#&gt;               'INACTIVE': &lt;Status.INACTIVE: 2&gt;, \n#&gt;               ...etc})\ntype(Status.ACTIVE)\n#&gt; &lt;enum 'Status'&gt;"
  },
  {
    "objectID": "qmd/econometrics-psa.html#misc",
    "href": "qmd/econometrics-psa.html#misc",
    "title": "Propensity Score Analysis",
    "section": "Misc",
    "text": "Misc\n\nNotes from:\n\nnyhackr meet-up Video\nVignette: PSAgraphics: An R Package to Support Propensity Score Analysis\nBook: Applied Propensity Score Analysis in R (unfinished)\n\nAny classification model should be able to produce propensity scores.\n\nExamples (code): Logistic Regression, Bayesian Logistic Regression, Probit BART, Conditional Inference Trees (partykit), Decision Trees, Random Forest.\n\nIf two people have similar propensity scores, then they will be similar in all the values of the covariates that were used to create that score\npsa::psa_simulation_shiny - App that simulates and visualizes balance of groups according effect size, sample size, estimand (e.g ATE, ATT, etc.), and PSA method\nProcess\n\n\nPhase 1\n\nSelect covariates\n\nTypically whatever variables are available if itâ€™s a secondhand dataset\nIf running and experiment, collect data that will demonstrate baseline equivalence (?), which can later be used in a PSA\n\nChoose PSA method\nCheck Balance (i.e.Â do observations in treatment and control look equivalent)\nRepeat if necessary until sufficient balance is achieved\n\nPhase 2\n\nEstimate Treatment Effects\n\nPhase 3\n\nSensitivity Analysis\n\nTest how sensitive the results are to an unobserved confounder\n\ni.e.Â How much variance would a variable need to have before it changes the results (sign change or just significance?)\n\n\nGo back to Phase I and choose a different method\n\nTests how sensitive the results are to the PSA method chosen\n\n\n\nMethods:\n\nStratification - Treatment and Comparison (aka Control) units are divided into strata (or subclasses) based upon a propensity score (e.g.Â quantiles), so that treated and comparison units are similar within each strata.\n\nLogistic Regression (treatment/control ~ covariates) can be used to estimate scores (i.e.Â predicted logits)\n\nlr.out &lt;- \n  glm(\n  treatment ~ x1 + x2 + x3,\n  data = dat,\n  family = binomial(link = logit)\n  )\ndat$ps &lt;- fitted(lr.out)\n\n5 stratum removes &gt; 90% of the bias in estimated treatment effect (Cochran 1968)\nbreaks5 &lt;- psa::get_strata_breaks(dat$ps)\ndat$strata5 &lt;-\n  cut(\n    x = dat$ps,\n    breaks = breaks5$breaks,\n    include.lowest = TRUE,\n    labels = breaks5$labels$strata\n  )\nWith Regression, each strata has very similar numbers of observations.\nTrees, strata are determined by the leaf nodes and each strata generally differs in the numbers of observations.\n\nIn a decision tree example, strata had some funky proportions in relation to the categorical and continuous predictors (see Paper)\n\nWithin each stratum, independent sample t-tests are conducted and then pooled to provide an overall estimate\n\nMatching - Each treatment unit is paired with a comparison unit base upon the pre-treatment covariates\n\nAlgorithms\n\nPropensity Score Matching\nLimited Exact Matching\nFull Matching\nNearest Neighbor Matching\nOptimal/Generic Matching\nMahalanobis distance matching (quantitative covariates only)\nMatching with and without replacement\nOne-to-One or One-to-Many Matching\n\nChoice of algorithm is trial and error â€” whichever one gives the best balance.\nDependent sample tests (e.g.Â repeated mearsures, t-test w/paired = TRUE) are conducted using the match pairs.\nExample:\ndata(lalonde, package = \"Matching\")\nrr &lt;- \n  Matching::Match(\n    Y = lalonde$re78, \n    Tr = lalonde$treat, \n    X = lalonde$ps, \n    M = 1,\n    estimand = 'ATT',\n    ties = FALSE)\nsummary(rr)\n#&gt; \n#&gt; Estimate...  2579.8 \n#&gt; SE.........  637.69 \n#&gt; T-stat.....  4.0456 \n#&gt; p.val......  5.2189e-05 \n#? \n#&gt; Original number of observations..............  445 \n#&gt; Original number of treated obs...............  185 \n#&gt; Matched number of observations...............  185 \n#&gt; Matched number of observations  (unweighted).  185\n\nX: Variables to match on or PS\nM: how many matches per unit that you want where 1 is a 1:1 match between treatment and control\n\nFor imbalanced data, youâ€™d want set to something higher depending on the level of imbalance between treatment and control groups.\nFor values greater than 1, units get used more than once which may not be accepted in some fields.\n\nties: For units that have multiple matches, TRUE means that the matched dataset will include the multiple control group matches with weights that reflect it (see docs for more details). FALSE means one of the multiple matches is chosen at random.\nestimand: ATE and ATC also available; Value printed in the results as Estimate.\n\nDependent Sample Assessment Plot\n\nmatches &lt;- data.frame(Treat = lalonde[rr$index.treated,'re78'],\n                      Control = lalonde[rr$index.control,'re78'])\ngranovagg::granovagg.ds(\n  matches[,c('Control','Treat')], \n  xlab = 'Treat', \n  ylab = 'Control')\n\n{granovagg} is a ggplot extension of {granova}. The package has been archived on CRAN and the github has broken links and no documentation.\nVery similar characteristics and interpretation as the Propensity Score Assessment Plot except instead of the circles which represented strata, each dot represents a matched pair between treatment and control. (Also a different color scheme)\nThe purple CI bar is difficult to spot since itâ€™s very short, but it stradles the green dashed ATE line and doesnâ€™t include the diagonal (i.e.Â zero).\n\nBalance Assessment\n\nlalonde_formu &lt;- treat ~ age + I(age^2) + educ + I(educ^2) + black +\n    hisp + married + nodegr + re74  + I(re74^2) + re75 + I(re75^2)\npsa::MatchBalance(df = lalonde, \n                  formu = lalonde_formu,\n                  formu.Y = update.formula(lalonde_formu, \n                                           re78 ~ .),\n                  M = 1, \n                  estimand = 'ATT', \n                  ties = FALSE) |&gt; \n  plot()\n\nCurrently this package doesnâ€™t have a website or CRAN docs. See Matching::Match above for details on some of these arguments or see the R script or ?psa::MatchBalance if you have it installed.\nCharts\n\nTop Left: For 2% of the matched pairs, they only matched on 12% of the covariates.\n\nThe percent annotation was cut-off on his chart but it very likely says 88%. Since the y-axis in â€œ% unmatchedâ€, the percent matched is 12%.\n\nBottom-Left: A breakdown of the unmatched covariates for each individual matched pair. Each column is a matched pair. Each colored segment in that column represents unmatched covariate. Each row is a covariate which is labelled in the bar graph at the bottom-right.\n\nThe first matched pair was unmatched on the covariates: nodegr, black, re74, and hisp\nSome of the segments have a light coloring instead of dark and Iâ€™m not sure what that means.\n\nBottom-Right: The aggregated unmatched percentage for each covariate across all matched pairs\nTop-Right: Standardized Estimate for each covariate. A variable thatâ€™s CI doesnâ€™t include 0 indicates an imbalance. (e.g.Â nodegr)\n\nEstimate is the mean difference from a paired t-test function thatâ€™s been standardized by dividing the estimate by the sd of the variable over the whole sample (i.e.Â treatment and control groups).\nUsing paired because the units have been matched.\nSo if the data has been perfectly matched for a variable, the mean difference between the treatment units and control units should be 0.\nA CI of this estimate that includes 0 means that the variable is balanced.\n\n\nYou can apply Partial Exact Matching to try bring any imbalanced variables into balance. This method allows you to be able to specify variables that want to guarantee that units to be matched upon. Doing this affects the matching upon other variables though. So, a variable that was matched before might not still be matched after using this method.\n\npsa::MatchBalance(df = lalonde, \n                  formu = lalonde_formu,\n                  formu.Y = update.formula(lalonde_formu, \n                                           re78 ~ .),\n                  M = 1, \n                  estimand = 'ATT', \n                  ties = FALSE,\n                  exact.covs = c('nodegr')) |&gt; \n  plot()\n\nYou can see nodegr has been fixed in the Top-Right error-bar chart. It also appears this has brought re75 into balance as well while not throwing the rest of the variables out of balance.\nThe x-axis range has narrowed which gives the appearance that the error bars have widened, but I donâ€™t think they have.\n\n\n\nWeighting - Each observation is weighted by the inverse of probability of being in that group\n\nPropensity Scores are used as weights for the regression model that youâ€™ll use to perform your analysis.\n\nThe specific weights will depend on the estimand (e.g ATE, ATT, etc.)\n\nAverage Treatment Effect (ATE): \\(\\text{ATE} = E(Y_1 - Y_0|X) = E(Y_1|X) - E(Y_0|X)\\)\n\nWeight: \\(w_{\\text{ATE}} = \\frac{Z_i}{\\pi_i} + \\frac{1-Z_i}{1-\\pi_i}\\)\n\\(Z_i = 1\\) says the unit is in the treatment group\n\\(\\pi_i\\) is the propensity score for unit i\nExample:\ndat &lt;- \n  dat |&gt; \n  mutate(\n    ate_weights = psa::calculate_ps_weights(treatment, \n                                            ps, \n                                            estimand = \"ATE\"))\n\n# via {psa}\npsa::treatment_effect(\n  treatment = dat$treatment,\n  outcome = dat$outcome,\n  weights = dat$ate_weights\n)\n# via lm\nlm(outcome ~ treatment,\n   data = dat,\n   weights = dat$ate_weights)\n\nATE Among the Treated (ATT): \\(\\text{ATT} = E(Y_1 - Y_0|X,C = 1) = E(Y_1|X,C = 1) - E(Y_)|X,C = 1)\\)\n\nWeight: \\(w_{\\text{ATT}} = \\frac{\\pi_i Z_i}{\\pi_i} + \\frac{\\pi_i(1-Z_i)}{1-\\pi_i}\\)\nExample:\ndat &lt;- \n  dat |&gt; \n  mutate(\n    att_weights = psa::calculate_ps_weights(treatment, \n                                            ps, \n                                            estimand = \"ATT\"))\n\n# via {psa}\npsa::treatment_effect(\n  treatment = dat$treatment,\n  outcome = dat$outcome,\n  weights = dat$att_weights\n)\n# via lm\nlm(outcome ~ treatment,\n   data = dat,\n   weights = dat$att_weights)\n\nATE Among the Control (ATC): \\(\\text{ATC} = E(Y_1- Y_0|X = 0) = E(Y_1|X = 0) - E(Y_0|X = 0)\\)\n\nWeight: \\(w_{\\text{ATC}} = \\frac{(1-\\pi_i)Z_i}{\\pi_i} + \\frac{(1-e_i)(1-Z_i)}{1-\\pi_i}\\)\nNot certain what \\(e_i\\) is, but it might be the residual for the unit from the propensity score model.\nExample: Same as above except with ATC weights\n\nATE Among the Evenly Matched (ATM): \\(\\text{ATM}_d = E(Y_1 - Y_0|M_d = 1)\\)\n\nWeight: \\(w_{\\text{ATM}} = \\frac{\\min\\{\\pi_i, 1-\\pi_i\\}}{Z_i \\pi_i(1-Z_i)(1-\\pi_i)}\\)\nExample: Same as above except with ADM weights\n\nShows mirrored axis histogram with PS on x-axis and count on y-axis; guessing treatment/control are top/bottom histograms?\nTreament Effect for Weighting\n\\[\n\\text{TE} = \\frac{\\sum Y_iZ_i}{\\sum Z_i w_i} - \\frac{Y_i(1-Z_i)w_i}{\\sum(1-Z_i)w_i}\n\\]\n\nThis formula is an alternative to using the weights in a model to estimate the effect.\n\n\n\nVisualization\n\nDistribution of propensity scores\n\nggplot(dat) +\n  geom_histogram(\n    data = dat[dat$treatment == 1, ],\n    aes(x = ps, y = after_stat(count)),\n    bins = 50,\n    fill = cols[2]\n  ) +\n  geom_histogram(\n    data = dat[dat$treatment == 0, ],\n    aes(x = ps, y = -after_stat(count)),\n    bins = 50,\n    fill = cols[3]\n  ) +\n  geom_hline(\n    yintercept = 0,\n    lwd = 0.5\n  ) + \n  scale_y_continuous(lable = abs)\n\nTreatment above and Control below\n\nCovariate Balance Plot\n\nPSAgraphics::cv.bal.psa(\n  dat[, 1:3],\n  data$treatment,\n  dat$ps,\n  strata = 5\n)\n\nAbsolute standardized Covariate effect sizes with (blue) and without (red) PS adjustment\nWant blue dots close to zero which says that after PS adjustment, the covariates have little predictive power in determining whether a unit is in the treatment group or control group. It means the PS are effectively adjusting for the selection bias of the treatment/control â€œassignmentâ€ in the observational data.\n\nBoxplot by Propensity Score Strata by Treatment\n\nPSAgraphics::box.psa(\n  continuous = dat$x2,\n  treatment = dat$treatment,\n  strata = dat$strata5\n)\n\nFor a continuous predictor, it allows you to visually examine the balance produced by the PS strata by comparing the distributions between treatment and control within each strata\nConnected dots are the means within each distribution and numbers below each box are its sample size.\n\nSetting trim = 0.5 will connect medians. (Range of trim is from 0 to 0.5)\n\nWithin strata, the more similar the distributions between treatment and control, the better the balance. Balance should be assessed for each predictor.\nTrends or patterns between strata can hint at potential variation and may help explain effects detected in the later performed analysis\n\nUnits in higher strata (i.e.Â higher PS) are more likely to be treated than those in lower strata, so looking at the values of predictor, you can say whether those with higher or lower values of the predictor are more likely to be treated.\n\nWith balance = TRUE (default is FALSE),\n\nHistogram\n\nPermuted data are randomly assigned to strata, absolute differences of means calculated within each strata, and the differences summed to produce the statistic.\nRepeated until thereâ€™s a distribution which is then visualized as a histogram\nThe sum of the mean differences of the original data is represented by a red dot.\nThe further left the red dot is from the mean (or median with trim = 0.5) of the permuation distribution, the better the balance\n\ni.e.Â the smaller the sum of mean differences of the original data as compared the permuted distribution of sums of mean differences, the better.\nThe rank shows where the original data statistic ranks in comparison to all of the summed differences in the permuted distribution.\n\nTotal_number_of_ranks = number_of_permutations + 1 (aka original data)\n\n\n\nBoxplot\n\nP-Values for KS-tests on treatment and control distributions are placed above the sample sizes of each strata. A lack of balance for a given strata would be indicated by a p-value &lt; 0.05.\n\n\n\nStacked Bar Chart by Propensity Score Strata by Treatment\n\nPSAgraphics::cat.psa(\n  categorical = dat$x3,\n  treatment = dat$treatment,\n  strata = dat$strat5\n)\n\nFor a categorical predictor, it allows you to visually examine the balance produced by the PS strata by comparing the category proportions between treatment and control within each strata\nWithin each strata, side-by-side segmented bars can be used to compare proportions of cases in each category\nSample sizes are above each bar\nif subsequent analysis indicates large differences in size or direction of effects for different strata, then comparing covariate distributions across strata may give an initial indication of potential causes.\nWith balance = TRUE (default is FALSE), itâ€™s very similar to box.psa.\n\nHistogram\n\nUnits within each category are permuted between strata\nDifferences in proportions is used instead of difference in means\nInterpretation is the same\n\nFisherâ€™s Exact Tests are performed and the p-values are shown at the bottom of the bars for each strata. A lack of balance for a given strata would be indicated by a p-value &lt; 0.05.\n\n\nPropensity Score Assessment Plot\n\nstrata5 &lt;- cut(lalonde$ps, \n               quantile(lalonde$ps, seq(0, 1, 1/5)), \n               include.lowest = TRUE, \n               labels = letters[1:5])\ncirc.psa(lalonde$re78, \n         lalonde$treat, \n         strata5)\n\nWhen differences in outcomes between treatments vary across strata, the investigator may want to learn how these differences are related to changes in covariate distributions within the strata.\nDisplays contributions of individual strata to the overall effect, weighing contributions of individual strata according to the relative sizes of the respective strata. The overall effect is plotted as a heavy dashed diagonal line that runs parallel to the identity diagonal.\nThe x-axis is the response values when treatment = 0, and the y-axis is the response values when treatment = 1. Labels depend on the values of the treatment variable.\nCircles\n\nEach circle represents a stratum, and the sizes of circles vary according to their respective sample sizes.\nThe center of each stratumâ€™s circle corresponds to outcome means for the respective control and treatment groups for that stratum.\n\nThe Crosses represent the difference in response means between treatment and control.\nThe blue dashed line is the mean of the differences between the strata which is the ATE, and the green line is its 95%CI.\n\nCIs that span the diagonal means no significant effect.\nCIs become increasingly unreliable for larger values of the trim arg.\n\nThe vertical and horizontal dashed red lines represent the (weighted) response means for the control and treatment groups respectively.\nInterpretation\n\nCircles/Crosses that fall on the diagonal means that those strata do not have a treatment effect.\nCircles on the lower side of diagonal black line show that the corresponding x-axis (e.g.Â treatment = 0) mean for that strata is larger than the y-axis (e.g.Â treatment = 1) mean for that stratum\nCircles close proximity to one another on the same side of the diagonal indicates concordance of outcome values in these strata\nCircles far outside of the cluster of circles should be investigated via the other predictor charts above to see what characteristics make-up that particular strata. If the sizes of each strata arenâ€™t relatively balanced, then a strata with few units may present outside the cluster. It may be useful try a larger amount of strata to see how that affects the outlier circle as it may help narrow the strata profile.\nThe distance of the crosses from the diagonal represents the size of estimated effect. A stratum with a much larger effect would influence the ATE (blue line) more strongly.\n\n\nLOESS Plot\n\npsa::loess_plot(\n  ps = psadf[psadf$Y &lt; 30000,]$ps, \n  outcome = psadf[psadf$Y &lt; 30000,]$Y, \n  treatment = as.logical(psadf[psadf$Y &lt; 30000,]$Tr))\n\nIncludes:\n\nTop: PS density plots for treatment and control. Looking to make sure thereâ€™s substantial overlap of the two densities.\nRight: Density plot of the outcome variable by treatment variable. Example shows significant skew and should be logged.\nCenter: LOESS plot\n\nParallel lines would indicate that there is a treatment effect, and it is homogeneous (i.e.Â same for all units) across all propensity scores which is not often the case.\n\nAn important feature of PSA in detecting heterogeneous, or uneven, treatments based upon different â€œprofiles.â€\n\nI think a profile would be kind of like a latent variable description of a cluster, but in this case, clusters are strata determined by the propensity scores. Profiles can be built by looking at the predictor vs propensity score strata charts above.\n\n\nCan be used to find strata boundarys. Potential boundaries are indicated where the treatment and control lines narrow or cross. Spaces between the lines are where you can expect a treatment effect.\n\ne.g.Â Setting int = c(0.375, 0.55, 0.875, 1) says that 0.357 and 1 are the minimum and maximum propensity scores and 0.55 and 0.875 are scores where the loess lines narrow or cross. This would represent 3 strata: (0.375, 0.55], (0.55, 0.875], and (0.875, 1].\nSeems like this would be highly dependent on the parameters of the LOESS parameters that are set unless you have a lot of data.\n\n\nStratification Plot\n\npsa::stratification_plot(ps = psadf$ps,\n                         treatment = psadf$Tr,\n                         outcome = psadf$Y,\n                         n_strata = 5)\n\nInstead of assuming a continuous vector of propensity scores as with the LOESS, it visualizes the mean differences in the response variable for each strata\nEssentially a visualization of an independent t-test."
  },
  {
    "objectID": "qmd/econometrics-psa.html#sensitivity-analysis",
    "href": "qmd/econometrics-psa.html#sensitivity-analysis",
    "title": "Propensity Score Analysis",
    "section": "Sensitivity Analysis",
    "text": "Sensitivity Analysis\n\nObserve how the treatment effect size changes in the presence of unobserved confounders.\n\nIf the observational analysis results only depends on the observed covariates which is unlikely, then the analysis is free of hidden bias (i.e.Â the p-value is valid if there are no unobserved confounders)\n\nHidden bias exists if two units with the same covariate values have different propensity scores.\nTechnique currently only available for the Matching PSA method\n\nNot sure why this is only for Matching since the formula below is just an odds ratio of propensity scores. Maybe heâ€™s talking about the package used in his example.\n\nSelection Bias Ratio\n\\[\n\\Gamma = \\frac{O_a}{O_b} = \\frac{\\frac{\\pi_a}{1-\\pi_a}}{\\frac{\\pi_b}{1-\\pi_b}}\n\\]\n\nThe ratio of the odds of the treated unit being in the treated group to the odds of the control unit being in the control group\nConverting Gamma to probability\nExamples\n\nIn smoking studies, the ratio is around 6 which means the results are very robust to unobserved confounders. Anything around 1 is considered sensitive to confounders.\nFor Social Science studies, itâ€™s typically between 1 and 2.\n\nConverting to the probability of 1 unit of the matched pair being treated\n\\[\n\\frac{1}{\\Gamma + 1} \\leq p_a, p_b \\leq \\frac{\\Gamma}{\\Gamma + 1}\n\\]\n\n\\(p_a\\) is the probability of a unit in group \\(a\\) being treated.\n\\(\\Gamma = 1 \\quad \\longrightarrow \\quad p_a =0.5 \\;\\&\\; p_b = 0.5\\) which means each unit is equally likely to get treated\n\\(0.5 \\leq \\Gamma \\leq 2 \\quad \\longrightarrow \\quad 0.33 \\leq p_a, p_b \\leq 0.66\\) which means no unit can be more than twice as likely as its match to get treated\n\\(0.33 \\leq \\Gamma \\leq 3 \\quad \\longrightarrow \\quad 0.25 \\leq p_a,p_b \\leq 0.75\\) which means no unit can be more than three times as likely as its match to get treated.\n\n\nWilcoxon Signed Rank Test\n\nUsed in the sensitivity test to determine which level of \\(\\Gamma\\) will produce a non-significant treatment effect.\nSteps\n\nDrop matched pairs that have the same outcome value\nCalculate the difference in outcome value between each matched pair\nRank the absolute differences from smallest (1) to largest (N)\nCalculate W\n\\[\nW = \\left|\\; \\sum_1^N \\operatorname{sgn} (x_{T,i} - x_{C,i}) \\cdot R_i \\; \\right|\n\\]\n\n\\(N\\) is the number of ranked pairs\n\\(R_i\\) is the Rank for pair \\(i\\)\n\\(x_{T,i}\\) and \\(x_{C,i}\\) are the outcome values for each unit of pair \\(i\\)\n\n\n\nExample:\nrbounds::psens(x = lalonde$re78[rr$index.treated], \n               y = lalonde$re78[rr$index.control],\n               Gamma = 2, \n               GammaInc = 0.1)\n\n#&gt; \n#&gt;  Rosenbaum Sensitivity Test for Wilcoxon Signed Rank P-Value \n#&gt;  \n#&gt; Unconfounded estimate ....  2e-04 \n#&gt; \n#&gt;  Gamma Lower bound Upper bound\n#&gt;    1.0       2e-04      0.0002\n#&gt;    1.1       0e+00      0.0016\n#&gt;    1.2       0e+00      0.0069\n#&gt;    1.3       0e+00      0.0215\n#&gt;    1.4       0e+00      0.0527\n\n{rbounds} performs Rosenbaum Bounds Sensitivity Tests for Matched and Unmatched Data. Also has functions for IV models and 2x2 contingency tables.\n\nThere is another function for continuous/ordinal outcomes called hlsens which uses the Hodges-Lehman point estimate. After reading the wiki, the output of this function seems to be a median difference or effect size for a given Gamma, but Iâ€™m not sure how thatâ€™s supposed to be interpreted in this context.\nFor binary outcomes, use binarysens. The x and y args say to use counts of discrepant pairs. This means counts where the treatment group and control groups have different outcomes. So in the 2x2 table, thatâ€™s the counts in cells (1,2) and (2,1)\n\nBryerâ€™s slides say to use McNemarâ€™s test, but after looking at the code, I donâ€™t see how Gamma can be incorporated into that test.\n\n\nArguments\n\nx: Treatment group outcome values ordered by matched pairs\ny: Control group outcome values ordered by matched pairs\nGamma: Largest value of Gamma that you want to test\nGammaInc: Increment value of Gamma.\n\nThe Lower bound and Upper bound are a sort of CI for the Wilcoxon testâ€™s p-values\nLook for the value of Gamma that would nullify a significant treatment effect in your analysis.\n\ne.g.Â \\(\\Gamma = 1.4\\) (since itâ€™s Upper bound value is &gt; 0.05) says that a confounder that adds 40% more variance in explaining the outcome would result in a Wilcoxon Test that doesnâ€™t reject the Null Hypothesis (i.e.Â no significant treatment effect).\n\n\nBootstrapping\n\nWith bootstrapping, you can check the sensitivity to model choice.\nSampling with replacement is done for the treatment and control observations is done separately\nFor each bootstrap sample balance statistics and treatment effects are estimated using each method (five by default). Overall treatment effect with confidence interval is estimated from the bootstrap samples\nExample\nlalonde_formu &lt;- treat ~ age + I(age^2) + educ + I(educ^2) + black +\n    hisp + married + nodegr + re74  + I(re74^2) + re75 + I(re75^2)\npsaboot &lt;- \n  PSAboot::PSAboot(Tr = lalonde$treat,\n                   Y = lalonde$re78,\n                   X = lalonde,\n                   formu = lalonde_formu)\nsummary(psaboot)\n#&gt; Stratification Results:\n#&gt;    Complete estimate = 1658\n#&gt;    Complete CI = [242, 3074]\n#&gt;    Bootstrap pooled estimate = 1476\n#&gt;    Bootstrap weighted pooled estimate = 1461\n#&gt;    Bootstrap pooled CI = [66.5, 2885]\n#&gt;    59% of bootstrap samples have confidence intervals that do not span zero.\n#&gt;       59% positive.\n#&gt;       0% negative.\n#&gt; ctree Results:\n#&gt;    Complete estimate = 1598\n#&gt;    Complete CI = [-6.62, 3203]\n#&gt;    Bootstrap pooled estimate = 1465\n#&gt;    Bootstrap weighted pooled estimate = 1472\n#&gt;    Bootstrap pooled CI = [172, 2758]\n#&gt;    38.1% of bootstrap samples have confidence intervals that do not span zero.\n#&gt;       38.1% positive.\n#&gt;       0% negative.\n#... etc for the other methods\n\nBy default, PSAboot uses logistic regression (i.e.Â boot.strata) {partykit::ctree} (See Algorithms, ML &gt;&gt; Trees &gt;&gt; Distributional Trees/Forests), {rpart} (decision trees), {Matching}, and {MatchIt}, but boot.weighting is also available by manually specifying the functions through the methods argument.\ncontrol.ratio and treat.ratio arguments allow for undersampling in the case of imbalanced data\nDefault parallel = TRUE, M = 100. Also has a seed argument. Watch your RAM for large values of M, because I donâ€™t think thereâ€™s any garbage collection in his code.\n\nBalance Plot\n\n\nMetric: Average Balance (mean difference/sd(strata) for strata or sd(N) for matching) across all covariates\nUnadjusted (red): No propensity score adjustment\nComplete (blue): Propensity scores used but no bootstrapping\nPooled (black): Propensity scores and bootstrapped\n\nSo the distribution is made-up â€œblueâ€ measurements calculated from bootstrap resamples. Then, the black line is the mean of that distribution\n\nThe red and blue estimates seem to the same as the ones in the Covariate Balance Plot (PSAgraphics::cv.bal.psa)\nInterpretation: The weighting method is the most balanced.\n\nBox Plot\n\nPSAboot::boxplot(psaboot)\n\nFrom the Bryer video, I assume this is a boxplot version of the balance plot with the same interpretation of each color (black, red, and blue) except that since the input isnâ€™t a balance object, these are unstandardized mean differences? (note x-axis).\n\nIâ€™m confused as to why Weighting is no longer the best method (i.e.Â closest to zero) though.\n\nThere currently isnâ€™t any sufficient documentation to make certain of estimate or the green lines, but I think the green lines look like averages of the whiskers.\n\nCorrelation Between Boot Distributions\n\nPSAboot::matrixplot(psaboot)\n\n{rpart} shows the least relationship with the other methods.\n\n\n\nMultinomial\n\nSteps\n\nEstimate three separate propensity score models for each pair of groups (i.e.Â Control-to-Treat1, Control-to-Treat2, Treat1-to-Treat2).\nDetermine the matching order. The default is to start with the largest of two treatments, then the other treatment, followed by the control.\nFor each unit in group 1, find all units from group 2 within a certain threshold (i.e.Â difference between PSs is within a specified caliper).\nFor each unit in group 2, find all units from group 3 within a certain threshold.\nCalculate the distance (difference) between each unit 3 found and the original unit 1. Eliminate candidates that exceed the caliper.\nCalculate a total distance (sum of the three distances) and retain the smallest unique M group 1 units (by default M=2)\n\nPS estimated from multiple logistic regression models. One for each combination of of categories of the treatment variable (e.g.Â treatment_1 ~ control, treatment_2 ~ control, treatment_1 ~ treatment_2)\nFinds matched triplets that minimize the total distance (i.e.Â sum of the standardized distance between propensity scores within the three models). within a caliper.\nProvides multiple methods for determining which matched triplets are retained:\n\nOptimal: which attempts to retain all treatment units.\nFull: which retains all matched triplets within the specified caliper (.25 by default as suggested by Rosenbaum).\nAnalog of the one-to-many for matched triplets. Specify how many times each treat1 and treat2 unit can be matched.\nUnique which allows each unit to be matched once, and only once.\n\nFunctions for conducting repeated measures ANOVA and Freidman Ranksum Tests are provided."
  },
  {
    "objectID": "qmd/diagnostics-bayes.html#misc",
    "href": "qmd/diagnostics-bayes.html#misc",
    "title": "14Â  Bayes",
    "section": "14.1 Misc",
    "text": "14.1 Misc\n\nPrior sensitivity analysis\n\n{priorsense}\n\nVideo, Thread"
  },
  {
    "objectID": "qmd/diagnostics-bayes.html#correlations-between-parameter-posteriors",
    "href": "qmd/diagnostics-bayes.html#correlations-between-parameter-posteriors",
    "title": "14Â  Bayes",
    "section": "14.2 Correlations between parameter posteriors",
    "text": "14.2 Correlations between parameter posteriors\n\nCorrelated parameters and their uncertainties will co-vary within the posterior distribution\n\ne.g.Â High intercepts will often mean high slopes\nCentering/standardization of predictors can remove correlation between parameters\n\nWithout independent parameters\n\nParameters canâ€™t be interpreted independently\nParameter effects on prediction arenâ€™t independent\n\nbrms::pairs(model_fit) (SR Ch 4)\n\nExample: SR Ch 8,9\n\npost &lt;- posterior_samples(mod_obj)\npost %&gt;%\nÂ  select(-lp__ ) %&gt;%\nÂ  ggally::ggpairs()\n\nIgnore first â€œb_â€; no idea why that got added\na_cid1 is the intercept for factor variable, cid = 1\nb_cid1 is the slope for the predictor variable, geological ruggedness, when cid = 1\nSlope and intercept conditional on cid = 1 has the highest correlation at 0.174"
  },
  {
    "objectID": "qmd/diagnostics-bayes.html#convergence",
    "href": "qmd/diagnostics-bayes.html#convergence",
    "title": "14Â  Bayes",
    "section": "14.3 Convergence",
    "text": "14.3 Convergence\n\n14.3.1 Metrics\n\nMisc\n\nNotes from\n\nRank-normalization, folding, and localization: An improved RË† for assessing convergence of MCMC\n\nLots of detailed convergence analysis examples\n\n\nbayestestR::diagnostic_posterior has â€œESSâ€, â€œRhatâ€, â€œMCSEâ€\n\nAccepts rstanarm, brms models\n\nValues potentially indicate multimodal distribution (Vehtari, Thread)\n\n\nâ€œChain stacking might help, but would need to know more about the posterior to be more confident on recommendationâ€\n\n\nRhat - Gelman-Rubin convergence diagnostic\n\nestimate of the convergence of Markov chains to the target distribution\n\nChecks if the start and end of each chain explores the same region\nChecks that independent chains explore the same region\n\nCan require long chains to work well\n** This diagnostic can fail for more complex models (i.e.Â bad chains even when value = 1) **\n\nNew metric called R* might be better (docs) but thereâ€™s arenâ€™t any guidelines on the values, so probably just useful for model comparison for now.\n\nRatio of variances\n\nAs total variance among all chains shrinks to the average variance within chains, R-hat approaches 1\nIf converges, Rhat = 1+\n\nGuideline\n\nIf value is above 1.00, it usually indicates that the chain has not yet converged, and probably you shouldnâ€™t trust the samples.\n\nSolution\n\nIf you draw more iterations, it could be fine, or it could never converge.\n\n\n\n\n14.3.1.1 Autocorrelation Metrics\n\nMarkov chains are typically autocorrelated, so that sequential samples are not entirely independent.\nEffects of Autocorrelation\n\nCan be an indicator of non-convergence\nIncreases uncertainty (standard errors)\nWhen chains have high autocorrelation, they can get stuck in regions of the parameter space making the sampling inefficient.\n\nI understand this to mean that less of the parameter space gets sampled\n\n\nSolutions\n\nIf you get warnings, taking more samples usually helps\nIncreasing max tree depth helps if max tree depth is continually being reached\n\nMCMCvis::MCMCdiag(fit, round = 2) produces diagnostics and shows sampler settings your model\n\nAccepts rstan, nimble, rjags, jagsUI, R2jags, rstanarm, and brms model objects\n\n\n\nEffective Sample Size (ESS)\n\nMeasures the amount by which autocorrelation in samples increases uncertainty (standard errors) relative to an independent sample.\n\nTells you how many samples the chain would have if there was 0 autocorrelation between samples in the chain\n\nMore autocorrelation means fewer effective number of samples.\n\nGuidelines for all ESS Metrics (tail or bulk)\n\nLarger is better\nBad: ESS &lt; 400 indicates convergence problemsÂ  (Vehtari)\nOkay: ESS â‰ˆ 800 corresponds to low relative efficiency of 1% (Vehtari)\nGood: ESS &gt; 1000 is sufficient for stable estimates (BÃ¼rkner, 2017)\nVery Good: ESS â‰¥ iteration amount\n\nGreater than means that something called anti-correlation is going on which is good\n\nExample: 2000 total samples with 1000 of those used for warm-up which is brms default. 4 chains x 1000 samples = 4000 post-warm-up samples. So for each parameter, the ESS should be around that or above\n\n\nn_eff in {rethinking} precis output\n\nSame as Bulk_ESS\n\nBulk_ESS - effective sample size around the bulk of the posterior (i.e.Â around the mean or median) (same as McElreathâ€™s n_eff)\n\nâ€œassesses how well the center of the distribution is resolvedâ€\n\ni.e.Â Measures how well HMC sampled the posterior around the bulk of the distribution in order to determine its shape.\n\nExample: Summary will give ESS stats\nas_draws_rvars(brms_fit) %&gt;%Â  Â \nÂ  Â  summarize_draws()\nvariable mean median Â  sd mad Â  q5 q95Â  rhat ess_bulk ess_tailÂ \nlp__ Â  Â  Â  -38.56Â  -38.20 1.30 1.02Â  -41.09Â  -37.21Â  Â  1Â  Â  1880Â  Â  2642Â \nalpha_c Â  Â  Â  Â  9.32 9.32 0.14 0.14 9.09 9.55Â  Â  1Â  Â  3364Â  Â  2436Â \nbeta Â  Â  Â  Â  0.02 0.02 0.01 0.01 0.01 0.03Â  Â  1Â  Â  3864Â  Â  2525Â \nsigma Â  Â  Â  Â  1.12 1.11 0.10 0.10 0.97 1.29Â  Â  1Â  Â  3014Â  Â  2776\nExample: Select variable and ESS Values for Quantiles\nas_draws_rvars(brms_fit) %&gt;%\nÂ  subset_draws(\"beta100\") %&gt;%\nÂ  summarize_draws(ess_mean, ~ess_quantile(.x, probs = c(0.05, 0.95)))\nvariableÂ  ess_meanÂ  ess_q5Â  ess_q95\nbeta100Â  Â  Â  3816Â  Â  2525Â  Â  3153\n\nThese are ESS values for\n\nThe summary estimate (aka point estimate) which is the mean of the posterior in this case\nAnd the CI values of that summary estimate\n\nSo it makes sense youâ€™d have lower numbers of effective samples in the tails of the posterior than in the bulk since itâ€™s going to get sampled less than the bulk\n\n\nTail_ESS - effective sample size in the tails of the posterior\n\nmeasures how well HMC sampled the posterior in the tails of the distribution in order to determine their shape.\n\n\nAutocorrelation plots for chains\n\npost &lt;- posterior_samples(mod_obj)\npost %&gt;%Â \nmcmc_acf(pars = vars(b_a_cid1:sigma),\nÂ  Â  Â  lags = 5) +\ntheme_pomological_fancy(base_family = \"Marck Script\")\n\nExample from Ch 8,9 Statistical Rethinking\nL-shaped autocorrelation plots like these are good.\n\nThose are the kinds of shapes youâ€™d expect when you have reasonably large effective samples."
  },
  {
    "objectID": "qmd/diagnostics-bayes.html#mixing",
    "href": "qmd/diagnostics-bayes.html#mixing",
    "title": "14Â  Bayes",
    "section": "14.4 Mixing",
    "text": "14.4 Mixing\n\nPosteriors and Trace Plots\nbrms::plot(mod_obj)\nExample: Ch 8,9 Statistical Rethinking\n\n\nSee above for parameter descriptions\nTrace plots with fat, lazy caterpillars like these are good\n\nTrank Plot (Trace Rank Plot)\n\npost &lt;- posterior_samples(b9.1b, add_chain = T)\npost %&gt;%Â \nÂ  bayesplot::mcmc_rank_overlay(pars = vars(b_a_cid1:sigma)) +\nÂ  Â  Â  scale_color_pomological() +\nÂ  Â  Â  ggtitle(\"My custom trank plots\") +\nÂ  Â  Â  coord_cartesian(ylim = c(25, NA)) +\nÂ  Â  Â  theme_pomological_fancy(base_family = \"Marck Script\") +\nÂ  Â  Â  theme(legend.position = c(.95, .2))"
  },
  {
    "objectID": "qmd/diagnostics-bayes.html#posterior-predictive-check",
    "href": "qmd/diagnostics-bayes.html#posterior-predictive-check",
    "title": "14Â  Bayes",
    "section": "14.5 Posterior Predictive Check",
    "text": "14.5 Posterior Predictive Check\n\nExample:\n\nBad Fit\n\nbrms::pp_check(model, nsamples = 100) + xlim(0, 20)\nGood Fit\n\n\nWith {bayesplot}\n\nbayesplot::ppc_rootogram(y = testdata$observedResponse,\nÂ  Â  Â  Â  Â  Â  Â  yrep = posterior_predict(model4, nsamples = 1000)) +\nÂ  xlim(0, 20)"
  },
  {
    "objectID": "qmd/diagnostics-bayes.html#gof-plots",
    "href": "qmd/diagnostics-bayes.html#gof-plots",
    "title": "14Â  Bayes",
    "section": "14.6 GOF Plots",
    "text": "14.6 GOF Plots\n\nMisc\n\nAlso see Statistical Rethinking, Chapter 5&gt;&gt; inferential plots\n\nPredictor Residual, Counterfactual Plots, and Posterior Predictive Plots\n\n\nData with regression line\n\n# scatter plot of observed pts with the regression line from the model\n# Defined the by the alpha and beta estimate\ndat %&gt;%\nÂ  ggplot(aes(x = weight, y = height)) +\nÂ  geom_abline(intercept = fixef(b4.3)[1],\nÂ  Â  Â  Â  Â  Â  Â  slopeÂ  Â  = fixef(b4.3)[2]) +\nÂ  geom_point(shape = 1, size = 2, color = \"royalblue\") +\nÂ  theme_bw() +\nÂ  theme(panel.grid = element_blank())\nPredicted vs Observed with PI and CI around regression line\n\n# x value (weight) range we want for the CI of the line\nweight_seq &lt;- tibble(weight = seq(from = 25, to = 70, by = 1))\n\n# predicted values (height) for each x value\n# 95% CIs generated by default\nmu_summary &lt;-Â  fitted(b4.3, newdata = weight_seq) %&gt;%\nÂ  Â  as_tibble() %&gt;%\nÂ  Â  # let's tack on the `weight` values from `weight_seq`\nÂ  Â  bind_cols(weight_seq)\n\n# 95% PIs generated by default\npred_height &lt;-Â  predict(b4.3,\nÂ  Â  Â  newdata = weight_seq) %&gt;%\nÂ  Â  as_tibble() %&gt;%\nÂ  Â  bind_cols(weight_seq)\n\n# includes regression line, CI, and PI\ndat %&gt;%Â  ggplot(aes(x = weight)) +\nÂ  # PIs\nÂ  geom_ribbon(data = pred_height, aes(ymin = Q2.5,\nÂ  Â  Â  Â  Â  Â  Â  ymax = Q97.5), fill = \"grey83\") +\nÂ  # CIs\nÂ  geom_smooth(data = mu_summary, aes(y = Estimate, ymin = Q2.5, ymax = Q97.5),\nÂ  Â  Â  Â  Â  Â  Â  stat = \"identity\", fill = \"grey70\", color = \"black\",\nÂ  Â  Â  Â  Â  Â  Â  alpha = 1, size = 1/2) +\nÂ  geom_point(aes(y = height), color = \"navyblue\", shape = 1,\nÂ  Â  Â  Â  Â  Â  size = 1.5, alpha = 2/3) +\nÂ  coord_cartesian(xlim = range(d2$weight),\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  ylim = range(d2$height)) +\nÂ  theme(text = element_text(family = \"Times\"),\nÂ  Â  Â  Â  panel.grid = element_blank())"
  },
  {
    "objectID": "qmd/diagnostics-bayes.html#residuals",
    "href": "qmd/diagnostics-bayes.html#residuals",
    "title": "14Â  Bayes",
    "section": "14.7 Residuals",
    "text": "14.7 Residuals\n\nDHARMa package\n\nvignettes:\n\nhttps://cran.r-project.org/web/packages/DHARMa/vignettes/DHARMaForBayesians.html\nhttps://cran.r-project.org/web/packages/DHARMa/vignettes/DHARMa.html\n\nExample:\n\nBad Fit\n\npacman::p_load(brms, DHARMa)\n\nmodel.check &lt;- createDHARMa(\nÂ  simulatedResponse = t(posterior_predict(model)),\nÂ  observedResponse = testdata$observedResponse,\nÂ  fittedPredictedResponse = apply(t(posterior_epred(model)), 1, mean),\nÂ  integerResponse = TRUE)\nplot(model.check)\n\nHierarchical dataset fit with a poisson model. Everything is bad in this fit.\n\nGood Fit\n\n\nHierarchical dataset fit with a hierarchical negative binomial model.\n\nExample (group variation check)\n\nÂ  Â  plot(model.check, form = testdata$group)\n\nThis is hierarchical dataset fit with a hierarchical poisson model. The within-group box-plots show that model has captured the group variance sufficiently as both tests have non-significant (n.s.) results.\n\nExample: Overdispersion\n\nBad fit\n\nÂ  Â  testDispersion(model.check)\n\nThis is hierarchical dataset fit with a hierarchical poisson model. In the previous chart (left panel), the overdispersion test failed. This histogram shows how much the model is off.\n\nGood Fit"
  },
  {
    "objectID": "qmd/diagnostics-bayes.html#predictive-accuracy",
    "href": "qmd/diagnostics-bayes.html#predictive-accuracy",
    "title": "14Â  Bayes",
    "section": "14.8 Predictive Accuracy",
    "text": "14.8 Predictive Accuracy\n\nMisc\n\nSee Statistical Rethinking, Chapter 7for details\nloo package website has some nice CV workflows\n\nAlso ensembling, time series\n\nDonâ€™t compare models with different numbers of observations (SR, Ch 11)\n\ne.g.Â 1/0 logistic regression model vs aggregated logistic regression model\n\nPSIS-LOO (IS-LOO, WAIC, etc) has difficulties if each observation has their own parameter(s) (aka â€œrandom effectsâ€) (Vehtari thread + post)\n\nModel Comparison\n\nMcElreath: To judge whether two models are â€œeasy to distinguishâ€ (i.e.Â kinda like whether their scores are statistically different), we look at the differences between the model with the best WAIC and the WAICs of the other models along with the standard error of the difference of the WAIC scores\n\nbrms::loo_compare(loo_obj, loo_obj)\n\nWhere a â€œloo_objâ€ is a brms::loo(fit) or brms::waic(fit) object\nCan also take a list of loo objects\nsimplify = FALSE gives a more detailed summary\n\nIf the difference in ELPD is much larger or several times the estimated standard error of the difference, then the top model is expected to have better predictive performance\n\n\nPareto-Smoothed Importance Sampling Cross-Validation (PSIS)\n\nWeights observations based on influence on the posterior\nUses highly influential observations to formulate a pareto distribution and sample from it\nbrms::loo - wrappers for loo::loo (docs)\nEstimates out-of-sample LOO-CV lppd\n\nloo pkg\n\nâ€œelpd_looâ€ - larger is better\nâ€œlooicâ€ - is just (-2 * elpd_loo) to convert it to the deviance scale, therefore smaller is better\nMay need to use add_criterion(brms_fit, \"loo\") in order to use the loo function\n\nRethinking pkg: smaller is better\n\nThe shape parameter of the distribution, k, is estimated. When k &gt; 0.5, then the distribution has infinite variance. PSIS weights perform well as long as k &lt; 0.7. Large k values can be used to identify influential observations (i.e.Â rare observations/potential outliers).\n\nFor brms, warnings for high k values will show when using add_criterion(brms_mod, \"loo\")Â \nOutliers make it tough to estimate out-of-sample accuracy, since rare values are unlikely to be in the new sample. (i.e.Â overfitting risk)\nAlso, warnings about high k values can occur when the sample size is small\n\nWhen looking at the posterior, keep in mind that â€œinfluentialâ€ data values might be significantly affecting the posterior distribution.\n\nSolutions\n\nIf there are only a few outliers, and you are sure to report results both with and without them, dropping outliers might be okay.\nIf there are several outliers, then a form of Robust Regression can be used or a Mixture Model.\n\nCommon to use a Studentâ€™s T distribution instead of a Gaussian for the outcome variable specification\n\nThe Student-t distribution arises from a mixture of Gaussian distributions with different variances. If the variances are diverse, then the tails can be quite thick.\nHas an extra shape parameter, Î½, that controls how thick the tails are.\n\nÎ½ = âˆ is a Gaussian distribution\nAs v â€“&gt; 1+ , tails start becoming fat\nÎ½ can be estimated with very large datasets that have plenty of rare events\n\n\n\n\nExample\n# shows k values for all data points below 0.5 threshold\nloo::loo(b8.3) %&gt;%Â \nÂ  plot()\n# K values\ntibble(k = b8.3$criteria$loo$diagnostics$pareto_k,Â \nÂ  Â  Â  row = 1:170) %&gt;%Â \nÂ  arrange(desc(k))\n# k value diagnostic table - shows how many are points have bad k values and that group's min n_eff\nloo(b8.3) %&gt;% loo::pareto_k_table()\n\n\nWidely Applicable Information Criterion (WAIC)\n\nDeviance with a penalty term based on the variance of the outcome variableâ€™s observation-level log-probabilities from the posterior\nEstimates out-of-sample deviance\n\n{loo}: brms::waic\n\nâ€œelpd_waicâ€: larger is better\nâ€œwaicâ€: is just (-2 * elpd_waic) to convert it to deviance scale, therefore smaller is better\nMay need to use add_criterion(brms_fit, \"waic\") in order to use the waic function\n\nRethinking pkg: smaller is better\n\nEffective number of parameters, pwaic (aka the penalty term or overfitting penalty)\n\nSays compute the variance in log-probabilities for each observation i, and then sum up these variances to get the total penalty.\nCalled such because in ordinary linear regressions the sum of all penalty terms from all points tends to be equal to the number of free parameters in the model\n\nWhen the sum is larger than the number of free parameters, it can indicate an outlier is present which will increase the overfitting risk.\n\nSee Solutions for outliers under PSIS &gt;&gt; Shape parameter, k\n\n\nWeights\n\nThese weights can be a quick way to see how big the differences are among models.\nEach model weight is essentially a proportion of itâ€™s WAIC or PSIS difference compared to the total of all the WAIC or PSIS differences.\n\nLarger is better\n\nExample\nbrms::model_weights(b8.1b, b8.2, b8.3, weights = \"loo\") %&gt;%\nÂ  Â  Â  Â  round(digits = 2)\n## b8.1bÂ  b8.2Â  b8.3Â \n##Â  0.00Â  0.03Â  0.97\n\nInterpretation:\n\nb8.3 has more than 95% of the model weight. Thatâ€™s very strong support for including the interaction effect, if prediction is our goal.\nThe modicum of weight given to b8.2 suggests that the posterior means for the slopes in b8.3 are a little overfit.\n\n\n\nBayes Factor\n\nThe ratio (or difference when logged) of the average likelihoods (the denominator of bayes theorem) of two models.\nSince the average likelihood has been averaged over the priors, it has a natural penalty for more complex models\nProblems\n\nEven when priors are weak and have little influence on posterior distributions within models, priors can have a huge impact on comparisons between models.\nNot always possible to compute the average likelihood"
  },
  {
    "objectID": "qmd/diagnostics-bayes.html#sec-diag-bay-misc",
    "href": "qmd/diagnostics-bayes.html#sec-diag-bay-misc",
    "title": "Bayes",
    "section": "Misc",
    "text": "Misc\n\nPrior sensitivity analysis\n\n{priorsense}\n\nVideo, Thread"
  },
  {
    "objectID": "qmd/diagnostics-bayes.html#sec-diag-bay-cbpp",
    "href": "qmd/diagnostics-bayes.html#sec-diag-bay-cbpp",
    "title": "Bayes",
    "section": "Correlations Between Parameter Posteriors",
    "text": "Correlations Between Parameter Posteriors\n\nCorrelated parameters and their uncertainties will co-vary within the posterior distribution\n\ne.g.Â High intercepts will often mean high slopes\nCentering/standardization of predictors can remove correlation between parameters\n\nWithout independent parameters\n\nParameters canâ€™t be interpreted independently\nParameter effects on prediction arenâ€™t independent\n\nbrms::pairs(model_fit) (SR Ch 4)\n\nExample: SR Ch 8,9\n\npost &lt;- posterior_samples(mod_obj)\npost %&gt;%\nÂ  select(-lp__ ) %&gt;%\nÂ  ggally::ggpairs()\n\nIgnore first â€œb_â€; no idea why that got added\na_cid1 is the intercept for factor variable, cid = 1\nb_cid1 is the slope for the predictor variable, geological ruggedness, when cid = 1\nSlope and intercept conditional on cid = 1 has the highest correlation at 0.174"
  },
  {
    "objectID": "qmd/diagnostics-bayes.html#sec-diag-bay-conv",
    "href": "qmd/diagnostics-bayes.html#sec-diag-bay-conv",
    "title": "Bayes",
    "section": "Convergence",
    "text": "Convergence\n\nMetrics\n\nMisc\n\nNotes from\n\nRank-normalization, folding, and localization: An improved RË† for assessing convergence of MCMC\n\nLots of detailed convergence analysis examples\n\n\nbayestestR::diagnostic_posterior has â€œESSâ€, â€œRhatâ€, â€œMCSEâ€\n\nAccepts rstanarm, brms models\n\nValues potentially indicate multimodal distribution (Vehtari, Thread)\n\n\nâ€œChain stacking might help, but would need to know more about the posterior to be more confident on recommendationâ€\n\n\nRhat - Gelman-Rubin convergence diagnostic\n\nEstimate of the convergence of Markov chains to the target distribution\n\nChecks if the start and end of each chain explores the same region\nChecks that independent chains explore the same region\n\nCan require long chains to work well\n** This diagnostic can fail for more complex models (i.e.Â bad chains even when value = 1) **\n\nNew metric called R* might be better (docs) but thereâ€™s arenâ€™t any guidelines on the values, so probably just useful for model comparison for now.\n\nRatio of variances\n\nAs total variance among all chains shrinks to the average variance within chains, R-hat approaches 1\nIf converges, Rhat = 1+\n\nGuideline\n\nIf value is above 1.00, it usually indicates that the chain has not yet converged, and probably you shouldnâ€™t trust the samples.\n\nSolution\n\nIf you draw more iterations, it could be fine, or it could never converge.\n\n\n\n\nAutocorrelation Metrics\n\nMarkov chains are typically autocorrelated, so that sequential samples are not entirely independent.\nEffects of Autocorrelation\n\nCan be an indicator of non-convergence\nIncreases uncertainty (standard errors)\nWhen chains have high autocorrelation, they can get stuck in regions of the parameter space making the sampling inefficient.\n\nI understand this to mean that less of the parameter space gets sampled\n\n\nSolutions\n\nIf you get warnings, taking more samples usually helps\nIncreasing max tree depth helps if max tree depth is continually being reached\n\nMCMCvis::MCMCdiag(fit, round = 2) produces diagnostics and shows sampler settings your model\n\nAccepts rstan, nimble, rjags, jagsUI, R2jags, rstanarm, and brms model objects\n\n\n\nEffective Sample Size (ESS)\n\nMeasures the amount by which autocorrelation in samples increases uncertainty (standard errors) relative to an independent sample.\n\nTells you how many samples the chain would have if there was 0 autocorrelation between samples in the chain\n\nMore autocorrelation means fewer effective number of samples.\n\nGuidelines for all ESS Metrics (tail or bulk)\n\nLarger is better\nBad: ESS &lt; 400 indicates convergence problemsÂ  (Vehtari)\nOkay: ESS â‰ˆ 800 corresponds to low relative efficiency of 1% (Vehtari)\nGood: ESS &gt; 1000 is sufficient for stable estimates (BÃ¼rkner, 2017)\nVery Good: ESS â‰¥ iteration amount\n\nGreater than means that something called anti-correlation is going on which is good\n\nExample: 2000 total samples with 1000 of those used for warm-up which is brms default. 4 chains x 1000 samples = 4000 post-warm-up samples. So for each parameter, the ESS should be around that or above\n\n\nn_eff in {rethinking} precis output\n\nSame as Bulk_ESS\n\nBulk_ESS - effective sample size around the bulk of the posterior (i.e.Â around the mean or median) (same as McElreathâ€™s n_eff)\n\nâ€œassesses how well the center of the distribution is resolvedâ€\n\ni.e.Â Measures how well HMC sampled the posterior around the bulk of the distribution in order to determine its shape.\n\nExample: Summary will give ESS stats\nas_draws_rvars(brms_fit) %&gt;%Â  Â \nÂ  Â  summarize_draws()\nvariable mean median Â  sd mad Â  q5 q95Â  rhat ess_bulk ess_tailÂ \nlp__ Â  Â  Â  -38.56Â  -38.20 1.30 1.02Â  -41.09Â  -37.21Â  Â  1Â  Â  1880Â  Â  2642Â \nalpha_c Â  Â  Â  Â  9.32 9.32 0.14 0.14 9.09 9.55Â  Â  1Â  Â  3364Â  Â  2436Â \nbeta Â  Â  Â  Â  0.02 0.02 0.01 0.01 0.01 0.03Â  Â  1Â  Â  3864Â  Â  2525Â \nsigma Â  Â  Â  Â  1.12 1.11 0.10 0.10 0.97 1.29Â  Â  1Â  Â  3014Â  Â  2776\nExample: Select variable and ESS Values for Quantiles\nas_draws_rvars(brms_fit) %&gt;%\nÂ  subset_draws(\"beta100\") %&gt;%\nÂ  summarize_draws(ess_mean, ~ess_quantile(.x, probs = c(0.05, 0.95)))\nvariableÂ  ess_meanÂ  ess_q5Â  ess_q95\nbeta100Â  Â  Â  3816Â  Â  2525Â  Â  3153\n\nThese are ESS values for\n\nThe summary estimate (aka point estimate) which is the mean of the posterior in this case\nAnd the CI values of that summary estimate\n\nSo it makes sense youâ€™d have lower numbers of effective samples in the tails of the posterior than in the bulk since itâ€™s going to get sampled less than the bulk\n\n\nTail_ESS - effective sample size in the tails of the posterior\n\nmeasures how well HMC sampled the posterior in the tails of the distribution in order to determine their shape.\n\n\nAutocorrelation plots for chains\n\npost &lt;- posterior_samples(mod_obj)\npost %&gt;%Â \nmcmc_acf(pars = vars(b_a_cid1:sigma),\nÂ  Â  Â  lags = 5) +\ntheme_pomological_fancy(base_family = \"Marck Script\")\n\nExample from Ch 8,9 Statistical Rethinking\nL-shaped autocorrelation plots like these are good.\n\nThose are the kinds of shapes youâ€™d expect when you have reasonably large effective samples."
  },
  {
    "objectID": "qmd/diagnostics-bayes.html#sec-diag-bay-mix",
    "href": "qmd/diagnostics-bayes.html#sec-diag-bay-mix",
    "title": "Bayes",
    "section": "Mixing",
    "text": "Mixing\n\nPosteriors and Trace Plots\nbrms::plot(mod_obj)\n\nExample: Ch 8,9 Statistical Rethinking\n\n\nSee above for parameter descriptions\nTrace plots with fat, lazy caterpillars like these are good\n\n\nTrank Plot (Trace Rank Plot)\n\npost &lt;- posterior_samples(b9.1b, add_chain = T)\npost %&gt;%Â \nÂ  bayesplot::mcmc_rank_overlay(pars = vars(b_a_cid1:sigma)) +\nÂ  Â  Â  scale_color_pomological() +\nÂ  Â  Â  ggtitle(\"My custom trank plots\") +\nÂ  Â  Â  coord_cartesian(ylim = c(25, NA)) +\nÂ  Â  Â  theme_pomological_fancy(base_family = \"Marck Script\") +\nÂ  Â  Â  theme(legend.position = c(.95, .2))"
  },
  {
    "objectID": "qmd/diagnostics-bayes.html#sec-diag-bay-ppdc",
    "href": "qmd/diagnostics-bayes.html#sec-diag-bay-ppdc",
    "title": "Bayes",
    "section": "Posterior Predictive Check",
    "text": "Posterior Predictive Check\n\nExample:\n\nBad Fit\n\nbrms::pp_check(model, nsamples = 100) + xlim(0, 20)\nGood Fit\n\n\nWith {bayesplot}\n\nbayesplot::ppc_rootogram(y = testdata$observedResponse,\nÂ  Â  Â  Â  Â  Â  Â  yrep = posterior_predict(model4, nsamples = 1000)) +\nÂ  xlim(0, 20)"
  },
  {
    "objectID": "qmd/diagnostics-bayes.html#sec-diag-bay-gof",
    "href": "qmd/diagnostics-bayes.html#sec-diag-bay-gof",
    "title": "Bayes",
    "section": "GOF Plots",
    "text": "GOF Plots\n\nMisc\n\nAlso see Statistical Rethinking, Chapter 5 &gt;&gt; Inferential Plots\n\nPredictor Residual, Counterfactual Plots, and Posterior Predictive Plots\n\n\nData with regression line\n\n# scatter plot of observed pts with the regression line from the model\n# Defined the by the alpha and beta estimate\ndat %&gt;%\nÂ  ggplot(aes(x = weight, y = height)) +\nÂ  geom_abline(intercept = fixef(b4.3)[1],\nÂ  Â  Â  Â  Â  Â  Â  slopeÂ  Â  = fixef(b4.3)[2]) +\nÂ  geom_point(shape = 1, size = 2, color = \"royalblue\") +\nÂ  theme_bw() +\nÂ  theme(panel.grid = element_blank())\nPredicted vs Observed with PI and CI around regression line\n\n# x value (weight) range we want for the CI of the line\nweight_seq &lt;- tibble(weight = seq(from = 25, to = 70, by = 1))\n\n# predicted values (height) for each x value\n# 95% CIs generated by default\nmu_summary &lt;-Â  fitted(b4.3, newdata = weight_seq) %&gt;%\nÂ  Â  as_tibble() %&gt;%\nÂ  Â  # let's tack on the `weight` values from `weight_seq`\nÂ  Â  bind_cols(weight_seq)\n\n# 95% PIs generated by default\npred_height &lt;-Â  predict(b4.3,\nÂ  Â  Â  newdata = weight_seq) %&gt;%\nÂ  Â  as_tibble() %&gt;%\nÂ  Â  bind_cols(weight_seq)\n\n# includes regression line, CI, and PI\ndat %&gt;%Â  ggplot(aes(x = weight)) +\nÂ  # PIs\nÂ  geom_ribbon(data = pred_height, aes(ymin = Q2.5,\nÂ  Â  Â  Â  Â  Â  Â  ymax = Q97.5), fill = \"grey83\") +\nÂ  # CIs\nÂ  geom_smooth(data = mu_summary, aes(y = Estimate, ymin = Q2.5, ymax = Q97.5),\nÂ  Â  Â  Â  Â  Â  Â  stat = \"identity\", fill = \"grey70\", color = \"black\",\nÂ  Â  Â  Â  Â  Â  Â  alpha = 1, size = 1/2) +\nÂ  geom_point(aes(y = height), color = \"navyblue\", shape = 1,\nÂ  Â  Â  Â  Â  Â  size = 1.5, alpha = 2/3) +\nÂ  coord_cartesian(xlim = range(d2$weight),\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  ylim = range(d2$height)) +\nÂ  theme(text = element_text(family = \"Times\"),\nÂ  Â  Â  Â  panel.grid = element_blank())"
  },
  {
    "objectID": "qmd/diagnostics-bayes.html#sec-diag-bay-resid",
    "href": "qmd/diagnostics-bayes.html#sec-diag-bay-resid",
    "title": "Bayes",
    "section": "Residuals",
    "text": "Residuals\n\n{DHARMa}\n\nVignettes:\n\nDHARMa for Bayesians\nIntroduction to DHARMa\n\nExample:\n\nBad Fit\n\npacman::p_load(brms, DHARMa)\n\nmodel.check &lt;- createDHARMa(\nÂ  simulatedResponse = t(posterior_predict(model)),\nÂ  observedResponse = testdata$observedResponse,\nÂ  fittedPredictedResponse = apply(t(posterior_epred(model)), 1, mean),\nÂ  integerResponse = TRUE)\nplot(model.check)\n\nHierarchical dataset fit with a poisson model. Everything is bad in this fit.\n\nGood Fit\n\n\nHierarchical dataset fit with a hierarchical negative binomial model.\n\n\n\nExampleGroup Variation Check\n\nÂ  Â  plot(model.check, form = testdata$group)\n\nThis is hierarchical dataset fit with a hierarchical poisson model. The within-group box-plots show that model has captured the group variance sufficiently as both tests have non-significant (n.s.) results.\n\nExample: Overdispersion\n\nBad fit\n\nÂ  Â  testDispersion(model.check)\n\nThis is hierarchical dataset fit with a hierarchical poisson model. In the previous chart (left panel), the overdispersion test failed. This histogram shows how much the model is off.\n\nGood Fit"
  },
  {
    "objectID": "qmd/diagnostics-bayes.html#sec-diag-bay-pa",
    "href": "qmd/diagnostics-bayes.html#sec-diag-bay-pa",
    "title": "Bayes",
    "section": "Predictive Accuracy",
    "text": "Predictive Accuracy\n\nMisc\n\nSee Statistical Rethinking, Chapter 7 for details\nloo package website has some nice CV workflows\n\nAlso ensembling, time series\n\nDonâ€™t compare models with different numbers of observations (SR, Ch 11)\n\ne.g.Â 1/0 logistic regression model vs aggregated logistic regression model\n\nPSIS-LOO (IS-LOO, WAIC, etc) has difficulties if each observation has their own parameter(s) (aka â€œrandom effectsâ€) (Vehtari thread + post)\n\nModel Comparison\n\nMcElreath: To judge whether two models are â€œeasy to distinguishâ€ (i.e.Â kinda like whether their scores are statistically different), we look at the differences between the model with the best WAIC and the WAICs of the other models along with the standard error of the difference of the WAIC scores\n\nbrms::loo_compare(loo_obj, loo_obj)\n\nWhere a â€œloo_objâ€ is a brms::loo(fit) or brms::waic(fit) object\nCan also take a list of loo objects\nsimplify = FALSE gives a more detailed summary\n\nIf the difference in ELPD is much larger or several times the estimated standard error of the difference, then the top model is expected to have better predictive performance\n\n\nPareto-Smoothed Importance Sampling Cross-Validation (PSIS)\n\nWeights observations based on influence on the posterior\nUses highly influential observations to formulate a pareto distribution and sample from it\nbrms::loo - wrappers for loo::loo (docs)\nEstimates out-of-sample LOO-CV lppd\n\n{loo}\n\nâ€œelpd_looâ€ - Larger is better\nâ€œlooicâ€ - is just (-2 * elpd_loo) to convert it to the deviance scale, therefore smaller is better\nMay need to use add_criterion(brms_fit, \"loo\") in order to use the loo function\n\n{Rethinking}: Smaller is better\n\nThe shape parameter of the distribution, k, is estimated. When k &gt; 0.5, then the distribution has infinite variance. PSIS weights perform well as long as k &lt; 0.7. Large k values can be used to identify influential observations (i.e.Â rare observations/potential outliers).\n\nFor brms, warnings for high k values will show when using add_criterion(brms_mod, \"loo\")Â \nOutliers make it tough to estimate out-of-sample accuracy, since rare values are unlikely to be in the new sample. (i.e.Â overfitting risk)\nAlso, warnings about high k values can occur when the sample size is small\n\nWhen looking at the posterior, keep in mind that â€œinfluentialâ€ data values might be significantly affecting the posterior distribution.\n\nSolutions\n\nIf there are only a few outliers, and you are sure to report results both with and without them, dropping outliers might be okay.\nIf there are several outliers, then a form of Robust Regression can be used or a Mixture Model.\n\nCommon to use a Studentâ€™s T distribution instead of a Gaussian for the outcome variable specification\n\nThe Student-t distribution arises from a mixture of Gaussian distributions with different variances. If the variances are diverse, then the tails can be quite thick.\nHas an extra shape parameter, Î½, that controls how thick the tails are.\n\nÎ½ = âˆ is a Gaussian distribution\nAs v â€“&gt; 1+ , tails start becoming fat\nÎ½ can be estimated with very large datasets that have plenty of rare events\n\n\n\n\nExample\n# shows k values for all data points below 0.5 threshold\nloo::loo(b8.3) %&gt;%Â \nÂ  plot()\n# K values\ntibble(k = b8.3$criteria$loo$diagnostics$pareto_k,Â \nÂ  Â  Â  row = 1:170) %&gt;%Â \nÂ  arrange(desc(k))\n# k value diagnostic table - shows how many are points have bad k values and that group's min n_eff\nloo(b8.3) %&gt;% loo::pareto_k_table()\n\n\nWidely Applicable Information Criterion (WAIC)\n\nDeviance with a penalty term based on the variance of the outcome variableâ€™s observation-level log-probabilities from the posterior\nEstimates out-of-sample deviance\n\n{loo}: brms::waic\n\nâ€œelpd_waicâ€: Larger is better\nâ€œwaicâ€: is just (-2 * elpd_waic) to convert it to deviance scale, therefore smaller is better\nMay need to use add_criterion(brms_fit, \"waic\") in order to use the waic function\n\n{Rethinking}: Smaller is better\n\nEffective number of parameters, pwaic (aka the penalty term or overfitting penalty)\n\nSays compute the variance in log-probabilities for each observation i, and then sum up these variances to get the total penalty.\nCalled such because in ordinary linear regressions the sum of all penalty terms from all points tends to be equal to the number of free parameters in the model\n\nWhen the sum is larger than the number of free parameters, it can indicate an outlier is present which will increase the overfitting risk.\n\nSee Solutions for outliers under PSIS &gt;&gt; Shape parameter, k\n\n\nWeights\n\nThese weights can be a quick way to see how big the differences are among models.\nEach model weight is essentially a proportion of itâ€™s WAIC or PSIS difference compared to the total of all the WAIC or PSIS differences.\n\nLarger is better\n\nExample\nbrms::model_weights(b8.1b, b8.2, b8.3, weights = \"loo\") %&gt;%\nÂ  Â  Â  Â  round(digits = 2)\n## b8.1bÂ  b8.2Â  b8.3Â \n##Â  0.00Â  0.03Â  0.97\n\nInterpretation:\n\nb8.3 has more than 95% of the model weight. Thatâ€™s very strong support for including the interaction effect, if prediction is our goal.\nThe modicum of weight given to b8.2 suggests that the posterior means for the slopes in b8.3 are a little overfit.\n\n\n\nBayes Factor\n\nThe ratio (or difference when logged) of the average likelihoods (the denominator of bayes theorem) of two models.\nSince the average likelihood has been averaged over the priors, it has a natural penalty for more complex models\nProblems\n\nEven when priors are weak and have little influence on posterior distributions within models, priors can have a huge impact on comparisons between models.\nNot always possible to compute the average likelihood"
  },
  {
    "objectID": "qmd/db-postgres.html#sec-db-pstgr-dock",
    "href": "qmd/db-postgres.html#sec-db-pstgr-dock",
    "title": "postgres",
    "section": "Docker",
    "text": "Docker\n\nSteps\n\nStart docker desktop\nStart powershell\ndocker run --name pg_database -p 5432:5432 -e POSTGRES_PASSWORD=ericb2022 -d postgres:latest\n\n1st 5432 is local computer port\n2nd 5432 is the required postgres image port\n-e is for defining an environment variable; here its the db password that I set to ericb2022\n-d\n\nRuns the container in the background\nAllows you to run commands in the same terminal window that you used the container run command in\n\nâ€œpostgres:latestâ€ is the name of the image to build the container from\n\nClose powershell\nIn docker desktop, the â€œpg_databaseâ€ container should be running\n\nConnect to the db\n\nSteps\n\npsql should be in your list of path environment variables\n\nRight-click Start &gt;&gt; System &gt;&gt; advanced settings (right panel) &gt;&gt; environment variables &gt;&gt; highlight path &gt;&gt; edit\nâ€œC:\\Program Files\\PostgreSQL\\14\\binâ€\n\n** Note the â€œ14â€ in the path which is the current version. Therefore, when postgres is updated, this path will have to be updated **\n\n\npsql --host localhost --port 5432 --dbname postgres --username postgres\n\nNote these are all default values, so this is equivalent to psql -U postgres\n--host (-h) is the ip address or computer name that you want to connect to\n\nlocalhost is for the docker container thatâ€™s running\n\n5432 is the default â€“port (-p) for a postgres container\n--dbname (-d) is the name of the database on the server\n\nâ€œpostgresâ€ is a db that ships with postgres\n\n--username (-U) is a username that has permission to access the db\n\nâ€œpostgresâ€ is the default super-user name\n\n\nA prompt will then ask you for that usernameâ€™s password\n\nThe container above has the password ericb2022\n\nThis didnâ€™t work for me, needed to use my postgres password that I set-up when I installed postgres and pgAdmin.\nMy local postgres server and the container are listening on the same port, so maybe if I changed the first port number to something else, it would connect to the container.\n\n\nTo exit db, \\q\n\n\nCreate a db\n\nSteps\n\ncreatedb -h localhost -p 5432 -U postgres -O eric two_trees\n\n-U is the user account used to create the db\n-O is used to assign ownership to another user account\n\nâ€œroleâ€ (i.e.Â user account) must already exist\n\nâ€œtwo_treesâ€ is the name of the new db\nYou will be prompted for userâ€™s password\n\nList of dbs on the server\n\npsql -h localhost -p 5432 -U postgres -l\n\n-l lists all dbs on server\nYou will be prompted for userâ€™s password\n\n\n\n\nRun a sql script\n\npsql -d acweb -f test.sql\n\n-d is for the database name (e.g.Â acweb)\n-f is for running a file (e.g.Â test.sql)\n\n\nAdd users\n\nCreate user/role (once inside db)\nCREATE USER &lt;user name1&gt;;\nCREATE ROLE &lt;user name2&gt;;\nALTER ROLE &lt;user name2&gt; LOGIN\n\nCREATE USER will give the user login attribute/permission while CREATE ROLE will not\n\nALTER ROLE gives the user attributes/permissions (e.g.Â login permission)\n\nCreate user/role (at the CLI) - createuser &lt;user name&gt;"
  },
  {
    "objectID": "qmd/db-postgres.html#python",
    "href": "qmd/db-postgres.html#python",
    "title": "postgres",
    "section": "Python",
    "text": "Python\n\n{{psycopg2}}\n\nMisc\n\nNotes from Fastest Way to Load Data Into PostgreSQL Using Python\n\n\nConnect to db\nimport psycopg2\n\nconnection = psycopg2.connect(\n    host=\"localhost\",\n    database=\"testload\",\n    user=\"haki\",\n    password=None,\n)\nconnection.autocommit = True\nCreate a table\ndef create_staging_table(cursor) -&gt; None:\n    cursor.execute(\"\"\"\n        DROP TABLE IF EXISTS staging_beers;\n        CREATE UNLOGGED TABLE staging_beers (\n            id                  INTEGER,\n            name                TEXT,\n            tagline             TEXT,\n            first_brewed        DATE,\n            description         TEXT,\n            image_url           TEXT,\n            abv                 DECIMAL,\n            ibu                 DECIMAL,\n            target_fg           DECIMAL,\n            target_og           DECIMAL,\n            ebc                 DECIMAL,\n            srm                 DECIMAL,\n            ph                  DECIMAL,\n            attenuation_level   DECIMAL,\n            brewers_tips        TEXT,\n            contributed_by      TEXT,\n            volume              INTEGER\n        );\n    \"\"\")\n\nwith connection.cursor() as cursor:\n  create_staging_table(cursor)\n\nThe function receives a cursor and creates a unlogged table called staging_beers."
  },
  {
    "objectID": "qmd/code-optimization.html#sec-code-opt-py",
    "href": "qmd/code-optimization.html#sec-code-opt-py",
    "title": "Optimization",
    "section": "Python",
    "text": "Python\n\nBenchmarking\n\n{{time}}(built-in module)\nimport time\nstart = time.perf_counter()\ntime.sleep(1) # do work\nelapsed = time.perf_counter() - start\nprint(f'Time {elapsed:0.4}')\n#&gt; Time 1.001\n\nMemory Usage\n\n{{memory-profiler}}\n\nBasic usage for a function\nfrom memory_profiler import memory_usage\nmem, retval = memory_usage((fn, args, kwargs), retval=True, interval=1e-7)\n\ninterval: For very quick operations the function fn might be executed more than once. By setting interval to a value lower than 1e-6, we force it to execute only once.\nretval: Tells the function to return the result of fn.\n\nNon-interactive usage\n$ python -m memory_profiler example.py\n#&gt; Line #    Mem usage  Increment   Line Contents\n#&gt; ==============================================\n#&gt;      3                           @profile\n#&gt;      4      5.97 MB    0.00 MB   def my_func():\n#&gt;      5     13.61 MB    7.64 MB       a = [1] * (10 ** 6)\n#&gt;      6    166.20 MB  152.59 MB       b = [2] * (2 * 10 ** 7)\n#&gt;      7     13.61 MB -152.59 MB       del b\n#&gt;      8     13.61 MB    0.00 MB       return a\n\n\nProfile decorator\nimport time\nfrom functools import wraps\nfrom memory_profiler import memory_usage\n\ndef profile(fn):\n    @wraps(fn)\n    def inner(*args, **kwargs):\n        fn_kwargs_str = ', '.join(f'{k}={v}' for k, v in kwargs.items())\n        print(f'\\n{fn.__name__}({fn_kwargs_str})')\n\n        # Measure time\n        t = time.perf_counter()\n        retval = fn(*args, **kwargs)\n        elapsed = time.perf_counter() - t\n        print(f'Time   {elapsed:0.4}')\n\n        # Measure memory\n        mem, retval = memory_usage((fn, args, kwargs), retval=True, timeout=200, interval=1e-7)\n\n        # Get Peak Memory Usage\n        print(f'Memory {max(mem) - min(mem)}')\n        return retval\n\n    return inner\n\n@profile\ndef work(n):\n   for i in range(n):\n       2 ** n\n\nwork(10)\n#&gt; work()\n#&gt; Time   0.06269\n#&gt; Memory 0.0\n\nwork(n=10000)\n#&gt; work(n=10000)\n#&gt; Time   0.3865\n#&gt; Memory 0.0234375"
  },
  {
    "objectID": "qmd/db-postgres.html#sec-db-pstgr-py",
    "href": "qmd/db-postgres.html#sec-db-pstgr-py",
    "title": "postgres",
    "section": "Python",
    "text": "Python\n\n{{psycopg2}}\n\nMisc\n\nNotes from Fastest Way to Load Data Into PostgreSQL Using Python\ntl;dr\n\nLarge Data: use copy_to\nMedium to Small Data:\n\nTime and memory isnâ€™t an issue: Use extract_values or maybe copy_to if you donâ€™t have JSON.\n\n\n\nConnect to db\nimport psycopg2\n\nconnection = psycopg2.connect(\n    host=\"localhost\",\n    database=\"testload\",\n    user=\"haki\",\n    password=None,\n)\nconnection.autocommit = True\nCreate a table\ndef create_staging_table(cursor) -&gt; None:\n    cursor.execute(\"\"\"\n        DROP TABLE IF EXISTS staging_beers;\n        CREATE UNLOGGED TABLE staging_beers (\n            id                  INTEGER,\n            name                TEXT,\n            tagline             TEXT,\n            first_brewed        DATE,\n            description         TEXT,\n            image_url           TEXT,\n            abv                 DECIMAL,\n            ibu                 DECIMAL,\n            target_fg           DECIMAL,\n            target_og           DECIMAL,\n            ebc                 DECIMAL,\n            srm                 DECIMAL,\n            ph                  DECIMAL,\n            attenuation_level   DECIMAL,\n            brewers_tips        TEXT,\n            contributed_by      TEXT,\n            volume              INTEGER\n        );\n    \"\"\")\n\nwith connection.cursor() as cursor:\n  create_staging_table(cursor)\n\nThe function receives a cursor and creates a unlogged table called staging_beers.\n\nInsert many rows at once\n\nNotes from Fastest Way to Load Data Into PostgreSQL Using Python\nThe best way to load data into a database is using the copy command (last method in this section). The issue here is that copy needs a .csv file and not json.\n\nThis might be an issue just because of psycopg2 library doesnâ€™t support json or that there is a postgres extension that isnâ€™t supported by the library. This also might not be a problem in the future.\n\nData\nbeers = iter_beers_from_api()\nnext(beers)\n{'id': 1,\n 'name': 'Buzz',\n 'tagline': 'A Real Bitter Experience.',\n 'first_brewed': '09/2007',\n 'description': 'A light, crisp and bitter IPA brewed...',\n 'image_url': 'https://images.punkapi.com/v2/keg.png',\n 'abv': 4.5,\n 'ibu': 60,\n 'target_fg': 1010,\n...\n}\nnext(beers)\n{'id': 2,\n 'name': 'Trashy Blonde',\n 'tagline': \"You Know You Shouldn't\",\n 'first_brewed': '04/2008',\n 'description': 'A titillating, ...',\n 'image_url': 'https://images.punkapi.com/v2/2.png',\n 'abv': 4.1,\n 'ibu': 41.5,\n ...\n }\n\nData is from beers api\niter_beers_from_api is a udf that takes the json from the api and creates a generator object that iterates through each beer.\n\nInsert data in db using execute_values (low memory usage and still pretty fast)\ndef insert_execute_values_iterator(\n    connection,\n    beers: Iterator[Dict[str, Any]],\n    page_size: int = 100,\n) -&gt; None:\n    with connection.cursor() as cursor:\n        create_staging_table(cursor)\n        psycopg2.extras.execute_values(cursor, \"\"\"\n            INSERT INTO staging_beers VALUES %s;\n        \"\"\", ((\n            beer['id'],\n            beer['name'],\n            beer['tagline'],\n            parse_first_brewed(beer['first_brewed']),\n            beer['description'],\n            beer['image_url'],\n            beer['abv'],\n            beer['ibu'],\n            beer['target_fg'],\n            beer['target_og'],\n            beer['ebc'],\n            beer['srm'],\n            beer['ph'],\n            beer['attenuation_level'],\n            beer['brewers_tips'],\n            beer['contributed_by'],\n            beer['volume']['value'],\n        ) for beer in beers), page_size=page_size)\n\ninsert_execute_values_iterator(page_size=1000)\n\nparse_first_brewed is a udf that transforms a date string to datetime type.\nbeer[â€˜volumeâ€™][â€˜valueâ€™]: Data is in json and the value for volume is subsetted from the nested field.\nBenchmark: At page_size = 1000, 1.468s, 0.0MB of RAM used\nThe generator((bear['id'], â€¦ , bear['volume']['value'], for beer in beers) keeps data from being stored in memory during transformation\npage_size: maximum number ofÂ arglist items to include in every statement. If there are more items the function will execute more than one statement.\n\nHere arglist is the data in the form of generator\n\n\nInsert data in db using copy_from (Fast but memory intensive)\nimport io\n\ndef clean_csv_value(value: Optional[Any]) -&gt; str:\n    if value is None:\n        return r'\\N'\n    return str(value).replace('\\n', '\\\\n')\n\ndef copy_stringio(connection, beers: Iterator[Dict[str, Any]]) -&gt; None:\n    with connection.cursor() as cursor:\n        create_staging_table(cursor)\n        csv_file_like_object = io.StringIO()\n        for beer in beers:\n            csv_file_like_object.write('|'.join(map(clean_csv_value, (\n                beer['id'],\n                beer['name'],\n                beer['tagline'],\n                parse_first_brewed(beer['first_brewed']),\n                beer['description'],\n                beer['image_url'],\n                beer['abv'],\n                beer['ibu'],\n                beer['target_fg'],\n                beer['target_og'],\n                beer['ebc'],\n                beer['srm'],\n                beer['ph'],\n                beer['attenuation_level'],\n                beer['contributed_by'],\n                beer['brewers_tips'],\n                beer['volume']['value'],\n            ))) + '\\n')\n        csv_file_like_object.seek(0)\n        cursor.copy_from(csv_file_like_object, 'staging_beers', sep='|')\n\nclean_csv_value: Transforms a single value\n\nEscape new lines: some of the text fields include newlines, so we escape \\n -&gt; \\\\n.\nEmpty values are transformed to \\N: The string \"\\N\" is the default string used by PostgreSQL to indicate NULL in COPY (this can be changed using the NULL option).\n\ncsv_file_like_object: Generate a file like object using io.StringIO. A StringIO object contains a string which can be used like a file. In our case, a CSV file.\ncsv_file_like_object.write: Transform a beer to a CSV row\n\nTransform the data: transformations on first_brewed and volume are performed here.\nPick a delimiter: Some of the fields in the dataset contain free text with commas. To prevent conflicts, we pick â€œ|â€ as the delimiter (another option is to use QUOTE).\n\n\nInsert data (streaming) in db using copy_from (Fastest and low memory but complicated, at least with json)\n\nBuffering function\nfrom typing import Iterator, Optional\nimport io\n\nclass StringIteratorIO(io.TextIOBase):\n    def __init__(self, iter: Iterator[str]):\n        self._iter = iter\n        self._buff = ''\n\n    def readable(self) -&gt; bool:\n        return True\n\n    def _read1(self, n: Optional[int] = None) -&gt; str:\n        while not self._buff:\n            try:\n                self._buff = next(self._iter)\n            except StopIteration:\n                break\n        ret = self._buff[:n]\n        self._buff = self._buff[len(ret):]\n        return ret\n\n    def read(self, n: Optional[int] = None) -&gt; str:\n        line = []\n        if n is None or n &lt; 0:\n            while True:\n                m = self._read1()\n                if not m:\n                    break\n                line.append(m)\n        else:\n            while n &gt; 0:\n                m = self._read1(n)\n                if not m:\n                    break\n                n -= len(m)\n                line.append(m)\n        return ''.join(line)\n\nThe regular io.StringIO creates a file-like object but is memory-heavy. This function creates buffer that will feed each line of the file into a buffer, stream it to copy, empty the buffer, and load the next line.\n\nCopy to db\ndef clean_csv_value(value: Optional[Any]) -&gt; str:\n    if value is None:\n        return r'\\N'\n    return str(value).replace('\\n', '\\\\n')\n\ndef copy_string_iterator(connection, beers: Iterator[Dict[str,\nAny]]) -&gt; None:\n    with connection.cursor() as cursor:\n        create_staging_table(cursor)\n        beers_string_iterator = StringIteratorIO((\n            '|'.join(map(clean_csv_value, (\n                beer['id'],\n                beer['name'],\n                beer['tagline'],\n                parse_first_brewed(beer['first_brewed']).isoformat(),\n                beer['description'],\n                beer['image_url'],\n                beer['abv'],\n                beer['ibu'],\n                beer['target_fg'],\n                beer['target_og'],\n                beer['ebc'],\n                beer['srm'],\n                beer['ph'],\n                beer['attenuation_level'],\n                beer['brewers_tips'],\n                beer['contributed_by'],\n                beer['volume']['value'],\n            ))) + '\\n'\n            for beer in beers\n        ))\n        cursor.copy_from(beers_string_iterator, 'staging_beers', sep='|')\n\nSimilar to other code above"
  }
]