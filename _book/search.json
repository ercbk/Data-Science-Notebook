[
  {
    "objectID": "qmd/surveys-census-data.html",
    "href": "qmd/surveys-census-data.html",
    "title": "Census Data",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Surveys",
      "Census Data"
    ]
  },
  {
    "objectID": "qmd/surveys-census-data.html#sec-surv-cens-misc",
    "href": "qmd/surveys-census-data.html#sec-surv-cens-misc",
    "title": "Census Data",
    "section": "",
    "text": "Notes from\n\nTidycensus Workshop 2024\n\nFIPS GEOID\n\npopular variable calculations from variables in ACS\nCensus Geocoder (link)\n\nEnter an address and codes for various geographies are returned\nBatch geocoding available for up to 10K records\n\nCodes for geographies returned in a .csv file\n\n\nTIGERweb (link)\n\nAllows you to get geography codes by searching for an area on a map\nOnce zoomed-in on your desired area, you turn on geography layers to find the geography code for your area.\n\nUS Census Regions",
    "crumbs": [
      "Surveys",
      "Census Data"
    ]
  },
  {
    "objectID": "qmd/surveys-census-data.html#sec-surv-cens-geo",
    "href": "qmd/surveys-census-data.html#sec-surv-cens-geo",
    "title": "Census Data",
    "section": "Geographies",
    "text": "Geographies\n\n\nMisc\n\n{tidycensus} docs on various geographies, function arguments, and which surveys (ACS, Census) they’re available in.\nACS Geography Boundaries by Year (link)\n\nTypes\n\nLegal/Administrative\n\nCensus gets boundaries from outside party (state, county, city, etc.)\ne.g. election areas, school districts, counties, county subdivisions\n\nStatistical\n\nCensus creates these boundaries\ne.g. regions, census tracts, ZCTAs, block groups, MSAs, urban areas\n\n\nNested Areas\n\n\nCensus Tracts\n\nAreas within a county\nAround 1200 to 8000 people\nSmall towns, rural areas, neighborhoods\n** Census tracts may cross city boundaries **\n\nBlock Groups\n\nAreas within a census tract\nAround 600 to 3000 people\n\nCensus Blocks\n\nAreas within a block group\nNot for ACS, only for the 10-yr census\n\n\nPlaces\n\nMisc\n\nOne place cannot overlap another place\nExpand and contract as population or commercial activity increases or decreases\nMust represent an organized settlement of people living in close proximity.\n\nIncorporated Places\n\ncities, towns, villages\nUpdated through Boundary and Annexation Survey (BAS) yearly\n\nCensus Designated Places (CDPs)\n\nAreas that can’t become Incorporated Places because of state or city regulations\nConcentrations of population, housing, commericial structures\nUpdated through Boundary and Annexation Survey (BAS) yearly\n\n\nCounty Subdivisions\n\nMinor Civil Divisions (MCDs)\n\nLegally defined by the state or county, stable entity. May have elected government\ne.g. townships, charter townships, or districts\n\nCensus County Divisions (CCDs)\n\nno population requirment\nSubcounty units with stable boundaries and recognizable names\n\n\nZip Code Tabulation Areas (ZCTAs)\n\n\nMisc\n\n{crosswalkZCTA} - Contains the US Census Bureau’s 2020 ZCTA to County Relationship File, as well as convenience functions to translate between States, Counties and ZIP Code Tabulation Areas (ZCTAs)\n\nApproximate USPS Code distribution for housing units\n\nThe most frequently occurring zip code within an census block is assigned to a census block\nThen blocks are aggregated into areas (ZCTAs)\n\nZCTAs do NOT nest within any other geographies\n\nI guess the aggregated ZCTA blocks can overlap block groups\n\n2010 ZCTAs exclude large bodies of water and unpopulated areas",
    "crumbs": [
      "Surveys",
      "Census Data"
    ]
  },
  {
    "objectID": "qmd/surveys-census-data.html#sec-surv-cens-acs",
    "href": "qmd/surveys-census-data.html#sec-surv-cens-acs",
    "title": "Census Data",
    "section": "American Community Survey (ACS)",
    "text": "American Community Survey (ACS)\n\nAbout\n\nYearly estimates based on samples of the population over a 5yr period\n\nTherefore a Margin of Error (MoE) is included with the estimates.\n\nDetailed social, economic, housing, and demographic characteristics\ncensus.gov/acs\n\nACS Release Schedule (releases)\n\nSeptember - 1-Year Estimates (from previous year’s collection)\n\nEstimates for areas with populations of &gt;65K\n\nOctober - 1-Year Supplemental Estimates\n\nEstimates for areas with populations between 20K-64999\n\nDecember - 5-Year Estimates\n\nEstimates for areas including census tract and block groups\n\n\nData Collected\n\nPopulation\n\nSocial\n\nAncestry, Citizenship, Citizen Voting Age  Population, Disability, Education Attainment, Fertility, Grandparents, Language, Marital Status, Migration, School Enrollment, Veterans\n\nDemographic\n\nAge, Hispanic Origin, Race, Relationship, Sex\n\nEconomic\n\nClass of worker, Commuting, Employment Status, Food Stamps (SNAP), Health Insurance, Hours/Week, Weeks/Year, Income, Industry & Occupation\n\n\nHousing\n\nComputer & Internet Use, Costs (Mortgage, Taxes, Insurance), Heating Fuel, Home Value, Occupancy, Plumbing/Kitchen Facilities, Structure, Tenure (Own/Rent), Utilities, Vehicles, Year Built/Year Movied In",
    "crumbs": [
      "Surveys",
      "Census Data"
    ]
  },
  {
    "objectID": "qmd/surveys-census-data.html#sec-surv-cens-dic",
    "href": "qmd/surveys-census-data.html#sec-surv-cens-dic",
    "title": "Census Data",
    "section": "Dicennial US Census",
    "text": "Dicennial US Census\n\nMisc\n\nA complete count — not based on samples like the ACS\nApplies differential privacy to preserve respondent confidentiality\n\nAdds noise to data. Greater effect at lower levels (i.e. block level)\nThe exception is that is no differetial privacy for household-level data.\n\n\n\n\nPL94-171\n\nPopulation data which the government needs for redistricting\nsumfile = “pl”\nState Populations\npop20 &lt;- \n  get_decennial(\n    geography = \"state\",\n    variables = \"P1_001N\",\n    year = 2020\n  )\n\nFor 2020, default is sumfile = “pl”\n\n\n\n\nDHC\n\nAge, Sex, Race, Ethnicity, and Housing Tenure (most popular dataset)\nsumfile = “dhc”\nCounty\ntx_population &lt;- \n  get_decennial(\n    geography = \"county\",\n    variables = \"P1_001N\",\n    state = \"TX\",\n    sumfile = \"dhc\",\n    year = 2020\n  )\nCensus Block (analogous to a city block)\nmatagorda_blocks &lt;- \n  get_decennial(\n    geography = \"block\",\n    variables = \"P1_001N\",\n    state = \"TX\",\n    county = \"Matagorda\",\n    sumfile = \"dhc\",\n    year = 2020\n  )\n\n\n\nDemographic Profile\n\nPretabulated percentages from dhc\nsumfile = “dp”\n\nTabulations for 118th Congress and Island Areas (i.e. Congressional Districts)\n\nsumfile = “cd118”\n\n\nC suffix variables are counts while P suffix variables are percentages\n\n0.4 is 0.4% not 40%\n\nExample: Same-sex married and partnered in California by County\nca_samesex &lt;- \n  get_decennial(\n    geography = \"county\",\n    state = \"CA\",\n    variables = c(married = \"DP1_0116P\",\n                  partnered = \"DP1_0118P\"),\n    year = 2020,\n    sumfile = \"dp\",\n    output = \"wide\"\n  )\n\n\n\nDetailed DHC-A\n\nDetailed demographic data; Thousands of racial and ethnic groups; Tabulation by sex and age.\nDifferent groups are in different tables, so specific groups can be hard to locate.\nAdaptive design means the demographic group (i.e. variable) will only be available in certain areas. For privacy, data gets supressed when the area has low population.\n\nThere’s typically considerable sparsity especially when going down census tract\n\nArgs\n\nsumfile = “ddhca”\npop_group - Population group code (See get_pop_groups below)\n\n“all” for all groups\npop_group_label = TRUE - Adds group labels\n\n\nget_pop_groups(2020, \"ddhca\") - Gets group codes for ethnic groups\n\nFor various groups there could be at least two variables (e..g Somaili, Somali and any combination)\nFor time series analysis, analagous groups to 2020’s for 2000 is SF2/SF4 and for 2010 is SF2. (SF stands for Summary File)\n\ncheck_ddhca_groups - Checks which variables are available for a specific group\n\nExample: Somali\ncheck_ddhca_groups(\n  geography = \"county\", \n  pop_group = \"1325\", \n  state = \"MN\", \n  county = \"Hennepin\"\n)\n\nExample: Minnesota group populations\nload_variables(2020, \"ddhca\") %&gt;% \n  View()\nmn_population_groups &lt;- \n  get_decennial(\n    geography = \"state\",\n    variables = \"T01001_001N\", # total population\n    state = \"MN\",\n    year = 2020,\n    sumfile = \"ddhca\",\n    pop_group = \"all\", # for all groups\n    pop_group_label = TRUE\n  )\n\nIncludes aggregate categories like European Alone, Other White Alone, etc., so you can’t just aggregate the value column to get the total population in Minnesota.\n\nSo, in order to calculate ethnic group ratios of the total state or county, etc. population, you need to get those state/county totals from other tables (e.g. PL94-171)\n\n\nUse dot density and not chloropleths to visualize these sparse datasets\n\nExample: Somali populations by census tract in Minneapolis\n\nhennepin_somali &lt;- \n  get_decennial(\n    geography = \"tract\",\n    variables = \"T01001_001N\", # total population\n    state = \"MN\",\n    county = \"Hennepin\",\n    year = 2020,\n    sumfile = \"ddhca\",\n    pop_group = \"1325\", # somali\n    pop_group_label = TRUE,\n    geometry = TRUE\n  )\n\nsomali_dots &lt;- \n  as_dot_density(\n    hennepin_somali,\n    value = \"value\", # column name which is by default, \"value\"\n    values_per_dot = 25\n  )\n\nmapview(somali_dots, \n        cex = 0.01, \n        layer.name = \"Somali population&lt;br&gt;1 dot = 25 people\",\n        col.regions = \"navy\", \n        color = \"navy\")\n\nvalues_per_dot = 25 says make each dot worth 25 units (e.g. people or housing units)\n\n\n\n\n\nTime Series Analysis\n\n{tidycensus} only has 2010 and 2020 censuses\n\nSee https://nhgis.org for older census data\n\nIssue: county names and boundaries change over time (e.g. Alaska redraws a lot)\n\nCensus gives a different GeoID to counties that get renamed even though they’re the same county.\nNA values showing up after you calculate how the value changes over time is a good indication of this problem. Check for NAs: filter(county_change, is.na(value10))\n\nExample: Join 2010 and 2020 and Calculate Percent Change\ncounty_pop_10 &lt;- \n  get_decennial(\n    geography = \"county\",\n    variables = \"P001001\", \n    year = 2010,\n    sumfile = \"sf1\"\n  )\n\ncounty_pop_10_clean &lt;- \n  county_pop_10 %&gt;%\n    select(GEOID, value10 = value) \n\ncounty_pop_20 &lt;- \n  get_decennial(\n    geography = \"county\",\n    variables = \"P1_001N\",\n    year = 2020,\n    sumfile = \"dhc\"\n  ) %&gt;%\n    select(GEOID, NAME, value20 = value)\n\ncounty_joined &lt;- \n  county_pop_20 %&gt;%\n    left_join(county_pop_10_clean, by = \"GEOID\") \n\ncounty_joined\n\ncounty_change &lt;- \n  county_joined %&gt;%\n    mutate( \n      total_change = value20 - value10, \n      percent_change = 100 * (total_change / value10) \n    ) \nExample: Age distribution over time in Michigan\n\n\nCode available in the github repo or R/Workshops/tidycensus-umich-workshop-2024-main/census-2020/bonus-chart.R\nDistribution shape remains pretty much the same, but decreasing for most age cohorts, i.e. people are leaving the state across most age groups.\n\ne.g. The large hump representing the group of people in there mid-40s in 2000 steadily decreases over time.",
    "crumbs": [
      "Surveys",
      "Census Data"
    ]
  },
  {
    "objectID": "qmd/surveys-census-data.html#sec-surv-cens-dic-tidyc",
    "href": "qmd/surveys-census-data.html#sec-surv-cens-dic-tidyc",
    "title": "Census Data",
    "section": "tidycensus",
    "text": "tidycensus\n\nGet an API key\n\nRequest a key, then activate the key from the link in your email.(https://api.census.gov/data/key_signup.html)\nSet as an environment variable: census_api_key(\"&lt;api key&gt;\", install = TRUE)\n\nOr add this line to .Renviron file, CENSUS_API_KEY=‘&lt;api key’\n\n\nSearch Variables\n\nColumns\n\nName - ID of the variable (Use this in the survey functions)\nLabel - Detailed description of the variable\nContext - Subject of the table that the variable is located in.\n\nPrefixes (Variables can have combinations of prefixes)\n\nP: i.e. Person; Data available at the census block and larger\nCT: Data available at the census track and larger\nH: Data available at the Housing Unit level\n\nI think housing unit is an alternatve unit. So instead of the unit being a person, which I assume is the typical unit, it’s a housing unit (~family).\nNot affected by Differential Privacy (i.e. no noise added; true value)\nExample: Total Deleware housing units at census block level\ndp_households &lt;- \n      get_decennial(\n            geography = \"block\",\n            variables = \"H1_001N\",\n            state = \"DE\",\n            sumfile = \"dhc\",\n            year = 2020\n      )\n\n\nExample: DHC data in census for 2020\n\nvars &lt;- load_variables(2020, \"dhc\")\n\nView(vars)\n\nView table, click filter, and then search for parameters (e.g. Age, Median, etc.) with the Label, Context boxes, and overall search box\n\n\nsummary_var - Argument for supplying an additional variable that you need to calculate some kind of summary statistic\n\nExample: Race Percentage per Congressional District\n\nrace_vars &lt;- c(\n  Hispanic = \"P5_010N\", # all races identified as hispanic\n  White = \"P5_003N\", # white not hispanic\n  Black = \"P5_004N\", # black not hispanic\n  Native = \"P5_005N\", # native american not hispanic\n  Asian = \"P5_006N\", # asian not hispanic\n  HIPI = \"P5_007N\" # hawaiian, islander not hispanic\n)\n\ncd_race &lt;- \n  get_decennial(\n    geography = \"congressional district\",\n    variables = race_vars,\n    summary_var = \"P5_001N\", # total population for county\n    year = 2020,\n    sumfile = \"cd118\"\n)\n\ncd_race_percent &lt;- \n  cd_race %&gt;%\n    mutate(percent = 100 * (value / summary_value)) %&gt;% \n    select(NAME, variable, percent)\n\ngeometry = TRUE- Joins shapefile with data and returns a SF (Simple Features) dataframe for mapping\n\nMisc\n\nYou can create a discrete color palette with the at argument in the mapview function.\n\nExample\n# check min and max of your data to select range of bins\nmin(iowa_over_65, na.rm = TRUE) # 0\nmax(iowa_over_65, na.rm = TRUE) # 38.4\n\nm1 &lt;- \n  mapview(iowa_over_65, \n          zcol = \"value\",\n          layer.name = \"% age 65 and up&lt;br&gt;Census tracts in Iowa\",\n          col.regions = inferno(100, direction = -1),\n          at = c(0, 10, 20, 30, 40))\n\nThis will result in a discrete palette with bins of 0-10, 10-20, etc. Looks like an overlap, so I’m sure which bin contains the endpoints.\n\n\n\nExample: Over 65 in Iowa by census tract\n\nlibrary(mapviw); library(viridisLite)\n\niowa_over_65 &lt;- \n  get_decennial(\n    geography = \"tract\",\n    variables = \"DP1_0024P\",\n    state = \"IA\",\n    geometry = TRUE,\n    sumfile = \"dp\",\n    year = 2020\n  )\nm1 &lt;- \n  mapview(iowa_over_65, zcol = \"value\",\n          layer.name = \"% age 65 and up&lt;br&gt;Census tracts in Iowa\",\n          col.regions = inferno(100, direction = -1))\nExport as an HTML file\nhtmlwidgets::saveWidget(m1@map, \"iowa_over_65.html\")\n\nCan embed it elsewhere (html report or website) by adding it as an asset",
    "crumbs": [
      "Surveys",
      "Census Data"
    ]
  },
  {
    "objectID": "qmd/model-building-concepts.html",
    "href": "qmd/model-building-concepts.html",
    "title": "Concepts",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Model Building",
      "Concepts"
    ]
  },
  {
    "objectID": "qmd/model-building-concepts.html#sec-modbld-misc",
    "href": "qmd/model-building-concepts.html#sec-modbld-misc",
    "title": "Concepts",
    "section": "",
    "text": "Packages\n\n{multiverse} - makes it easy to specify and execute all combinations of reasonable analyses of a dataset\n\n\n\nPaper, Summary of it’s usage\nLots of vignettes\n\n\nRegression Workflow (Paper)\n\nMake ML model pipelines reusable and reproducible\n\n\nNotes from 7 Tips to Future-Proof Machine Learning Projects\nModularization - Useful for debugging and iteration\n\nDon’t used declarative programming. Create functions/classes for preprocessing, training, tuning, etc., and keep in separate files. You’ll call these functions in the main script\n\nHelper function\n## file preprocessing.py ##\ndef data_preparation(data):\n    data = data.drop(['Evaporation', 'Sunshine', 'Cloud3pm', 'Cloud9am'], axis=1)\n    numeric_cols = ['MinTemp', 'MaxTemp', 'Rainfall', 'WindGustSpeed', 'WindSpeed9am']\n    data[numeric_cols] = data[numeric_cols].fillna(data[numeric_cols].mean())\n    data['Month'] = pd.to_datetime(data['Date']).dt.month.apply(str)\n    return data\nMain script\nfrom preprocessing import data_preparation \ntrain_preprocessed = data_preparation(train_data)\ninference_preprocessed = data_preparation(inference_data)\n\nKeep parameters in a separate config file\n\nConfig file\n## parameters.py ##\nDROP_COLS = ['Evaporation', 'Sunshine', 'Cloud3pm', 'Cloud9am']\nNUM_COLS = ['MinTemp', 'MaxTemp', 'Rainfall', 'WindGustSpeed', 'WindSpeed9am']\nProprocessing script\n## preprocessing.py ##\nfrom parameters import DROP_COLS, NUM_COLS\ndef data_preparation(data):\n    data = data.drop(DROP_COLS, axis=1)\n    data[NUM_COLS] = data[NUM_COLS].fillna(data[NUM_COLS].mean())\n    data['Month'] = pd.to_datetime(data['Date']).dt.month.apply(str)\n    return data\n\n\nVersioning Code, Data, and Models - Useful for investigating drift\n\nSee tools like DVC, MLFlow, Weights and Biases, etc. for model and data versioning\n\nImportant to save data snapshots throughout the project lifecycle, for example: raw data, processed data, train data, validation data, test data and inference data.\n\nGithub and dbt for code versioning\n\nConsistent Structures - Consistency in project structures and naming can reduce human error, improve communication, and just make things easier to find.\n\nNaming examples:\n\n&lt;model-name&gt;-&lt;parameters&gt;-&lt;model-version&gt;\n&lt;model-name&gt;-&lt;data-version&gt;-&lt;use-case&gt;\n\nExample: Reduced project template based on {{cookiecutter}}\n├── data\n│   ├── output      &lt;- The output data from the model. \n│   ├── processed      &lt;- The final, canonical data sets for modeling.\n│   └── raw            &lt;- The original, immutable data dump.\n│\n├── models             &lt;- Trained and serialized models, model predictions, or model summaries\n│\n├── notebooks          &lt;- Jupyter notebooks. \n│\n├── reports            &lt;- Generated analysis as HTML, PDF, LaTeX, etc.\n│   └── figures        &lt;- Generated graphics and figures to be used in reporting\n│\n├── requirements.txt   &lt;- The requirements file for reproducing the analysis environment, e.g.\n│                         generated with `pip freeze &gt; requirements.txt`\n│\n├── code              &lt;- Source code for use in this project.\n    ├── __init__.py    &lt;- Makes src a Python module\n    │\n    ├── data           &lt;- Scripts to generate and process data\n    │   ├── data_preparation.py\n    │   └── data_preprocessing.py\n    │\n    ├── models         &lt;- Scripts to train models and then use trained models to make\n    │   │                 predictions\n    │   ├── inference_model.py\n    │   └── train_model.py\n    │\n    └── analysis  &lt;- Scripts to create exploratory and results oriented visualizations\n        └── analysis.py\n\n\nModel is performing well on the training set but much worse on the validation/test set\n\n\nAndrew Ng calls the validation set the “Dev Set” 🙄\nTest: Random sample the training set and use that as your validation set. Score your model on this new validation set\n\n“Train-Dev” is the sampled validation set\nPossibilities\n\nVariance: The data distribution of the training set is the same as the validation/test sets\n\n\nThe model has been overfit to the training data\n\nData Mismatch: The data distribution of the training set is NOT the same as the validation/test sets\n\n\nUnlucky and the split was bad\n\nSomething maybe is wrong with the splitting function\n\nSplit ratio needs adjusting. Validation set isn’t getting enough data to be representative.\n\n\n\n\nModel is performing well on the validation/test set but not in the real world\n\nInvestigate the validation/test set and figure out why it’s not reflecting real world data. Then, apply corrections to the dataset.\n\ne.g. distributions of your validation/tests sets should look like the real world data.\n\nChange the metric\n\nConsider weighting cases that your model is performing extremely poorly on.\n\n\nSplits\n\nHarrell: “not appropriate to split data into training and test sets unless n&gt;20,000 because of the luck (or bad luck) of the split.”\nIf your dataset is over 1M rows, then having a test set of 200K might be overkill (e.g. ratio of 60/20/20).\n\nMight be better to use a ratio of 98/1/1 for big data projects and 60/20/20 for smaller data projects\n\nlink\n\nShows that simple data splitting does not give valid confidence intervals (even asymptotically) when one refits the model on the whole dataset. Thus, if one wants valid confidence intervals for prediction error, we can only recommend either data splitting without refitting the model (which is viable when one has ample data), or nested CV.",
    "crumbs": [
      "Model Building",
      "Concepts"
    ]
  },
  {
    "objectID": "qmd/cli.html",
    "href": "qmd/cli.html",
    "title": "CLI",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "CLI"
    ]
  },
  {
    "objectID": "qmd/cli.html#sec-cli-misc",
    "href": "qmd/cli.html#sec-cli-misc",
    "title": "CLI",
    "section": "",
    "text": "Resources\n\nData Science at the Command Line\n\nctrl-rshell command history search\n\nMcFly - intelligent command history search engine that takes into account your working directory and the context of recently executed commands. McFly’s suggestions are prioritized in real time with a small neural network\n\nPath to a folder that’s above root folder:\n\n1 level up: ../desired-folder\n2 levels up: ../../desired-folder\n\nDebian vs. Ubuntu (from ChatGPT)\n\nStability vs. Freshness:\n\nDebian: Debian is known for its stability and reliability. It has a rigorous testing process and a conservative approach to updates, which makes it suitable for servers and systems where stability is crucial.\nUbuntu: Ubuntu is based on Debian but tends to be more up-to-date with software packages. It follows a time-based release cycle, with regular releases every six months. This can be appealing if you want access to the latest features and software.\n\nPackage Management:\n\nDebian: Debian uses the Debian Package Management System (dpkg) and Advanced Package Tool (APT) for package management. It has a vast repository of software packages.\nUbuntu: Ubuntu also uses dpkg and APT but adds its own software management tools like Snap and Ubuntu Software Center. This can make software installation more user-friendly.\n\nCommunity and Support:\n\nDebian: Debian has a large and dedicated community, and it’s known for its strong commitment to free and open-source software principles. It has a stable support structure, but community support may not be as user-friendly as Ubuntu’s.\nUbuntu: Ubuntu has a large and active community, and it offers both free and paid support options. The Ubuntu community is known for its user-friendliness and helpful forums, making it a good choice for beginners.\n\nVariants and Flavors:\n\nDebian: Debian offers different flavors, known as “Debian spins,” catering to various needs, such as Debian Stable, Debian Testing, and Debian Unstable. These variants differ in terms of software stability and freshness.\nUbuntu: Ubuntu has several official flavors (e.g., Ubuntu Desktop, Ubuntu Server, Kubuntu, Xubuntu) that come with different desktop environments. This variety allows users to choose an environment that suits their preferences.\n\nLicensing:\n\nDebian: Debian has a strict commitment to free and open-source software, prioritizing software that adheres to its Free Software Guidelines.\nUbuntu: While Ubuntu also includes mostly free and open-source software, it may include some proprietary drivers and software by default, which can be a concern for users who prioritize a completely open-source system.\n\nPerformance (Google Search AI)\n\nDebian is considered lightweight and much faster than Ubuntu. It comes with few pre-installed software.\n\nHardware (Google Search AI)\n\nDebian works well on older hardware. Debian still offers a 32-bit version of the distro, while Ubuntu no longer offers a 32-bit version.",
    "crumbs": [
      "CLI"
    ]
  },
  {
    "objectID": "qmd/cli.html#sec-cli-r",
    "href": "qmd/cli.html#sec-cli-r",
    "title": "CLI",
    "section": "R",
    "text": "R\n\nMake an R script pipeable (From link)\nparallel \"echo 'zipping bin {}'; cat chunked/*_bin_{}_*.csv | ./upload_as_rds.R '$S3_DEST'/chr_'$DESIRED_CHR'_bin_{}.rds\"\n#!/usr/bin/env Rscript\nlibrary(readr)\nlibrary(aws.s3)\n\n# Read first command line argument\ndata_destination &lt;- commandArgs(trailingOnly = TRUE)[1]\n\ndata_cols &lt;- list(SNP_Name = 'c', ...)\n\ns3saveRDS(\n  read_csv(\n        file(\"stdin\"), \n        col_names = names(data_cols),\n        col_types = data_cols \n    ),\n  object = data_destination\n)\n\nBy passing readr::read_csv the function, file(\"stdin\"), it loads the data piped to the R script into a dataframe, which then gets written as an .rds file directly to s3 using {aws.s3}.\n\nKilling a process\nsystem(\"taskkill /im java.exe /f\", intern=FALSE, ignore.stdout=FALSE)\nStarting a process in the background\n# start MLflow server\nsys::exec_background(\"mlflow server\")\nDelete an opened file in the same R session\n\nYou **MUST** unlink it before any kind of manipulation of object\n\nI think this works because readr loads files lazily by default\n\nExample:\nwisc_csv_filename &lt;- \"COVID-19_Historical_Data_by_County.csv\"\ndownload_location &lt;- file.path(Sys.getenv(\"USERPROFILE\"), \"Downloads\")\nwisc_file_path &lt;- file.path(download_location, wisc_csv_filename)\nwisc_tests_new &lt;- readr::read_csv(wisc_file_path)\n# key part, must unlink before any kind of code interaction\n# supposedly need recursive = TRUE for Windows, but I didn't need it\n# Throws an error (hence safely) but still works\nsafe_unlink &lt;- purrr::safely(unlink)\nsafe_unlink(wisc_tests_new)\n\n# manipulate obj\nwisc_tests_clean &lt;- wisc_tests_new %&gt;%\n      janitor::clean_names() %&gt;%\n      select(date, geo, county = name, negative, positive) %&gt;%\n      filter(geo == \"County\") %&gt;%\n      mutate(date = lubridate::as_date(date)) %&gt;%\n      select(-geo)\n# clean-up\nfs::file_delete(wisc_file_path)\n\nFind out which process is locking or using a file\n\nOpen Resource Monitor, which can be found\n\nBy searching for Resource Monitor or resmon.exe in the start menu, or\nAs a button on the Performance tab in your Task Manager\n\nGo to the CPU tab\nUse the search field in the Associated Handles section\n\ntype the name of file in the search field and it’ll search automatically\n35548",
    "crumbs": [
      "CLI"
    ]
  },
  {
    "objectID": "qmd/cli.html#sec-cli-awk",
    "href": "qmd/cli.html#sec-cli-awk",
    "title": "CLI",
    "section": "AWK",
    "text": "AWK\n\n\nMisc\n\nResources\n\nDocs\nAwk - A Tutorial and Introduction\n\n\nPrint first few rows of columns 1 and 2\nawk -F, '{print $1,$2}' adult_t.csv|head\nFilter lines where no of hours/ week (13th column) &gt; 98\nawk -F, ‘$13 &gt; 98’ adult_t.csv|head\nFilter lines with “Doctorate” and print first 3 columns\nawk '/Doctorate/{print $1, $2, $3}' adult_t.csv\nRandom sample 8% of the total lines from a .csv (keeps header)\n'BEGIN {srand()} !/^$/ {if(rand()&lt;=0.08||FNR==1) print &gt; \"rand.samp.csv\"}' big_fn.csv\nDecompresses, chunks, sorts, and writes back to S3 (From link)\n# Let S3 use as many threads as it wants\naws configure set default.s3.max_concurrent_requests 50\n\nfor chunk_file in $(aws s3 ls $DATA_LOC | awk '{print $4}' | grep 'chr'$DESIRED_CHR'.csv') ; do\n\n        aws s3 cp s3://$batch_loc$chunk_file - |\n        pigz -dc |\n        parallel --block 100M --pipe  \\\n        \"awk -F '\\t' '{print \\$1\\\",...\\\"$30\\\"&gt;\\\"chunked/{#}_chr\\\"\\$15\\\".csv\\\"}'\"\n\n        # Combine all the parallel process chunks to single files\n        ls chunked/ |\n        cut -d '_' -f 2 |\n        sort -u |\n        parallel 'cat chunked/*_{} | sort -k5 -n -S 80% -t, | aws s3 cp - '$s3_dest'/batch_'$batch_num'_{}'\n\n        # Clean up intermediate data\n        rm chunked/*\ndone\n\nUses pigz to parallelize decompression\nUses GNU Parallel (site, docs, tutorial1, tutorial2) to parallelize chunking (100MB chunks in 1st section)\nChunks data into smaller files and sorts them into directories based on a chromosome column (I think)\nAvoids writing to disk",
    "crumbs": [
      "CLI"
    ]
  },
  {
    "objectID": "qmd/cli.html#sec-cli-bash",
    "href": "qmd/cli.html#sec-cli-bash",
    "title": "CLI",
    "section": "Bash",
    "text": "Bash\n\nMisc\n\nNotes from\n\nBash for Data Scientists, Data Engineers & MLOps Engineers\n\nBunch of other stuff that I didn’t take notes on\n\nBash Scripting on Linux: The Complete Guide - video course\n\nResources\n\nBash Scripting Cheatsheet\nCurl Docs\n\nman &lt;command&gt; displays documentation for command\nSpecial Characters\n\n\n“&gt;” redirects the output from a program to a file.\n\n“&gt;&gt;” does the same thing, but it’s appending to an existing file instead of overwriting it, if it already exists.\n\n\n\n\n\nCommands\n\nBasic Commands\n\n\necho $SHELL - prints the type of shell you’re using\necho $PATH - prints all stored pathes\nexport PATH=\"my_new_path:$PATH\" - store a new path\nCommand Syntax: command -options arguments\nPiping Commands: cat user_names.txt|sort|uniq\n\n\n\nAliases\n\nCustom commands that you can define in order to avoid typing lengthy commands over and over again\nExamples\nalias ll=\"ls -lah\"\nalias gs=\"git status\"\nalias gp=\"git push origin master\"\nCreate safeguards for yourself\nalias mv=\"mv -i\"\n\nmv will automatically use the i flag, so the terminal will warn you if the file you’re about to move does already exist under the new directory,\n\nThis way you don’t accidentally overwrite files that you didn’t mean to overwrite.\n\n\n\n\n\nFiles/Directories\n\nList\n\n\nList 10 most recently modified files: ls -lt | head\nList files sorted by file size: ls -l -S\n\nCreate/Delete Directories\nmkdir &lt;dir_name&gt;\nrmdir &lt;dir_name&gt;\nOutput to file: echo “This is an example for redirect” &gt; file1.txt\nAppend line to file: echo “This is the second line of the file” &gt;&gt; file1.txt\nCreate/Delete file(s):\n# Create files\ntouch file1.txt\ntouch file1.txt file2.tx\n\n# Delete files\nrm file1.txt\nrm file1.txt file2.txt\nMove files/dir; Rename\n# Move single file\nmv my_file.txt /tmp\n# Move multiple files\nmv file1 file2 file3 /tmp\n# Move a directory or multiple directories\nmv d1 d2 d3 /tmp\n# Rename the file using move command\nmv my_file1.txt my_file_newname.txt\n\nFile(s) and directories being moved to “tmp” directory\n\nSearch\n\nFind\n# syntax find &lt;path&gt; &lt;expression&gt;\n# Find by name\nfind . -name “my_file.csv\"\n#Wildcard search\nfind . -name \"*.jpg\"\n# Find all the files in a folder\nfind /temp\n# Search only files\nfind /temp -type f\n# Search only directories\nfind /temp -type d\n# Find file modified in last 3 hours\nfind . -mmin -180\n# Find files modified in last 2 days\nfind . -mtime -2\n# Find files not modified in last 2 days\nfind . -mtime +2\n# Find the file by size\nfind -type f -size +10M\n\nLocate (faster)\n\nDocs\nInstall\nbash sudo apt install mlocate # Debian\nUsage\n\nsudo updatedb # update before using\nlocate .csv\nSplit files\n# default: 1000 lines per file, names of new files: xaa, xab, xac, etc.\nsplit my_file\n\n# add a prefix to new file names\nsplit my_file my_prefix\n\n# specify split threshold (e.g. 5000) by number of lines\nsplit --lines=5000 my_file\n\n# specify split threshold by size (e.g. 10MB)\nsplit --bytes=10 MB my_file\nPermissions\n\nls -l See list of files and the permissions\n-rwxrwxrwx - sytax of permissions for a folder or directory\n\n“rwx” stand for read, write, and execute rights, respectively\nThe 3 “rwx” blocks are for (1) user, (2) user group, and (3) everyone else.\n\nIn the given example, all 3 of these entities have read, write, as well as execute permissions.\n\nThe dash indicates that this is a file. Instead of the dash, you can also see a “d” for directory or “l” for a symbolic link.\n\nchmod - edit permissions\n\nExample: chmod u+x my_program.py - makes this file executable for yourself\n\nsudo - “super user” - using this prefix gives you all the permissions to all the files\n\nsudo su - opens a stand alone super user shell\n\n\n\n\n\nPrint\n\nPrint file content\ncat &lt; my_file.txt\n# or\ncat my_file.txt\nPrint 1 pg at a time: less my_file.txt\nPrint specific number of lines: head -n&lt;num_lines&gt; &lt;file.csv&gt;\nPrint file content from bottom to top: tac my_file.txt\ncat -b log.txt | grep error : shows all lines in log.txt that contain the string ‘error’, along with the line number (-b)\n\n\n\nLogicals and Conditionals\n\nLogicals\n\n; : command1 ; command2\n\ncommand 1 and command 2 run independently of each other\n\n& : command1 & command2\n\ncommand 1 runs in the background and command 2 runs in the background\n\n&& : command1 && command2\n\nIf the first command errors out then the second command is not executed\n\n|| : command1 || command2\n\nThe second commmand is only execute if the first command errors\n\nExample\ncd my_dir && pwd || echo “No such directory exist.Check”\n\nIf the my_dir exists, then the current working directory is printed. If the my_dir doesn’t exist, then the message “No such directory exists. check” message is printed.\n\n\nConditionals\n\nUse [[ ]] for conditions in if / while statements, instead of [ ] or test.\n\n[[ ]] is a bash builtin, and is more powerful than [ ] or test.\nExample: if [[ -n \"${TRACE-}\" ]]; then set -o xtrace; fi\n\n\n\n\n\nString Matching\n\nExample: Search for “error” and write to file\n#output to a file again\ncat file1 file2 file3 | grep error | cat &gt; error_file.txt\n#Append to the end\ncat file1 file2 file3 | grep error | cat &gt;&gt; error_file.txt\n\nPrints lines into grep which searches for “error” in each line. Lines with “error” get written to “error_file.txt”\n\nFilter lines\ngrep -i “Doctorate” adult_t.csv |grep -i “Husband”|grep -i “Black”|csvlook\n# -i, --ignore-case-Ignore  case  distinctions,  so that characters that differ only in case match each other.\n\nSelect all the candidates who have doctorates and a husband and race are Black\ncsvlook is pretty printing from csvkit package (see Big Data &gt;&gt; Larger Than Memory &gt;&gt; csvkit)\n\nCount how many rows fit the criteria\ngrep -i “Doctorate” adult_t.csv | wc -l\n\nCounts how many rows have “Doctorate”\n\n-wc is “word count”\n\n\n\n\n\n\nVariables\n\nLocal Variable:\n\nDeclared at the command prompt\nUse lower case for name\nAvailable only in the current shell\nNot accessible by child processes or programs\nAll user-defined variables are local variables\n\nEnvironment (global) variables:\n\nCreate with export command\nUse upper case for name\nAvailable to child processes\n\nDeclare local and environment variables then access via “$”\n# local\nev_car=’Tesla’\necho 'The ev car I like is' $ev_car\n\n# environment\nexport EV_CAR=’Tesla’\necho 'The ev car I like is' $EV_CAR\n\nNo spaces in variable assignment\n\nAlways quote variable accesses with double-quotes.\n\nOne place where it’s okay not to is on the left-hand-side of an [[ ]] condition. But even there I’d recommend quoting.\nWhen you need the unquoted behaviour, using bash arrays will likely serve you much better.\n\nFunctions\n\nUse local variables in functions.\nAccept multiple ways that users can ask for help and respond in kind.\n\nCheck if the first arg is -h or –help or help or just h or even -help, and in all these cases, print help text and exit.\n\nWhen printing error messages, please redirect to stderr.\n\nUse echo 'Something unexpected happened' &gt;&2 for this\n\n\n\n\n\nScripting\n\nUse the .sh (or .bash) extension for your script\nUse long options, where possible (like –silent instead of -s). These serve to document your commands explicitly.\nIf appropriate, change to the script’s directory close to the start of the script.\n\nAnd it’s usually always appropriate.\nUse cd \"$(dirname \"$0\")\", which works in most cases.\n\nUse shellcheck. Heed its warnings.\nShebang line\n\nContains the absolute path of the bash interpreter\n\nList paths to all shells: cat/etc/shells\n\nUse as the first line even if you don’t give executable permission to the script file.\nStarts with “#!” the states the path of the interpreter\nExample: #!/bin/bash\n\nInterpreter installed in directory “/bin”\n\nExample: #!/usr/bin/env bash\n\nCommands that should start your script\n\nUse set -o errexit\n\nSo that when a command fails, bash exits instead of continuing with the rest of the script.\n\nUse set -o nounset\n\nThis will make the script fail, when accessing an unset variable. Saves from horrible unintended consequences, with typos in variable names.\nWhen you want to access a variable that may or may not have been set, use \"${VARNAME-}\" instead of \"$VARNAME\", and you’re good.\n\nUse set -o pipefail\n\nThis will ensure that a pipeline command is treated as failed, even if one command in the pipeline fails.\n\nUse set -o xtrace, with a check on $TRACE env variable.\n\nFor copy-paste: if [[ -n \"${TRACE-}\" ]]; then set -o xtrace; fi.\nThis helps in debugging your scripts, a lot.\nPeople can now enable debug mode, by running your script as TRACE=1 ./script.sh instead of ./script.sh .\n\n\nExample: Basic Execution a Bash Script\n\nCreate a directory bash_script: mkdir bash_script\nCreate a hello_world.sh file: touch hello_script.sh\nOpen hello_script.sh (text editor?)\nAdd code, save, and close\n    #!/bin/bash\n    echo ‘Hello World’\nMake file executable: chmod +x hello_world.sh\nExecute file: ./hello_world.sh\n\nTemplate\n#!/usr/bin/env bash\nset -o errexit\nset -o nounset\nset -o pipefail\nif [[ -n \"${TRACE-}\" ]]; then\n    set -o xtrace\nfi\nif [[ \"$1\" =~ ^-*h(elp)?$ ]]; then\n    echo 'Usage: ./script.sh arg-one arg-two\nThis is an awesome bash script to make your life better.\n'\n    exit\nfi\ncd \"$(dirname \"$0\")\"\nmain() {\n    echo do awesome stuff\n}\nmain \"$@\"\n\n\n\nJob Management\n\nPrograms/Scripts will by default run in the foreground, and prevent you from doing anything else until the program is done.\nWhile program is running:\n\ncontrol+c - Will send a SIGINT (signal interrupt) signal to the program, which instructs the machine to interrupt the program immediately (unless the program has a way to handle these signals internally).\ncontrol+z - Will pause the program.\n\nAfter pausing the program can be continued either by bringing it to the foreground (fg), or by sending it to the backgroud (bg).\n\n\nExecute script to run in the background: python run.py &\njobs - shows all running jobs and process ids (PIDS)\nkill - sends signals to jobs running in the background\n\nkill -STOP %1 sends a STOP signal, pausing program 1.\nkill -KILL %1 sends a KILL signal, terminating program 1 permanently.\n\n\n\n\ntmux (‘terminal multiplexer’)\n\nEnables you to easily create new terminal sessions and navigate between them. This can be extremely useful, for example you can use one terminal to navigate your file system and another terminal to execute jobs.\nInstallation (if necessary): sudo apt install tmux\n\nTypically comes with the linux installation\n\nSessions\n\ntmux - starts an unnamed session\ntmux new -s moose creates new terminal session with name ‘moose’\ntmux ls - lists all running sessions\ntmux kill-session -t moose - kills session named “moose”\nexit - stops and quits the current session\nKill all sessions (various opinions on how to do this)\n\ntmux kill-session\ntmux kill-server\ntmux ls | grep : | cut -d. -f1 | awk '{print substr($1, 0, length($1)-1)}' | xargs kill\n\n\nAttach/Detach\n\nWhen you log out of a remote machine (either on purpose or accidentally), all of the programs that were actively running inside your shell are automatically terminated. On the other hand, if you run your programs inside a tmux shell, you can come simply detach the tmux window, log out, close your computer, and come back to that shell later as if you’ve never been logged out.\ntmux detach - detach current session\ncontrol+bthen pressd`: When you have multiple sesssions running, this will allow you to select the session to detach\nFrom inside bash and not inside a session\n\ntmux a : attach to latest created session\ntmux a -t moose : attach to session called ‘moose’\n\n\nPane Creation and Navigation\n\ncontrol+b then press ” (i.e. shift+’): add another terminal pane below\ncontrol+b then press % (i.e. shift+5) : add another terminal pane to the right\ncontrol+b then press → : move to the terminal pane on the right (similar for left, up, down)\n\n\n\n\nSSH\n\nTypically uses a key pair to log into remote machines\n\nKey pair consists of a public key (which both machines have access to) and a private key (which only your own machine has access to)\n“ssh-keygen” is a program for generating such a key pair.\n\nIf you run ssh-keygen, it will by default create a public key named “id_rsa.pub” and a private key named “id_rsa”, and place both into your “~/.ssh” directory\nYou’ll need to add the public key to the remote machine by piping together cat, ssh, and a streaming operator\n\ncat .ssh/id_rsa.pub | ssh user@remote 'cat &gt;&gt; ~/.ssh/authorized_keys'\n\n\n\nConnect to the remote machine: ssh remote -i ~/.ssh/id_rsa\nCreate a config file instead\n\nLocation: “~/.ssh/config”\nContents\nHost dev\n  HostName remote\n  IdentityFile ~/.ssh/id_rsa\n\nConnect using config: ssh dev\nFor Windows and using Putty, see\n\nAWS &gt;&gt; EC2 &gt;&gt; Connect to/ Terminate Instance\nProjects Notebook &gt;&gt; Article, Nested Cross Validation &gt;&gt; Notes &gt;&gt; Running EC2 instances checklist\n\n\n\n\nVim\n\nCommand-line based text editor\nCommon Usage\n\nLogging into a remote machine and need to make a code change there. vim is a standard program and therefore usually available on any machine you work on.\nWhen running git commit, by default git opens vim for writing a commit message. So at the very least you’ll want to know how to write, save, and close a file.\n\n2 modes: Navigation Mode; Edit Mode\n\nWhen Vim is launched you’re in Navigation mode\nPress i to start edit mode, in which you can make changes to the file.\nPress Esc key to leave edit mode and go back to navigation mode.\n\nCommands (Cheatsheet)\n\nx deletes a character\ndd deletes an entire row\nb (back) goes to the previous word\nn (next) goes to the next word\n:wq saves your changes and closes the file\n:q! ignores your changes and closes the file\n\n\n\n\nPackages\n\nCommon package managers: apt, Pacman, yum, and portage\nAPT (Advanced Package Tool)\n\nInstall Packages\n# one pkg\nsudo apt-get install &lt;package_name&gt;\n# multiple\nsudo apt-get install &lt;pkg_name1&gt; &lt;pkg_name2&gt;\n\nInstall but no upgrade: sudo apt-get install &lt;pkg_name&gt; --no-upgrade\n\nSearch for an installed package: apt-cache search &lt;pkg_name&gt;\nUpdate package information prior to “upgrading” the packages\nsudo apt-get update\n\nDownloads the package lists from the repositories and “updates” them to get information on the newest versions of packages and their dependencies.\n\nUpgrade\n# all installed packages\nsudo apt-get upgrade\n\n# To upgrade only a specific program\nsudo apt-get upgrade &lt;package_name&gt;\n\n# Upgrades and handles dependencies; delete obsolete, add new\napt-get dist-upgrade\n\n# together\nsudo apt-get update && sudo apt-get dist-upgrade\n\n\n\n\nExpressions\n\nSort data, filter only unique lines, and write to file: cat adult_t.csv | sort | uniq -c &gt; sorted_list.csv",
    "crumbs": [
      "CLI"
    ]
  },
  {
    "objectID": "qmd/cli.html#sec-cli-powsh",
    "href": "qmd/cli.html#sec-cli-powsh",
    "title": "CLI",
    "section": "Powershell",
    "text": "Powershell\n\nMisc\n\nDocs, Sample Scripts\nUse Get-Help &lt;cmdlet-name&gt; to access help information for specific cmdlets.\n\n$_ (Docs)\n\nAutomatic Variable; Alias for $PSItem\n\nDocs for automatic variables\n\nUseful for looping through objects in a directory. See “Change Name (or Extensions) of Multiple Files” for an example of usage\nGet the properties of an object\n\nExample: Using Get-ItemProperty (Docs)\nGet-ItemProperty scrapsheet.txt | Format-List\n\nProperty names will be on the left side of the output\n\nExample: Using Get-Member (Docs)\n Get-ChildItem *.txt | Select-Object -First 1 | Get-Member\n\nSelects the first text file in the directory and gets the Properties and a bunch of other stuff like methods which also can be used with $_\nDescriptions are awful for a lot of the stuff, but for the most part, you can guess what the property is.\n\n\nSome properties that can be used with $_\n\nFile System Objects:\n\n.Name: Returns the name component of a file\n.FullName: Returns the full path of the file.\n.Length: Returns the size of the file in bytes.\n.CreationTime: Returns the date and time the file was created.\n.LastWriteTime: Returns the date and time the file was last modified.\n.IsReadOnly: Returns $true if the file is read-only, $false otherwise.\n\nProcess Objects:\n\n.Id: Returns the unique identifier (PID) of the process.\n.Name: Returns the name of the process.\n.MainWindowTitle: Returns the title of the process’s main window (if applicable).\n.WorkingDirectory: Returns the working directory of the process.\n.CPU: Returns the CPU usage of the process.\n\nRegistry Objects:\n\n.Name: Returns the name of the registry key.\n.Hive: Returns the hive of the registry key (e.g., HKLM, HKCU).\n.Value: Returns the value of the registry key (if applicable).\n.Path: Returns the full path of the registry key.\n\n\n\nComments: &lt;# comment #&gt;\nChange directories\n Set-Location \"Documents\\R\\Projects\"\nCreate a New Folder\n New-Item -ItemType Directory -Path \"Folder Name\"\n\nAssumes you’re already in the directory that you want the folder in. You can also use a path, e.g. \"C:\\Temp\\Documents\\New Folder\\Subfolder1\\\\Subfolder2\".\n\nChange Name of File\nRename-Item -Path \"c:\\logfiles\\daily_file.txt\" -NewName \"monday_file.txt\"\nChange Name (or Extensions) of Multiple Files\nGet-ChildItem *.md | Rename-Item -NewName { $_.Name -replace '.md','.qmd' }\n\nGet-ChildItem: cmdlet gets all the files in the current folder that have a .mc file extension\nResults piped to Rename-Item\n\nNewName: Has a value that’s a script block that runs before the value is submitted to the NewName parameter.\n\n$_: (automatic variable) It represents each file object as it comes to the command through the pipeline.\n-replace: Operator replaces the file extension of each file with .qmd.\n\nNotice that matching using the -replace operator is not case sensitive.\n\n\n\n\nExecute a File\nInvoke-Item configuration.cmd\nMulti-line Commands\nffmpeg -i input.mkv -map 0:v:0 `\n       -map 0:a:2 -map 0:a:0 -map 0:a:1 -map 0:a:3 `\n       -map 0:s -c copy `\n       -disposition:a:0 default `\n       reordered.mkv\n\nIn bash, it’s a backslash (\\), but in Powershell, it’s a backtick ( ` )\n*Don’t forget that there’s a space between the last character and the backtick.*\nIn practice, this will look like\nffmpeg -i .input.mkv -map 0:v:0 `\n&gt;&gt; -map 0:a:2 -map 0:a:0 -map 0:a:1 -map 0:a:3 `\n&gt;&gt; -map 0:s -c copy `\n&gt;&gt; -disposition:a:0 default `\n&gt;&gt; reordered.mkv\n\nString Matching\n\nPrint line with pattern\nSelect-String -Path \"file*.txt\" -Pattern \"error\"\nfile1.txt:3:This is the error line of the file\nfile2.txt:3:This is the error line of the file\nfile3.txt:3:This is the error line of the file\n\nMatches the 3rd line of each file\n\n\nGet stats on a process\nGet-Process -Name chrome\n\nHandles: The number of handles that the process has opened.\nNPM(K): The amount of non-paged memory that the process is using, in kilobytes.\nPM(K): The amount of pageable memory that the process is using, in kilobytes.\nWS(K): The size of the working set of the process, in kilobytes. The working set consists of the pages of memory that were recently referenced by the process.\nVM(M): The amount of virtual memory that the process is using, in megabytes. Virtual memory includes storage in the paging files on disk.\nCPU(s): The amount of processor time that the process has used on all processors, in seconds.\nID: The process ID (PID) of the process.\nProcessName: The name of the process. For explanations of the concepts related to processes, see the Glossary in Help and Support Center and the Help for Task Manager.\n\nEnvironment Variables\n\nSet an environment variable\nSet-Item -Name PYTHONSTARTUP -Value C:\\path\\to\\pythonstartup.py\n\nSame expression to modify existing environment variable\nOr\n$env:QUARTO_DENO_EXTRA_OPTIONS = \"--v8-flags=--max-old-space-size=8192\"\n\nDelete environment variable\nRemove-Item -Name &lt;variable_name&gt;\nVerify value of an environment variable\n$env:&lt;variable_name&gt;\n\nPorts\n\nFind application using a port.\nnetstat -aon | findstr ':80'\nnetstat -anp | find \":80\"\n\nIf port 80 is being used by the application, it will return a PID. Then you can find it in Task Manager &gt;&gt; Processess\n\nList all Listening and Established ports\nnetstat -anob\nCheck for processes using a port\nGet-Process -Id (Get-NetTCPConnection -LocalPort 80).OwningProcess\nTest connection to local port to see if it’s open\nTest-NetConnection -ComputerName localhost -Port 80 | Select-Object TcpTestSucceeded\nCheck firewall settings for an app\nnetsh advfirewall firewall show rule name=\"name_of_app\"",
    "crumbs": [
      "CLI"
    ]
  },
  {
    "objectID": "qmd/cli.html#sec-cli-batscri",
    "href": "qmd/cli.html#sec-cli-batscri",
    "title": "CLI",
    "section": "Batch Scripting",
    "text": "Batch Scripting\n\nMisc\n\nResources\n\nWindows Batch Scripting\n\nTo keep the prompt window open after script execution, place these either of these commands at end of your script.\n\npause: Keeps window open until you press any key.\nVia timer: e.g. timeout /t 300\ncmd /k: The prompt will remain active and you can execute additional commands manually.\n\n\nExample: Create variables and execute\n@echo off\n\nrem Set the path to the Rscript executable\nset RSCRIPT=\"C:\\Users\\user\\AppData\\Local\\Programs\\R\\R-4.2.3\\bin\\Rscript.exe\"\n\nrem Set the path to the R script to execute\nset RSCRIPT_FILE=\"C:\\Users\\user\\my_r_script.R\"\n\nrem Execute the R script\n%RSCRIPT% %RSCRIPT_FILE%\n\nrem Pause so the user can see the output\nexit\n\n@echo off - This line turns off the echoing of commands in the command prompt window, making the output cleaner.\nrem - Keyword that denotes a comment in a batch file.\nset RSCRIPT= - This line assigns the path to the Rscript executable to the environment variable RSCRIPT.\nset RSCRIPT_FILE= - The path to the R script file is assigned to the environment variable RSCRIPT_FILE.\n%RSCRIPT% %RSCRIPT_FILE% - Executes the R script using the Rscript executable and passes the path to the R script file as an argument.\nexit - This command exits the batch file and closes the command prompt window.\n\nExample: Exit if script errors\nRscript \"C:\\Users\\ercbk\\Documents\\R\\Projects\\Indiana-COVID-19-Tracker\\R\\collection\\build-opentab-dat.R\"\n\nREM if the data building script errors, bat script terminates without running other scripts or commands\nif %errorlevel% neq 0 exit /b %errorlevel%\n\ncd \"C:\\Users\\ercbk\\Documents\\R\\Projects\\Indiana-COVID-19-Tracker\"\n\ngit add data/YoY_Seated_Diner_Data.csv\ngit commit -m \"opentab data update\"\ngit pull\ngit push\n\nEXIT",
    "crumbs": [
      "CLI"
    ]
  },
  {
    "objectID": "qmd/cli.html#sec-cli-wsl",
    "href": "qmd/cli.html#sec-cli-wsl",
    "title": "CLI",
    "section": "WSL",
    "text": "WSL\n\nResources\n\nDocs\nTo update password (link) using username\n\nLoad Linux: wsl -d Ubuntu-22.04 where -d is for –distribution\nWSL Help: wsl --help\nExit linux terminal back to command prompt or powershell: exit",
    "crumbs": [
      "CLI"
    ]
  },
  {
    "objectID": "qmd/json.html",
    "href": "qmd/json.html",
    "title": "JSON",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "JSON"
    ]
  },
  {
    "objectID": "qmd/json.html#sec-json-misc",
    "href": "qmd/json.html#sec-json-misc",
    "title": "JSON",
    "section": "",
    "text": "Packages\n\n{yyjsonr} - A fast JSON parser/serializer, which converts R data to/from JSON and NDJSON. It is around 2x to 10x faster than jsonlite at both reading and writing JSON.\n{RcppSimdJson} - Comparable to {yyjsonr} in performance.\n\nAlso see\n\nBig Data &gt;&gt; Larger than Memory\nSQL &gt;&gt; Processing Expressions &gt;&gt; Nested Data\nDatabases &gt;&gt; DuckDB &gt;&gt; Misc\n\nhrbmstr recommends trying duckdb before using the cli tools in “Big Data”\n\n\nTools\n\n{listviewer}: Allows you to interactively explore and edit json files through the Viewer in the IDE. Docs show how it can be embedded into a Shiny app as well.\n\nExample\nlibrary(listviewer)\nmoose &lt;- jsonlite::read_json(\"path/to/file.json\")\njsonedit(moose)\nreactjson(moose)\n\nI’ve also used this a .config file which looked like a json file when I opened in a text editor, so this seems to work on anything json-like.\nreactjson has a copy button which is nice so that you can paste your edited version into a file.\njsonedit seems like it has more features, but I didn’t see a copy button. But there’s a view in which you can manually select everything a copy it via keyboard shortcut.",
    "crumbs": [
      "JSON"
    ]
  },
  {
    "objectID": "qmd/json.html#sec-json-jsonlite",
    "href": "qmd/json.html#sec-json-jsonlite",
    "title": "JSON",
    "section": "{jsonlite}",
    "text": "{jsonlite}\n\nRead",
    "crumbs": [
      "JSON"
    ]
  },
  {
    "objectID": "qmd/json.html#sec-json-py",
    "href": "qmd/json.html#sec-json-py",
    "title": "JSON",
    "section": "Python",
    "text": "Python\n\nExample: Parse Nested JSON into a dataframe (article)\n\nRaw JSON\n\n\n“entry” has the data we want\n“…” at the end indicates there are multiple objectss inside the element, “entry”\n\nProbably other root elements other than “feed” as well\n\n\nRead a json file from a URL using {{requests}} and convert to list\n\nimport requests\n\nurl = \"https://itunes.apple.com/gb/rss/customerreviews/id=1500780518/sortBy=mostRecent/json\"\n\nr = requests.get(url)\n\ndata = r.json()\nentries = data[\"feed\"][\"entry\"]\n\nIt looks like the list conversion also ordered the elements alphabetically\nThe output list is subsetted by the root element “feed” and the child element “entry”\n\nGet a feel for the final structure you want by hardcoding elements into a df\nparsed_data = defaultdict(list)\n\nfor entry in entries:\n    parsed_data[\"author_uri\"].append(entry[\"author\"][\"uri\"][\"label\"])\n    parsed_data[\"author_name\"].append(entry[\"author\"][\"name\"][\"label\"])\n    parsed_data[\"author_label\"].append(entry[\"author\"][\"label\"])\n    parsed_data[\"content_label\"].append(entry[\"content\"][\"label\"])\n    parsed_data[\"content_attributes_type\"].append(entry[\"content\"][\"attributes\"][\"type\"])\n    ... \nGeneralize extracting the properties of each object in “entry” with a nested loop\nparsed_data = defaultdict(list)\n\nfor entry in entries:\n    for key, val in entry.items():\n        for subkey, subval in val.items():\n            if not isinstance(subval, dict):\n                parsed_data[f\"{key}_{subkey}\"].append(subval)\n            else:\n                for att_key, att_val in subval.items():\n                    parsed_data[f\"{key}_{subkey}_{att_key}\"].append(att_val)\n\ndefaultdict creates a key from a list element (e.g. “author”) and groups the properties into a list of values where the value may also be a dict.\n\nSee Python, General &gt;&gt; Types &gt;&gt; Dictionaries\n\nFor each item in “entry”, it looks at the first key-value pair knowing that value is always a dictionary (object in JSON)\nThen handles two different cases\n\nFirst Case: The value dictionary is flat and does not contain another dictionary, only key-value pairs.\n\nCombine the outer key with the inner key to a column name and take the value as column value for each pair.\n\nSecond Case: Dictionary contains a key-value pair where the value is again a dictionary.\n\nAssumes at most two levels of nested dictionaries\nIterates over the key-value pairs of the inner dictionary and again combines the outer key and the most inner key to a column name and take the inner value as column value.\n\n\n\nRecursive function that handles json elements with deeper structures\n\ndef recursive_parser(entry: dict, data_dict: dict, col_name: str = \"\") -&gt; dict:\n    \"\"\"Recursive parser for a list of nested JSON objects\n\n    Args:\n        entry (dict): A dictionary representing a single entry (row) of the final data frame.\n        data_dict (dict): Accumulator holding the current parsed data.\n        col_name (str): Accumulator holding the current column name. Defaults to empty string.\n    \"\"\"\n    for key, val in entry.items():\n        extended_col_name = f\"{col_name}_{key}\" if col_name else key\n        if isinstance(val, dict):\n            recursive_parser(entry[key], data_dict, extended_col_name)\n        else:\n            data_dict[extended_col_name].append(val)\n\nparsed_data = defaultdict(list)\n\nfor entry in entries:\n    recursive_parser(entry, parsed_data, \"\")\n\ndf = pd.DataFrame(parsed_data)\n\nNotice the check for a deeper structure with isinstance. If there is one, then the function is called again.\nFunction outputs a dict which is coerced into dataframe\nTo get rid of “label” in column names: df.columns = [col if not \"label\" in col else \"_\".join(col.split(\"_\")[:-1]) for col in df.columns]\nobject types can be cast into more efficient types: df[\"im:rating\"] = df[\"im:rating\"].astype(int)",
    "crumbs": [
      "JSON"
    ]
  },
  {
    "objectID": "qmd/cli-linux.html",
    "href": "qmd/cli-linux.html",
    "title": "Linux",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "CLI",
      "Linux"
    ]
  },
  {
    "objectID": "qmd/cli-linux.html#sec-cli-lin-misc",
    "href": "qmd/cli-linux.html#sec-cli-lin-misc",
    "title": "Linux",
    "section": "",
    "text": "Notes from\n\nBash for Data Scientists, Data Engineers & MLOps Engineers\n\nBunch of other stuff that I didn’t take notes on\n\nBash Scripting on Linux: The Complete Guide - video course\n\nResources\n\nBash Scripting Cheatsheet\nCurl Docs\n\nman &lt;command&gt; displays documentation for command\nSpecial Characters\n\n\n“&gt;” redirects the output from a program to a file.\n\n“&gt;&gt;” does the same thing, but it’s appending to an existing file instead of overwriting it, if it already exists.\n\n\nDebian vs. Ubuntu (from ChatGPT)\n\nStability vs. Freshness:\n\nDebian: Debian is known for its stability and reliability. It has a rigorous testing process and a conservative approach to updates, which makes it suitable for servers and systems where stability is crucial.\nUbuntu: Ubuntu is based on Debian but tends to be more up-to-date with software packages. It follows a time-based release cycle, with regular releases every six months. This can be appealing if you want access to the latest features and software.\n\nPackage Management:\n\nDebian: Debian uses the Debian Package Management System (dpkg) and Advanced Package Tool (APT) for package management. It has a vast repository of software packages.\nUbuntu: Ubuntu also uses dpkg and APT but adds its own software management tools like Snap and Ubuntu Software Center. This can make software installation more user-friendly.\n\nCommunity and Support:\n\nDebian: Debian has a large and dedicated community, and it’s known for its strong commitment to free and open-source software principles. It has a stable support structure, but community support may not be as user-friendly as Ubuntu’s.\nUbuntu: Ubuntu has a large and active community, and it offers both free and paid support options. The Ubuntu community is known for its user-friendliness and helpful forums, making it a good choice for beginners.\n\nVariants and Flavors:\n\nDebian: Debian offers different flavors, known as “Debian spins,” catering to various needs, such as Debian Stable, Debian Testing, and Debian Unstable. These variants differ in terms of software stability and freshness.\nUbuntu: Ubuntu has several official flavors (e.g., Ubuntu Desktop, Ubuntu Server, Kubuntu, Xubuntu) that come with different desktop environments. This variety allows users to choose an environment that suits their preferences.\n\nLicensing:\n\nDebian: Debian has a strict commitment to free and open-source software, prioritizing software that adheres to its Free Software Guidelines.\nUbuntu: While Ubuntu also includes mostly free and open-source software, it may include some proprietary drivers and software by default, which can be a concern for users who prioritize a completely open-source system.\n\nPerformance (Google Search AI)\n\nDebian is considered lightweight and much faster than Ubuntu. It comes with few pre-installed software.\n\nHardware (Google Search AI)\n\nDebian works well on older hardware. Debian still offers a 32-bit version of the distro, while Ubuntu no longer offers a 32-bit version.",
    "crumbs": [
      "CLI",
      "Linux"
    ]
  },
  {
    "objectID": "qmd/cli-linux.html#sec-cli-lin-com",
    "href": "qmd/cli-linux.html#sec-cli-lin-com",
    "title": "Linux",
    "section": "Commands",
    "text": "Commands\n\nBasic Commands\n\n\necho $SHELL - prints the type of shell you’re using\necho $PATH - prints all stored pathes\nexport PATH=\"my_new_path:$PATH\" - store a new path\nCommand Syntax: command -options arguments\nPiping Commands: cat user_names.txt|sort|uniq\n\n\n\nAliases\n\nCustom commands that you can define in order to avoid typing lengthy commands over and over again\nExamples\nalias ll=\"ls -lah\"\nalias gs=\"git status\"\nalias gp=\"git push origin master\"\nCreate safeguards for yourself\nalias mv=\"mv -i\"\n\nmv will automatically use the i flag, so the terminal will warn you if the file you’re about to move does already exist under the new directory,\n\nThis way you don’t accidentally overwrite files that you didn’t mean to overwrite.\n\n\n\n\n\nFiles/Directories\n\nList\n\n\nList 10 most recently modified files: ls -lt | head\nList files sorted by file size: ls -l -S\n\nCreate/Delete Directories\nmkdir &lt;dir_name&gt;\nrmdir &lt;dir_name&gt;\nOutput to file: echo “This is an example for redirect” &gt; file1.txt\nAppend line to file: echo “This is the second line of the file” &gt;&gt; file1.txt\nCreate/Delete file(s):\n# Create files\ntouch file1.txt\ntouch file1.txt file2.tx\n\n# Delete files\nrm file1.txt\nrm file1.txt file2.txt\nMove files/dir; Rename\n# Move single file\nmv my_file.txt /tmp\n# Move multiple files\nmv file1 file2 file3 /tmp\n# Move a directory or multiple directories\nmv d1 d2 d3 /tmp\n# Rename the file using move command\nmv my_file1.txt my_file_newname.txt\n\nFile(s) and directories being moved to “tmp” directory\n\nSearch\n\nFind\n# syntax find &lt;path&gt; &lt;expression&gt;\n# Find by name\nfind . -name “my_file.csv\"\n#Wildcard search\nfind . -name \"*.jpg\"\n# Find all the files in a folder\nfind /temp\n# Search only files\nfind /temp -type f\n# Search only directories\nfind /temp -type d\n# Find file modified in last 3 hours\nfind . -mmin -180\n# Find files modified in last 2 days\nfind . -mtime -2\n# Find files not modified in last 2 days\nfind . -mtime +2\n# Find the file by size\nfind -type f -size +10M\n\nLocate (faster)\n\nDocs\nInstall\nbash sudo apt install mlocate # Debian\nUsage\n\nsudo updatedb # update before using\nlocate .csv\nSplit files\n# default: 1000 lines per file, names of new files: xaa, xab, xac, etc.\nsplit my_file\n\n# add a prefix to new file names\nsplit my_file my_prefix\n\n# specify split threshold (e.g. 5000) by number of lines\nsplit --lines=5000 my_file\n\n# specify split threshold by size (e.g. 10MB)\nsplit --bytes=10 MB my_file\nPermissions\n\nls -l See list of files and the permissions\n-rwxrwxrwx - sytax of permissions for a folder or directory\n\n“rwx” stand for read, write, and execute rights, respectively\nThe 3 “rwx” blocks are for (1) user, (2) user group, and (3) everyone else.\n\nIn the given example, all 3 of these entities have read, write, as well as execute permissions.\n\nThe dash indicates that this is a file. Instead of the dash, you can also see a “d” for directory or “l” for a symbolic link.\n\nchmod - edit permissions\n\nExample: chmod u+x my_program.py - makes this file executable for yourself\n\nsudo - “super user” - using this prefix gives you all the permissions to all the files\n\nsudo su - opens a stand alone super user shell\n\n\n\n\n\nPrint\n\nPrint file content\ncat &lt; my_file.txt\n# or\ncat my_file.txt\nPrint 1 pg at a time: less my_file.txt\nPrint specific number of lines: head -n&lt;num_lines&gt; &lt;file.csv&gt;\nPrint file content from bottom to top: tac my_file.txt\ncat -b log.txt | grep error : shows all lines in log.txt that contain the string ‘error’, along with the line number (-b)\n\n\n\nLogicals and Conditionals\n\nLogicals\n\n; : command1 ; command2\n\ncommand 1 and command 2 run independently of each other\n\n& : command1 & command2\n\ncommand 1 runs in the background and command 2 runs in the background\n\n&& : command1 && command2\n\nIf the first command errors out then the second command is not executed\n\n|| : command1 || command2\n\nThe second commmand is only execute if the first command errors\n\nExample\ncd my_dir && pwd || echo “No such directory exist.Check”\n\nIf the my_dir exists, then the current working directory is printed. If the my_dir doesn’t exist, then the message “No such directory exists. check” message is printed.\n\n\nConditionals\n\nUse [[ ]] for conditions in if / while statements, instead of [ ] or test.\n\n[[ ]] is a bash builtin, and is more powerful than [ ] or test.\nExample: if [[ -n \"${TRACE-}\" ]]; then set -o xtrace; fi\n\n\n\n\n\nString Matching\n\nExample: Search for “error” and write to file\n#output to a file again\ncat file1 file2 file3 | grep error | cat &gt; error_file.txt\n#Append to the end\ncat file1 file2 file3 | grep error | cat &gt;&gt; error_file.txt\n\nPrints lines into grep which searches for “error” in each line. Lines with “error” get written to “error_file.txt”\n\nFilter lines\ngrep -i “Doctorate” adult_t.csv |grep -i “Husband”|grep -i “Black”|csvlook\n# -i, --ignore-case-Ignore  case  distinctions,  so that characters that differ only in case match each other.\n\nSelect all the candidates who have doctorates and a husband and race are Black\ncsvlook is pretty printing from csvkit package (see Big Data &gt;&gt; Larger Than Memory &gt;&gt; csvkit)\n\nCount how many rows fit the criteria\ngrep -i “Doctorate” adult_t.csv | wc -l\n\nCounts how many rows have “Doctorate”\n\n-wc is “word count”",
    "crumbs": [
      "CLI",
      "Linux"
    ]
  },
  {
    "objectID": "qmd/cli-linux.html#sec-cli-lin-var",
    "href": "qmd/cli-linux.html#sec-cli-lin-var",
    "title": "Linux",
    "section": "Variables",
    "text": "Variables\n\nLocal Variable:\n\nDeclared at the command prompt\nUse lower case for name\nAvailable only in the current shell\nNot accessible by child processes or programs\nAll user-defined variables are local variables\n\nEnvironment (global) variables:\n\nCreate with export command\nUse upper case for name\nAvailable to child processes\n\nDeclare local and environment variables then access via “$”\n# local\nev_car=’Tesla’\necho 'The ev car I like is' $ev_car\n\n# environment\nexport EV_CAR=’Tesla’\necho 'The ev car I like is' $EV_CAR\n\nNo spaces in variable assignment\n\nAlways quote variable accesses with double-quotes.\n\nOne place where it’s okay not to is on the left-hand-side of an [[ ]] condition. But even there I’d recommend quoting.\nWhen you need the unquoted behaviour, using bash arrays will likely serve you much better.\n\nFunctions\n\nUse local variables in functions.\nAccept multiple ways that users can ask for help and respond in kind.\n\nCheck if the first arg is -h or –help or help or just h or even -help, and in all these cases, print help text and exit.\n\nWhen printing error messages, please redirect to stderr.\n\nUse echo 'Something unexpected happened' &gt;&2 for this",
    "crumbs": [
      "CLI",
      "Linux"
    ]
  },
  {
    "objectID": "qmd/cli-linux.html#sec-cli-lin-script",
    "href": "qmd/cli-linux.html#sec-cli-lin-script",
    "title": "Linux",
    "section": "Scripting",
    "text": "Scripting\n\nUse the .sh (or .bash) extension for your script\nUse long options, where possible (like –silent instead of -s). These serve to document your commands explicitly.\nIf appropriate, change to the script’s directory close to the start of the script.\n\nAnd it’s usually always appropriate.\nUse cd \"$(dirname \"$0\")\", which works in most cases.\n\nUse shellcheck. Heed its warnings.\nShebang line\n\nContains the absolute path of the bash interpreter\n\nList paths to all shells: cat/etc/shells\n\nUse as the first line even if you don’t give executable permission to the script file.\nStarts with “#!” the states the path of the interpreter\nExample: #!/bin/bash\n\nInterpreter installed in directory “/bin”\n\nExample: #!/usr/bin/env bash\n\nCommands that should start your script\n\nUse set -o errexit\n\nSo that when a command fails, bash exits instead of continuing with the rest of the script.\n\nUse set -o nounset\n\nThis will make the script fail, when accessing an unset variable. Saves from horrible unintended consequences, with typos in variable names.\nWhen you want to access a variable that may or may not have been set, use \"${VARNAME-}\" instead of \"$VARNAME\", and you’re good.\n\nUse set -o pipefail\n\nThis will ensure that a pipeline command is treated as failed, even if one command in the pipeline fails.\n\nUse set -o xtrace, with a check on $TRACE env variable.\n\nFor copy-paste: if [[ -n \"${TRACE-}\" ]]; then set -o xtrace; fi.\nThis helps in debugging your scripts, a lot.\nPeople can now enable debug mode, by running your script as TRACE=1 ./script.sh instead of ./script.sh .\n\n\nExample: Basic Execution a Bash Script\n\nCreate a directory bash_script: mkdir bash_script\nCreate a hello_world.sh file: touch hello_script.sh\nOpen hello_script.sh (text editor?)\nAdd code, save, and close\n    #!/bin/bash\n    echo ‘Hello World’\nMake file executable: chmod +x hello_world.sh\nExecute file: ./hello_world.sh\n\nTemplate\n#!/usr/bin/env bash\nset -o errexit\nset -o nounset\nset -o pipefail\nif [[ -n \"${TRACE-}\" ]]; then\n    set -o xtrace\nfi\nif [[ \"$1\" =~ ^-*h(elp)?$ ]]; then\n    echo 'Usage: ./script.sh arg-one arg-two\nThis is an awesome bash script to make your life better.\n'\n    exit\nfi\ncd \"$(dirname \"$0\")\"\nmain() {\n    echo do awesome stuff\n}\nmain \"$@\"",
    "crumbs": [
      "CLI",
      "Linux"
    ]
  },
  {
    "objectID": "qmd/cli-linux.html#sec-cli-lin-jobm",
    "href": "qmd/cli-linux.html#sec-cli-lin-jobm",
    "title": "Linux",
    "section": "Job Management",
    "text": "Job Management\n\nPrograms/Scripts will by default run in the foreground, and prevent you from doing anything else until the program is done.\nWhile program is running:\n\ncontrol+c - Will send a SIGINT (signal interrupt) signal to the program, which instructs the machine to interrupt the program immediately (unless the program has a way to handle these signals internally).\ncontrol+z - Will pause the program.\n\nAfter pausing the program can be continued either by bringing it to the foreground (fg), or by sending it to the backgroud (bg).\n\n\nExecute script to run in the background: python run.py &\njobs - shows all running jobs and process ids (PIDS)\nkill - sends signals to jobs running in the background\n\nkill -STOP %1 sends a STOP signal, pausing program 1.\nkill -KILL %1 sends a KILL signal, terminating program 1 permanently.",
    "crumbs": [
      "CLI",
      "Linux"
    ]
  },
  {
    "objectID": "qmd/cli-linux.html#sec-cli-lin-tmux",
    "href": "qmd/cli-linux.html#sec-cli-lin-tmux",
    "title": "Linux",
    "section": "tmux (terminal multiplexer)",
    "text": "tmux (terminal multiplexer)\n\nEnables you to easily create new terminal sessions and navigate between them. This can be extremely useful, for example you can use one terminal to navigate your file system and another terminal to execute jobs.\nInstallation (if necessary): sudo apt install tmux\n\nTypically comes with the linux installation\n\nSessions\n\ntmux - starts an unnamed session\ntmux new -s moose creates new terminal session with name ‘moose’\ntmux ls - lists all running sessions\ntmux kill-session -t moose - kills session named “moose”\nexit - stops and quits the current session\nKill all sessions (various opinions on how to do this)\n\ntmux kill-session\ntmux kill-server\ntmux ls | grep : | cut -d. -f1 | awk '{print substr($1, 0, length($1)-1)}' | xargs kill\n\n\nAttach/Detach\n\nWhen you log out of a remote machine (either on purpose or accidentally), all of the programs that were actively running inside your shell are automatically terminated. On the other hand, if you run your programs inside a tmux shell, you can come simply detach the tmux window, log out, close your computer, and come back to that shell later as if you’ve never been logged out.\ntmux detach - detach current session\ncontrol+bthen pressd`: When you have multiple sesssions running, this will allow you to select the session to detach\nFrom inside bash and not inside a session\n\ntmux a : attach to latest created session\ntmux a -t moose : attach to session called ‘moose’\n\n\nPane Creation and Navigation\n\ncontrol+b then press ” (i.e. shift+’): add another terminal pane below\ncontrol+b then press % (i.e. shift+5) : add another terminal pane to the right\ncontrol+b then press → : move to the terminal pane on the right (similar for left, up, down)",
    "crumbs": [
      "CLI",
      "Linux"
    ]
  },
  {
    "objectID": "qmd/cli-linux.html#sec-cli-lin-ssh",
    "href": "qmd/cli-linux.html#sec-cli-lin-ssh",
    "title": "Linux",
    "section": "SSH",
    "text": "SSH\n\nTypically uses a key pair to log into remote machines\n\nKey pair consists of a public key (which both machines have access to) and a private key (which only your own machine has access to)\n“ssh-keygen” is a program for generating such a key pair.\n\nIf you run ssh-keygen, it will by default create a public key named “id_rsa.pub” and a private key named “id_rsa”, and place both into your “~/.ssh” directory\nYou’ll need to add the public key to the remote machine by piping together cat, ssh, and a streaming operator\n\ncat .ssh/id_rsa.pub | ssh user@remote 'cat &gt;&gt; ~/.ssh/authorized_keys'\n\n\n\nConnect to the remote machine: ssh remote -i ~/.ssh/id_rsa\nCreate a config file instead\n\nLocation: “~/.ssh/config”\nContents\nHost dev\n  HostName remote\n  IdentityFile ~/.ssh/id_rsa\n\nConnect using config: ssh dev\nFor Windows and using Putty, see\n\nAWS &gt;&gt; EC2 &gt;&gt; Connect to/ Terminate Instance\nProjects Notebook &gt;&gt; Article, Nested Cross Validation &gt;&gt; Notes &gt;&gt; Running EC2 instances checklist",
    "crumbs": [
      "CLI",
      "Linux"
    ]
  },
  {
    "objectID": "qmd/cli-linux.html#sec-cli-lin-vim",
    "href": "qmd/cli-linux.html#sec-cli-lin-vim",
    "title": "Linux",
    "section": "Vim",
    "text": "Vim\n\nCommand-line based text editor\nCommon Usage\n\nLogging into a remote machine and need to make a code change there. vim is a standard program and therefore usually available on any machine you work on.\nWhen running git commit, by default git opens vim for writing a commit message. So at the very least you’ll want to know how to write, save, and close a file.\n\n2 modes: Navigation Mode; Edit Mode\n\nWhen Vim is launched you’re in Navigation mode\nPress i to start edit mode, in which you can make changes to the file.\nPress Esc key to leave edit mode and go back to navigation mode.\n\nCommands (Cheatsheet)\n\nx deletes a character\ndd deletes an entire row\nb (back) goes to the previous word\nn (next) goes to the next word\n:wq saves your changes and closes the file\n:q! ignores your changes and closes the file",
    "crumbs": [
      "CLI",
      "Linux"
    ]
  },
  {
    "objectID": "qmd/cli-linux.html#sec-cli-lin-pkg",
    "href": "qmd/cli-linux.html#sec-cli-lin-pkg",
    "title": "Linux",
    "section": "Packages",
    "text": "Packages\n\nCommon package managers: apt, Pacman, yum, and portage\nAPT (Advanced Package Tool)\n\nInstall Packages\n# one pkg\nsudo apt-get install &lt;package_name&gt;\n# multiple\nsudo apt-get install &lt;pkg_name1&gt; &lt;pkg_name2&gt;\n\nInstall but no upgrade: sudo apt-get install &lt;pkg_name&gt; --no-upgrade\n\nSearch for an installed package: apt-cache search &lt;pkg_name&gt;\nUpdate package information prior to “upgrading” the packages\nsudo apt-get update\n\nDownloads the package lists from the repositories and “updates” them to get information on the newest versions of packages and their dependencies.\n\nUpgrade\n# all installed packages\nsudo apt-get upgrade\n\n# To upgrade only a specific program\nsudo apt-get upgrade &lt;package_name&gt;\n\n# Upgrades and handles dependencies; delete obsolete, add new\napt-get dist-upgrade\n\n# together\nsudo apt-get update && sudo apt-get dist-upgrade",
    "crumbs": [
      "CLI",
      "Linux"
    ]
  },
  {
    "objectID": "qmd/cli-linux.html#sec-cli-lin-expr",
    "href": "qmd/cli-linux.html#sec-cli-lin-expr",
    "title": "Linux",
    "section": "Expressions",
    "text": "Expressions\n\nSort data, filter only unique lines, and write to file: cat adult_t.csv | sort | uniq -c &gt; sorted_list.csv",
    "crumbs": [
      "CLI",
      "Linux"
    ]
  },
  {
    "objectID": "qmd/cli-general.html",
    "href": "qmd/cli-general.html",
    "title": "General",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "CLI",
      "General"
    ]
  },
  {
    "objectID": "qmd/cli-general.html#sec-cli-gen-misc",
    "href": "qmd/cli-general.html#sec-cli-gen-misc",
    "title": "General",
    "section": "",
    "text": "Resources\n\nData Science at the Command Line\n\nctrl-rshell command history search\n\nMcFly - intelligent command history search engine that takes into account your working directory and the context of recently executed commands. McFly’s suggestions are prioritized in real time with a small neural network\n\nPath to a folder that’s above root folder:\n\n1 level up: ../desired-folder\n2 levels up: ../../desired-folder",
    "crumbs": [
      "CLI",
      "General"
    ]
  },
  {
    "objectID": "qmd/cli-general.html#sec-cli-gen-r",
    "href": "qmd/cli-general.html#sec-cli-gen-r",
    "title": "General",
    "section": "R",
    "text": "R\n\nMake an R script pipeable (From link)\nparallel \"echo 'zipping bin {}'; cat chunked/*_bin_{}_*.csv | ./upload_as_rds.R '$S3_DEST'/chr_'$DESIRED_CHR'_bin_{}.rds\"\n#!/usr/bin/env Rscript\nlibrary(readr)\nlibrary(aws.s3)\n\n# Read first command line argument\ndata_destination &lt;- commandArgs(trailingOnly = TRUE)[1]\n\ndata_cols &lt;- list(SNP_Name = 'c', ...)\n\ns3saveRDS(\n  read_csv(\n        file(\"stdin\"), \n        col_names = names(data_cols),\n        col_types = data_cols \n    ),\n  object = data_destination\n)\n\nBy passing readr::read_csv the function, file(\"stdin\"), it loads the data piped to the R script into a dataframe, which then gets written as an .rds file directly to s3 using {aws.s3}.\n\nKilling a process\nsystem(\"taskkill /im java.exe /f\", intern=FALSE, ignore.stdout=FALSE)\nStarting a process in the background\n# start MLflow server\nsys::exec_background(\"mlflow server\")\nDelete an opened file in the same R session\n\nYou **MUST** unlink it before any kind of manipulation of object\n\nI think this works because readr loads files lazily by default\n\nExample:\nwisc_csv_filename &lt;- \"COVID-19_Historical_Data_by_County.csv\"\ndownload_location &lt;- file.path(Sys.getenv(\"USERPROFILE\"), \"Downloads\")\nwisc_file_path &lt;- file.path(download_location, wisc_csv_filename)\nwisc_tests_new &lt;- readr::read_csv(wisc_file_path)\n# key part, must unlink before any kind of code interaction\n# supposedly need recursive = TRUE for Windows, but I didn't need it\n# Throws an error (hence safely) but still works\nsafe_unlink &lt;- purrr::safely(unlink)\nsafe_unlink(wisc_tests_new)\n\n# manipulate obj\nwisc_tests_clean &lt;- wisc_tests_new %&gt;%\n      janitor::clean_names() %&gt;%\n      select(date, geo, county = name, negative, positive) %&gt;%\n      filter(geo == \"County\") %&gt;%\n      mutate(date = lubridate::as_date(date)) %&gt;%\n      select(-geo)\n# clean-up\nfs::file_delete(wisc_file_path)\n\nFind out which process is locking or using a file\n\nOpen Resource Monitor, which can be found\n\nBy searching for Resource Monitor or resmon.exe in the start menu, or\nAs a button on the Performance tab in your Task Manager\n\nGo to the CPU tab\nUse the search field in the Associated Handles section\n\ntype the name of file in the search field and it’ll search automatically\n35548",
    "crumbs": [
      "CLI",
      "General"
    ]
  },
  {
    "objectID": "qmd/cli-general.html#sec-cli-gen-awk",
    "href": "qmd/cli-general.html#sec-cli-gen-awk",
    "title": "General",
    "section": "AWK",
    "text": "AWK\n\n\nMisc\n\nResources\n\nDocs\nAwk - A Tutorial and Introduction\n\n\nPrint first few rows of columns 1 and 2\nawk -F, '{print $1,$2}' adult_t.csv|head\nFilter lines where no of hours/ week (13th column) &gt; 98\nawk -F, ‘$13 &gt; 98’ adult_t.csv|head\nFilter lines with “Doctorate” and print first 3 columns\nawk '/Doctorate/{print $1, $2, $3}' adult_t.csv\nRandom sample 8% of the total lines from a .csv (keeps header)\n'BEGIN {srand()} !/^$/ {if(rand()&lt;=0.08||FNR==1) print &gt; \"rand.samp.csv\"}' big_fn.csv\nDecompresses, chunks, sorts, and writes back to S3 (From link)\n# Let S3 use as many threads as it wants\naws configure set default.s3.max_concurrent_requests 50\n\nfor chunk_file in $(aws s3 ls $DATA_LOC | awk '{print $4}' | grep 'chr'$DESIRED_CHR'.csv') ; do\n\n        aws s3 cp s3://$batch_loc$chunk_file - |\n        pigz -dc |\n        parallel --block 100M --pipe  \\\n        \"awk -F '\\t' '{print \\$1\\\",...\\\"$30\\\"&gt;\\\"chunked/{#}_chr\\\"\\$15\\\".csv\\\"}'\"\n\n        # Combine all the parallel process chunks to single files\n        ls chunked/ |\n        cut -d '_' -f 2 |\n        sort -u |\n        parallel 'cat chunked/*_{} | sort -k5 -n -S 80% -t, | aws s3 cp - '$s3_dest'/batch_'$batch_num'_{}'\n\n        # Clean up intermediate data\n        rm chunked/*\ndone\n\nUses pigz to parallelize decompression\nUses GNU Parallel (site, docs, tutorial1, tutorial2) to parallelize chunking (100MB chunks in 1st section)\nChunks data into smaller files and sorts them into directories based on a chromosome column (I think)\nAvoids writing to disk",
    "crumbs": [
      "CLI",
      "General"
    ]
  },
  {
    "objectID": "qmd/cli-windows.html",
    "href": "qmd/cli-windows.html",
    "title": "Windows",
    "section": "",
    "text": "Powershell",
    "crumbs": [
      "CLI",
      "Windows"
    ]
  },
  {
    "objectID": "qmd/cli-windows.html#sec-cli-win-powsh",
    "href": "qmd/cli-windows.html#sec-cli-win-powsh",
    "title": "Windows",
    "section": "",
    "text": "Misc\n\nDocs, Sample Scripts\nUse Get-Help &lt;cmdlet-name&gt; to access help information for specific cmdlets.\n\n$_ (Docs)\n\nAutomatic Variable; Alias for $PSItem\n\nDocs for automatic variables\n\nUseful for looping through objects in a directory. See “Change Name (or Extensions) of Multiple Files” for an example of usage\nGet the properties of an object\n\nExample: Using Get-ItemProperty (Docs)\nGet-ItemProperty scrapsheet.txt | Format-List\n\nProperty names will be on the left side of the output\n\nExample: Using Get-Member (Docs)\n Get-ChildItem *.txt | Select-Object -First 1 | Get-Member\n\nSelects the first text file in the directory and gets the Properties and a bunch of other stuff like methods which also can be used with $_\nDescriptions are awful for a lot of the stuff, but for the most part, you can guess what the property is.\n\n\nSome properties that can be used with $_\n\nFile System Objects:\n\n.Name: Returns the name component of a file\n.FullName: Returns the full path of the file.\n.Length: Returns the size of the file in bytes.\n.CreationTime: Returns the date and time the file was created.\n.LastWriteTime: Returns the date and time the file was last modified.\n.IsReadOnly: Returns $true if the file is read-only, $false otherwise.\n\nProcess Objects:\n\n.Id: Returns the unique identifier (PID) of the process.\n.Name: Returns the name of the process.\n.MainWindowTitle: Returns the title of the process’s main window (if applicable).\n.WorkingDirectory: Returns the working directory of the process.\n.CPU: Returns the CPU usage of the process.\n\nRegistry Objects:\n\n.Name: Returns the name of the registry key.\n.Hive: Returns the hive of the registry key (e.g., HKLM, HKCU).\n.Value: Returns the value of the registry key (if applicable).\n.Path: Returns the full path of the registry key.\n\n\n\nComments: &lt;# comment #&gt;\nChange directories\n Set-Location \"Documents\\R\\Projects\"\nCreate a New Folder\n New-Item -ItemType Directory -Path \"Folder Name\"\n\nAssumes you’re already in the directory that you want the folder in. You can also use a path, e.g. \"C:\\Temp\\Documents\\New Folder\\Subfolder1\\\\Subfolder2\".\n\nChange Name of File\nRename-Item -Path \"c:\\logfiles\\daily_file.txt\" -NewName \"monday_file.txt\"\nChange Name (or Extensions) of Multiple Files\nGet-ChildItem *.md | Rename-Item -NewName { $_.Name -replace '.md','.qmd' }\n\nGet-ChildItem: cmdlet gets all the files in the current folder that have a .mc file extension\nResults piped to Rename-Item\n\nNewName: Has a value that’s a script block that runs before the value is submitted to the NewName parameter.\n\n$_: (automatic variable) It represents each file object as it comes to the command through the pipeline.\n-replace: Operator replaces the file extension of each file with .qmd.\n\nNotice that matching using the -replace operator is not case sensitive.\n\n\n\n\nExecute a File\nInvoke-Item configuration.cmd\nMulti-line Commands\nffmpeg -i input.mkv -map 0:v:0 `\n       -map 0:a:2 -map 0:a:0 -map 0:a:1 -map 0:a:3 `\n       -map 0:s -c copy `\n       -disposition:a:0 default `\n       reordered.mkv\n\nIn bash, it’s a backslash (\\), but in Powershell, it’s a backtick ( ` )\n*Don’t forget that there’s a space between the last character and the backtick.*\nIn practice, this will look like\nffmpeg -i .input.mkv -map 0:v:0 `\n&gt;&gt; -map 0:a:2 -map 0:a:0 -map 0:a:1 -map 0:a:3 `\n&gt;&gt; -map 0:s -c copy `\n&gt;&gt; -disposition:a:0 default `\n&gt;&gt; reordered.mkv\n\nString Matching\n\nPrint line with pattern\nSelect-String -Path \"file*.txt\" -Pattern \"error\"\nfile1.txt:3:This is the error line of the file\nfile2.txt:3:This is the error line of the file\nfile3.txt:3:This is the error line of the file\n\nMatches the 3rd line of each file\n\n\nGet stats on a process\nGet-Process -Name chrome\n\nHandles: The number of handles that the process has opened.\nNPM(K): The amount of non-paged memory that the process is using, in kilobytes.\nPM(K): The amount of pageable memory that the process is using, in kilobytes.\nWS(K): The size of the working set of the process, in kilobytes. The working set consists of the pages of memory that were recently referenced by the process.\nVM(M): The amount of virtual memory that the process is using, in megabytes. Virtual memory includes storage in the paging files on disk.\nCPU(s): The amount of processor time that the process has used on all processors, in seconds.\nID: The process ID (PID) of the process.\nProcessName: The name of the process. For explanations of the concepts related to processes, see the Glossary in Help and Support Center and the Help for Task Manager.\n\nEnvironment Variables\n\nSet an environment variable\nSet-Item -Name PYTHONSTARTUP -Value C:\\path\\to\\pythonstartup.py\n\nSame expression to modify existing environment variable\nOr\n$env:QUARTO_DENO_EXTRA_OPTIONS = \"--v8-flags=--max-old-space-size=8192\"\n\nDelete environment variable\nRemove-Item -Name &lt;variable_name&gt;\nVerify value of an environment variable\n$env:&lt;variable_name&gt;\n\nPorts\n\nFind application using a port.\nnetstat -aon | findstr ':80'\nnetstat -anp | find \":80\"\n\nIf port 80 is being used by the application, it will return a PID. Then you can find it in Task Manager &gt;&gt; Processess\n\nList all Listening and Established ports\nnetstat -anob\nCheck for processes using a port\nGet-Process -Id (Get-NetTCPConnection -LocalPort 80).OwningProcess\nTest connection to local port to see if it’s open\nTest-NetConnection -ComputerName localhost -Port 80 | Select-Object TcpTestSucceeded\nCheck firewall settings for an app\nnetsh advfirewall firewall show rule name=\"name_of_app\"",
    "crumbs": [
      "CLI",
      "Windows"
    ]
  },
  {
    "objectID": "qmd/cli-windows.html#sec-cli-win-batscri",
    "href": "qmd/cli-windows.html#sec-cli-win-batscri",
    "title": "Windows",
    "section": "Batch Scripting",
    "text": "Batch Scripting\n\nMisc\n\nResources\n\nWindows Batch Scripting\n\nTo keep the prompt window open after script execution, place these either of these commands at end of your script.\n\npause: Keeps window open until you press any key.\nVia timer: e.g. timeout /t 300\ncmd /k: The prompt will remain active and you can execute additional commands manually.\n\n\nExample: Create variables and execute\n@echo off\n\nrem Set the path to the Rscript executable\nset RSCRIPT=\"C:\\Users\\user\\AppData\\Local\\Programs\\R\\R-4.2.3\\bin\\Rscript.exe\"\n\nrem Set the path to the R script to execute\nset RSCRIPT_FILE=\"C:\\Users\\user\\my_r_script.R\"\n\nrem Execute the R script\n%RSCRIPT% %RSCRIPT_FILE%\n\nrem Pause so the user can see the output\nexit\n\n@echo off - This line turns off the echoing of commands in the command prompt window, making the output cleaner.\nrem - Keyword that denotes a comment in a batch file.\nset RSCRIPT= - This line assigns the path to the Rscript executable to the environment variable RSCRIPT.\nset RSCRIPT_FILE= - The path to the R script file is assigned to the environment variable RSCRIPT_FILE.\n%RSCRIPT% %RSCRIPT_FILE% - Executes the R script using the Rscript executable and passes the path to the R script file as an argument.\nexit - This command exits the batch file and closes the command prompt window.\n\nExample: Exit if script errors\nRscript \"C:\\Users\\ercbk\\Documents\\R\\Projects\\Indiana-COVID-19-Tracker\\R\\collection\\build-opentab-dat.R\"\n\nREM if the data building script errors, bat script terminates without running other scripts or commands\nif %errorlevel% neq 0 exit /b %errorlevel%\n\ncd \"C:\\Users\\ercbk\\Documents\\R\\Projects\\Indiana-COVID-19-Tracker\"\n\ngit add data/YoY_Seated_Diner_Data.csv\ngit commit -m \"opentab data update\"\ngit pull\ngit push\n\nEXIT",
    "crumbs": [
      "CLI",
      "Windows"
    ]
  },
  {
    "objectID": "qmd/cli-windows.html#sec-cli-win-wsl",
    "href": "qmd/cli-windows.html#sec-cli-win-wsl",
    "title": "Windows",
    "section": "WSL",
    "text": "WSL\n\nResources\n\nDocs\nTo update password (link) using username\n\nLoad Linux: wsl -d Ubuntu-22.04 where -d is for –distribution\nWSL Help: wsl --help\nExit linux terminal back to command prompt or powershell: exit",
    "crumbs": [
      "CLI",
      "Windows"
    ]
  },
  {
    "objectID": "qmd/confidence-and-prediction-intervals.html",
    "href": "qmd/confidence-and-prediction-intervals.html",
    "title": "Confidence & Prediction Intervals",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Confidence & Prediction Intervals"
    ]
  },
  {
    "objectID": "qmd/confidence-and-prediction-intervals.html#sec-cipi-misc",
    "href": "qmd/confidence-and-prediction-intervals.html#sec-cipi-misc",
    "title": "Confidence & Prediction Intervals",
    "section": "",
    "text": "Also see Mathematices, Statistics &gt;&gt; Descriptive Statistics &gt;&gt; Understanding CI, sd, and sem Bars\nSE used for CIs of the difference in proportion\n\\[\n\\text{SE} = \\sqrt{\\frac{p_1(1-p_1)}{n_1} + \\frac{p_2(1-p_2)}{n_2}}\n\\]",
    "crumbs": [
      "Confidence & Prediction Intervals"
    ]
  },
  {
    "objectID": "qmd/confidence-and-prediction-intervals.html#sec-cipi-terms",
    "href": "qmd/confidence-and-prediction-intervals.html#sec-cipi-terms",
    "title": "Confidence & Prediction Intervals",
    "section": "Terms",
    "text": "Terms\n\nConfidence Intervals: A range of values within which we are reasonably confident the true parameter (e.g mean) of a population lies, based on a sample statistic (e.g. t-stat).\n\nFrequentist Interpretation: The confidence interval is constructed by a procedure, which, if you were to repeat the experiment and collecting samples many many times, in 95% of the experiments, the corresponding confidence intervals would cover the true value of the population mean. (link)\n\\[\n[100\\cdot(1-\\alpha)]\\;\\%\\: \\text{CI for}\\: \\hat\\beta_i = \\hat\\beta_i \\pm \\left[t_{(1-\\alpha/2)(n-k)} \\cdot \\text{SE}(\\hat\\beta_i)\\right]\n\\]\n\n\\(t\\) is the t-stat for\n\n\\(n-k\\) = sample size - number of predictors\n\\(1-\\alpha\\) for 2-sided; \\(1 - (\\alpha/2)\\) for 1 sided (I think)\n\n\\(\\text{SE}(\\beta_i)\\) is the sqrt of the corresponding value on the diagonal of the variance-covariance matrix for the coefficients.\n\nBayesian Interpretation: the true value is in that interval with 95% probability\n\nCoverage or Empirical Coverage: The level of coverage actually observed when evaluated on a dataset, typically a holdout dataset not used in training the model. Rarely will your model produce the Expected Coverage exactly\n\nAdaptive Coverage: Setting your Expected Coverage so that your Empirical Coverage = Target Coverage. A conformal prediction algorithm is adaptive if it not only achieves marginal coverage, but also (approximately) conditional coverage\n\nExample: 90% target coverage\n\nIf our model is slightly overfit, you might see that a 90% expected coverage leads to an 85% empirical coverage on a holdout dataset. To align your target and empirical coverage at 90%, may require setting expected coverage at something like 93%\n\n\nExpected Coverage: The level of confidence in the model for the prediction intervals.\nConditional Coverage: The coverage for each individual class of the outcome variable or subset of data specified by a grouping variable.\nMarginal Coverage: The overall average coverage across all classes of the outcome variable. All conformal methods achieve at or near the Expected Coverage averaged across classes but not necessarily for each individual class.\nTarget Coverage: The level of coverage you want to attain on a holdout dataset\n\ni.e. The proportion of observations you want to fall within your prediction intervals\n\n\nJeffrey’s Interval: Bayesian CIs for Binomial proportions (i.e. probability of an event)\n# probability of event\n# n_rain in the number of events (rainy days)\n# n is the number of trials (total days)\nmutate(pct_rain = n_rain / n, \n       # jeffreys interval\n       # bayesian CI for binomial proportions\n       low = qbeta(.025, n_rain + .5, n - n_rain + .5), \n       high = qbeta(.975, n_rain + .5, n - n_rain + .5))\nPrediction Interval: Used to estimate the range within which a future observation is likely to fall\n\nStandard Procedure for computing PIs for predictions (See link for examples and further details)\n\\[\n\\hat Y_0 \\pm t^{n-p}_{\\alpha/2} \\;\\hat\\sigma \\sqrt{1 + \\vec x_0'(X'X)^{-1}\\vec x_0}\n\\]\n\n\\(Y_0\\) is a single prediction\n\\(t\\) is the t-stat for\n\n\\(n-p\\) = sample size - number of predictors\n\\(1 - \\alpha\\) for 2-sided; \\(1 - (\\alpha/2)\\) for 1 sided (I think)\n\n\\(\\hat\\sigma\\) is the variance given by residual standard error, summary(Model1)$sigma\n\\[\nS^2 = \\frac{1}{n-p}\\;||\\;Y-X\\hat \\beta\\;||^2\n\\]\n\n\\(S = \\hat \\sigma\\)\nI think this is also the \\(\\operatorname{MSE}/\\operatorname{dof}\\) that you sometimes see in other formulas\n\n\\(x_0\\) is new data for the predictor variable values for the prediction (also would need to include a 1 for the intercept)\n\\((X'X)^{-1}\\) is the variance covariance matrix, vcov(model)",
    "crumbs": [
      "Confidence & Prediction Intervals"
    ]
  },
  {
    "objectID": "qmd/confidence-and-prediction-intervals.html#sec-cipi-diag",
    "href": "qmd/confidence-and-prediction-intervals.html#sec-cipi-diag",
    "title": "Confidence & Prediction Intervals",
    "section": "Diagnostics",
    "text": "Diagnostics\n\nMean Interval Score (MIS)\n\n(Proper) Score of both coverage and interval width\n\nI don’t think there’s a closed range, so it’s meant for model comparison\nLower is better\n\ngreybox::MIS and (scaled) greybox::sMIS\n\nOnline docs don’t have these functions, but docs in RStudio do\n\nAlso scoringutils::interval_score\n\nDocs have formula\n\nThe actual paper is dense Need to take the mean of MIS\n\n\n\nCoverage\n\nExample: Coverage %\ncoverage &lt;- function(df, ...){\n  df %&gt;%\n    mutate(covered = ifelse(Sale_Price &gt;= .pred_lower & Sale_Price pred_upper, 1, 0)) %&gt;% \n    group_by(...) %&gt;% \n    summarise(n = n(),\n              n_covered = sum(\n                covered\n              ),\n              stderror = sd(covered) / sqrt(n),\n              coverage_prop = n_covered / n)\n}\nrf_preds_test %&gt;% \n  coverage() %&gt;% \n  mutate(across(c(coverage_prop, stderror), ~.x * 100)) %&gt;% \n  gt::gt() %&gt;% \n  gt::fmt_number(\"stderror\", decimals = 2) %&gt;% \n  gt::fmt_number(\"coverage_prop\", decimals = 1)\n\nFrom Quantile Regression Forests for Prediction Intervals\nSale_Price is the outcome variable\nrf_preds_test is the resulting object from predict with a tidymodels model as input\n\nExample: Test consistency of coverage across quintiles\npreds_intervals %&gt;%  # preds w/ PIs\n  mutate(price_grouped = ggplot2::cut_number(.pred, 5)) %&gt;%  # quintiles\n  mutate(covered = ifelse(Sale_Price &gt;= .pred_lower & Sale_Price &lt;= .pred_upper, 1, 0)) %&gt;% \n  with(chisq.test(price_grouped, covered))\n\np value &lt; 0.05 says coverage significantly differs by quintile\nSale_Price is the outcome variable\n\n\nInterval Width\n\nNarrower bands should mean a more precise model\nExample: Average interval width across quintiles\nlm_interval_widths &lt;- preds_intervals %&gt;% \n  mutate(interval_width = .pred_upper - .pred_lower,\n        interval_pred_ratio = interval_width / .pred) %&gt;% \n  mutate(price_grouped = ggplot2::cut_number(.pred, 5)) %&gt;% # quintiles\n  group_by(price_grouped) %&gt;% \n  summarize(n = n(),\n            mean_interval_width_percentage = mean(interval_pred_ratio),\n            stdev = sd(interval_pred_ratio),\n            stderror = stdev / sqrt(n)) %&gt;% \n  mutate(x_tmp = str_sub(price_grouped, 2, -2)) %&gt;% \n  separate(x_tmp, c(\"min\", \"max\"), sep = \",\") %&gt;% \n  mutate(across(c(min, max), as.double)) %&gt;% \n  select(-price_grouped)\n\nlm_interval_widths %&gt;% \n  mutate(across(c(mean_interval_width_percentage, stdev, stderror), ~.x*100)) %&gt;% \n  gt::gt() %&gt;% \n  gt::fmt_number(c(\"stdev\", \"stderror\"), decimals = 2) %&gt;% \n  gt::fmt_number(\"mean_interval_width_percentage\", decimals = 1)\n\nInterval width has actually been transformed into a percentage as related to the prediction (removes the scale of the outcome variable)",
    "crumbs": [
      "Confidence & Prediction Intervals"
    ]
  },
  {
    "objectID": "qmd/confidence-and-prediction-intervals.html#sec-cipi-boot",
    "href": "qmd/confidence-and-prediction-intervals.html#sec-cipi-boot",
    "title": "Confidence & Prediction Intervals",
    "section": "Bootstrapping",
    "text": "Bootstrapping\n\nMisc\n\nDo NOT bootstrap the standard deviation\n\narticle\nbootstrap is “based on a weak convergence of moments”\nif you use an estimate based standard deviation of the bootstrap, you are being overly conservative (i.e. overestimate the sd)\n\nbootstrapping uses the original, initial sample as the population from which to resample, whereas Monte Carlo simulation is based on setting up a data generation process (with known values of the parameters of a known distribution). Where Monte Carlo is used to test drive estimators, bootstrap methods can be used to estimate the variability of a statistic and the shape of its sampling distribution\nPackages\n\n{ebtools::get_boot_ci}\n\n\nSteps\n\nResample with replacement\nCalculate statistic of resample\nStore statistic\nRepeat 10K or so times\nCalculate mean, sd, and quantiles for CIs across all collected statistics\n\nCIs\n\nPlenty of articles for means and models, see bkmks\nrsample::reg_intervals is a convenience function for lm, glm, survival models\n\nPIs\n\nBootstrapping PIs is a bit complicated\n\nSee Shalloway’s article (code included)\nonly use out-of-sample estimates to produce the interval\nestimate the uncertainty of the sample using the residuals from a separate set of models built with cross-validation",
    "crumbs": [
      "Confidence & Prediction Intervals"
    ]
  },
  {
    "objectID": "qmd/confidence-and-prediction-intervals.html#sec-cipi-conf",
    "href": "qmd/confidence-and-prediction-intervals.html#sec-cipi-conf",
    "title": "Confidence & Prediction Intervals",
    "section": "Conformal Prediction Intervals",
    "text": "Conformal Prediction Intervals\n\nMisc\n\nPackages\n\n{{mapie}} - Handles scikit-learn, tf, pytorch, etc. with wrappers. Computes conformal PIs for Regression, Classification, and Time Series models.\n\nRegression\n\nMethods: naive, split, jackknife, jackknife+, jackknife-minmax, jackknife-after-bootstrap, CV, CV+, CV-minmax, ensemble batch prediction intervals (EnbPI).\n“Since the typical coverage levels estimated by jackknife+ follow very closely the target coverage levels, this method should be used when accurate and robust prediction intervals are required.”\n“For practical applications where N is large and/or the computational time of each leave-one-out simulation is high, it is advised to adopt the CV+ method” even though the interval width will be slightly larger than jackknife+\n“The jackknife-minmax and CV-minmax methods are more conservative since they result in higher theoretical and practical coverages due to the larger widths of the prediction intervals. It is therefore advised to use them when conservative estimates are needed.”\n“The conformalized quantile regression method allows for more adaptiveness on the prediction intervals which becomes key when faced with heteroscedastic data.”\nEnbPI is for time series and residuals must be updated each time new observations are available\n\nClassification\n\nMethods: LAC, Top-K, Adaptive Prediction Sets (APS), Regularized Adaptive Prediction Sets (RAPS), Split and Cross-Conformal methods.\nThe difference between these methods is the way the conformity scores are computed\nLAC method is not adaptive: the coverage guarantee only holds on average (i.e. marginal coverage). Difficult classification cases may have prediction sets that are too small, and easy cases may have sets that are too large. (See below for details on process). Doesn’t seem like to great a task to manually make it adaptive though (See example below).\nAPS’ conformity score used to determine the threshold is a constrained sum of the predicted probabilities for that observation. Only the predicted probabilites \\(\\ge\\) the predicted probability of the true class are included in the sum. Everything else is the same as the LAC algorithm, although the default behavior is to keep the last class that crosses the threshold through the argument, include_last_label = [True, “randomized”, False]. The value of the argument can determine whether conditional coverage is (approximately) attained with True being the most liberal setting. Note that only “randomized” can produce empty predicted class sets. Algorithm tends to produce large predicted class sets when there are many classes in the outcome variable.\nRAPS attenuates the lengthier predicted class sets in APS through regularization. A penalty, \\(\\lambda\\), is added to predicted probabilities with ranks greater that some value, \\(k\\). Everything else is the same as APS.\nNot sure what Split is, but Cross-Conformal is CV applied to LAC and APS.\n\n\n\nNotes from\n\nHow to Handle Uncertainty in Forecasts: A deep dive into conformal prediction\n\nThe conformity score formula used in this article, \\(s_i = |\\;y_i - \\hat p_i(y_i\\;|\\;X_i)\\;|\\) where \\(y_i\\) is the observed class and \\(\\hat p\\) is the predicted probability, has the same results as to the one below, but it’s not workable in production since there is no observed class.\n\nConformal Prediction for Machine Learning Classification — From the Ground Up\n“MAPIE” Explained Exactly How You Wished Someone Explained to You\n\nResources\n\nIntroduction To Conformal Prediction With Python A Short Guide for Quantifying Uncertainty of Machine Learning Models\n\nSee R &gt;&gt; Documents &gt;&gt; Machine Learning\n\n\nNormal PIs require iid data while conformal PIs only require the “identically distributed” part (not independent) and therefore should provide more robust coverage.\nContinuous outcome (link) using quantile regression\n\n\n\nClassification\n\nLAC (aka Score Method) Process\n\nSplit data into Train, Calibration (aka Validation), and Test\nTrain the model on the training set\nOn the calibration (aka validation) set, compute the conformity scores only for the observed class (i.e. true label) for each observation\n\\[\ns_{i, j}  = 1 - \\hat p_{i,j}(y_i | X_i)\n\\]\n\nVariables\n\n\\(s_{i,j}\\): Conformity Score for the ith observation and class \\(j\\)\n\\(y_i\\): Observed Class\n\\(\\hat p_{i,j}\\): Predicted probability by the model for class \\(j\\)\n\\(X_i\\): Predictors\n\\(i\\): Index of the observed data\n\\(j\\): Class of the outcome variable\n\nRange: [0, 1]\nIn general, Low = good, High = bad\nIn R, the predicted probabilities for statistical models are always for the event (i.e. \\(y_i = 1\\)) in a binary outcome context, so when the observed class = 0, the score will be \\(s_{i,0} = 1-(1- \\hat p_{i, 1}(y_i | X_i)) = \\hat p_{i, 1}(y_i | X_i)\\) which is just the predicted probability.\n\nOrder the conformity scores from highest to lowest\nAdjust the chosen the \\(\\alpha\\) using a finite sample correction, \\(q_{\\text{level}} = 1- \\frac{ceil((n_{\\text{cal}}+1)\\alpha)}{n_{\\text{cal}}}\\) and calculate the quantile.\nCalculate the critical value or threshold for the quantile\n\n\nx-axis corresponds to an ordered set of conformity scores\nIf \\(\\alpha = 0.05\\), find the score value at the the 95th percentile (e.g. quantile(scores, 0.95))\nBlue: conformity scores are not statistically significant. They’re within our prediction interval.\nRed: Very large conformity scores indicate high divergence from the true label. These conformal scores are statistically significant and thereby outside of our prediction interval.\n\nPredict on the Test set and calculate conformity scores for each class\nFor each test set observation, select classes that have scores below the threshold score as the model prediction.\n\nAn observation could potentially have both classes or no classes selected. ( Not sure if this is true in a binary outcome situation)\n\n\nExample: LAC Method, Multinomial\n\nModel\nclassifier = LogisticRegression(random_state=42)\nclassifier.fit(X_train, y_train)\nScores calculated using only the predicted probability for the true class on the Validation set (aka Calibration set)\n# Get predicted probabilities for calibration set\ny_pred = classifier.predict(X_Cal)\ny_pred_proba = classifier.predict_proba(X_Cal)\nsi_scores = []\n# Loop through all calibration instances\nfor i, true_class in enumerate(y_cal):\n    # Get predicted probability for observed/true class\n    predicted_prob = y_pred_proba[i][true_class]\n    si_scores.append(1 - predicted_prob) \nThe threshold determines what coverage our predicted labels will have\nnumber_of_samples = len(X_Cal)\nalpha = 0.05\nqlevel = (1 - alpha) * ((number_of_samples + 1) / number_of_samples)\nthreshold = np.percentile(si_scores, qlevel*100)\nprint(f'Threshold: {threshold:0.3f}')\n#&gt; Threshold: 0.598\n\nFinite sample correction for the 95th quantile: multiply 0.95 by (n+1)/n\n\nThreshold is then used to get predicted labels of the test set\n# Get standard predictions for comparison\ny_pred = classifier.predict(X_test)\n# Calc scores, then only take scores in the 95% conformal PI\nprediction_sets = (1 - classifier.predict_proba(X_test) &lt;= threshold)\n\n# Get labels for predictions in conformal PI\ndef get_prediction_set_labels(prediction_set, class_labels):\n    # Get set of class labels for each instance in prediction sets\n    prediction_set_labels = [\n        set([class_labels[i] for i, x in enumerate(prediction_set) if x]) for prediction_set in \n        prediction_sets]\n    return prediction_set_labels\n\n# Compare conformal prediction with observed and traditional preds\nresults_sets = pd.DataFrame()\nresults_sets['observed'] = [class_labels[i] for i in y_test]\nresults_sets['conformal'] = get_prediction_set_labels(prediction_sets, class_labels)\nresults_sets['traditional'] = [class_labels[i] for i in y_pred]\nresults_sets.head(10)\n#&gt;    observed  conformal        traditional\n#&gt; 0  blue      {blue}           blue\n#&gt; 1  green     {green}          green\n#&gt; 2  blue      {blue}           blue\n#&gt; 3  green     {green}          green\n#&gt; 4  orange    {orange}         orange\n#&gt; 5  orange    {orange}         orange\n#&gt; 6  orange    {orange}         orange\n#&gt; 7  orange    {blue, orange}   blue\n#&gt; 8  orange    {orange}         orange\n#&gt; 9  orange    {orange}         orange\n\nconformity scores are calculated for each potential class using the predicted probabilities on the test set\nThe predicted class for an observation is determined by whether a class has a score below the threshold.\nTherefore, an observation may have 1 or more predicted classes or 0 predicted classes.\n\nStatistics (See Statistics section for functions)\n\nOverall\nweighted_coverage = get_weighted_coverage(\n    results['Coverage'], results['Class counts'])\n\nweighted_set_size = get_weighted_set_size(\n    results['Average set size'], results['Class counts'])\n\nprint (f'Overall coverage: {weighted_coverage}')\nprint (f'Average set size: {weighted_set_size}')\n#&gt; Overall coverage: 0.947\n#&gt; Average set size: 1.035\n\nOverall coverage is very close to the target coverage of 95%, therefore, marginal coverage is achieved which is expected for this method\n\nPer Class\nresults = pd.DataFrame(index=class_labels)\nresults['Class counts'] = get_class_counts(y_test)\nresults['Coverage'] = get_coverage_by_class(prediction_sets, y_test)\nresults['Average set size'] = get_average_set_size(prediction_sets, y_test)\nresults\n#&gt;         Class counts  Coverage   Average set size\n#&gt; blue    241           0.817427   1.087137\n#&gt; orange  848           0.954009   1.037736\n#&gt; green   828           0.977053   1.016908\n\nOverall coverage (i.e. for all labels) will be at or very near 95% but coverage for individual classes may vary.\n\nAn illustration of how this method lacks Conditional Coverage\nSolution: Get thresholds for each class. (See next example)\n\nNote that the blue class had substantially fewer observations that the other 2 classes.\n\n\n\nExample: LAC-adapted - Threshold per Class\n\nDon’t think {{mapie}} has this option.\nAlso possible do this for subgroups of data, such as ensuring equal coverage for a diagnostic across racial groups, if we found coverage using a shared threshold led to problems.\nCalculate individual class thresholds\n# Set alpha (1 - coverage)\nalpha = 0.05\nthresholds = []\n# Get predicted probabilities for calibration set\ny_cal_prob = classifier.predict_proba(X_Cal)\n# Get 95th percentile score for each class's s-scores\nfor class_label in range(n_classes):\n    mask = y_cal == class_label\n    y_cal_prob_class = y_cal_prob[mask][:, class_label]\n    s_scores = 1 - y_cal_prob_class\n    q = (1 - alpha) * 100\n    class_size = mask.sum()\n    correction = (class_size + 1) / class_size\n    q *= correction\n    threshold = np.percentile(s_scores, q)\n    thresholds.append(threshold)\nApply individual class thresholds to test set scores\n# Get Si scores for test set\npredicted_proba = classifier.predict_proba(X_test)\nsi_scores = 1 - predicted_proba\n\n# For each class, check whether each instance is below the threshold\nprediction_sets = []\nfor i in range(n_classes):\n    prediction_sets.append(si_scores[:, i] &lt;= thresholds[i])\nprediction_sets = np.array(prediction_sets).T\n\n# Get prediction set labels and show first 10\nprediction_set_labels = get_prediction_set_labels(prediction_sets, class_labels)\nStatistics\n\nOverall\nweighted_coverage = get_weighted_coverage(\n    results['Coverage'], results['Class counts'])\n\nweighted_set_size = get_weighted_set_size(\n    results['Average set size'], results['Class counts'])\n\nprint (f'Overall coverage: {weighted_coverage}')\nprint (f'Average set size: {weighted_set_size}')\n#&gt; Overall coverage: 0.95\n#&gt; Average set size: 1.093\n\nSimilar to previous example\n\nPer Class\nresults = pd.DataFrame(index=class_labels)\nresults['Class counts'] = get_class_counts(y_test)\nresults['Coverage'] = get_coverage_by_class(prediction_sets, y_test)\nresults['Average set size'] = get_average_set_size(prediction_sets, y_test)\nresults\n#&gt;         Class counts  Coverage   Average set size\n#&gt; blue    241           0.954357   1.228216\n#&gt; orange  848           0.956368   1.139151\n#&gt; green   828           0.942029   1.006039\n\nCoverages now very close to 95% and the average set sizes have increased, especially for Blue.\n\n\n\n\n\n\nContinuous\n\nConformalized Quantile Regression Process\n\nSplit data into Training, Calibration, and Test sets\n\nTraining data: data on which the quantile regression model learns.\nCalibration data: data on which CQR calibrates the intervals.\n\nIn the example, he split the data into 3 equal sets\n\nTest data: data on which we evaluate the goodness of intervals.\n\nFit quantile regression model on training data.\nUse the model obtained at previous step to predict intervals on calibration data.\n\nPIs are predictions at the quantiles:\n\n(alpha/2)*100) (e.g 0.025, alpha = 0. 05)\n(1-(alpha/2))*100) (e.g. 0.975)\n\n\nCompute conformity scores on calibration data and intervals obtained at the previous step.\n\nResiduals are calculated for the PI vectors\nScores are calculated by taking the row-wise maximum of both (upper/lower quantile) residual vectors (e.g s_i &lt;- pmax(lower_pi_res, upper_pi_res))\n\nGet 1-alpha quantile from the distribution of conformity scores (e.g threshold &lt;- quantile(s_i, 0.95)\n\nThis score value will be the threshold\n\nUse the model obtained at step 1 to make predictions on test data.\n\nCompute PI vectors (i.e. predictions at the previously stated quantiles) on Test set\ni.e. Same calculation as with the calibration data in step 2 where you use the model to predict at upper and lower PI quantiles.\n\nCompute lower/upper end of the interval by subtracting/adding the threshold from/to the quantile predictions (aka PIs)\n\nLower conformity interval: lower_pi &lt;- test_lower_pred  - threshold\nUpper conformity interval: upper_pi &lt;- test_upper_pred + threshold\n\n\nExample: Quantile Random Forest\nimport numpy as np\nfrom skgarden import RandomForestQuantileRegressor\n\nalpha = .05\n\n# 1. Fit quantile regression model on training data\nmodel = RandomForestQuantileRegressor().fit(X_train, y_train)\n\n# 2. Make prediction on calibration data\ny_cal_interval_pred = np.column_stack([\n    model.predict(X_cal, quantile=(alpha/2)*100), \n    model.predict(X_cal, quantile=(1-alpha/2)*100)])\n\n# 3. Compute conformity scores on calibration data\ny_cal_conformity_scores = np.maximum(\n    y_cal_interval_pred[:,0] - y_cal, \n    y_cal - y_cal_interval_pred[:,1])\n\n# 4. Threshold: Get 1-alpha quantile from the distribution of conformity scores\n#    Note: this is a single number\nquantile_conformity_scores = np.quantile(\n    y_cal_conformity_scores, 1-alpha)\n\n# 5. Make prediction on test data\ny_test_interval_pred = np.column_stack([\n    model.predict(X_test, quantile=(alpha/2)*100), \n    model.predict(X_test, quantile=(1-alpha/2)*100)])\n\n# 6. Compute left (right) end of the interval by\n#    subtracting (adding) the quantile to the predictions\ny_test_interval_pred_cqr = np.column_stack([\n    y_test_interval_pred[:,0] - quantile_conformity_scores,\n    y_test_interval_pred[:,1] + quantile_conformity_scores])\n\n\n\nStatistics\n\nAverage Set Size\n\nThe average number of predicted classes per observation since there can be more than 1 predicted class in the conformal PI\nExample:\n# average set size for each class\ndef get_average_set_size(prediction_sets, y_test):\n    average_set_size = []\n    for i in range(n_classes):\n        average_set_size.append(\n            np.mean(np.sum(prediction_sets[y_test == i], axis=1)))\n    return average_set_size   \n\n# Overall average set size (weighted by class size)\n# Get class counts\ndef get_class_counts(y_test):\n    class_counts = []\n    for i in range(n_classes):\n        class_counts.append(np.sum(y_test == i))\n    return class_counts\n\ndef get_weighted_set_size(set_size, class_counts):\n    total_counts = np.sum(class_counts)\n    weighted_set_size = np.sum((set_size * class_counts) / total_counts)\n    weighted_set_size = round(weighted_set_size, 3)\n    return weighted_set_size\n\nCoverage\n\nClassification: Percentage of correct classifications\nExample: Classification\n# coverage for each class\ndef get_coverage_by_class(prediction_sets, y_test):\n    coverage = []\n    for i in range(n_classes):\n        coverage.append(np.mean(prediction_sets[y_test == i, i]))\n    return coverage\n\n# overall coverage (weighted by class size)\n# Get class counts\ndef get_class_counts(y_test):\n    class_counts = []\n    for i in range(n_classes):\n        class_counts.append(np.sum(y_test == i))\n    return class_counts\n\ndef get_weighted_coverage(coverage, class_counts):\n    total_counts = np.sum(class_counts)\n    weighted_coverage = np.sum((coverage * class_counts) / total_counts)\n    weighted_coverage = round(weighted_coverage, 3)\n    return weighted_coverage\n\n\n\n\nVisualization\n\nConfusion Matrix\n\n\nBinary target where labels are 0 and 1\nInterpretation\n\nTop-left: predictions where both labels are not statistically significant (i.e. inside the “prediction interval”).\n\nThe model predicts both classes well since both labels have low scores.\nDepending the threshold, maybe the model could be relatively agnostic (e.g. predicted probabilites like 0.50-0.50, 0.60-0.40)\n\nBottom-right: predictions where both labels are statistically significant  (i.e. outside the “prediction interval”).\n\nModel totally whiffs. Confident it’s one label when it’s actually another.\n\nExample\n\n1 (truth) - low predicted probability = high score -&gt; Red and significant\n0 - high predicted probability = high score -&gt; Red and significant\n\n\n\nTop-right: predictions where all 0 labels are not statistically significant.\n\nModel predicted the 0=class well (i.e. low scores) but the 1-class poorly (i.e. high scores)\n\nBottom-left: predictions where all 1 labels are not statistically significant. Here, the model predicted that 1 is the true class.\n\nVice versa of top-right",
    "crumbs": [
      "Confidence & Prediction Intervals"
    ]
  },
  {
    "objectID": "qmd/big-data.html",
    "href": "qmd/big-data.html",
    "title": "Big Data",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Big Data"
    ]
  },
  {
    "objectID": "qmd/big-data.html#sec-bgdat-misc",
    "href": "qmd/big-data.html#sec-bgdat-misc",
    "title": "Big Data",
    "section": "",
    "text": "RcppArmadillo::fastLmPure Not sure what this does but it’s rcpp so maybe faster than lm for big data.\n.lm.fit is a base R lm function that is 30%-40% faster than lm.",
    "crumbs": [
      "Big Data"
    ]
  },
  {
    "objectID": "qmd/big-data.html#sec-bgdat-hghperf",
    "href": "qmd/big-data.html#sec-bgdat-hghperf",
    "title": "Big Data",
    "section": "High Performance",
    "text": "High Performance\n\n{rpolars}: arrow product; uses SIMD which is a low-level vectorization that can be used to speed up simple operations like addition, subtraction, division, and multiplication\n\nResources\n\nCookbook Polars for R\n\nAlso see collapse &gt;&gt; vs arrow/polars\nExample: Read and Summarize\ndf &lt;- pl$scan_csv(file_name)$\n    group_by(\"state\")$\n    agg(\n        pl$\n          col(\"measurement\")$\n          min()$\n          alias(\"min_m\"),\n        pl$\n          col(\"measurement\")$\n          max()$\n          alias(\"max_m\"),\n        pl$\n          col(\"measurement\")$\n          mean()$\n          alias(\"mean_m\")\n    )$\n    collect()\n\nFastest at this operation according to this benchmark\n\nExample: groupby state + min, max, mean\n# polars sql\nlf &lt;- polars::pl$LazyFrame(D) \npolars::pl$SQLContext(frame = lf)$execute(\n  \"select min(measurement) as min_m, \n          max(measurement) as max_m, \n          avg(measurement) as mean_m \n  from frame \n  group by state\")$collect()\n\n# polars\npolars::pl$\n  DataFrame(D)$\n  group_by(\"state\")$\n  agg(polars::pl$\n        col(\"measurement\")$\n        min()$alias(\"min_m\"),\n      polars::pl$\n        col(\"measurement\")$\n        max()$alias(\"max_m\"),\n      polars::pl$\n        col(\"measurement\")$\n        mean()$alias(\"mean_m\"))\n\n{collapse}: Fast grouped & weighted statistical computations, time series and panel data transformations, list-processing, data manipulation functions, summary statistics and various utilities such as support for variable labels. Class-agnostic framework designed to work with vectors, matrices, data frames, lists and related classes i.e. xts, data.table, tibble, pdata.frame, sf.\n\noptions(collapse_mask = \"all\")\nlibrary(collapse)\n\nCode chunk above can optimize any script. No other changes necessary. Quick demo.\nvs arrow/polars (benchmark)\n\nDepends on the data/groups ratio\n\nIf you have “many groups and little data in each group” then use collapse\n\nIf your calculations involve “more complex statistics algorithms like the median (involving selection) or mode or distinct value count (involving hashing)(cannot, to my knowledge, benefit from SIMD)” then use collapse.\n\nExample: groupby state + min, max, mean\nD |&gt;\n  fgroup_by(state) |&gt; \n  fsummarise(min = fmin(measurement), \n             max = fmax(measurement), \n             mean = fmean(measurement)) |&gt;\n  fungroup()\n\n{r2c}: Fast grouped statistical computation; currently limited to a few functions, sometimes faster than {collapse}\n{data.table}: Enhanced data frame class with concise data manipulation framework offering powerful aggregation, extremely flexible split-apply-combine computing, reshaping, joins, rolling statistics, set operations on tables, fast csv read/write, and various utilities such as transposition of data.\n\nExample: groupby state + min, max, mean\nD[ ,.(mean = mean(measurement),\n      min = min(measurement),\n      max = max(measurement)),\n   by=state]\n\n# Supposedly faster\nrbindlist(lapply(unique(D$state), \n                 \\(x) data.table(state = x, \n                                 y[state == x, \n                                   .(mean(measurement), \n                                     min(measurement), \n                                     max(measurement))\n                                   ]\n                                 )))\n\n{rfast}: A collection of fast (utility) functions for data analysis. Column- and row- wise means, medians, variances, minimums, maximums, many t, F and G-square tests, many regressions (normal, logistic, Poisson), are some of the many fast functions\n\nThe vast majority of the functions accept matrices only, not data.frames.\nDo not have matrices or vectors with have missing data (i.e NAs). There are no checks and C++ internally transforms them into zeros (0), so you may get wrong results.\nExample: groupby state + min, max, mean\nlev_int &lt;- as.numeric(D$state)\nminmax &lt;- Rfast::group(D$measurement, lev_int, method = \"min.max\")\ndata.frame(\n    state = levels(D$state),\n    mean = Rfast::group(D$measurement, lev_int, method = \"mean\"),\n    min = minmax[1, ],\n    max = minmax[2, ]\n)\n\n{matrixStats}: Efficient row-and column-wise (weighted) statistics on matrices and vectors, including computations on subsets of rows and columns.\n{kit}: Fast vectorized and nested switches, some parallel (row-wise) statistics, and some utilities such as efficient partial sorting and unique values.\n{fst}: A compressed data file format that is very fast to read and write. Full random access in both rows and columns allows reading subsets from a ‘.fst’ file.",
    "crumbs": [
      "Big Data"
    ]
  },
  {
    "objectID": "qmd/big-data.html#sec-bgdat-lgmem",
    "href": "qmd/big-data.html#sec-bgdat-lgmem",
    "title": "Big Data",
    "section": "Larger than Memory",
    "text": "Larger than Memory\n\nOnly work with a sample of the data\n\nRandom sample in CLI\n\nSee binder for code\nAlso this snippet from Healy for a zipped csv.\n\n\nImproved version\ngzip -cd giantfile.csv.gz | (read HEADER; echo $HEADER; perl -ne 'print if (rand() &lt; 0.001)’) &gt; sample.csv\n\nRemoves the need to decompress the file twice, adds the header row, and removes the risk of a double header row\n\n\n\nOnly read the first n lines\n\nset n_max arg in readr::read_*\n\n\ndatasette.io - App for exploring and publishing data. It helps people take data of any shape, analyze and explore it, and publish it as an interactive website and accompanying API.\n\nWell documented, many plugins\n\nRill - A tool for effortlessly transforming data sets into powerful, opinionated dashboards using SQL.\n\nDocs, Example Projects\nPowered by Sveltekit & DuckDB = conversation-fast, not wait-ten-seconds-for-result-set fast\nWorks with your local and remote datasets – imports and exports Parquet and CSV (s3, gcs, https, local)\nNo more data analysis “side-quests” – helps you build intuition about your dataset through automatic profiling\nNo “run query” button required – responds to each keystroke by re-profiling the resulting dataset\nRadically simple interactive dashboards – thoughtful, opinionated, interactive dashboard defaults to help you quickly derive insights from your data\nDashboards as code – each step from data to dashboard has versioning, Git sharing, and easy project rehydration\n\nOnline duckdb shell for parquet files (gist, https://shell.duckdb.org/)\nselect max(wind) \nfrom 'https://raw.githubusercontent.com/RobinL/iris_parquet/main/gridwatch/gridwatch_2023-01-08.parquet';\n-- Takes 6 seconds on the first query, 200ms on subsequent similar queries\n\nselect * \nfrom 'https://raw.githubusercontent.com/RobinL/iris_parquet/main/NSPL/NSPL.parquet' \nwhere pcd = 'SW1A1AA';\n-- Takes 13 seconds on the first query, 100ms on subsequent similar queries\nCSV Editors\n\nFor editing or reformatting cells\nPopular spreadsheet programs like googlesheets (100MB) and excel (25MB online) have file size limits and they’re slow to upload to. The following programs are free(-ish) local alternatives only limited by your RAM.\nSuggest for files over a few hundred MBs that you open as Read-Only\n\nOpening the files as “Editable” will probably balloon the memory cost to at least 5 times the file size. (e.g. 350MB csv \\(\\rightarrow\\) 2GB RAM)\n\nModern CSV - Nice modern interface, read-only mode that can open large csvs (100s of MBs) without making much of a dent in your RAM, fully featured (moreso if you pay a small-ish one time fee)\n\nDocs, Feature free/upgrade list\nStill has some functionality in read-only mode (e.g. search, sort)\n\nOpenRefine - Has read-only, Several add-ons, Completely open source.\n\nDocs, List of Extensions\nNo functionality when read-only (must create a project to do anything) — just reading\nStarts with a 1024 MB RAM usage limit which is proably fine for editing around a 100MB csv. Need to set the limit higher in a config file in order to edit larger files.\nOnce you create a project, I think it has some editing features that you’d have to pay for with Modern CV.\nOpens other file formats besides csv (e.g. xlsx, xml, json, etc)\n\n\ncsvkit - suite of command-line tools for converting to and working with CSV\n\nInstallation docs\n\nOne of the articles your terminal has to be a bash terminal but I dunno\n\nIf so, they recommend cmder or enabling the Linux subsystem with WSL2.\n\n\nNotes from\n\nArticle with additional examples and options\n\nFeatures\n\nPrint CSV files out nicely formatted\nCut out specific columns\nGet statistical information about columns\n\nConvert excel files to CSV files:\nin2csv excel_file.xlsx &gt; new_file.csv\n# +remove .xlsx file\nin2csv excel_file.xlsx &gt; new_file.csv && rm excel_file\nSearch within columns with regular expressions:\ncsvgrep -c county -m \"HOLT\" new_file.csv\n# subset of columns (might be faster) with pretty formatting\ncsvcut -c county,total_cost new_file.csv | csvgrep -c county -m \"HOLT\" | csvlook\n\nSearches for “HOLT” in the “county” column\n\nQuery with SQL\n\nsyntax csvsql --query \"ENTER YOUR SQL QUERY HERE\" FILE_NAME.csv\nExample\n\n\nView top lines: head new_file.csv\nView columns names: csvcut -n new_file.csv\nSelect specific columns: csvcut -c county,total_cost,ship_date new_file.csv\n\nWith pretty output: csvcut -c county,total_cost,ship_date new_file.csv | csvlook\nCan also use column indexes instead of names\n\nJoin 2 files: csvjoin -c cf data1.csv data2.csv &gt; joined.csv\n\n“cf” is the common column between the 2 files\n\nEDA-type stats:\ncsvstat new_file.csv\n# subset of columns\ncsvcut -c total_cost,ship_date new_file.csv | csvstat\n\nJSONata - a lightweight, open-source query and transformation language for JSON data, inspired by the ‘location path’ semantics of XPath 3.1.\n\nMisc\n\nNotes from: Hrbrmstr’s article\nJSONata also doesn’t throw errors for non-existing data in the input document. If during the navigation of the location path, a field is not found, then the expression returns nothing.\n\nThis can be beneficial in certain scenarios where the structure of the input JSON can vary and doesn’t always contain the same fields.\n\nTreats single values and arrays containing a single value as equivalent\nBoth JSONata and jq can work in the browser (JSONata embedding code, demo), but jq has a slight speed edge thanks to WASM. However, said edge comes at the cost of a slow-first-start\n\nFeatures\n\nDeclarative syntax that is pretty easy to read and write, which allows us to focus on the desired output rather than the procedural steps required to achieve it\nBuilt-in operators and functions for manipulating and combining data, making it easier to perform complex transformations without writing custom code in a traditional programming language like python or javascript\nUser-defined functions that let us extend JSONata’s capabilities and tailor it to our specific needs\nFlexible output structure that lets us format query results into pretty much any output type\n\n\njq + jsonlite - json files\njsoncrack.com - online editor/tool to visualize nested json (or regular json)\njj - cli tool for nested json. Full support for ndjson as well as setting/updating/deleting values. Plus it lets you perform similar pretty/ugly printing that jq does.\nsqlite3 - CLI utility allows the user to manually enter and execute SQL statements against an SQLite database or against a ZIP archive.\n\nalso directly against csv files (post)\n\ntextql - Execute SQL against structured text like CSV or TSV\n\nRequire Go language installed\nOnly for Macs or running a docker image\n\ncolumnq-cli - sql query json, csv, parquet, arrow, and more\nfread + CLI tools\n\nFor large csvs and fixing large csv with jacked-up formating see article, RBlogger version\n\n{arrow}\n\nconvert file into parquet files\n\npass the file path to open_dataset, use group_by to partition the Dataset into manageable chunks\nuse write_datasetto write each chunk to a separate Parquet file—all without needing to read the full CSV file into R\n\ndplyr support\n\nmultiplyr\n\nOption for data &gt; 10M rows and you only have access to one machine\nSpreads data over local cores\n\n{sparklyr}\n\nspin up a spark cluster\ndplyr support\nSet-up a cloud bucket and load data into it. Then, read into a local spark cluster. Process data.\n\n{h2o}\n\nh2o.import_file(path=path) holds data in the h2o cluster and not in memory\n\n{disk.frame}\n\nsupports many dplyr verbs\nsupports  future package to take advantage of multi-core CPUs but single machine focused\nstate-of-the-art data storage techniques such as fast data compression, and random access to rows and columns provided by the fst package to provide superior data manipulation speeds\n\nMatrix ops\n\nsee bkmks: mathematics &gt;&gt; packages\n\n{ff}\n\nsee bkmks: data &gt;&gt; loading/saving/memory\nThink it converts files to a ff file type, then you load them and use ffapply to perform row and column operations with base R functions and expressions\nmay not handle character and factor types but may work with {bit} pkg to solve this",
    "crumbs": [
      "Big Data"
    ]
  },
  {
    "objectID": "qmd/big-data.html#sec-bgdat-viz",
    "href": "qmd/big-data.html#sec-bgdat-viz",
    "title": "Big Data",
    "section": "Viz",
    "text": "Viz\n\nScatter plots\n\n{scattermore}, {ggpointdensity}\n{ggrastr}\n\nRasterize only specific layers of a ggplot2 plot (for instance, large scatter plots with many points) while keeping all labels and text in vector format. This allows users to keep plots within a reasonable size limit without losing the vector properties of scale-sensitive information.\ngithub; tweet\n\n\nH2O\n\nh2o.aggregator Reduces data size to a representive sample, then you can visualize a clustering-based method for reducing a numerical/categorical dataset into a dataset with fewer rows A count column is added to show how many rows is represented by the exemplar row (I think)\n\nAggregator maintains outliers as outliers but lumps together dense clusters into exemplars with an attached count column showing the member points.\nFor cat vars:\n\nAccumulate the category frequencies.\nFor the top 1,000 or fewer categories (by frequency), generate dummy variables (called one-hot encoding by ML people, called dummy coding by statisticians).\nCalculate the first eigenvector of the covariance matrix of these dummy variables.\nReplace the row values on the categorical column with the value from the eigenvector corresponding to the dummy values.\n\ndocs; article\n\n\n{dbplot}\n\nplots data that are in databases\n\nAlso able to plot data within a spark cluster\n\ndocs\n\nObservableHQ\n\n{{{deepscatter}}}\n\nThread (using Arrow, duckdb)",
    "crumbs": [
      "Big Data"
    ]
  },
  {
    "objectID": "qmd/bayes-workflow.html",
    "href": "qmd/bayes-workflow.html",
    "title": "Workflow",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Bayes",
      "Workflow"
    ]
  },
  {
    "objectID": "qmd/bayes-workflow.html#sec-bayes-wkflw-misc",
    "href": "qmd/bayes-workflow.html#sec-bayes-wkflw-misc",
    "title": "Workflow",
    "section": "",
    "text": "Also see Model Building, Concepts &gt;&gt; Misc &gt;&gt; Regression Workflow\nNotes from\n\nBayesian Workflow (Gelman, Vehtari) (arXiv link)\n\nResources\n\nVehtari Video: On Bayesian Workflow (2022) (Based on the paper, but I haven’t watched it, yet)\nNabiximols treatment efficiency: Vehtari’s example of applyijng his workflow in the context of comparing continuous and discrete observation models.\n\nCurrent Checklist\n\nCheck convergence diagnostics\nDo posterior predictive checking\nCheck residual plots\nModel comparison (if prediction)\n\nAnalysis Checklist (Thread)\n\nA suitably flexible Bayesian regression adjustment model,\nChosen by cross-validation/LOO,\nIncluding Gaussian processes for the unit-level effects over time (and space/network if relevant),\nImputation of missing data, and\nInformative priors for biases in the data collection process.",
    "crumbs": [
      "Bayes",
      "Workflow"
    ]
  },
  {
    "objectID": "qmd/glossary-ds-terms.html",
    "href": "qmd/glossary-ds-terms.html",
    "title": "Glossary: DS terms",
    "section": "",
    "text": "200 Status - An API serving an ML model returns a HTTP 200 OK success status response code indicates that the request has succeeded.\nAMI - amazon machine image. Thing that has R and the main packages you need to load onto the cloud server\nAnti-Patterns - certain patterns in software development that are considered bad programming practices.\n\nAs opposed to design patterns which are common approaches to common problems which have been formalized and are generally considered a good development practice, anti-patterns are the opposite and are undesirable.\n\nArm - a group of patients receiving a specific treatment (or no treatment). Trials involving several arms, or randomized trials, treat randomly-selected groups of patients with different therapies in order to compare their medical outcomes. Experimental arms, which receive an experimental drug, are compared with control arms. Single-arm or non-randomized trials, in which everyone enrolled in a trial receives the experimental therapy\nArtifacts - objects that are created as a result of a process. e.g. model objects, cleaned data sets, visuals, etc.\nAsynchronous Programming - code runs (or must run) after something else happens and also not sequentially (e.g. when a function calls a callback function in JS).\nAthena - amazon query service that works with S3. Best for analyses using kubernetes. ODBC drivers are best with interactive app\nB2C, B2B - business-to-consumer, business-to-business, describes a business that’s end-product is being sold to a consumer or a business.\nBalanced Design (aka orthogonal) has an equal number of observations for all possible level combinations. For example in an experiment where gender is an independent variable, an equal number of males receive the treatment as do females receive treatment. If the male/female counts were unequal, then the experiment is unbalanced.\n\nStat tests have greater power for balanced designs\nTest stat less susceptible to to small departures from the assumption of equal variances (homoscedasticity).\n\nBatch - collect a large number of data points, process them periodically and store results somewhere (contrasts with real-time in which a data input leads to an immediate prediction)\nBootstrapping (CS) - usually applies to a situation where a system depends on itself to start, sort of a chicken and egg problem. (e.g. How do you start an OS initialization process if you don’t have the OS running yet?) Typically a simple file that starts a large process.\nBounce, Email - When an email cannot be delivered to an email server.\n\nHard Bounce - indicates a permanent reason an email cannot be delivered (e.g. Recipient email address doesn’t exist; Recipient email server has completely blocked delivery)\nSoft Bounce - indicates a temporary delivery issue (for details on the reasons, see link)\n\nBounce Rate - the percentage of visitors to a particular website who navigate away from the site after viewing only one page. Low bounce rate can indicate the landing page needs improvement\nBPI - Business process improvement is a management exercise in which enterprise leaders use various methodologies to analyze their procedures to identify areas where they can improve accuracy, effectiveness and/or efficiency and then redesign those processes to realize the improvements.\nBLUE - best linear unbiased estimator, e.g. regression line\nCAC - customer acquisition cost - measures how much an organization spends to acquire new customers. The total cost of sales and marketing efforts, as well as property or equipment, needed to convince a customer to buy a product or service.\nCapEx - Capital Expenditure - 1 of 2 main forward budgeting mechanisms for a corporation (also see OpEx). Often used to undertake new projects or investments or large-scale asset acquisitions (buildings and vehicles)\nClinical Trial - research studies (e.g. RCT) performed in people that are aimed at evaluating a medical, surgical, or behavioral intervention\nCDI - Customer Data Infrastructure - built to collect behavioral data from primary or first-party data sources, but some solutions also support a handful of secondary data sources (third-party tools)\nCDP - Customer Data Platform - add-ons from CDI vendors; a layer on top of CDI that offers a set of capabilities to analyze data using a visual interface.\nCDN - content delivery network - a system of distributed servers (network) that deliver pages and other web content to a user, based on the geographic locations of the user, the origin of the webpage and the content delivery server.\nCLV/CLTV - Customer Lifetime Value - how much money a customer will bring your brand throughout their entire time as a paying customer.\nCOGS - Cost of goods sold (aka Cost of Sales) - refers to the direct costs of producing the goods sold by a company. This amount includes the cost of the materials and labor directly used to create the good. It excludes indirect expenses, such as distribution costs and sales force costs.\nComplete Factorial Design - a research study involving two or more independent variables in which every possible combination of the levels of each variable is represented. For instance, in a study of two drug treatments, one (A) having two dosages and the other (B) having three dosages, a complete factorial design would pair the dosages administered to different individuals or groups of participants as follows: A1 with B1, A1 with B2, A1 with B3, A2 with B1, A2 with B2, and A2 with B3.\nCPG - Consumer packaged goods are items used daily by average consumers that require routine replacement or replenishment, such as food, beverages, clothes, tobacco, makeup, and household products.\nCPC - Cost Per Click - refers to the cost an advertiser pays each time an online user clicks on his or her digital ad\nCRM - customer relationship management i.e. customer service. Salesforce tracks this data. Example: what features your salesperson promised, and when? How much revenue you have from each customer? Or which salesperson sold the most in the past year?\ncron- standard tool used on Unix and Unix-like systems to schedule the periodic execution in the background of a command or script (like a batch script)\nCrossed Factors - when every category of one factor co-occurs in the design with every category of the other factor. In other words, there is at least one observation in every combination of categories for the two factors. (in contrast to “nested factors”). As a consequence, interaction terms involving these two factors is allowed.\nCrossover Study - A type of clinical trial in which the study participants receive each treatment in a random order. With this type of study, every patient serves as his or her own control. Crossover studies are often used when researchers feel it would be difficult to recruit participants willing to risk going without a promising new treatment.\nCross-Section Data - randomly sampled data from a population. Like a survey. Aka observational data. See experimental data for comparison.\n\nPooled - differs from panel data in that it is observations of different subjects (instead of the same subjects) in different time periods.\nRolling - both the presence of an individual in the sample and the time at which the individual is included in the sample are determined randomly.\n\nCross-Tabs - section of survey analysis where the aggregated results are broken down by demography, party affiliation, etc.\nCTA - marketing term, call-to-action. any device designed to prompt an immediate response or encourage an immediate sale; words or phrases that can be incorporated into sales scripts, advertising messages or web pages that encourage consumers to take prompt action\nCTR - click through rate: the ratio of users who click on a specific link to the number of total users who view a page, email, or advertisement. It is commonly used to measure the success of an online advertising campaign for a particular website as well as the effectiveness of email campaigns.\nCRM - Customer Relationship Management - acquiring new customers but especially about retaining existing ones\nDAU - daily active users, ex: daily avg # of registered users of the site over past 30 days\nDBA - Database Administrator is an admin role that understands the particular database technology and how to get the best out of it. This includes improving performance, backups and recovery.\nDDL - Data definition or description language - Subset of SQL. Used to:\n\nKeep a snapshot of the database structure\nSet up a test system where the database acts like the production system but contains no data\nProduce templates for new objects that you can create based on existing ones. For example, generate the DDL for the Customer table, then edit the DDL to create the table Customer_New with the same schema.\n\nDesparate Impact Analysis - Analysis of the result of the application of a standard, requirement, test or other screening tool used for selection that—though appearing neutral—has an adverse effect on individuals who belong to a legally protected class Differential Dropout**]{style=‘color: #009499’} - Differing dropout rates between treatment arms\nDMA - Designated Market Area; a geographic region where Nielsen, the ratings company, analyzes and quantifies how television is viewed. Residents can receive the same local TV and radio stations\nDNS -  Domain Name System**]{style=‘color: #009499’} -  translates domain names to IP addresses so browsers can load Internet resources.\nDSL - domain-specific language - a computer language specialized to a particular application domain\nEMR - Amazon version of a spark cluster used for big data processing and analysis.\nEndogenous - A model variable is correlated with other variables excluded from the model (omitted variable bias). Determined by measuring the correlation between the variable and residuals of the model. If a predictor variable hasn’t been randomly assigned, it’s likely to be endogenous.\nEquitability - concept that says a dependence measure should give equal importance to linear and nonlinear relationships. Consistent strength measurements across different variable relationships that have similar amounts of noise.\nERP - enterprise resource planning, sort of a catch-all for manufacturing, supply-chain, etc, see the wiki\nETL - extract, transfer, load - usually refers to transferring data from one location to another\nEndpoint (biostats) - Outcome variable measured in a medical study. e.g. Death, stroke, or quality of life are good endpoints. Blood tests and images on scans are not good endpoints.\n\nA composite endpoint is one that consists of two or more events\n\nExample: death due to cardiovascular causes or hospitalization due to heart failure\n\nSo the binary outcome would be a 1 if either of those events took place or a 0 if they did not. Or in a survival model, time until either of those events.\n\n\n\nEOF - End of file - Input from a terminal never really “ends” (unless the device is disconnected), but it is useful to enter more than one “file” into a terminal, so a key sequence is reserved to indicate end of input.\nex ante - based on assumption and prediction and being essentially subjective and estimative\nex post - based on knowledge and retrospection and being essentially objective and factual\nExperimental Data - data from a RCE/RCT. Compare with observational data\nFaaS - Function as a service - type of cloud service for developing, running, and managing apps (e.g. AWS Lambda)\nFactorial Design - Experiment where you’re interested in the effect of two or more independent variables.\nFraud Rules - fraud scores are calculated based on rules, which add or subtract points. The user action may be a transaction, signup or login. Rules look at data points such as an email address, IP address, or social media presence.\nFraud Score - assigned values to how risky a user action is. Scoring determined by fraud rules.\nFuzzy Design - See Sharp Design\nGHA - Github Actions\nGMV - Gross merchandises value - the total value of merchandise sold over a given period of time through a customer-to-customer (C2C) exchange site\nGRP - Gross Rating Point. A standard measure in advertising, it measures advertising impact. You calculate it as a percent of the target market reached multiplied by the exposure frequency. Thus, if you get advertise to 30% of the target market and give them 4 exposures, you would have 120 GRP.\nHTE - Heterogeneous Treatment Effect - Also called differential treatment effect, includes difference of means, odds ratios, and Hazard ratios for time-to-event outcome vars\n\nAscertaining subpopulations for which a treatment is most beneficial (or harmful) is an important goal of many clinical trials.\nOutcome heterogeneity is due to wide distributions of baseline prognostic factors. When strong risk factors exist, there is hetergeneity in the outcome variable.\n\nSolution: add baseline predictors to your model that account for these strong risk factors.\n\nHeterogeneity of Treatment Effects - The degree to which different treatments have differential causal effects on each unit.\n\nHit Ratio - percent of records that were read in order to complete a query in a database. Cloud db providers often charge by the number of records searched\nHomogeneity of Treatment Effects - See Heterogeneity of Treatment Effects\nHPC - High Performance Computing\nHoneypot - data (for example, in a network site) that appears to be a legitimate part of the site, but is actually isolated and monitored, and that seems to contain information or a resource of value to attackers, who are then blocked.\nIaaS - infrastructure-as-a-service ( Hardware is provided by an external provider and managed for you)\nIAM - identity and access management, keys and passwords etc\nIRB - institutional review board, reviews studies ethical and moral issues\nITT - Intent-to-Treat analysis includes all randomized patients in the groups to which they were randomly assigned, regardless of their adherence with the entry criteria, regardless of the treatment they actually received, and regardless of subsequent withdrawal from treatment or deviation from the protocol. Avoids overoptimistic estimates of the efficacy of an intervention resulting from the removal of non-compliers by accepting that noncompliance and protocol deviations are likely to occur in actual clinical practice. So mimics likely situation in the real world, but not good for estimating the causal effect of a treatment.\nKernels - (article) - system kernels - the interface between the operating system, i.e. the software, and the hardware components in a device. It is used in all devices with an operating system, for example, computers, laptops, smartphones, smartwatches, etc.\n\nWhen we use a program on a computer, such as Excel, we handle it on the so-called Graphical User Interface (GUI). The program converts every button click or other action into machine code and sends it to the operating system kernel. If we want to add a new column in an Excel table, this call goes to the system core. This in turn passes the call on to the computer processing unit (CPU), which executes the action.\nJupyter Kernels - an engine that executes notebook code and is specific to a particular programming language (e.g. python kernel)\nKaggle Kernels - a free platform from Kaggle to run Jupyter notebooks in the browser. Advantage is that you don’t have to set-up an environment locally.\n\nKPI- key performance indicator\nKYC - Know-Your-Customer is info a company collects to verify your identity to combat fraud. Used by telecoms and financial services\nLazy Evaluation - ” never pulls data into R unless you explicitly ask for it. It delays doing any work until the last possible moment. It collects together everything you want to do and then sends it to the database in one step.”\nLikelihood - probability of seeing this data given a specific value for a distribution parameter (eg mean, sd). Goal is to search for parameter values until the likelihood is maximized.\nLOB - Line of Business is a general term which refers to a product or a set of related products that serve a particular customer transaction or business need. (i.e. product categories)\n\nExamples\n\nConsumer Banking: credit cards, line of credit or loan program, mortgages, and corporate, small business and personal bank accounts.\nFinancial services and brokerages: mergers and acquisitions or partnerships, real estate investments, and wealth management\nProperty and casualty insurance companies: property and casualty insurance (i.e., homeowners, car, boat, renters, etc.), life insurance, health insurance, and commercial business insurance.\n\nSub-lines of Business would be sub-categories within each LOB\n\nLongitudinal Data - see panel data\nLTV - see CLV/CLTV\nManual Review - A human is reviews the case to determine whether action is needed. In fraud, an model output may trigger a “manual review” to determine whether an event was indeed fraudulent.\nMLlib - Apache Spark machine learning library\nMVC - Minimum Viable Corpus - a data size threshold; such that below this threshold, the data simply isn’t useful/valuable. Used in data products business.\nMVP - minimum viable project, agile term. Version of a new product which allows a team to collect the maximum amount of validated learning about customers with the least effort\nNamespace - allows you to use two functions with the same name but from different packages, e.g. dplyr::select or in general, package::function. https://stackoverflow.com/questions/3384204/what-are-namespaces/3384384#3384384\nNNH - Numbers Needed to Harm - a derived statistic that tells us how many patients must receive a particular treatment for 1 additional patient to experience a particular adverse outcome. Lower NNT and higher NNH values are associated with a more favorable treatment profile.\nNNT - Numbers Needed to Treat - a derived statistic that tells us how many patients must receive a particular treatment for 1 additional patient to experience a favorable outcome such as treatment response. Lower NNT and higher NNH values are associated with a more favorable treatment profile.\nNPS - Net Promoter Score - a measure of customer loyalty. Widely used market research metric that typically takes the form of a single survey question asking respondents to rate the likelihood that they would recommend a company, product, or a service to a friend or colleague.\nNRT - near real-time, aka streaming data\nObservational Data - see cross sectional data\nOEM - original equipment manufacturer\nOKR - Objectives and Key Results is a popular management strategy for goal setting within organizations. A framework for turning strategic intent into measurable outcomes for an organization.\nOnline Machine Learning - A method of machine learning where the model incrementally learns from a stream of data points in real-time. It’s a dynamic process that adapts its predictive algorithm over time, allowing the model to change as new data arrives.\nOn-Prem - on-premises — working with servers in the the building and not in the cloud.\nOOD - out-of-distribution - data which differ from the training data and on which a model might underperform\nOpen Cohort - subjects can leave or be added over time.\nOpEx - Operational Expenditures - 1 of 2 main forward budgeting mechanisms for a corporation (also see CapEx). Relates to day-to-day expenses (such as payroll and software subscriptions). Smaller payouts over time.\nOpportunity Sizing - Quantitative analysis to select a subset of ideas to which to devote resources in product development\nNested Factors - happens when all the levels of one factor only occur in combination with one level of another factor (in contrast to “crossed factors”). As a consequence, your model can’t have an interaction term involving these two variables.\nP&L - Profit and Loss Statement Panel data - cross section data with a time element. Repeated measures of the same subject over time. Synonym for Longitudinal Data\nParcel - a land record that defines the boundary of a piece of land. These boundaries are the basic administrative unit of local government in regards to land and property. Managing ownership and tax records are the primary reason local governments generate these files. So these are boundaries differentiating ownership of properties.\nPEP8 - style guide for python\nPI - principal investigator\nPivot Table - Excel name for a group_by %\\&gt;% summarize calculation\n\ne.g. from a table of individual fruit sales: group_by(fruit_type, country) %\\&gt;% summarize(total_amt = sum(amount))\n\nPLG - Product-led growth is an end user-focused growth model that relies on the product itself as the primary driver of customer acquisition, conversion, and expansion. e.g. open source a product, let the customer go through the documentation and use and experiment with the product on their own time. In contrast to sales pitching a product to a customer and letting them use it for a trial basis.\nPM - product manager\nPoC - Proof of Concept\nPOS - point of sale, The point of sale or point of purchase is the time and place where a retail transaction is completed. It can be in a physical store, where POS terminals and systems are used to process card payments or a virtual sales point such as a computer or mobile electronic device.\nRCE - randomized controlled experiment, subjects randomly assigned to two groups, treatment and control. Double blind means the researcher doesn’t know who is in which group.\nRCT - randomized clinical trial\nRDD - Regression discontinuity design\nRedis - REmote DIctionary Server - is an in-memory, key-value database, commonly referred to as a data structure server. Used when volume of read and write operations exceed the capabilities of traditional databases. With Redis’s capability to easily persist the data to disk, it is a superior alternative to the traditional memcached solution for caching.\nRefactoring - updating or optimizing code\nRegression Testing - checks if changes made to a system negatively impacted or broke any of the existing features. It is often performed right after each update or commit to the code base to identify new bugs and ensure that your system works properly.\nRFI - Request for Information - Used to collect written information about the capabilities of various suppliers. Normally it follows a format that can be used for comparative purposes. An RFI is primarily used to gather information to help make a decision on what steps to take next. RFIs are therefore seldom the final stage and are instead often used in combination with request for proposal (RFP), request for tender (RFT), and request for quotation (RFQ).\nRFM - recency, frequency, monetary value - method of estimating customer value; common in retail\nRFP - Request for Proposal - A document that an organization, often a government agency or large enterprise, posts to elicit a response – a formal bid – from potential vendors for a desired solution. The RFP specifies what the customer is looking for and describes each evaluation criterion on which a vendor’s proposal will be assessed.\n\nROAS - return on ad spend\nRUG - Regional User Group\nS3 - Amazon simple storage service, database\nSaaS - Software-as-a-service is a mechanism through which companies offer the functionality of their apps, which remain on their company servers, to other companies or customers.\nSCO - sales cycle optimization, active process of providing content on your site (and beyond) that speaks to each of the key phases\nSEO - Search engine optimization, generating high page rankings for key search terms\nSDK - software development kit\nSharp Design - Each individual or group receives the same “amount” of treatment (e.g. a state law or medication dosage). Opposite being fuzzy design (?)\nSKU - Stock Keeping Unit**]{style=‘color: #009499’} - Usually a bar code that has all the information to distinguish it from another product. These attributes can include manufacturer, description, material, size, color, packaging, and warranty terms. When a business takes inventory of its stock, it counts the quantity it has of each SKU.\nSLA - service level agreement - a contract between a service provider and its internal or external customers that documents what services the provider will furnish and defines the service standards this provider is obligated to meet. service. Important for holding prediction latency of an app to a certain standard or maintaining data reliability with vendors. (see link for more details on SLA, SLO, and SLI)\nSLI - service level indicators - metrics that measure compliance with an SLO (see link for more details on SLA, SLO, and SLI)\nSLO - service level objectives - objectives your team must meet in order to meet the conditions of the SLA (see link for more details on SLA, SLO, and SLI)\nSMB - (small to medium-sized business) generally defined as companies with fewer than 1000 employees and less than $1 billion in annual revenue.\nSME - Subject Matter Experts\nSPC - Statistical process control is a method of quality control which employs statistical methods to monitor and control a process\nSpill - missed opportunity metric, measures “lost trading days” on which flights or hotels filled too quickly (the result of pricing too low)\nSpoil - missed opportunity metric, measures empty seats or rooms (often the result of pricing too high)\nSSH - secure shell is a cryptographic Network protocol for operating Network Services securely over an unsecured Network. Typical applications include remote command line login in remote command execution\nstdout - standard output, which is the terminal by default\nTDD - Test-driven development is a style of programming where coding, testing, and design are tightly interwoven\nTF-IDF- stands for term frequency-inverse document frequency, and is often used in information retrieval and text mining.\nThroughput - the amount of material or items passing through a system or process.\ntx - treatment, seen as variable with different treatments as values\nURI - Uniform Resource Identifier - a string of characters that unambiguously identifies a particular resource. e.g. s3//bucket/path/to/folder or http://127.0.0.1:5000or c:\\Users\\me\\path\\to\\folder\nUTM - Urchin Traffic Monitor - used to identify marketing channels\n\ne.g. http://yourwebsite.com/your-post-title/?utm_source=google\n\nutm code = string after “?”\n\nThis person clicked a google ad to get to your site\n\nName comes from Urchin Tracker, a web analytics software that served as the base for Google Analytics.\n\nVPS - virtual private server\nWIP - Work-in-Progress\nWithin Person Study - multiple treatments on each person either all in the same period or different treatments in different periods\nYear-Over-Year - used to make comparisons between one time period and another that is one year earlier.\n\nFormula (percentage): (value_this_year / value_previous_year) - 1\nExample: (sales_Jul_2023 / sales_Jul_2022) - 1",
    "crumbs": [
      "Glossary: DS terms"
    ]
  }
]