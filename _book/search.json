[
  {
    "objectID": "qmd/surveys-census-data.html",
    "href": "qmd/surveys-census-data.html",
    "title": "Census Data",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Surveys",
      "Census Data"
    ]
  },
  {
    "objectID": "qmd/surveys-census-data.html#sec-surv-cens-misc",
    "href": "qmd/surveys-census-data.html#sec-surv-cens-misc",
    "title": "Census Data",
    "section": "",
    "text": "Notes from\n\nTidycensus Workshop 2024\n\nResources\n\nAnalyzing Census Data: Methods, Maps, and Models in R by Kyle Walker\n\nOther Surveys\n\nAmerican Housing Survey (AHS)\n\nZillow also publishes a bunch of housing data (See Real Estate &gt;&gt; Features)\n\nGeneral Social Survey (GSS)\n\nConducted since 1972. Contains a standard core of demographic, behavioral, and attitudinal questions, plus topics of special interest. Among the topics covered are civil liberties, crime and violence, intergroup tolerance, morality, national spending priorities, psychological well-being, social mobility, and stress and traumatic events.\nPackages\n\n{gssr} - Convenient interface that provides access to GSS data\n{gssrdoc} - Companion package to {gssr}\n\nProvides documentation for all GSS variables in the cumulative data file via R‚Äôs help system.\nBrowse variables by name in the package‚Äôs help file or type ? followed by the name of the variable at the console to get a standard R help page containing information on the variable, the values it takes and (in most cases) a crosstabulation of the variable‚Äôs values for each year of the GSS.\n\n\n\n\nFor details on joining census data to other data, see Chapter 7.2 in Analyzing Census Data\nFIPS GEOID\n\nCensus Geocoder (link)\n\nEnter an address and codes for various geographies are returned\nBatch geocoding available for up to 10K records\n\nCodes for geographies returned in a .csv file\n\n\nTIGERweb (link)\n\nAllows you to get geography codes by searching for an area on a map\nOnce zoomed-in on your desired area, you turn on geography layers to find the geography code for your area.\n\nUS Census Regions",
    "crumbs": [
      "Surveys",
      "Census Data"
    ]
  },
  {
    "objectID": "qmd/surveys-census-data.html#sec-surv-cens-geo",
    "href": "qmd/surveys-census-data.html#sec-surv-cens-geo",
    "title": "Census Data",
    "section": "Geographies",
    "text": "Geographies\n\n\nMisc\n\n{tidycensus} docs on various geographies, function arguments, and which surveys (ACS, Census) they‚Äôre available in.\nACS Geography Boundaries by Year (link)\n\nTypes\n\nLegal/Administrative\n\nCensus gets boundaries from outside party (state, county, city, etc.)\ne.g.¬†election areas, school districts, counties, county subdivisions\n\nStatistical\n\nCensus creates these boundaries\ne.g.¬†regions, census tracts, ZCTAs, block groups, MSAs, urban areas\n\n\nNested Areas\n\n\nCensus Tracts\n\nAreas within a county\nAround 1200 to 8000 people\nSmall towns, rural areas, neighborhoods\n** Census tracts may cross city boundaries **\n\nBlock Groups\n\nAreas within a census tract\nAround 600 to 3000 people\n\nCensus Blocks\n\nAreas within a block group\nNot for ACS, only for the 10-yr census\n\n\nPlaces\n\nMisc\n\nOne place cannot overlap another place\nExpand and contract as population or commercial activity increases or decreases\nMust represent an organized settlement of people living in close proximity.\n\nIncorporated Places\n\ncities, towns, villages\nUpdated through Boundary and Annexation Survey (BAS) yearly\n\nCensus Designated Places (CDPs)\n\nAreas that can‚Äôt become Incorporated Places because of state or city regulations\nConcentrations of population, housing, commericial structures\nUpdated through Boundary and Annexation Survey (BAS) yearly\n\n\nCounty Subdivisions\n\nMinor Civil Divisions (MCDs)\n\nLegally defined by the state or county, stable entity. May have elected government\ne.g.¬†townships, charter townships, or districts\n\nCensus County Divisions (CCDs)\n\nno population requirment\nSubcounty units with stable boundaries and recognizable names\n\n\nZip Code Tabulation Areas (ZCTAs)\n\n\nMisc\n\nRegular zip codes are problematic ‚Äî can cross state lines.\n{crosswalkZCTA} - Contains the US Census Bureau‚Äôs 2020 ZCTA to County Relationship File, as well as convenience functions to translate between States, Counties and ZIP Code Tabulation Areas (ZCTAs)\n\nApproximate USPS Code distribution for housing units\n\nThe most frequently occurring zip code within an census block is assigned to a census block\nThen blocks are aggregated into areas (ZCTAs)\n\nZCTAs do NOT nest within any other geographies\n\nI guess the aggregated ZCTA blocks can overlap block groups\n\n2010 ZCTAs exclude large bodies of water and unpopulated areas",
    "crumbs": [
      "Surveys",
      "Census Data"
    ]
  },
  {
    "objectID": "qmd/surveys-census-data.html#sec-surv-cens-acs",
    "href": "qmd/surveys-census-data.html#sec-surv-cens-acs",
    "title": "Census Data",
    "section": "American Community Survey (ACS)",
    "text": "American Community Survey (ACS)\n\nMisc\n\nDefault MOE is a 90%CI\nPopular variable calculations from variables in ACS\nFor variables, vars &lt;- load_variables(2022, \"acs5\")\n\nFor the 2022 5-year ACS,\n\n\"acs5\" for the Detailed Tables;\n\"acs5/profile\" for the Data Profile; aggregated statistics for acs5\n\np (suffix): Percentage with appropriate denominator\n\n\"acs5/subject\" for the Subject Tables; and\n\"acs5/cprofile\" for the Comparison Profile\n\nGeographies only shown for 5-year ACS\n\n\nAbout\n\nYearly estimates based on samples of the population over a 5yr period\n\nTherefore a Margin of Error (MoE) is included with the estimates.\n\nAvailable as 1-year estimates (for geographies of population 65,000 and greater) and 5-year estimates (for geographies down to the block group)\nDetailed social, economic, housing, and demographic characteristics. Variables covering e.g.¬†income, education, language, housing characteristics\ncensus.gov/acs\n\nACS Release Schedule (releases)\n\nSeptember - 1-Year Estimates (from previous year‚Äôs collection)\n\nEstimates for areas with populations of &gt;65K\n\nOctober - 1-Year Supplemental Estimates\n\nEstimates for areas with populations between 20K-64999\n\nDecember - 5-Year Estimates\n\nEstimates for areas including census tract and block groups\n\n\nData Collected\n\nPopulation\n\nSocial\n\nAncestry, Citizenship, Citizen Voting Age¬† Population, Disability, Education Attainment, Fertility, Grandparents, Language, Marital Status, Migration, School Enrollment, Veterans\n\nDemographic\n\nAge, Hispanic Origin, Race, Relationship, Sex\n\nEconomic\n\nClass of worker, Commuting, Employment Status, Food Stamps (SNAP), Health Insurance, Hours/Week, Weeks/Year, Income, Industry & Occupation\n\n\nHousing\n\nComputer & Internet Use, Costs (Mortgage, Taxes, Insurance), Heating Fuel, Home Value, Occupancy, Plumbing/Kitchen Facilities, Structure, Tenure (Own/Rent), Utilities, Vehicles, Year Built/Year Movied In\n\n\nExample: Median Household Income for Texas Counties\ntexas_income &lt;- get_acs(\n  geography = \"county\",\n  variables = \"B19013_001\", # median household income\n  state = \"TX\",\n  year = 2022\n)\n\nDefault MOE is a 90%CI (i.e.¬†estimate \\(\\pm\\) MOE)\n\nExample: Census Tract for Multiple Counties in NY\nnyc_income &lt;- get_acs(\n  geography = \"tract\",\n  variables = \"B19013_001\",\n  state = \"NY\",\n  county = c(\"New York\", \"Kings\", \"Queens\",\n             \"Bronx\", \"Richmond\"),\n  year = 2022,\n  geometry = TRUE\n)\nmapview(nyc_income, zcol = \"estimate\")\nExample: Multiple Races Percentages for San Diego County\n\nsan_diego_race_wide &lt;- get_acs(\n  geography = \"tract\",\n  variables = c(\n    Hispanic = \"DP05_0073P\",\n    White = \"DP05_0079P\",\n    Black = \"DP05_0080P\",\n    Asian = \"DP05_0082P\"\n  ),\n  state = \"CA\",\n  county = \"San Diego\",\n  geometry = TRUE,\n  output = \"wide\",\n  year = 2022\n)\nfaceted_choro &lt;- ggplot(san_diego_race, aes(fill = estimate)) + \n  geom_sf(color = NA) + \n  theme_void() + \n  scale_fill_viridis_c(option = \"rocket\") + \n  facet_wrap(~variable) + \n  labs(title = \"Race / ethnicity by Census tract\",\n       subtitle = \"San Diego County, California\",\n       fill = \"ACS estimate (%)\",\n       caption = \"2018-2022 ACS | tidycensus R package\")\n\nAllows for comparison, but for groups with less variation as compared to other groups since scaled according to all groups\n\nYou‚Äôd want to make a separate map for the Black population in order to compare variation between counties.\n\n\nMigrations Flows\n\nExample:\nfulton_inflow &lt;- \n  get_flows(\n    geography = \"county\",\n    state = \"GA\",\n    county = \"Fulton\",\n    geometry = TRUE,\n    year = 2020\n  ) %&gt;%\n  filter(variable == \"MOVEDIN\") %&gt;%\n  na.omit()\n\nfulton_top_origins &lt;- \n  fulton_inflow %&gt;%\n    slice_max(estimate, \n              n = 30) \n\nlibrary(rdeck)\n\nSys.getenv(\"MAPBOX_ACCESS_TOKEN\")\n\nfulton_top_origins$centroid1 &lt;- \n  st_transform(fulton_top_origins$centroid1, 4326)\nfulton_top_origins$centroid2 &lt;- \n  st_transform(fulton_top_origins$centroid2, 4326)\n\nflow_map &lt;- \n  rdeck(\n    map_style = mapbox_light(), \n    initial_view_state = view_state(center = c(-98.422, 38.606), \n                                    zoom = 3, \n                                    pitch = 45)\n  ) %&gt;%\n  add_arc_layer(\n    get_source_position = centroid2,\n    get_target_position = centroid1,\n    data = as_tibble(fulton_top_origins),\n    get_source_color = \"#274f8f\",\n    get_target_color = \"#274f8f\",\n    get_height = 1,\n    get_width = scale_linear(estimate, range = 1:5),\n    great_circle = TRUE\n  )\n\nWidth of lines is scaled to counts",
    "crumbs": [
      "Surveys",
      "Census Data"
    ]
  },
  {
    "objectID": "qmd/surveys-census-data.html#sec-surv-cens-dic",
    "href": "qmd/surveys-census-data.html#sec-surv-cens-dic",
    "title": "Census Data",
    "section": "Dicennial US Census",
    "text": "Dicennial US Census\n\nMisc\n\nA complete count ‚Äî not based on samples like the ACS\nApplies differential privacy to preserve respondent confidentiality\n\nAdds noise to data. Greater effect at lower levels (i.e.¬†block level)\nThe exception is that is no differetial privacy for household-level data.\n\n\n\n\nPL94-171\n\nPopulation data which the government needs for redistricting\nsumfile = ‚Äúpl‚Äù\nState Populations\npop20 &lt;- \n  get_decennial(\n    geography = \"state\",\n    variables = \"P1_001N\",\n    year = 2020\n  )\n\nFor 2020, default is sumfile = ‚Äúpl‚Äù\n\n\n\n\nDHC\n\nAge, Sex, Race, Ethnicity, and Housing Tenure (most popular dataset)\nsumfile = ‚Äúdhc‚Äù\nCounty\ntx_population &lt;- \n  get_decennial(\n    geography = \"county\",\n    variables = \"P1_001N\",\n    state = \"TX\",\n    sumfile = \"dhc\",\n    year = 2020\n  )\nCensus Block (analogous to a city block)\nmatagorda_blocks &lt;- \n  get_decennial(\n    geography = \"block\",\n    variables = \"P1_001N\",\n    state = \"TX\",\n    county = \"Matagorda\",\n    sumfile = \"dhc\",\n    year = 2020\n  )\n\n\n\nDemographic Profile\n\nPretabulated percentages from dhc\nsumfile = ‚Äúdp‚Äù\n\nTabulations for 118th Congress and Island Areas (i.e.¬†Congressional Districts)\n\nsumfile = ‚Äúcd118‚Äù\n\n\nC suffix variables are counts while P suffix variables are percentages\n\n0.4 is 0.4% not 40%\n\nExample: Same-sex married and partnered in California by County\nca_samesex &lt;- \n  get_decennial(\n    geography = \"county\",\n    state = \"CA\",\n    variables = c(married = \"DP1_0116P\",\n                  partnered = \"DP1_0118P\"),\n    year = 2020,\n    sumfile = \"dp\",\n    output = \"wide\"\n  )\n\n\n\nDetailed DHC-A\n\nDetailed demographic data; Thousands of racial and ethnic groups; Tabulation by sex and age.\nDifferent groups are in different tables, so specific groups can be hard to locate.\nAdaptive design means the demographic group (i.e.¬†variable) will only be available in certain areas. For privacy, data gets supressed when the area has low population.\n\nThere‚Äôs typically considerable sparsity especially when going down census tract\n\nArgs\n\nsumfile = ‚Äúddhca‚Äù\npop_group - Population group code (See get_pop_groups below)\n\n‚Äúall‚Äù for all groups\npop_group_label = TRUE - Adds group labels\n\n\nget_pop_groups(2020, \"ddhca\") - Gets group codes for ethnic groups\n\nFor various groups there could be at least two variables (e..g Somaili, Somali and any combination)\nFor time series analysis, analagous groups to 2020‚Äôs for 2000 is SF2/SF4 and for 2010 is SF2. (SF stands for Summary File)\n\ncheck_ddhca_groups - Checks which variables are available for a specific group\n\nExample: Somali\ncheck_ddhca_groups(\n  geography = \"county\", \n  pop_group = \"1325\", \n  state = \"MN\", \n  county = \"Hennepin\"\n)\n\nExample: Minnesota group populations\nload_variables(2020, \"ddhca\") %&gt;% \n  View()\nmn_population_groups &lt;- \n  get_decennial(\n    geography = \"state\",\n    variables = \"T01001_001N\", # total population\n    state = \"MN\",\n    year = 2020,\n    sumfile = \"ddhca\",\n    pop_group = \"all\", # for all groups\n    pop_group_label = TRUE\n  )\n\nIncludes aggregate categories like European Alone, Other White Alone, etc., so you can‚Äôt just aggregate the value column to get the total population in Minnesota.\n\nSo, in order to calculate ethnic group ratios of the total state or county, etc. population, you need to get those state/county totals from other tables (e.g.¬†PL94-171)\n\n\nUse dot density and not chloropleths to visualize these sparse datasets\n\nExample: Somali populations by census tract in Minneapolis\n\nhennepin_somali &lt;- \n  get_decennial(\n    geography = \"tract\",\n    variables = \"T01001_001N\", # total population\n    state = \"MN\",\n    county = \"Hennepin\",\n    year = 2020,\n    sumfile = \"ddhca\",\n    pop_group = \"1325\", # somali\n    pop_group_label = TRUE,\n    geometry = TRUE\n  )\n\nsomali_dots &lt;- \n  as_dot_density(\n    hennepin_somali,\n    value = \"value\", # column name which is by default, \"value\"\n    values_per_dot = 25\n  )\n\nmapview(somali_dots, \n        cex = 0.01, \n        layer.name = \"Somali population&lt;br&gt;1 dot = 25 people\",\n        col.regions = \"navy\", \n        color = \"navy\")\n\nvalues_per_dot = 25 says make each dot worth 25 units (e.g.¬†people or housing units)\n\n\n\n\n\nTime Series Analysis\n\n{tidycensus} only has 2010 and 2020 censuses\n\nSee https://nhgis.org for older census data\n\nIssue: county names and boundaries change over time (e.g.¬†Alaska redraws a lot)\n\nCensus gives a different GeoID to counties that get renamed even though they‚Äôre the same county.\nNA values showing up after you calculate how the value changes over time is a good indication of this problem. Check for NAs: filter(county_change, is.na(value10))\n\nExample: Join 2010 and 2020 and Calculate Percent Change\ncounty_pop_10 &lt;- \n  get_decennial(\n    geography = \"county\",\n    variables = \"P001001\", \n    year = 2010,\n    sumfile = \"sf1\"\n  )\n\ncounty_pop_10_clean &lt;- \n  county_pop_10 %&gt;%\n    select(GEOID, value10 = value) \n\ncounty_pop_20 &lt;- \n  get_decennial(\n    geography = \"county\",\n    variables = \"P1_001N\",\n    year = 2020,\n    sumfile = \"dhc\"\n  ) %&gt;%\n    select(GEOID, NAME, value20 = value)\n\ncounty_joined &lt;- \n  county_pop_20 %&gt;%\n    left_join(county_pop_10_clean, by = \"GEOID\") \n\ncounty_joined\n\ncounty_change &lt;- \n  county_joined %&gt;%\n    mutate( \n      total_change = value20 - value10, \n      percent_change = 100 * (total_change / value10) \n    ) \nExample: Age distribution over time in Michigan\n\n\nCode available in the github repo or R/Workshops/tidycensus-umich-workshop-2024-main/census-2020/bonus-chart.R\nDistribution shape remains pretty much the same, but decreasing for most age cohorts, i.e.¬†people are leaving the state across most age groups.\n\ne.g.¬†The large hump representing the group of people in there mid-40s in 2000 steadily decreases over time.",
    "crumbs": [
      "Surveys",
      "Census Data"
    ]
  },
  {
    "objectID": "qmd/surveys-census-data.html#sec-surv-cens-dic-tidyc",
    "href": "qmd/surveys-census-data.html#sec-surv-cens-dic-tidyc",
    "title": "Census Data",
    "section": "tidycensus",
    "text": "tidycensus\n\nGet an API key\n\nRequest a key, then activate the key from the link in your email.(https://api.census.gov/data/key_signup.html)\n\nRequired for hitting the census API over 500 times per day which isn‚Äôt as hard as you‚Äôd think.\n\nSet as an environment variable: census_api_key(\"&lt;api key&gt;\", install = TRUE)\n\nOr add this line to .Renviron file, CENSUS_API_KEY=‚Äò&lt;api key‚Äô\n\n\nSearch Variables\n\nColumns\n\nName - ID of the variable (Use this in the survey functions)\nLabel - Detailed description of the variable\nContext - Subject of the table that the variable is located in.\n\nPrefixes (Variables can have combinations of prefixes)\n\nP: i.e.¬†Person; Data available at the census block and larger\nCT: Data available at the census track and larger\nH: Data available at the Housing Unit level\n\nI think housing unit is an alternatve unit. So instead of the unit being a person, which I assume is the typical unit, it‚Äôs a housing unit (~family).\nNot affected by Differential Privacy (i.e.¬†no noise added; true value)\nExample: Total Deleware housing units at census block level\ndp_households &lt;- \n      get_decennial(\n            geography = \"block\",\n            variables = \"H1_001N\",\n            state = \"DE\",\n            sumfile = \"dhc\",\n            year = 2020\n      )\n\n\nExample: DHC data in census for 2020\n\nvars &lt;- load_variables(2020, \"dhc\")\n\nView(vars)\n\nView table, click filter, and then search for parameters (e.g.¬†Age, Median, etc.) with the Label, Context boxes, and overall search box\n\n\nsummary_var - Argument for supplying an additional variable that you need to calculate some kind of summary statistic\n\nExample: Race Percentage per Congressional District\n\nrace_vars &lt;- c(\n  Hispanic = \"P5_010N\", # all races identified as hispanic\n  White = \"P5_003N\", # white not hispanic\n  Black = \"P5_004N\", # black not hispanic\n  Native = \"P5_005N\", # native american not hispanic\n  Asian = \"P5_006N\", # asian not hispanic\n  HIPI = \"P5_007N\" # hawaiian, islander not hispanic\n)\n\ncd_race &lt;- \n  get_decennial(\n    geography = \"congressional district\",\n    variables = race_vars,\n    summary_var = \"P5_001N\", # total population for county\n    year = 2020,\n    sumfile = \"cd118\"\n)\n\ncd_race_percent &lt;- \n  cd_race %&gt;%\n    mutate(percent = 100 * (value / summary_value)) %&gt;% \n    select(NAME, variable, percent)\n\ngeometry = TRUE- Joins shapefile with data and returns a SF (Simple Features) dataframe for mapping\n\nMisc\n\nYou can create a discrete color palette with the at argument in the mapview function.\n\nExample\n# check min and max of your data to select range of bins\nmin(iowa_over_65, na.rm = TRUE) # 0\nmax(iowa_over_65, na.rm = TRUE) # 38.4\n\nm1 &lt;- \n  mapview(iowa_over_65, \n          zcol = \"value\",\n          layer.name = \"% age 65 and up&lt;br&gt;Census tracts in Iowa\",\n          col.regions = inferno(100, direction = -1),\n          at = c(0, 10, 20, 30, 40))\n\nThis will result in a discrete palette with bins of 0-10, 10-20, etc. Looks like an overlap, so I‚Äôm sure which bin contains the endpoints.\n\n\n\nExample: Over 65 in Iowa by census tract\n\nlibrary(mapviw); library(viridisLite)\n\niowa_over_65 &lt;- \n  get_decennial(\n    geography = \"tract\",\n    variables = \"DP1_0024P\",\n    state = \"IA\",\n    geometry = TRUE,\n    sumfile = \"dp\",\n    year = 2020\n  )\nm1 &lt;- \n  mapview(iowa_over_65, zcol = \"value\",\n          layer.name = \"% age 65 and up&lt;br&gt;Census tracts in Iowa\",\n          col.regions = inferno(100, direction = -1))\n\n{mapview} is interactive and great for exploration of data\n\nExport as an HTML file\nhtmlwidgets::saveWidget(m1@map, \"iowa_over_65.html\")\n\nCan embed it elsewhere (html report or website) by adding it as an asset\n\nIn {ggplot}\ntexas_income_sf &lt;- get_acs(\n  geography = \"county\",\n  variables = \"B19013_001\",\n  state = \"TX\",\n  year = 2022,\n  geometry = TRUE\n)\n\nplot(texas_income_sf['estimate'])\n\nRemove water from geographies\nnyc_income_tiger &lt;- get_acs(\n  geography = \"tract\",\n  variables = \"B19013_001\",\n  state = \"NY\",\n  county = c(\"New York\", \"Kings\", \"Queens\",\n             \"Bronx\", \"Richmond\"),\n  year = 2022,\n  cb = FALSE,\n  geometry = TRUE\n)\n\nlibrary(tigris)\nlibrary(sf)\nsf_use_s2(FALSE)\n\nnyc_erase &lt;- erase_water(\n  nyc_income_tiger,\n  area_threshold = 0.5,\n  year = 2022\n)\n\n\nmapview(nyc_erase, zcol = \"estimate\")\nBubble Maps are better for better for counts\n\nsan_diego_race_counts &lt;- get_acs(\n  geography = \"tract\",\n  variables = c(\n    Hispanic = \"DP05_0073\",\n    White = \"DP05_0079\",\n    Black = \"DP05_0080\",\n    Asian = \"DP05_0082\"\n  ),\n  state = \"CA\",\n  county = \"San Diego\",\n  geometry = TRUE,\n  year = 2022\n)\n\nsan_diego_hispanic &lt;- filter(\n  san_diego_race_counts, \n  variable == \"Hispanic\"\n)\n\ncentroids &lt;- st_centroid(san_diego_hispanic)\n\n\ngrad_symbol &lt;- ggplot() + \n  geom_sf(data = san_diego_hispanic, color = \"black\", fill = \"lightgrey\") + \n  geom_sf(data = centroids, aes(size = estimate),\n          alpha = 0.7, color = \"navy\") + \n  theme_void() + \n  labs(title = \"Hispanic population by Census tract\",\n       subtitle = \"2018-2022 ACS, San Diego County, California\",\n       size = \"ACS estimate\") + \n  scale_size_area(max_size = 6) \nDot Density to show heterogeneity and mixing between groups\nsan_diego_race_dots &lt;- as_dot_density(\n  san_diego_race_counts,\n  value = \"estimate\",\n  values_per_dot = 200,\n  group = \"variable\"\n)\n\nsan_diego_race_dots\n\ndot_density_map &lt;- ggplot() + \n  geom_sf(data = san_diego_hispanic, color = \"lightgrey\", fill = \"white\") + \n  geom_sf(data = san_diego_race_dots, aes(color = variable), size = 0.01) + \n  scale_color_brewer(palette = \"Set1\") + \n  guides(color = guide_legend(override.aes = list(size = 3))) + \n  theme_void() + \n  labs(color = \"Race / ethnicity\",\n       caption = \"2018-2022 ACS | 1 dot = approximately 200 people\")",
    "crumbs": [
      "Surveys",
      "Census Data"
    ]
  },
  {
    "objectID": "qmd/model-building-concepts.html",
    "href": "qmd/model-building-concepts.html",
    "title": "Concepts",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Model Building",
      "Concepts"
    ]
  },
  {
    "objectID": "qmd/model-building-concepts.html#sec-modbld-misc",
    "href": "qmd/model-building-concepts.html#sec-modbld-misc",
    "title": "Concepts",
    "section": "",
    "text": "Packages\n\n{multiverse} - makes it easy to specify and execute all combinations of reasonable analyses of a dataset\n\n\n\nPaper, Summary of it‚Äôs usage\nLots of vignettes\n\n\nRegression Workflow (Paper)\n\nMake ML model pipelines reusable and reproducible\n\n\nNotes from 7 Tips to Future-Proof Machine Learning Projects\nModularization - Useful for debugging and iteration\n\nDon‚Äôt used declarative programming. Create functions/classes for preprocessing, training, tuning, etc., and keep in separate files. You‚Äôll call these functions in the main script\n\nHelper function\n## file preprocessing.py ##\ndef data_preparation(data):\n    data = data.drop(['Evaporation', 'Sunshine', 'Cloud3pm', 'Cloud9am'], axis=1)\n    numeric_cols = ['MinTemp', 'MaxTemp', 'Rainfall', 'WindGustSpeed', 'WindSpeed9am']\n    data[numeric_cols] = data[numeric_cols].fillna(data[numeric_cols].mean())\n    data['Month'] = pd.to_datetime(data['Date']).dt.month.apply(str)\n    return data\nMain script\nfrom preprocessing import data_preparation \ntrain_preprocessed = data_preparation(train_data)\ninference_preprocessed = data_preparation(inference_data)\n\nKeep parameters in a separate config file\n\nConfig file\n## parameters.py ##\nDROP_COLS = ['Evaporation', 'Sunshine', 'Cloud3pm', 'Cloud9am']\nNUM_COLS = ['MinTemp', 'MaxTemp', 'Rainfall', 'WindGustSpeed', 'WindSpeed9am']\nProprocessing script\n## preprocessing.py ##\nfrom parameters import DROP_COLS, NUM_COLS\ndef data_preparation(data):\n    data = data.drop(DROP_COLS, axis=1)\n    data[NUM_COLS] = data[NUM_COLS].fillna(data[NUM_COLS].mean())\n    data['Month'] = pd.to_datetime(data['Date']).dt.month.apply(str)\n    return data\n\n\nVersioning Code, Data, and Models - Useful for investigating drift\n\nSee tools like DVC, MLFlow, Weights and Biases, etc. for model and data versioning\n\nImportant to save data snapshots throughout the project lifecycle, for example: raw data, processed data, train data, validation data, test data and inference data.\n\nGithub and dbt for code versioning\n\nConsistent Structures - Consistency in project structures and naming can reduce human error, improve communication, and just make things easier to find.\n\nNaming examples:\n\n&lt;model-name&gt;-&lt;parameters&gt;-&lt;model-version&gt;\n&lt;model-name&gt;-&lt;data-version&gt;-&lt;use-case&gt;\n\nExample: Reduced project template based on {{cookiecutter}}\n‚îú‚îÄ‚îÄ data\n‚îÇ   ‚îú‚îÄ‚îÄ output      &lt;- The output data from the model. \n‚îÇ   ‚îú‚îÄ‚îÄ processed      &lt;- The final, canonical data sets for modeling.\n‚îÇ   ‚îî‚îÄ‚îÄ raw            &lt;- The original, immutable data dump.\n‚îÇ\n‚îú‚îÄ‚îÄ models             &lt;- Trained and serialized models, model predictions, or model summaries\n‚îÇ\n‚îú‚îÄ‚îÄ notebooks          &lt;- Jupyter notebooks. \n‚îÇ\n‚îú‚îÄ‚îÄ reports            &lt;- Generated analysis as HTML, PDF, LaTeX, etc.\n‚îÇ   ‚îî‚îÄ‚îÄ figures        &lt;- Generated graphics and figures to be used in reporting\n‚îÇ\n‚îú‚îÄ‚îÄ requirements.txt   &lt;- The requirements file for reproducing the analysis environment, e.g.\n‚îÇ                         generated with `pip freeze &gt; requirements.txt`\n‚îÇ\n‚îú‚îÄ‚îÄ code              &lt;- Source code for use in this project.\n    ‚îú‚îÄ‚îÄ __init__.py    &lt;- Makes src a Python module\n    ‚îÇ\n    ‚îú‚îÄ‚îÄ data           &lt;- Scripts to generate and process data\n    ‚îÇ   ‚îú‚îÄ‚îÄ data_preparation.py\n    ‚îÇ   ‚îî‚îÄ‚îÄ data_preprocessing.py\n    ‚îÇ\n    ‚îú‚îÄ‚îÄ models         &lt;- Scripts to train models and then use trained models to make\n    ‚îÇ   ‚îÇ                 predictions\n    ‚îÇ   ‚îú‚îÄ‚îÄ inference_model.py\n    ‚îÇ   ‚îî‚îÄ‚îÄ train_model.py\n    ‚îÇ\n    ‚îî‚îÄ‚îÄ analysis  &lt;- Scripts to create exploratory and results oriented visualizations\n        ‚îî‚îÄ‚îÄ analysis.py\n\n\nModel is performing well on the training set but much worse on the validation/test set\n\n\nAndrew Ng calls the validation set the ‚ÄúDev Set‚Äù üôÑ\nTest: Random sample the training set and use that as your validation set. Score your model on this new validation set\n\n‚ÄúTrain-Dev‚Äù is the sampled validation set\nPossibilities\n\nVariance: The data distribution of the training set is the same as the validation/test sets\n\n\nThe model has been overfit to the training data\n\nData Mismatch: The data distribution of the training set is NOT the same as the validation/test sets\n\n\nUnlucky and the split was bad\n\nSomething maybe is wrong with the splitting function\n\nSplit ratio needs adjusting. Validation set isn‚Äôt getting enough data to be representative.\n\n\n\n\nModel is performing well on the validation/test set but not in the real world\n\nInvestigate the validation/test set and figure out why it‚Äôs not reflecting real world data. Then, apply corrections to the dataset.\n\ne.g.¬†distributions of your validation/tests sets should look like the real world data.\n\nChange the metric\n\nConsider weighting cases that your model is performing extremely poorly on.\n\n\nSplits\n\nHarrell: ‚Äúnot appropriate to split data into training and test sets unless n&gt;20,000 because of the luck (or bad luck) of the split.‚Äù\nIf your dataset is over 1M rows, then having a test set of 200K might be overkill (e.g.¬†ratio of 60/20/20).\n\nMight be better to use a ratio of 98/1/1 for big data projects and 60/20/20 for smaller data projects\n\nlink\n\nShows that simple data splitting does not give valid confidence intervals (even asymptotically) when one refits the model on the whole dataset. Thus, if one wants valid confidence intervals for prediction error, we can only recommend either data splitting without refitting the model (which is viable when one has ample data), or nested CV.",
    "crumbs": [
      "Model Building",
      "Concepts"
    ]
  },
  {
    "objectID": "qmd/cli.html",
    "href": "qmd/cli.html",
    "title": "CLI",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "CLI"
    ]
  },
  {
    "objectID": "qmd/cli.html#sec-cli-misc",
    "href": "qmd/cli.html#sec-cli-misc",
    "title": "CLI",
    "section": "",
    "text": "Resources\n\nData Science at the Command Line\n\nctrl-rshell command history search\n\nMcFly - intelligent command history search engine that takes into account your working directory and the context of recently executed commands. McFly‚Äôs suggestions are prioritized in real time with a small neural network\n\nPath to a folder that‚Äôs above root folder:\n\n1 level up: ../desired-folder\n2 levels up: ../../desired-folder\n\nDebian vs.¬†Ubuntu (from ChatGPT)\n\nStability vs.¬†Freshness:\n\nDebian: Debian is known for its stability and reliability. It has a rigorous testing process and a conservative approach to updates, which makes it suitable for servers and systems where stability is crucial.\nUbuntu: Ubuntu is based on Debian but tends to be more up-to-date with software packages. It follows a time-based release cycle, with regular releases every six months. This can be appealing if you want access to the latest features and software.\n\nPackage Management:\n\nDebian: Debian uses the Debian Package Management System (dpkg) and Advanced Package Tool (APT) for package management. It has a vast repository of software packages.\nUbuntu: Ubuntu also uses dpkg and APT but adds its own software management tools like Snap and Ubuntu Software Center. This can make software installation more user-friendly.\n\nCommunity and Support:\n\nDebian: Debian has a large and dedicated community, and it‚Äôs known for its strong commitment to free and open-source software principles. It has a stable support structure, but community support may not be as user-friendly as Ubuntu‚Äôs.\nUbuntu: Ubuntu has a large and active community, and it offers both free and paid support options. The Ubuntu community is known for its user-friendliness and helpful forums, making it a good choice for beginners.\n\nVariants and Flavors:\n\nDebian: Debian offers different flavors, known as ‚ÄúDebian spins,‚Äù catering to various needs, such as Debian Stable, Debian Testing, and Debian Unstable. These variants differ in terms of software stability and freshness.\nUbuntu: Ubuntu has several official flavors (e.g., Ubuntu Desktop, Ubuntu Server, Kubuntu, Xubuntu) that come with different desktop environments. This variety allows users to choose an environment that suits their preferences.\n\nLicensing:\n\nDebian: Debian has a strict commitment to free and open-source software, prioritizing software that adheres to its Free Software Guidelines.\nUbuntu: While Ubuntu also includes mostly free and open-source software, it may include some proprietary drivers and software by default, which can be a concern for users who prioritize a completely open-source system.\n\nPerformance (Google Search AI)\n\nDebian is considered lightweight and much faster than Ubuntu. It comes with few pre-installed software.\n\nHardware (Google Search AI)\n\nDebian works well on older hardware. Debian still offers a 32-bit version of the distro, while Ubuntu no longer offers a 32-bit version.",
    "crumbs": [
      "CLI"
    ]
  },
  {
    "objectID": "qmd/cli.html#sec-cli-r",
    "href": "qmd/cli.html#sec-cli-r",
    "title": "CLI",
    "section": "R",
    "text": "R\n\nMake an R script pipeable (From link)\nparallel \"echo 'zipping bin {}'; cat chunked/*_bin_{}_*.csv | ./upload_as_rds.R '$S3_DEST'/chr_'$DESIRED_CHR'_bin_{}.rds\"\n#!/usr/bin/env Rscript\nlibrary(readr)\nlibrary(aws.s3)\n\n# Read first command line argument\ndata_destination &lt;- commandArgs(trailingOnly = TRUE)[1]\n\ndata_cols &lt;- list(SNP_Name = 'c', ...)\n\ns3saveRDS(\n  read_csv(\n        file(\"stdin\"), \n        col_names = names(data_cols),\n        col_types = data_cols \n    ),\n  object = data_destination\n)\n\nBy passing readr::read_csv the function, file(\"stdin\"), it loads the data piped to the R script into a dataframe, which then gets written as an .rds file directly to s3 using {aws.s3}.\n\nKilling a process\nsystem(\"taskkill /im java.exe /f\", intern=FALSE, ignore.stdout=FALSE)\nStarting a process in the background\n# start MLflow server\nsys::exec_background(\"mlflow server\")\nDelete an opened file in the same R session\n\nYou **MUST** unlink it before any kind of manipulation of object\n\nI think this works because readr loads files lazily by default\n\nExample:\nwisc_csv_filename &lt;- \"COVID-19_Historical_Data_by_County.csv\"\ndownload_location &lt;- file.path(Sys.getenv(\"USERPROFILE\"), \"Downloads\")\nwisc_file_path &lt;- file.path(download_location, wisc_csv_filename)\nwisc_tests_new &lt;- readr::read_csv(wisc_file_path)\n# key part, must unlink before any kind of code interaction\n# supposedly need recursive = TRUE for Windows, but I didn't need it\n# Throws an error (hence safely) but still works\nsafe_unlink &lt;- purrr::safely(unlink)\nsafe_unlink(wisc_tests_new)\n\n# manipulate obj\nwisc_tests_clean &lt;- wisc_tests_new %&gt;%\n¬† ¬† ¬† janitor::clean_names() %&gt;%\n¬† ¬† ¬† select(date, geo, county = name, negative, positive) %&gt;%\n¬† ¬† ¬† filter(geo == \"County\") %&gt;%\n¬† ¬† ¬† mutate(date = lubridate::as_date(date)) %&gt;%\n¬† ¬† ¬† select(-geo)\n# clean-up\nfs::file_delete(wisc_file_path)\n\nFind out which process is locking or using a file\n\nOpen Resource Monitor, which can be found\n\nBy searching for Resource Monitor or resmon.exe in the start menu, or\nAs a button on the Performance tab in your Task Manager\n\nGo to the CPU tab\nUse the search field in the Associated Handles section\n\ntype the name of file in the search field and it‚Äôll search automatically\n35548",
    "crumbs": [
      "CLI"
    ]
  },
  {
    "objectID": "qmd/cli.html#sec-cli-awk",
    "href": "qmd/cli.html#sec-cli-awk",
    "title": "CLI",
    "section": "AWK",
    "text": "AWK\n\n\nMisc\n\nResources\n\nDocs\nAwk - A Tutorial and Introduction\n\n\nPrint first few rows of columns 1 and 2\nawk -F, '{print $1,$2}' adult_t.csv|head\nFilter lines where no of hours/ week (13th column) &gt; 98\nawk -F, ‚Äò$13 &gt; 98‚Äô adult_t.csv|head\nFilter lines with ‚ÄúDoctorate‚Äù and print first 3 columns\nawk '/Doctorate/{print $1, $2, $3}' adult_t.csv\nRandom sample 8% of the total lines from a .csv (keeps header)\n'BEGIN {srand()} !/^$/ {if(rand()&lt;=0.08||FNR==1) print &gt; \"rand.samp.csv\"}' big_fn.csv\nDecompresses, chunks, sorts, and writes back to S3 (From link)\n# Let S3 use as many threads as it wants\naws configure set default.s3.max_concurrent_requests 50\n\nfor chunk_file in $(aws s3 ls $DATA_LOC | awk '{print $4}' | grep 'chr'$DESIRED_CHR'.csv') ; do\n\n        aws s3 cp s3://$batch_loc$chunk_file - |\n        pigz -dc |\n        parallel --block 100M --pipe  \\\n        \"awk -F '\\t' '{print \\$1\\\",...\\\"$30\\\"&gt;\\\"chunked/{#}_chr\\\"\\$15\\\".csv\\\"}'\"\n\n        # Combine all the parallel process chunks to single files\n        ls chunked/ |\n        cut -d '_' -f 2 |\n        sort -u |\n        parallel 'cat chunked/*_{} | sort -k5 -n -S 80% -t, | aws s3 cp - '$s3_dest'/batch_'$batch_num'_{}'\n\n        # Clean up intermediate data\n        rm chunked/*\ndone\n\nUses pigz to parallelize decompression\nUses GNU Parallel (site, docs, tutorial1, tutorial2) to parallelize chunking (100MB chunks in 1st section)\nChunks data into smaller files and sorts them into directories based on a chromosome column (I think)\nAvoids writing to disk",
    "crumbs": [
      "CLI"
    ]
  },
  {
    "objectID": "qmd/cli.html#sec-cli-bash",
    "href": "qmd/cli.html#sec-cli-bash",
    "title": "CLI",
    "section": "Bash",
    "text": "Bash\n\nMisc\n\nNotes from\n\nBash for Data Scientists, Data Engineers & MLOps Engineers\n\nBunch of other stuff that I didn‚Äôt take notes on\n\nBash Scripting on Linux: The Complete Guide - video course\n\nResources\n\nBash Scripting Cheatsheet\nCurl Docs\n\nman &lt;command&gt; displays documentation for command\nSpecial Characters\n\n\n‚Äú&gt;‚Äù redirects the output from a program to a file.\n\n‚Äú&gt;&gt;‚Äù does the same thing, but it‚Äôs appending to an existing file instead of overwriting it, if it already exists.\n\n\n\n\n\nCommands\n\nBasic Commands\n\n\necho $SHELL - prints the type of shell you‚Äôre using\necho $PATH - prints all stored pathes\nexport PATH=\"my_new_path:$PATH\" - store a new path\nCommand Syntax: command -options arguments\nPiping Commands: cat user_names.txt|sort|uniq\n\n\n\nAliases\n\nCustom commands that you can define in order to avoid typing lengthy commands over and over again\nExamples\nalias ll=\"ls -lah\"\nalias gs=\"git status\"\nalias gp=\"git push origin master\"\nCreate safeguards for yourself\nalias mv=\"mv -i\"\n\nmv will automatically use the i flag, so the terminal will warn you if the file you‚Äôre about to move does already exist under the new directory,\n\nThis way you don‚Äôt accidentally overwrite files that you didn‚Äôt mean to overwrite.\n\n\n\n\n\nFiles/Directories\n\nList\n\n\nList 10 most recently modified files: ls -lt | head\nList files sorted by file size: ls -l -S\n\nCreate/Delete Directories\nmkdir &lt;dir_name&gt;\nrmdir &lt;dir_name&gt;\nOutput to file: echo ‚ÄúThis is an example for redirect‚Äù &gt; file1.txt\nAppend line to file: echo ‚ÄúThis is the second line of the file‚Äù &gt;&gt; file1.txt\nCreate/Delete file(s):\n# Create files\ntouch file1.txt\ntouch file1.txt file2.tx\n\n# Delete files\nrm file1.txt\nrm file1.txt file2.txt\nMove files/dir; Rename\n# Move single file\nmv my_file.txt /tmp\n# Move multiple files\nmv file1 file2 file3 /tmp\n# Move a directory or multiple directories\nmv d1 d2 d3 /tmp\n# Rename the file using move command\nmv my_file1.txt my_file_newname.txt\n\nFile(s) and directories being moved to ‚Äútmp‚Äù directory\n\nSearch\n\nFind\n# syntax find &lt;path&gt; &lt;expression&gt;\n# Find by name\nfind . -name ‚Äúmy_file.csv\"\n#Wildcard search\nfind . -name \"*.jpg\"\n# Find all the files in a folder\nfind /temp\n# Search only files\nfind /temp -type f\n# Search only directories\nfind /temp -type d\n# Find file modified in last 3 hours\nfind . -mmin -180\n# Find files modified in last 2 days\nfind . -mtime -2\n# Find files not modified in last 2 days\nfind . -mtime +2\n# Find the file by size\nfind -type f -size +10M\n\nLocate (faster)\n\nDocs\nInstall\nbash sudo apt install mlocate # Debian\nUsage\n\nsudo updatedb # update before using\nlocate .csv\nSplit files\n# default: 1000 lines per file, names of new files: xaa, xab, xac, etc.\nsplit my_file\n\n# add a prefix to new file names\nsplit my_file my_prefix\n\n# specify split threshold (e.g. 5000) by number of lines\nsplit --lines=5000 my_file\n\n# specify split threshold by size (e.g. 10MB)\nsplit --bytes=10 MB my_file\nPermissions\n\nls -l See list of files and the permissions\n-rwxrwxrwx - sytax of permissions for a folder or directory\n\n‚Äúrwx‚Äù stand for read, write, and execute rights, respectively\nThe 3 ‚Äúrwx‚Äù blocks are for (1) user, (2) user group, and (3) everyone else.\n\nIn the given example, all 3 of these entities have read, write, as well as execute permissions.\n\nThe dash indicates that this is a file. Instead of the dash, you can also see a ‚Äúd‚Äù for directory or ‚Äúl‚Äù for a symbolic link.\n\nchmod - edit permissions\n\nExample: chmod u+x my_program.py - makes this file executable for yourself\n\nsudo - ‚Äúsuper user‚Äù - using this prefix gives you all the permissions to all the files\n\nsudo su - opens a stand alone super user shell\n\n\n\n\n\nPrint\n\nPrint file content\ncat &lt; my_file.txt\n# or\ncat my_file.txt\nPrint 1 pg at a time: less my_file.txt\nPrint specific number of lines: head -n&lt;num_lines&gt; &lt;file.csv&gt;\nPrint file content from bottom to top: tac my_file.txt\ncat -b log.txt | grep error : shows all lines in log.txt that contain the string ‚Äòerror‚Äô, along with the line number (-b)\n\n\n\nLogicals and Conditionals\n\nLogicals\n\n; : command1 ; command2\n\ncommand 1 and command 2 run independently of each other\n\n& : command1 & command2\n\ncommand 1 runs in the background and command 2 runs in the background\n\n&& : command1 && command2\n\nIf the first command errors out then the second command is not executed\n\n|| : command1 || command2\n\nThe second commmand is only execute if the first command errors\n\nExample\ncd my_dir && pwd || echo ‚ÄúNo such directory exist.Check‚Äù\n\nIf the my_dir exists, then the current working directory is printed. If the my_dir doesn‚Äôt exist, then the message ‚ÄúNo such directory exists. check‚Äù message is printed.\n\n\nConditionals\n\nUse [[ ]] for conditions in if / while statements, instead of [ ] or test.\n\n[[ ]] is a bash builtin, and is more powerful than [ ] or test.\nExample: if [[ -n \"${TRACE-}\" ]]; then set -o xtrace; fi\n\n\n\n\n\nString Matching\n\nExample: Search for ‚Äúerror‚Äù and write to file\n#output to a file again\ncat file1 file2 file3 | grep error | cat &gt; error_file.txt\n#Append to the end\ncat file1 file2 file3 | grep error | cat &gt;&gt; error_file.txt\n\nPrints lines into grep which searches for ‚Äúerror‚Äù in each line. Lines with ‚Äúerror‚Äù get written to ‚Äúerror_file.txt‚Äù\n\nFilter lines\ngrep -i ‚ÄúDoctorate‚Äù adult_t.csv |grep -i ‚ÄúHusband‚Äù|grep -i ‚ÄúBlack‚Äù|csvlook\n# -i, --ignore-case-Ignore¬† case¬† distinctions,¬† so that characters that differ only in case match each other.\n\nSelect all the candidates who have doctorates and a husband and race are Black\ncsvlook is pretty printing from csvkit package (see Big Data &gt;&gt; Larger Than Memory &gt;&gt; csvkit)\n\nCount how many rows fit the criteria\ngrep -i ‚ÄúDoctorate‚Äù adult_t.csv | wc -l\n\nCounts how many rows have ‚ÄúDoctorate‚Äù\n\n-wc is ‚Äúword count‚Äù\n\n\n\n\n\n\nVariables\n\nLocal Variable:\n\nDeclared at the command prompt\nUse lower case for name\nAvailable only in the current shell\nNot accessible by child processes or programs\nAll user-defined variables are local variables\n\nEnvironment (global) variables:\n\nCreate with export command\nUse upper case for name\nAvailable to child processes\n\nDeclare local and environment variables then access via ‚Äú$‚Äù\n# local\nev_car=‚ÄôTesla‚Äô\necho 'The ev car I like is' $ev_car\n\n# environment\nexport EV_CAR=‚ÄôTesla‚Äô\necho 'The ev car I like is' $EV_CAR\n\nNo spaces in variable assignment\n\nAlways quote variable accesses with double-quotes.\n\nOne place where it‚Äôs okay not to is on the left-hand-side of an [[ ]] condition. But even there I‚Äôd recommend quoting.\nWhen you need the unquoted behaviour, using bash arrays will likely serve you much better.\n\nFunctions\n\nUse local variables in functions.\nAccept multiple ways that users can ask for help and respond in kind.\n\nCheck if the first arg is -h or ‚Äìhelp or help or just h or even -help, and in all these cases, print help text and exit.\n\nWhen printing error messages, please redirect to stderr.\n\nUse echo 'Something unexpected happened' &gt;&2 for this\n\n\n\n\n\nScripting\n\nUse the .sh (or .bash) extension for your script\nUse long options, where possible (like ‚Äìsilent instead of -s). These serve to document your commands explicitly.\nIf appropriate, change to the script‚Äôs directory close to the start of the script.\n\nAnd it‚Äôs usually always appropriate.\nUse cd \"$(dirname \"$0\")\", which works in most cases.\n\nUse shellcheck. Heed its warnings.\nShebang line\n\nContains the absolute path of the bash interpreter\n\nList paths to all shells: cat/etc/shells\n\nUse as the first line even if you don‚Äôt give executable permission to the script file.\nStarts with ‚Äú#!‚Äù the states the path of the interpreter\nExample: #!/bin/bash\n\nInterpreter installed in directory ‚Äú/bin‚Äù\n\nExample: #!/usr/bin/env bash\n\nCommands that should start your script\n\nUse set -o errexit\n\nSo that when a command fails, bash exits instead of continuing with the rest of the script.\n\nUse set -o nounset\n\nThis will make the script fail, when accessing an unset variable. Saves from horrible unintended consequences, with typos in variable names.\nWhen you want to access a variable that may or may not have been set, use \"${VARNAME-}\" instead of \"$VARNAME\", and you‚Äôre good.\n\nUse set -o pipefail\n\nThis will ensure that a pipeline command is treated as failed, even if one command in the pipeline fails.\n\nUse set -o xtrace, with a check on $TRACE env variable.\n\nFor copy-paste: if [[ -n \"${TRACE-}\" ]]; then set -o xtrace; fi.\nThis helps in debugging your scripts, a lot.\nPeople can now enable debug mode, by running your script as TRACE=1 ./script.sh instead of ./script.sh .\n\n\nExample: Basic Execution a Bash Script\n\nCreate a directory bash_script: mkdir bash_script\nCreate a hello_world.sh file: touch hello_script.sh\nOpen hello_script.sh (text editor?)\nAdd code, save, and close\n¬† ¬† #!/bin/bash\n¬† ¬† echo ‚ÄòHello World‚Äô\nMake file executable: chmod +x hello_world.sh\nExecute file: ./hello_world.sh\n\nTemplate\n#!/usr/bin/env bash\nset -o errexit\nset -o nounset\nset -o pipefail\nif [[ -n \"${TRACE-}\" ]]; then\n¬† ¬† set -o xtrace\nfi\nif [[ \"$1\" =~ ^-*h(elp)?$ ]]; then\n¬† ¬† echo 'Usage: ./script.sh arg-one arg-two\nThis is an awesome bash script to make your life better.\n'\n¬† ¬† exit\nfi\ncd \"$(dirname \"$0\")\"\nmain() {\n¬† ¬† echo do awesome stuff\n}\nmain \"$@\"\n\n\n\nJob Management\n\nPrograms/Scripts will by default run in the foreground, and prevent you from doing anything else until the program is done.\nWhile program is running:\n\ncontrol+c - Will send a SIGINT (signal interrupt) signal to the program, which instructs the machine to interrupt the program immediately (unless the program has a way to handle these signals internally).\ncontrol+z - Will pause the program.\n\nAfter pausing the program can be continued either by bringing it to the foreground (fg), or by sending it to the backgroud (bg).\n\n\nExecute script to run in the background: python run.py &\njobs - shows all running jobs and process ids (PIDS)\nkill - sends signals to jobs running in the background\n\nkill -STOP %1 sends a STOP signal, pausing program 1.\nkill -KILL %1 sends a KILL signal, terminating program 1 permanently.\n\n\n\n\ntmux (‚Äòterminal multiplexer‚Äô)\n\nEnables you to easily create new terminal sessions and navigate between them. This can be extremely useful, for example you can use one terminal to navigate your file system and another terminal to execute jobs.\nInstallation (if necessary): sudo apt install tmux\n\nTypically comes with the linux installation\n\nSessions\n\ntmux - starts an unnamed session\ntmux new -s moose creates new terminal session with name ‚Äòmoose‚Äô\ntmux ls - lists all running sessions\ntmux kill-session -t moose - kills session named ‚Äúmoose‚Äù\nexit - stops and quits the current session\nKill all sessions (various opinions on how to do this)\n\ntmux kill-session\ntmux kill-server\ntmux ls | grep : | cut -d. -f1 | awk '{print substr($1, 0, length($1)-1)}' | xargs kill\n\n\nAttach/Detach\n\nWhen you log out of a remote machine (either on purpose or accidentally), all of the programs that were actively running inside your shell are automatically terminated. On the other hand, if you run your programs inside a tmux shell, you can come simply detach the tmux window, log out, close your computer, and come back to that shell later as if you‚Äôve never been logged out.\ntmux detach - detach current session\ncontrol+bthen pressd`: When you have multiple sesssions running, this will allow you to select the session to detach\nFrom inside bash and not inside a session\n\ntmux a : attach to latest created session\ntmux a -t moose : attach to session called ‚Äòmoose‚Äô\n\n\nPane Creation and Navigation\n\ncontrol+b then press ‚Äù (i.e.¬†shift+‚Äô): add another terminal pane below\ncontrol+b then press % (i.e.¬†shift+5) : add another terminal pane to the right\ncontrol+b then press ‚Üí : move to the terminal pane on the right (similar for left, up, down)\n\n\n\n\nSSH\n\nTypically uses a key pair to log into remote machines\n\nKey pair consists of a public key (which both machines have access to) and a private key (which only your own machine has access to)\n‚Äússh-keygen‚Äù is a program for generating such a key pair.\n\nIf you run ssh-keygen, it will by default create a public key named ‚Äúid_rsa.pub‚Äù and a private key named ‚Äúid_rsa‚Äù, and place both into your ‚Äú~/.ssh‚Äù directory\nYou‚Äôll need to add the public key to the remote machine by piping together cat, ssh, and a streaming operator\n\ncat .ssh/id_rsa.pub | ssh user@remote 'cat &gt;&gt; ~/.ssh/authorized_keys'\n\n\n\nConnect to the remote machine: ssh remote -i ~/.ssh/id_rsa\nCreate a config file instead\n\nLocation: ‚Äú~/.ssh/config‚Äù\nContents\nHost dev\n¬† HostName remote\n¬† IdentityFile ~/.ssh/id_rsa\n\nConnect using config: ssh dev\nFor Windows and using Putty, see\n\nAWS &gt;&gt; EC2 &gt;&gt; Connect to/ Terminate Instance\nProjects Notebook &gt;&gt; Article, Nested Cross Validation &gt;&gt; Notes &gt;&gt; Running EC2 instances checklist\n\n\n\n\nVim\n\nCommand-line based text editor\nCommon Usage\n\nLogging into a remote machine and need to make a code change there. vim is a standard program and therefore usually available on any machine you work on.\nWhen running git commit, by default git opens vim for writing a commit message. So at the very least you‚Äôll want to know how to write, save, and close a file.\n\n2 modes: Navigation Mode; Edit Mode\n\nWhen Vim is launched you‚Äôre in Navigation mode\nPress i to start edit mode, in which you can make changes to the file.\nPress Esc key to leave edit mode and go back to navigation mode.\n\nCommands (Cheatsheet)\n\nx deletes a character\ndd deletes an entire row\nb (back) goes to the previous word\nn (next) goes to the next word\n:wq saves your changes and closes the file\n:q! ignores your changes and closes the file\n\n\n\n\nPackages\n\nCommon package managers: apt, Pacman, yum, and portage\nAPT (Advanced Package Tool)\n\nInstall Packages\n# one pkg\nsudo apt-get install &lt;package_name&gt;\n# multiple\nsudo apt-get install &lt;pkg_name1&gt; &lt;pkg_name2&gt;\n\nInstall but no upgrade: sudo apt-get install &lt;pkg_name&gt; --no-upgrade\n\nSearch for an installed package: apt-cache search &lt;pkg_name&gt;\nUpdate package information prior to ‚Äúupgrading‚Äù the packages\nsudo apt-get update\n\nDownloads the package lists from the repositories and ‚Äúupdates‚Äù them to get information on the newest versions of packages and their dependencies.\n\nUpgrade\n# all installed packages\nsudo apt-get upgrade\n\n# To upgrade only a specific program\nsudo apt-get upgrade &lt;package_name&gt;\n\n# Upgrades and handles dependencies; delete obsolete, add new\napt-get dist-upgrade\n\n# together\nsudo apt-get update && sudo apt-get dist-upgrade\n\n\n\n\nExpressions\n\nSort data, filter only unique lines, and write to file: cat adult_t.csv | sort | uniq -c &gt; sorted_list.csv",
    "crumbs": [
      "CLI"
    ]
  },
  {
    "objectID": "qmd/cli.html#sec-cli-powsh",
    "href": "qmd/cli.html#sec-cli-powsh",
    "title": "CLI",
    "section": "Powershell",
    "text": "Powershell\n\nMisc\n\nDocs, Sample Scripts\nUse Get-Help &lt;cmdlet-name&gt;¬†to access help information for specific cmdlets.\n\n$_ (Docs)\n\nAutomatic Variable; Alias for $PSItem\n\nDocs for automatic variables\n\nUseful for looping through objects in a directory. See ‚ÄúChange Name (or Extensions) of Multiple Files‚Äù for an example of usage\nGet the properties of an object\n\nExample: Using Get-ItemProperty (Docs)\nGet-ItemProperty scrapsheet.txt | Format-List\n\nProperty names will be on the left side of the output\n\nExample: Using Get-Member (Docs)\n Get-ChildItem *.txt | Select-Object -First 1 | Get-Member\n\nSelects the first text file in the directory and gets the Properties and a bunch of other stuff like methods which also can be used with $_\nDescriptions are awful for a lot of the stuff, but for the most part, you can guess what the property is.\n\n\nSome properties that can be used with $_\n\nFile System Objects:\n\n.Name: Returns the name component of a file\n.FullName: Returns the full path of the file.\n.Length: Returns the size of the file in bytes.\n.CreationTime: Returns the date and time the file was created.\n.LastWriteTime: Returns the date and time the file was last modified.\n.IsReadOnly: Returns $true if the file is read-only, $false otherwise.\n\nProcess Objects:\n\n.Id: Returns the unique identifier (PID) of the process.\n.Name: Returns the name of the process.\n.MainWindowTitle: Returns the title of the process‚Äôs main window (if applicable).\n.WorkingDirectory: Returns the working directory of the process.\n.CPU: Returns the CPU usage of the process.\n\nRegistry Objects:\n\n.Name: Returns the name of the registry key.\n.Hive: Returns the hive of the registry key (e.g., HKLM, HKCU).\n.Value: Returns the value of the registry key (if applicable).\n.Path: Returns the full path of the registry key.\n\n\n\nComments: &lt;# comment #&gt;\nChange directories\n Set-Location \"Documents\\R\\Projects\"\nCreate a New Folder\n New-Item -ItemType Directory -Path \"Folder Name\"\n\nAssumes you‚Äôre already in the directory that you want the folder in. You can also use a path, e.g.¬†\"C:\\Temp\\Documents\\New Folder\\Subfolder1\\\\Subfolder2\".\n\nChange Name of File\nRename-Item -Path \"c:\\logfiles\\daily_file.txt\" -NewName \"monday_file.txt\"\nChange Name (or Extensions) of Multiple Files\nGet-ChildItem *.md | Rename-Item -NewName { $_.Name -replace '.md','.qmd' }\n\nGet-ChildItem: cmdlet gets all the files in the current folder that have a .mc file extension\nResults piped to Rename-Item\n\nNewName: Has a value that‚Äôs a script block that runs before the value is submitted to the NewName parameter.\n\n$_: (automatic variable) It represents each file object as it comes to the command through the pipeline.\n-replace: Operator replaces the file extension of each file with .qmd.\n\nNotice that matching using the -replace operator is not case sensitive.\n\n\n\n\nExecute a File\nInvoke-Item configuration.cmd\nMulti-line Commands\nffmpeg -i input.mkv -map 0:v:0 `\n       -map 0:a:2 -map 0:a:0 -map 0:a:1 -map 0:a:3 `\n       -map 0:s -c copy `\n       -disposition:a:0 default `\n       reordered.mkv\n\nIn bash, it‚Äôs a backslash (\\), but in Powershell, it‚Äôs a backtick ( ` )\n*Don‚Äôt forget that there‚Äôs a space between the last character and the backtick.*\nIn practice, this will look like\nffmpeg -i .input.mkv -map 0:v:0 `\n&gt;&gt; -map 0:a:2 -map 0:a:0 -map 0:a:1 -map 0:a:3 `\n&gt;&gt; -map 0:s -c copy `\n&gt;&gt; -disposition:a:0 default `\n&gt;&gt; reordered.mkv\n\nString Matching\n\nPrint line with pattern\nSelect-String -Path \"file*.txt\" -Pattern \"error\"\nfile1.txt:3:This is the error line of the file\nfile2.txt:3:This is the error line of the file\nfile3.txt:3:This is the error line of the file\n\nMatches the 3rd line of each file\n\n\nGet stats on a process\nGet-Process -Name chrome\n\nHandles: The number of handles that the process has opened.\nNPM(K): The amount of non-paged memory that the process is using, in kilobytes.\nPM(K): The amount of pageable memory that the process is using, in kilobytes.\nWS(K): The size of the working set of the process, in kilobytes. The working set consists of the pages of memory that were recently referenced by the process.\nVM(M): The amount of virtual memory that the process is using, in megabytes. Virtual memory includes storage in the paging files on disk.\nCPU(s): The amount of processor time that the process has used on all processors, in seconds.\nID: The process ID (PID) of the process.\nProcessName: The name of the process. For explanations of the concepts related to processes, see the Glossary in Help and Support Center and the Help for Task Manager.\n\nEnvironment Variables\n\nSet an environment variable\nSet-Item -Name PYTHONSTARTUP -Value C:\\path\\to\\pythonstartup.py\n\nSame expression to modify existing environment variable\nOr\n$env:QUARTO_DENO_EXTRA_OPTIONS = \"--v8-flags=--max-old-space-size=8192\"\n\nDelete environment variable\nRemove-Item -Name &lt;variable_name&gt;\nVerify value of an environment variable\n$env:&lt;variable_name&gt;\n\nPorts\n\nFind application using a port.\nnetstat -aon | findstr ':80'\nnetstat -anp | find \":80\"\n\nIf port 80 is being used by the application, it will return a PID. Then you can find it in Task Manager &gt;&gt; Processess\n\nList all Listening and Established ports\nnetstat -anob\nCheck for processes using a port\nGet-Process -Id (Get-NetTCPConnection -LocalPort 80).OwningProcess\nTest connection to local port to see if it‚Äôs open\nTest-NetConnection -ComputerName localhost -Port 80 | Select-Object TcpTestSucceeded\nCheck firewall settings for an app\nnetsh advfirewall firewall show rule name=\"name_of_app\"",
    "crumbs": [
      "CLI"
    ]
  },
  {
    "objectID": "qmd/cli.html#sec-cli-batscri",
    "href": "qmd/cli.html#sec-cli-batscri",
    "title": "CLI",
    "section": "Batch Scripting",
    "text": "Batch Scripting\n\nMisc\n\nResources\n\nWindows Batch Scripting\n\nTo keep the prompt window open after script execution, place these either of these commands at end of your script.\n\npause: Keeps window open until you press any key.\nVia timer: e.g.¬†timeout /t 300\ncmd /k: The prompt will remain active and you can execute additional commands manually.\n\n\nExample: Create variables and execute\n@echo off\n\nrem Set the path to the Rscript executable\nset RSCRIPT=\"C:\\Users\\user\\AppData\\Local\\Programs\\R\\R-4.2.3\\bin\\Rscript.exe\"\n\nrem Set the path to the R script to execute\nset RSCRIPT_FILE=\"C:\\Users\\user\\my_r_script.R\"\n\nrem Execute the R script\n%RSCRIPT% %RSCRIPT_FILE%\n\nrem Pause so the user can see the output\nexit\n\n@echo off - This line turns off the echoing of commands in the command prompt window, making the output cleaner.\nrem - Keyword that denotes a comment in a batch file.\nset RSCRIPT= - This line assigns the path to the Rscript executable to the environment variable RSCRIPT.\nset RSCRIPT_FILE= - The path to the R script file is assigned to the environment variable RSCRIPT_FILE.\n%RSCRIPT% %RSCRIPT_FILE% - Executes the R script using the Rscript executable and passes the path to the R script file as an argument.\nexit - This command exits the batch file and closes the command prompt window.\n\nExample: Exit if script errors\nRscript \"C:\\Users\\ercbk\\Documents\\R\\Projects\\Indiana-COVID-19-Tracker\\R\\collection\\build-opentab-dat.R\"\n\nREM if the data building script errors, bat script terminates without running other scripts or commands\nif %errorlevel% neq 0 exit /b %errorlevel%\n\ncd \"C:\\Users\\ercbk\\Documents\\R\\Projects\\Indiana-COVID-19-Tracker\"\n\ngit add data/YoY_Seated_Diner_Data.csv\ngit commit -m \"opentab data update\"\ngit pull\ngit push\n\nEXIT",
    "crumbs": [
      "CLI"
    ]
  },
  {
    "objectID": "qmd/cli.html#sec-cli-wsl",
    "href": "qmd/cli.html#sec-cli-wsl",
    "title": "CLI",
    "section": "WSL",
    "text": "WSL\n\nResources\n\nDocs\nTo update password (link) using username\n\nLoad Linux: wsl -d Ubuntu-22.04 where -d is for ‚Äìdistribution\nWSL Help: wsl --help\nExit linux terminal back to command prompt or powershell: exit",
    "crumbs": [
      "CLI"
    ]
  },
  {
    "objectID": "qmd/json.html",
    "href": "qmd/json.html",
    "title": "JSON",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "JSON"
    ]
  },
  {
    "objectID": "qmd/json.html#sec-json-misc",
    "href": "qmd/json.html#sec-json-misc",
    "title": "JSON",
    "section": "",
    "text": "Packages\n\n{yyjsonr} - A fast JSON parser/serializer, which converts R data to/from JSON and NDJSON. It is around 2x to 10x faster than jsonlite at both reading and writing JSON.\n{RcppSimdJson} - Comparable to {yyjsonr} in performance.\n\nAlso see\n\nBig Data &gt;&gt; Larger than Memory\nSQL &gt;&gt; Processing Expressions &gt;&gt; Nested Data\nDatabases &gt;&gt; DuckDB &gt;&gt; Misc\n\nhrbmstr recommends trying duckdb before using the cli tools in ‚ÄúBig Data‚Äù\n\n\nTools\n\n{listviewer}: Allows you to interactively explore and edit json files through the Viewer in the IDE. Docs show how it can be embedded into a Shiny app as well.\n\nExample\nlibrary(listviewer)\nmoose &lt;- jsonlite::read_json(\"path/to/file.json\")\njsonedit(moose)\nreactjson(moose)\n\nI‚Äôve also used this a .config file which looked like a json file when I opened in a text editor, so this seems to work on anything json-like.\nreactjson has a copy button which is nice so that you can paste your edited version into a file.\njsonedit seems like it has more features, but I didn‚Äôt see a copy button. But there‚Äôs a view in which you can manually select everything a copy it via keyboard shortcut.",
    "crumbs": [
      "JSON"
    ]
  },
  {
    "objectID": "qmd/json.html#sec-json-jsonlite",
    "href": "qmd/json.html#sec-json-jsonlite",
    "title": "JSON",
    "section": "{jsonlite}",
    "text": "{jsonlite}\n\nRead",
    "crumbs": [
      "JSON"
    ]
  },
  {
    "objectID": "qmd/json.html#sec-json-py",
    "href": "qmd/json.html#sec-json-py",
    "title": "JSON",
    "section": "Python",
    "text": "Python\n\nExample: Parse Nested JSON into a dataframe (article)\n\nRaw JSON\n\n\n‚Äúentry‚Äù has the data we want\n‚Äú‚Ä¶‚Äù at the end indicates there are multiple objectss inside the element, ‚Äúentry‚Äù\n\nProbably other root elements other than ‚Äúfeed‚Äù as well\n\n\nRead a json file from a URL using {{requests}} and convert to list\n\nimport requests\n\nurl = \"https://itunes.apple.com/gb/rss/customerreviews/id=1500780518/sortBy=mostRecent/json\"\n\nr = requests.get(url)\n\ndata = r.json()\nentries = data[\"feed\"][\"entry\"]\n\nIt looks like the list conversion also ordered the elements alphabetically\nThe output list is subsetted by the root element ‚Äúfeed‚Äù and the child element ‚Äúentry‚Äù\n\nGet a feel for the final structure you want by hardcoding elements into a df\nparsed_data = defaultdict(list)\n\nfor entry in entries:\n    parsed_data[\"author_uri\"].append(entry[\"author\"][\"uri\"][\"label\"])\n    parsed_data[\"author_name\"].append(entry[\"author\"][\"name\"][\"label\"])\n    parsed_data[\"author_label\"].append(entry[\"author\"][\"label\"])\n    parsed_data[\"content_label\"].append(entry[\"content\"][\"label\"])\n    parsed_data[\"content_attributes_type\"].append(entry[\"content\"][\"attributes\"][\"type\"])\n    ... \nGeneralize extracting the properties of each object in ‚Äúentry‚Äù with a nested loop\nparsed_data = defaultdict(list)\n\nfor entry in entries:\n    for key, val in entry.items():\n        for subkey, subval in val.items():\n            if not isinstance(subval, dict):\n                parsed_data[f\"{key}_{subkey}\"].append(subval)\n            else:\n                for att_key, att_val in subval.items():\n                    parsed_data[f\"{key}_{subkey}_{att_key}\"].append(att_val)\n\ndefaultdict creates a key from a list element (e.g.¬†‚Äúauthor‚Äù) and groups the properties into a list of values where the value may also be a dict.\n\nSee Python, General &gt;&gt; Types &gt;&gt; Dictionaries\n\nFor each item in ‚Äúentry‚Äù, it looks at the first key-value pair knowing that value is always a dictionary (object in JSON)\nThen handles two different cases\n\nFirst Case: The value dictionary is flat and does not contain another dictionary, only key-value pairs.\n\nCombine the outer key with the inner key to a column name and take the value as column value for each pair.\n\nSecond Case: Dictionary contains a key-value pair where the value is again a dictionary.\n\nAssumes at most two levels of nested dictionaries\nIterates over the key-value pairs of the inner dictionary and again combines the outer key and the most inner key to a column name and take the inner value as column value.\n\n\n\nRecursive function that handles json elements with deeper structures\n\ndef recursive_parser(entry: dict, data_dict: dict, col_name: str = \"\") -&gt; dict:\n    \"\"\"Recursive parser for a list of nested JSON objects\n\n    Args:\n        entry (dict): A dictionary representing a single entry (row) of the final data frame.\n        data_dict (dict): Accumulator holding the current parsed data.\n        col_name (str): Accumulator holding the current column name. Defaults to empty string.\n    \"\"\"\n    for key, val in entry.items():\n        extended_col_name = f\"{col_name}_{key}\" if col_name else key\n        if isinstance(val, dict):\n            recursive_parser(entry[key], data_dict, extended_col_name)\n        else:\n            data_dict[extended_col_name].append(val)\n\nparsed_data = defaultdict(list)\n\nfor entry in entries:\n    recursive_parser(entry, parsed_data, \"\")\n\ndf = pd.DataFrame(parsed_data)\n\nNotice the check for a deeper structure with isinstance. If there is one, then the function is called again.\nFunction outputs a dict which is coerced into dataframe\nTo get rid of ‚Äúlabel‚Äù in column names: df.columns = [col if not \"label\" in col else \"_\".join(col.split(\"_\")[:-1]) for col in df.columns]\nobject types can be cast into more efficient types: df[\"im:rating\"] = df[\"im:rating\"].astype(int)",
    "crumbs": [
      "JSON"
    ]
  },
  {
    "objectID": "qmd/cli-linux.html",
    "href": "qmd/cli-linux.html",
    "title": "Linux",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "CLI",
      "Linux"
    ]
  },
  {
    "objectID": "qmd/cli-linux.html#sec-cli-lin-misc",
    "href": "qmd/cli-linux.html#sec-cli-lin-misc",
    "title": "Linux",
    "section": "",
    "text": "Notes from\n\nBash for Data Scientists, Data Engineers & MLOps Engineers\n\nBunch of other stuff that I didn‚Äôt take notes on\n\nBash Scripting on Linux: The Complete Guide - video course\n\nResources\n\nDoc - All on one page so you can just ctrl + f\nBash Scripting Cheatsheet\nCurl Docs\n\nman &lt;command&gt; displays documentation for command\nSpecial Characters\n\n\n‚Äú&gt;‚Äù redirects the output from a program to a file.\n\n‚Äú&gt;&gt;‚Äù does the same thing, but it‚Äôs appending to an existing file instead of overwriting it, if it already exists.\n\n\n.bashrc is a shell script that Bash runs whenever it is started interactively. It initializes an interactive shell session. You can put any command in that file that you could type at the command prompt.\n\nYou put commands here to set up the shell for use in your particular environment, or to customize things to your preferences.\nA common thing to put in .bashrc are aliases that you want to always be available.\n\nDebian vs.¬†Ubuntu (from ChatGPT)\n\nStability vs.¬†Freshness:\n\nDebian: Debian is known for its stability and reliability. It has a rigorous testing process and a conservative approach to updates, which makes it suitable for servers and systems where stability is crucial.\nUbuntu: Ubuntu is based on Debian but tends to be more up-to-date with software packages. It follows a time-based release cycle, with regular releases every six months. This can be appealing if you want access to the latest features and software.\n\nPackage Management:\n\nDebian: Debian uses the Debian Package Management System (dpkg) and Advanced Package Tool (APT) for package management. It has a vast repository of software packages.\nUbuntu: Ubuntu also uses dpkg and APT but adds its own software management tools like Snap and Ubuntu Software Center. This can make software installation more user-friendly.\n\nCommunity and Support:\n\nDebian: Debian has a large and dedicated community, and it‚Äôs known for its strong commitment to free and open-source software principles. It has a stable support structure, but community support may not be as user-friendly as Ubuntu‚Äôs.\nUbuntu: Ubuntu has a large and active community, and it offers both free and paid support options. The Ubuntu community is known for its user-friendliness and helpful forums, making it a good choice for beginners.\n\nVariants and Flavors:\n\nDebian: Debian offers different flavors, known as ‚ÄúDebian spins,‚Äù catering to various needs, such as Debian Stable, Debian Testing, and Debian Unstable. These variants differ in terms of software stability and freshness.\nUbuntu: Ubuntu has several official flavors (e.g., Ubuntu Desktop, Ubuntu Server, Kubuntu, Xubuntu) that come with different desktop environments. This variety allows users to choose an environment that suits their preferences.\n\nLicensing:\n\nDebian: Debian has a strict commitment to free and open-source software, prioritizing software that adheres to its Free Software Guidelines.\nUbuntu: While Ubuntu also includes mostly free and open-source software, it may include some proprietary drivers and software by default, which can be a concern for users who prioritize a completely open-source system.\n\nPerformance (Google Search AI)\n\nDebian is considered lightweight and much faster than Ubuntu. It comes with few pre-installed software.\n\nHardware (Google Search AI)\n\nDebian works well on older hardware. Debian still offers a 32-bit version of the distro, while Ubuntu no longer offers a 32-bit version.",
    "crumbs": [
      "CLI",
      "Linux"
    ]
  },
  {
    "objectID": "qmd/cli-linux.html#sec-cli-lin-com",
    "href": "qmd/cli-linux.html#sec-cli-lin-com",
    "title": "Linux",
    "section": "Commands",
    "text": "Commands\n\nBasic Commands\n\n\necho $SHELL - prints the type of shell you‚Äôre using\necho $PATH - prints all stored pathes\nexport PATH=\"my_new_path:$PATH\" - store a new path\nCommand Syntax: command -options arguments\nPiping Commands: cat user_names.txt|sort|uniq\n\n\n\nAliases\n\nCustom commands that you can define in order to avoid typing lengthy commands over and over again\nExamples\nalias ll=\"ls -lah\"\nalias gs=\"git status\"\nalias gp=\"git push origin master\"\nCreate safeguards for yourself\nalias mv=\"mv -i\"\n\nmv will automatically use the i flag, so the terminal will warn you if the file you‚Äôre about to move does already exist under the new directory,\n\nThis way you don‚Äôt accidentally overwrite files that you didn‚Äôt mean to overwrite.\n\n\n\n\n\nFiles/Directories\n\nList\n\n\nList 10 most recently modified files: ls -lt | head\nList files sorted by file size: ls -l -S\nList multiple directories: ls ./docs ./text ./data\n\nLook at first 3 rows: head -n3 students.csv\nCreate/Delete Directories\nmkdir &lt;dir_name&gt;\nrmdir &lt;dir_name&gt;\n\n-v means ‚Äúverbose‚Äù so it tells us if it was successful\n\nOutput to file: echo ‚ÄúThis is an example for redirect‚Äù &gt; file1.txt\nAppend line to file: echo ‚ÄúThis is the second line of the file‚Äù &gt;&gt; file1.txt\nCreate/Delete file(s):\n# Create files\ntouch file1.txt\ntouch file1.txt file2.tx\n\n# Delete files\nrm file1.txt\nrm file1.txt file2.txt\nMove files/dir; Rename\n# Move single file\nmv my_file.txt /tmp\n# Move multiple files\nmv file1 file2 file3 /tmp\n# move only .csv files to data directory and be verbose\nmv -v *.csv ./data/\n# Move a directory or multiple directories\nmv d1 d2 d3 /tmp\n# Rename the file using move command\nmv my_file1.txt my_file_newname.txt\n\nFile(s) and directories being moved to ‚Äútmp‚Äù directory\n\nSearch\n\nFind\n# syntax find &lt;path&gt; &lt;expression&gt;\n# Find by name\nfind . -name ‚Äúmy_file.csv\"\n#Wildcard search\nfind . -name \"*.jpg\"\n# Find all the files in a folder\nfind /temp\n# Search only files\nfind /temp -type f\n# Search only directories\nfind /temp -type d\n# Find file modified in last 3 hours\nfind . -mmin -180\n# Find files modified in last 2 days\nfind . -mtime -2\n# Find files not modified in last 2 days\nfind . -mtime +2\n# Find the file by size\nfind -type f -size +10M\nSearch inside files\n\nzgrep - Search the compressed file or files content just providing the search term.\n\nDefault: Prints the matched file name and the complete line\nFlags\n\n-i: Ignore case\n-n: Print only matched lines\n-v: Print only unmatched lines (i.e.¬†not pattern)\n-o: Print only the matched part\n-l: Print only file names\n-h: Print only file lines\n-c: Count matched lines\n-e: Multiple search terms\n\nExample: Search multiple files\nzgrep ismail auth.log.*.gz\n\nSearches for the term ‚Äúismail‚Äù all files beginning with ‚Äúauth.log.‚Äù in their names.\n*Could also provide each file‚Äôs name separated by a space*\n\nExample: Search for multiple terms\nzgrep -e \"ismail\" -e \"ahmet\" auth.log.2.gz\n\n\n\nLocate (faster)\n\nDocs\nInstall\nbash sudo apt install mlocate # Debian\nUsage\nsudo updatedb # update before using\nlocate .csv\n\nUnzip: unzip ./foia.zip\nSplit files\n# default: 1000 lines per file, names of new files: xaa, xab, xac, etc.\nsplit my_file\n\n# add a prefix to new file names\nsplit my_file my_prefix\n\n# specify split threshold (e.g. 5000) by number of lines\nsplit --lines=5000 my_file\n\n# specify split threshold by size (e.g. 10MB)\nsplit --bytes=10 MB my_file\nPermissions\n\nls -l See list of files and the permissions\n-rwxrwxrwx - sytax of permissions for a folder or directory\n\n‚Äúrwx‚Äù stand for read, write, and execute rights, respectively\nThe 3 ‚Äúrwx‚Äù blocks are for (1) user, (2) user group, and (3) everyone else.\n\nIn the given example, all 3 of these entities have read, write, as well as execute permissions.\n\nThe dash indicates that this is a file. Instead of the dash, you can also see a ‚Äúd‚Äù for directory or ‚Äúl‚Äù for a symbolic link.\n\nchmod - edit permissions\n\nExample: chmod u+x my_program.py - makes this file executable for yourself\n\nsudo - ‚Äúsuper user‚Äù - using this prefix gives you all the permissions to all the files\n\nsudo su - opens a stand alone super user shell\n\n\n\n\n\nPrint\n\nPrint file content\ncat &lt; my_file.txt\n# or\ncat my_file.txt\nPrint 1 pg at a time: less my_file.txt\nPrint specific number of lines: head -n&lt;num_lines&gt; &lt;file.csv&gt;\nPrint file content from bottom to top: tac my_file.txt\ncat -b log.txt | grep error : shows all lines in log.txt that contain the string ‚Äòerror‚Äô, along with the line number (-b)\n\n\n\nLogicals and Conditionals\n\nLogicals\n\n; : command1 ; command2\n\ncommand 1 and command 2 run independently of each other\n\n& : command1 & command2\n\ncommand 1 runs in the background and command 2 runs in the background\n\n&& : command1 && command2\n\nIf the first command errors out then the second command is not executed\n\n|| : command1 || command2\n\nThe second commmand is only execute if the first command errors\n\nExample\ncd my_dir && pwd || echo ‚ÄúNo such directory exist.Check‚Äù\n\nIf the my_dir exists, then the current working directory is printed. If the my_dir doesn‚Äôt exist, then the message ‚ÄúNo such directory exists. check‚Äù message is printed.\n\n\nConditionals\n\nUse [[ ]] for conditions in if / while statements, instead of [ ] or test.\n\n[[ ]] is a bash builtin, and is more powerful than [ ] or test.\nExample: if [[ -n \"${TRACE-}\" ]]; then set -o xtrace; fi\n\n\n\n\n\nString Matching\n\nExample: Search for ‚Äúerror‚Äù and write to file\n#output to a file again\ncat file1 file2 file3 | grep error | cat &gt; error_file.txt\n#Append to the end\ncat file1 file2 file3 | grep error | cat &gt;&gt; error_file.txt\n\nPrints lines into grep which searches for ‚Äúerror‚Äù in each line. Lines with ‚Äúerror‚Äù get written to ‚Äúerror_file.txt‚Äù\n\nFilter lines\ngrep -i ‚ÄúDoctorate‚Äù adult_t.csv |grep -i ‚ÄúHusband‚Äù|grep -i ‚ÄúBlack‚Äù|csvlook\n# -i, --ignore-case-Ignore¬† case¬† distinctions,¬† so that characters that differ only in case match each other.\n\nSelect all the candidates who have doctorates and a husband and race are Black\ncsvlook is pretty printing from csvkit package (see Big Data &gt;&gt; Larger Than Memory &gt;&gt; csvkit)\n\nCount how many rows fit the criteria\ngrep -i ‚ÄúDoctorate‚Äù adult_t.csv | wc -l\n\nCounts how many rows have ‚ÄúDoctorate‚Äù\n\n-wc is ‚Äúword count‚Äù",
    "crumbs": [
      "CLI",
      "Linux"
    ]
  },
  {
    "objectID": "qmd/cli-linux.html#sec-cli-lin-var",
    "href": "qmd/cli-linux.html#sec-cli-lin-var",
    "title": "Linux",
    "section": "Variables",
    "text": "Variables\n\nLocal Variable:\n\nDeclared at the command prompt\nUse lower case for name\nAvailable only in the current shell\nNot accessible by child processes or programs\nAll user-defined variables are local variables\n\nEnvironment (global) variables:\n\nCreate with export command\nUse upper case for name\nAvailable to child processes\n\nDeclare local and environment variables then access via ‚Äú$‚Äù\n# local\nev_car=‚ÄôTesla‚Äô\necho 'The ev car I like is' $ev_car\n\n# environment\nexport EV_CAR=‚ÄôTesla‚Äô\necho 'The ev car I like is' $EV_CAR\n\nNo spaces in variable assignment\n\nAlways quote variable accesses with double-quotes.\n\nOne place where it‚Äôs okay not to is on the left-hand-side of an [[ ]] condition. But even there I‚Äôd recommend quoting.\nWhen you need the unquoted behaviour, using bash arrays will likely serve you much better.\n\nFunctions\n\nUse local variables in functions.\nAccept multiple ways that users can ask for help and respond in kind.\n\nCheck if the first arg is -h or ‚Äìhelp or help or just h or even -help, and in all these cases, print help text and exit.\n\nWhen printing error messages, please redirect to stderr.\n\nUse echo 'Something unexpected happened' &gt;&2 for this",
    "crumbs": [
      "CLI",
      "Linux"
    ]
  },
  {
    "objectID": "qmd/cli-linux.html#sec-cli-lin-script",
    "href": "qmd/cli-linux.html#sec-cli-lin-script",
    "title": "Linux",
    "section": "Scripting",
    "text": "Scripting\n\nUse the .sh (or .bash) extension for your script\nUse long options, where possible (like ‚Äìsilent instead of -s). These serve to document your commands explicitly.\nIf appropriate, change to the script‚Äôs directory close to the start of the script.\n\nAnd it‚Äôs usually always appropriate.\nUse cd \"$(dirname \"$0\")\", which works in most cases.\n\nUse shellcheck ‚Äî analysis too for shell scripts. Heed its warnings. (link)\nShebang line\n\nContains the absolute path of the bash interpreter\n\nList paths to all shells: cat/etc/shells\n\nUse as the first line even if you don‚Äôt give executable permission to the script file.\nStarts with ‚Äú#!‚Äù the states the path of the interpreter\nExample: #!/bin/bash\n\nInterpreter installed in directory ‚Äú/bin‚Äù\n\nExample: #!/usr/bin/env bash\n\nCommands that should start your script\n\nUse set -o errexit\n\nSo that when a command fails, bash exits instead of continuing with the rest of the script.\n\nUse set -o nounset\n\nThis will make the script fail, when accessing an unset variable. Saves from horrible unintended consequences, with typos in variable names.\nWhen you want to access a variable that may or may not have been set, use \"${VARNAME-}\" instead of \"$VARNAME\", and you‚Äôre good.\n\nUse set -o pipefail\n\nThis will ensure that a pipeline command is treated as failed, even if one command in the pipeline fails.\n\nUse set -o xtrace, with a check on $TRACE env variable.\n\nFor copy-paste: if [[ -n \"${TRACE-}\" ]]; then set -o xtrace; fi.\nThis helps in debugging your scripts, a lot.\nPeople can now enable debug mode, by running your script as TRACE=1 ./script.sh instead of ./script.sh .\n\n\nExample: Basic Execution a Bash Script\n\nCreate a directory bash_script: mkdir bash_script\nCreate a hello_world.sh file: touch hello_script.sh\nOpen hello_script.sh (text editor?)\nAdd code, save, and close\n#!/bin/bash\necho ‚ÄòHello World‚Äô\nMake file executable: chmod +x hello_world.sh\nExecute file: ./hello_world.sh\n\nSetting and Executing Scripts with Arguments\n\nTemplate\n#!/usr/bin/env bash\nset -o errexit\nset -o nounset\nset -o pipefail\nif [[ -n \"${TRACE-}\" ]]; then\n¬† ¬† set -o xtrace\nfi\nif [[ \"$1\" =~ ^-*h(elp)?$ ]]; then\n¬† ¬† echo 'Usage: ./script.sh arg-one arg-two\nThis is an awesome bash script to make your life better.\n'\n¬† ¬† exit\nfi\ncd \"$(dirname \"$0\")\"\nmain() {\n¬† ¬† echo do awesome stuff\n}\nmain \"$@\"",
    "crumbs": [
      "CLI",
      "Linux"
    ]
  },
  {
    "objectID": "qmd/cli-linux.html#sec-cli-lin-jobm",
    "href": "qmd/cli-linux.html#sec-cli-lin-jobm",
    "title": "Linux",
    "section": "Job Management",
    "text": "Job Management\n\nPrograms/Scripts will by default run in the foreground, and prevent you from doing anything else until the program is done.\nWhile program is running:\n\ncontrol+c - Will send a SIGINT (signal interrupt) signal to the program, which instructs the machine to interrupt the program immediately (unless the program has a way to handle these signals internally).\ncontrol+z - Will pause the program.\n\nAfter pausing the program can be continued either by bringing it to the foreground (fg), or by sending it to the backgroud (bg).\n\n\nExecute script to run in the background: python run.py &\njobs - shows all running jobs and process ids (PIDS)\nkill - sends signals to jobs running in the background\n\nkill -STOP %1 sends a STOP signal, pausing program 1.\nkill -KILL %1 sends a KILL signal, terminating program 1 permanently.",
    "crumbs": [
      "CLI",
      "Linux"
    ]
  },
  {
    "objectID": "qmd/cli-linux.html#sec-cli-lin-tmux",
    "href": "qmd/cli-linux.html#sec-cli-lin-tmux",
    "title": "Linux",
    "section": "tmux",
    "text": "tmux\n\nTerminal Multiplexer\nEnables you to easily create new terminal sessions and navigate between them. This can be extremely useful, for example you can use one terminal to navigate your file system and another terminal to execute jobs.\nInstallation (if necessary): sudo apt install tmux\n\nTypically comes with the linux installation\n\nSessions\n\ntmux - starts an unnamed session\ntmux new -s moose creates new terminal session with name ‚Äòmoose‚Äô\ntmux ls - lists all running sessions\ntmux kill-session -t moose - kills session named ‚Äúmoose‚Äù\nexit - stops and quits the current session\nKill all sessions (various opinions on how to do this)\n\ntmux kill-session\ntmux kill-server\ntmux ls | grep : | cut -d. -f1 | awk '{print substr($1, 0, length($1)-1)}' | xargs kill\n\n\nAttach/Detach\n\nWhen you log out of a remote machine (either on purpose or accidentally), all of the programs that were actively running inside your shell are automatically terminated. On the other hand, if you run your programs inside a tmux shell, you can come simply detach the tmux window, log out, close your computer, and come back to that shell later as if you‚Äôve never been logged out.\ntmux detach - detach current session\ncontrol+bthen pressd`: When you have multiple sesssions running, this will allow you to select the session to detach\nFrom inside bash and not inside a session\n\ntmux a : attach to latest created session\ntmux a -t moose : attach to session called ‚Äòmoose‚Äô\n\n\nPane Creation and Navigation\n\ncontrol+b then press ‚Äù (i.e.¬†shift+‚Äô): add another terminal pane below\ncontrol+b then press % (i.e.¬†shift+5) : add another terminal pane to the right\ncontrol+b then press ‚Üí : move to the terminal pane on the right (similar for left, up, down)",
    "crumbs": [
      "CLI",
      "Linux"
    ]
  },
  {
    "objectID": "qmd/cli-linux.html#sec-cli-lin-ssh",
    "href": "qmd/cli-linux.html#sec-cli-lin-ssh",
    "title": "Linux",
    "section": "SSH",
    "text": "SSH\n\nTypically uses a key pair to log into remote machines\n\nKey pair consists of a public key (which both machines have access to) and a private key (which only your own machine has access to)\n‚Äússh-keygen‚Äù is a program for generating such a key pair.\n\nIf you run ssh-keygen, it will by default create a public key named ‚Äúid_rsa.pub‚Äù and a private key named ‚Äúid_rsa‚Äù, and place both into your ‚Äú~/.ssh‚Äù directory\nYou‚Äôll need to add the public key to the remote machine by piping together cat, ssh, and a streaming operator\n\ncat .ssh/id_rsa.pub | ssh user@remote 'cat &gt;&gt; ~/.ssh/authorized_keys'\n\n\n\nConnect to the remote machine: ssh remote -i ~/.ssh/id_rsa\nCreate a config file instead\n\nLocation: ‚Äú~/.ssh/config‚Äù\nContents\nHost dev\n¬† HostName remote\n¬† IdentityFile ~/.ssh/id_rsa\n\nConnect using config: ssh dev\nFor Windows and using Putty, see\n\nAWS &gt;&gt; EC2 &gt;&gt; Connect to/ Terminate Instance\nProjects Notebook &gt;&gt; Article, Nested Cross Validation &gt;&gt; Notes &gt;&gt; Running EC2 instances checklist",
    "crumbs": [
      "CLI",
      "Linux"
    ]
  },
  {
    "objectID": "qmd/cli-linux.html#sec-cli-lin-vim",
    "href": "qmd/cli-linux.html#sec-cli-lin-vim",
    "title": "Linux",
    "section": "Vim",
    "text": "Vim\n\nCommand-line based text editor\nCommon Usage\n\nLogging into a remote machine and need to make a code change there. vim is a standard program and therefore usually available on any machine you work on.\nWhen running git commit, by default git opens vim for writing a commit message. So at the very least you‚Äôll want to know how to write, save, and close a file.\n\n2 modes: Navigation Mode; Edit Mode\n\nWhen Vim is launched you‚Äôre in Navigation mode\nPress i to start edit mode, in which you can make changes to the file.\nPress Esc key to leave edit mode and go back to navigation mode.\n\nCommands (Cheatsheet)\n\nx deletes a character\ndd deletes an entire row\nb (back) goes to the previous word\nn (next) goes to the next word\n:wq saves your changes and closes the file\n:q! ignores your changes and closes the file",
    "crumbs": [
      "CLI",
      "Linux"
    ]
  },
  {
    "objectID": "qmd/cli-linux.html#sec-cli-lin-pkg",
    "href": "qmd/cli-linux.html#sec-cli-lin-pkg",
    "title": "Linux",
    "section": "Packages",
    "text": "Packages\n\nCommon package managers: apt, Pacman, yum, and portage\nAPT (Advanced Package Tool)\n\nThe apt command is a smaller section of the apt-get and apt-cache options. The apt command gives the end user just enough tools to install, remove, search and update APT packages. The apt-get command has a lot more options that are useful for writing low-level scripts and tools.\nInstall Packages\n# one pkg\nsudo apt-get install &lt;package_name&gt;\n# multiple\nsudo apt-get install &lt;pkg_name1&gt; &lt;pkg_name2&gt;\n\nInstall but no upgrade: sudo apt-get install &lt;pkg_name&gt; --no-upgrade\n\nSearch for an installed package: apt-cache search &lt;pkg_name&gt;\nUpdate package information prior to ‚Äúupgrading‚Äù the packages\nsudo apt-get update\n\nDownloads the package lists from the repositories and ‚Äúupdates‚Äù them to get information on the newest versions of packages and their dependencies.\n\nUpgrade\n# all installed packages\nsudo apt-get upgrade\n\n# To upgrade only a specific program\nsudo apt-get upgrade &lt;package_name&gt;\n\n# Upgrades and handles dependencies; delete obsolete, add new\napt-get dist-upgrade\n\n# together\nsudo apt-get update && sudo apt-get dist-upgrade",
    "crumbs": [
      "CLI",
      "Linux"
    ]
  },
  {
    "objectID": "qmd/cli-linux.html#sec-cli-lin-expr",
    "href": "qmd/cli-linux.html#sec-cli-lin-expr",
    "title": "Linux",
    "section": "Expressions",
    "text": "Expressions\n\nSort data, filter only unique lines, and write to file: cat adult_t.csv | sort | uniq -c &gt; sorted_list.csv",
    "crumbs": [
      "CLI",
      "Linux"
    ]
  },
  {
    "objectID": "qmd/cli-general.html",
    "href": "qmd/cli-general.html",
    "title": "General",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "CLI",
      "General"
    ]
  },
  {
    "objectID": "qmd/cli-general.html#sec-cli-gen-misc",
    "href": "qmd/cli-general.html#sec-cli-gen-misc",
    "title": "General",
    "section": "",
    "text": "Resources\n\nData Science at the Command Line\n\nctrl-rshell command history search\n\nMcFly - intelligent command history search engine that takes into account your working directory and the context of recently executed commands. McFly‚Äôs suggestions are prioritized in real time with a small neural network\n\nPath to a folder that‚Äôs above root folder:\n\n1 level up: ../desired-folder\n2 levels up: ../../desired-folder",
    "crumbs": [
      "CLI",
      "General"
    ]
  },
  {
    "objectID": "qmd/cli-general.html#sec-cli-gen-r",
    "href": "qmd/cli-general.html#sec-cli-gen-r",
    "title": "General",
    "section": "R",
    "text": "R\n\nResources\n\nInvoking R from the command line for using R and R CMD\n\nRscript need to be on PATH\nRun R (default version) in the shell:\nRS\n# or \nrig run\n\nRS might require {rig} to be installed\nTo run a specific R version that‚Äôs already installed: R-4.2\n\nRun an R script:\nRscript \"path\\to\\my-script.R\"\n# or\nrig run -f &lt;script-file&gt;\nEvaluate an R expression:\nRscript -e &lt;expression&gt; \n# or \nrig run -e &lt;expression&gt;\nRun an R app: rig run &lt;path-to-app&gt;\n\nPlumber APIs\nShiny apps\nQuarto documents (also with embedded Shiny apps)\nRmd documents (also with embedded Shiny apps)\nStatic web sites\n\nMake an R script pipeable (From link)\nparallel \"echo 'zipping bin {}'; cat chunked/*_bin_{}_*.csv | ./upload_as_rds.R '$S3_DEST'/chr_'$DESIRED_CHR'_bin_{}.rds\"\n#!/usr/bin/env Rscript\nlibrary(readr)\nlibrary(aws.s3)\n\n# Read first command line argument\ndata_destination &lt;- commandArgs(trailingOnly = TRUE)[1]\n\ndata_cols &lt;- list(SNP_Name = 'c', ...)\n\ns3saveRDS(\n  read_csv(\n        file(\"stdin\"), \n        col_names = names(data_cols),\n        col_types = data_cols \n    ),\n  object = data_destination\n)\n\nBy passing readr::read_csv the function, file(\"stdin\"), it loads the data piped to the R script into a dataframe, which then gets written as an .rds file directly to s3 using {aws.s3}.\n\nKilling a process\nsystem(\"taskkill /im java.exe /f\", intern=FALSE, ignore.stdout=FALSE)\nStarting a process in the background\n# start MLflow server\nsys::exec_background(\"mlflow server\")\nDelete an opened file in the same R session\n\nYou **MUST** unlink it before any kind of manipulation of object\n\nI think this works because readr loads files lazily by default\n\nExample:\nwisc_csv_filename &lt;- \"COVID-19_Historical_Data_by_County.csv\"\ndownload_location &lt;- file.path(Sys.getenv(\"USERPROFILE\"), \"Downloads\")\nwisc_file_path &lt;- file.path(download_location, wisc_csv_filename)\nwisc_tests_new &lt;- readr::read_csv(wisc_file_path)\n# key part, must unlink before any kind of code interaction\n# supposedly need recursive = TRUE for Windows, but I didn't need it\n# Throws an error (hence safely) but still works\nsafe_unlink &lt;- purrr::safely(unlink)\nsafe_unlink(wisc_tests_new)\n\n# manipulate obj\nwisc_tests_clean &lt;- wisc_tests_new %&gt;%\n¬† ¬† ¬† janitor::clean_names() %&gt;%\n¬† ¬† ¬† select(date, geo, county = name, negative, positive) %&gt;%\n¬† ¬† ¬† filter(geo == \"County\") %&gt;%\n¬† ¬† ¬† mutate(date = lubridate::as_date(date)) %&gt;%\n¬† ¬† ¬† select(-geo)\n# clean-up\nfs::file_delete(wisc_file_path)\n\nFind out which process is locking or using a file\n\nOpen Resource Monitor, which can be found\n\nBy searching for Resource Monitor or resmon.exe in the start menu, or\nAs a button on the Performance tab in your Task Manager\n\nGo to the CPU tab\nUse the search field in the Associated Handles section\n\ntype the name of file in the search field and it‚Äôll search automatically\n35548",
    "crumbs": [
      "CLI",
      "General"
    ]
  },
  {
    "objectID": "qmd/cli-general.html#sec-cli-gen-awk",
    "href": "qmd/cli-general.html#sec-cli-gen-awk",
    "title": "General",
    "section": "AWK",
    "text": "AWK\n\n\nMisc\n\nResources\n\nDocs\nAwk - A Tutorial and Introduction\n\n\nPrint first few rows of columns 1 and 2\nawk -F, '{print $1,$2}' adult_t.csv|head\nFilter lines where no of hours/ week (13th column) &gt; 98\nawk -F, ‚Äò$13 &gt; 98‚Äô adult_t.csv|head\nFilter lines with ‚ÄúDoctorate‚Äù and print first 3 columns\nawk '/Doctorate/{print $1, $2, $3}' adult_t.csv\nRandom sample 8% of the total lines from a .csv (keeps header)\n'BEGIN {srand()} !/^$/ {if(rand()&lt;=0.08||FNR==1) print &gt; \"rand.samp.csv\"}' big_fn.csv\nDecompresses, chunks, sorts, and writes back to S3 (From link)\n# Let S3 use as many threads as it wants\naws configure set default.s3.max_concurrent_requests 50\n\nfor chunk_file in $(aws s3 ls $DATA_LOC | awk '{print $4}' | grep 'chr'$DESIRED_CHR'.csv') ; do\n\n        aws s3 cp s3://$batch_loc$chunk_file - |\n        pigz -dc |\n        parallel --block 100M --pipe  \\\n        \"awk -F '\\t' '{print \\$1\\\",...\\\"$30\\\"&gt;\\\"chunked/{#}_chr\\\"\\$15\\\".csv\\\"}'\"\n\n        # Combine all the parallel process chunks to single files\n        ls chunked/ |\n        cut -d '_' -f 2 |\n        sort -u |\n        parallel 'cat chunked/*_{} | sort -k5 -n -S 80% -t, | aws s3 cp - '$s3_dest'/batch_'$batch_num'_{}'\n\n        # Clean up intermediate data\n        rm chunked/*\ndone\n\nUses pigz to parallelize decompression\nUses GNU Parallel (site, docs, tutorial1, tutorial2) to parallelize chunking (100MB chunks in 1st section)\nChunks data into smaller files and sorts them into directories based on a chromosome column (I think)\nAvoids writing to disk",
    "crumbs": [
      "CLI",
      "General"
    ]
  },
  {
    "objectID": "qmd/cli-windows.html",
    "href": "qmd/cli-windows.html",
    "title": "Windows",
    "section": "",
    "text": "PowerShell",
    "crumbs": [
      "CLI",
      "Windows"
    ]
  },
  {
    "objectID": "qmd/cli-windows.html#sec-cli-win-powsh",
    "href": "qmd/cli-windows.html#sec-cli-win-powsh",
    "title": "Windows",
    "section": "",
    "text": "Misc\n\nDocs, Sample Scripts\nUse Get-Help &lt;cmdlet-name&gt; to access help information for specific cmdlets.\nCheck version: $PSVersionTable\n\nFor a breakdown of the version number (e.g.¬†build, revison, etc.): $PSVersionTable.PSVersion\n\nUpdate to latest stable version: github\nComments: # comment\nClear terminal: clear or cls or Clear-Host\nBefore you‚Äôll be able to run a script, you need to open PowerShell as administrator and execute this command: Set-ExecutionPolicy RemoteSigned\nSingle Wildcard: ?\n\nExample: Matching for am? would give you files named ‚Äúamy‚Äù ‚Äúamd‚Äù and ‚Äúam3.‚Äù\n\nShortcuts\n\nRun selected PowerShell code in current terminal using F8\nLaunch online help for the symbol under the cursor using Ctrl + F1\n\n\n\n\nLoops\n\nIterables\n\nArrays : $folders = @('C:\\Folder','C:\\Program Files\\Folder2','C:\\Folder3')\n$_ (Docs)\n\nAutomatic Variable; Alias for $PSItem\n\nDocs for automatic variables\n\nUseful for looping through objects in a directory. See ‚ÄúChange Name (or Extensions) of Multiple Files‚Äù for an example of usage\nGet the properties of an object\n\nExample: Using Get-ItemProperty (Docs)\nGet-ItemProperty scrapsheet.txt | Format-List\n\nProperty names will be on the left side of the output\n\nExample: Using Get-Member (Docs)\n Get-ChildItem *.txt | Select-Object -First 1 | Get-Member\n\nSelects the first text file in the directory and gets the Properties and a bunch of other stuff like methods which also can be used with $_\nDescriptions are awful for a lot of the stuff, but for the most part, you can guess what the property is.\n\n\nSome properties that can be used with $_\n\nFile System Objects:\n\n.Name: Returns the name component of a file\n.FullName: Returns the full path of the file.\n.Length: Returns the size of the file in bytes.\n.CreationTime: Returns the date and time the file was created.\n.LastWriteTime: Returns the date and time the file was last modified.\n.IsReadOnly: Returns $true if the file is read-only, $false otherwise.\n\nProcess Objects:\n\n.Id: Returns the unique identifier (PID) of the process.\n.Name: Returns the name of the process.\n.MainWindowTitle: Returns the title of the process‚Äôs main window (if applicable).\n.WorkingDirectory: Returns the working directory of the process.\n.CPU: Returns the CPU usage of the process.\n\nRegistry Objects:\n\n.Name: Returns the name of the registry key.\n.Hive: Returns the hive of the registry key (e.g., HKLM, HKCU).\n.Value: Returns the value of the registry key (if applicable).\n.Path: Returns the full path of the registry key.\n\n\n\n\n\n\nForeach\n\nUses a typical for-loop structure\nSee Snippets for an example of iterating over the output of Get-ChildItem\nIterate over an array\n# Create an array of folders\n$folders = @('C:\\Folder','C:\\Program Files\\Folder2','C:\\Folder3')\n\n# Perform iteration to create the same file in each folder\nforeach ($i in $folders) {\n    Add-Content -Path \"$i\\SampleFile.txt\" -Value \"This is the content of the file\"\n}\n\n$i is the for-loop variable and $folders is the iterable\nAdd-Content creates a text file in each of the folders in the array.\n\n\n\n\nForEach-Object\n\nSimilar to {purrr::map}\nIterable is piped into ForEach-Object\n$folders = @('C:\\Folder','C:\\Program Files\\Folder2','C:\\Folder3')\n$folders | ForEach-Object (Add-Content -Path \"$_\\SampleFile.txt\" -Value \"This is the content of the file\")\n\nDoes the same thing as the first example in the Foreach section\nAdd-Content creates a text file in each of the folders in the array.\n$_ is the for-loop variable ‚Äî called an ‚Äúautomatic variable.‚Äù See Iterables section.\n\n\n\n\nForEach Method\n\nSimilar to using Pyhon‚Äôs apply on an iterable.\nMethod applied an array\n$folders = @('C:\\Folder','C:\\Program Files\\Folder2','C:\\Folder3')\n$folders.ForEach({\n    Add-Content -Path \"$_\\SampleFile.txt\" -Value \"This is the content of the file\"\n})\n\nDoes the same thing as the first example in the Foreach section\nAdd-Content creates a text file in each of the folders in the array.\n$_ is the for-loop variable ‚Äî called an ‚Äúautomatic variable.‚Äù See Iterables section.\n\n\n\n\n\nCommands\n\nChange directories\n Set-Location \"Documents\\R\\Projects\"\nCreate a New Folder\n New-Item -ItemType Directory -Path \"Folder Name\"\n\nAssumes you‚Äôre already in the directory that you want the folder in. You can also use a path, e.g.¬†\"C:\\Temp\\Documents\\New Folder\\Subfolder1\\\\Subfolder2\".\n\nChange Name of File\nRename-Item -Path \"c:\\logfiles\\daily_file.txt\" -NewName \"monday_file.txt\"\nChange Name (or Extensions) of Multiple Files\nGet-ChildItem *.md | Rename-Item -NewName { $_.Name -replace '.md','.qmd' }\n\nGet-ChildItem: cmdlet gets all the files in the current folder that have a .mc file extension\nResults piped to Rename-Item\n\nNewName: Has a value that‚Äôs a script block that runs before the value is submitted to the NewName parameter.\n\n$_: (automatic variable) It represents each file object as it comes to the command through the pipeline.\n-replace: Operator replaces the file extension of each file with .qmd.\n\nNotice that matching using the -replace operator is not case sensitive.\n\n\n\n\nExecute a File\nInvoke-Item configuration.cmd\nMulti-line Commands\nffmpeg -i input.mkv -map 0:v:0 `\n       -map 0:a:2 -map 0:a:0 -map 0:a:1 -map 0:a:3 `\n       -map 0:s -c copy `\n       -disposition:a:0 default `\n       reordered.mkv\n\nIn bash, it‚Äôs a backslash (\\), but in Powershell, it‚Äôs a backtick ( ` )\n*Don‚Äôt forget that there‚Äôs a space between the last character and the backtick.*\nIn practice, this will look like\nffmpeg -i .input.mkv -map 0:v:0 `\n&gt;&gt; -map 0:a:2 -map 0:a:0 -map 0:a:1 -map 0:a:3 `\n&gt;&gt; -map 0:s -c copy `\n&gt;&gt; -disposition:a:0 default `\n&gt;&gt; reordered.mkv\n\nString Matching\n\nPrint line with pattern\nSelect-String -Path \"file*.txt\" -Pattern \"error\"\nfile1.txt:3:This is the error line of the file\nfile2.txt:3:This is the error line of the file\nfile3.txt:3:This is the error line of the file\n\nMatches the 3rd line of each file\n\n\nGet stats on a process\nGet-Process -Name chrome\n\nHandles: The number of handles that the process has opened.\nNPM(K): The amount of non-paged memory that the process is using, in kilobytes.\nPM(K): The amount of pageable memory that the process is using, in kilobytes.\nWS(K): The size of the working set of the process, in kilobytes. The working set consists of the pages of memory that were recently referenced by the process.\nVM(M): The amount of virtual memory that the process is using, in megabytes. Virtual memory includes storage in the paging files on disk.\nCPU(s): The amount of processor time that the process has used on all processors, in seconds.\nID: The process ID (PID) of the process.\nProcessName: The name of the process. For explanations of the concepts related to processes, see the Glossary in Help and Support Center and the Help for Task Manager.\n\nEnvironment Variables\n\nSet an environment variable\nSet-Item -Name PYTHONSTARTUP -Value C:\\path\\to\\pythonstartup.py\n\nSame expression to modify existing environment variable\nOr\n$env:QUARTO_DENO_EXTRA_OPTIONS = \"--v8-flags=--max-old-space-size=8192\"\n\nDelete environment variable\nRemove-Item -Name &lt;variable_name&gt;\nVerify value of an environment variable\n$env:&lt;variable_name&gt;\n\nPorts\n\nFind application using a port.\nnetstat -aon | findstr ':80'\nnetstat -anp | find \":80\"\n\nIf port 80 is being used by the application, it will return a PID. Then you can find it in Task Manager &gt;&gt; Processess\n\nList all Listening and Established ports\nnetstat -anob\nCheck for processes using a port\nGet-Process -Id (Get-NetTCPConnection -LocalPort 80).OwningProcess\nTest connection to local port to see if it‚Äôs open\nTest-NetConnection -ComputerName localhost -Port 80 | Select-Object TcpTestSucceeded\nCheck firewall settings for an app\nnetsh advfirewall firewall show rule name=\"name_of_app\"\n\n\n\n\nSnippets\n\nRead in name of servers and ping each of them\n\n$servers = Get-Content .\\servers.txt\n\nforeach ($server in $servers) {\n    try {\n        $null = Test-Connection -ComputerName $server -Count 1 -ErrorAction STOP\n        Write-Output \"$server - OK\"\n    }\n    catch {\n        Write-Output \"$server - $($_.Exception.Message)\"\n    }\n}\n\nGet-Content reads the server names from each line in the the server.txt file\nforeach iterates through the server names\ntry tests the connection and catch outputs an error message if a server fails.\nIf Test-Connection fails the error message is stored in the $null variable\nThe error message line has an interesting syntax\n\n$_ is an automatic variable that represents $null which contains the error message which is selected by .Exception-Message.\n$() evaluates the expression\n\n\nTake files from a directory and iterate them as inputs to a function.\n$directory = \"C:\\Users\\me\\Documents\\AnyCap Screen Recorder\"\n\n# Define the FFmpeg command\n$ffmpegCommand = '-i {0} ' +\n                 '-c:v libx265 ' +\n                 '-crf 28 ' +\n                 '-preset medium ' +\n                 '-vf scale=-1:720 ' +\n                 '-c:a copy ' +\n                 'C:\\Users\\me\\Documents\\temp-storage\\{1}'\n\n# Get all files in the directory\n$files = Get-ChildItem -Path $directory `\n                       -Filter \"*.mp4\" \n\n# Loop through each file and apply the FFmpeg command\nforeach ($file in $files) {\n  # Construct the full command with the current file path\n  $fullCommand = $ffmpegCommand -f \"`\"$($file.FullName)`\"\", $file.Name\n  # Execute the FFmpeg command\n  Start-Process -FilePath \"ffmpeg.exe\" `\n                -ArgumentList $fullCommand `\n                -Wait `\n                -NoNewWindow\n}\n\nWrite-Host \"Finished processing files!\"\n\n$ffmpegCommand variable is a concantenated string using multiple lines for readability. {0} and {1} are placeholders to be filled in later.\n\nNote the space included at the end of each argument before the single quote since there‚Äôs no space included during concantenation.\n\nGet-ChildItem retrieves files from the specified directory ($directory).\n\n-Filter filters files that match the pattern (e.g., *.mp4).\n\nThe foreach loop iterates through each file ($file) in the $files collection.\n\n-f flag stands for format. Says to replace {0} and {1} in the $ffmpegCommand template with these properties.\n\"`\"$($file.FullName)`\"\"\n\nSince the directory name has spaces in it, extra quotes must included in order for the path to be quoted within the output string. A quoted file path in necessary forffmpeg to be able to read a directory name with spaces in it.\n$file.FullName: This is the full path of the current file. It is enclosed in $() to ensure that the property is properly evaluated and its value is included in the string.\n\nIf there were no spaces in the directory name, then $file.FullName is only thing that would be required. Everything else in this description could be discarded\n\n`\"$($file.FullName)`\": The double quotes \"...\" are used to create a string literal. Placing the entire expression $($file.FullName) within these double quotes ensures that the value of $file.FullName is treated as a single string, even if it contains spaces or special characters.\n\nThe backticks are escape characters in PowerShell and indicate that the double quotes should be treated as literal characters and not as operators formatting a string.\n\n\"`\"\\$(\\$file.FullName)`\"\": The additional double quotes at the beginning and end are used to format the expression as string for when it‚Äôs used as an argument in Start-Process.\nThe resulting path in the ffmpeg argument will look like: \"&lt;full file path&gt;\".\n\nStart-Process launches ffmpeg.exe with the constructed $fullCommand arguments.\n\n-Wait ensures the command finishes before continuing.\n-NoNewWindow says run ffmpeg in the same console window and don‚Äôt open a new one.",
    "crumbs": [
      "CLI",
      "Windows"
    ]
  },
  {
    "objectID": "qmd/cli-windows.html#sec-cli-win-batscri",
    "href": "qmd/cli-windows.html#sec-cli-win-batscri",
    "title": "Windows",
    "section": "Batch Scripting",
    "text": "Batch Scripting\n\nMisc\n\nResources\n\nWindows Batch Scripting\n\nTo keep the prompt window open after script execution, place these either of these commands at end of your script.\n\npause: Keeps window open until you press any key.\nVia timer: e.g.¬†timeout /t 300\ncmd /k: The prompt will remain active and you can execute additional commands manually.\n\n\nExample: Create variables and execute\n@echo off\n\nrem Set the path to the Rscript executable\nset RSCRIPT=\"C:\\Users\\user\\AppData\\Local\\Programs\\R\\R-4.2.3\\bin\\Rscript.exe\"\n\nrem Set the path to the R script to execute\nset RSCRIPT_FILE=\"C:\\Users\\user\\my_r_script.R\"\n\nrem Execute the R script\n%RSCRIPT% %RSCRIPT_FILE%\n\nrem Pause so the user can see the output\nexit\n\n@echo off - This line turns off the echoing of commands in the command prompt window, making the output cleaner.\nrem - Keyword that denotes a comment in a batch file.\nset RSCRIPT= - This line assigns the path to the Rscript executable to the environment variable RSCRIPT.\nset RSCRIPT_FILE= - The path to the R script file is assigned to the environment variable RSCRIPT_FILE.\n%RSCRIPT% %RSCRIPT_FILE% - Executes the R script using the Rscript executable and passes the path to the R script file as an argument.\nexit - This command exits the batch file and closes the command prompt window.\n\nExample: Exit if script errors\nRscript \"C:\\Users\\ercbk\\Documents\\R\\Projects\\Indiana-COVID-19-Tracker\\R\\collection\\build-opentab-dat.R\"\n\nREM if the data building script errors, bat script terminates without running other scripts or commands\nif %errorlevel% neq 0 exit /b %errorlevel%\n\ncd \"C:\\Users\\ercbk\\Documents\\R\\Projects\\Indiana-COVID-19-Tracker\"\n\ngit add data/YoY_Seated_Diner_Data.csv\ngit commit -m \"opentab data update\"\ngit pull\ngit push\n\nEXIT",
    "crumbs": [
      "CLI",
      "Windows"
    ]
  },
  {
    "objectID": "qmd/cli-windows.html#sec-cli-win-wsl",
    "href": "qmd/cli-windows.html#sec-cli-win-wsl",
    "title": "Windows",
    "section": "WSL",
    "text": "WSL\n\nResources\n\nDocs\nTo update password (link) using username\n\nNotes from\n\nBeware the IDEs of Windows (Subsystem for Linux)\n\nLoad Linux: wsl -d Ubuntu-22.04 where -d is for ‚Äìdistribution\nStop Linux\n\nall distros: wsl shutdown\n1 distro: wsl terminate &lt;distro name&gt;\n\nExit linux terminal back to command prompt or powershell: exit\nWSL Help: wsl --help\nWSL Version: wsl --version\nAvailable Linux Distributions and the version of WSL they‚Äôre installed under\nPS C:\\Users\\tbats&gt; wsl --list --verbose\n  NAME            STATE           VERSION\n* Ubuntu-22.04    Stopped         2\n\nVersion 2 says 22.04 is working in WSL 2\n\nUninstall a distribution: wsl --unregister &lt;distro name&gt;\nUpdate WSL: wsl --update\n\n--web-download: Download the latest update from the GitHub rather than the Microsoft Store.\n\nDownload apps\nsudo apt install &lt;app&gt;\n\nSee ClI, Linux &gt;&gt; Packages\n\nUpdate Linux Distro\nsudo apt update\nsudo apt upgrade\nWSL Configs:\n\nGlobal config affects all distributions and the Distribution config on affects that particular distribution\nGlobal Config: C:\\Users\\&lt;user name&gt;\\.wslconfig\n\nDocs\nThe .wslconfig file does not exist by default. It must be created and stored in your %UserProfile% directory to apply these configuration settings.\n\nDistribution Config:\n\nDocs\nType \\\\wsl.localhost in file explorer path box\n\nOr if you know the name of the distribution: \\\\wsl.localhost\\Ubuntu-22.04\\etc\nCreates a Network connection to wsl.localhost\n\nClick on the particular distribution\nGo to etc\\wsl.conf\n\n\nPaths\n\nUbuntu mounts the Windows C: drive at /mnt/c/\nWindows locates the Ubuntu root directory at \\wsl.localhost\\Ubuntu-22.04\nRoot Paths\n\n\n\n\n\n\n\n\nFolder:\nUbuntu path (bash):\nWindows path (PowerShell):\n\n\n\n\nUbuntu user directory\n/home/&lt;linux user name&gt;\n\\wsl.localhost\\Ubuntu-22.04\\home\\&lt;linux user name&gt;\n\n\nWindows user directory\n/mnt/c/Users/&lt;windows user name&gt;\nC:\\Users\\&lt;windows user name&gt;\n\n\n\nExample:\n\nLocating the Projects folder on the Linux file system while in Bash, /home/&lt;linux user name&gt;/Projects\nLocating the Projects folder on the Linux file system while in PowerShell, \\wsl.localhost\\Ubuntu-22.04\\home\\&lt;linux user name&gt;\\Projects\n\n\nIncrease RAM allocation and number of processors\n\nDocs\nDefaults:\n\nWSL limits distros to only 50% of your total memory on Windows\nThe same number of logical processors on Windows\n\nSetting in .wslconfig\n# Settings apply across all Linux distros running on WSL 2\n[wsl2]\n\n# Limits VM memory to use no more than 4 GB, this can be set as whole numbers using GB or MB\nmemory=4GB \n\n# Sets the VM to use two virtual processors\nprocessors=2\n\n[wsl2] is a section label even though all the settings underneath it aren‚Äôt indented. I‚Äôm guessing all settings underneath it belong to it until another section heading is encounterd.",
    "crumbs": [
      "CLI",
      "Windows"
    ]
  },
  {
    "objectID": "qmd/confidence-and-prediction-intervals.html",
    "href": "qmd/confidence-and-prediction-intervals.html",
    "title": "Confidence & Prediction Intervals",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Confidence & Prediction Intervals"
    ]
  },
  {
    "objectID": "qmd/confidence-and-prediction-intervals.html#sec-cipi-misc",
    "href": "qmd/confidence-and-prediction-intervals.html#sec-cipi-misc",
    "title": "Confidence & Prediction Intervals",
    "section": "",
    "text": "Also see Mathematices, Statistics &gt;&gt; Descriptive Statistics &gt;&gt; Understanding CI, sd, and sem Bars\nSE used for CIs of the difference in proportion\n\\[\n\\text{SE} = \\sqrt{\\frac{p_1(1-p_1)}{n_1} + \\frac{p_2(1-p_2)}{n_2}}\n\\]\nProfile CIs\n\nAlso see\n\nRegression, Ordinal &gt;&gt; Proportional Odds &gt;&gt; Example 3\nRegression, Ordinal &gt;&gt; Mixed Effects &gt;&gt; Example 1\n\nUnlike Wald CIs that rely on the normal distribution of the parameter estimate, profile CIs leverage the likelihood function of the model.\nThey essentially fix all the parameters in the model except the one of interest and then find the values for that parameter where the likelihood drops to a specific level. This level is determined by the desired confidence level (e.g., 95% CI).\nAdvantages\n\nApplicable to a greater range of models\nGenerally more accurate than Wald CIs especially for smaller datasets\n\nPreferred for:\n\nNonlinear models, complex models like mixed models, copulas, extreme value models, etc.\nWhen the distribution of the parameter estimate is skewed or not normal.",
    "crumbs": [
      "Confidence & Prediction Intervals"
    ]
  },
  {
    "objectID": "qmd/confidence-and-prediction-intervals.html#sec-cipi-terms",
    "href": "qmd/confidence-and-prediction-intervals.html#sec-cipi-terms",
    "title": "Confidence & Prediction Intervals",
    "section": "Terms",
    "text": "Terms\n\nConfidence Intervals: A range of values within which we are reasonably confident the true parameter (e.g mean) of a population lies, based on a sample statistic (e.g.¬†t-stat).\n\nFrequentist Interpretation: The confidence interval is constructed by a procedure, which, if you were to repeat the experiment and collecting samples many many times, in 95% of the experiments, the corresponding confidence intervals would cover the true value of the population mean. (link)\n\\[\n[100\\cdot(1-\\alpha)]\\;\\%\\: \\text{CI for}\\: \\hat\\beta_i = \\hat\\beta_i \\pm \\left[t_{(1-\\alpha/2)(n-k)} \\cdot \\text{SE}(\\hat\\beta_i)\\right]\n\\]\n\n\\(t\\) is the t-stat for\n\n\\(n-k\\) = sample size - number of predictors\n\\(1-\\alpha\\) for 2-sided; \\(1 - (\\alpha/2)\\) for 1 sided (I think)\n\n\\(\\text{SE}(\\beta_i)\\) is the sqrt of the corresponding value on the diagonal of the variance-covariance matrix for the coefficients.\n\nBayesian Interpretation: the true value is in that interval with 95% probability\n\nCoverage or Empirical Coverage: The level of coverage actually observed when evaluated on a dataset, typically a holdout dataset not used in training the model. Rarely will your model produce the Expected Coverage exactly\n\nAdaptive Coverage: Setting your Expected Coverage so that your Empirical Coverage = Target Coverage. A conformal prediction algorithm is adaptive if it not only achieves marginal coverage, but also (approximately) conditional coverage\n\nExample: 90% target coverage\n\nIf our model is slightly overfit, you might see that a 90% expected coverage leads to an 85% empirical coverage on a holdout dataset. To align your target and empirical coverage at 90%, may require setting expected coverage at something like 93%\n\n\nExpected Coverage: The level of confidence in the model for the prediction intervals.\nConditional Coverage: The coverage for each individual class of the outcome variable or subset of data specified by a grouping variable.\nMarginal Coverage: The overall average coverage across all classes of the outcome variable. All conformal methods achieve at or near the Expected Coverage averaged across classes but not necessarily for each individual class.\nTarget Coverage: The level of coverage you want to attain on a holdout dataset\n\ni.e.¬†The proportion of observations you want to fall within your prediction intervals\n\n\nJeffrey‚Äôs Interval: Bayesian CIs for Binomial proportions (i.e.¬†probability of an event)\n# probability of event\n# n_rain in the number of events (rainy days)\n# n is the number of trials (total days)\nmutate(pct_rain = n_rain / n,¬†\n¬† ¬† ¬† ¬†# jeffreys interval\n¬† ¬† ¬† ¬†# bayesian CI for binomial proportions\n¬† ¬† ¬† ¬†low = qbeta(.025, n_rain + .5, n - n_rain + .5),¬†\n¬† ¬† ¬† ¬†high = qbeta(.975, n_rain + .5, n - n_rain + .5))\nPrediction Interval: Used to estimate the range within which a future observation is likely to fall\n\nStandard Procedure for computing PIs for predictions (See link for examples and further details)\n\\[\n\\hat Y_0 \\pm t^{n-p}_{\\alpha/2} \\;\\hat\\sigma \\sqrt{1 + \\vec x_0'(X'X)^{-1}\\vec x_0}\n\\]\n\n\\(Y_0\\) is a single prediction\n\\(t\\) is the t-stat for\n\n\\(n-p\\) = sample size - number of predictors\n\\(1 - \\alpha\\) for 2-sided; \\(1 - (\\alpha/2)\\) for 1 sided (I think)\n\n\\(\\hat\\sigma\\) is the variance given by residual standard error, summary(Model1)$sigma\n\\[\nS^2 = \\frac{1}{n-p}\\;||\\;Y-X\\hat \\beta\\;||^2\n\\]\n\n\\(S = \\hat \\sigma\\)\nI think this is also the \\(\\operatorname{MSE}/\\operatorname{dof}\\) that you sometimes see in other formulas\n\n\\(x_0\\) is new data for the predictor variable values for the prediction (also would need to include a 1 for the intercept)\n\\((X'X)^{-1}\\) is the variance covariance matrix, vcov(model)",
    "crumbs": [
      "Confidence & Prediction Intervals"
    ]
  },
  {
    "objectID": "qmd/confidence-and-prediction-intervals.html#sec-cipi-diag",
    "href": "qmd/confidence-and-prediction-intervals.html#sec-cipi-diag",
    "title": "Confidence & Prediction Intervals",
    "section": "Diagnostics",
    "text": "Diagnostics\n\nMean Interval Score (MIS)\n\n(Proper) Score of both coverage and interval width\n\nI don‚Äôt think there‚Äôs a closed range, so it‚Äôs meant for model comparison\nLower is better\n\ngreybox::MIS and (scaled) greybox::sMIS\n\nOnline docs don‚Äôt have these functions, but docs in RStudio do\n\nAlso scoringutils::interval_score\n\nDocs have formula\n\nThe actual paper is dense Need to take the mean of MIS\n\n\n\nCoverage\n\nExample: Coverage %\ncoverage &lt;- function(df, ...){\n¬† df %&gt;%\n¬† ¬† mutate(covered = ifelse(Sale_Price &gt;= .pred_lower & Sale_Price pred_upper, 1, 0)) %&gt;%¬†\n¬† ¬† group_by(...) %&gt;%¬†\n¬† ¬† summarise(n = n(),\n¬† ¬† ¬† ¬† ¬† ¬† ¬† n_covered = sum(\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† covered\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ),\n¬† ¬† ¬† ¬† ¬† ¬† ¬† stderror = sd(covered) / sqrt(n),\n¬† ¬† ¬† ¬† ¬† ¬† ¬† coverage_prop = n_covered / n)\n}\nrf_preds_test %&gt;%¬†\n¬† coverage() %&gt;%¬†\n¬† mutate(across(c(coverage_prop, stderror), ~.x * 100)) %&gt;%¬†\n¬† gt::gt() %&gt;%¬†\n¬† gt::fmt_number(\"stderror\", decimals = 2) %&gt;%¬†\n¬† gt::fmt_number(\"coverage_prop\", decimals = 1)\n\nFrom Quantile Regression Forests for Prediction Intervals\nSale_Price is the outcome variable\nrf_preds_test is the resulting object from predict with a tidymodels model as input\n\nExample: Test consistency of coverage across quintiles\npreds_intervals %&gt;%¬† # preds w/ PIs\n¬† mutate(price_grouped = ggplot2::cut_number(.pred, 5)) %&gt;%¬† # quintiles\n¬† mutate(covered = ifelse(Sale_Price &gt;= .pred_lower & Sale_Price &lt;= .pred_upper, 1, 0)) %&gt;%¬†\n  with(chisq.test(price_grouped, covered))\n\np value &lt; 0.05 says coverage significantly differs by quintile\nSale_Price is the outcome variable\n\n\nInterval Width\n\nNarrower bands should mean a more precise model\nExample: Average interval width across quintiles\nlm_interval_widths &lt;- preds_intervals %&gt;%¬†\n¬† mutate(interval_width = .pred_upper - .pred_lower,\n¬† ¬† ¬† ¬† interval_pred_ratio = interval_width / .pred) %&gt;%¬†\n¬† mutate(price_grouped = ggplot2::cut_number(.pred, 5)) %&gt;% # quintiles\n¬† group_by(price_grouped) %&gt;%¬†\n¬† summarize(n = n(),\n¬† ¬† ¬† ¬† ¬† ¬† mean_interval_width_percentage = mean(interval_pred_ratio),\n¬† ¬† ¬† ¬† ¬† ¬† stdev = sd(interval_pred_ratio),\n¬† ¬† ¬† ¬† ¬† ¬† stderror = stdev / sqrt(n)) %&gt;%¬†\n¬† mutate(x_tmp = str_sub(price_grouped, 2, -2)) %&gt;%¬†\n¬† separate(x_tmp, c(\"min\", \"max\"), sep = \",\") %&gt;%¬†\n¬† mutate(across(c(min, max), as.double)) %&gt;%¬†\n¬† select(-price_grouped)\n\nlm_interval_widths %&gt;%¬†\n¬† mutate(across(c(mean_interval_width_percentage, stdev, stderror), ~.x*100)) %&gt;%¬†\n¬† gt::gt() %&gt;%¬†\n¬† gt::fmt_number(c(\"stdev\", \"stderror\"), decimals = 2) %&gt;%¬†\n¬† gt::fmt_number(\"mean_interval_width_percentage\", decimals = 1)\n\nInterval width has actually been transformed into a percentage as related to the prediction (removes the scale of the outcome variable)",
    "crumbs": [
      "Confidence & Prediction Intervals"
    ]
  },
  {
    "objectID": "qmd/confidence-and-prediction-intervals.html#sec-cipi-boot",
    "href": "qmd/confidence-and-prediction-intervals.html#sec-cipi-boot",
    "title": "Confidence & Prediction Intervals",
    "section": "Bootstrapping",
    "text": "Bootstrapping\n\nMisc\n\nDo NOT bootstrap the standard deviation\n\narticle\nbootstrap is ‚Äúbased on a weak convergence of moments‚Äù\nif you use an estimate based standard deviation of the bootstrap, you are being overly conservative (i.e.¬†overestimate the sd)\n\nbootstrapping uses the original, initial sample as the population from which to resample, whereas Monte Carlo simulation is based on setting up a data generation process (with known values of the parameters of a known distribution). Where Monte Carlo is used to test drive estimators, bootstrap methods can be used to estimate the variability of a statistic and the shape of its sampling distribution\nPackages\n\n{ebtools::get_boot_ci}\n\n\nSteps\n\nResample with replacement\nCalculate statistic of resample\nStore statistic\nRepeat 10K or so times\nCalculate mean, sd, and quantiles for CIs across all collected statistics\n\nCIs\n\nPlenty of articles for means and models, see bkmks\nrsample::reg_intervals is a convenience function for lm, glm, survival models\n\nPIs\n\nBootstrapping PIs is a bit complicated\n\nSee Shalloway‚Äôs article (code included)\nonly use out-of-sample estimates to produce the interval\nestimate the uncertainty of the sample using the residuals from a separate set of models built with cross-validation",
    "crumbs": [
      "Confidence & Prediction Intervals"
    ]
  },
  {
    "objectID": "qmd/confidence-and-prediction-intervals.html#sec-cipi-conf",
    "href": "qmd/confidence-and-prediction-intervals.html#sec-cipi-conf",
    "title": "Confidence & Prediction Intervals",
    "section": "Conformal Prediction Intervals",
    "text": "Conformal Prediction Intervals\n\nMisc\n\nPackages\n\n{{mapie}} - Handles scikit-learn, tf, pytorch, etc. with wrappers. Computes conformal PIs for Regression, Classification, and Time Series models.\n\nRegression\n\nMethods: naive, split, jackknife, jackknife+, jackknife-minmax, jackknife-after-bootstrap, CV, CV+, CV-minmax, ensemble batch prediction intervals (EnbPI).\n‚ÄúSince the typical coverage levels estimated by jackknife+ follow very closely the target coverage levels, this method should be used when accurate and robust prediction intervals are required.‚Äù\n‚ÄúFor practical applications where N is large and/or the computational time of each leave-one-out simulation is high, it is advised to adopt the CV+ method‚Äù even though the interval width will be slightly larger than jackknife+\n‚ÄúThe jackknife-minmax and CV-minmax methods are more conservative since they result in higher theoretical and practical coverages due to the larger widths of the prediction intervals. It is therefore advised to use them when conservative estimates are needed.‚Äù\n‚ÄúThe conformalized quantile regression method allows for more adaptiveness on the prediction intervals which becomes key when faced with heteroscedastic data.‚Äù\nEnbPI is for time series and residuals must be updated each time new observations are available\n\nClassification\n\nMethods: LAC, Top-K, Adaptive Prediction Sets (APS), Regularized Adaptive Prediction Sets (RAPS), Split and Cross-Conformal methods.\nThe difference between these methods is the way the conformity scores are computed\nLAC method is not adaptive: the coverage guarantee only holds on average (i.e.¬†marginal coverage). Difficult classification cases may have prediction sets that are too small, and easy cases may have sets that are too large. (See below for details on process). Doesn‚Äôt seem like to great a task to manually make it adaptive though (See example below).\nAPS‚Äô conformity score used to determine the threshold is a constrained sum of the predicted probabilities for that observation. Only the predicted probabilites \\(\\ge\\) the predicted probability of the true class are included in the sum. Everything else is the same as the LAC algorithm, although the default behavior is to keep the last class that crosses the threshold through the argument, include_last_label = [True, ‚Äúrandomized‚Äù, False]. The value of the argument can determine whether conditional coverage is (approximately) attained with True being the most liberal setting. Note that only ‚Äúrandomized‚Äù can produce empty predicted class sets. Algorithm tends to produce large predicted class sets when there are many classes in the outcome variable.\nRAPS attenuates the lengthier predicted class sets in APS through regularization. A penalty, \\(\\lambda\\), is added to predicted probabilities with ranks greater that some value, \\(k\\). Everything else is the same as APS.\nNot sure what Split is, but Cross-Conformal is CV applied to LAC and APS.\n\n\n\nNotes from\n\nHow to Handle Uncertainty in Forecasts: A deep dive into conformal prediction\n\nThe conformity score formula used in this article, \\(s_i = |\\;y_i - \\hat p_i(y_i\\;|\\;X_i)\\;|\\) where \\(y_i\\) is the observed class and \\(\\hat p\\) is the predicted probability, has the same results as to the one below, but it‚Äôs not workable in production since there is no observed class.\n\nConformal Prediction for Machine Learning Classification ‚Äî From the Ground Up\n‚ÄúMAPIE‚Äù Explained Exactly How You Wished Someone Explained to You\n\nResources\n\nIntroduction To Conformal Prediction With Python A Short Guide for Quantifying Uncertainty of Machine Learning Models\n\nSee R &gt;&gt; Documents &gt;&gt; Machine Learning\n\n\nNormal PIs require iid data while conformal PIs only require the ‚Äúidentically distributed‚Äù part (not independent) and therefore should provide more robust coverage.\nContinuous outcome (link) using quantile regression\n\n\n\nClassification\n\nLAC (aka Score Method) Process\n\nSplit data into Train, Calibration (aka Validation), and Test\nTrain the model on the training set\nOn the calibration (aka validation) set, compute the conformity scores only for the observed class (i.e.¬†true label) for each observation\n\\[\ns_{i, j}  = 1 - \\hat p_{i,j}(y_i | X_i)\n\\]\n\nVariables\n\n\\(s_{i,j}\\): Conformity Score for the ith observation and class \\(j\\)\n\\(y_i\\): Observed Class\n\\(\\hat p_{i,j}\\): Predicted probability by the model for class \\(j\\)\n\\(X_i\\): Predictors\n\\(i\\): Index of the observed data\n\\(j\\): Class of the outcome variable\n\nRange: [0, 1]\nIn general, Low = good, High = bad\nIn R, the predicted probabilities for statistical models are always for the event (i.e.¬†\\(y_i = 1\\)) in a binary outcome context, so when the observed class = 0, the score will be \\(s_{i,0} = 1-(1- \\hat p_{i, 1}(y_i | X_i)) = \\hat p_{i, 1}(y_i | X_i)\\) which is just the predicted probability.\n\nOrder the conformity scores from highest to lowest\nAdjust the chosen the \\(\\alpha\\) using a finite sample correction, \\(q_{\\text{level}} = 1- \\frac{ceil((n_{\\text{cal}}+1)\\alpha)}{n_{\\text{cal}}}\\) and calculate the quantile.\nCalculate the critical value or threshold for the quantile\n\n\nx-axis corresponds to an ordered set of conformity scores\nIf \\(\\alpha = 0.05\\), find the score value at the the 95th percentile (e.g.¬†quantile(scores, 0.95))\nBlue: conformity scores are not statistically significant. They‚Äôre within our prediction interval.\nRed: Very large conformity scores indicate high divergence from the true label. These conformal scores are statistically significant and thereby outside of our prediction interval.\n\nPredict on the Test set and calculate conformity scores for each class\nFor each test set observation, select classes that have scores below the threshold score as the model prediction.\n\nAn observation could potentially have both classes or no classes selected. ( Not sure if this is true in a binary outcome situation)\n\n\nExample: LAC Method, Multinomial\n\nModel\nclassifier = LogisticRegression(random_state=42)\nclassifier.fit(X_train, y_train)\nScores calculated using only the predicted probability for the true class on the Validation set (aka Calibration set)\n# Get predicted probabilities for calibration set\ny_pred = classifier.predict(X_Cal)\ny_pred_proba = classifier.predict_proba(X_Cal)\nsi_scores = []\n# Loop through all calibration instances\nfor i, true_class in enumerate(y_cal):\n    # Get predicted probability for observed/true class\n    predicted_prob = y_pred_proba[i][true_class]\n    si_scores.append(1 - predicted_prob) \nThe threshold determines what¬†coverage our predicted labels will have\nnumber_of_samples = len(X_Cal)\nalpha = 0.05\nqlevel = (1 - alpha) * ((number_of_samples + 1) / number_of_samples)\nthreshold = np.percentile(si_scores, qlevel*100)\nprint(f'Threshold: {threshold:0.3f}')\n#&gt; Threshold: 0.598\n\nFinite sample correction for the 95th quantile: multiply 0.95 by¬†(n+1)/n\n\nThreshold is then used to get predicted labels of the test set\n# Get standard predictions for comparison\ny_pred = classifier.predict(X_test)\n# Calc scores, then only take scores in the 95% conformal PI\nprediction_sets = (1 - classifier.predict_proba(X_test) &lt;= threshold)\n\n# Get labels for predictions in conformal PI\ndef get_prediction_set_labels(prediction_set, class_labels):\n    # Get set of class labels for each instance in prediction sets\n    prediction_set_labels = [\n        set([class_labels[i] for i, x in enumerate(prediction_set) if x]) for prediction_set in \n        prediction_sets]\n    return prediction_set_labels\n\n# Compare conformal prediction with observed and traditional preds\nresults_sets = pd.DataFrame()\nresults_sets['observed'] = [class_labels[i] for i in y_test]\nresults_sets['conformal'] = get_prediction_set_labels(prediction_sets, class_labels)\nresults_sets['traditional'] = [class_labels[i] for i in y_pred]\nresults_sets.head(10)\n#&gt;    observed  conformal        traditional\n#&gt; 0  blue      {blue}           blue\n#&gt; 1  green     {green}          green\n#&gt; 2  blue      {blue}           blue\n#&gt; 3  green     {green}          green\n#&gt; 4  orange    {orange}         orange\n#&gt; 5  orange    {orange}         orange\n#&gt; 6  orange    {orange}         orange\n#&gt; 7  orange    {blue, orange}   blue\n#&gt; 8  orange    {orange}         orange\n#&gt; 9  orange    {orange}         orange\n\nconformity scores are calculated for each potential class using the predicted probabilities on the test set\nThe predicted class for an observation is determined by whether a class has a score below the threshold.\nTherefore, an observation may have 1 or more predicted classes or 0 predicted classes.\n\nStatistics (See Statistics section for functions)\n\nOverall\nweighted_coverage = get_weighted_coverage(\n    results['Coverage'], results['Class counts'])\n\nweighted_set_size = get_weighted_set_size(\n    results['Average set size'], results['Class counts'])\n\nprint (f'Overall coverage: {weighted_coverage}')\nprint (f'Average set size: {weighted_set_size}')\n#&gt; Overall coverage: 0.947\n#&gt; Average set size: 1.035\n\nOverall coverage is very close to the target coverage of 95%, therefore, marginal coverage is achieved which is expected for this method\n\nPer Class\nresults = pd.DataFrame(index=class_labels)\nresults['Class counts'] = get_class_counts(y_test)\nresults['Coverage'] = get_coverage_by_class(prediction_sets, y_test)\nresults['Average set size'] = get_average_set_size(prediction_sets, y_test)\nresults\n#&gt;         Class counts  Coverage   Average set size\n#&gt; blue    241           0.817427   1.087137\n#&gt; orange  848           0.954009   1.037736\n#&gt; green   828           0.977053   1.016908\n\nOverall coverage (i.e.¬†for all labels) will be at or very near 95% but coverage for individual classes may vary.\n\nAn illustration of how this method lacks Conditional Coverage\nSolution: Get thresholds for each class. (See next example)\n\nNote that the blue class had substantially fewer observations that the other 2 classes.\n\n\n\nExample: LAC-adapted - Threshold per Class\n\nDon‚Äôt think {{mapie}} has this option.\nAlso possible do this for subgroups of data, such as ensuring equal coverage for a diagnostic across racial groups, if we found coverage using a shared threshold led to problems.\nCalculate individual class thresholds\n# Set alpha (1 - coverage)\nalpha = 0.05\nthresholds = []\n# Get predicted probabilities for calibration set\ny_cal_prob = classifier.predict_proba(X_Cal)\n# Get 95th percentile score for each class's s-scores\nfor class_label in range(n_classes):\n    mask = y_cal == class_label\n    y_cal_prob_class = y_cal_prob[mask][:, class_label]\n    s_scores = 1 - y_cal_prob_class\n    q = (1 - alpha) * 100\n    class_size = mask.sum()\n    correction = (class_size + 1) / class_size\n    q *= correction\n    threshold = np.percentile(s_scores, q)\n    thresholds.append(threshold)\nApply individual class thresholds to test set scores\n# Get Si scores for test set\npredicted_proba = classifier.predict_proba(X_test)\nsi_scores = 1 - predicted_proba\n\n# For each class, check whether each instance is below the threshold\nprediction_sets = []\nfor i in range(n_classes):\n    prediction_sets.append(si_scores[:, i] &lt;= thresholds[i])\nprediction_sets = np.array(prediction_sets).T\n\n# Get prediction set labels and show first 10\nprediction_set_labels = get_prediction_set_labels(prediction_sets, class_labels)\nStatistics\n\nOverall\nweighted_coverage = get_weighted_coverage(\n    results['Coverage'], results['Class counts'])\n\nweighted_set_size = get_weighted_set_size(\n    results['Average set size'], results['Class counts'])\n\nprint (f'Overall coverage: {weighted_coverage}')\nprint (f'Average set size: {weighted_set_size}')\n#&gt; Overall coverage: 0.95\n#&gt; Average set size: 1.093\n\nSimilar to previous example\n\nPer Class\nresults = pd.DataFrame(index=class_labels)\nresults['Class counts'] = get_class_counts(y_test)\nresults['Coverage'] = get_coverage_by_class(prediction_sets, y_test)\nresults['Average set size'] = get_average_set_size(prediction_sets, y_test)\nresults\n#&gt;         Class counts  Coverage   Average set size\n#&gt; blue    241           0.954357   1.228216\n#&gt; orange  848           0.956368   1.139151\n#&gt; green   828           0.942029   1.006039\n\nCoverages now very close to 95% and the average set sizes have increased, especially for Blue.\n\n\n\n\n\n\nContinuous\n\nConformalized Quantile Regression Process\n\nSplit data into Training, Calibration, and Test sets\n\nTraining data: data on which the quantile regression model learns.\nCalibration data: data on which CQR calibrates the intervals.\n\nIn the example, he split the data into 3 equal sets\n\nTest data: data on which we evaluate the goodness of intervals.\n\nFit quantile regression model on training data.\nUse the model obtained at previous step to predict intervals on calibration data.\n\nPIs are predictions at the quantiles:\n\n(alpha/2)*100) (e.g 0.025, alpha = 0. 05)\n(1-(alpha/2))*100) (e.g.¬†0.975)\n\n\nCompute conformity scores on calibration data and intervals obtained at the previous step.\n\nResiduals are calculated for the PI vectors\nScores are calculated by taking the row-wise maximum of both (upper/lower quantile) residual vectors (e.g s_i &lt;- pmax(lower_pi_res, upper_pi_res))\n\nGet 1-alpha quantile from the distribution of conformity scores (e.g threshold &lt;- quantile(s_i, 0.95)\n\nThis score value will be the threshold\n\nUse the model obtained at step 1 to make predictions on test data.\n\nCompute PI vectors (i.e.¬†predictions at the previously stated quantiles) on Test set\ni.e.¬†Same calculation as with the calibration data in step 2 where you use the model to predict at upper and lower PI quantiles.\n\nCompute lower/upper end of the interval by subtracting/adding the threshold from/to the quantile predictions (aka PIs)\n\nLower conformity interval: lower_pi &lt;- test_lower_pred  - threshold\nUpper conformity interval: upper_pi &lt;- test_upper_pred + threshold\n\n\nExample: Quantile Random Forest\nimport numpy as np\nfrom skgarden import RandomForestQuantileRegressor\n\nalpha = .05\n\n# 1. Fit quantile regression model on training data\nmodel = RandomForestQuantileRegressor().fit(X_train, y_train)\n\n# 2. Make prediction on calibration data\ny_cal_interval_pred = np.column_stack([\n¬† ¬† model.predict(X_cal, quantile=(alpha/2)*100),¬†\n¬† ¬† model.predict(X_cal, quantile=(1-alpha/2)*100)])\n\n# 3. Compute conformity scores on calibration data\ny_cal_conformity_scores = np.maximum(\n¬† ¬† y_cal_interval_pred[:,0] - y_cal,¬†\n¬† ¬† y_cal - y_cal_interval_pred[:,1])\n\n# 4. Threshold: Get 1-alpha quantile from the distribution of conformity scores\n#¬† ¬† Note: this is a single number\nquantile_conformity_scores = np.quantile(\n¬† ¬† y_cal_conformity_scores, 1-alpha)\n\n# 5. Make prediction on test data\ny_test_interval_pred = np.column_stack([\n¬† ¬† model.predict(X_test, quantile=(alpha/2)*100),¬†\n¬† ¬† model.predict(X_test, quantile=(1-alpha/2)*100)])\n\n# 6. Compute left (right) end of the interval by\n#¬† ¬† subtracting (adding) the quantile to the predictions\ny_test_interval_pred_cqr = np.column_stack([\n¬† ¬† y_test_interval_pred[:,0] - quantile_conformity_scores,\n¬† ¬† y_test_interval_pred[:,1] + quantile_conformity_scores])\n\n\n\nStatistics\n\nAverage Set Size\n\nThe average number of predicted classes per observation since there can be more than 1 predicted class in the conformal PI\nExample:\n# average set size for each class\ndef get_average_set_size(prediction_sets, y_test):\n    average_set_size = []\n    for i in range(n_classes):\n        average_set_size.append(\n            np.mean(np.sum(prediction_sets[y_test == i], axis=1)))\n    return average_set_size   \n\n# Overall average set size (weighted by class size)\n# Get class counts\ndef get_class_counts(y_test):\n    class_counts = []\n    for i in range(n_classes):\n        class_counts.append(np.sum(y_test == i))\n    return class_counts\n\ndef get_weighted_set_size(set_size, class_counts):\n    total_counts = np.sum(class_counts)\n    weighted_set_size = np.sum((set_size * class_counts) / total_counts)\n    weighted_set_size = round(weighted_set_size, 3)\n    return weighted_set_size\n\nCoverage\n\nClassification: Percentage of correct classifications\nExample: Classification\n# coverage for each class\ndef get_coverage_by_class(prediction_sets, y_test):\n    coverage = []\n    for i in range(n_classes):\n        coverage.append(np.mean(prediction_sets[y_test == i, i]))\n    return coverage\n\n# overall coverage (weighted by class size)\n# Get class counts\ndef get_class_counts(y_test):\n    class_counts = []\n    for i in range(n_classes):\n        class_counts.append(np.sum(y_test == i))\n    return class_counts\n\ndef get_weighted_coverage(coverage, class_counts):\n    total_counts = np.sum(class_counts)\n    weighted_coverage = np.sum((coverage * class_counts) / total_counts)\n    weighted_coverage = round(weighted_coverage, 3)\n    return weighted_coverage\n\n\n\n\nVisualization\n\nConfusion Matrix\n\n\nBinary target where labels are 0 and 1\nInterpretation\n\nTop-left: predictions where both labels are not statistically significant (i.e.¬†inside the ‚Äúprediction interval‚Äù).\n\nThe model predicts both classes well since both labels have low scores.\nDepending the threshold, maybe the model could be relatively agnostic (e.g.¬†predicted probabilites like 0.50-0.50, 0.60-0.40)\n\nBottom-right: predictions where both labels are statistically significant¬† (i.e.¬†outside the ‚Äúprediction interval‚Äù).\n\nModel totally whiffs. Confident it‚Äôs one label when it‚Äôs actually another.\n\nExample\n\n1 (truth) - low predicted probability = high score -&gt; Red and significant\n0 - high predicted probability = high score -&gt; Red and significant\n\n\n\nTop-right: predictions where all 0 labels are not statistically significant.\n\nModel predicted the 0=class well (i.e.¬†low scores) but the 1-class poorly (i.e.¬†high scores)\n\nBottom-left: predictions where all 1 labels are not statistically significant. Here, the model predicted that 1 is the true class.\n\nVice versa of top-right",
    "crumbs": [
      "Confidence & Prediction Intervals"
    ]
  },
  {
    "objectID": "qmd/big-data.html",
    "href": "qmd/big-data.html",
    "title": "Big Data",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Big Data"
    ]
  },
  {
    "objectID": "qmd/big-data.html#sec-bgdat-misc",
    "href": "qmd/big-data.html#sec-bgdat-misc",
    "title": "Big Data",
    "section": "",
    "text": "RcppArmadillo::fastLmPure Not sure what this does but it‚Äôs rcpp so maybe faster than lm for big data.\n.lm.fit is a base R lm function that is 30%-40% faster than lm.",
    "crumbs": [
      "Big Data"
    ]
  },
  {
    "objectID": "qmd/big-data.html#sec-bgdat-hghperf",
    "href": "qmd/big-data.html#sec-bgdat-hghperf",
    "title": "Big Data",
    "section": "High Performance",
    "text": "High Performance\n\n{rpolars}: arrow product; uses SIMD which is a low-level vectorization that can be used to speed up simple operations like addition, subtraction, division, and multiplication\n\nResources\n\nCookbook Polars for R\n\nExample: Read and Summarize\ndf &lt;- pl$scan_csv(file_name)$\n    group_by(\"state\")$\n    agg(\n        pl$\n          col(\"measurement\")$\n          min()$\n          alias(\"min_m\"),\n        pl$\n          col(\"measurement\")$\n          max()$\n          alias(\"max_m\"),\n        pl$\n          col(\"measurement\")$\n          mean()$\n          alias(\"mean_m\")\n    )$\n    collect()\n\nFastest at this operation according to this benchmark\n\nExample: groupby state + min, max, mean\n# polars sql\nlf &lt;- polars::pl$LazyFrame(D) \npolars::pl$SQLContext(frame = lf)$execute(\n  \"select min(measurement) as min_m, \n          max(measurement) as max_m, \n          avg(measurement) as mean_m \n  from frame \n  group by state\")$collect()\n\n# polars\npolars::pl$\n  DataFrame(D)$\n  group_by(\"state\")$\n  agg(polars::pl$\n        col(\"measurement\")$\n        min()$alias(\"min_m\"),\n      polars::pl$\n        col(\"measurement\")$\n        max()$alias(\"max_m\"),\n      polars::pl$\n        col(\"measurement\")$\n        mean()$alias(\"mean_m\"))\n\n{collapse}: Fast grouped & weighted statistical computations, time series and panel data transformations, list-processing, data manipulation functions, summary statistics and various utilities such as support for variable labels. Class-agnostic framework designed to work with vectors, matrices, data frames, lists and related classes i.e.¬†xts, data.table, tibble, pdata.frame, sf.\n\noptions(collapse_mask = \"all\")\nlibrary(collapse)\n\nCode chunk above can optimize any script. No other changes necessary. Quick demo.\nvs arrow/polars (benchmark)\n\nDepends on the data/groups ratio\n\nIf you have ‚Äúmany groups and little data in each group‚Äù then use collapse\n\nIf your calculations involve ‚Äúmore complex statistics algorithms like the median (involving selection) or mode or distinct value count (involving hashing)(cannot, to my knowledge, benefit from SIMD)‚Äù then use collapse.\n\nset_collapse(mask = \"manip\"|\"all\") to remove f- prefixes\nExample: groupby state + min, max, mean\nD |&gt;\n  fgroup_by(state) |&gt; \n  fsummarise(min = fmin(measurement), \n             max = fmax(measurement), \n             mean = fmean(measurement)) |&gt;\n  fungroup()\n\n{r2c}: Fast grouped statistical computation; currently limited to a few functions, sometimes faster than {collapse}\n{data.table}: Enhanced data frame class with concise data manipulation framework offering powerful aggregation, extremely flexible split-apply-combine computing, reshaping, joins, rolling statistics, set operations on tables, fast csv read/write, and various utilities such as transposition of data.\n\nExample: groupby state + min, max, mean\nD[ ,.(mean = mean(measurement),\n      min = min(measurement),\n      max = max(measurement)),\n   by=state]\n\n# Supposedly faster\nrbindlist(lapply(unique(D$state), \n                 \\(x) data.table(state = x, \n                                 y[state == x, \n                                   .(mean(measurement), \n                                     min(measurement), \n                                     max(measurement))\n                                   ]\n                                 )))\n\n{rfast}: A collection of fast (utility) functions for data analysis. Column- and row- wise means, medians, variances, minimums, maximums, many t, F and G-square tests, many regressions (normal, logistic, Poisson), are some of the many fast functions\n\nThe vast majority of the functions accept matrices only, not data.frames.\nDo not have matrices or vectors with have missing data (i.e NAs). There are no checks and C++ internally transforms them into zeros (0), so you may get wrong results.\nExample: groupby state + min, max, mean\nlev_int &lt;- as.numeric(D$state)\nminmax &lt;- Rfast::group(D$measurement, lev_int, method = \"min.max\")\ndata.frame(\n    state = levels(D$state),\n    mean = Rfast::group(D$measurement, lev_int, method = \"mean\"),\n    min = minmax[1, ],\n    max = minmax[2, ]\n)\n\n{matrixStats}: Efficient row-and column-wise (weighted) statistics on matrices and vectors, including computations on subsets of rows and columns.\n{kit}: Fast vectorized and nested switches, some parallel (row-wise) statistics, and some utilities such as efficient partial sorting and unique values.\n{fst}: A compressed data file format that is very fast to read and write. Full random access in both rows and columns allows reading subsets from a ‚Äò.fst‚Äô file.",
    "crumbs": [
      "Big Data"
    ]
  },
  {
    "objectID": "qmd/big-data.html#sec-bgdat-lgmem",
    "href": "qmd/big-data.html#sec-bgdat-lgmem",
    "title": "Big Data",
    "section": "Larger than Memory",
    "text": "Larger than Memory\n\nOnly work with a sample of the data\n\nRandom sample in CLI\n\nSee binder for code\nAlso this snippet from Healy for a zipped csv.\n\n\nImproved version\ngzip -cd giantfile.csv.gz | (read HEADER; echo $HEADER; perl -ne 'print if (rand() &lt; 0.001)‚Äô) &gt; sample.csv\n\nRemoves the need to decompress the file twice, adds the header row, and removes the risk of a double header row\n\n\n\nOnly read the first n lines\n\nset n_max arg in readr::read_*\n\n\ndatasette.io - App for exploring and publishing data. It helps people take data of any shape, analyze and explore it, and publish it as an interactive website and accompanying API.\n\nWell documented, many plugins\n\nRill - A tool for effortlessly transforming data sets into powerful, opinionated dashboards using SQL.\n\nDocs, Example Projects\nPowered by Sveltekit & DuckDB = conversation-fast, not wait-ten-seconds-for-result-set fast\nWorks with your local and remote datasets ‚Äì imports and exports Parquet and CSV (s3, gcs, https, local)\nNo more data analysis ‚Äúside-quests‚Äù ‚Äì helps you build intuition about your dataset through automatic profiling\nNo ‚Äúrun query‚Äù button required ‚Äì responds to each keystroke by re-profiling the resulting dataset\nRadically simple interactive dashboards ‚Äì thoughtful, opinionated, interactive dashboard defaults to help you quickly derive insights from your data\nDashboards as code ‚Äì each step from data to dashboard has versioning, Git sharing, and easy project rehydration\n\nOnline duckdb shell for parquet files (gist, https://shell.duckdb.org/)\nselect max(wind)¬†\nfrom 'https://raw.githubusercontent.com/RobinL/iris_parquet/main/gridwatch/gridwatch_2023-01-08.parquet';\n-- Takes 6 seconds on the first query, 200ms on subsequent similar queries\n\nselect *¬†\nfrom 'https://raw.githubusercontent.com/RobinL/iris_parquet/main/NSPL/NSPL.parquet'¬†\nwhere pcd = 'SW1A1AA';\n-- Takes 13 seconds on the first query, 100ms on subsequent similar queries\nCSV Editors\n\nFor editing or reformatting cells\nPopular spreadsheet programs like googlesheets (100MB) and excel (25MB online) have file size limits and they‚Äôre slow to upload to. The following programs are free(-ish) local alternatives only limited by your RAM.\nSuggest for files over a few hundred MBs that you open as Read-Only\n\nOpening the files as ‚ÄúEditable‚Äù will probably balloon the memory cost to at least 5 times the file size. (e.g.¬†350MB csv \\(\\rightarrow\\) 2GB RAM)\n\nModern CSV - Nice modern interface, read-only mode that can open large csvs (100s of MBs) without making much of a dent in your RAM, fully featured (moreso if you pay a small-ish one time fee)\n\nDocs, Feature free/upgrade list\nStill has some functionality in read-only mode (e.g.¬†search, sort)\n\nOpenRefine - Has read-only, Several add-ons, Completely open source.\n\nDocs, List of Extensions\nNo functionality when read-only (must create a project to do anything) ‚Äî just reading\nStarts with a 1024 MB RAM usage limit which is proably fine for editing around a 100MB csv. Need to set the limit higher in a config file in order to edit larger files.\nOnce you create a project, I think it has some editing features that you‚Äôd have to pay for with Modern CV.\nOpens other file formats besides csv (e.g.¬†xlsx, xml, json, etc)\n\n\nxsv - A command line program for indexing, slicing, analyzing, splitting, and joining CSV files. Written in Rust.\n\nIsn‚Äôt well maintained. But it is written in Rust, so may be able handle larger files that would make csvkit to slow to use.\n\ncsvkit - Suite of command-line tools for converting to and working with CSV. Written in Python.\n\nInstallation docs\n\nOne of the articles your terminal has to be a bash terminal but I dunno\n\nIf so, they recommend cmder or enabling the Linux subsystem with WSL2.\n\n\nNotes from\n\nArticle with additional examples and options\n\nFeatures\n\nPrint CSV files out nicely formatted\nCut out specific columns\nGet statistical information about columns\n\nConvert excel files to CSV files:\nin2csv excel_file.xlsx &gt; new_file.csv\n# +remove .xlsx file\nin2csv excel_file.xlsx &gt; new_file.csv && rm excel_file\nSearch within columns with regular expressions:\ncsvgrep -c county -m \"HOLT\" new_file.csv\n# subset of columns (might be faster) with pretty formatting\ncsvcut -c county,total_cost new_file.csv | csvgrep -c county -m \"HOLT\" | csvlook\n\nSearches for ‚ÄúHOLT‚Äù in the ‚Äúcounty‚Äù column\n\nQuery with SQL\n\nsyntax csvsql --query \"ENTER YOUR SQL QUERY HERE\" FILE_NAME.csv\nExample\n\n\nView top lines: head new_file.csv\nView columns names: csvcut -n new_file.csv\nSelect specific columns: csvcut -c county,total_cost,ship_date new_file.csv\n\nWith pretty output: csvcut -c county,total_cost,ship_date new_file.csv | csvlook\nCan also use column indexes instead of names\n\nJoin 2 files: csvjoin -c cf data1.csv data2.csv &gt; joined.csv\n\n‚Äúcf‚Äù is the common column between the 2 files\n\nEDA-type stats:\ncsvstat new_file.csv\n# subset of columns\ncsvcut -c total_cost,ship_date new_file.csv | csvstat\n\nJSONata - a lightweight, open-source query and transformation language for JSON data, inspired by the ‚Äòlocation path‚Äô semantics of XPath 3.1.\n\nMisc\n\nNotes from: Hrbrmstr‚Äôs article\nJSONata also doesn‚Äôt throw errors for non-existing data in the input document. If during the navigation of the location path, a field is not found, then the expression returns nothing.\n\nThis can be beneficial in certain scenarios where the structure of the input JSON can vary and doesn‚Äôt always contain the same fields.\n\nTreats single values and arrays containing a single value as equivalent\nBoth JSONata and¬†jq¬†can work in the browser (JSONata embedding code, demo), but¬†jq¬†has a slight speed edge thanks to WASM. However, said edge comes at the cost of a slow-first-start\n\nFeatures\n\nDeclarative syntax that is pretty easy to read and write, which allows us to focus on the desired output rather than the procedural steps required to achieve it\nBuilt-in operators and functions for manipulating and combining data, making it easier to perform complex transformations without writing custom code in a traditional programming language like python or javascript\nUser-defined functions that let us extend JSONata‚Äôs capabilities and tailor it to our specific needs\nFlexible output structure that lets us format query results into pretty much any output type\n\n\njq + jsonlite - json files\njsoncrack.com - online editor/tool to visualize nested json (or regular json)\njj - cli tool for nested json. Full support for ndjson as well as setting/updating/deleting values. Plus it lets you perform similar pretty/ugly printing that jq does.\nsqlite3 - CLI utility allows the user to manually enter and execute SQL statements against an SQLite database or against a ZIP archive.\n\nalso directly against csv files (post)\n\ntextql - Execute SQL against structured text like CSV or TSV\n\nRequire Go language installed\nOnly for Macs or running a docker image\n\ncolumnq-cli - sql query json, csv, parquet, arrow, and more\nfread + CLI tools\n\nFor large csvs and fixing large csv with jacked-up formating see article, RBlogger version\n\n{arrow}\n\nconvert file into parquet files\n\npass the file path to open_dataset, use group_by to partition the Dataset into manageable chunks\nuse write_datasetto write each chunk to a separate Parquet file‚Äîall without needing to read the full CSV file into R\n\ndplyr support\n\nmultiplyr\n\nOption for data &gt; 10M rows and you only have access to one machine\nSpreads data over local cores\n\n{sparklyr}\n\nspin up a spark cluster\ndplyr support\nSet-up a cloud bucket and load data into it. Then, read into a local spark cluster. Process data.\n\n{h2o}\n\nh2o.import_file(path=path) holds data in the h2o cluster and not in memory\n\n{disk.frame}\n\nsupports many dplyr verbs\nsupports¬† future package to take advantage of multi-core CPUs but single machine focused\nstate-of-the-art data storage techniques such as fast data compression, and random access to rows and columns provided by the fst package to provide superior data manipulation speeds\n\nMatrix ops\n\nsee bkmks: mathematics &gt;&gt; packages\n\n{ff}\n\nsee bkmks: data &gt;&gt; loading/saving/memory\nThink it converts files to a ff file type, then you load them and use ffapply to perform row and column operations with base R functions and expressions\nmay not handle character and factor types but may work with {bit} pkg to solve this",
    "crumbs": [
      "Big Data"
    ]
  },
  {
    "objectID": "qmd/big-data.html#sec-bgdat-viz",
    "href": "qmd/big-data.html#sec-bgdat-viz",
    "title": "Big Data",
    "section": "Viz",
    "text": "Viz\n\nScatter¬†plots\n\n{scattermore}, {ggpointdensity}\n{ggrastr}\n\nRasterize only specific layers of a ggplot2 plot (for instance, large scatter plots with many points) while keeping all labels and text in vector format. This allows users to keep plots within a reasonable size limit without losing the vector properties of scale-sensitive information.\ngithub; tweet\nExample\ngeom_point &lt;- function(...) {\n  ggrastr::rasterise(ggplot2::geom_point(...), dpi = 300)\n}\n\n\nH2O\n\nh2o.aggregator Reduces data size to a representive sample, then you can visualize a clustering-based method for reducing a numerical/categorical dataset into a dataset with fewer rows A count column is added to show how many rows is represented by the exemplar row (I think)\n\nAggregator maintains outliers as outliers but lumps together dense clusters into exemplars with an attached count column showing the member points.\nFor cat vars:\n\nAccumulate the category frequencies.\nFor the top 1,000 or fewer categories (by frequency), generate dummy variables (called one-hot encoding by ML people, called dummy coding by statisticians).\nCalculate the first eigenvector of the covariance matrix of these dummy variables.\nReplace the row values on the categorical column with the value from the eigenvector corresponding to the dummy values.\n\ndocs; article\n\n\n{dbplot}\n\nplots data that are in databases\n\nAlso able to plot data within a spark cluster\n\ndocs\n\nObservableHQ\n\n{{{deepscatter}}}\n\nThread (using Arrow, duckdb)",
    "crumbs": [
      "Big Data"
    ]
  },
  {
    "objectID": "qmd/bayes-workflow.html",
    "href": "qmd/bayes-workflow.html",
    "title": "Workflow",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Bayes",
      "Workflow"
    ]
  },
  {
    "objectID": "qmd/bayes-workflow.html#sec-bayes-wkflw-misc",
    "href": "qmd/bayes-workflow.html#sec-bayes-wkflw-misc",
    "title": "Workflow",
    "section": "",
    "text": "Also see Model Building, Concepts &gt;&gt; Misc &gt;&gt; Regression Workflow\nNotes from\n\nBayesian Workflow (Gelman, Vehtari) (arXiv link)\n\nResources\n\nVehtari Video: On Bayesian Workflow (2022) (Based on the paper, but I haven‚Äôt watched it, yet)\nNabiximols treatment efficiency: Vehtari‚Äôs example of applyijng his workflow in the context of comparing continuous and discrete observation models.\nSupporting Bayesian modelling workflows with iterative filtering for multiverse analysis (Haven‚Äôt read yet)\n\nCurrent Checklist\n\nCheck convergence diagnostics\nDo posterior predictive checking\nCheck residual plots\nModel comparison (if prediction)\n\nAnalysis Checklist (Thread)\n\nA suitably flexible Bayesian regression adjustment model,\nChosen by cross-validation/LOO,\nIncluding Gaussian processes for the unit-level effects over time (and space/network if relevant),\nImputation of missing data, and\nInformative priors for biases in the data collection process.",
    "crumbs": [
      "Bayes",
      "Workflow"
    ]
  },
  {
    "objectID": "qmd/glossary-ds-terms.html",
    "href": "qmd/glossary-ds-terms.html",
    "title": "Glossary: DS terms",
    "section": "",
    "text": "200 Status - An API serving an ML model returns a HTTP 200 OK success status response code indicates that the request has succeeded.\nAMI - amazon machine image. Thing that has R and the main¬†packages you need to load onto the cloud server\nAnti-Patterns - certain patterns in software development that are considered bad programming practices.\n\nAs opposed to design patterns which are common approaches to common problems which have been formalized and are generally considered a good development practice, anti-patterns are the opposite and are undesirable.\n\nArm -¬†a group of patients receiving a specific treatment (or no treatment). Trials involving several arms, or randomized trials, treat randomly-selected groups of patients with different therapies in order to compare their medical outcomes. Experimental arms, which receive an experimental drug, are compared with control arms.¬†Single-arm or non-randomized trials, in which everyone enrolled in a trial receives the experimental therapy\nArtifacts - objects that are created as a result of a process. e.g.¬†model objects, cleaned data sets, visuals, etc.\nAsynchronous Programming - code runs (or must run) after something else happens and also not sequentially (e.g.¬†when a function calls a callback function in JS).\nAthena - amazon query service that works with S3. Best for analyses using kubernetes. ODBC drivers are best with interactive app\nB2C, B2B - business-to-consumer, business-to-business, describes a business that‚Äôs end-product is being sold to a consumer or a business.\nBalanced Design (aka orthogonal) has an equal number of observations for all possible level combinations. For example in an experiment where gender is an independent variable, an equal number of males receive the treatment as do females receive treatment. If the male/female counts were unequal, then the experiment is unbalanced.\n\nStat tests have greater power for balanced designs\nTest stat less susceptible to to small departures from the assumption of equal variances (homoscedasticity).\n\nBatch - collect a large number of data points, process them periodically and store results somewhere (contrasts with real-time in which a data input leads to an immediate prediction)\nBootstrapping (CS) - usually applies to a situation where a system depends on itself to start, sort of a chicken and egg problem. (e.g.¬†How do you start an OS initialization process if you don‚Äôt have the OS running yet?) Typically a simple file that starts a large process.\nBounce, Email - When an email cannot be delivered to an email server.\n\nHard Bounce - indicates a permanent reason an email cannot be delivered (e.g.¬†Recipient email address doesn‚Äôt exist; Recipient email server has completely blocked delivery)\nSoft Bounce - indicates a temporary delivery issue (for details on the reasons, see link)\n\nBounce Rate - the percentage of visitors to a particular website who navigate away from the site after viewing only one page. Low bounce rate can indicate the landing page needs improvement\nBPI - Business process improvement is a management exercise in which enterprise leaders use various methodologies to analyze their procedures to identify areas where they can improve accuracy, effectiveness and/or efficiency and then redesign those processes to realize the improvements.\nBLUE - best linear unbiased estimator, e.g.¬†regression line\nCAC - customer acquisition cost - measures how much an organization spends to acquire new customers. The total cost of sales and marketing efforts, as well as property or equipment, needed to convince a customer to buy a product or service.\nCapEx - Capital Expenditure - 1 of 2 main forward budgeting mechanisms for a corporation (also see OpEx). Often used to undertake new projects or investments or large-scale asset acquisitions (buildings and vehicles)\nClinical Trial - research studies (e.g.¬†RCT) performed in people that are aimed at evaluating a medical, surgical, or behavioral intervention\nCDI - Customer Data Infrastructure - built to collect behavioral data from primary or first-party data sources, but some solutions also support a handful of secondary data sources (third-party tools)\nCDP - Customer Data Platform - add-ons from CDI vendors; a layer on top of CDI that offers a set of capabilities to analyze data using a visual interface.\nCDN - content delivery network - a system of distributed servers (network) that deliver pages and other web content to a user, based on the geographic locations of the user, the origin of the webpage and the content delivery server.\nCLV/CLTV - Customer Lifetime Value - how much money a customer will bring your brand throughout their entire time as a paying customer.\nCOGS - Cost of goods sold (aka Cost of Sales) - refers to the direct costs of producing the goods sold by a company. This amount includes the cost of the materials and labor directly used to create the good. It excludes indirect expenses, such as distribution costs and sales force costs.\nComplete Factorial Design - a research study involving two or more independent variables in which every possible combination of the levels of each variable is represented. For instance, in a study of two drug treatments, one (A) having two dosages and the other (B) having three dosages, a complete factorial design would pair the dosages administered to different individuals or groups of participants as follows: A1 with B1, A1 with B2, A1 with B3, A2 with B1, A2 with B2, and A2 with B3.\nCPG - Consumer packaged goods are items used daily by average consumers that require routine replacement or replenishment, such as food, beverages, clothes, tobacco, makeup, and household products.\nCPC - Cost Per Click¬†- refers to the cost an advertiser pays each time an online user clicks on his or her digital ad\nCRM - customer relationship management i.e.¬†customer service. Salesforce tracks this data. Example: what features your salesperson promised, and when? How much revenue you have from each customer? Or which salesperson sold the most in the past year?\ncron- standard tool used on Unix and Unix-like systems to schedule the periodic execution in the background of a command or script (like a batch script)\nCrossed Factors - when every category of one factor co-occurs in the design with every category of the other factor. In other words, there is at least one observation in every combination of categories for the two factors. (in contrast to ‚Äúnested factors‚Äù). As a consequence, interaction terms involving these two factors is allowed.\nCrossover Study - A type of clinical trial in which the study participants receive each treatment in a random order. With this type of study, every patient serves as his or her own control. Crossover studies are often used when researchers feel it would be difficult to recruit participants willing to risk going without a promising new treatment.\nCross-Section Data - randomly sampled data from a population. Like a survey. Aka observational data. See experimental data for comparison.\n\nPooled - differs from panel data in that it is observations of different subjects (instead of the same subjects) in different time periods.\nRolling - both the presence of an individual in the sample and the time at which the individual is included in the sample are determined randomly.\n\nCross-Tabs - section of survey analysis where the aggregated results are broken down by demography, party affiliation, etc.\nCTA - marketing term, call-to-action.¬†any device designed to prompt an immediate response or encourage an immediate sale; words or phrases that can be incorporated into sales scripts, advertising messages or web pages that encourage consumers to take prompt action\nCTR - click through rate: the ratio of users who click on a specific link to the number of total users who view a page, email, or advertisement. It is commonly used to measure the success of an online advertising campaign for a particular website as well as the effectiveness of email campaigns.\nCRM - Customer Relationship Management - acquiring new customers but especially about retaining existing ones\nDAU - daily active users, ex: daily avg # of registered users of the site over past 30 days\nDBA - Database Administrator is an admin role that understands the particular database technology and how to get the best out of it. This includes improving performance, backups and recovery.\nDDL - Data definition or description language - Subset of SQL. Used to:\n\nKeep a snapshot of the database structure\nSet up a test system where the database acts like the production system but contains no data\nProduce templates for new objects that you can create based on existing ones. For example, generate the DDL for the Customer table, then edit the DDL to create the table Customer_New with the same schema.\n\nDesparate Impact Analysis - Analysis of the result of the application of a standard, requirement, test or other screening tool used for selection that‚Äîthough appearing neutral‚Äîhas an adverse effect on individuals who belong to a legally protected class Differential Dropout**]{style=‚Äòcolor: #009499‚Äô} -¬†Differing dropout rates between treatment arms\nDMA - Designated Market Area; a geographic region where Nielsen, the ratings company, analyzes and quantifies how television is viewed. Residents can receive the same local TV and radio stations\nDNS -¬†¬†Domain Name System**]{style=‚Äòcolor: #009499‚Äô} -¬† translates domain names to IP addresses so browsers can load Internet resources.\nDSL - domain-specific language - a computer language specialized to a particular application domain\nEMR - Amazon version of a spark cluster used for big data processing and analysis.\nEndogenous - A model variable is correlated with other variables excluded from the model (omitted variable bias). Determined by measuring the correlation between the variable and residuals of the model. If a predictor variable hasn‚Äôt been randomly assigned, it‚Äôs likely to be endogenous.\nEquitability - concept that says¬†a dependence measure should give equal importance to linear and nonlinear relationships. Consistent strength measurements across different variable relationships that have similar amounts of noise.\nERP - enterprise resource planning, sort of a catch-all for manufacturing, supply-chain, etc, see the wiki\nETL - extract, transfer, load - usually refers to transferring data from one location to another\nEndpoint (biostats) - Outcome variable measured in a medical study. e.g.¬†Death, stroke, or quality of life are good endpoints. Blood tests and images on scans are not good endpoints.\n\nA composite endpoint is one that consists of two or more events\n\nExample: death due to cardiovascular causes or hospitalization due to heart failure\n\nSo the binary outcome would be a 1 if either of those events took place or a 0 if they did not. Or in a survival model, time until either of those events.\n\n\n\nEOF - End of file - Input from a terminal never really ‚Äúends‚Äù (unless the device is disconnected), but it is useful to enter more than one ‚Äúfile‚Äù into a terminal, so a key sequence is reserved to indicate end of input.\nex ante - based on assumption and prediction and being essentially subjective and estimative\nex post - based on knowledge and retrospection and being essentially objective and factual\nExperimental Data - data from a RCE/RCT. Compare with observational data\nFaaS - Function as a service - type of cloud service for developing, running, and managing apps (e.g.¬†AWS Lambda)\nFactorial Design - Experiment where you‚Äôre interested in the effect of two or more independent variables.\nFraud Rules - fraud scores are calculated based on rules, which add or subtract points. The user action may be a transaction, signup or login. Rules look at data points such as an email address, IP address, or social media presence.\nFraud Score - assigned values to how risky a user action is. Scoring determined by fraud rules.\nFuzzy Design - See Sharp Design\nGHA - Github Actions\nGMV - Gross merchandises value - the total value of merchandise sold over a given period of time through a customer-to-customer (C2C) exchange site\nGRP - Gross Rating Point. A standard measure in advertising, it measures advertising impact. You calculate it as a percent of the target market reached multiplied by the exposure frequency. Thus, if you get advertise to 30% of the target market and give them 4 exposures, you would have 120 GRP.\nHTE - Heterogeneous Treatment Effect - Also called differential treatment effect, includes difference of means, odds ratios, and Hazard ratios for time-to-event outcome vars\n\nAscertaining subpopulations for which a treatment is most beneficial (or harmful) is an important goal of many clinical trials.\nOutcome heterogeneity is due to wide distributions of baseline prognostic factors. When strong risk factors exist, there is hetergeneity in the outcome variable.\n\nSolution: add baseline predictors to your model that account for these strong risk factors.\n\nHeterogeneity of Treatment Effects - The degree to which different treatments have differential causal effects on each unit.\n\nHit Ratio - percent of records that were read in order to complete a query in a database. Cloud db providers often charge by the number of records searched\nHomogeneity of Treatment Effects - See Heterogeneity of Treatment Effects\nHPC - High Performance Computing\nHoneypot - data (for example, in a network site) that appears to be a legitimate part of the site, but is actually isolated and monitored, and that seems to contain information or a resource of value to attackers, who are then blocked.\nIaaS - infrastructure-as-a-service ( Hardware is provided by an external provider and managed for you)\nIAM - identity and access management, keys and passwords etc\nIRB - institutional review board, reviews studies ethical and moral issues\nITT - Intent-to-Treat analysis¬†includes all randomized patients in the groups to which they were randomly assigned, regardless of their adherence with the entry criteria, regardless of the treatment they actually received, and regardless of subsequent withdrawal from treatment or deviation from the protocol. Avoids overoptimistic estimates of the efficacy of an intervention resulting from the removal of non-compliers by accepting that noncompliance and protocol deviations are likely to occur in actual clinical practice. So mimics likely situation in the real world, but not good for estimating the causal effect of a treatment.\nKernels - (article) - system kernels - the interface between the operating system, i.e.¬†the software, and the hardware components in a device. It is used in all devices with an operating system, for example, computers, laptops, smartphones, smartwatches, etc.\n\nWhen we use a program on a computer, such as Excel, we handle it on the so-called Graphical User Interface (GUI). The program converts every button click or other action into machine code and sends it to the operating system kernel. If we want to add a new column in an Excel table, this call goes to the system core. This in turn passes the call on to the computer processing unit (CPU), which executes the action.\nJupyter Kernels - an engine that executes notebook code and is specific to a particular programming language (e.g.¬†python kernel)\nKaggle Kernels - a free platform from Kaggle to run Jupyter notebooks in the browser. Advantage is that you don‚Äôt have to set-up an environment locally.\n\nKPI- key performance indicator\nKYC - Know-Your-Customer is info a company collects to verify your identity to combat fraud. Used by telecoms and financial services\nLazy Evaluation - ‚Äù never pulls data into R unless you explicitly ask for it. It delays doing any work until the last possible moment. It collects together everything you want to do and then sends it to the database in one step.‚Äù\nLikelihood - probability of seeing this data given a specific value for a distribution parameter (eg mean, sd). Goal is to search for parameter values until the likelihood is maximized.\nLOB - Line of Business is a general term which refers to a product or a set of related products that serve a particular customer transaction or business need. (i.e.¬†product categories)\n\nExamples\n\nConsumer Banking: credit cards, line of credit or loan program, mortgages, and corporate, small business and personal bank accounts.\nFinancial services and brokerages: mergers and acquisitions or partnerships, real estate investments, and wealth management\nProperty and casualty insurance companies: property and casualty insurance (i.e., homeowners, car, boat, renters, etc.), life insurance, health insurance, and commercial business insurance.\n\nSub-lines of Business would be sub-categories within each LOB\n\nLongitudinal Data - see panel data\nLTV - see CLV/CLTV\nManual Review - A human is reviews the case to determine whether action is needed. In fraud, an model output may trigger a ‚Äúmanual review‚Äù to determine whether an event was indeed fraudulent.\nMLlib - Apache Spark machine learning library\nMVC - Minimum Viable Corpus - a data size threshold; such that below this threshold, the data simply isn‚Äôt useful/valuable. Used in data products business.\nMVP - minimum viable project, agile term. Version of a new product which allows a team to collect the maximum amount of validated learning about customers with the least effort\nNamespace - allows you to use two functions with the same name but from different packages, e.g.¬†dplyr::select or in general, package::function. https://stackoverflow.com/questions/3384204/what-are-namespaces/3384384#3384384\nNNH - Numbers Needed to Harm - a derived statistic that tells us how many patients must receive a particular treatment for 1 additional patient to experience a particular adverse outcome. Lower NNT and higher NNH values are associated with a more favorable treatment profile.\nNNT - Numbers Needed to Treat - a derived statistic that tells us how many patients must receive a particular treatment for 1 additional patient to experience a favorable outcome such as treatment response. Lower NNT and higher NNH values are associated with a more favorable treatment profile.\nNPS - Net Promoter Score - a measure of customer loyalty. Widely used market research metric that typically takes the form of a single survey question asking respondents to rate the likelihood that they would recommend a company, product, or a service to a friend or colleague.\nNRT - near real-time, aka streaming data\nObservational Data - see cross sectional data\nOEM - original equipment manufacturer\nOKR - Objectives and Key Results is a popular management strategy for goal setting within organizations. A framework for turning strategic intent into measurable outcomes for an organization.\nOnline Machine Learning - A method of machine learning where the model incrementally learns from a stream of data points in real-time. It‚Äôs a dynamic process that adapts its predictive algorithm over time, allowing the model to change as new data arrives.\nOn-Prem - on-premises ‚Äî working with servers in the the building and not in the cloud.\nOOD - out-of-distribution - data which differ from the training data and on which a model might underperform\nOpen Cohort - subjects can leave or be added over time.\nOpEx - Operational Expenditures - 1 of 2 main forward budgeting mechanisms for a corporation (also see CapEx). Relates to day-to-day expenses (such as payroll and software subscriptions). Smaller payouts over time.\nOpportunity Sizing - Quantitative analysis to select a subset of ideas to which to devote resources in product development\nNested Factors - happens when all the levels of one factor only occur in combination with one level of another factor (in contrast to ‚Äúcrossed factors‚Äù). As a consequence, your model can‚Äôt have an interaction term involving these two variables.\nP&L - Profit and Loss Statement Panel data - cross section data with a time element. Repeated measures of the same subject over time. Synonym for Longitudinal Data\nParcel - a land record that defines the boundary of a piece of land. These boundaries are the basic administrative unit of local government in regards to land and property. Managing ownership and tax records are the primary reason local governments generate these files. So these are boundaries differentiating ownership of properties.\nPEP8 - style guide for python\nPI - principal investigator\nPivot Table - Excel name for a group_by %\\&gt;% summarize calculation\n\ne.g.¬†from a table of individual fruit sales: group_by(fruit_type, country) %\\&gt;% summarize(total_amt = sum(amount))\n\nPLG - Product-led growth is an end user-focused growth model that relies on the product itself as the primary driver of customer acquisition, conversion, and expansion. e.g.¬†open source a product, let the customer go through the documentation and use and experiment with the product on their own time. In contrast to sales pitching a product to a customer and letting them use it for a trial basis.\nPM - product manager\nPoC - Proof of Concept\nPOS - point of sale, The point of sale or point of purchase is the time and place where a retail transaction is completed. It can be in a physical store, where POS terminals and systems are used to process card payments or a virtual sales point such as a computer or mobile electronic device.\nRCE - randomized controlled experiment, subjects randomly assigned to two groups, treatment and control. Double blind means the researcher doesn‚Äôt know who is in which group.\nRCT - randomized clinical trial\nRDD - Regression discontinuity design\nRedis - REmote DIctionary Server - is an in-memory, key-value database, commonly referred to as a data structure server. Used when volume of read and write operations exceed the capabilities of traditional databases. With Redis‚Äôs capability to easily persist the data to disk, it is a superior alternative to the traditional memcached solution for caching.\nRefactoring - updating or optimizing code\nRegression Testing - checks if changes made to a system negatively impacted or broke any of the existing features. It is often performed right after each update or commit to the code base to identify new bugs and ensure that your system works properly.\nRFI - Request for Information - Used to collect written information about the capabilities of various suppliers. Normally it follows a format that can be used for comparative purposes. An RFI is primarily used to gather information to help make a decision on what steps to take next. RFIs are therefore seldom the final stage and are instead often used in combination with request for proposal (RFP), request for tender (RFT), and request for quotation (RFQ).\nRFM - recency, frequency, monetary value - method of estimating customer value; common in retail\nRFP - Request for Proposal - A document that an organization, often a government agency or large enterprise, posts to elicit a response ‚Äì a formal bid ‚Äì from potential vendors for a desired solution. The RFP specifies what the customer is looking for and describes each evaluation criterion on which a vendor‚Äôs proposal will be assessed.\n\nROAS - return on ad spend\nRUG - Regional User Group\nS3 - Amazon simple storage service, database\nSaaS - Software-as-a-service is a mechanism through which companies offer the functionality of their apps, which remain on their company servers, to other companies or customers.\nSCO - sales cycle optimization, active process of providing content on your site (and beyond) that speaks to each of the key phases\nSEO - Search engine optimization, generating high page rankings for key search terms\nSDK - software development kit\nSharp Design - Each individual or group receives the same ‚Äúamount‚Äù of treatment (e.g.¬†a state law or medication dosage). Opposite being fuzzy design (?)\nSKU - Stock Keeping Unit**]{style=‚Äòcolor: #009499‚Äô} - Usually a bar code that has all the information to distinguish it from another product. These attributes can include manufacturer, description, material, size, color, packaging, and warranty terms. When a business takes inventory of its stock, it counts the quantity it has of each SKU.\nSLA - service level agreement - a contract between a service provider and its internal or external customers that documents what services the provider will furnish and defines the service standards this provider is obligated to meet. service. Important for holding prediction latency of an app to a certain standard or maintaining data reliability with vendors. (see link for more details on SLA, SLO, and SLI)\nSLI - service level indicators - metrics that measure compliance with an SLO (see link for more details on SLA, SLO, and SLI)\nSLO - service level objectives - objectives your team must meet in order to meet the conditions of the SLA (see link for more details on SLA, SLO, and SLI)\nSMB - (small to medium-sized business) generally defined as companies with fewer than 1000 employees and less than $1 billion in annual revenue.\nSME - Subject Matter Experts\nSPC - Statistical process control is a method of quality control which employs statistical methods to monitor and control a process\nSpill - missed opportunity metric, measures ‚Äúlost trading days‚Äù on which flights or hotels filled too quickly (the result of pricing too low)\nSpoil - missed opportunity metric, measures empty seats or rooms (often the result of pricing too high)\nSSH - secure shell is a cryptographic Network protocol for operating Network Services securely over an unsecured Network. Typical applications include remote command line login in remote command execution\nstdout - standard output, which is the terminal by default\nTDD - Test-driven development is a style of programming where coding, testing, and design are tightly interwoven\nTF-IDF- stands for term frequency-inverse document frequency, and is often used in information retrieval and text mining.\nThroughput - the amount of material or items passing through a system or process.\ntx - treatment, seen as variable with different treatments as values\nURI - Uniform Resource Identifier - a string of characters that unambiguously identifies a particular resource. e.g.¬†s3//bucket/path/to/folder or http://127.0.0.1:5000or c:\\Users\\me\\path\\to\\folder\nUTM - Urchin Traffic Monitor - used to identify marketing channels\n\ne.g.¬†http://yourwebsite.com/your-post-title/?utm_source=google\n\nutm code = string after ‚Äú?‚Äù\n\nThis person clicked a google ad to get to your site\n\nName comes from Urchin Tracker, a web analytics software that served as the base for Google Analytics.\n\nVPS - virtual private server\nWIP - Work-in-Progress\nWithin Person Study - multiple treatments on each person either all in the same period or different treatments in different periods\nYear-Over-Year - used to make comparisons between one time period and another that is one year earlier.\n\nFormula (percentage): (value_this_year / value_previous_year) - 1\nExample: (sales_Jul_2023 / sales_Jul_2022) - 1",
    "crumbs": [
      "Glossary: DS terms"
    ]
  },
  {
    "objectID": "qmd/python-general.html",
    "href": "qmd/python-general.html",
    "title": "General",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/python-general.html#sec-py-gen-misc",
    "href": "qmd/python-general.html#sec-py-gen-misc",
    "title": "General",
    "section": "",
    "text": "Tools (see article, article for installation and usage)\n\nruff - rust-based, linter and sorts imports\n\nFast, sensible default settings, focuses on more important things out of the box, and has less legacy burden\n\npydocstring - tool for checking compliance with Python docstring conventions\nblack - code formatter\nisort - sorts your imports\npytest, pytest-watch - unit tests\ncommitizen - guides you through a series of steps to create a commit message that conforms to the structure of a Conventional Commit\nnbQA - linting in jupyter notebooks\nmypy - type checker; good support and docs\npylance - checks type hinting in VSCode (see Functions &gt;&gt; Documentation &gt;&gt; Type Hinting)\ndoit - task runner; {targets}-like tool; tutorial\npre-commit - specify which checks you want to run against your code before committing changes to your git repository\nREADME templates - link\n\nPut as much config as possible into pyproject.toml. A lot of configurations tools will happily read from it, and it will give you one source of truth.\nAn underscore _ at the beginning is used to denote private variables in Python.\ndef set_temperature(self, value):\n¬† ¬† ¬† ¬† if value &lt; -273.15:\n¬† ¬† ¬† ¬† ¬† ¬† raise ValueError(\"Temperature below -273.15 is not possible.\")\n¬† ¬† ¬† ¬† self._temperature = value\n\nyou can still access ‚Äú_temperature‚Äù but it‚Äôs just meant for internal use by the class and the underscore indicates this\n\n{{warnings::warnings.filterwarnings(‚Äòignore‚Äô)}}\nsys.getsizeof(obj) to get the size of an object in memory.",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/python-general.html#terms",
    "href": "qmd/python-general.html#terms",
    "title": "General",
    "section": "Terms",
    "text": "Terms\n\nclasses - code template for creating objects, we can think of it as a blueprint. It describes the possible states and behaviors that every object of a certain type could have.\nobject - data structure storing information about the state and behavior of a certain entity and is an instance of a class\nstub file - a file containing a skeleton of the public interface of that Python module, including classes, variables, functions ‚Äì and most importantly, their types. (Source)",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/python-general.html#base",
    "href": "qmd/python-general.html#base",
    "title": "General",
    "section": "Base",
    "text": "Base\n\nInfo method\n\nX.info()\nRemove an object: del\nCheck object type\n\ntype() : outputs the type of an object\nisinstance() : outputs type and inheritance of an object\nSee article for details on differences\n\nImport Libraries\nimport logging\nimport bentoml\nfrom transformers import (\n¬† ¬† SpeechT5Processor,\n¬† ¬† SpeechT5ForTextToSpeech,\n¬† ¬† SpeechT5HifiGan,\n¬† ¬† WhisperForConditionalGeneration,\n¬† ¬† WhisperProcessor,\n)",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/python-general.html#sec-py-gen-fund",
    "href": "qmd/python-general.html#sec-py-gen-fund",
    "title": "General",
    "section": "Fundamentals",
    "text": "Fundamentals\n\nSlicing\n\nFormat my_list[start:stop:step]\n\n** start value is inclusive and the end value is exclusive\n\n1 element and more than 1 element\n\"Python\"[0] # P\n\"Python\"[0:1] # P\n\"Python\"[0:5] # Pytho\nillustrates how when using a range, the last element is exclusive\nNegative indexing my_list[0:-1]\n\nEverything but the last object\n\nSkip every second element\nmy_list = list(\"Python\")\nmy_list[0:len(my_list):2]\n&gt;&gt; ['P', 't', 'o']\nstart at 0, end at len(my_list), step = 2\nShortcuts\nmy_list[0:-1] == my_list[:-1]\nmy_list[0:len(my_list):2] == my_list[::2]\n\"Python\"[::-1] == \"Python\"[-1:-7:-1]\n\nDefaults\n\n0 for the start value\nlen(list) for the stop value\n1 for the step value\n\nDefaults for negative step value\n\n-1 for the start value\n-len(list) - 1 for the stop value\n\n\nAlias vs new object\nb = a # alias\nb = a[:] # new object\n\nWith the alias, changes to a will happen to b as well\n\nCommon use cases\n\n\n\n\n\n\n\nEvery element but the first and the last one\n[1:-1]\n\n\nEvery element in reverse order\n[::-1]\n\n\nEvery element but the first and the last one in reverse order\n[-2:0:-1]\n\n\nEvery second element but the first and the last one in reverse order\n[-2:0:-2]\n\n\n\nUsing slice function\nsequence = list(\"Python\")\nmy_slice = slice(None, None, 2) # equivalent to [::2]\nindices = my_slice.indices(len(sequence))\n&gt;&gt; (0, 6, 2)\n\nShows start = 0, stop = 6, step = 2\n\n\n\n\nF-Strings\n\nCheatsheet\nParameterize with {}\n&gt;&gt; x = 5\n&gt;&gt; f\"One icecream is worth [{x}]{style='color: #990000'} dollars\"\n'One icecream is worth 5 dollars'\n! - functions\n\n!r ‚Äî Shows the string delimiter, calls the repr() method.\n\nrepr‚Äôs goal is to be unambiguous and str‚Äôs is to be readable. For example, if we suspect a float has a small rounding error, repr will show us while str may not\n\n!a ‚Äî Shows the Ascii for the characters.\n!s ‚Äî Converts the value to a string.\n\nGuessing this the str() method (see !r for details)\n\n\nfood2brand = \"Mcdonalds\"\nfood2 = \"French fries\"\nf\"I like eating {food2brand} {food2!r}\"\n\"I like eating Mcdonalds 'French fries'\"\nChange format with ‚Äú:‚Äù\n&gt;&gt; import datetime\n&gt;&gt; date = datetime.datetime.utcnow()\n&gt;&gt; f\"The date is {date:%m-%Y %d}\"\n'The date is 02-2022 15'\nFormatting with ‚Äú&gt;‚Äù and ‚Äú&lt;‚Äù\n\n\n&lt;6 says width is 6 characters and text starts at the left edge\n&gt;10.2f says width is 10 characters, text starts the right hand edge, and number is rounded to 2 decimal places\n\n\n\n\nOperators\n\n(docs)\nExponential: 5**3\nInteger division: 5//3\nModulo: 5%3\nIdentity: is\nx = 5\ny = 3\nprint(\"The result for x is y is\", x is y)\nThe result for x is y is false\n\nThink you can also use == here too\n\nLogical: and and or\nprint(\"The result for 5 &gt; 3 and 6 &gt; 8 is\", 5 &gt; 3 and 6 &gt; 8)\nprint(\"The result for 5 &gt; 3 or 6 &gt; 8 is\", 5 &gt; 3 or 6 &gt; 8)\nThe result for 5 &gt; 3 and 6 &gt; 8 is False\nThe result for 5 &gt; 3 or 6 &gt; 8 is True\nSubset: in and not in\nprint(\"Is the number 3 in the list [1,2,3]?\", 3 in [1,2,3])\nIs the number 3 in the list [1,2,3]? True\n\nprint(\"Is the number 3 not in the list [1,2,3]?\", 3 not in [1,2,3])\nIs the number 3 not in the list [1,2,3]? False\nAssignment",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/python-general.html#sec-py-gen-dattyp",
    "href": "qmd/python-general.html#sec-py-gen-dattyp",
    "title": "General",
    "section": "Types",
    "text": "Types\n\nScalars\n\nCreate scalars by subsetting a list\ninputs = [1, 0.04, 0.9]\n# 1 numeric¬†\nrmse = inputs[0] # rmse = 1 and is type 'float'\n# multiple numerics\nrmse, mape, rsq = inputs\n\nTuples\n\nLists are mutable and tuples are not\n\ni.e.¬†we can add or remove elements to a list after we create it but we cannot do such thing to a tuple\n\nSyntax: name_of_tuple = (a, b)\n\nLists\n\nCreate list of objects (e.g.¬†floats)\nacc_values = [rmse, mape, rsq]\n\nalt method: asterisk-comma notation\n*acc_names, = \"RMSE\", \"MAPE\", \"R-SQ\"\n\nasterisk is ‚Äúunzipping operator‚Äù\n\n\nMake a copy\nold_list = [2, 3, 4]\nnew_list = list(old_list)\n\nDictionaries\n\n** if creating a simple dict, more performant to use curly braces **\n\nAvoid d = dict(1=1, x='x')\n\nJoin 2 dicts -¬† d.update(d2)\n\nIf d and d2 share keys, d2‚Äôs values for those keys will be used\n\nAccess a value from a key: sample_dict['key_name']\nMake a copy\nold_dict = {stuff: 2, more_stuff: 3}\nnew_dict = dict(old_dict)\nConvert list of tuples to a dict\nacc_dict = dict(acc_list)\n\nzip creates lists of tuples (See Loops &gt;&gt; zip section)\n\nAdd key, value pair to a dict\ntransaction_data['user_address'] = '221b Baker Street, London - UK'\n# or\ntransaction_data.update(user_address='221b Baker Street, London - UK')\nUnpack dict into separate tuples for key:value pairs\nrmse, mape, rsq = acc_dict.items()\nrmse\n('RMSE', 1)\n\n** fastest way to iterate over both keys and values in a dict **\ncan also use zip to unpack pairs into a list (see loops &gt;&gt; zip)\n\nUnpack dict into separate lists for keys and values\nacc_keys = list(acc_dict.keys())¬†\nacc_values = list(acc_dict.values())\n\n** fastest way to iterate over a dict‚Äôs keys or values **\n\nUnpack values from dicts into separate scalars\nrmse, mape, rsq = acc_dict.values()\nrmse\n1\nPull the value for a key (e.g.¬†k) or return the default value - d.get(k, default)\n\nDefault is ‚ÄúNone‚Äù. I think this can be set with d.setdefault(k, default)\n\nCheck for specific key (logical)\n‚Äòsend_currency‚Äô in transaction_data\n‚Äòsend_currency‚Äô in transaction_data.keys()\n‚Äòsend_currency‚Äô not in transaction_data.keys()\n\nLike %in% in R\n\nCheck for specific value (logical)\n‚ÄòGBP‚Äô in transaction_data.values()\nCheck for key, value pair\n(‚Äòsend_currency‚Äô, ‚ÄòGBP‚Äô) in transaction_data.items()\nPretty printing of dictionaries\n¬† ¬† _ = [print(k, \":\", f'{v:.1f}') for k,v in acc_dict.items()]\n¬† ¬† RMSE : 1.00\n¬† ¬† MAPE : 0.04\n¬† ¬† R-sq : 0.90\n\nfor-in loop format (see Loops &gt;&gt; Comprehension)\nprint returns ‚Äúnone‚Äù for each key:value at the bottom of the output for some reason. Assigning the print statement to a variable fixes it.\n\ndefaultdict\n\nCreates a key from a list element and groups the properties into a list of values where the value may also be a dict.\nFrom {{collections}}\nAlso see\n\nPybites video\nJSON &gt;&gt; Python &gt;&gt; Example: Parse Nested JSON into a dataframe\n\n\n\nSets\n\nIf performing set logic, always more performant to use sets instead of dicts or lists\n\n\nIf using numpy/pandas, using the .unique() syntax is more efficient for arrays/series‚Äô with numeric values\nIf using strings, it‚Äôs more efficient to use list(set(my_array))\n\n\nStrings\n\nOperators\nOperator Description\n%d Signed decimal integer\n%u unsigned decimal integer\n%c Character\n%s String\n%f Floating-point real number\nExample\n\nname = \"india\"\nage = 19\nmarks = 20.56\nstring1 = 'Hey %s' % (name)\nprint(string1)\nstring2 = 'my age is %d' % (age)\nprint(string2)\nstring3= 'Hey %s, my age is %d' % (name, age)\nprint(string3)\nstring3= 'Hey %s, my subject mark is %f' % (name, marks)\nprint(string3)",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/python-general.html#sec-py-gen-pip",
    "href": "qmd/python-general.html#sec-py-gen-pip",
    "title": "General",
    "section": "pip",
    "text": "pip\n\nLooks for packages on https://pypi.org, downloads, and installs it\nMisc\n\nIf you installed python using the app-store, replace python with python3.\nDon‚Äôt use sudo to install libraries, since it will install things outside of the virtual environment.\nNor should you use ‚Äú‚Äìuser‚Äù, since it‚Äôs made to install things outside of the virtual environment.\nDon‚Äôt mix pip, venv and Anaconda. Avoid Anaconda if you can. If you have to use Anaconda, don‚Äôt use pip and venv. Limit yourself to Anaconda tools.\nIf you get SSL errors (common if you are in a hotel or a company network) use the ‚Äìtrusted-host pypi.org ‚Äìtrusted-host files.pythonhosted.org options with pip to work around the problem.\n\ne.g.¬†python -m pip install pendulum --trusted-host pypi.org --trusted-host files.pythonhosted.org\n\nIf you are behind a corporate proxy that requires authentication (common if you are in a company network), you can use the ‚Äìproxy option with pip to give the proxy address and your credentials.\n\ne.g.¬†python -m pip install pendulum --proxy http://your_username:yourpassword@proxy_address\nIt also works with the https_proxy environment variables\n\npip config set global.require-virtualenv True will only allow pip to install packages while a virtual environment is activated.\n\nProbably needs to be set back to False when updating pip.\nMay become the default in Python 3.13\n\n\nInstall library\n$ python -m pip install &lt;library_name&gt;\n\n# inside ipython or a colab notebook, \"!\" signifies a shell command\n!pip install &lt;library_name&gt;\nInstall library from github\npython -m pip install git+https://github.com/bbalasub1/glmnet_python.git@1.0\n\n‚Äú@1.0‚Äù is the version number\n\nUninstall library\n$ python -m pip uninstall &lt;library_name&gt;\n\nWon‚Äôt uninstall the dependencies of this library.\nIf you wish to also uninstall the unused dependencies as well, take a look at pip-autoremove\n\nRemove all packages in environment\n$ python -m pip uninstall -y -r &lt;(pip freeze)\nRemove all packages in environment but write the names of the packages to a requirements.txt file first\n$ python -m pip freeze &gt; requirements.txt && python3 -m pip uninstall -r         requirements.txt -y\nInstall requirements.txt\n$ python -m pip install -r requirements.txt\nWrite names of all the packages in your environment to a requirement.txt file\n$ python -m pip freeze &gt; requirements.txt\n\nWrites the specific version of the packages that you have installed in your environment (e.g.¬†pandas==1.0.0)\n\nThis may not be what you always want, so you‚Äôll need to manually change to just the library name in that case (e.g.¬†pandas)\n\nOnly aware of the packages installed using the pip install command\n\ni.e.¬†any packages installed using a different approach such as peotry, setuptools, condaetc. won‚Äôt be included in the final requirements.txt file.\n\nDoes not account for dependency versioning conflicts\nSaves all packages in the environment including those that are not relevent to the project\nIf you are not using a virtual environment, pip freeze generates a requirement file containing all the libraries in including those beyond the scope of your project.\n\nList your installed libraries\n$ python -m pip list\nSee if you have a particular library installed\n$ python -m pip list | grep &lt;library_name&gt;\nGet library info (name, version, summary, license, dependencies and other)\n$ python -m pip show &lt;library_name&gt;\nCheck that all installed packages are compatible\n$ python -m pip check\nUpdate package\n$ python -m pip install package_name --upgrade\nSearch for PyPI libraries (pip source for libraries)\n$ python -m pip search &lt;search_term&gt;\n\nreturns all libraries matching search term\n\nDownload a package without installing it\npython -m pip download &lt;library name&gt;\n\nIt will download the package and all its dependencies in the current directory (the files, called, wheels, have a .whl extension).\nYou can then install them offline by doing python -m pip install on the wheels.\n\nBuild Wheel archives for the libraries and dependencies in your environment\n$ python -m pip wheel\n\nI think these are binaries, so they don‚Äôt need compiled if installed in a future environment\nReal Python Tutorial\n\nManage configuration\n$ python -m pip config &lt;action name&gt;\n\nActions: edit, get, list, set or unset\nExample\n$ python -m pip config set global.index-url https://your.global.index\n\nDisplay debug information specific to pip\n$ python -m pip debug",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/python-general.html#sec-py-gen-anac",
    "href": "qmd/python-general.html#sec-py-gen-anac",
    "title": "General",
    "section": "Anaconda",
    "text": "Anaconda\n\nCheck configuration of current environment\nconda list\n\nShows python version used, package names installed and their versions\n\nInstall packages\nconda install &lt;package1&gt; &lt;package2&gt;\nInstall a package from a specific channel\nconda install &lt;package_name&gt; -c &lt;channel_name&gt; -y # Short form\nconda install &lt;package_name&gt; --channel &lt;channel_name&gt; -y # Long form\nPackage installation channels (some packages not available in default channel)\n\nCheck current channels\nconda config --show channels\n\nThe order in which these channels are displayed shows the channel priority.\n\nWhen a package is installed, anaconda will the check the channel at the top of list first then work it‚Äôs way down\n\n\nAdd a channel\nconda config --add channels conda-forge\n\nAdds ‚Äúconda-forge‚Äù to list of available channels\n\nRemove a channel\nconda config --remove channels conda-forge\n\nRemoves the ‚Äúconda-forge‚Äù channel",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/python-general.html#sec-py-gen-env",
    "href": "qmd/python-general.html#sec-py-gen-env",
    "title": "General",
    "section": "Environments",
    "text": "Environments\n\nMisc\n\nWhen you‚Äôre in a virtual environment\n\nAnytime you use the ‚Äúpython‚Äù command while your virtual environment is activated, it will be only the one from this env.\nIf you start a Python shell now, it will see only the things in the current directory, and the things installed in the virtual environment.\nIf you run a script, it will see only the things in the current directory, and the things installed in the virtual environment.\nIf you run a command, the command will be taken from the virtual environment.\nAnd they will only use exactly the version of Python of the virtual environment.\n\nStore environment files with project\nBreakage\n\nYou cannot move a virtual environment, it will stop working. Create a ‚Äúrequirements.txt‚Äù file, delete the virtual environment and create a new one.\nDon‚Äôt rename a directory containing a virtual environment. Or if you do, prepare yourself to create a ‚Äúrequirements.txt‚Äù file, delete the virtual environment and create a new one.\nIf you change the Python used in the virtual environment, such as when uninstalling it, it will stop working.\n\nCreate one big virtual environment for all small scripts.\n\nIf you make a lot of venv, you may be tempted to install everything at the system level for convenience. After all, it‚Äôs a bore to create and activate a virtual environment each time you want to write a five liner. A good balance is one single virtual environment you use for all things quick and dirty.\n\nCreate several virtual environments per versions of python if your project needs to support several versions. You may need several requirements.txt files as well, one for each env.\nRecommendations for a stable dependency environment for your project (article)\n\nDon‚Äôt install the latest major version of Python\n\nMaximum: 1 version under the latest version\nMinimum: 4 versions under the latest version (e.g.¬†latest = 3.11, min = 3.7)\n\nUse only the python.org installer on Windows and Mac, or official repositories on Linux.\nNever install or run anything outside of a virtual environment\nLimit yourself to the basics: ‚Äúpip‚Äù and ‚Äúvenv‚Äù\nIf you run a command, use ‚Äú-m‚Äù\n\nIt lets you run any importable Python module, no matter where you are. Because most commands are Python modules, we can use this to say, ‚Äúrun the module X of this particular python‚Äù.\nThere is currently no way for you to run any python command reliably without ‚Äú-m‚Äù.\nExamples:\n# Don't do :\npip install\n# Do:\npython -m pip install\n\n# Don't do :\nblack\n# Do:\npython -m black\n\n# Don't do :\njupyter notebook\n# Do:\npython -m jupyter notebook\n\nWhen creating a virtual environment, be explicit about which Python you use\n\nGet current python versions installed: py --list-paths (windows)\n\n\n\n\n\npyenv\n\nJust a simple Python version manager ‚Äî think {rig}\n{{pyenv}}, {{pyenv-win}}\nSet-up in RStudio (article)\nCompiles Python under the hood when you install it. But compiling can fail in a thousand ways\npyenv install --list: To see what python versions are available to install\npyenv install &lt;version number&gt;: To install a specific version\npyenv versions: To see what python versions are installed on your system\npyenv global &lt;version number&gt;: The set one python version as a global default\npyenv local &lt;version number&gt;: The set a python version to be used within a specific directory/project\\\n\n\n\npdm\n\nDocs\nPackage and dependency manager similar to npm. Doesn‚Äôt require virtual environments.\nFeatures: auto-updating pyproject.toml, isolating dependencies from dependencies-of-dependencies, active development and error handling\n\n\n\nvenv\n\nMisc\n\nShipped with Python\nDon‚Äôt mix pip, venv and Anaconda. Avoid Anaconda if you can. If you have to use Anaconda, don‚Äôt use pip and venv. Limit yourself to Anaconda tools.\n\nCreate\n\nWindows: py -&lt;py version&gt; -m venv &lt;env name&gt;\nMac/Linux: python3.8 -m venv .venv\n\nWhere the python version is 3.8 and the environment name is ‚Äú.venv‚Äù\nMac and Linux hide folders with names that have preceding ‚Äú.‚Äù by default, so make sure you have ‚Äúdisplay hiddent folders‚Äù activated or you won‚Äôt see it.\n\nNaming Environments\n\nName your environment directory ‚Äú.venv‚Äù, because:\n\nSome editors check for this name and automatically load it.\nIt contains ‚Äúvenv‚Äù, so it‚Äôs explicit.\nIt has a dot, so it‚Äôs hidden on Linux.\n\nIf you have more than one environment directory, use a suffix to distinguish them.\n\ne.g.¬†A project that must work with two different versions of Python (3.9 and 3.10), I will have a ‚Äú.venv39‚Äù and a ‚Äú.venv310‚Äù\n\nNaming enviroments for misc uses\n\n‚Äú.venv_test‚Äù: located in a personal directory to install new tools you want to play with. It‚Äôs disposable, and often broken, so you can delete it and recreate it regularly.\n‚Äú.venv_scripts‚Äù: Used for all the small scripts. You don‚Äôt want to create one virtual environment for each script, so centralize everything. It‚Äôs better than installing outside of a virtual environment, but is not a big constraint.\n\n\n\nActivate\n\nWindows: .venv\\Scripts\\activate\n\nWhere .venv is the name of the virtual environment\nMay need .bat as extension to activate\n\nMac/Linux: source .venv/bin/activate\n\nAfter that, you can use python -m pip install to install packages.\nDeactivate: deactivate\n\n\n\nvirtualenv\n\nDocs\nCreate a virtual environment\n python3 -m venv &lt;env_name&gt;\n\n-m venv tells python to run the virtual environment module, venv\nMake sure you‚Äôre in your projects directory\nRemember to add ‚Äú&lt;env_name&gt;/‚Äù to .gitignore\n\nActivate environment\nsource venv/bin/activate # Mac or Linux\nvenv\\Scripts\\activate # Windows\n\n&gt;&gt; (&lt;env_name&gt;) $\n\nPrompt should change if the environment is activated\nAll pip¬† installed packages will now be installed into the ‚Äú&lt;env_name&gt;/lib/python3.9/site-packages‚Äù directory\n\nUse the python contained within your virtual environment\npython main.py\n\nNot sure why you wouldn‚Äôt just activate the environment.\n\nDeactivate environment\ndeactivate\n\nno python¬† or env_name needed?\n\nReproducing environment\n\nDone using requirements.txt (see pip section for details on writing and installing)\n\nI don‚Äôt think the python version is included, so that will need to communicated manually\n\n\n\n\n\nAnaconda\n\nList environments\nconda env list # method 1\nconda info --envs # method 2\n\ndefault environment is called ‚Äúbase‚Äù\nActive environment will be in parentheses\nActive environment will be the one in the list with an asterix\n\nCreate a new conda environment\nconda create -n &lt;env name&gt;\nconda activate &lt;env name&gt;\nCreate a new conda environment with a specific python version\nconda create -n py310 python=3.10\nconda activate py310\nconda install jupyter jupyterlab\njupyter lab\n\nAlso install and launch jupyter lab\n\nCreate an environment from a yaml file\nconda env create -f environment.yml # Short form\nconda env create --file environment.yml # Long form\nRemove an environment\nconda deactivate &lt;env_name&gt; # Need to deactivate the environment first\nconda env remove -n &lt;env_name&gt;\n\nShould also delete environment folders (conda env list shows path to folders)\n\nClone an existing environment\nconda create -n testclone --clone test # Short form\nconda create --name testclone --clone test # Long form\n\n‚Äútestclone‚Äù is a copy of ‚Äútest‚Äù\n\nActivate an environment\nconda activate &lt;env_name&gt;\nActivate environment with reticulate in R\nreticulate::use_python(\"/usr/local/bin/python\")¬† ¬†\nreticulate::use_condaenv(\"&lt;env name&gt;\", \"/home/jtimm/anaconda3/bin/conda\")\nDeactivate an environment\nconda activate # Option 1: activates base\nconda deactivate test # Option 2\nExport the specifications of the current environment into a YAML file into the current directory\nconda env export &gt; environment.yml # Option 1\nconda env export -f environment.yml # Option 2\nExample: Conda workflow\n\nCreate an environment that uses a specific python version\n\nWithout a specified python version, the environment will use the same version as ‚Äúbase‚Äù\n\nconda create -n anothertest python=3.9.7 -y\n\n-n is the name flag and ‚Äúanothertest‚Äù is the name of the environment\nUses Python 3.9.7\nWithout the -y flag, there‚Äôd be a prompt you‚Äôd have to answer ‚Äúyes‚Äù to\n\nActivate the environment\nconda activate anothertest\nInstall packages\n\n\nInstalling packages one at time can lead to dependency conflicts.\nConda‚Äôs official documentation recommends to install all packages at the same time so that the dependency conflicts are resolved\nconda install \"numpy&gt;=1.11\" nltk==3.6.2 jupyter -y # install specific versions\nconda install numpy nltk jupyter -y # install all latest versions\n\nDo work and deactivate environment\nconda deactivate anothertest\n\nExample Raschka workflow\n# create & activate\nconda create¬† --prefix ~/code/myproj python=3.8\nconda activate ~/code/myproj\n# export env\nconda env export &gt; myproj.yml\n# create new env from yaml\nconda env create --file myproj.yml --prefix ~/code/myproj2\n\n\n\nPoetry\n\nDocs (like renv)\nApparently buggy (article)\npip‚Äôs dependency resolver is more flexible and won‚Äôt die on you if the package specifies bad metadata, while poetry‚Äôs strictness may mean you can‚Äôt install some packages at all.\nCreate project\n\npoetry new &lt;project-dir-name&gt;\n\nautomatically creates a directory for your project with a skeleton\n‚Äúpyproject.toml‚Äù maintains dependencies for the project with the following sections:\n\ntool.poetry provides an area to capture information about your project such as the name, version and author(s).\ntool.poetry.dependencies lists all dependencies for your project.\ntool.poetry.dev-dependencies lists dependencies your project needs for development that should not be present in any version deployed to a production environment.\nbuild-system references the fact that Poetry has been used to manage the project.\n\n\nAdd library and create lock file: poetry add &lt;library name&gt;\n\nWhen the first library is added, a ‚Äúpoetry.lock‚Äù file wil be generated\n\nActivate environment: poetry shell\n\nDeactivate environment: exit\n\nRun script: poetry run python my_script.py\nPackage the project: poetry build\n\nCreates tar.gz and wheel files (.whl) in ‚Äúdist‚Äù dir\n\nExample: poetry workflow (+pyenv, virtualenv)\n# Create a virtual environment called \"my-new-project\"\n# using Python 3.8.8\npyenv virtualenv 3.8.8 my-new-project\n# Activate the virtual environment\npyenv activate my-new-project\n\n{{pyenv}} - For managing the exact version of Python and activating the environment\nName your package the same name as the directory which is the same name as the virtual environment.\n\nDashes for the latter two and underscores for the package\n\nIntitialize the project and add packages (similar to renv) bash              poetry init     poetry add numpy\nReinstall dependencies\n# navigate to my project directory and run\npoetry install\nTurn off virtualenv management\n# right after installing poetry, run:\npoetry config virtualenvs.create false\n\nDefault poetry behavior is that it will manage your virtual environments for you. This may not be desirable because:\n\nCan‚Äôt just run a script from the command line. Instead, have to run poetry run my-script\n\nAwkward when you want to dockerize your code\n\nEnforces a virtual environment management framework on everybody in a shared codebase\nYour Makefile now needs to know about poetry",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/python-general.html#sec-py-gen-deps",
    "href": "qmd/python-general.html#sec-py-gen-deps",
    "title": "General",
    "section": "Dependencies",
    "text": "Dependencies\n\nMisc\n\nAfter mastering pip, {{pip-tools}} is recommended.\n\nGet a complete list of dependencies (e.g.¬†dependencies of dependencies) with {{deptree}}\ndeptree\n# output\nFlask==2.2.2¬† # flask\n¬† Werkzeug==2.2.2¬† # Werkzeug&gt;=2.2.2\n¬† ¬† MarkupSafe==2.1.1¬† # MarkupSafe&gt;=2.1.1\n¬† Jinja2==3.1.2¬† # Jinja2&gt;=3.0\n¬† ¬† MarkupSafe==2.1.1¬† # MarkupSafe&gt;=2.0\n¬† itsdangerous==2.1.2¬† # itsdangerous&gt;=2.0\n¬† click==8.1.3¬† # click&gt;=8.0\n# deptree and pip trees\n\nFlask depends on Werkzeug which depends on MarkupSafe\n\nWerkzeug and MarkupSafe qualify as transitive dependencies for this project\n\nCommented part on the right is the compatible range\n\nrequirements.txt format\n# comment\npandas==1.0.0\npyspark\npip: write names of all the packages in your environment to a requirement.txt file\n$ python3 -m pip freeze &gt; requirements.txt\n\nSee pip section for issues with this method\n\n{{pipx}}\n\nA tool for installing Python CLI utilities that gives them their own hidden virtual environment for their dependencies\nAdds the tool itself to your PATH - so you can install stuff without worrying about it breaking anything else\nInstall\npipx install datasette\n\n{{pipreqs}}\n\nScans all the python files (.py) in your project, then generates the requirements.txt file based on the import statements in each python file of the project\nSet-up: pip install pipreqs\nGenerate requirements.txt file: pipreqs /&lt;your_project_root_path&gt;/\nUpdate requirements.txt:¬† pipreqs --force /&lt;your_project_root_path&gt;/¬†\nIgnore the libraries of some python files from a specific subfolder\npipreqs /&lt;your_project_root_path&gt;/ --ignore  /&lt;your_project_root_path&gt;/folder_to_ignore/\n\n{{pip-compile-multi}}\n\nNotes from:\n\nEnd Python Dependency Hell with pip-compile-multi\n\nCreates and nests multiple requirement files\n\ne.g.¬†Able to keep dev environment from production environment separate\n\nAutoresolution of cross-requirement file conflicts\n\nDependency DAG (how all requirement files are connected) must have exactly one ‚Äúsink‚Äù node\n\nOrganize your most ubiquitous dependencies into a single ‚Äúcore‚Äù set of dependencies that all other nodes require (a source node), and all of your development dependencies in a node that requires all others (directly or indirectly) require (a sink).\n\nSimplifies and allows use of autoresolution functionality\n\nExample: DAG (directionality of the arrows is opposite compared to library docs)",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/python-general.html#sec-py-gen-loadsav",
    "href": "qmd/python-general.html#sec-py-gen-loadsav",
    "title": "General",
    "section": "Loading/Saving",
    "text": "Loading/Saving\n\nMisc\n\n{{pickle}} needs custom class(es) to be defined in another module/file and then imported. Otherwise, PicklingError will be raised.\n\n\n\nFile paths\n\nMisc\n\n{{pathlib}} is recommended\n\n{{os}}\n\nGet current working directory: os.getcwd()\nList all files and directories in working directory: os.listdir()\nList all files and directories from a subdirectory: os.listdir(os.getcwd()+'\\\\01_Main_Directory')\nUsing os.walk(): gathers paths, folders, and files\n\nPaths\n\n\npath = os.getcwd()+'\\\\01_Main_Directory'\nfor folder_path, folders, files in os.walk(path):\n¬† ¬† print(folder_path)\nFolders\n\n\nSimilar code, just replace print(folder_path) with print(folders)\n\nFiles\n\n\n{{glob}}\n\nGet a file path string\nimport glob\npath = os.getcwd()+'\\\\01_Main_Directory'\nfor filepath in glob.glob(path):\n¬† ¬† print(filepath)\n# C:\\Users\\Suraj\\Challenges\\01_Main_Directory\nList all files and subdirectories from a path\npath = os.getcwd()+'\\\\01_Main_Directory\\\\*'\nfor filepath in glob.glob(path):\n¬† ¬† print(filepath)\n\nNote the * wildcard\n\nList all files and subdirectories with a ‚Äú1‚Äù in the name\npath = os.getcwd()+'\\\\01_Main_Directory\\\\*1.*'\nfor filepath in glob.glob(path):\n¬† ¬† print(filepath)\nGet a list of csv file paths from a directory: all_files = glob.glob(\"C:/Users/path/to/dir/*.csv\")\n\nNote that you don‚Äôt need a loop to save to an object\n\nList all files and subdirectories and files in those subdirectories\npath = os.getcwd()+'\\\\01_Main_Directory\\\\**\\\\*.txt'\nfor filepath in glob.glob(path, recursive=True):\n¬† ¬† print(filepath)\n#Output\nC:\\Users\\Suraj\\Challenges\\01_Main_Directory\\ABCD_1.txt\nC:\\Users\\Suraj\\Challenges\\01_Main_Directory\\ABCD_2.txt\nC:\\Users\\Suraj\\Challenges\\01_Main_Directory\\ABCD_3.txt\nC:\\Users\\Suraj\\Challenges\\01_Main_Directory\\ABCD_4.txt\nC:\\Users\\Suraj\\Challenges\\01_Main_Directory\\ABCD_5.txt\nC:\\Users\\Suraj\\Challenges\\01_Main_Directory\\Sub_Dictionary_1\\File_1_in_SubDict_1.txt\nC:\\Users\\Suraj\\Challenges\\01_Main_Directory\\Sub_Dictionary_1\\File_2_in_SubDict_1.txt\nC:\\Users\\Suraj\\Challenges\\01_Main_Directory\\Sub_Dictionary_2\\File_1_in_SubDict_2.txt\nC:\\Users\\Suraj\\Challenges\\01_Main_Directory\\Sub_Dictionary_2\\File_2_in_SubDict_2.txt\n\nComponents: ‚Äú**‚Äù and ‚Äúrecursive=True‚Äù\n\n\n{{pathlib}}\n\nProvides a single Path class with a range of methods (instead of separate functions) that can be used to perform various operations on a path.\nCreate a path object for a directory\nfrom pathlib import Path\npath = Path('origin/data/for_arli')\nCheck if a folder or a file is available in a given path\nif path.exists():\n¬† ¬† print(f\"[{path}]{style='color: #990000'} exists.\")\n¬† ¬† if path.is_file():\n¬† ¬† ¬† ¬† print(f\"[{path}]{style='color: #990000'} is a file.\")\n¬† ¬† elif path.is_dir():\n¬† ¬† ¬† ¬† print(f\"[{path}]{style='color: #990000'} is a directory.\")\nelse:\n¬† ¬† raise ValueError(f\"[{path}]{style='color: #990000'} does not exists\")\n\nChecks if the path ‚Äòorigin/data/for_arli‚Äô exists\n\nif it does, it will check whether it is a file or a directory.\nIf the path does not exist, it will print a raise an Error indicating that the path does not exist.\n\n\nList all files/folders in a path\nfor f in path.iterdir():\n¬† ¬† print(f)\n\nUse it in combination with the previous is_dir() and is_file()¬† methods to list either files or directories.\n\nDelete files/folders in a path\nfor f in path.iterdir():\n¬† ¬† f.unlink()\n\npath.rmdir()\n\nunlink deletes each file in the path\nrmdir deletes the directory.\n\ndirectory must be empty\n\n\nCreate a sequence of directories\n# existing directory: D:\\scripts\\myfolder\np = Path(\"D:\\scripts\\myfolder\\logs\\newfolder\")\np.mkdir(parents=True, exist_ok=True)\n\nCreate path object with desired sequence of directories (e.g.¬†logs\\newfolder)\nmkdir with parents=True creates the sequence of directories\n\nW/exist_ok=True no error with occur if the directory already exists\n\n\nRename directory: path.rename('origin/data/new_name')\nConcatenate a path with string\npath = Path(\"/origin/data/for_arli\")\n# Join another path to the original path\nnew_path = path.joinpath(\"la\")\nprint(new_path) # prints 'origin/data/for_arli/bla'\n\nIt also handles the join between two Path objects\n\nDirectory stats\nprint(path.stat()) # print statistics¬†\nprint(path.owner()) # print owner\n\ne.g.¬†creation time, modification time, etc.\n\nWrite to a file\n# Open a file for writing\npath = Path('origin/data/for_arli/example.txt')\nwith path.open(mode='w') as f:\n¬† ¬† # Write to the file\n¬† ¬† f.write('Hello, World!')\n\nYou do not need to create manually example.txt.\n\nRead a file\npath = Path('example.txt')\nwith path.open(mode='r') as f:\n¬† ¬† # Read from the file\n¬† ¬† contents = f.read()\n¬† ¬† print(contents) # Output: Hello World!\n\n\n\n\nModels\n\nSaving and Loading an estimator as a binary using {{joblib}} (aside: pipelines are estimators)\nimport joblib\n#saving the pipeline into a binary file\njoblib.dump(pipe, 'wine_pipeline.bin')\n#loading the saved pipeline from a binary file\npipe = joblib.load('wine_pipeline.bin')\nSaving and loading a trained model as a pickle file\nimport pickle\n# open file connection\npickle_file = open('model.pkl', 'ab')\n# save the model\npickle.dump(model_obj, pickle_file)\n# close file connection¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬†\npickle_file.close()\n\n# Open conn and save\ntest_dict = {\"Hello\": \"World!\"}\nwith open(\"test.pickle\", \"wb\") as outfile:\n# \"wb\" argument opens the file in binary mode\npickle.dump(test_dict, outfile)\n\n# open file connection\npickle_file = open('model.pkl', 'rb')\n# load saved model\nmodel = pickle.load(pickle_file)\n\n# open conn and load\n# Deserialization\nwith open(\"test.pickle\", \"rb\") as infile:\n¬† ¬† test_dict_reconstructed = pickle.load(infile)\n\nCan serialize almost everything including classes and functions\n\n\n\n\nEnvironment Variables\n\n{{os}}\n\nCheck existence\nenv_var_exists = 'ENV' in os.environ\n# or\nenv_var_exists = os.environ.has_key('ENV')\nList environment variables: print(os.environ)\nLoading\nimport os\n# Errors when not present\nenv_var = os.environ['ENV'] # where ENV is the name of the environment variable\n# Returns None when not present\nenv_var = os.environ.get('ENV', 'DEFAULT_VALUE') # using default value is optional\nSet/Export or overwrite\nos.environ['ENV'] = 'dev'\nLoad or create if not present\ntry:\n¬† ¬† env_var = os.environ['ENV']\nexcept KeyError:\n¬† ¬† os.environ['ENV'] = 'dev'\nDelete\nif 'ENV' in os.environ:\n¬† ¬† del os.environ['ENV']\n\n{{python-decouple}}\n\nAccess environment variables from whatever environment it is running in.\nCreate a .env file in the project root directory: touch .env\nOpen .env in nano text editor: nano .env\n\nNano text editor is pre-installed on macOS and most Linux distros\nCheck if installed/version: nano --version\nBasic usage tutorial\n\nAdd environment variables to file\nUSER=alex\nKEY=hfy92kadHgkk29fahjsu3j922v9sjwaucahf\n\nSave: Ctrl+o\nExit: Ctrl+x\n\n* Add .env to your .gitignore file *\nAccess\nfrom decouple import config\nAPI_USERNAME = config('USER')\nAPI_KEY = config('KEY')\n\n{{python-dotenv}}\n\nReads .env files\nProbably more popular than {{python-decouple}}\nHas a companion R package, {dotenv}, so .env files can be used in projects that use both R and Python.",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/python-general.html#sec-py-gen-funs",
    "href": "qmd/python-general.html#sec-py-gen-funs",
    "title": "General",
    "section": "Functions",
    "text": "Functions\n\nMisc\n\nBenchmarking a function\n\nUsing IPython function\n%time dat['col1001'] = some_function(dat['col1'], dat['col2'], dat['col3'])\n\n%%time prints the wall time for the entire cell whereas %time gives you the time for first line only\n\nUsing a decorator\n\n\nAssigning functions based on arg type\ndef process_data(data):\n¬† ¬† if isinstance(data, dict):\n¬† ¬† ¬† ¬† process_dict(data)¬†\n¬† ¬† else:\n¬† ¬† ¬† ¬† process_list(data)¬†\ndef process_dict(data: dict):\n¬† ¬† print(\"Dict is processed\")\ndef process_list(data: list):\n¬† ¬† print(\"List is processed\")\n\nAssigns data to a particular function depending on whether it‚Äôs a dict or a list\nisinstance checks that the passed argument is of the proper type or a subclass\n\nWrapping functions\nfrom functools import partial\nget_count_df = partial(get_count, df=df)\n\nWraps function to make df the default value for df arg\n\n\n\n\nDocumentation\n\nFunctions should at least include docstrings and type hinting\nDocstrings\n\nTypes: Google-style, Numpydoc, reStructured Text, EpyTex\nInformation to include\n\nFunction description, arg description, return value description, Description of errors, Optional extra notes or examples of usage.\n\nAccess functions docstring:\n\nprint(func_name.__doc__)\nFor large docstrings\nimport inspect\nprint(inspect.getdoc(func_name))\n\nExample: Google-style\ndef send_request(key: str, lat: float = 0, lon: float = 0):\n¬† ¬† \"\"\"Send a request to Climacell Weather API\n¬† ¬† to get weather info based on lat/lon.\n\n¬† ¬† Climacell API provides realtime weather\n¬† ¬† information which can be accessed using\n¬† ¬† their 'Realtime Endpoint'.\n\n¬† ¬† Args:\n¬† ¬† ¬† key (str): an API key with length of 32 chars.\n¬† ¬† ¬† lat (float, optional): value for latitude.\n¬† ¬† ¬† ¬† Default=0\n¬† ¬† ¬† lon (float, optional): value for longitude.\n¬† ¬† ¬† ¬† Default=0\n\n¬† ¬† Returns:\n¬† ¬† ¬† int: status code of the result¬†\n¬† ¬† ¬† dict: Result of the call as a dict\n\n¬† ¬† Notes:\n¬† ¬† ¬† See https://www.climacell.co/weather-api/¬†\n¬† ¬† ¬† for more info on Weather API. You can get\n¬† ¬† ¬† API key from there, too.\n¬† ¬† \"\"\"\n\nFirst sentence should contain the purpose of the function\n\nExample: Numpydoc\ndef send_request(key: str, lat: float = 0, lon: float = 0):\n¬† ¬† \"\"\"\n¬† ¬† Send a request to Climacell Weather API\n¬† ¬† to get weather info based on lat/lon.\n\n¬† ¬† Climacell API provides realtime weather\n¬† ¬† information which can be accessed using\n¬† ¬† their 'Realtime Endpoint'.\n\n¬† ¬† Parameters\n¬† ¬† ----------\n¬† ¬† ¬† key (str): an API key with length of 32 chars.\n¬† ¬† ¬† lat (float, optional): value for latitude.\n¬† ¬† ¬† ¬† Default=0\n¬† ¬† ¬† lon (float, optional): value for longitude.\n¬† ¬† ¬† ¬† Default=0\n\n¬† ¬† Returns\n¬† ¬† -------\n¬† ¬† ¬† int: status code of the result¬†\n¬† ¬† ¬† dict: Result of the call as a dict\n\n¬† ¬† Notes\n¬† ¬† -----\n¬† ¬† ¬† See https://www.climacell.co/weather-api/¬†\n¬† ¬† ¬† for more info on Weather API. You can get\n¬† ¬† ¬† API key from there, too.\n¬† ¬† \"\"\"\n\nType Hinting\n\nThis doesn‚Äôt check the type; it‚Äôs just metadata\n\nsee isinstance (see below), NotImplementedError (see below), or {{typecheck}} and {{mypy} (see bkmks) for type checking that will throw errors\n\nUsing type hints enables you to perform type checking. If you use an IDE like PyCharm or Visual Studio Code, you‚Äôll get visual feedback if you‚Äôre using unexpected types:\nVariables: my_variable_name: tuple[int, ...]\n\nvariable should be a tuple that contains only integers. The ellipsis says the total quantity is unimportant.\n\nFunctions\ndef get_count(threshold: str, column: str, df: pd.DataFrame) -&gt; int:\n¬† ¬† return (df[column] &gt; threshold).sum()\n\n‚Äúthreshold‚Äù, ‚Äúcolumn‚Äù should be strings (str)\n‚Äúdf‚Äù should be a pandas dataframe (pd.DataFrame)\nOutput should be an integer (int)\n\nFunction as an arg: Callable[[Arg1Type, Arg2Type], ReturnType]\n\nExample:\nfrom collections.abc import Callable\ndef foo(bar: Callable[[int, int], int], a: int, b: int) -&gt; int:\n¬† ¬† return bar(a, b)\n\n‚Äúbar‚Äù is a function arg for the function, ‚Äúfoo‚Äù\n‚Äúbar‚Äù is supposed to take: 2 integer args ([int, int]) and return an integer (int)\n\nExample:\ndef calculate(i: int, action: Callable[..., int], *args: int) -&gt; int:\n¬† ¬† return action(i, *args)\n\n‚Äúaction‚Äù takes any number and type of arguments but must return an integer.\nWith *args: int, you also allow a variable number of optional arguments, as long as they‚Äôre integers.\n\nExample: Lambda\nf: Callable[[int, int], int] = lambda x, y: 3*x + y\n\nMay not work\n\n\n\n\n\n\nArgs and Operators\n\nMisc\n\n** Args are not reset to default values after each call **\n\nExample:\n\ndef func(list1=[]):¬† ¬† ¬† # here l1 is a default argument set to []\n¬† ¬† list1.append(\"Temp\")\n¬† ¬† return list1\n\n‚ÄúNone‚Äù + conditional must be used to get the arg to reset back to the default value\n\ndef func(l1=None):¬† ¬† ¬†\n¬† ¬† if l1 is None:¬†\n¬† ¬† ¬† ¬† l1 = []\n¬† ¬† l1.append(\"Temp\")¬†\n¬† ¬† return l1\n\n*\n\nUnpacks Lists\n\nnum_list = [1,2,3,4,5]\nnum_list_2 = [6,7,8,9,10]\n\nprint(*num_list)\n# 1 2 3 4 5\nnew_list = [*num_list, *num_list_2] # merge multiple lists\n# [1,2,3,4,5,6,7,8,9,10]\n*args\n\nFunctions that can accept a varying number of values\n\ndef names_tuple(*args):\n¬† ¬† return args\n\nnames_tuple('Michael', 'John', 'Nancy')\n# ('Michael', 'John', 'Nancy')\nnames_tuple('Jennifer', 'Nancy')\n# ('Jennifer', 'Nancy')\n**\n\nUnpacks Dictionaries\n\nnum_dict = {‚Äòa‚Äô: 1, ‚Äòb‚Äô: 2, ‚Äòc‚Äô: 3}\nnum_dict_2 = {‚Äòd‚Äô: 4, ‚Äòe‚Äô: 5, ‚Äòf‚Äô: 6}\n\nprint(*num_dict) # only keys printed\n# a b c\nnew_dict = {**num_dict, **num_dict_2} # merge dictionaries\n# {‚Äòa‚Äô: 1, ‚Äòb‚Äô: 2, ‚Äòc‚Äô: 3, ‚Äòd‚Äô: 4, ‚Äòe‚Äô: 5, ‚Äòf‚Äô: 6}\n**kwargs\n\nFunctions that can accept a varying number of variable/value pairs (like a ‚Ä¶ in R)\n\ndef names_dict(**kwargs):\n¬† ¬† return kwargs\n\nnames_dict(Jane = 'Doe')\n# {'Jane': 'Doe'}\nnames_dict(Jane = 'Doe', John = 'Smith')\n# {'Jane': 'Doe', 'John': 'Smith'}\nFunction as an arg\ndef classic_boot(df, estimator, seed=1):\n¬† ¬† df_boot = df.sample(n=len(df), replace=True, random_state=seed)\n¬† ¬† estimate = estimator(df_boot)\n¬† ¬† return estimate\n\nBootstrap function with an ‚Äúestimator‚Äù function (e.g.¬†mean) as arg\nUsing a Callable\n\nClass as an arg\nfrom dataclasses import dataclass\n\n@dataclass\nclass Person:\n    name: str\n    age: int\n    address: str\n    phone: str\n    email: str\n\ndef process_data(person: Person):\n    print(f\"Processing data for {person.name}, {person.age}, living at {person.address}. Contact info: {person.phone}, {person.email}\")\n\nperson = Person(\"Alice\", 30, \"123 Main St\", \"555-1234\", \"alice@example.com\")\nprocess_data(person)\n\nMakes function more readable when the function requires a bunch of args\n\nClass as an arg (safer alternative)\nfrom typing import NamedTuple\n\nclass Person(NamedTuple):\n    name: str\n    age: int\n    address: str\n    phone: str\n    email: str\n\ndef process_data(person: Person):\n    print(f\"Processing data for {person.name}, {person.age}, living at {person.address}. Contact info: {person.phone}, {person.email}\")\n\nperson = Person(\"Alice\", 30, \"123 Main St\", \"555-1234\", \"alice@example.com\")\nprocess_data(person)\n\nUsing NamedTuple means that the attributes cannot be overridden\n\ne.g.¬†Executing person.name = \"Bob\" will result in an error because tuples can‚Äôt be modified.\n\n\nMake an arg optional\nlass Address:\n¬† ¬† def __init__(self, street, city, state, zipcode, street2=''):\n¬† ¬† ¬† ¬† self.street = street\n¬† ¬† ¬† ¬† self.street2 = street2\n¬† ¬† ¬† ¬† self.city = city\n¬† ¬† ¬† ¬† self.state = state\n¬† ¬† ¬† ¬† self.zipcode = zipcode\n\n‚Äústreet2‚Äù has default value of an empty string, so it‚Äôs optional\n\n\n\n\nBest Practices\n\nDon‚Äôt mutate dataframes inside functions. Create new objects instead.\n\nBad: Mutates df\n\n\nThe value of df has been assigned to variable in_df[df] when it was passed to modify_df as an argument.\nBoth the original df and the in_df inside the function point to the same memory location, even if they go by different variable names. During the modification of its attributes, the location of the mutable object remains unchanged.\nSince the original instance has been modified, it‚Äôs redundant to return the DataFrame and assign it to the variable.\nNote: This situation is not the case for immutable objects like Strings.\n\nGood: Utilizes a deep copy of the df before mutating\ndef compute_length(word: str) -&gt; int:\n    return len(word)\n\ndef prepare_data(df: pd.DataFrame) -&gt; pd.DataFrame:\n    return pd.concat([\n        df.copy(deep=True),  # deep copy\n        df.name.apply(compute_length).rename(\"name_len\"),\n        ...\n    ], axis=1)\n\n\n\n\nLambda\n\nUseful if you just have 1 expression that you need to execute.\nBest Practices\n\nlambda is an anonymous function, hence it is not a good idea to store it in a variable for future use\nDon‚Äôt use lambdas for single functions (e.g.¬†sqrt). Make sure it‚Äôs an expression.\n\nExample\n# bad\nsqrt_list = list(map(lambda x: math.sqrt(x), mylist))\n# good\nsqrt_list = list(map(math.sqrt, mylist))\n\nAffects performance\n\n\nDon‚Äôt use for complex expressions that require more than 1 line (meh)\n\nPer PEP8 guidelines, Limit all lines to a maximum of 79 characters\nExample\n# bad (118 characters)\ndf[\"FinalStatus\"] = df[\"Status\"].map(lambda x: 'Completed' if x ==\n'Delivered' or x == 'Shipped' else 'Not Completed')\n# instead\ndf[\"FinalStatus\"] = ''\ndf.loc[(df[\"Status\"] == 'Delivered') |\n¬† ¬† ¬† (df[\"Status\"] == 'Shipped'),\n¬† ¬† ¬† 'FinalStatus'] = 'Completed'\ndf.loc[(df[\"Status\"] == 'Not Delivered') |\n¬† ¬† ¬† (df[\"Status\"] == 'Not Shipped'),\n¬† ¬† ¬† 'FinalStatus'] = 'Not Completed'\n\n\nExample: 1 arg\n# py\nlambda x: np.sin(x / period * 2 * np.pi)\n# r\n~sin(.x / period * 2 * pi)\n# r\n\\(x) {sin(x / period * 2 * pi)}\nExample: 2 args\nGreater = lambda x, y : x if(x &gt; y) else y\nGreater(0.002, 0.5897)\nLambda-Filter\n\nFaster than a comprehension\n\nsee Loops &gt;&gt; Comprehensions\n\nFormat: filter(function, data_object)\n\nReturns a filter object, which needs to be converted into data structure such as list or set\n\nExample: Basic\nyourlist = list(np.arange(2,50,3))\nlist(filter(lambda x:x**2&lt;100, yourlist))\n# Output¬†\n[2, 5, 8]\nExample: Filter w/logical\nimport pandas as pd\nimport datetime as dt\n# create a list of 10,000 dates\ndatlist = pd.date_range(dt.datetime.today(), periods=10000).tolist()¬†\n# convert the dates to strings via list comprehension\ndatstrlist = [d.strftime(\"Day %d in %B of year %Y is a %A\") for d in datlist]\ndatstrlist[:4]\n['Day 21 in October of year 2021 is a Thursday', 'Day 22 in October of year 2021 is a Friday', 'Day 23 in October of year 2021 is a Saturday', 'Day 24 in October of year 2021 is a Sunday']\n\nstrLamb = filter(lambda d: ((d.endswith(\"urday\") or d.endswith(\"unday\")) and \"Oc\" in d), datstrlist)\n\nSearches for Saturdays and Sundays in the month of October of all years in list of strings\n\nExample: Nested Lists\ngroup1 = [1,2,3,43,23,42,8,3,7]\ngroup2 = [[3, 34, 23, 32, 42], [6, 11, 9], [1, 3,9,7,2,8]]\n[list(filter(lambda x: x in group1, sublist)) for sublist in group2]\n&gt;&gt; [[3, 23, 42], [], [1, 3, 7, 2, 8]]\n\nProbably useful for json\nfor-loop attached to the end of the list-filter combo\nEach sublist of group 2 is fed into the lambda-filter and compared to the group 1 list\n\n\nIterating over each element of a list\n\nExample: map\nlist(map(lambda x: x**2+x**3, yourlist))\n\nmap returns a map object that needs to be converted\n\nExample: 2 Lists\nmylist = list(np.arange(4,52,3))\nyourlist = list(np.arange(2,50,3))\nlist(map(lambda x,y: x**2+y**2, yourlist, mylist))\n\nLike a pmap\n\n\nNested lambdas\n\nExample: map\narr = [1,2,3,4,5]\nlist(map(lambda x: x*2, filter(lambda x: x%2 == 0, arr)))\n&gt;&gt; [4,8]\n\nWork inside out (locate where the data object, arr, appears)\n‚Äúarr‚Äù is filtered by the first lambda function for even numbers then iterated by map to be squared by the second lambda function\n\n\nIterate over rows of a column in a df\n\nExample: Using formula over rows\ngrade['evaluate']=grade['MathScore'].apply(lambda x: round((x**x)/2,2))\n\n‚Äúgrade‚Äù is the df; ‚ÄúMathScore‚Äù is a numeric column; ‚Äúevaluate‚Äù is the new column in the df\nFormula applied to each value of ‚ÄúMathScore‚Äù to generate each value of evaluate\n\nExample: Conditional over rows\ngrade['group']=grade['MathScore'].apply(lambda x: 'Excellent' if x&gt;=3.0 else 'Average')\n\n‚Äúgrade‚Äù is the df; ‚ÄúMathScore‚Äù is a numeric column; ‚Äúgroup‚Äù is the new column in the df\nConditional applied to each value of ‚ÄúMathScore‚Äù to generate each value of ‚Äúgroup‚Äù\n\nUsing {{swifter}} for parallelization\nimport swifter\ndf['e'] = df.swifter.apply(lambda x: infer(x['a'], x['b'], x['c'], x['d']), axis = 1)\n\nIn a Pivot Table (like a crosstab)\n\nExample\n\ngrades_df\n\n2 names (‚Äúname‚Äù)\n6 scores (‚Äúscore‚Äù)\nOnly 2 letter grades associated with these scores (‚Äúletter grade‚Äù)\n\nTask: drop lowest score for each letter grade, then calculate the average score for each letter grade\n\ngrades_df.pivot_table(index='name',\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† columns='letter grade',\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† values='score',\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† aggfunc = lambda series : (sorted(list(series))[-1] + sorted(list(series))[-2]) / 2)\n\nletter grade¬† ¬† A¬† ¬† B\nname\nArif¬† ¬† ¬† ¬† ¬† 96.5¬† 87.0\nKayla¬† ¬† ¬† ¬† 95.5¬† 84.0\n\nindex: each row will be a ‚Äúname‚Äù\ncolumns: each column will be a ‚Äúletter grade‚Äù\nvalues: value in the cells will be from the ‚Äúscore‚Äù column according to each combination columns in the index and columns args\naggfunc: uses a lambda to compute the aggregated values\n\n‚Äúseries‚Äù is used a the variable¬† in the lambda function\nsorts series (ascending), takes the top two values (using negative list indexing), and averages them\n\n\n\n\n\n\nScope\n\nPopulated objects within functions persist if you instantiate the object in the argument\n\n\n‚Äúall_numbers‚Äù retained it‚Äôs previous value when the 2nd call to the function was made\n\n\n\n\nClosures\n\nInner functions that can access values in the outer function, even after the outer function has finished its execution\nExample\n\n# closure way\ndef balanceOwed(roomN,rate,nights):\n¬† ¬† def increaseByMeals(extra):\n¬† ¬† ¬† ¬† amountOwned=rate*nights+extra\n¬† ¬† ¬† ¬† print(f\"Dear Guest of Room [{roomN}]{style='color: #990000'}, you have\",¬†\n¬† ¬† ¬† ¬† \"a due balance:\", \"${:.2f}\".format(amountOwned))\n¬† ¬† ¬† ¬† return amountOwned\n¬† ¬† return increaseByMeals\n\nba = balanceOwned(201,400,3)\nba(200)\nba(150)\nba(180)\nba(190)\nDear Guest of Room 201, you have a due balance: $1400.00\nDear Guest of Room 201, you have a due balance: $1350.00\nDear Guest of Room 201, you have a due balance: $1380.00\nDear Guest of Room 201, you have a due balance: $1390.00\n\nTedious way: For each value of ‚Äúextra‚Äù (e.g.¬†meals), the function needs to be called even if the other values of the arguments don‚Äôt change.\nClosure way:\n\nincreaseByMeals() is a closure function, because it remembers the values of the outer function balanceOwed(), even after the execution of the latter\nbalanceOwed() is called with its three arguments only once and then after its execution, we call it four times with the meal expenses (‚Äúextra‚Äù).",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/python-general.html#sec-py-gen-mods",
    "href": "qmd/python-general.html#sec-py-gen-mods",
    "title": "General",
    "section": "Modules",
    "text": "Modules\n\n.py files are called ‚Äúmodules.‚Äù\nA directory with .py files in which one of the files is an ‚Äú__init__.py‚Äù is called a package.\nMisc\n\nResource: Make your Python life easier by learning how imports find things\nsys.path contains the list of paths where Python is looking for things to import. Your virtual environment and the directory containing your entry point are automatically added to sys.path.\n\nsys.path¬†is a list. Which means you can¬†.append(). Any directory you add there will have its content importable. It‚Äôs a useful hack, but use it as a last resort.\n\nWhen using -m flag to run a script, if you pass a package instead of a module, the package must contain a ‚Äú__main__.py‚Äù file for it to work. This __main__.py module will run.\nIf you have scripts in your projects, don‚Äôt run them directly. Run them using ‚Äú-m‚Äù, and you can assume everything starts from the root.\n\nExample:\ntop_dir\n‚îú‚îÄ‚îÄ foo\n‚îÇ   ‚îú‚îÄ‚îÄ bar.py\n‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n‚îÇ   ‚îî‚îÄ‚îÄ blabla.py\n‚îî‚îÄ‚îÄ blabla.py\n\nRunning python foo/bar.py, ‚Äútop_dir‚Äù is the current working directory, but ‚Äúfoo‚Äù is added to the sys.path.\nRunning python -m foo.bar, ‚Äútop_dir‚Äù is the current working directory and added to sys.path.\n\nImports can all start from the root of the project and opened file paths as well.\n\n\n\n\nUsage\n\nProject Structure\n‚îú‚îÄ‚îÄ main.py\n‚îú‚îÄ‚îÄ packages\n‚îÇ¬† ‚îî‚îÄ‚îÄ __init__.py\n‚îÇ¬† ‚îî‚îÄ‚îÄ module_1.py\n‚îÇ¬† ‚îî‚îÄ‚îÄ module_2.py\n‚îÇ¬† ‚îî‚îÄ‚îÄ module_3.py\n‚îî‚îÄ‚îÄ ‚îî‚îÄ‚îÄ module_4.py\n\n‚Äú__init__.py‚Äù contains only 1 line which declares all the functions (or classes?) that are in the modules\n__all__ = [\"func1\", \"func2\"]\n\nIf the module files contained classes with multiple functions, I think you‚Äôd just declare the classes and not every function in that class.\n\nIf using classes, each module should only have 1 class.\n\n\nScripts need to include ‚Äú_main_‚Äù in order to used in other scripts\n# test_function.py\ndef function1():¬†\n¬† ¬† print(\"Hello world\")¬†\nfunction1()\n\n# Define the __main__ script\nif __name__ == '__main__':¬† ¬†\n¬† ¬† # execute only if run as a script\n¬† ¬† function1()\n\nSays if this file is being run non-interactively (i.e.¬†as a script), run this chunk\nAdd else: chunk, then that chunk will be run only if the file is imported as a module\nAllows you to allow or prevent parts of code from being run when the modules are imported\nImporting a module without _main_ in a jupyter notebook results in this\n\n\nLoading\n\nDO NOT USE from &lt;library&gt; import *\n\nThis will import anything and everything from that library and causes several problems:\n\nYou don‚Äôt know what is in that package, so you have no idea what you just imported, or even if what you want is in there.\nYou just filled your local namespace with an unknown quantity of mysterious names, and you don‚Äôt know what they will shadow.\nYour editor will have a hard time helping you since it doesn‚Äôt know what you imported.\nYour colleague will hate you because they have no idea what variables come from where.\n\nException: In the shell, it‚Äôs handy. Sometimes, you want to import all things in __init__.py and you have ‚Äú__all__‚Äù defined (see above)\n\nFrom the working directory, it‚Äôs like importing from a library: from file1 import function1\nFrom a subdirectory, from subdirectory.file1 import function1\nFrom a directory outside the project, add the module to sys.path before importing it\nimport sys\nsys.path.append('/User/moduleDirectory')\n\nWhen a module is imported, it first searches for built-in modules, then the paths listed in sys.path\nThis appends the new path to the end of the sys.path\nimport sys\nsys.path.insert(1, '/User/moduleDirectory')\nPuts this path at the front of the sys.path directory list.\nimport sys\nsys.path.remove('/User/NewDirectory')\n\n*delete path from sys.path after you finish*\nPython will also search this path for future projects unless they are removed",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/python-general.html#sec-py-gen-cond",
    "href": "qmd/python-general.html#sec-py-gen-cond",
    "title": "General",
    "section": "Conditionals",
    "text": "Conditionals\n\nIf-Else\n\nSyntax\nif &lt;expression&gt;:\n¬† ¬† do something\nelse:\n¬† ¬† do something else\nExample\nregenerate = False\nif regenerate:\n    concepts_list = df2Graph(df, model='zephyr:latest')\n    dfg1 = graph2Df(concepts_list)\n    if not os.path.exists(outputdirectory):\n        os.makedirs(outputdirectory)\n\n    dfg1.to_csv(outputdirectory/\"graph.csv\", sep=\"|\", index=False)\n    df.to_csv(outputdirectory/\"chunks.csv\", sep=\"|\", index=False)\nelse:\n    dfg1 = pd.read_csv(outputdirectory/\"graph.csv\", sep=\"|\")\n\nTry-Except\n\nExample\nimport os\ntry:\n¬† ¬† env_var = os.environ['ENV']\nexcept KeyError:\n¬† ¬† # Do something\n\nIf ‚ÄúENV‚Äù is not a present a KeyError is thrown. Then, except section executed.\n\n\nMatch (&gt; Python 3.10) (switch function)\nmatch object:\n¬† ¬† case &lt;pattern_1&gt;:\n¬† ¬† ¬† ¬† &lt;action_1&gt;\n¬† ¬† case &lt;pattern_2&gt;:\n¬† ¬† ¬† ¬† &lt;action_2&gt;\n¬† ¬† case &lt;pattern_3&gt;:\n¬† ¬† ¬† ¬† &lt;action_3&gt;\n¬† ¬† case _:\n¬† ¬† ¬† ¬† &lt;action_wildcard&gt;\n\n‚Äúobject‚Äù is just a variable name; could be anything\n‚Äúcase_‚Äù is the value used when none of the other cases are a match\nExample: function input inside user function\ndef http_error(status):\n¬† ¬† match status:\n¬† ¬† ¬† ¬† case 200:\n¬† ¬† ¬† ¬† ¬† ¬† return 'OK'\n¬† ¬† ¬† ¬† case 400:\n¬† ¬† ¬† ¬† ¬† ¬† return 'Bad request'\n¬† ¬† ¬† ¬† case 401 | 403 | 404:\n¬† ¬† ¬† ¬† ¬† ¬† return 'Not allowed'\n¬† ¬† ¬† ¬† case _:\n¬† ¬† ¬† ¬† ¬† ¬† return 'Something is wrong'\nExample: dict input inside a function\ndef get_service_level(user_data: dict):\n¬† ¬† match user_data:\n¬† ¬† ¬† ¬† case {'subscription': _, 'msg_type': 'info'}:\n¬† ¬† ¬† ¬† ¬† ¬† print('Service level = 0')\n¬† ¬† ¬† ¬† case {'subscription': 'free', 'msg_type': 'error'}:\n¬† ¬† ¬† ¬† ¬† ¬† print('Service level = 1')\n¬† ¬† ¬† ¬† case {'subscription': 'premium', 'msg_type': 'error'}:\n¬† ¬† ¬† ¬† ¬† ¬† print('Service level = 2')\nExample: inside a class\nclass ServiceLevel:\n¬† ¬† def __init__(self, subscription, msg_type):\n¬† ¬† ¬† ¬† self.subscription = subscription\n¬† ¬† ¬† ¬† self.msg_type = msg_type\n\n¬† ¬† def get_service_level(user_data):\n¬† ¬† ¬† ¬† match user_data:\n¬† ¬† ¬† ¬† ¬† ¬† case ServiceLevel(subscription=_, msg_type='info'):\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† print('Level = 0')\n¬† ¬† ¬† ¬† ¬† ¬† case ServiceLevel(subscription='free', msg_type='error'):\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† print('Level = 1')\n¬† ¬† ¬† ¬† ¬† ¬† case ServiceLevel(subscription='premium', msg_type='error'):\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† print('Level = 2')\n¬† ¬† ¬† ¬† ¬† ¬† case _:\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† print('Provide valid parameters')\n\nNote that inside the function, the change from ‚Äú:‚Äù to ‚Äú=‚Äù¬† and ‚Äú()‚Äù following the class name in the ‚Äúcase‚Äù portion of the match\n\n\nAssert\n\nUsed to confirm a condition\n\nIncorrect: assert condition, message¬†\n\nCorrect method:¬†\nif not condition:¬†\n¬† ¬† raise AssertionError\n\nassert is useful for debugging code because it lets you test if a condition in your code returns True, if not, the program will raise an AssertionError.\n** Do not use in production, because when code is executed with the -O (optimize) flag, the assert statements are removed from the bytecode. **",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/python-general.html#sec-py-gen-loops",
    "href": "qmd/python-general.html#sec-py-gen-loops",
    "title": "General",
    "section": "Loops",
    "text": "Loops\n\nMisc\n\nList Comprehensions vs Generators in terms of memory usage\n\n{{tqdm}} - progress bar for loops\nfrom tqdm import tqdm\nfor i in tqdm(range(10000))\n¬† ¬† ...\nbreak terminates the loop containing it\n\nIf in a nested loop, it will terminate the inner-most loop containing it\n\ncontinue is used to skip the remaining code inside a loop for the current iteration only; forces the start of the next iteration of the loop\npass does nothing\n\nused when a statement or a condition is required to be present in the program but we do not want any command or code to execute\n\n\n\n\nIterators\n\nRemembers values\nExample\nD = {\"123\":\"Y\",\"111\":\"PT\",\"313\":\"Y\",\"112\":\"Y\",\"201\":\"PT\"}\nff = filter(lambda e:e[1]==\"Y\", D.items())\n\nprint(next(ff))\n&gt;&gt; ('123', 'Y')\nprint(next(ff))\n&gt;&gt; ('313', 'Y')\napply\n\naxis\n\n0 or ‚Äòindex‚Äô: apply function to each column.\n1 or ‚Äòcolumns‚Äô: apply function to each row.\n\nExample: Function applied to rows of a column of a dataframe (i.e.¬†cells)\ndef df2Graph(dataframe: pd.DataFrame, model=None) -&gt; list:\n  # dataframe.reset_index(inplace=True)\n  results = dataframe.apply(\n    lambda row: graphPrompt(row.text, {\"chunk_id\": row.chunk_id}, model), axis=1\n  )\n\ntext and chunk_id are column names of the dataframe\nrow is the row of the dataframe since axis=1, and from that row, the columns text and chunk_id are subsetted in the arguments of user-defined function.\n\nExample: Formula applied to rows of a column of a dataframe (i.e.¬†cells)\ngrade['evaluate']=grade['MathScore'].apply(lambda x: round((x**x)/2,2))\n\ngrade is the df; MathScore is a numeric column; evaluate is the new column in the df\n\n\n\n\n\nGenerators\n\nGenerators are iterators, a kind of iterable you can only iterate over once. (normal iterators like lists, strings, etc. can be repeatedly iterated over)\nGenerators do not store all the values in memory, they generate the values on the fly\n\nyield - Pauses the function saving all its states and later continues from there on successive calls.\n\nAllows you to consume one element at a time and work with it without requiring you to have every element in memory.\nProduces a generator\n\n\nMisc\n\n{{itertools}} islice can slice a generator.\nAlso see APIs &gt;&gt; {{requests}} for an example\n\nExample: Using a comprehension¬†\nmygenerator = (x*x for x in range(3))\nfor i in mygenerator:\n...¬† ¬† print(i)\n\nProduce a list and ( ) produce a generator¬†\n\nExample: Using a function\ndef create_generator():\n¬† ¬† mylist = range(3)\n¬† ¬† for i in mylist:\n¬† ¬† ¬† ¬† yield i*i\n\nfor i in mygenerator:\n¬† ¬† print(i)\n0\n1\n4\n\nThe first time the for calls the generator object created from your function, it will run the code in your function from the beginning until it hits yield, then it‚Äôll return the first value of the loop.\nThen, each subsequent call will run another iteration of the loop you have written in the function and return the next value.\nThis will continue until the generator is considered empty, which happens when the function runs without hitting yield.\n\nThat can be because the loop has come to an end, or because you no longer satisfy an ‚Äúif/else‚Äù\n\n\nExample: Sending values to (yield)\ndef grep(pattern):\nprint \"Looking for %s\" % pattern\nwhile True:\n¬† ¬† line = (yield)\n¬† ¬† if pattern in line:\n¬† ¬† ¬† ¬† print line,\ng = grep(\"python\")¬† # instantiate with \"python\" pattern to search for\n\ng.next() # Prime it\n&gt;&gt; Looking for python\n\ng.send(\"A series of tubes\") # \"python\" not present so returns nothing\ng.send(\"python generators rock!\") # \"python\" present so returns line\n&gt;&gt; python generators rock!\ng.close() # closes coroutine\n\n(yield) receives the input of the .send method and creates a generator object which is assigned to ‚Äúline‚Äù.\nAll coroutines must be ‚Äúprimed‚Äù by first calling .next() (or send(None))\n\nThis advances execution to the location of the first yield expression\n\n\nExample: Sending values to (yield)\ndef writer():\n¬† ¬† \"\"\"A coroutine that writes data *sent* to it to fd, socket, etc.\"\"\"\n¬† ¬† while True:\n¬† ¬† ¬† ¬† w = (yield)\n¬† ¬† ¬† ¬† print('&gt;&gt; ', w)\ndef writer_wrapper(coro):\n¬† ¬† # TBD\n¬† ¬† pass\nw = writer()\nwrap = writer_wrapper(w)\nwrap.send(None)¬† # \"prime\" the coroutine\nfor i in range(4):\n¬† ¬† wrap.send(i)\n&gt;&gt;¬† 0\n&gt;&gt;¬† 1\n&gt;&gt;¬† 2\n&gt;&gt;¬† 3\n\nA more complex framework if you want to break the workflow into multiple functions\n\n\n\nUsing yield from\n\nAllows for two-way usage (reading/sending) of generators\nExample (reading from a generator)\ndef reader():\n¬† ¬† \"\"\"A generator that fakes a read from a file, socket, etc.\"\"\"\n¬† ¬† for i in range(4):\n¬† ¬† ¬† ¬† yield '&lt;&lt; %s' % i\n\n# with yield\ndef reader_wrapper(g):\n¬† ¬† # Manually iterate over data produced by reader\n¬† ¬† for v in g:\n¬† ¬† ¬† ¬† yield v\n# OR with yield from\ndef reader_wrapper(g):\n¬† ¬† yield from g\nwrap = reader_wrapper(reader())\nfor i in wrap:\n¬† ¬† print(i)\n\nBasic; only eliminates 1 line of code\n\nExample (sending to a generator)\n# with (yield)\ndef writer_wrapper(coro):\n¬† ¬† coro.send(None)¬† # prime the coro\n¬† ¬† while True:\n¬† ¬† ¬† ¬† try:\n¬† ¬† ¬† ¬† ¬† ¬† x = (yield)¬† # Capture the value that's sent\n¬† ¬† ¬† ¬† ¬† ¬† coro.send(x)¬† # and pass it to the writer\n¬† ¬† ¬† ¬† except StopIteration:\n¬† ¬† ¬† ¬† ¬† ¬† pass\n# OR with yield from\ndef writer_wrapper(coro):\n¬† ¬† yield from coro\n\nNeed to see example 4 for the writer() code and the use case\nShows the other advantage of using ‚Äúyield from‚Äù: it automatically includes the code to stop prime and stop the loop.\n\nReusable generator\n\n\nreading example using ‚Äúyield from‚Äù\n\nSlicing a generator\nfrom itertools import islice\ndef gen():\n¬† ¬† yield from range(1,11)\ng = gen()\nmyslice = islice(g, 2)\n&gt;&gt; list(myslice)\n[1, 2]\n&gt;&gt; [i for i in g]\n[3,4,5,6,7,8,9,10]\n\n\n\n\nFor\n\n\nSyntax - for &lt;sequence&gt;: &lt;loop body&gt;\nNumeric Range\nfor i = 1 to 10\n¬† ¬† &lt;loop body&gt;\n\n# from 0 to 519\nfor i in range(520)\n¬† ¬† &lt;loop body&gt;\n\nres = 0\nfor idx in np.arange(0, 100000):\n¬† res += df.loc[idx, 'int']\n\nnp.arange() ran 8000 times faster than the same chunk using range()\n\nList\nnumbers = [1, 2, 3, 4, 5, 6, 7, 8, 9]\neven_numbers = []\nfor item in numbers:\n¬† ¬† if item % 2 == 0:\n¬† ¬† ¬† ¬† even_numbers.append(item)\nprint(even_numbers)\n\n# results: [2, 4, 6, 8]\nList: index and value\nnumbers = [1, 2, 3, 4, 5, 6, 7, 8, 9]\nfor index, element in enumerate(numbers):\n¬† ¬† if element % 2 != 0:\n¬† ¬† ¬† ¬† numbers[index] = element * 2\n¬† ¬† else:\n¬† ¬† ¬† ¬† continue\nprint(numbers)\n# results: [2, 2, 6, 4, 10, 6, 14, 8, 18]\n\nenumerate also gets the index of the respective element at the same time\n\nWith three expressions\nfor (for i = 1; i &lt;= 10; i+=1)\n¬† ¬† &lt;loop body&gt;\nCollection-Based\n\nIf the collection is a dict, then this just iterates over the keys\n\nfor i in &lt;collection&gt;:\n¬† ¬† &lt;loop body&gt;\nIterate over a sliding window\n\nOver dictionary keys and values of a dict\nfor a,b in transaction_data.items():\n¬† ¬† print(a,‚Äô~‚Äô,b)\n\nThe .items method includes both key and value, so it iterates over the pairs.\n\nOver nested dictionaries\nfor k, v in transaction_data_n.items():\n¬† ¬† if type(v) is dict:\n¬† ¬† ¬† ¬† for nk, nv in v.items():¬†\n¬† ¬† ¬† ¬† ¬† ¬† print(nk,‚Äô ‚Üí‚Äô, nv)\n\nIf the item of the dict is itself a dict then another loop iterates through its items.\nnk and nv stand for nested key and nested value\n\nSelecting a specific item in a nested dictionary\nfor k, v in transaction_data_n.items():\n¬† ¬† if type(v) is dict and k == 'transaction_2':\n¬† ¬† ¬† ¬† for sk, sv in v.items():\n¬† ¬† ¬† ¬† ¬† ¬† print(sk,'--&gt;', sv)\n\nOnly transaction_2‚Äô s items are printed\n\nRows of a data.frame\nres = 0\nfor row in df.itertuples():\n¬† res += getattr(row, 'int')\n\nitertuples()¬† is 30x faster than iterrows()\n\n\n\n\nzip\n\nCombine lists into 1 list of tuples\nacc_values = [1, 0.04, 0.9]\nacc_names = [\"RMSE\", \"MAPE\", \"R-sq\"]\nacc_list = list(zip(acc_names, acc_values))\nacc_list\n[('RMSE', 1), ('MAPE', 0.04), ('R-sq', 0.9)]\n\nzip does take lists of different lengths but will create shortest length list with corresponding elements\nCombine lists of unequal lengths but keep the non-paired elements\nfrom itertools import zip_longest\nacc_names3 = [\"RMSE\", \"MAPE\", \"R-sq\", \"MSE\"]\nacc_values3 = [rmse, mape, rsq]¬†\nacc_list3 = list(zip_longest(acc_names3, acc_values3))\n\nUnzip list of tuples into separate lists\nnames, values = zip(*acc_list)\n\nAsterisk is the ‚Äúunzipping operator‚Äù\n\nUnpack dict into a list of separate tuples for key:value pairs\nacc_tuples = list(zip(acc_dict.keys(), acc_dict.values()))\nacc_tuples\n[('RMSE', 1), ('MAPE', 0.04), ('R-sq', 0.9)]\n\n\n\nComprehensions\n\nMisc\n\n‚Äòfor ‚Äî in‚Äô construct within comprehensions is faster than the traditional for-loops\n\nnot faster than (all?) lambda-filters (see functions &gt;&gt; lambda)\n\nReturns lists or dicts (just change the bracket types)\n\nDicts\n\nSyntax: mydict = {key:val for key, val in zip(keys_list, vals_list)}\nCombine key:value lists into a dictionary\nacc_dict = {k:v for k,v in zip(acc_names, acc_values)}\nReturn value and output of expression\nmydict = {v: v**2 for v in numberslist}\nIf numberslist =[1,2,3], then mydict = {1:1, 2:4, 3:9}\n\nLists\n\nSyntax: newlist = [expression for item in iterable if condition == True]\nWith expression\nmylist = [x**2 for x in numberslist]\n\nif numberslist =[1,2,3], then mylist = [1,4,9]\n\nSet values in a list to uppercase\nnewlist = [x.upper() for x in fruits]\nWith conditional expression (if ‚Äî else)\n\nAppend to the comprehension to filter the dictionary or list\nSyntax: mylist = [expressionA if (condition2==True) else expressionB for item in list if (condition1==True)]\nExample: newlist = [x if x != \"banana\" else \"orange\" for x in fruits]\n\nReturn ‚Äúorange‚Äù instead of ‚Äúbanana‚Äù\n\nExample: new_list = [(x**2) if (x&gt;90) else (x**3) for x in old_list if (x%2==0)]\n\nSays\n\nSquare an argument if it exceeds 90, else cube it¬†\nReturn all the exponentiated results only if the argument was an even number\n\n\nExample: c = [d for d in datstrlist if ((d.endswith(\"urday\") or d.endswith(\"unday\")) and \"Oc\" in d)]\n\nString filter than looks for strings with saturdays and sundays in october\n*Slower than a lamda-filter* (See Functions &gt;&gt; lambda)\n\n\n\nNested\n\nSyntax: myset = {{expression(itemA, itemB) for itemA in setA} for itemB in setB}\nExample: {j for i in range(2, int(N**0.5)+1) for j in range(i**2, N, i)}\n\nN = 100000\nCreates a set of all the integers from 2 to 100,000.\nPaces through all the integers i up to the square root of N\nDiscards from the set of 100,000 those numbers j which are equal or larger than the square of i\n\nExample: From link\n# Function to get set labels\ndef get_prediction_set_labels(prediction_set, class_labels):\n    # Get set of class labels for each instance in prediction sets\n    prediction_set_labels = [\n        set([class_labels[i] for i, x in enumerate(prediction_set) if x]) for prediction_set in \n        prediction_sets]\n    return prediction_set_labels\n\nReturns a list where each object in the list is a set object (e.g.¬†{green}, {green, orange})",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/python-general.html#sec-py-gen-debug",
    "href": "qmd/python-general.html#sec-py-gen-debug",
    "title": "General",
    "section": "Debugging",
    "text": "Debugging\n\nMisc\nTerms\n\nException Errors - Raised when the syntax is correct but the program results in an error.\nSyntax Errors - Occur when the interpreter detects invalid syntax (relatively easier to fix)\n\ne.g.¬†unmatched parenthesis\n\nTraceback - A report that helps us understand the reason for an exception.\n\nContains function calls made in the code along with their line numbers",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/python-general.html#sec-py-gen-errhand",
    "href": "qmd/python-general.html#sec-py-gen-errhand",
    "title": "General",
    "section": "Error Handling",
    "text": "Error Handling\n\ntry + except\n\nSays try the main code snippet, but if an exception (error) occurs, run the secondary code snippet, the workaround.\n\ndef pct_difference_error_handling(n1, n2):\n¬† '''Function that takes two numbers and return the percentual difference\n¬† between n1 and n2, being n1 the reference number'''\n\n¬† # Try the main code\n¬† try:\n¬† ¬† pct_diff = (n1-n2)/n1\n¬† ¬† return f'The difference between {n1} and {n2} is {n1-n2}, which is {pct_diff*100}% of {n1}'\n\n¬† # If you find an error, use this code instead\n¬† except:\n¬† ¬† pct_diff = (int(n1)-int(n2))/int(n1)\n¬† ¬† return f'The difference between {n1} and {n2} is {int(n1)-int(n2){style='color: #990000'}[}]{style='color: #990000'}, which is {pct_diff*100}% of {n1}'\n\n¬† # Optional\n¬† finally:\n¬† ¬† print(\"Code ended\")\n\nAssumes the error will be the user enters a string instead of a numeric. If errors, converts string to numeric and calcs.\nfinally: - This argument will always run, regardless if the try block raises an error or not. So it could be a completion message or a summary, for example.",
    "crumbs": [
      "Python",
      "General"
    ]
  },
  {
    "objectID": "qmd/distributions.html",
    "href": "qmd/distributions.html",
    "title": "Distributions",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Distributions"
    ]
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-terms",
    "href": "qmd/distributions.html#sec-distr-terms",
    "title": "Distributions",
    "section": "Terms",
    "text": "Terms\n\nConditional Probability Distributions\n\nNotes from https://www.causact.com/joint-distributions-tell-you-everything.html#joint-distributions-tell-you-everything\nNotation: \\(P(Y | X) = P(Y \\;\\text{and}\\; X) / P(X) = P(Y, X) / P(X)\\)\n\ni.e.¬†ratio of 2 marginal distributions\n\nExample: two tests for cancer are conducted to determine whether a biopsy should be performed\n\nConditional approach: Biopsy everyone at determined to be high risk from test 1; measure the genetic marker (aka test 2) for patients at intermediate risk and biopsy those with a probability of cancer past a certain level based on the marker\n\nWhen we perform regression analysis, we are essentially estimating conditional distributions. The conditional distribution, \\(P(Y|X_1, \\ldots, X_n)\\) represents the distribution of the response variable, \\(Y\\), given the specific values of the predictor variables, \\(X_1, \\ldots, X_n\\).\n\nEmpirical CDF\n\\[\nF_n (x) = \\frac {1}{n} \\sum_{i = 1}^n I(X_i \\leq x)\n\\]\n\nWhere \\(X_1, X_2,\\ldots,X_n\\) are from a population with CDF, \\(F_n (x)\\)\nProcess\n\nTake n samples from an unknown distribution. The more samples you take, the closer the empirical distribution will resemble the true distribution.\nSort these samples, and place them on the x-axis.\nStart plotting a ‚Äòstep-function‚Äô style line ‚Äî each time you encounter a datapoint on the x-axis, increase the step by 1/N.\n\nExample\n\n\nThe CDF of a normal distribution (green) and its empirical CDF (blue)\n\n\nJoint Probability Distribution - Assigns a probability value to all possible combinations of values for a set of random variables.\n\nNotation: \\(P(x_1, x_2, ... ,x_n)\\)\nPlugging in a value for each random variable returns a probability for that combination of values\nExample: Two tests for cancer are conducted to determine whether a biopsy should be performed\n\nJoint approach: biopsy anyone who is either at high risk of cancer (test 1) or who was determined to have a probability of cancer past a certain level, based on the marker from the genetic test (test 2)\nCompare with example in Conditional Probability Distributions\n\nIn the context of regression modeling: the joint distribution refers to the distribution of all the variables involved in the regression analysis. For example, if you have a regression model with a response variable \\(Y\\) and predictor variables \\(X_1, \\ldots, X_n\\), the joint distribution would describe the combined distribution of \\(Y, X_1, \\ldots, X_n\\).\n\nLocation - Distribution parameter determines the shift of the distribution\n\ne.g.¬†mean, mu, of the normal distribution.\n\nMarginal Probability Distribution - Assigns a probability value to all possible combinations of values for a subset of random variables\n\nNotation: \\(P(x_1)\\)\n\n\\(P(x_1,x_2)\\) is sometimes called the Joint Marginal Probability Distribution\n\nThe marginal distribution, \\(P(Y)\\) where \\(Y\\) is a subset of random variables, is calculated from the joint distribution, \\(P(Y = y, Z = z)\\) where \\(Z\\) is the subset of random variables not in \\(Y\\) .\n\n\\(P(Y) = \\sum_{Z=z} P(Y = y, Z = z)\\)\n\nIf \\(Y\\) is just one variable\n\nSays sum all the joint probabilities for all the combinations of values for the variables in \\(Z\\) while holding \\(Y\\) constant\nRepeat for each value of \\(Y\\) to get this summed probability value\nThe marginal distribution is made up of all these values, one for each value of \\(Y\\) (or combination of values if \\(Y\\) is a subset of variables)\n\n\nWhen the joint probability distribution is in tabular form, one just sums up the probabilities in each row where \\(Y = y\\).\nIn the context of regression modeling, the marginal distribution of \\(Y\\) represents the distribution of \\(Y\\) alone, without considering the specific values of the predictor variables.\n\n\nScale - Distribution parameter; the larger the scale parameter, the more spread out the distribution\n\ne.g.¬†s.d., sigma, \\(\\sigma\\) of the normal distribtution\nRate Parameter: the inverse of the scale parameter (see Gamma distribution)\n\nShape - Distribution parameter that affects the shape of a distribution rather than simply shifting it (as a location parameter does) or stretching/shrinking it (as a scale parameter does).\n\ne.g.¬†‚ÄúPeakedness‚Äù refers to how round the main peak is",
    "crumbs": [
      "Distributions"
    ]
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-tests",
    "href": "qmd/distributions.html#sec-distr-tests",
    "title": "Distributions",
    "section": "Tests",
    "text": "Tests\n\nWhy normality tests are great‚Ä¶ as a teaching example and should be avoided in research\n\ntl;dr; KS test has very low power as a Normality test as compared to Shapiro-Wilk, and Shapiro-Wilk isn‚Äôt very good for n &lt; 100\nFor detecting moderate skew, you want at least n &gt; 75 to get 80% power for Shapiro-Wilk\nShapiro-Wilk can detect very fat tails at n &lt; 100, but would require larger sample sizes to detect more moderately thick tails.\nKS is worthless in detecting fat tails and near-worthless at detecting skew\nWhen n gets large (e.g.¬†1000s), these types of tests will almost always reject the null even when the practical deviation from normality is not practically significant.\n\nKolmogorov‚ÄìSmirnov test (KS)\n\nUsed to compare distributions\n\nCan be used as a Normality test or any distribution test\nCan compare two samples\n\nMisc\n\nVectors may need to be standardized (e.g.¬†normality test) first unless comparing two samples H0: Both distributions are from the same distribution\n\nPackages\n\n{KSgeneral} has tests to use for contiuous, mixed, and discrete distributions written in C++\n{stats} and {dgof} also have functions, ks.test\n\nBoth handle continuous and discrete distributions\n\nAll functions take a numeric vector and a base R density function (e.g.¬†pnorm, pexp, etc.) as args\n\nKSgeneral docs don‚Äôt say you can supply your own comparison sample (2nd arg) only the density function but with stats and dgof, you can.\nAlthough they have function to compute the CDFs, so if you need speed, it might be possible to use their functions and do it manually\n\n\n2-sample test as the greatest distance between the CDFs (Cumulative Distribution Function) of each sample\n\nSpecifically, this test determines the distribution of your unknown data sample by constructing and comparing the sample‚Äôs empirical CDF¬† (see Terms) with the CDF you hypothesized. If the two CDFs are close, your unknown data sample likely follows the hypothesized distribution.\n\nKS statistic, \\(D_{n,m} = \\max|\\text{CDF}_1 - \\text{CDF}_2|\\) where \\(n\\) as the number of observations on Sample 1 and \\(m\\) as the number of observations in Sample 2\nCompare the KS statistic with the respective KS distribution based on parameter ‚Äúen‚Äù to obtain the p-value of the test\n\n\\(en = (m \\times n) / (m + n)\\)",
    "crumbs": [
      "Distributions"
    ]
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-beta",
    "href": "qmd/distributions.html#sec-distr-beta",
    "title": "Distributions",
    "section": "Beta",
    "text": "Beta\n\n\nDefined on the interval [0,1]\nThe key difference between the Binomial and Beta distributions is that for the Beta distribution the probability, x, is a random variable, however for the Binomial distribution the probability, x, is a fixed parameter.\nShape parameters are \\(\\alpha\\) and \\(\\beta\\), usually.\n\n\\(\\alpha\\) and \\(\\beta\\) are two positive parameters that appear as exponents of the random variable\n\npdf\n\\[\nf(x) = \\frac {x^{\\alpha - 1} (1-x)^{\\beta - 1}} {B(\\alpha, \\beta)}\n\\]\n\\(\\mathbb{E}(X) = \\frac {\\alpha} {\\alpha + \\beta}\\)\n\\(\\text{Var}(X) = \\frac {\\alpha \\cdot \\beta} {(\\alpha + \\beta)^2 \\cdot (\\alpha + \\beta + 1)}\\)",
    "crumbs": [
      "Distributions"
    ]
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-betbin",
    "href": "qmd/distributions.html#sec-distr-betbin",
    "title": "Distributions",
    "section": "Beta-Binomial",
    "text": "Beta-Binomial\n\n\n\n\n\n\n\n\nWhere k is the number of events in n trials\n\n\n\n\n\n\n\nWhere \\(\\theta\\) is the probability of an event\n\n\n\n\n\n\n\nUsed when the probability of success, p, in a fixed number of Bernoulli trials is unknown or random and can change from trial to trial.\nShape parameters Œ± and Œ≤ define the probability of success (i.e.¬†the success parameter is modeled by the Beta Distribution).\n\nFor large values of Œ± and Œ≤, the distribution approaches a binomial distribution.\nWhen Œ± and Œ≤ both equal 1, the distribution equals a discrete uniform distribution from 0 to n\n\nAccuracy analysis data from psychology follow beta-binomial distributions (Jaeger, 2008; Kruschke, 2014)",
    "crumbs": [
      "Distributions"
    ]
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-dirichlet",
    "href": "qmd/distributions.html#sec-distr-dirichlet",
    "title": "Distributions",
    "section": "Dirichlet",
    "text": "Dirichlet\n\nA family of continuous multivariate probability distributions parameterized by a vector Œ± of positive reals",
    "crumbs": [
      "Distributions"
    ]
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-exp",
    "href": "qmd/distributions.html#sec-distr-exp",
    "title": "Distributions",
    "section": "Exponential",
    "text": "Exponential\n\n\nNotes from\n\nStatistical Rethinking &gt;&gt; Chapter 10\n\nConstrained to be zero or positive\nFundamental distribution of distance and duration, kinds of measurements that represent displacement from some point of reference, either in time or space.\nIf the probability of an event is constant in time or across space, then the distribution of events tends towards exponential.\nIts shape is described by a single parameter, the rate of events \\(\\lambda\\), or the average displacement \\(\\lambda ‚àí1\\) .\nThis distribution is the core of survival and event history analysis",
    "crumbs": [
      "Distributions"
    ]
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-gamma",
    "href": "qmd/distributions.html#sec-distr-gamma",
    "title": "Distributions",
    "section": "Gamma",
    "text": "Gamma\n\n\nNotes from\n\nStatistical Rethinking &gt;&gt; Chapter 10\n\nConstrained to be zero or positive\nLike Exponential but can have a peak above zero\nIf an event can only happen after two or more exponentially distributed events happen, the resulting waiting times will be gamma distributed.\n\ne.g.¬†age of cancer onset is approximately gamma distributed, since multiple events are necessary for onset.\n\nThe gamma can be viewed as the sum of iid n exponential random variables. Exponential random variables have a rate parameter, so it makes sense for the Gamma to inherit a rate parameterization. The rate parameter also happens to be related to a scale parameter, so it makes sense for the Gamma to have a scale parameterization.\nShape parameter \\(k\\) and a scale parameter \\(\\theta\\)\n\\(\\mathbb{E}[X] = k\\theta = \\frac{\\alpha}{\\beta}\\)\n\nShape parameter \\(\\alpha = k\\) and an\nInverse Scale parameter (aka Rate Parameter) \\(\\beta = \\frac {1}{\\theta}\\)\nTherefore if you want a gamma distributions with a certain ‚Äúmean‚Äù and ‚Äústandard deviation,‚Äù you‚Äôd:\n\nSet your mean to \\(\\mathbb{E}[X]\\), your standard deviation to \\(\\theta\\) (probably but maybe it‚Äôs \\(\\beta\\))\nCalculate \\(\\beta\\)\nCalculate \\(\\alpha\\)\nprior(gamma(alpha, beta))\n\n\nExample: Gamma distribution as the sums of random exponential variables\n\nn &lt;- 12\nbeta &lt;- 1.2\n\nrvs &lt;- replicate(1000, {\n  sum(rexp(n, beta))\n})\n\nhist(rvs, freq = F)\ncurve(dgamma(x, shape = n, rate = beta), col='red', add=T)\n\nGamma distribution density overlayed with a histogram of exponential variable sums\n\nUsed in Survival Regression",
    "crumbs": [
      "Distributions"
    ]
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-gauss",
    "href": "qmd/distributions.html#sec-distr-gauss",
    "title": "Distributions",
    "section": "Gaussian",
    "text": "Gaussian\n\nSpecial case of Student‚Äôs t-distribution with the \\(\\nu\\) parameter (i.e.¬†degree of freedom) set to infinity.",
    "crumbs": [
      "Distributions"
    ]
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-gumb",
    "href": "qmd/distributions.html#sec-distr-gumb",
    "title": "Distributions",
    "section": "Gumbel",
    "text": "Gumbel\n\n\nKnown as the type-I generalized extreme value distribution\n\nEVT says it is likely to be useful if the distribution of the underlying sample data is of the normal or exponential type.\n\nUsed to model the distribution of the maximum (or the minimum) of a number of samples of various distributions.\n\nTo model minimums, use the negative of the original data.\n\nUse Cases\n\nRepresent the distribution of the maximum level of a river in a particular year if there was a list of maximum values for the past ten years.\nPredicting the chance that an extreme earthquake, flood or other natural disaster will occur.\nDistribution of the residuals in Multinomial Logit and Nested Logit models\n\nParameters\n\nGumbel(\\(\\mu, \\beta\\)) (location, scale)\nMean: \\(\\mu + \\beta\\gamma\\) where \\(\\gamma\\) is Euler‚Äôs constant (\\(\\approx\\) 0.5772)\nMedian: \\(\\mu - \\beta \\ln(\\ln(2))\\)\nMode: \\(\\mu\\)\nVariance: \\(\\frac{\\pi^2}{6}\\beta^2\\)\nStandard Gumbel: When \\(\\mu = 0\\), mean = \\(\\gamma\\), median = \\(-\\ln(\\ln(2)) \\approx 0.3665\\) and the standard deviation = \\(\\pi/\\sqrt{6}\\)",
    "crumbs": [
      "Distributions"
    ]
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-multgauss",
    "href": "qmd/distributions.html#sec-distr-multgauss",
    "title": "Distributions",
    "section": "Multivariate Gaussian",
    "text": "Multivariate Gaussian\n\nIf the random variable components in the vector are not normally distributed themselves, the result is not multivariate normally distributed.\nVariance-Covariance matrix must be semi-definite and therefore symmetric\n\nExample of not symmetric for two random variables",
    "crumbs": [
      "Distributions"
    ]
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-pareto",
    "href": "qmd/distributions.html#sec-distr-pareto",
    "title": "Distributions",
    "section": "Pareto",
    "text": "Pareto\n\nAlso see Extreme Value Theory &gt;&gt; Distribution Tail Classification\n‚ÄúGaussian distributions tend to prevail when events are completely independent of each other. As soon as you introduce the assumption of interdependence across events, Paretian distributions tend to surface because positive feedback loops tend to amplify small initial events.‚Äù\nPareto has similar relationship with the exponential distribution as lognormal does with normal \\[\nY_{exp} = \\log \\frac {X_{pareto}} {x_m}\n\\]\n\nWhere \\(X_{pareto} = x_m e^{Y_{\\text{exp}}}\\)\n\n\\(x_m\\) is the (positive) minimum of the randomly distributed pareto variable, X that has index Œ±\n\\(Y_{exp}\\) is exponentially distributed with rate \\(\\alpha\\)\n\n\nSome theoretical statistical moments may not exist\n\nIf the theoretical moments do not exist, then calculating the sample moments is useless\nExample: Pareto (\\(\\alpha\\) = 1.5) has a finite mean and an infinite variance\n\nNeed \\(\\alpha &gt; 2\\) for a finite variance\nNeed \\(\\alpha &gt; 1\\) for a finite mean\nIn general you need \\(\\alpha &gt; p\\) for the pth moment to exist\nIf the nth moment is not finite, then the (n+1)th moment is not finite.\n\n\nFat Tails \\[\n\\bar{F} = x^{-\\alpha} L(x)\n\\]\n\n\\(L(x)\\) is just characterized as slowly varying function that gets dominated by the decaying inverse power law element, \\(x-\\alpha\\). as \\(x\\) goes to infinity\n\n\\(\\alpha\\) is a shape parameter, aka ‚Äútail index‚Äù aka ‚ÄúPareto index‚Äù",
    "crumbs": [
      "Distributions"
    ]
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-poisson",
    "href": "qmd/distributions.html#sec-distr-poisson",
    "title": "Distributions",
    "section": "Poisson",
    "text": "Poisson\n\nObtained as the limit of the binomial distribution when the number of attempts is high and the success probability low. Or the Poisson distribution can be approximated by a normal distribution when Œª is large\nProbability Mass Function \\[\n\\text{Pr}(Y = y) = f(y; \\lambda) = \\frac {e^{-\\lambda} \\cdot \\lambda^y} {y!}\n\\]\n\n\\(\\mathbb{E}[Y] = \\text{Var}(Y) = \\lambda\\)\n\n{distributions3}\n\nStats\nY &lt;- Poisson(lambda = 1.5) \nprint(Y) \n## [1] \"Poisson distribution (lambda = 1.5)\"\n\nmean(Y) \n## [1] 1.5 \nvariance(Y) \n## [1] 1.5 \npdf(Y, 0:5) \n## [1] 0.22313 0.33470 0.25102 0.12551 0.04707 0.01412 \ncdf(Y, 0:5) \n## [1] 0.2231 0.5578 0.8088 0.9344 0.9814 0.9955 \nquantile(Y, c(0.1, 0.5, 0.9)) \n## [1] 0 1 3 \nset.seed(0) \nrandom(Y, 5) \n## [1] 3 1 1 2 3\n\nVisualize\n\nplot(Poisson(0.5), main = expression(lambda == 0.5), xlim = c(0, 15)) \nplot(Poisson(2),   main = expression(lambda == 2),   xlim = c(0, 15)) \nplot(Poisson(5),   main = expression(lambda == 5),   xlim = c(0, 15)) \nplot(Poisson(10),  main = expression(lambda == 10),  xlim = c(0, 15))",
    "crumbs": [
      "Distributions"
    ]
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-studt",
    "href": "qmd/distributions.html#sec-distr-studt",
    "title": "Distributions",
    "section": "Student‚Äôs t-distribution",
    "text": "Student‚Äôs t-distribution\n\nStandard Deviation\n\\[\n\\text{sd} = \\sqrt {\\frac {\\nu} {\\nu - 2}}\n\\]\n\n\\(\\nu\\) = degrees of freedom\n\nWhen ŒΩ is small, the Student‚Äôs t-distribution is more robust to multivariate outliers\nThe smaller the degree of freedom, the more ‚Äúheavy-tailed‚Äù it is\n\n\n-3 on the y-axis says that the probability of being in the tail is 1 in 103\n\nDon‚Äôt pay attention to the x-axis. Just note how much the probability of being in the tail gets larger as the dof get smaller\n\nAs the degrees of freedom goes to 1, the t distribution goes to the Cauchy distribution\nAs the degrees of freedom goes to infinity, it goes to the Normal distribution.",
    "crumbs": [
      "Distributions"
    ]
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-tri",
    "href": "qmd/distributions.html#sec-distr-tri",
    "title": "Distributions",
    "section": "Triangular",
    "text": "Triangular\n\nTriangle shaped distribution\nUseful when you have a known min and max value\nextraDistr::rtriang(n, a, b, c) %\\&gt;% hist()\n\n# Discrete distribution\nextraDistr::rtriang(n, a, b, c) %\\&gt;% round() \\`\\`\\`\n\nn is the number of random values you wish to draw\na is the min value\nb is the max value\nc is the mode\n\nCan use to adjust the skew of the distribution\n\n\n\n\n\n\nWhere k is the number of events in n trials\nWhere \\(\\theta\\) is the probability of an event",
    "crumbs": [
      "Distributions"
    ]
  },
  {
    "objectID": "qmd/mathematics-glossary.html",
    "href": "qmd/mathematics-glossary.html",
    "title": "Glossary",
    "section": "",
    "text": "A priori - a type of knowledge that can be derived by reason alone\n\nA priori analyses are performed as part of the research planning process.\n\nA posteriori - a type of knowledge that expresses an empirical fact unknowable by reason alone.\n\nSame as post-hoc. Post-Hoc analysis is conducted after the experiment.\n\nbias - see unbiased estimator\nceteris paribus - latin for ‚Äúall things being equal‚Äù or ‚Äúother things held constant.‚Äù\nclosed form - a mathematical expression that uses a finite number of standard operations. It may contain constants, variables, certain well-known operations, and functions, but usually no limit, differentiation, or integration.\nconsistency - Requires that the outcome of the procedure with unlimited data should identify the underlying truth. Usage is restricted to cases where essentially the same procedure can be applied to any number of data items. In complicated applications of statistics, there may be several ways in which the number of data items may grow. For example, records for rainfall within an area might increase in three ways: records for additional time periods; records for additional sites with a fixed area; records for extra sites obtained by extending the size of the area. In such cases, the property of consistency may be limited to one or more of the possible ways a sample size can grow\ndegrees of freedom - When discussed about variable-sample size tradeoff, usually means n-p, where is the number of rows and p is the number of variables. The more variables used in the model the fewer degrees of freedom and therefore less power and precision.\nexchangeability - means we can swap around, or reorder, variables in the sequence without changing their joint distribution.\n\nEvery IID (independent, identically distributed) sequence is exchangeable - but not the other way around. Every exchangeable sequence is identically distributed, though\n\nExample: If you draw a sequence of red and blue marbles from a bag without replacement, the sample is exchangeable but not independent. e.g.¬†drawing a red marble affects the probability of drawing a red or blue marble next.\n\n\nefficiency - A test, estimator, etc. is more efficient than another test, estimator, etc. if it requires fewer observation to obtain the same level of performance.\nergodicity - the idea that a point of a moving system, either a dynamical system or a stochastic process, will eventually visit all parts of the space that the system moves in, in a uniform and random sense\nexternal validity - Our estimates are externally valid if inferences and conclusions can be generalized from the population and setting studied to other populations and settings. (also see internal validity)\nidentifiable (aka point-indentifiable) - theoretically possible to learn the true values of this model‚Äôs underlying parameters after obtaining an infinite number of observations from it (see non-identifiability, partially-indentifiable)\nill-conditioned - In SVD decomposition, when there‚Äôs a huge difference between largest and smallest eigenvalue of¬†the original matrix, A, the ratio of which is called condition number.\ninternal validity - our estimates are internally valid if statistical inferences about causal effects are valid for the population being studied. (also see external validity)\nintractable - problems for which there exist no efficient algorithms to solve them. Most intractable problems have an algorithm ‚Äì the same algorithm ‚Äì that provides a solution, and that algorithm is the brute-force search\nlocality - effects have causes and chains of cause and effect must be unbroken in space and time (not the case in ‚Äòentanglement‚Äô)\nmarginalization - The process of eliminating one or more variables from a joint probability distribution or a multivariate statistical model to obtain the distribution or model for a subset of variables. The resulting distribution or model is called a marginal distribution or marginal model. It allows you to focus on the behavior of specific variables while considering the uncertainty associated with others.\n\nFor example, marginalizing over a joint distribution (i.e.¬†many variables) gets you a marginal distribution (i.e.¬†fewer variables). In other words, if you have a joint probability distribution for two variables \\(X\\) and \\(Y\\), the marginal distribution of \\(X\\) is obtained by summing or integrating over all possible values of \\(Y\\). Similarly, the marginal distribution of \\(Y\\) is obtained by summing or integrating over all possible values of \\(X\\).\nNotation: \\(P(X) = \\sum_Y P(X,Y) \\;\\text{or}\\; P(X) = \\int P(X,Y)\\;dY\\)\nOnce you have to the marginal distribution, this allows you compute conditional distributions. For example, after obtaining the marginal distribution, \\(P(X,Y)\\), from the joint distribution, \\(P(X,Y,Z)\\), you can compute the conditional distributions, \\(P(X|Y)\\) and \\(P(Y|X)\\).\nThe uncertainty associated with \\(Z\\) is indirectly considered in the sense that the marginal distribution \\(P(X,Y)\\) accounts for all possible values of \\(Z\\) by integrating over them. However, \\(P(X,Y)\\) itself doesn‚Äôt provide explicit information about the uncertainty associated with \\(Z\\).\n\nnon-identifiability - the structure of the data and model do not make it possible to estimate the parameter‚Äôs value. Multicollinearity is a type of non-identifiability problem. (i.e.¬†two or more parametrizations of the model are observationally equivalent) (see identifiable, partially-indentifiable)\noverdetermined system - In linear regression, when there are more observations than features, n &gt; p\npartial coefficient - The coefficient of a variable in a multivariable regression. In a simple regression, the coefficient of the variable is just called the ‚Äúregression coefficient.‚Äù\npartially-indentifiable (aka set identifiable) - non-identifiable but possible to learn the true values of a certain subset of the model parameters\nrobust - a ‚Äúrobust‚Äù estimator in statistics is one that is insensitive to outliers, whereas a ‚Äúrobust‚Äù estimator in econometrics is insensitive to heteroskedasticity and autocorrelation (hyndman)\nsupport (aka range) - the set of values that the random variable can take.\n\nFor discrete random variables, it is the set of all the realizations that have a strictly positive probability of being observed.\nFor continuous random variables, it is the set of all numbers whose probability density is strictly positive.\nSee link for examples\n\nunderspecification - In general, the solution to a problem is underspecified if there are many distinct solutions that solve the problem equivalently.\nAn unbiased estimator is an accurate statistic that‚Äôs used to approximate a population parameter.\n\n‚ÄúAccurate‚Äù in this sense means that it‚Äôs neither an overestimate nor an underestimate. If an overestimate or underestimate does happen, the mean of the difference is called a ‚Äúbias.‚Äù\n\nWeak Law of Large Numbers (Bernoulli‚Äôs theorem) - states that if you have a sample of independent and identically distributed random variables, as the sample size grows larger, the sample mean will tend toward the population mean",
    "crumbs": [
      "Mathematics",
      "Glossary"
    ]
  },
  {
    "objectID": "qmd/git-general.html",
    "href": "qmd/git-general.html",
    "title": "22¬† General",
    "section": "",
    "text": "22.1 Misc",
    "crumbs": [
      "Git",
      "<span class='chapter-number'>22</span>¬† <span class='chapter-title'>General</span>"
    ]
  },
  {
    "objectID": "qmd/git-general.html#misc",
    "href": "qmd/git-general.html#misc",
    "title": "22¬† General",
    "section": "",
    "text": "b0rk Cheatsheet\n\nView HTML file in browser\n\nSyntax: ‚Äúhttps://raw.githack.com/&lt;acct name&gt;/&lt;repo name&gt;/&lt;branch name&gt;/&lt;directory name&gt;/&lt;file name&gt;.html‚Äù\n\nInstalling from a git repo (From link)\n\nMake a fork of the repo and then clone it to your local machine.\nTo update, after setting an upstream remote (git remote add upstream git://github.com/benfulcher/hctsa.git) you can use git pull upstream main.\nTo update the submodule in the repo, git submodule update --init\n\nStart R project and Git repo in whichever order (I think)\n\nCreate R project in RStudio\n\nChoose ‚ÄúNew Directory‚Äù for all the templated projects (e.g.¬†quarto book, shiny, etc.). None of the other choices have them.\n\nIf you‚Äôve already created a directory, it will NOT overwrite this directory or add to it. So you‚Äôll either have alter the name of your old directory or choose a new name.\n\n\nCreate repo on Github\n\nAdd license and readme\n\nDo work\nTools &gt;&gt; Version Control &gt;&gt; Project Set-up &gt;&gt; Version Control System &gt;&gt; Select Git\nOpen terminal and go to working directory of project\ngit checkout -B main\ngit pull origin main --allow-unrelated-histories\ngit add .\ngit commit -m \"initial commit\"\ngit push --set-upstream origin main \n\nTurn off ‚ÄúLF will be replaced by CRLF the next time Git touches it‚Äù\n\nMessage spams terminal when committing changes from a window machines. Has to do with line endings in windows vs unix.\nTurn off: git config core.autocrlf true\nSee SO post for more details\n\nURL format to download files from repositories\n\nhttps://raw.githubusercontent.com/user/repository/branch/filename\n\n# Or evidently this way works too\n# adds ?raw=true to the end of the url\nfeat_all_url &lt;- url(\"https://github.com/notast/hierarchical-forecasting/blob/main/3feat_all.RData?raw=true\")\nload(feat_all_url)\nclose(feat_all_url)\nGet filelist from repo and download to a directory\n\n** Directory urls change as commits are made **\n\nlibrary(httr)\n\n# example: get url for the data dir of covidcast repo\nreq &lt;- httr::GET(\"https://api.github.com/repos/ercbk/Indiana-COVIDcast-Dashboard/git/trees/master?recursive=1\") %&gt;%¬†\n¬† httr::content()\n# alphabetical order\ntrees &lt;- req$tree %&gt;%¬†\n¬† map(., ~pluck(.x, 1)) %&gt;%¬†\n¬† as.character()\n# returns 20 which is first instance, so 19 should the \"data\" folder\ndetect_index(trees, ~str_detect(., \"data/\"))\n# url for data dir\nreq$tree[[19]]$url\n\n# example\n# Get all the file paths from a repo\nreq &lt;- GET(\"https://api.github.com/repos/etiennebacher/tidytuesday/git/trees/master?recursive=1\")\n# any request errors get printed\nstop_for_status(req)\nfile_paths &lt;- unlist(lapply(content(req)$tree, \"[\", \"path\"), use.names = F)\n# file_path wanted &lt;- filter file path to file you want\n# gets the very last part of the path\nfile_wanted &lt;- basename(file_path_wanted)\norigin &lt;- paste0(\"https://raw.githubusercontent.com/etiennebacher/tidytuesday/master/\", file_wanted)\ndestination &lt;- \"output-path-with-filename-ext\"\n# if file doesn't already exist, download it from repo into destination\nif (!file.exists(destination)) {\n¬†     # if root dir doesn't exist create it\n¬†     if (!file.exists(\"_gallery/img\")) {\n¬† ¬†     dir.create(\"_gallery/img\")\n¬†     }\n¬†     download.file(origin, destination)\nThe insides of .git",
    "crumbs": [
      "Git",
      "<span class='chapter-number'>22</span>¬† <span class='chapter-title'>General</span>"
    ]
  },
  {
    "objectID": "qmd/git-general.html#config-options",
    "href": "qmd/git-general.html#config-options",
    "title": "22¬† General",
    "section": "22.2 Config Options",
    "text": "22.2 Config Options\n\nNotes from: Popular git config options - More options listed that are not presented here.\nSetting Options\n\nAdd via CLI: git config --global &lt;name&gt; &lt;value&gt;\n\nExample: git config --global diff.algorithm histogram\n\nDelete by going into ~/.gitconfig and delete the parameter and value\n\nmerge.conflictstyle diff3 - Provides extra information on merge conflicts\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\ndef parse(input):\n    return input.split(\"\\n\")\n||||||| b9447fc\ndef parse(input):\n    return input.split(\"\\n\\n\")\n=======\ndef parse(text):\n    return text.split(\"\\n\\n\")\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; somebranch\n\nBelow &lt;&lt;&lt;&lt;&lt;&lt; HEAD: This is your local code that you‚Äôre trying to push\nBetween |||||||| b9447fc and =======: This is the original version of the code\nAbove &lt;&lt;&lt;&lt;&lt;&lt; somebranch: This is code from the branch that got merged before yours (I think)\nTherefore, the correct merge conflict resolution is return text.split(\"\\n\"), since that combines the changes from both sides.\n\nmerge.conflictstyle zdiff3 - A newer version of merge.conflictstyle diff3\nA\nB\nC\nD\nE\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; ours\nF\nG\n||||||| base\n# Add More Letters\n=======\nX\nY\nZ\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; theirs\n\nAbove &lt;&lt;&lt;&lt;&lt;&lt; ours: This is the original code plus the code that belongs to the branch that got merged that is not in conflict with your code\nBelow &lt;&lt;&lt;&lt;&lt;&lt; ours: This is the code that is in conflict with the branch (e.g.¬†main) your merging into.\nBelow |||||||| base: This is the code that has been removed from the original code for both mergers\nAbove &lt;&lt;&lt;&lt;&lt;&lt; theirs: This is code for another branch that was merged before yours that is in conflict with your code.\n\npush.default current - Says that when using git push to always push the local branch to a remote branch with the same name.\n\npush.default simple is the default in Git. Means git push only works if your branch is already tracking a remote branch.\nI guess it‚Äôs possible to push a local branch to a remote branch of a different name.\n\ninit.defaultBranch main - Create a main branch instead of a master branch when creating a new repo. I normally do this on Github.\ncommit.verbose true - This adds the whole commit diff in the text editor where you‚Äôre writing your commit message, to help you remember what you were doing.\nrerere.enabled true - This enables rerere (‚Äùreuse recovered resolution‚Äù), which remembers how you resolved merge conflicts during a git rebase and automatically resolves conflicts for you when it can.\ncore.pager delta - The ‚Äúpager‚Äù is what git uses to display the output of git diff, git log, git show, etc.\n\nValues:\n\ndelta: A fancy diff viewing tool with syntax highlighting\nless -x5,9 - Sets tabstops, which I guess helps if you have a lot of files with tabs in them?\nless -F -X - Not sure about this one, -F seems to disable the pager if everything fits on one screen if but her git seems to do that already anyway\ncat - To disable paging altogether\n\nDelta also suggests that you set up interactive.diffFilter delta ‚Äìcolor-only to syntax highlight code when you run git add -p.\n\ndiff.algorithm histogram - Improves the Patience algorithm for presenting diffs. See link in article for more details.\n\nDefault (I think the default algorithm is Myers.)\n-.header {\n+.footer {\n     margin: 0;\n }\n\n-.footer {\n+.header {\n     margin: 0;\n+    color: green;\n }\n\nfooter didn‚Äôt actually have margin: 0 and color: green in the original code like this diff makes it seem. In reality, the two rules have switched order with header gaining the additional property, color: green.\n\nHistogram\n-.header {\n-    margin: 0;\n-}\n-\n .footer {\n     margin: 0;\n }\n\n+.header {\n+    margin: 0;\n+    color: green;\n+}\n\nThis shows header‚Äôs old rule without color: green at the top and being removed. footer is accurately depicted as unchanged. Then, it shows header with the addtional property, color: green, added below footer.\n\n\nincludeIf - Allows you to use different options depending which directory your project is in.\n\nExample: Use this config file only if you‚Äôre in the ‚Äúwork‚Äù directory\n[includeIf \"gitdir:~/code/&lt;work&gt;/\"]\n    path = \"~/code/&lt;work&gt;/.gitconfig\"\n\nGood if, for example, you want to have a work email set for work repos and personal email for set for personal repos\n\n\ninsteadOf - Useful to correct little mistakes often you make\n\nSee article for other usecases\nExample: If you accidently clone using http when you want to use SSH\n[url \"git@github.com:\"]\n    insteadOf = \"https://github.com/\"\n\nNow when you accidently clone a repo using the http address, it‚Äôll change it to the ssh address in .git/config. Now you‚Äôll be using ssh to push changes which is more secure.\n\n\nSubmodules\nstatus.submoduleSummary true\ndiff.submodule log\nsubmodule.recurse true\n\nSee thread for details\nThe top two ‚Äúmake git status and git diff display some more useful information on how things differ in submodules.‚Äù\nThe bottom one aids in the updating of submodules when switching branches\n\ndiff.colorMoved default - Uses different colours to highlight lines in diffs that have been ‚Äúmoved‚Äù\n\ndiff.colorMovedWS allow-indentation-change - With diff.colorMoved set, also ignores indentation changes\n\ngpg.format ssh - Allows you to sign commits with SSH keys\nmerge.tool meld (or nvim, or nvimdiff) - Enables use git mergetool to help resolve merge conflicts",
    "crumbs": [
      "Git",
      "<span class='chapter-number'>22</span>¬† <span class='chapter-title'>General</span>"
    ]
  },
  {
    "objectID": "qmd/git-general.html#optimizations",
    "href": "qmd/git-general.html#optimizations",
    "title": "22¬† General",
    "section": "22.3 Optimizations",
    "text": "22.3 Optimizations\n\nFor large repos, simple actions, like running git status or adding new commits can take many seconds. Cloning repos can take many hours.\nBenefits\n\nIt improves the overall performance of your development workflow, allowing you to work more efficiently. This is especially important when working with large organizations and open source projects, where multiple developers are constantly committing changes to the same repository. A faster repository means less time waiting for Git commands such as git clone or git push to finish. It helps to optimize the storage space, as large files are replaced by pointers which take up less space. This can help avoid storage issues, especially when working with remote servers.\n\nMisc\n\nSee How to Improve Performance in Git: The Complete Guide\n\nExplainer, config settings, advanced gc, checkout, and clone commands\n\n\nUse .gitignore\n\nGenerated files, like cache or build files\n\nThey will be modified at each different generation ‚Äî and there‚Äôs no need to keep track of those changes.\n\nThird-party libraries\n\nInstead, aim for a list of the required dependencies (and the correct version) so that everyone can download and install them whenever the repo is cloned.\n\nFor example, with a package.json file for JavaScript projects you can (and should) exclude the /node_modules folder.\n.DS_Store files (which are automatically created by macOS) are another good candidate\n\n\n\nGit LFS\n\nDesigned specifically to handle large file versioning. LFS saves your local repositories from becoming unnecessarily big, preventing you from downloading unnessary data.\n\nGit LFS intercepts any large files and sends them to a separate server, leaving a smaller pointer file in the repository that links to the actual asset on the Git LFS server.\n\nThis is an extension to the standard Git feature set, so you will need to make sure that your code hosting provider supports it (all the popular ones do).\nAlso need to download and install the CLI extension on your machine before installing it in your repository.\nSet-Up\n$ git lfs install\n$ git lfs track \"*.wav\"\n$ git lfs track \"images/*.psd\"\n$ git lfs track \"videos\"\n$ git add .gitattributes\n\nTells Git LFS which file extensions it should manage.\n.gitattributes notes the file names and patterns in this text file and, just like any other change, it should be staged and committed to the repository.\nCan now add files and commit as normal\nList all file extensions being tracked: git lfs track\nList all files being managed: git lfs ls-files\n\n\nDon‚Äôt download the version history if you don‚Äôt need to\n\ngit clone ‚Äìdepth 1 gitj@github.com:name/repo.git",
    "crumbs": [
      "Git",
      "<span class='chapter-number'>22</span>¬† <span class='chapter-title'>General</span>"
    ]
  },
  {
    "objectID": "qmd/git-general.html#troubleshooting",
    "href": "qmd/git-general.html#troubleshooting",
    "title": "22¬† General",
    "section": "22.4 Troubleshooting",
    "text": "22.4 Troubleshooting\n\nDiverged Branches\n\n\nKeeps asking for username/password when pushing\n\nSolution: You (or if you used usethis::use_github/git) probably set-up a https connection when you need a ssh connection.\n\nsee https://docs.github.com/en/get-started/getting-started-with-git/managing-remote-repositories#changing-a-remote-repositorys-url to change from https to ssh.\n\n\nUndo a commit, but save changes made (e.g.¬†you forgot to pull before you pushed)\n\nSteps\n\ngit log - Shows commit history. Copy the hash for your last commit\ngit diff &lt;last commit hash&gt; &gt; patch - save the diff of the latest commit to a file\ngit reset --hard HEAD^ to revert to the previous commit\n\n**After this, your changes will be lost locally **\n\ngit log - confirm that you are now at the previous commit\ngit pull - correct the mistake you made in first place\npatch -p1 &lt; patch - apply the changes you originally made\ngit diff - to confirm that the changes have been reapplied\nNow, you do the regular commit, push routine\n\n\nUndo uncommitted changes: git stash followed by git stash drop\n\n‚Äúbut only use if you commit often‚Äù - guessing this is not good if your commit is somehow large and/or involves multiple files\n\nSearch commits by string: git log --grep &lt;string&gt;\nPinpoint bugs in your commit history\n\nInstead of sequentially searching each previous commit to look for the bad commit, git bisect helps you perform a bisect search for the commit which saves time.\nScenario: A bug is introduced in a codebase, but it is not discovered until later. The feature used to work, but now, it does not. The feature was definitely known to work 3 weeks ago.\nManual Workflow\n\nMake sure you‚Äôre in the current commit that‚Äôs bad and start git bisect\ngit bisect start\n1git bisect bad\n2git log --before=\"3 weeks\"\n3git checkout 3348b0\n\n1\n\nThis labels the current commit as bad (i.e.¬†bug is present)\n\n2\n\nThis lists every commit for last 3 weeks\n\n3\n\nSwitch to the commit that‚Äôs the version of the project that was 3 weeks ago when supposedly the feature was working. The first commit listed (i.e.¬†top) will be the commit closest to 3 weeks ago ‚Äî with older commits below it. You only need to use the first 6 or so digits of the commit hash.\n\n\nRecompile code and test commit for bug\ndevtools::load_all()\n\nload_all will recompile your package using this current version‚Äôs code\nAfter recompiling code, use your reproducible examplet to see if the bug is present in this version\nIf the bug is stil present, then go to the next older commit and repeat process. Keep loading older commits until you find one that doesn‚Äôt have the bug.\n\nIf ths is the case and assuming you don‚Äôt have to go back too much further to find a ‚Äúgood‚Äù commit, then you can stop here since you‚Äôll have found the bad commit that introduced the bug.\nIf you don‚Äôt find a good commit around this time period, then quit the current git bisect session using git bisect reset and choose whichever commit you stop at as the new starting point for a new git bisect session and repeat this whole workflow.\n\n\nGo to terminal and mark this commit as good\ngit bisect good\n\nGit will automatically switch you to commit that‚Äôs the midway point between the ‚Äústart‚Äù commit and the commit you labeled as ‚Äúgood.‚Äù\nIt tells you how many commits that are currently between you and the ‚Äústart‚Äù commit which is the same amount as between this midway commit and the commit you labelled as ‚Äúgood.\nIt also tells you how many more bisections (‚Äústeps‚Äù) you‚Äôll have to go through to find the commit resposible for the bug.\n\nRepeat Step 2 and test verstion for the bug. Then label commit as good or bad\ngit bisect bad\n\nAfterwards, git will automatically checkout to the commit that is either midway between this commit and ‚Äústart‚Äù or the end commit based on whether you label this current commit as good or bad.\n\nContinue labelling commits until git‚Äôs message is ‚Äú&lt;some commit hash&gt; is the first bad commit.‚Äù\n\nGit will also show you the commit message and a list of files that were changed.\n\nUse git show &lt;commit hash&gt; to see the diff\n(Optional) Use git bisect log &gt; file-name to save the session to a file.\nUse git bisect reset to exit and return you to where you were at the start of this workflow (HEAD)\n\nAutomatic Workflow\n\nWrite script that includes you reproducible exaample and have it return an error code of 0 if it does not contain the bug or return an non-zero error if it does contain the bug.\n\nExample\ndevtools::load_all()\n\nif (nr != nrow(df)) {\n  stop(\"error\")\n}\n\nload_allwill recompile your package using this current version‚Äôs code\nReturns non-zero error code if condition is not triggered (i.e.¬†False) and a 0 error code when the condition is triggered (i.e.¬†True).\nCould also use stopifnot here.\n\n\n(Optional) If you already know the commit hash of commit from 3 weeks ago and that does not have the bug, you can bypass the step 3.\n# Make sure you're in the current commit that's got the bug\ngit bisect start\n1git bisect bad\n2git bisect good 3348b0\n\n1\n\nThis labels the current commit as bad (i.e.¬†bug is present)\n\n2\n\nThis labels the commit from 3 weeks ago that you know doesn‚Äôt have the bug\n\n\nDo steps 1, 2, and 3 of the Manual Workflow\nRun auto-bisect\ngit bisect run Rscript test.R\n\ntest.R is the script from step 1 that determines whether the version (i.e.¬†commit hash) of your code has the bug.\nThis run through all the steps of the Manual Workflow and determine with the version of the code is ‚Äúgood‚Äù or ‚Äúbad‚Äù by whether the script returns an error code of zero or non-zero.\n\nRead final message to get the commit hash with bug in it.\n\nMessage will be ‚Äú&lt;some commit hash&gt; is the first bad commit.‚Äù\nGit will also show you the commit message and a list of files that were changed.\n\nSee steps 6, 7, and 8 of the Manual Workflow",
    "crumbs": [
      "Git",
      "<span class='chapter-number'>22</span>¬† <span class='chapter-title'>General</span>"
    ]
  },
  {
    "objectID": "qmd/git-general.html#pulling",
    "href": "qmd/git-general.html#pulling",
    "title": "22¬† General",
    "section": "22.5 Pulling",
    "text": "22.5 Pulling\n\nSave your changes, pull in an update, apply your changes\ngit stash\ngit pull\ngit stash pop\n\ngit stash pop throws away the (topmost, by default) stash after applying it, whereas\ngit stash apply leaves it in the stash list for possible later reuse (or you can then git stash drop it).\n\nRe potential merge conflicts\n\n‚ÄúFor instance, say your stashed changes conflict with other changes that you‚Äôve made since you first created the stash. Both pop and apply will helpfully trigger merge conflict resolution mode, allowing you to nicely resolve such conflicts‚Ä¶ and neither will get rid of the stash, even though perhaps you‚Äôre expecting pop too. Since a lot of people expect stashes to just be a simple stack, this often leads to them popping the same stash accidentally later because they thought it was gone.‚Äù\n\nPulling is fetching + merging\n\nFetching just gets the info about the commits made to the remote repo\ngit fetch origin\nSome technical discussion for always using git pull ‚Äìff\n\nhttps://blog.sffc.xyz/post/185195398930/why-you-should-use-git-pull-ff-only-git-is-a\nhttps://megakemp.com/2019/03/20/the-case-for-pull-rebase/\nit‚Äôs still confusing but pull rebase sounds fine to me\n‚Äìglobal tag says do it for all my repos\nnot sure what the true and only are for\n\ngit pull ‚Äìhelp will open doc in browser\n\n\nPulling by rebase\n\nLocal: using this method as default\ngit config pull.rebase true\ngit pull\nRemote\ngit pull --rebase\n\nPulling by fast-forward\n\nLocal: using this method as default\ngit config --global pull.ff only\ngit pull\nRemote\ngit pull --ff",
    "crumbs": [
      "Git",
      "<span class='chapter-number'>22</span>¬† <span class='chapter-title'>General</span>"
    ]
  },
  {
    "objectID": "qmd/git-general.html#branching",
    "href": "qmd/git-general.html#branching",
    "title": "22¬† General",
    "section": "22.6 Branching",
    "text": "22.6 Branching\n\nMisc\n\nCreate a new branch for each ticket you are working on or each data model. It can get sloppy when you put all your code changes on one branch.\nHEAD\n\nDetached HEAD\n\n\nCreate a branch (e.g.¬†‚Äútesting‚Äù)\ngit branch testing\nWork in a branch\ngit checkout testing\nThe files in your working directory change to the version saved in that branch\nIt adds, removes, and modifies files automatically to make sure your working copy is what the branch looked like on your last commit to it.\nCreate and work in a branch\n# new way\ngit switch -c testing\nor\ngit checkout -b testing\nor\ngit branch testing\ngit checkout testing\ncreates the branch and switches you to working in that branch\nIf you did a bunch of changes in a codebase, only to realize that you‚Äôre working on `master`,¬† switch will bring those local changes with you to the new branch. So I guess they won‚Äôt affect master then.\n\nUnless If you already committed to main, then those changes are both in your new branch and in main. So you would still have to clean up the main branch.\n\nDeleting a branch\n\nlocal branch\ngit branch -d testing\n\nremote branch\ngit push &lt;remoteName&gt; --delete &lt;branchName&gt;\nSee existing branches\ngit branch\nSee what has been commited the remote repo branches\ngit fetch origin\ngit branch -vv\n‚Äúorigin‚Äù is the name of the remote\nresult\ntesting¬† ¬† 7e424c3 [origin/testing: ahead 2, behind 1] change abc¬†\nmaster¬† ¬† ¬† 1ae2a45 [origin/master] Deploy index fix\n* issue¬† ¬† f8674d9 [origin/issue: behind 1] should do it¬† ¬† ¬† ¬† ¬†\ncart¬† ¬† ¬† ¬† 5ea463a Try something new\nformat: branch, last commit sha-1, local branch status vs remote branch status, commit message\nthe star indicates the HEAD pointer‚Äôs location (where you‚Äôre at, i.e.¬†checkout)\ntesting branch\n\n‚Äúahead 2‚Äù means¬† I committed twice to the local testing branch¬†and this work has not been pushed to the remote testing branch repo yet.\n‚Äúbehind 1‚Äù means someone has pushed a commit to the remote testing branch repo and we haven‚Äôt merged this work to our local testing branch\n\nGet the last 10 branches that you‚Äôve committed to locally:\ngit branch --sort=-committerdate | head -n 10\nRename branch\n# change locally\ngit branch --move &lt;bad-branch-name&gt; &lt;corrected-branch-name&gt;\n# change remotely in repo\ngit push --set-upstream origin &lt;corrected-branch-name&gt;\n# confirm change\ngit branch --all\nHEAD determines to which branch new commits are added\n\nExample\n\n‚Äútesting‚Äù branch is created (not shown in above picture)\n\nHEAD points at ‚Äúmaster‚Äù branch\n‚Äúmaster‚Äù branch and the new ‚Äútesting‚Äù branch both point at commit, f30ab.\nf30ab commit points to previous commit 34ac2\n\nuser executes checkout to ‚Äútesting‚Äù branch (not shown in picture)\n\nHEAD now points to testing branch\n\nuser commits 87ab2 (shown in pic)\n\n87ab2 is committed to the ‚Äútesting‚Äù branch\n‚Äútesting‚Äù branch is now ahead of the ‚Äúmaster‚Äù branch by 1 commit\n\n\nExample\n\nEverything above happens but now another user commits the master branch.\n\nBoth branches are in conflict. The testing branch is ahead and behind by 1 commit\n\n\n\nMerging\n\n\nNotes\n\nNEVER merge your branch locally on your machine with the master branch, ALWAYS merge online via pull request\n\nSteps\n\nPush final changes and use of a pull request\nSwitch to master branch locally and pull the merged changes\n\n\n\nUpdate branch with work that‚Äôs been done in master branch\n\nAfter updating your local branch, push to remote repo (no commit necessary)\n# while in branch\ngit merge master\n\n\nFast-Forward\n\nExample\n\nBefore the merge\n\nthe testing branch is 1 commit ahead of the master branch and the master branch doesnt have a new commit\n\nAfter the merge\n\nmaster is moved forward to the testing branch commit\n\n\nCode (merging work in branch with the master branch for production)\n# currently in test branch\ngit checkout master\ngit merge testing\n\nLines in file are marked\n# &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD:index.html\n# &lt;div id=\"footer\"&gt;contact : email.support@github.com&lt;/div&gt;\n# =======\n# &lt;div id=\"footer\"&gt;\n# please contact us at support@github.com\n# &lt;/div&gt;\n# &gt;&gt;&gt;&gt;&gt;&gt;&gt; iss53:index.html\nAbove ======= is the master branch version of the code and below is the iss53 branch version\nMake necessary changes and save the file\ngit add . or git add &lt;resolved file&gt;\n\nTells git that conflict is resolved\n\nCheck status to confirm everything has been resolved\ngit status\n\n    On branch master\n    All conflicts fixed but you are still merging.\n      (use \"git commit\" to conclude merge)\n    Changes to be committed:\n      modified:  index.html\ngit commit\n\nno message required (there‚Äôs a default message) but you can add one if you want\n\nExample\n\niss53 branch ahead of master by 2 commits (c3, c5) and behind 1 commit (c2)\nSame code as Fast-Forward merge but git handles the merge a bit differently\ngit checkout master¬†\ngit merge iss53\n\n\n\nC6 (right pic) is called a ‚Äúmerge commit.‚Äù Its created by git and points to two commits instead of one.\nNo need to merge with master (i.e.¬†update local iss53 branch with c4 changes in master) before committing final changes\n\nIf there are changes in the same lines of code C4 and C5, then there will be a conflict (See below, Conflicts &gt;&gt; Example)\n\n\nConflicts\n\nExample\n\nChanged files in C4 (see above example) are in the same lines of the same files that you made changes to in C5\n\nRemember: you‚Äôre now in the master branch since you did checkout master as part of the merge code\nSteps\n\nCheck status to which files are causing the conflict (e.g.¬†index.html)\ngit status\n¬† Unmerged paths:\n¬† (use \"git add &lt;file&gt;...\" to mark resolution)¬†\n¬† ¬† both modified:¬† ¬† ¬† index.html\n\n\n\n\nMoving between branches\n\nfrom master to testing\ngit checkout testing\n\nlocal files are deleted and replaced with branch versions\n\nalternative: worktree\n\nExample\n\nWhat happens when you move from branch-a to branch-b\nBRANCH-A¬† ¬† ¬† ¬† BRANCH-B\nalpha.txt¬† ¬† ¬† alpha.txt\nbravo.txt\ncharlie.txt¬† ¬† charlie.txt\n                delta.txt\n\nbravo text is deleted from your local disc and delta.txt is added\nIf any changes to alpha.txt or charlie.txt have been made and no commit has been made, the checkout will be aborted\n\nSo either revert the changes or commit the changes\n\nUntracked files or newly created files\n\nIf you have branch-A checked out and you create a new file called echo.txt, Git will not touch this file when you checkout branch-B. This way, you can decide that you want to commit echo.txt against branch-B without having to go through the hassle of (1) move the file outside the repo, (2) checkout the correct branch, and (3) move the file back into the repo.",
    "crumbs": [
      "Git",
      "<span class='chapter-number'>22</span>¬† <span class='chapter-title'>General</span>"
    ]
  },
  {
    "objectID": "qmd/git-general.html#collaboration",
    "href": "qmd/git-general.html#collaboration",
    "title": "22¬† General",
    "section": "22.7 Collaboration",
    "text": "22.7 Collaboration\n\nAdd collaborators to your repository\nOne person invites the others and provides them with read/write access (github docs)\n\nSteps\n\nGo to the settings for your repository\nmanage access &gt;&gt; ‚Äúinvite a collaborator‚Äù\n\nSearch for each collaborator by full name, acct name, or email\nClick ‚ÄúAdd &lt;name&gt; to &lt;repo&gt;‚Äù\n\nEach collaborator will need to accept the invitation\n\nSent by email",
    "crumbs": [
      "Git",
      "<span class='chapter-number'>22</span>¬† <span class='chapter-title'>General</span>"
    ]
  },
  {
    "objectID": "qmd/js.html",
    "href": "qmd/js.html",
    "title": "JS",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "JS"
    ]
  },
  {
    "objectID": "qmd/js.html#sec-js-misc",
    "href": "qmd/js.html#sec-js-misc",
    "title": "JS",
    "section": "",
    "text": "Resources\n\nOnline Interactive Cheat Sheet\nLearn Just Enough JavaScript\n\nBasics: variables, objects, arrays, functions, conditionals, loops\n\nHow to run R code in the browser with webR\n\nNice breakdown of generic JS code to run scripts on a webpage\n\nJavaScript for Data Science\n\nhrbmstr: ‚Äújavascript has the advantage over R/Python for both visualization speed ‚Äî thanks to GPU integration ‚Äî and interface creation ‚Äî thanks to the ubiquity of HTML5 ‚Äî means that people will increasingly bring their own data to websites for initial exploration first‚Äù\nconsole.log is the print method",
    "crumbs": [
      "JS"
    ]
  },
  {
    "objectID": "qmd/js.html#sec-js-basics",
    "href": "qmd/js.html#sec-js-basics",
    "title": "JS",
    "section": "Basics",
    "text": "Basics\n\nOperators\n\n// : comments\n... : If you want to copy all the values in your array, and add some new ones, you can use the {‚Ä¶} notation.\n${&lt;code&gt;} : Anything within the${} get ran as code\n\nExample:\n`${b.letter}: ${ (b.frequency*100).toFixed(2) }%`\n\nBackticks indicate it‚Äôs like a glue string or f string (i.e.¬†uses code)\nb.letter and b.frequency are properties in an array\nto.Fixed is a method that rounds the value to to 2 decimal places\nThis was an example of a tooltip, so output would look like ‚ÄúF: 12.23%‚Äù\n\n\n\nVariables\nmyNumber = 10 * 1000\nvariableSetToCodeBlock = {\n¬† const today = new Date();\n¬† return today.getFullYear()\n}\nObject: myObject = ({name: \"Paul\", age: 25})\n\nContained within curly braces, { }\nSubset property, name:\n\nmyObject.name which returns value, Paul\nmyObject[\"name\"] which is useful if you have spaces, etc. in your property names\n\nTypes\n\nMap: Object holds key-value pairs and remembers the original insertion order of the keys\n\ne.g.¬†See Stats &gt;&gt; By Group\nD3 Groups, Rollup, Index Docs\n\n\n\nArrays\n\nList of objects\n\nContained within brackets, [ ]\nEach row is an object and each column is a property of that object and that property has a value associated with it\n\nBasic examples\nmyArray = [1, 2, 3, 4]\nmyArray = [[1, 2], [3, 4]] // arrays within arrays\nmyArray = [1, 'cat', {name: 'kitty'}] // objects within arrays\nDF-like array\nmyData = [\n¬† {name: 'Paul', city: 'Denver'},\n¬† {name: 'Robert', city: 'Denver'},\n¬† {name: 'Ian', city: 'Boston'},\n¬† {name: 'Cobus', city: 'Boston'},\n¬† {name: 'Ayodele', city: 'New York'},\n¬† {name: 'Mike', city: 'New York'},\n]\n\nEquivalent Functions: Traditional vs Arrow\n// traditional\nfunction myFunctionWithParameters(firstName, lastName) {\n¬† return `My first name is ${firstName}, and my last name is ${lastName}.`\n}\n// arrow\nmyModernFunctionWithParameters = (firstName, lastName) =&gt; {\n¬† return `My first name is ${firstName}, and my last name is ${lastName}.`\n}\n\nArrow: Arguments are in the parentheses and the function is inside the curly braces\nString with variables needs to be surrounded by backticks\n\nFunctions Inside Methods: Traditional vs Arrow\n// traditional\n[1, 2, 3, 4, 5].filter(function(d) { return d &lt; 3 })\n// arrow\n[1, 2, 3, 4, 5].filter(d =&gt; d &lt; 3)\n\nThe argument is d but without parentheses and the function is d &lt; 3 without the curly braces\nThe function inputs each row/value of the array, so d is a row/value of the array. Then, the function does something to that row.\n\nConditionals\n\n== vs ===\n1 == '1' // true\n1 === '1' // false\n\n== is a logical test to see if two values are the same\n\n=== is a logical test to see if two values are the same and also checks if the value types are the same\n\nIf/Then\nif(1 &gt; 2) {¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† // If this statement is true\n¬† ¬† return 'Math is broken'¬† ¬† ¬† ¬† ¬† ¬† ¬† // return this\n} else {¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† // if the first statement was not true\n¬† ¬† return 'Math still works!'¬† ¬† ¬† ¬† ¬† // return this\n}\n\n// using ternary operator \"?\"\n\nUsing ternary operator ‚Äú?‚Äù\n\nSyntax: condition ? exprIfTrue : exprIfFalse\nExample: d =&gt; d.frequency &gt;= minFreq ? \"steelblue\" : \"lightgray\"\n\nSays if the frequency property is &gt;= the variable, minFreq, value, then use steelblue otherwise use lightgray\n\n\n\n\nFor-Loop\nlet largestNumber = 0; // Declare a variable for the largest number\n\nfor(let i = 0; i &lt; myValues.length - 1; i++) {¬† ¬† // Loop through all the values in my array\n¬† ¬† if(myValues[i] &gt; largestNumber) {¬† ¬† ¬† ¬† ¬† ¬† ¬† // Check if the value in the array is larger that the largestNumber\n¬† ¬† ¬† largestNumber = myValues[i]¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† // If so, assign the value as the new largest number\n¬† ¬† }\n}\n\nreturn largestNumber\n\nThe first statement sets a variable (let i = 0)\nThe second statement provides a condition for when the loop will run (whenever i &lt; myValues.length - 1)\nThe third statement says what to do each time the code block is executed (i++, which means to add 1 to i)\n\nWhile-Loop\nlet largestNumber = 0;¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† // Create a variable for the largest number\nlet i = 0;\nwhile(i &lt; myValues.length - 1) {\n¬† ¬† if(myValues[i] &gt; largestNumber) {¬† ¬† ¬† ¬† // Check if the value in the array is larger that the largestNumber\n¬† ¬† ¬† largestNumber = myValues[i]¬† ¬† ¬† ¬† ¬† ¬† // If so, assign the value as the new largest number\n¬† ¬† }\n¬† ¬† i++;\n}\nreturn largestNumber",
    "crumbs": [
      "JS"
    ]
  },
  {
    "objectID": "qmd/js.html#sec-js-cleaning",
    "href": "qmd/js.html#sec-js-cleaning",
    "title": "JS",
    "section": "Cleaning",
    "text": "Cleaning\n\nMisc\n\nNotes from: Horst article\n\nFilter objects: myData.filter(d =&gt; d.city == 'Denver')\nSelect properties: myNewArray = salesData.map(d =&gt; ({ date: d.date, product: d.product, totalRevenue: d.totalRevenue }))\n\nIn some contexts, this, d =&gt; d[\"mileage (mpg)\"] , is also used to select columns\n\nArrange objects: salesData.sort((a, b) =&gt; a.totalRevenue - b.totalRevenue)\n\nReorders salesData by totalRevenue (low to high)\n\nMutate properties: salesData.map(d =&gt; ({...d, discountedPrice: 0.9 * d.unitPrice }))\n\nAdds a new column to salesData with a discountedPrice, which takes 10% off each unitPrice.\n\nGroup_By: d3.rollup(salesData, v =&gt; d3.sum(v, d =&gt; d.totalRevenue), d =&gt; d.region)\n\nReturn the sum of totalRevenue for each region in salesData.\nrollup might actually be a summarize and the group_by is handled in the syntax\n\nRename: salesData.map(d =&gt; ({...d, saleDate: d.date }))\n\nAdds a new column called saleDate by storing a version of the date with new name saleDate and keeping all other columns.\n\nSubset value: salesData.map(d =&gt; d.description)[3]\n\nAccess the fourth value from the description property in salesData\n\nUnite:\nsalesData.map(d =&gt; ({...d, fullDescription: `${d.product} ${d.description}`}))\n\nUnite the product and description columns into a single column called fullDescription, using a comma as a separator.\n\nLeft Join: *using {{{arquero}}} tables* salesData.join_left(productDetails, ['product', 'product_id'])\n\nJoin information from a productDetails table to salesData. Join on product in salesData and product_id in productDetails.",
    "crumbs": [
      "JS"
    ]
  },
  {
    "objectID": "qmd/js.html#sec-js-stats",
    "href": "qmd/js.html#sec-js-stats",
    "title": "JS",
    "section": "Stats",
    "text": "Stats\n\nMisc\n\nNotes from: Horst article\n\nIn examples, waterUsage is the array; waterGallons is the property.\n\n\nMean: d3.mean(waterUsage.map(d =&gt; d.waterGallons))\n\nReturns a Value\n\nStd.Dev: d3.deviation(waterUsage.map(d =&gt; d.waterGallons))\nMedian: d3.median(waterUsage.map(d =&gt; d.waterGallons))\nMin/Max: d3.min(waterUsage.map(d =&gt; d.waterGallons))\nTotal Observations (i.e.¬†nrow ): waterUsage.length\nBy Group:\n\npropertyId is the discrete, grouping variable\nMean: waterMeans = d3.rollup(waterUsage, v =&gt; d3.mean(v, d =&gt; d.waterGallons), d =&gt; d.propertyId)\n// Returns a map object\nwaterMeans\n{\n¬† \"A001\" =&gt; 39.53389830508475\n¬† \"B002\" =&gt; 53.57627118644068\n¬† \"C003\" =&gt; 27.45762711864407\n¬† \"D004\" =&gt; 80.1864406779661\n}\n\n// View in a JS Table\n// ** Must be in a separate cell **\nInputs.table(waterMeans.map(([propertyId, meanWaterGallons]) =&gt; ({propertyId, meanWaterGallons})))\nCount: d3.rollup(waterUsage, v =&gt; d3.count(v, d =&gt; d.waterGallons), d =&gt; d.propertyId)\n\nConditional Counts: waterUsage.filter(d =&gt; d.waterGallons &gt; 90 && d.propertyId == \"B002\").length\n\nApplies two conditionals and counts the observations\n\nRanks\nwaterUsage.map((d, i) =&gt; ({...d, rank: d3.rank(waterUsage.map(d =&gt; d.waterGallons), d3.descending)[i] + 1}))\n\n1 is added so that ranks start at 1 instead of 0\n\nPercentiles: d3.quantile(waterUsage.map(d =&gt; d.waterGallons), 0.9) (e.g.¬†90th)",
    "crumbs": [
      "JS"
    ]
  },
  {
    "objectID": "qmd/js.html#sec-js-obs",
    "href": "qmd/js.html#sec-js-obs",
    "title": "JS",
    "section": "Observable",
    "text": "Observable\n\nA collaborative, online notebook platform that comes with libraries loaded to make it fairly straightforward to dive into ad hoc data analysis or produce complete reports.\nIn Observable, if you‚Äôre running a JavaScript cell that contains more than just a simple variable assignment (like myVariable = 'Hello World' ), you need to run a code block (i.e.¬†bracket lines of code in curly braces, {}).\nYou can open your notebook in Safe Mode and edit your work without running it.\n\nGood for debugging (e.g.¬†infinite while-loops)",
    "crumbs": [
      "JS"
    ]
  },
  {
    "objectID": "qmd/js.html#sec-js-def",
    "href": "qmd/js.html#sec-js-def",
    "title": "JS",
    "section": "Definitions",
    "text": "Definitions\n\nJSON vs R List\n{¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† list(\n¬† ¬† boolean: true,¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† boolean = TRUE,\n¬† ¬† string: \"hello\",¬† ¬† ¬† ¬† ¬† ¬† ¬† string = \"hello\",\n¬† ¬† vector: [1,2,3]¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† vector = c(1,2,3)\n}¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† )\n\n// Access¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† # Access\njson.vector¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† list$vector\nDependencies\nHTML¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† R (shiny)\n&lt;head&gt;¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† tags$head(\n¬† ¬† &lt;!-- JavaScript --&gt;¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† tags$script(src = \"path/to/file.js\")\n¬† ¬† &lt;script src=\"path/to/file.js\"&gt;&lt;/script&gt;¬† ¬† ¬† ¬† ¬† ¬† ¬† tags$link(\n¬† ¬† &lt;!-- CSS --&gt;¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† rel = \"stylesheet\",\n¬† ¬† &lt;link rel=\"stylesheet\" href=\"path/to/file.css&gt;¬† ¬† ¬† ¬† href = \"path/to/file.css\n&lt;/head&gt;¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ))\nd is each row and =&gt; is function\n(d) =&gt; d.year === 2020\n\nSays for each row in your data, the year column must equal 2020\n\nCallback Function - A function that is passed to another function as a parameter. In other words, a function ‚Äúcalls back‚Äù to previously defined function.\nfunction print(callback) {¬†\n¬† ¬† callback();\n}\n\ncallback is the callback function and is a parameter of the print function\nCallbacks make sure that a function is not going to run before a task is completed but will run right after the task has completed.\nExample:\n// \"Click here\" button in a web app\n&lt;button id=\"callback-btn\"&gt;Click here&lt;/button&gt;\ndocument.queryselector(\"#callback-btn\")\n¬† ¬† .addEventListener(\"click\", function() {¬† ¬†\n¬† ¬† ¬† console.log(\"User has clicked on the button!\");\n});\n\nFirst, button selected by its id, and then we add an event listener with the addEventListener method. It takes 2 parameters. The first one is its type, click, and the second parameter is a callback function, which logs the message when the button is clicked.\n\n\nAnonymous Function - Same as a callback but unnamed. It‚Äôs a¬† function that is defined within another function.\nsetTimeout(function() {¬†\n¬† ¬† console.log(\"This message is shown after 3 seconds\");\n}, 3000);\n\n// if the function were named\nconst message = function() {¬†\n¬† ¬† console.log(\"This message is shown after 3 seconds\");\n}\n\n// as an arrow function\nsetTimeout(() =&gt; {¬†\n¬† ¬† console.log(\"This message is shown after 3 seconds\");\n}, 3000);\n\nThe function used as a parameter has no name. console.log is the contents of the function.",
    "crumbs": [
      "JS"
    ]
  },
  {
    "objectID": "qmd/js.html#sec-js-nfcd",
    "href": "qmd/js.html#sec-js-nfcd",
    "title": "JS",
    "section": "Notes From Covidcast Dashboard",
    "text": "Notes From Covidcast Dashboard\n\nNotes from\n\nCovidcast Dashboard: reactable + sparkline tooltip (link)\n\ndiv = vertical label or container , span = horizontal\nFormat: type, styling, value\n2 divs would result in a 2 element vertical label while 2 spans would be a 2 element horizontal label\nExample: A div container holding 2 spans which creates a ‚Äúdate value‚Äù horizontal label\n\"function (_ref) {\nvar datum = _ref.datum;\nreturn React.createElement(\n¬† 'div',\n¬† null,\n¬† datum.date && React.createElement(\n¬† ¬† ¬† 'span',\n¬† ¬† ¬† {style: {\n¬† ¬† ¬† ¬† ¬† backgroundColor: 'black', color: 'white',\n¬† ¬† ¬† ¬† ¬† padding: '3px', margin: '0px 4px 0px 0px', textAlign: 'center'\n¬† ¬† ¬† ¬† }},\n¬† ¬† ¬† datum.date[0].split('-').slice(1).join('/')\n¬† ),\n¬† React.createElement(\n¬† ¬† ¬† 'span',\n¬† ¬† ¬† {style: {\n¬† ¬† ¬† ¬† fontWeight: 'bold', fontSize: '1.1em',\n¬† ¬† ¬† ¬† padding: '2px'\n¬† ¬† ¬† }},\n¬† ¬† ¬† datum.y ? datum.y.toLocaleString(undefined, {maximumFractionDigits: 0}) : '--'\n¬† )\n¬† );\n}\"\n\nCSS: margin, padding\n\nFormat is top, right, bottom, left (ordered like a clock)\nRequires units like ‚Äúpx‚Äù\nNo commas separate the values\n{margin: '0px 4px', padding: '0px 0px 0px 4px'}\n\nMaybe for 0s it doesn‚Äôt matter\nSee bkmk in css/definitions for explanations behind specifications with less than 4 numbers\n\ne.g.¬†2 is ‚Äòtop/bottom left/right‚Äô\n\n\n\nString manipulation\ndatum.endDate[0].split('-').slice(1).join('/')\n\nTreats variable as a string object\nLooks in data arg, finds endDate variable\nIts a list variable so requires the [0] (0 part an index?)\nDate format is ymd, so splits value by ‚Äú-‚Äù separator, removes 1st value (year), joins the rest of the values (month, day) with ‚Äú/‚Äù\n\nIf slice(2), removes first 2 values (left to right)\n\n\nConditional\nlabelPosition = htmlwidgets::JS(\"(d, i) =&gt; (i === 0 || i === 1 ? 'right' : 'left')\")\n\nSays that if index of data value, d, is 0 or 1 then label should be positioned on the right of the point, else place the label on the left of the point",
    "crumbs": [
      "JS"
    ]
  },
  {
    "objectID": "qmd/simulation-data.html",
    "href": "qmd/simulation-data.html",
    "title": "Simulation, Data",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Simulation, Data"
    ]
  },
  {
    "objectID": "qmd/simulation-data.html#sec-sim-data-misc",
    "href": "qmd/simulation-data.html#sec-sim-data-misc",
    "title": "Simulation, Data",
    "section": "",
    "text": "Todo - for the ‚ÄútrtAssign‚Äù mess with ratio and the number of ratios\nAlso see\n\nBkmks Data &gt;&gt; Data Simulation\nPandas-Time Series-Simulation\n\nA Gaussian and Standard GARCH time-series that‚Äôs frequently encountered in econometrics\n\n\nPackages\n\n{simChef} (Vignette) - Rapidly plan, carry out, and summarize statistical simulation studies in a flexible, efficient, and low-code manner.\n\nIntuitive tidy grammar of data science simulations\nPowerful abstractions for distributed simulation processing backed by {future}\nAutomated generation of interactive R Markdown simulation documentation, situating results next to the workflows needed to reproduce them.\n\n{structmcmc} - A set of tools for performing structural inference for Bayesian Networks using MCMC\n\nPapers\n\nGeneration and analysis of synthetic data via Bayesian networks: a robust approach for uncertainty quantification via Bayesian paradigm",
    "crumbs": [
      "Simulation, Data"
    ]
  },
  {
    "objectID": "qmd/simulation-data.html#sec-sim-data-simstudy",
    "href": "qmd/simulation-data.html#sec-sim-data-simstudy",
    "title": "Simulation, Data",
    "section": "{simstudy}",
    "text": "{simstudy}\n\nMisc\n\nDocs\n\n\n\nReference\n\nAvailable distributions (link)\n\nProbability Distributions\nnonrandom: For constants; can be a numeric or a string with a formula that defines a dependency on another variable\nclusterSize: For variable cluster sizes but a constant total sample size\n\nformula: The (fixed) total sample size\nvariance: A (non-negative) dispersion measure that represents the variability of size across clusters\n\nIf the dispersion is set to 0, then cluster sizes are constant\n\n\ntrtAssign: For treatment assignment\n\nformula: Ratio which is separated by semicolons and number of treatments\n\ne.g.¬†2 values = 2 groups and ‚Äú1;2‚Äù says group 2 has twice as many units and group 1\n\nvariance: Stratification; ratio in formula is used as the stratification ratio (e.g.¬†unbalanced treatment groups ‚Üí unbalanced stratification)\nExample\ndef &lt;- \n  defData(def, \n          varname = \"rx\", \n          dist = \"trtAssign\",\n          formula = \"1;1;2\", \n          variance = \"male;over65\")\n\ncount(studytbl, rx)\n#&gt; # A tibble: 3 √ó 2\n#&gt; ¬† ¬† rx¬† ¬† n\n#&gt; ¬† &lt;int&gt; &lt;int&gt;\n#&gt; 1¬† ¬† 1¬† ¬† 84\n#&gt; 2¬† ¬† 2¬† ¬† 82\n#&gt; 3¬† ¬† 3¬† 164\n\ncount(studytbl, male, rx)\n#&gt; # A tibble: 6 √ó 3\n#&gt; ¬† male¬† ¬† rx¬† ¬† n\n#&gt; ¬† &lt;int&gt; &lt;int&gt; &lt;int&gt;\n#&gt; 1¬† ¬† 0¬† ¬† 1¬† ¬† 40\n#&gt; 2¬† ¬† 0¬† ¬† 2¬† ¬† 39\n#&gt; 3¬† ¬† 0¬† ¬† 3¬† ¬† 78\n#&gt; 4¬† ¬† 1¬† ¬† 1¬† ¬† 44\n#&gt; 5¬† ¬† 1¬† ¬† 2¬† ¬† 43\n#&gt; 6¬† ¬† 1¬† ¬† 3¬† ¬† 86\n\ncount(studytbl, over65, rx)\n#&gt; # A tibble: 6 √ó 3\n#&gt; ¬† over65¬† ¬† rx¬† ¬† n\n#&gt; ¬† &lt;int&gt; &lt;int&gt; &lt;int&gt;\n#&gt; 1¬† ¬† ¬† 0¬† ¬† 1¬† ¬† 66\n#&gt; 2¬† ¬† ¬† 0¬† ¬† 2¬† ¬† 65\n#&gt; 3¬† ¬† ¬† 0¬† ¬† 3¬† 130\n#&gt; 4¬† ¬† ¬† 1¬† ¬† 1¬† ¬† 18\n#&gt; 5¬† ¬† ¬† 1¬† ¬† 2¬† ¬† 17\n#&gt; 6¬† ¬† ¬† 1¬† ¬† 3¬† ¬† 34\n\n\nFunctions\n\ndefData(dtDefs = NULL, varname, formula, variance = 0, dist = \"normal\", link = \"identity\", id = \"id\") - Initially creates a data.table or adds a column to a data.table with instructions about creating a variable\n\nformula: Numeric constant or string formula for the mean, probability of event (binary), probability of success (binomial), etc.\n\ndefDataAdd(dtDefs = NULL, varname, formula, variance = 0, dist = \"normal\", link = \"identity\") - Creates a variable definition like defData but is used to augment a already generated dataset. Used as input to addColumns which will generate the variable data from the instructions in this object and add it as a column to the already generated dataset.\ngenCluster(dtClust, cLevelVar, numIndsVar, level1ID, allLevel2 = TRUE) - After generating cluster-level data, this function takes the number of clusters and the sizes of each cluster from that data, and does something like expand.grid to generate an individual-level dataset. Also, adds an id variable.\n\ndtClust: Cluster-Level Data\ncLevelVar: Cluster variable from the cluster-level data\nnumIndsvar: Variable with the number of units per cluster from the cluster-level data\nlevel1ID: Name you want for your individual-level ID variable\n\n\n\n\n\nVariable Dependence\n\nBinary depends on a Binary\n\nDefinitions\ndef &lt;- defData(varname = \"male\", dist = \"binary\",\n               formula = .5 , id=\"cid\")\ndef &lt;- defData(def, varname = \"over65\", dist = \"binary\",\n               formula = \"-1.7 + .8*male\", link=\"logit\")\nWhat‚Äôs happening\nmale &lt;- c(1,1,0,1,0,0,0,1,0,1)\nlogits &lt;- -1.7 + 0.8 * male\nprobabilities &lt;- boot::inv.logit(logits)\nover65 &lt;- rbinom(n = 10, size = 1, prob = probabilities)\n\nThe formula in the logits line defines the relationship between being male and being over 65yrs old.\nMales in this sample will have a higher probability (0.2890505) of being over 65yrs old than females (0.1544653)\nTo sample from a Bernoulli distribution, set size = 1\nover65 is an indicator where each value is determined by a separate probability parameter for a Bernoulli distribution\n\n\n\n\n\nClustered with Cluster-Level Random Effect\n\nExample: Fixed Cluster sizes; Balanced\n\nCluster Definitions\nd0 &lt;- defData(varname = \"n\", formula = 20, dist = \"nonrandom\")\nd0 &lt;- defData(d0, varname = \"a\", formula = 0, variance = 0.33)\nd0 &lt;- defData(d0, varname = \"rx\", formula = \"1;1\", dist = \"trtAssign\")\nd1 &lt;- defDataAdd(varname = \"y\", formula = \"18 + 1.6 * rx + a\",\n                 variance = 16, dist = \"normal\")\n\nn: sample size for the cluster\n\ndist = ‚Äúnonrandom‚Äù and formula = 20 says use a constant for the cluster sizer\n\nrx: treatment indicator\n\ndist = ‚ÄútrtAssign‚Äù and formula = ‚Äú1;1‚Äù says 2 treatment groups and they‚Äôre balanced\n\ny: the individual-level outcome is a function of the treatment assignment and the cluster effect, as well as random individual-level variation\na: random individual-level variation (i.e.¬†random effect)\n\nRandom Effects are sampled from \\(\\mathcal{N}(0, \\sigma)\\) where the variance is typically estimated in a Mixed Effects model.\n\n\nGenerate Cluster-Level Data\nset.seed(2761)\ndc &lt;- genData(10, d0, \"site\")\ndc\n##¬† ¬† site¬† n¬† ¬† ¬† a rx\n##¬† 1:¬† ¬† 1 20 -0.3548¬† 1\n##¬† 2:¬† ¬† 2 20 -1.1232¬† 1\n##¬† 3:¬† ¬† 3 20 -0.5963¬† 0\n##¬† 4:¬† ¬† 4 20 -0.0503¬† 1\n##¬† 5:¬† ¬† 5 20¬† 0.0894¬† 0\n##¬† 6:¬† ¬† 6 20¬† 0.5294¬† 1\n##¬† 7:¬† ¬† 7 20¬† 1.2302¬† 0\n##¬† 8:¬† ¬† 8 20¬† 0.9663¬† 1\n##¬† 9:¬† ¬† 9 20¬† 0.0993¬† 0\n## 10:¬† 10 20¬† 0.6508¬† 0\n\nGenerates 10 clusters labelled as site according to the instructions in d0\n\nGenerate Individual Level Data\ndd &lt;- genCluster(dc, \"site\", \"n\", \"id\")\ndd &lt;- addColumns(d1, dd)\ndd\n##¬† ¬† ¬† site¬† n¬† ¬† ¬† a rx¬† id¬† ¬† y\n##¬† 1:¬† ¬† 1 20 -0.355¬† 1¬† 1 17.7\n##¬† 2:¬† ¬† 1 20 -0.355¬† 1¬† 2 16.2\n##¬† 3:¬† ¬† 1 20 -0.355¬† 1¬† 3 19.2\n##¬† 4:¬† ¬† 1 20 -0.355¬† 1¬† 4 20.6\n##¬† 5:¬† ¬† 1 20 -0.355¬† 1¬† 5 14.7\n##¬† ---¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬†\n## 196:¬† 10 20¬† 0.651¬† 0 196 25.3\n## 197:¬† 10 20¬† 0.651¬† 0 197 22.1\n## 198:¬† 10 20¬† 0.651¬† 0 198 13.2\n## 199:¬† 10 20¬† 0.651¬† 0 199 15.6\n## 200:¬† 10 20¬† 0.651¬† 0 200 13.8\n\ngenCluster performs an expand.grid to generate an individual-level dataset along with adding an ID variable\naddColumns uses individual-level data and outcome variable definition to generate the outcome variable and add it to the dataset.\n\n\nExample: Varying Cluster Sizes and therefore Varying Sample Size\nd0 &lt;- defData(varname = \"n\", formula = 20, dist = \"poisson\")\ngenData(10, d0, \"site\")\n##¬† ¬† site¬† n\n##¬† 1:¬† ¬† 1 13\n##¬† 2:¬† ¬† 2 18\n##¬† 3:¬† ¬† 3 21\n##¬† 4:¬† ¬† 4 26\n##¬† 5:¬† ¬† 5 25\n##¬† 6:¬† ¬† 6 27\n##¬† 7:¬† ¬† 7 23\n##¬† 8:¬† ¬† 8 30\n##¬† 9:¬† ¬† 9 23\n## 10:¬† 10 20\n\nFormula sets the poisson distribution parameter, \\(\\lambda = 20\\). So sizes are sampled from poisson distribution with that mean/variance\nTo increase the variability between clusters, use the negative binomial distribution\nMost likely leads to an unbalanced design\n\nExample: Varying Cluster Sizes but Constant Sample Size\n# moderately varying cluster sizes\nd0 &lt;- defData(varname = \"n\", formula = 200, variance = 0.2, dist = \"clusterSize\")\ngenData(10, d0, \"site\")\n\n##¬† ¬† site¬† n\n##¬† 1:¬† ¬† 1 20\n##¬† 2:¬† ¬† 2 28\n##¬† 3:¬† ¬† 3 25\n##¬† 4:¬† ¬† 4 24\n##¬† 5:¬† ¬† 5 28\n##¬† 6:¬† ¬† 6 22\n##¬† 7:¬† ¬† 7¬† 7\n##¬† 8:¬† ¬† 8 13\n##¬† 9:¬† ¬† 9 22\n## 10:¬† 10 11\n\n# Very highly varying cluster sizes\nd0 &lt;- defData(varname = \"n\", formula = 200, variance = 5, dist = \"clusterSize\")\ngenData(10, d0, \"site\")\n##¬† ¬† site¬† n\n##¬† 1:¬† ¬† 1¬† 10\n##¬† 2:¬† ¬† 2¬† 2\n##¬† 3:¬† ¬† 3¬† 17\n##¬† 4:¬† ¬† 4¬† 2\n##¬† 5:¬† ¬† 5¬† 49\n##¬† 6:¬† ¬† 6 110\n##¬† 7:¬† ¬† 7¬† 1\n##¬† 8:¬† ¬† 8¬† 4\n##¬† 9:¬† ¬† 9¬† 1\n## 10:¬† 10¬† 4\n\nTotal sample size is fixed at 200 (formula), but individual cluster sizes are allowed to vary.\nvariance: A dispersion parameter that controls the amount of varying of the cluster sizes",
    "crumbs": [
      "Simulation, Data"
    ]
  },
  {
    "objectID": "qmd/association-general.html",
    "href": "qmd/association-general.html",
    "title": "General",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Association",
      "General"
    ]
  },
  {
    "objectID": "qmd/association-general.html#sec-assoc-gen-misc",
    "href": "qmd/association-general.html#sec-assoc-gen-misc",
    "title": "General",
    "section": "",
    "text": "Also see\n\nEDA &gt;&gt; Correlation\nNotebook &gt;&gt; Statistical Inference &gt;&gt; Correlation\n\n\\(E(œÖ|x)=0\\) is equivalent to \\(Cov(x,œÖ)=0\\) or \\(Cor(x,œÖ)=0\\)\nA negative correlation between variables is also called anticorrelation or inverse correlation\nIndependence - Two random variables are independent if the product of their individual probability density functions equals the joint probability density function\nFor the correleation between two variables where the data is repeated measures, see the 2-Stage approach in Econometrics, Mixed Models &gt;&gt; Model Equation",
    "crumbs": [
      "Association",
      "General"
    ]
  },
  {
    "objectID": "qmd/association-general.html#sec-assoc-gen-partcor",
    "href": "qmd/association-general.html#sec-assoc-gen-partcor",
    "title": "General",
    "section": "Partial Correlation",
    "text": "Partial Correlation\n\nStatistical Formula\n\\[\n\\frac{\\mbox{Cov}(X, Y) - \\mbox{Cov}(X, Z) \\cdot \\mbox{Cov}(Y, Z)}{\\sqrt{\\mbox{Var}(X) - \\mbox{Cov}(X, Z)^2}\\cdot \\sqrt{\\mbox{Var}(Y) - \\mbox{Cov}(Y, Z)^2}}\n\\]\nMeasures the association (or correlation) between two variables when the effects of one or more other variables are removed from such a relationship.\n\nIn the above equation, I think it‚Äôs the partial correlation between x and y given z.\n\nMisc\n\nResources\n\nDealing with correlation in designed field experiments: part I\n\nExcellent tutorial on partial, joint correlations in block design\n\nppcor pkg: An R Package for a Fast Calculation to Semi-partial Correlation Coefficients\n\nExplainer for semi-partial, partial correlation\n\nAlso see notebook for a method using regression models\n\n\nExample: psych::partial.r(y ~ x - z, data)\nExample: {correlation}\nhead(correlation::correlation(mtcars, partial = TRUE))\n\n#&gt; # Correlation Matrix (pearson-method)\n\n#&gt; Parameter1 | Parameter2 |     r |         95% CI | t(30) |      p\n#&gt; -----------------------------------------------------------------\n#&gt; mpg        |        cyl | -0.02 | [-0.37,  0.33] | -0.13 | &gt; .999\n#&gt; mpg        |       disp |  0.16 | [-0.20,  0.48] |  0.89 | &gt; .999\n#&gt; mpg        |         hp | -0.21 | [-0.52,  0.15] | -1.18 | &gt; .999\n#&gt; mpg        |       drat |  0.10 | [-0.25,  0.44] |  0.58 | &gt; .999\n#&gt; mpg        |         wt | -0.39 | [-0.65, -0.05] | -2.34 | &gt; .999\n#&gt; mpg        |       qsec |  0.24 | [-0.12,  0.54] |  1.34 | &gt; .999\n#&gt; \n#&gt; p-value adjustment method: Holm (1979)\n#&gt; Observations: 32\n\nVisualization\n\npacman::p_load(see, ggraph)\ncorrelation::correlation(mtcars, partial = TRUE) |&gt; \n  plot()\n\nGraphical LASSO\n\nComputing covariance matrices are computationally expensive while computing its inverse can be less so. This algorithm calculates the inverse covariance matrix (ICT), aka Precision Matrix, and it‚Äôs based on an interplay between probability theory and graph theory, in which the properties of an underlying graph specify the conditional independence properties of a set of random variables.\n\nSee Statistical Learning With Sparsity (Hastie, Tibshirani, Wainright)\n\nMathematical introduction to graphical models and Graphical LASSO, pg 241 (252 in pdf), See R &gt;&gt; Documents &gt;&gt; Regression\n\n\nAssumes that the observations have a multivariate Gaussian distribution\nMisc\n\nPackages\n\n{glasso} - The original package by the authors of the algorithm. Estimation of a sparse inverse covariance matrix using a lasso (L1) penalty. Facilities are provided for estimates along a path of values for the regularization parameter. Can be slow or nonconvergent for large dimension datasets.\n{huge} - Provides functions for estimating high dimensional undirected graphs from data. Also provides functions for fitting high dimensional semiparametric Gaussian copula models (Vignette)\n{cglasso} - Conditional Graphical Lasso Inference with Censored and Missing Values (Vignette)\n\n\nPreprocessing: All variables should be standardized.\nThe terms in the ICT are not equivalent but are proportional to the partial correlation between the two corresponding variables\n\nTransform the ICT, \\(\\Omega\\) into a partial correlation matrix, \\(R\\)\n\\[\nR_{j,k} = \\frac{-\\Omega_{i,j}}{\\sqrt{\\Omega_{j,j}\\Omega_{k,k}}}\n\\]\nparr.corr &lt;- matrix(nrow=nrow(P), ncol=ncol(P))\nfor(k in 1:nrow(parr.corr)) {\n  for(j in 1:ncol(parr.corr)) {\n    parr.corr[j, k] &lt;- -P[j,k]/sqrt(P[j,j]*P[k,k])\n  }\n}\ncolnames(parr.corr) &lt;- colnames(P)\nrownames(parr.corr) &lt;- colnames(P)\ndiag(parr.corr) &lt;- 0\nSetting the terms on the diagonal to zero prevents variables from having connections with themselves in a network graph if you want to visualize the relationships\n\nWhere the nodes are variables and edges are the partial correlations.\n\n\nHyperparameter, \\(\\rho\\) , adjusts the sparsity of the matrix output\n\nHigher: Isolates the strongest relationships in your data (more sparse)\nLower: Preserving more tenuous connections, perhaps identifying variables with connections to multiple groups (less sparse)\n\nCheck symmetry. Assymmetry in the ICT can arise due to numerical computation and rounding errors, which can cause problems later depending on what you want to do with the matrix.\nExample: Stock Analysis using {glasso} (link)\nrho &lt;- 0.75\ninvcov &lt;- glasso(S, rho=rho)  \n\n# inverse covariance matrix\nP &lt;- invcov$wi\ncolnames(P) &lt;- colnames(S)\nrownames(P) &lt;- rownames(S)\n\n# check symmetry\nif(!isSymmetric(P)) {\n  P[lower.tri(P)] = t(P)[lower.tri(P)]  \n}\n\nGoal: Remove stocks relationship with market Beta and other confounding stocks to get the true relationsip between stock pairs.\nPost also has a network visualization. Data was put through PCA, then DBSCAN to get clusters. The cluster assignments were used to color the clusters in the network graph.\nPost also examines output from a lower \\(\\rho\\) and has an interesting analysis of the non-connected variables (i.e.¬†no partial correlation).",
    "crumbs": [
      "Association",
      "General"
    ]
  },
  {
    "objectID": "qmd/association-general.html#sec-assoc-gen-cont",
    "href": "qmd/association-general.html#sec-assoc-gen-cont",
    "title": "General",
    "section": "Continuous",
    "text": "Continuous\n\nSpearman‚Äôs Rank\n\\[\n\\rho = 1 - \\frac{6\\sum_i d_i^2}{n(n^2-1)}\n\\]\n\n\\(d_i\\): The difference in ranks for the ith observation\nMeasures how well the relationship between the two variables can be described by a monotonic function\nRank correlation measures the similarity of the order of two sets of data, relative to each other (recall that PCC did not directly measure the relative rank).\n\nValues range from -1 to 1 where 0 is no association and 1 is perfect association\nNegative values don‚Äôt mean anything in ranked correlation, so just remove the negative\n\nLinear relationship is a specific type of monotonic relationship where the rate of increase remains constant ‚Äî in other words, unlike a linear relationship, the amount of change (increase or decrease) in a monotonic relationship can vary.\nSee bkmks for CIs\nPackages\n\n{stats::cor.test(method = ‚Äúspearman‚Äù)}\n{DescTools::SpearmanRho}\n{wCorr} - Pearson, Spearman, polyserial, and polychoric correlations, in weighted or unweighted form\n\n\nKendall‚Äôs Tau\n\nNon-parametric rank correlation\n\nNon-parametric because it only measures the rank correlation based on the relative ordering of the data (and not the specific values of the data).\n\nShould be pretty close to Sspearman‚Äôs Rank but a potentially faster calculation\nFlavors: a, b (makes adjustment for ties), c (for different sample sizes for each variable)\n\nUse Tau-b if the underlying scale of both variables has the same number of possible values (before ranking) and Tau-c if they differ.\ne.g.¬†One variable might be scored on a 5-point scale (very good, good, average, bad, very bad), whereas the other might be based on a finer 10-point scale. In this case, Tau-c would be recommended.\n\nPackages\n\n{stats::cor.test(method = ‚Äúkendall‚Äù)} - Doesn‚Äôt state specifically but I think it calculates a and b depending on whether ties are present or not\n{DescTools} - has all 3 flavors\n\n\nHoeffding‚Äôs D\n\nResource\n\nMy Favorite Statistical Measure: Hoeffding‚Äôs D - Detailed Explainer\n\nRank-based approach that measures the difference between the joint ranks of (X,Y) and the product of marginal ranks.(?) A non-parametric test of independence. the product of their marginal ranks.\nUnlike the Pearson or Spearman measures, it can pick up on nonlinear relationships.\nRange: [-.5,1]\nGuidelines: Larger values indicate a stronger relationship between the variables.\nPackages\n\n{Hmisc::hoeffd}\n{DescTools::HoeffD}\n\n\nBayesian\n\nSteps: {brms}\n\nList the variables you‚Äôd like correlations for within mvbind().\nPlace the mvbind() function within the left side of the model formula.\nOn the right side of the model formula, indicate you only want intercepts (i.e., ~ 1).\nWrap that whole formula within bf().\nThen use the + operator to append set_rescor(TRUE), which will ensure brms fits a model with residual correlations.\nUse non-default priors and the resp argument to specify which prior is associated with which criterion variable\n\nGaussian\n\nExample: multiple variables\nf9 &lt;-¬†\n ¬† brm(data = d,\n ¬† ¬†family = gaussian,\n ¬† ¬†bf(mvbind(x_s, y_s, z_s) ~ 0,\n ¬†   ¬† sigma ~ 0) +\n    set_rescor(TRUE),\n¬† ¬† prior(lkj(2), class = rescor),\n¬† ¬† chains = 4, cores = 4,\n¬† ¬† seed = 1)\n\n## Residual Correlations:¬†\n##¬† ¬† ¬† ¬† ¬† ¬† ¬† Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## rescor(xs,ys)¬† ¬† 0.90¬† ¬† ¬† 0.02¬† ¬† 0.87¬† ¬† 0.93 1.00¬† ¬† 3719¬† ¬† 3031\n## rescor(xs,zs)¬† ¬† 0.57¬† ¬† ¬† 0.07¬† ¬† 0.42¬† ¬† 0.69 1.00¬† ¬† 3047¬† ¬† 2773\n## rescor(ys,zs)¬† ¬† 0.29¬† ¬† ¬† 0.09¬† ¬† 0.11¬† ¬† 0.46 1.00¬† ¬† 2839¬† ¬† 2615\nStandardized data is used here but isn‚Äôt required\n\nWill need to set priors though (see article for further details)\n\nSince the data is standardized, the sd can be fixed at 1\n\nbrms models log of sd by default, hence sigma ~ 0 since log 1 = 0\n\nCorrelations are the estimates for rescor(xs,ys), rescor(xs,zs) rescor(ys,zs)\n\nStudent t-distribution\n\nIf the data has any outliers, pearson‚Äôs coefficient is substantially biased.\nExample: correlation between x and y\n\\\nf2 &lt;-¬†\n¬† ¬† brm(data = x.noisy,¬†\n¬† ¬† family = student,\n¬† ¬† bf(mvbind(x, y) ~ 1) + set_rescor(TRUE),\n¬† ¬† prior = c(prior(gamma(2, .1), class = nu),\n    ¬† ¬† ¬† ¬† ¬† prior(normal(0, 100), class = Intercept, resp = x),\n¬† ¬† ¬† ¬† ¬† ¬† ¬† prior(normal(0, 100), class = Intercept, resp = y),\n¬† ¬† ¬† ¬† ¬† ¬† ¬† prior(normal(0, 100), class = sigma, resp = x),\n¬† ¬† ¬† ¬† ¬† ¬† ¬† prior(normal(0, 100), class = sigma, resp = y),\n¬† ¬† ¬† ¬† ¬† ¬† ¬† prior(lkj(1), class = rescor)),\n¬† ¬† iter = 2000, warmup = 500, chains = 4, cores = 4,¬†\n¬† ¬† seed = 210191)\n\n## Population-Level Effects:¬†\n##¬† ¬† ¬† ¬† ¬† ¬† Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## x_Intercept¬† ¬† -2.07¬† ¬† ¬† 3.59¬† ¬† -9.49¬† ¬† 4.72 1.00¬† ¬† 2412¬† ¬† 2651\n## y_Intercept¬† ¬† 1.93¬† ¬† ¬† 7.20¬† -11.31¬† ¬† 16.81 1.00¬† ¬† 2454¬† ¬† 2815\n##¬†\n## Family Specific Parameters:¬†\n##¬† ¬† ¬† ¬† Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## sigma_x¬† ¬† 18.35¬† ¬† ¬† 2.99¬† ¬† 13.12¬† ¬† 24.76 1.00¬† ¬† 2313¬† ¬† 2816\n## sigma_y¬† ¬† 36.52¬† ¬† ¬† 5.90¬† ¬† 26.13¬† ¬† 49.49 1.00¬† ¬† 2216¬† ¬† 3225\n## nu¬† ¬† ¬† ¬† ¬† 2.65¬† ¬† ¬† 0.99¬† ¬† 1.36¬† ¬† 4.99 1.00¬† ¬† 3500¬† ¬† 2710\n## nu_x¬† ¬† ¬† ¬† 1.00¬† ¬† ¬† 0.00¬† ¬† 1.00¬† ¬† 1.00 1.00¬† ¬† 6000¬† ¬† 6000\n## nu_y¬† ¬† ¬† ¬† 1.00¬† ¬† ¬† 0.00¬† ¬† 1.00¬† ¬† 1.00 1.00¬† ¬† 6000¬† ¬† 6000\n##¬†\n## Residual Correlations:¬†\n##¬† ¬† ¬† ¬† ¬† ¬† Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n## rescor(x,y)¬† ¬† -0.93¬† ¬† ¬† 0.03¬† ¬† -0.97¬† ¬† -0.85 1.00¬† ¬† 2974¬† ¬† 3366\n\nN = 40 simulated from a multivariate normal with 3 outliers\nCorrelation is the rescor(x,y) estimate -0.93; true value is -0.96\n\nUsing a pearson coefficient, cor = -0.6365649\nUsing brms::brm with family = gaussian, rescor(x,y) estimate -0.61",
    "crumbs": [
      "Association",
      "General"
    ]
  },
  {
    "objectID": "qmd/association-general.html#sec-assoc-gen-disc",
    "href": "qmd/association-general.html#sec-assoc-gen-disc",
    "title": "General",
    "section": "Discrete",
    "text": "Discrete\n\nMisc\n\nAlso see\n\nMultiple Correspondence Analysis (MCA) (see bkmks &gt;&gt; Features &gt;&gt; Reduction)\nDiscrete Analysis Notebook\n\nPackages\n\n{PAsso} - Assesses the Partial Association Between Ordinal Variables\n\nAllows users to perform a wide spectrum of assessments, including quantification, visualization, and hypothesis testing.\nVignette\n\n\nBinary vs Binary Similarity measures (paper)\n\nNote that a pearson correlation between binaries can be useful (see EDA &gt;&gt; Misc &gt;&gt; {correlationfunnel})\nTypes:\n\nJaccard-Needham\nDice\nYule\nRussell-Rao\nSokal-Michener\nRogers-Tanimoto\nKulzinsky\n\nPackages\n\n{{scipy}} - Also has other similarity measures\n\n\n\nPhi Coefficient - Used for binary variables when the categories are truly binary and not crudely measuring some underlying continuous variable (i.e.¬†dichotomization of a continuous variable)\n\n‚ÄúA Pearson correlation coefficient estimated for two binary variables will return the phi coefficient‚Äù (Phi coefficient wiki)\n(Contingency Table) Two binary variables are considered positively associated if most of the data falls along the diagonal cells. In contrast, two binary variables are considered negatively associated if most of the data falls off the diagonal\nAlso see StackExchange discussion on the difference between Phi Coefficient and Tetrachoric correlation\n{DescTools::Phi}\n\nCramer‚Äôs V - Association between two nominal variables\n\nSee Discrete Analysis notebook\n{DescTools::CramerV}\n\nPolychoric - Suppose each of the ordinal variables was obtained by categorizing a normally distributed underlying variable, and those two unobserved variables follow a bivariate normal distribution. Then the (maximum likelihood) estimate of that correlation is the polychoric correlation.\n\n{polycor}\n{psych::polychoric}\n\nFor correct=FALSE, the results agree perfectly with {polycor}\nFor very small data sets, the correction for continuity for the polychoric correlations can lead to difficulties, particularly if using the global=FALSE option, or if doing just one correlation at a time. Setting a smaller correction value (i.e., correct =.1) seems to help.\n\n{DescTools::CorPolychor}\n{wCorr} - Pearson, Spearman, polyserial, and polychoric correlations, in weighted or unweighted form\n\nTetrachoric - Used for binary variables when those variables are a sort of crude measure of an underlying continuous variable\n\nAlso see StackExchange discussion on the difference between Phi Coefficient and Tetrachoric correlation\nExample of appropriate use case: Suppose there are two judges who judge cakes, say, on some continuous scale, then based on a fixed, perhaps unknown, cutoff, pronounce the cakes as ‚Äúbad‚Äù or ‚Äúgood‚Äù. Suppose the latent continuous metric of the two judges has correlation coefficient œÅ.\n‚Äúthe contingency tables are ‚Äòbalanced‚Äô row-wise and col-wise, you get good correlation between the two metrics, but the tetrachoric tends to be a bit larger than the phi coefficient. When the cutoffs are somewhat imbalanced, you get slightly worse correlation between the metrics, and the phi appears to ‚Äòshink‚Äô towards zero.‚Äù\nThe estimation procedure is two stage ML.\n\nCell frequencies for each pair of items are found. Cells with zero counts are replaced with .5 as a correction for continuity (correct=TRUE).\nThe marginal frequencies are converted to normal theory thresholds and the resulting table for each item pair is converted to the (inferred) latent Pearson correlation that would produce the observed cell frequencies with the observed marginals\n\n{psych::tetrachoric}\n\nThe correlation matrix gets printed, but the correlations can also be extracted with $rho\nCan be sped up considerably by using multiple cores and using the parallel package. The number of cores to use when doing polychoric or tetrachoric may be specified using the options command. (e.g options(\"mc.cores\"=4);)\nsmooth = TRUE - For sets of data with missing data, the matrix will sometimes not be positive definite. Uses a procedure to transform the negative eigenvalues.\nFor relatively small samples with dichotomous data if some cells are empty, or if the resampled matrices are not positive semi-definite, warnings are issued. this leads to serious problems if using multi.cores. The solution seems to be to not use multi.cores (e.g., options(mc.cores =1)\n\nGoodman and Kruskal‚Äôs Gamma\n\nA measure of rank correlation, i.e., the similarity of the orderings of the data when ranked by each of the quantities. It measures the strength of association of the cross tabulated data when both variables are measured at the ordinal level.\nFor 2-way contingincy tables (i.e.¬†2x2 tables)\nIt makes no adjustment for either table size or ties.\nValues range from ‚àí1 (100% negative association, or perfect inversion) to +1 (100% positive association, or perfect agreement). A value of zero indicates the absence of association.\n{DescTools::GoodmanKruskalGamma}",
    "crumbs": [
      "Association",
      "General"
    ]
  },
  {
    "objectID": "qmd/association-general.html#sec-assoc-gen-mix",
    "href": "qmd/association-general.html#sec-assoc-gen-mix",
    "title": "General",
    "section": "Mixed",
    "text": "Mixed\n\nMisc\n\nAlso see\n\nPaper: JEL Ratio Test is non-parametric test that uses the categorical Gini covariance.\n\n{psych::mixedCor} - Finds Pearson correlations for the continous variables, polychorics for the polytomous items, tetrachorics for the dichotomous items, and the polyserial or biserial correlations for the various mixed variables (no polydi?)\n\nBiserial - correlation between a continuous variable and binary variable, which is assumed to have resulted from a dichotomized normal variable\n\n{psych::biserial}\n\nPolydi - correlation between multinomial variable and binary variable\n\n{psych::polydi}\n\nPolyserial - polychoric correlation between a continuous variable and ordinal variable\n\nBased on the assumption that the joint distribution of the quantitative variable and a latent continuous variable underlying the ordinal variable is bivariate normal\n{polycor}\n{psych::polyserial}\n{wCorr} - Pearson, Spearman, polyserial, and polychoric correlations, in weighted or unweighted form\n\nX2Y\n\nHandles types: continuous-continuous, continuous-categorical, categorical-continuous and categorical-categorical\nCalculates the % difference in prediction error after fitting a decision tree between two variables of interest and the mean (numeric) or most frequent (categorical)\nFunction is available through a script (Code &gt;&gt; statistical-testing &gt;&gt; correlation)\n\narticle with documentation and usage, https://rviews.rstudio.com/2021/04/15/an-alternative-to-the-correlation-coefficient-that-works-for-numeric-and-categorical-variables/\n\nAll x2y values where the y variable is continuous will be measuring a % reduction in MAE. All x2y values where the y variable is categorical will be measuring a % reduction in Misclassification Error. Is a 30% reduction in MAE equal to a 30% reduction in Misclassification Error? It is problem dependent, there‚Äôs no universal right answer.\n\nOn the other hand, since (1) all x2y values are on the same 0-100% scale (2) are conceptually measuring the same thing, i.e., reduction in prediction error and (3) our objective is to quickly scan and identify strongly-related pairs (rather than conduct an in-depth investigation), the x2y approach may be adequate.\n\nNot symmetric, but can average both scores to get a pseudo-symmetric value\nBootstrap CIs available\n\nCopulas\n\nlatentcor PKG: semi-parametric latent Gaussian copula models",
    "crumbs": [
      "Association",
      "General"
    ]
  },
  {
    "objectID": "qmd/association-general.html#sec-assoc-gen-nonlin",
    "href": "qmd/association-general.html#sec-assoc-gen-nonlin",
    "title": "General",
    "section": "Non-linear",
    "text": "Non-linear\n\nMisc\n\nAlso see\n\nHoeffding‚Äôs D in Continuous\nGeneral Additive Models &gt;&gt; Diagnostics for a method of determining a nonlinear relationship for either continuous or categorical outcomes.\n\n\nŒæ (xi) coefficient\n\nPaper: A New Coefficient of Correlation\nArticle: Exploring the XI Correlation Coefficient\nExcels at oscillatory and highly non-monotonic dependencies\nXICOR::xicor - calculates Œæ and performs a significance test (H0: independent)\n\nXICOR::calculateXI just calculates the Œæ coefficient\n\nProperties (value ranges; interpretation)\n\nIf y is a function of x, then Œæ goes to 1 asymptotically as n (the number of data points, or the length of the vectors x and y) goes to Infinity.\nIf y and x are independent, then Œæ goes to 0 asymptotically as n goes to Infinity.\n\nValues can be negative, but this negativity does not have any innate significance other than being close to zero\nn &gt; 20 necessary\n\nn larger than about 250 probably sufficient to get a good estimate\n\nFairly efficient (O(nlogn), compared to some more powerful methods, which are O(n2))\nIt measures dependency in one direction only (is y dependent on x not vice versa)\nDoesn‚Äôt tell you if the relationship is direct or inverse",
    "crumbs": [
      "Association",
      "General"
    ]
  },
  {
    "objectID": "qmd/base-r.html",
    "href": "qmd/base-r.html",
    "title": "Base R",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Base R"
    ]
  },
  {
    "objectID": "qmd/base-r.html#sec-baser-misc",
    "href": "qmd/base-r.html#sec-baser-misc",
    "title": "Base R",
    "section": "",
    "text": "Magrittr + base\nmtcars %&gt;% {plot(.$hp, .$mpg)}\nmtcars %$% plot(hp, mpg)\n\nBy wrapping the RHS in curly braces, we can override the rule where the LHS is passed to the first argument ## Options {#sec-baser-opts .unnumbered}\n\nRemove scientific notation\noptions(scipen = 999)\nWide and long printing tibbles\n# in .Rprofile\nmakeActiveBinding(\".wide\", function() { print(.Last.value, width = Inf) }, .GlobalEnv)\n\nAfter printing a tibble, if you want to see it in wide, then just type .wide + ENTER.\nCan have similar bindings for `.long` and `.full`.\n\nHeredocs - Powerful feature in various programming languages that allow you to define a block of text within the code, preserving line breaks, indentation, and other whitespace.\ntext &lt;- r\"(\nThis is a\nmultiline string\nin R)\"\n\ncat(text)",
    "crumbs": [
      "Base R"
    ]
  },
  {
    "objectID": "qmd/base-r.html#sec-baser-usrfuns",
    "href": "qmd/base-r.html#sec-baser-usrfuns",
    "title": "Base R",
    "section": "User Defined Functions",
    "text": "User Defined Functions\n\nAnonymous (aka lambda) functions: \\(x) {} (&gt; R 4.1)\nfunction(x) {\n¬† x[which.max(x$mpg), ]\n}\n# equivalent to the above\n\\(x) {\n¬† x[which.max(x$mpg), ]\n}\nDefine and call an anonymous function at the same time\nn &lt;- c(1:10)\nmoose &lt;- (\\(x) x+1)(n)\nmoose\n#&gt; [1]  2  3  4  5  6  7  8  9 10 11\nDots (‚Ä¶)\n\nMisc\n\n{ellipsis}: Functions for testing functions with dots so they fail loudly\n{rlang} dynamic dots: article\n\nSplice arguments saved in a list with the splice operator, !!! .\nInject names with glue syntax on the left-hand side of := .\n\n\nUser Defined Functions\nmoose &lt;- function(...) {\n¬† ¬† dots &lt;- list(...)\n¬† ¬† dots_names &lt;- names(dots)\n¬† ¬† if (is.null(dots_names) || \"\" %in% dots_names {\n¬† ¬† ¬† ¬† stop(\"All arguments must be named\")\n¬† ¬† }\n}\nNested Functions\nf02 &lt;- function(...){\n  vv &lt;- list(...)\n  print(vv)\n}\nf01 &lt;- function(...){\n  f02(b = 2,...)\n}\n\nf01(a=1,c=3)\n#&gt; $b\n#&gt; [1] 2\n#&gt; \n#&gt; $a\n#&gt; [1] 1\n#&gt; \n#&gt; $c\n#&gt; [1] 3\nSubset dots values\nadd2 &lt;- function(...) {\n¬† ¬† ..1 + ..2\n}\nadd2(3, 0.14)\n# 3.14\nSubset dots dynamically: ...elt(n)\n\nSet a value to n and get back the value of that argument\n\nNumber of arguments in ‚Ä¶ : ...length()",
    "crumbs": [
      "Base R"
    ]
  },
  {
    "objectID": "qmd/base-r.html#sec-baser-funs",
    "href": "qmd/base-r.html#sec-baser-funs",
    "title": "Base R",
    "section": "Functions",
    "text": "Functions\n\ndo.call - allows you to call other functions by constructing the function call as a list\n\nArgs\n\nwhat ‚Äì Either a function or a non-empty character string naming the function to be called\nargs ‚Äì A list of arguments to the function call. The names attribute of args gives the argument names\nquote ‚Äì A logical value indicating whether to quote the arguments\nenvir ‚Äì An environment within which to evaluate the call. This will be most useful if what is a character string and the arguments are symbols or quoted expressions\n\nExample: Apply function to list of vectors\nvectors &lt;- list(c(1, 2, 3), c(4, 5, 6), c(7, 8, 9))\ncombined_matrix &lt;- do.call(rbind, vectors)\n\ncombined_matrix\n##      [,1] [,2] [,3]\n## [1,]    1    2    3\n## [2,]    4    5    6\n## [3,]    7    8    9\nExample: Apply multiple functions\ndata_frames &lt;- list(\n  data.frame(a = 1:3), \n  data.frame(a = 4:6), \n  data.frame(a = 7:9)\n  )\nmean_results &lt;- do.call(\n  rbind, \n  lapply(data_frames, function(df) mean(df$a))\n  )\n\nmean_results\n##      [,1]\n## [1,]    2\n## [2,]    5\n## [3,]    8\n\nFirst the mean is calculated for column a of each df using lapply\n\nlapply is supplying the data for do.call in the required format, which is a list or character vector.\n\nSecond the results are combined into a matrix with rbind\n\n\nsink - used to divert R output to an external connection.\n\nUse Cases: exporting data to a file, logging R output, or debugging R code.\nArgs\n\nfile: The name of the file to which R output will be diverted. If file is NULL, then R output will be diverted to the console.\nappend: A logical value indicating whether R output should be appended to the file (TRUE) or overwritten (FALSE). The default value is FALSE.\ntype: A character string. Either the output stream or the messages stream. The name will be partially match so can be abbreviated.\nsplit: logical: if TRUE, output will be sent to the new sink and the current output stream, like the Unix program tee.\n\nExample: Logging output of code to file\nsink(\"r_output.log\")      # Redirect output to this file\n# Your R code goes here\nsink()                    # Turn off redirection\n\noutput file could also have an extension like ‚Äú.txt‚Äù\n\nExample: Debugging\nsink(\"my_function.log\")   # Redirect output to this file\nmy_function()\nsink()                    # Turn off redirection\nExample: Appending output to a file\nsink(\"output.txt\", append = TRUE)  # Append output to the existing file\ncat(\"Additional text\\n\")  # Append custom text\nplain text\nsink()  # Turn off redirection\n\npmin and pmax\n\nFind the element-wise maximum and minimum values across vectors in R\nExample\nvec1 &lt;- c(3, 9, 2, 6)\nvec2 &lt;- c(7, 1, 8, 4)\npmax(vec1, vec2)\n#&gt; [1] 7 9 8 6\npmin(vec1, vec2)\n#&gt; [1] 3 1 2 4\nExample: With NAs\ndata1 &lt;- c(7, 3, NA, 12)\ndata2 &lt;- c(9, NA, 5, 8)\npmax(data1, data2, na.rm = TRUE)\n#&gt; [1] 9 3 5 12\n\nswitch\n\nExample:\nswitch(parallel,\n         windows = \"snow\" -&gt; para_proc,\n         other = \"multicore\" -&gt; para_proc,\n         no = \"no\" -&gt; para_proc,\n         stop(sprintf(\"%s is not one of the 3 possible parallel argument values. See documentation.\", parallel)))\n\nparallel is the function argument. If it doesn‚Äôt match one of the 3 values, then an error is thrown.\nIf the argument value is matched, then the quoted value is stored in para_proc\n\n\ndynGet\n\nLooks for objects in the environment of a function.\nWhen an object from the outer function is an input for a function nested around 3 layers deep or more, it may not be found by that most inner function. dynGet allows that function to find the object in the outer frame\nArguments\n\nminframe: Integer specifying the minimal frame number to look into (i.e.¬†how far back to look for the object)\ninherits: Should the enclosing frames of the environment be searched?\n\nExample:\n1function(args) {\n  if (method == \"kj\") {\n      ncv_list &lt;- purrr::map2(grid$dat, \n                              grid$repeats, \n                              function(dat, reps) {\n         rsample::nested_cv(dat,\n                            outside = vfold_cv(v = 10, \n                                               repeats = dynGet(\"reps\")),\n                            inside = bootstraps(times = 25))\n      })\n  }\n}\n\n2function(data) {\n    if (chk::vld_used(...)) {\n        dots &lt;- list(...)\n        init_boot_args &lt;-\n          list(data = dynGet(\"data\"),\n               stat_fun = cles_boot, # internal function\n               group_variables = group_variables,\n               paired = paired)\n        get_boot_args &lt;-\n          append(init_boot_args,\n                 dots)\n    }\n    cles_booted &lt;-\n      do.call(\n        get_boot_ci,\n        get_boot_args\n      )\n}\n\n1\n\nExample from Nested Cross-Validation Comparison\n\n2\n\nExample from {ebtools::cles}\n\n\n\nmatch.arg\n\nPartially matches a function‚Äôs argument values to list of choices. If the value doesn‚Äôt match the choices, then an error is thrown\nExample:\nkeep_input &lt;- \"input_le\"\nkeep_input_val &lt;- \n  match.arg(keep_input,\n            choices = c(\"input_lags\",\n                        \"input_leads\",\n                        \"both\"),\n            several.ok = FALSE)\nkeep_input_val\n#&gt; [1] \"input_leads\"\n\nseveral.ok = FALSE says only 1 match is allowed otherwise an error is thrown.\nThe error message is pretty informative btw.\n\n\nmatch.fun\n\nExample\nf &lt;- function(a,b) {\n  a + b\n}\ng &lt;- function(a,b,c) {\n  (a + b) * c\n}\nh &lt;- function(d,e) {\n  d - e\n}\nyolo &lt;- function(FUN, ...) {\n  FUN &lt;- match.fun(FUN)\n  params &lt;- list(...)\n  FUN_formals &lt;- formals(FUN)\n  idx &lt;- names(params) %in% names(FUN)\n  do.call(FUN, params[idx])\n}\nyolo(h, d = 2, e = 3)\n#&gt; -1",
    "crumbs": [
      "Base R"
    ]
  },
  {
    "objectID": "qmd/base-r.html#sec-baser-pipe",
    "href": "qmd/base-r.html#sec-baser-pipe",
    "title": "Base R",
    "section": "Pipe",
    "text": "Pipe\n\n\nBenefits of base pipe\n\nMagrittr pipe is bloated with special features which may make it slower than the base pipe\nIf not using tidyverse, it‚Äôs one less dependency (maybe one day it will be deprecated in tidyverse)\n\nBase pipe with base and anonymous functions\n# verbosely\nmtcars |&gt; (function(.) plot(.$hp, .$mpg))()\n# using the anonymous function shortcut, emulating the dot syntax\nmtcars |&gt; (\\(.) plot(.$hp, .$mpg))()\n# or if you prefer x to .\nmtcars |&gt; (\\(x) plot(x$hp, x$mpg))()\n# or if you prefer to be explicit with argument names\nmtcars |&gt; (\\(data) plot(data$hp, data$mpg))()\nUsing ‚Äú_‚Äù placeholder:\n\nmtcars |&gt; lm(mpg ~ disp, data = _)\nmtcars |&gt; lm(mpg ~ disp, data = _) |&gt; _$coef\n\nBase pipe .[ ]¬† hack\nwiki |&gt;\n¬† read_html() |&gt;\n¬† html_nodes(\"table\") |&gt;\n¬† (\\(.) .[[2]])() |&gt;\n¬† html_table(fill = TRUE) |&gt;\n¬† clean_names()\n# instead of\ndjia &lt;- wiki %&gt;%\n¬† read_html() %&gt;%\n¬† html_nodes(\"table\") %&gt;%\n¬† .[[2]] %&gt;%\n¬† html_table(fill = TRUE) %&gt;%\n¬† clean_names()\nMagrittr, base pipe differences\n\nmagrittr: %&gt;% allows you change the placement with a . placeholder.\n\nbase: R 4.2.0 added a _ placeholder to the base pipe, with one additional restriction: the argument has to be named\n\nmagrittr: With %&gt;% you can use . on the left-hand side of operators like ‚Äú\\(\", \\[\\[, \\[ and use in multiple arguments (e.g. df %&gt;% {split(.\\)x, .$y))\n\nbase: can hack this by using anonymous function\n\nsee Base pipe with base and anonymous functions above\nsee Base pipe .[ ]¬† hack above\n\n\nmagrittr: %&gt;% allows you to drop the parentheses when calling a function with no other arguments (e.g.¬†dat %&gt;% distinct)\n\nbase: |&gt; always requires the parentheses. (e.g.¬†dat |&gt; distinct())\n\nmagrittr: %&gt;% allows you to start a pipe with . to create a function rather than immediately executing the pipe\n\nPurrr with base pipe\ndata_list |&gt;\n¬† map(\\(x) clean_names(x))\n# instead of\ndata_list %&gt;%\n¬† map( ~.x %&gt;% clean_names)\n# with split\nstar |&gt;\n¬† split(~variable) |&gt;\n¬† map_df(\\(.) hedg_g(., reading ~ value), .id = \"variable\")\n# instead of\nstar %&gt;%\n¬† split(.$variable) %&gt;%\n¬† map_df(. %&gt;% hedg_g(., reading ~ value), .id = \"variable\")",
    "crumbs": [
      "Base R"
    ]
  },
  {
    "objectID": "qmd/base-r.html#sec-baser-str",
    "href": "qmd/base-r.html#sec-baser-str",
    "title": "Base R",
    "section": "Strings",
    "text": "Strings\n\nsprintf\nx &lt;- 123.456               # Create example data\n\nsprintf(\"%f\", x)           # sprintf with default specification\n#&gt; [1] \"123.456000\"\n\nsprintf(\"%.10f\", x)        # sprintf with ten decimal places\n#&gt; [1] \"123.4560000000\"\n\nsprintf(\"%.2f\", x)         # sprintf with two rounded decimal places\n#&gt; [1] \"123.46\"\n\nsprintf(\"%1.0f\", x)        # sprintf without decimal places\n#&gt; [1] \"123\"\n\nsprintf(\"%10.0f\", x)       # sprintf with space before number\n#&gt; [1] \"       123\"\n\nsprintf(\"%10.1f\", x)       # Space before number & decimal places\n#&gt; [1] \"     123.5\"\n\nsprintf(\"%-15f\", x)        # Space on right side\n#&gt; [1] \"123.456000     \"\n\nsprintf(\"%+f\", x)          # Print plus sign before number\n#&gt; [1] \"+123.456000\"\n\nsprintf(\"%e\", x)           # Exponential notation\n#&gt; [1] \"1.234560e+02\"\n\nsprintf(\"%E\", x)           # Exponential with upper case E\n#&gt; [1] \"1.234560E+02\"\n\nsprintf(\"%g\", x)           # sprintf without decimal zeros\n#&gt; [1] \"123.456\"\n\nsprintf(\"%g\", 1e10 * x)    # Scientific notation\n#&gt; [1] \"1.23456e+12\"\n\nsprintf(\"%.13g\", 1e10 * x) # Fixed decimal zeros\n#&gt; [1] \"1234560000000\"\n\npaste0(sprintf(\"%f\", x),   # Print %-sign at the end of number\n       \"%\")\n#&gt; [1] \"123.456000%\"\n\nsprintf(\"Let's create %1.0f more complex example %1.0f you.\", 1, 4)\n#&gt; [1] \"Let's create 1 more complex example 4 you.\"\nstr2lang - Allows you to turn plain text into code.\ngrowth_rate &lt;- \"circumference / age\"\nclass(str2lang(growth_rate))\n#&gt; [1] \"call\"\n\nExample: Basic\neval(str2lang(\"2 + 2\"))\n#&gt; [1] 4\n\neval(str2lang(\"x &lt;- 3\"))\nx\n#&gt; [1] 3\nExample: Run formula against a df\ngrowth_rate &lt;- \"circumference / age\"\nwith(Orange, eval(str2lang(growth_rate)))\n\n#&gt;   [1] 0.25423729 0.11983471 0.13102410 0.11454183 0.09748172 0.10349854\n#&gt;   [7] 0.09165613 0.27966102 0.14256198 0.16716867 0.15537849 0.13972380\n#&gt;  [13] 0.14795918 0.12831858 0.25423729 0.10537190 0.11295181 0.10756972\n#&gt;  [19] 0.09341998 0.10131195 0.08849558 0.27118644 0.12809917 0.16867470\n#&gt;  [25] 0.16633466 0.14541024 0.15233236 0.13527181 0.25423729 0.10123967\n#&gt;  [31] 0.12198795 0.12450199 0.11535337 0.12682216 0.11188369",
    "crumbs": [
      "Base R"
    ]
  },
  {
    "objectID": "qmd/base-r.html#sec-baser-cond",
    "href": "qmd/base-r.html#sec-baser-cond",
    "title": "Base R",
    "section": "Conditionals",
    "text": "Conditionals\n\n&& and || are intended for use solely with scalars, they return a single logical value.\n\nSince they always return a scalar logical, you should use && and || in your if/while conditional expressions (when needed). If an & or | is used, you may end up with a non-scalar vector inside if (‚Ä¶) {} and R will throw an error.\n\n& and | work with multivalued vectors, they return a vector whose length matches their input arguments.\nAlternative way of negating a condition or set of conditions: if (!(condition))\n\nMakes it less readable IMO, but maybe for a complicated set of conditions if makes more sense in your head to do it this way\nExample\nif (!(nr == nrow(iris) || (nr == nrow(iris) - 2))) {print(\"moose\")}\n\nUsing else if\nif (condition1) {\n  expr1\n} else if (condition2) {\n  expr2\n} else {\n  expr3\n}\nstopifnot\npred_fn &lt;- function(steps_forward, newdata) {\n¬† stopifnot(steps_forward &gt;= 1)\n¬† stopifnot(nrow(newdata) == 1)\n¬† model_f = model_map[[steps_forward]]\n¬† # apply the model to the last \"before the test period\" row to get\n¬† # the k-steps_forward prediction\n¬† as.numeric(predict(model_f, newdata = newdata))\n}\n%||%\n\nCollapse operator which acts like:\n`%||%` &lt;- function(x, y) {\n   if (is_null(x)) y else x\n}\n\nSays if the first (left-hand) input x is NULL, return y. If x is not NULL, return the input\n\nUse Cases\n\nDetermine whether a function argument is NULL\ngithub_remote &lt;- \n  function(repo, username = NULL, ...) {\n    meta &lt;- parse_git_repo(repo)\n    meta$username &lt;- username %||%\n      getOption(\"github.user\") %||%\n      stop(\"Unknown username\")\n  }\nWithin the print argument collapse\nlibrary(rlang)\n\nadd_commas &lt;- function(x) {\n  if (length(x) &lt;= 1) {\n    collapse_arg &lt;- NULL\n  } else {\n    collapse_arg &lt;- \", \"\n  }\n  print(paste0(x, collapse = collapse_arg %||% \"\"))\n}\n\nadd_commas(c(\"apples\"))\n[1] \"apples\"\nadd_commas(c(\"apples\", \"bananas\"))\n[1] \"apples, bananas\"",
    "crumbs": [
      "Base R"
    ]
  },
  {
    "objectID": "qmd/base-r.html#sec-baser-sort",
    "href": "qmd/base-r.html#sec-baser-sort",
    "title": "Base R",
    "section": "Ordering Columns and Sorting Rows",
    "text": "Ordering Columns and Sorting Rows\n\nAscending:\ndf[with(df, order(col2)), ]\n# or\ndf[order(df$col2), ]\n\ncol2 is the column used to sort the dfby\n\nDescending: df[with(df, order(-col2)), ]\nBy Multiple Columns\n\nDescending then Ascending: df[with(df, order(-col2, id)), ]\n\nChange position of columns\n# Reorder column by index manually\ndf2 &lt;- df[, c(5, 1, 4, 2, 3)]\ndf3 &lt;- df[, c(1, 5, 2:4)]\n# Reorder column by name manually\nnew_order = c(\"emp_id\",\"name\",\"superior_emp_id\",\"dept_id\",\"dept_branch_id\")\ndf2 &lt;- df[, new_order]",
    "crumbs": [
      "Base R"
    ]
  },
  {
    "objectID": "qmd/base-r.html#set-operations",
    "href": "qmd/base-r.html#set-operations",
    "title": "Base R",
    "section": "Set Operations",
    "text": "Set Operations\n\nUnique values in A that are not in B\na &lt;- c(\"thing\", \"object\")\nb &lt;- c(\"thing\", \"gift\")\n\nunique(a[!(a %in% b)])\n#&gt; [1] \"object\"\n\nsetdiff(a, b)\n\nsetdiff is slower\n\nUnique values of the two vectors combined\nunique(c(a, b))\n#&gt; [1] \"thing\"  \"object\" \"gift\"\n\nunion(a, b)\n\nunion is just a wrapper for unique",
    "crumbs": [
      "Base R"
    ]
  },
  {
    "objectID": "qmd/base-r.html#sec-baser-subset",
    "href": "qmd/base-r.html#sec-baser-subset",
    "title": "Base R",
    "section": "Subsetting",
    "text": "Subsetting\n\nLists and Vectors\n\nRemoving Rows\n# Remove specific value from vector\nx[!x == 'A']\n\n# Remove multiple values by list\nx[!x %in% c('A', 'D', 'E')]\n\n# Using setdiff\nsetdiff(x, c('A','D','E'))\n\n# Remove elements by index\nx[-c(1,2,5)]\n\n# Using which\nx[-which(x %in% c('D','E') )]\n\n# Remove elements by name\nx &lt;- c(C1='A',C2='B',C3='C',C4='E',C5='G')\nx[!names(x) %in% c('C1','C2')]\n\nDataframes\n\nRemove specific Rows\ndf &lt;- df[-c(25, 3, 62), ]\nRemove column by name\ndf &lt;- df[, which(names(df) == \"col_name\")]\ndf &lt;- subset(df, select = -c(col_name))\ndf &lt;- df[, !names(df) %in% c(\"col1\", \"col2\"), drop = FALSE]\nFilter and Select\ndf &lt;- subset(df, subset = col1 &gt; 56, select = c(col2, col3))",
    "crumbs": [
      "Base R"
    ]
  },
  {
    "objectID": "qmd/base-r.html#joins",
    "href": "qmd/base-r.html#joins",
    "title": "Base R",
    "section": "Joins",
    "text": "Joins\n\nInner join: inner &lt;- merge(flights, weather, by = mergeCols)\nLeft (outer) join: left  &lt;- merge(flights, weather, by = mergeCols, all.x = TRUE)\nRight (outer) join: right &lt;- merge(flights, weather, by = mergeCols, all.y = TRUE)\nFull (outer) join: full &lt;- merge(flights, weather, by = mergeCols, all = TRUE)\nCross Join (Cartesian product): cross &lt;- merge(flights, weather, by = NULL)\nNatural join: natural &lt;- merge(flights, weather)",
    "crumbs": [
      "Base R"
    ]
  },
  {
    "objectID": "qmd/base-r.html#sec-baser-err",
    "href": "qmd/base-r.html#sec-baser-err",
    "title": "Base R",
    "section": "Error Handling",
    "text": "Error Handling\n\nstop\n\nExample:\nswitch(parallel,\n       windows = \"snow\" -&gt; para_proc,\n       other = \"multicore\" -&gt; para_proc,\n       no = \"no\" -&gt; para_proc,\n       stop(sprintf(\"%s is not one of the 3 possible parallel argument values. See documentation.\", parallel)))\n\nparallel is the function argument. If it doesn‚Äôt match one of the 3 values, then an error is thrown.\n\n\ntry\n\nIf something errors, then do something else\nExample\ncurrent &lt;- try(remDr$findElement(using = \"xpath\",\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† '//*[contains(concat( \" \", @class, \" \" ),\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† concat( \" \", \"product-price-value\", \" \" ))]'),\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† silent = T)\n#If error : current price is NA\nif(class(current) =='try-error'){\n¬† ¬† currentp[i] &lt;- NA\n} else {\n¬† ¬† # do stuff\n}\n\ntryCatch\n\nRun the main code, but if it ‚Äúcatches‚Äù an error, then the secondary code (the workaround) will run.\nExample: Basic\nfor (r in 1:nrow(res)) {\n  cat(r, \"\\n\")\n\n  tmp_wikitext &lt;- get_wikitext(res$film[r], res$year[r])\n\n  # skip if get_wikitext fails\n  if (is.na(tmp_wikitext)) next\n  if (length(tmp_wikitext) == 0) next\n\n  # give the text to openai\n  tmp_chat &lt;- tryCatch(\n    get_results(client, tmp_wikitext),\n    error = \\(x) NA\n  )\n\n  # if openai returned a dict of 2\n  if (length(tmp_chat) == 2) {\n    res$writer[r] &lt;- tmp_chat$writer\n    res$producer[r] &lt;- tmp_chat$producer\n  }\n}\n\nget_results is called during each iteration, and if there‚Äôs an error a NA is returned.\n\nExample\npct_difference_error_handling &lt;- function(n1, n2) {\n# Try the main code\n¬† tryCatch(pct_diff &lt;- (n1-n2)/n1,\n¬† ¬† ¬† ¬† # If you find an error, use this code instead\n¬† ¬† ¬† ¬† ¬† error = return(\n¬† ¬† ¬† ¬† ¬† ¬† cat( 'The difference between', as.integer(n1), 'and', as.integer(n2), 'is',\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† (as.integer(n1)-as.integer(n2)), 'which is',\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† 100*(as.integer(n1)-as.integer(n2))/as.integer(n1),\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† '% of', n1 )#cat\n¬† ¬† ¬† ¬† ¬† ¬† ),\n¬† ¬† ¬† ¬† ¬† # finally = print('Code ended') # optional\n¬† ¬† ¬† ¬† ¬† )#trycatch\n¬† # If no error happens, return this statement\n¬† return ( cat('The difference between', n1, 'and', n2, 'is', n1-n2,\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ', which is', pct_diff*100, '% of', n1) )\n}\n\nAssumes the error will be the user enters a string instead of a numeric. If errors, converts string to numeric and calcs.\n‚Äúfinally‚Äù - This argument will always run, regardless if the try block raises an error or not. So it could be a completion message or a summary, for example.",
    "crumbs": [
      "Base R"
    ]
  },
  {
    "objectID": "qmd/base-r.html#sec-baser-mods",
    "href": "qmd/base-r.html#sec-baser-mods",
    "title": "Base R",
    "section": "Models",
    "text": "Models\n\nreformulate - Create formula sytax programmatically\n# Creating a formula using reformulate()\nformula &lt;- reformulate(c(\"hp\", \"cyl\"), response = \"mpg\")\n\n# Fitting a linear regression model\nmodel &lt;- lm(formula, data = mtcars)\n\nformula\n##&gt; mpg ~ hp + cyl\n\nCan also use as.formula\n\nDF2formula - Turns the column names from a data frame into a formula. The first column will become the outcome variable, and the rest will be used as predictors\nDF2formula(Orange)\n#&gt; Tree ~ age + circumference\nformula - Provides a way of extracting formulae which have been included in other objects\nrec_obj |&gt; prep() |&gt; formula()\n\nWhere ‚Äúrec_obj‚Äù is a tidymodels recipe object\n\nData from Model Object\n\nmodel$model return the data dataframe\ndeparse(model$call$data) gives you that name of your data object as a string.\n\nmodel$call$data gives you the data as an unevaluated symbol;\n\neval(model$call$data) gives you back the original data object, if it is available in the current environment.",
    "crumbs": [
      "Base R"
    ]
  },
  {
    "objectID": "qmd/privacy.html",
    "href": "qmd/privacy.html",
    "title": "Privacy",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Privacy"
    ]
  },
  {
    "objectID": "qmd/privacy.html#sec-priv-misc",
    "href": "qmd/privacy.html#sec-priv-misc",
    "title": "Privacy",
    "section": "",
    "text": "Also see Simulation, Data\nPackages\n\n{xxhashlite} - Very fast hash functions using xxHash",
    "crumbs": [
      "Privacy"
    ]
  },
  {
    "objectID": "qmd/privacy.html#sec-priv-tag",
    "href": "qmd/privacy.html#sec-priv-tag",
    "title": "Privacy",
    "section": "Tags",
    "text": "Tags\n\nTag sensitive information in dataframes\nnames(df)\n[1] \"date\" \"first_name\" \"card_number\" \"payment\"\n# assign pii tags\nattr(df, \"pii\") &lt;- c(\"name\", \"ccn\", \"transaction\")\n\nPersonally Identifiable Information (PII)\n\nTag dataframes with the names of regulations that are applicable\nattr(df, \"regs\") &lt;- c(\"CCPA\", \"GDPR\", \"GLBA\")\n\nCCPA is the privacy regulation for California\nGDPR is the privacy regulation for the European Union\nGLBA is the financial regulation for the United States\n\nNeeded because df has credit card and financial information\n\nSaving objects as .rds files preserves tags",
    "crumbs": [
      "Privacy"
    ]
  },
  {
    "objectID": "qmd/privacy.html#sec-priv-hash",
    "href": "qmd/privacy.html#sec-priv-hash",
    "title": "Privacy",
    "section": "Hashing",
    "text": "Hashing\n\n{digest}\n\nHash Function\n\nApply Hash Function to PII Fields",
    "crumbs": [
      "Privacy"
    ]
  },
  {
    "objectID": "qmd/cli-linux.html#debugging",
    "href": "qmd/cli-linux.html#debugging",
    "title": "Linux",
    "section": "Debugging",
    "text": "Debugging\n\n\nAlso see set -o xtrace in Scripting &gt;&gt; Commands that should start your script",
    "crumbs": [
      "CLI",
      "Linux"
    ]
  },
  {
    "objectID": "qmd/regression-survival.html",
    "href": "qmd/regression-survival.html",
    "title": "Survival",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Regression",
      "Survival"
    ]
  },
  {
    "objectID": "qmd/regression-survival.html#sec-reg-surv-misc",
    "href": "qmd/regression-survival.html#sec-reg-surv-misc",
    "title": "Survival",
    "section": "",
    "text": "Model for estimating the time until a particular event occurs\n\ne.g.¬†death of a patient being treated for a disease, failure of an engine part in a vehicle\n\nPrediction models for survival outcomes are important for clinicians who wish to estimate a patient‚Äôs risk (i.e.¬†probability) of experiencing a future outcome. The term ‚Äòsurvival‚Äô outcome is used to indicate any prognostic or time-to-event outcome, such as death, progression, or recurrence of disease. Such risk estimates for future events can support shared decision making for interventions in high-risk patients, help manage the expectations of patients, or stratify patients by disease severity for inclusion in trials.1 For example, a prediction model for persistent pain after breast cancer surgery might be used to identify high risk patients for intervention studies\nOutcome variable: Time until event occurs\nPackages\n\nCRAN Task View\n{survival}\n{censored} - {tidymodels} for censored and survival modelling\n\nML and regression models from partykit, glmnet, aorsf, and survival\nCase Study: How long until building complaints are dispositioned?\n\n{quantreg} - Quantile Survival Regression\n{msm} - Multi-State Models\n\nVignette\nSee Multistate Models for Medical Applications\n\nTutorial using a heart transplant dataset\n\nStandard survival models only directly model two states: alive and dead. Multi-state models enable directly modeling disease progression where patients are observed to be in various states of health or disease at random intervals, but for which, except for death, the times of entering or leaving states are unknown.\nMulti-state models easily accommodate interval censored intermediate states while making the usual assumption that death times are known but may be right censored.\n\n{grf} - Generalized Random Forest; Causal forest with time-to-event data\n{partykit} - Conditional inference trees; Model-based recursive partitioning trees; can be used with {survival} to create random survival forests\n\n{bonsai}: tidymodels, partykit conditional trees, forests; successor to treesnip ‚Äî Model Wrappers for Tree-Based Models\n\n{aorsf} - Optimized software to fit, interpret, and make predictions with oblique random survival forests (ORSFs)\n{{sklearn}} - Random Survival Forests, Survival Support Vector Machine\n{rmstbart} - Prognostic model that directly targets the RMST (See Terms) as a function of baseline covariates. The interpretation of each estimated RMST is transparent and does not require a proportional hazards assumption plus additional modeling of a baseline hazard function.\n\nFrom paper: Generalized Bayesian Additive Regression Trees for Restricted Mean Survival Time Inference. (Code)\n\n{survAH} - Performs two-sample comparisons based on average hazard with survival weight (AHSW). (See Terms)\n\nNotes from\n\nWhat is Cox‚Äôs proportional hazards model?\n\nWhy not use a standard regression model?\n\nUnits that ‚Äúsurvive‚Äù until the end of the study will have a censored survival time.\n\ni.e.¬†We won‚Äôt have an observed survival time for these units because they survive for an unknown time after the study is completed.\nWe don‚Äôt want to discard these units though, as they still have useful information.\n\n\nSample Size\nModels\n\nKaplan Meier model (i.e.¬†K-M survival curve)\n\nOften used as a baseline in survival analysis\nCan not be used to compare risk between groups and compute metrics like the hazard ratio\n\nExponential model, the Weibull model, Cox Proportional-Hazards, Log-logistic and the Accelerated Failure Time (AFT)\nMulti-State Models\nHazard rates and Cumulative Hazard rates are typical quantities of interest\n\nLog-Rank Test (aka Mantel-Cox test) - tests if two groups survival curves are different\n\nNon-Parametric; a special case with one binary X\nThe intuition behind the test is that if the two groups have different hazard rates, the two survival curves (so their slopes) will differ.\nCompares the observed number of events in each group to what would be expected if the survival curves were identical (i.e., if the null hypothesis were true).\nExample\nlibrary(survival)\ndat &lt;- data.frame(\n¬† group = c(rep(1, 6), rep(2, 6)),\n¬† time = c(4.1, 7.8, 10, 10, 12.3, 17.2, 9.7, 10, 11.1, 13.1, 19.7, 24.1),\n¬† event = c(1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0)\n)\ndat\n##¬† ¬† group time event\n## 1¬† ¬† ¬† 1¬† 4.1¬† ¬† 1\n## 2¬† ¬† ¬† 1¬† 7.8¬† ¬† 0\n## 3¬† ¬† ¬† 1 10.0¬† ¬† 1\n## 4¬† ¬† ¬† 1 10.0¬† ¬† 1\n## 5¬† ¬† ¬† 1 12.3¬† ¬† 0\n## 6¬† ¬† ¬† 1 17.2¬† ¬† 1\n## 7¬† ¬† ¬† 2¬† 9.7¬† ¬† 1\n## 8¬† ¬† ¬† 2 10.0¬† ¬† 1\n## 9¬† ¬† ¬† 2 11.1¬† ¬† 0\n## 10¬† ¬† 2 13.1¬† ¬† 0\n## 11¬† ¬† 2 19.7¬† ¬† 1\n## 12¬† ¬† 2 24.1¬† ¬† 0\nsurvdiff(Surv(time, event) ~ group,\n¬† data = dat\n)\n##¬† ¬† ¬† ¬† N Observed Expected (O-E)^2/E (O-E)^2/V\n## group=1 6¬† ¬† ¬† ¬† 4¬† ¬† 2.57¬† ¬† 0.800¬† ¬† ¬† 1.62\n## group=2 6¬† ¬† ¬† ¬† 3¬† ¬† 4.43¬† ¬† 0.463¬† ¬† ¬† 1.62\n##¬†\n##¬† Chisq= 1.6¬† on 1 degrees of freedom, p= 0.2\n\n# plot curves with pval from test\nfit &lt;- survfit(Surv(time, event) ~ group, data = dat)\nggsurvplot(fit,\n¬† pval = TRUE,\n¬† pval.method = TRUE\n)\n\npval &gt; 0.05, so there isn‚Äôt enough evidence to that they‚Äôre different.",
    "crumbs": [
      "Regression",
      "Survival"
    ]
  },
  {
    "objectID": "qmd/regression-survival.html#sec-reg-surv-terms",
    "href": "qmd/regression-survival.html#sec-reg-surv-terms",
    "title": "Survival",
    "section": "Terms",
    "text": "Terms\n\nAverage Hazard With Survival Weight (AH) aka Censoring-Free Incidence Rate (CFIR): Alternative to RMST that‚Äôs based on a statistical measure called person-time incidence rate, which factors in the period that patients are potentially tracked in a clinical trial. (See vignette for {survAH} an example and further details)\n\n‚ÄúIf you follow 100 patients for a year ‚Äî which would be 100 years of person-time ‚Äî we divide that into the number of times we would observe an event such as a patient‚Äôs death or cancer recurrence over that period. We can calculate this for each group of patients in the trial with the effect of lost-to-follow-up being removed.‚Äù\n\nPerson-Time Incidence Rate is defined by the ratio of the total number of observed events and the total person-time of exposure.\n\nAH-based tests can be more powerful than the log-rank test and the standard RMST-based tests in detecting delayed treatment effects.\nFormula\n\\[\n\\hat{\\eta}(\\tau) =\\frac{1-\\hat{S}(\\tau)}{\\int_0^\\tau \\hat{S}(t)dt} = \\frac{\\mathbb{E}[I(T \\le \\tau)]}{\\mathbb{E}[T\\wedge \\tau]}\n\\]\n\n\\(\\hat S(\\tau)\\) is the Kaplan-Meier estimator\nLast Term: The ratio of the expected total number of events we observe by \\(\\tau\\) and the expected total observation time by \\(\\tau\\) when there is no censoring until \\(\\tau\\).\nDifference in AH: \\(\\hat \\eta_1 - \\hat \\eta_0\\)\nRatio of AH: \\(\\frac{\\hat \\eta_1}{\\hat \\eta_0}\\)\n\nLong-Term Average Hazard\n\\[\n\\hat{\\eta}_k(\\tau_1, \\tau_2) = \\frac{\\hat S_k(\\tau_2) + \\hat S_k(\\tau_1)}{\\int_0^{\\tau_2} \\hat{S}(t)dt - \\int_0^{\\tau_1} \\hat{S}(t)dt} = \\frac{\\mathbb{E}[I(\\tau_1 \\lt T \\le \\tau_2)]}{\\mathbb{E}[T\\wedge \\tau_2] - \\mathbb{E}[T\\wedge \\tau_1]}\n\\]\n\nLast Term: The numerator is the probability of having an event between \\((\\tau_1,\\tau_2)\\), and the denominator is the expected time of being alive between \\((\\tau_1,\\tau_2)\\).¬†\n\nThus, \\(\\eta_k (\\tau_1,\\tau_2)\\) can be interpreted as an average intensity (think rate) of having an event over the time window \\((\\tau_1,\\tau_2)\\).\n\nPaper shows this metric having more power than AH or LT-RMST for delayed treatment effect situations such as immunotherapy trials.\n\n\nCensoring Time (C): Time at which censoring occurs\n\nFor each unit, we observe Survival Time (T) or C: \\(Y = \\min(T, C)\\)\nRight Censoring: occurs when the event has happened after the enrollment (but the time is unknown).\n\nThe patient does not experience the event for the whole duration of the study.\nThe patient withdraws from the study.\nThe patient is lost to follow-up.\n\nLeft Censoring: occurs when the event has happened before the enrollment (but the time is unknown).\n\nCumulative hazard function (aka Cumulative Hazard Rates)\n\nShows the total accumulated risk of an event occurring at time t\nThe area under the hazard function\n\nHazard Rate (aka Risk Score), \\(h(t \\;|\\; X)\\)\n\nThe hazard rate is the probability that a unit with predictors, \\(X\\), will experience an event at time, \\(t\\), given that the unit has survived just before time, \\(t\\).\nThe formula for the Hazard Rate is the Hazard function.\n\nHazard Ratio (aka Relative Risk of an event): Risk of an event given category / risk of an event given by reference category\n\nThe ratio of two instantaneous event rates\nCoefficient of the Cox Proportional Hazards model (e.g.¬†paper)\n\n\n\\(e^\\beta &gt; 1\\) (or \\(\\beta &gt; 0\\)) for an increased risk of event (e.g.¬†death).\n\\(e^\\beta &lt; 1\\) (or \\(\\beta &lt; 0\\)) for a reduced risk of event.\nHR of 2 is equivalent to raising the entire survival curve for a control subject to the second power to get the survival curve for an exposed subject\n\nExample: If a control subject has 5yr survival probability of 0.7 and the exposed:control HR is 2, the exposed subject has a 5yr survival probability of 0.49\nIf the HR is 1/2, the exposed subject has a survival curve that is the square root of the control, so \\(S(5)\\) would be \\(\\sqrt{0.7} = 0.837\\)\n\n\n\nRestricted Mean Survival Time (RMST) - The expected survival duration up to a pre-specified truncation time, \\(\\tau\\). It has direct clinical interpretation, which holds regardless of the survival model used in an analysis. Changes in RMST are often cited as an alternative to hazard ratios in the context of treatment comparisons and in survival regression modeling.\n\nUnlike proportional hazards, the interpretation of change in RMST holds regardless of whether or not a particular survival regression model holds.\nSee Restricted Mean Survival Time for Survival Analysis: A Quick Guide for Clinical Researchers\nStatistical comparisons based on the standard RMST provides lower power than the conventional log-rank test when detecting delayed treatment effects (e.g.¬†immunotherapy trials). (paper)\n\nSolution: Long-Term (aka Windowed) RMST which defines the lower limit of the definite integral at the point where the K-M curves separate, \\(\\eta\\), instead of 0, and keeps the upper limit at \\(\\tau\\).\nAlso see LT-AH\n\n\nStatus indicator, \\(\\delta\\)\n\n\\(\\delta = 1\\), if \\(T \\le C\\) (e.g.¬†unit fails before study ends)\n\nTrue survival time is observed\n\n\\(\\delta = 0\\), if \\(T \\gt C\\) (e.g.¬†unit survives until end of study or has dropped out)\n\nCensoring time is observed\n\n\nSurvival function (aka Survival Rate), \\(S(T \\lt t)\\):\n\nOutputs the probability of a subject surviving (i.e., not experiencing the event) beyond time t\nMonotonically decreasing (i.e.¬†level or decreasing)\nBaseline survival curve illustrates the survival function when all the covariates are set to their median value\n\nSurvival Time (T) (aka Death, Failure Time, Event Time): Time at which the event occurs",
    "crumbs": [
      "Regression",
      "Survival"
    ]
  },
  {
    "objectID": "qmd/regression-survival.html#sec-reg-surv-km",
    "href": "qmd/regression-survival.html#sec-reg-surv-km",
    "title": "Survival",
    "section": "Kaplan-Meir",
    "text": "Kaplan-Meir\n\nMisc\n\nUseful for validation of Proportional Hazards assumption. When lines cross the assumption, hazards are found to be non-proportional.\nHarrell RMS (Ch. 20.3):\n\nFor external validation: at least 200 events\nNeed 184 subjects with an event, or censored late, to estimate to within a margin of error of 0.1 everywhere, at the 0.95 confidence level\n\n\nOrder event times (\\(T\\)) of units from smallest to largest, \\(t_1 \\lt .... \\lt t_k\\)\nCalculate probability that a unit survives past event time, \\(t_i\\), given that they survived up until event time, \\(t_i\\) (i.e.¬†past \\(t_{i-1}\\)) (conditional probability)\n\ne.g.¬†for \\(t_1\\), it‚Äôs \\(\\frac{(n_1 - d_1)}{n_1}\\)\n\n\\(n_1\\) is the number of units that have survived at \\(t_1\\)\n\\(d_1\\) is the number of units that have experienced the event (e.g.¬†died) at \\(t_1\\)\nSimilar for other \\(t\\) values\n\nMedian survival time is where the survival probability equals 0.5\n\nSurvival function\n\\[\nS(t) = \\prod_{i\\;:\\;t_t \\le t} \\left(1- \\frac{d_i}{n_i}\\right)\n\\]\n\nThe survival function computes the products of these probabilities resulting in the K-M survival curve\nThe product of these conditional probabilities reflects the fact that to survive past event time, \\(t\\), a unit must have survived all previous event times and the current event time.\n\nExample: 50 patients\n\n\nDotted lines represent \\(95\\%\\) CI\nRed dots indicate time when patients died (aka event times)\nMedian survival time is ~ 13 yrs",
    "crumbs": [
      "Regression",
      "Survival"
    ]
  },
  {
    "objectID": "qmd/regression-survival.html#sec-reg-surv-exp",
    "href": "qmd/regression-survival.html#sec-reg-surv-exp",
    "title": "Survival",
    "section": "Exponential",
    "text": "Exponential\n\nAssumes that the hazard rate is constant\n\ni.e.¬†Risk of the event of interest occurring remains the same throughout the period of observation\n\nSurvival function\n\\[\nS(t) = e^{-\\frac{t}{\\lambda}}\n\\]\nHazard function\n\\[\nh(t) = \\frac{1}{\\lambda}\n\\]\n\n\\(h(t)\\) is the constand hazard rate\n\nEstimated parameter: \\(\\lambda\\)",
    "crumbs": [
      "Regression",
      "Survival"
    ]
  },
  {
    "objectID": "qmd/regression-survival.html#sec-reg-surv-weibull",
    "href": "qmd/regression-survival.html#sec-reg-surv-weibull",
    "title": "Survival",
    "section": "Weibull",
    "text": "Weibull\n\nAssumes the change in hazard rate is linear.\nSurvival function\n\\[\nS(t) = e^{-(\\frac{t}{\\lambda})^\\rho}\n\\]\nHazard function\n\\[\nh(t) = \\frac{\\rho}{\\lambda}\\cdot \\left(\\frac{t}{\\lambda}\\right)^{\\rho - 1}\n\\]\nEstimated parameters: \\(\\lambda\\) and \\(\\rho\\)\n\n\\(\\lambda\\) parameter indicates how long it takes for 63.2% of the subjects to experience the event.\n\\(\\rho\\) parameter indicates whether the hazard rate is increasing, decreasing, or constant.\n\nIf \\(\\rho\\) is greater than 1, the hazard rate is constantly increasing.\nIf \\(\\rho\\) is less than 1, the hazard rate is constantly decreasing.",
    "crumbs": [
      "Regression",
      "Survival"
    ]
  },
  {
    "objectID": "qmd/regression-survival.html#sec-reg-surv-coxph",
    "href": "qmd/regression-survival.html#sec-reg-surv-coxph",
    "title": "Survival",
    "section": "Cox‚Äôs Proportional Hazards",
    "text": "Cox‚Äôs Proportional Hazards\n\nMultivariable regression model\nAllows the hazard rate to fluctuate\nHarrell: ‚Äúunder PH assumption and absence of covariate interactions, HR is a good overall effect estimate for binary treatment‚Äù\nMisc\n\nPackages\n\n{glmnet} - Regularized Cox Regression\n{coxphf} - Cox Regression with Firth‚Äôs Penalized Likelihood\n\nSee Regression, Regularized &gt;&gt; Firth‚Äôs Estimator\n\n\nSample Size\n\nHarrell RMS (Ch. 20.3):\n\nTo achieve a Multiplicative Margin of Error (MMOE ) of 1.2 (?) in estimating \\(e^{\\hat \\beta}\\) with equal numbers of events in the two groups (balanced, binary treatment variable) and \\(\\alpha = 0.05 \\rightarrow\\) requires a total of 462 events\n\n\nResources\n\nMethods for non-proportional hazards in clinical trials: A systematic review\n\nTest for Differences in Treatment Effects\n\nFrom Statistical tests for comparing the associations of multiple exposures with a common outcome in Cox proportional hazard models\n\nAlso, examples show code for relatively complex designs.\n\nReminds me of some of the post-hoc ANOVA tests, but for survival models. (See Post-Hoc Analysis, ANOVA &gt;&gt; Diagnostics)\nPaper shows procedure for different exposure (i.e.¬†treatment) types, different numbers of exposures, etc., plus coded examples.\nProcedure:\n\nFit separate survival models for each treatment\nCreate a long format dataframe where the values of the treatment variables are one column (e.g.¬†exposures) and the names of the treatments are in another (e.g.¬†type)\nFit survival model where the there‚Äôs an interaction between the newly created variables (e.g.¬†exposures and types). There are also other interactions that should be included (see paper).\nWhen the exposures \\(\\times\\) types interaction coefficient has a p-value \\(\\lt\\) 0.05, it suggests that the difference in the exposure effects was statistically significant.\n\n\n\nAssumes\n\nHazard ratios (ratio of hazard rates or \\(e^\\beta\\) ) between groups/units remain constant over time (aka Proportional Hazards Assumption).\n\ni.e.¬†No matter how the hazard rates of the subjects change during the period of observation, the hazard rate of one group relative to the other will always stay the same\n\nHazard Ratios are independent of time\nExample: Immunotherapy typically violates PH assumptions (post)\n\nThe survival probabilities between the treatment (blue) and the chemo (red) cross at around the 4.2 months\n\nThe distance between the lines should remain somewhat constant throughout the trial in order to adhere to the PH assumptions (1st assumption)\nAlso think the lines should be somewhat straight. (2nd assumption)\n\nPatients in immunotherapy drug trials often experience a period of toxicity, but if they survive this period, they have a much better outcome down the road.\n\nExample: Delayed Difference in Immunotherapy Trials\n\n\nLines overlap until around 6 to 7 month then separate showing that the treatment effect is delayed.\nSolution:\n\nTests\n\nGrambsch and Therneau (G&T)\nSee Harrell RMS (Ch. 20.6.2)\n\nIf assumptions are violated,\n\nGelman says to try and ‚Äúexpand the model, at the very least by adding an interaction.‚Äù (post)\nSee Harrell RMS (Ch. 20.7)\nUse a different model\n\nAccelerated Failure Time (AFT) model (See ML &gt;&gt; Gradient Boosting Survival Trees)\nAdjusted Cox PH model with Time-Varying Coefficients\n\nMust choose a functional form describing how the effect of the treatment changes over time\n\nRecommended to use AIC criteria to guide one‚Äôs choice among a large number of candidates\n\n\n\n\n\nModels event time (T) outcome variable and outputs parameter estimates for treatment (X) effects\n\nProvides a way to have time-dependent (i.e.¬†repeated measures) explanatory variables (e.g.¬†age, health status, biomarkers)\nCan handle other types of censoring such as left or interval censoring.\nHas extensions such as lasso to handle high dimensional data\nDL and ML models also have versions of this method\n\nHazard function\n\\[\nh(t|X,L) = h_0(t)\\;e^{\\beta_1 X +\\beta_2 L}\n\\]\n\nThe hazard rate for a unit with exposure, \\(X\\), and adjustment variable, \\(L\\), is the product of a baseline hazard, \\(h_0(t)\\) (corresponding to \\(X = 0\\) and \\(L=0\\)) and a factor that depends on \\(X\\) and \\(L\\) and the regression parameters, \\(\\beta_1\\) and \\(\\beta_2\\).\nOptimized to yield partial maximum likelihood estimates, \\(\\hat \\beta\\).\n\nDoesn‚Äôt require the specification of \\(h_0(t)\\) which makes the method flexible and robust\n\n\nInterpretation:\n\n\\(\\beta\\): Represents the increase in the expected log of the relative hazard for each one unit increase in the predictor, holding other predictors constant.\nHazard Rate: The risk (probability) of an event given that the participant has survived up to a specific time. (Formula depends on treatment category, see example below.)\n\nEven though it‚Äôs a probability, it actually represents the expected number of events per one unit of time. As a result, the hazard rate in a group can exceed 1.\nExample\n\nIf the hazard rate is \\(0.2\\) at time \\(t\\) and the time units are months, then on average, \\(0.2\\) events are expected per person at risk per month.\nOr the reciprocal, \\(1/0.2 = 5\\), which is the expected event-free time (5 months) per person at risk.\n\n\nRelative Risk (of an event): Risk of an event given category divided by the risk of an event given by reference category\n\nHow many times greater (or less) of a risk of an event given a category compared to the risk of an event given a reference category. (i.e.¬†odds ratio)\nExample\n\n\\(e^\\beta = 0.68\\) means the change in category from the reference category results in a \\((1 - 0.68) = 0.32 = 32\\%\\) decrease in the hazard on average.\nFor a continuous variable (e.g.¬†age), if \\(\\beta = 0.11149\\) and \\(e^\\beta = 1.118\\), then there is an 11.8% increase in the expected hazard relative to a one year increase in age (or the expected hazard is 1.118 times higher in a person who is one year older than another)\n\n\nExample: Treatment (\\(x\\)) = Smoking\n\nRisk Score (aka Hazard Rate) given by smoking: (\\(\\boldsymbol{x=1}\\)): \\(h_0(t)e^{\\beta \\cdot x} = h_0(t)e^{\\beta \\cdot 1} = \\boldsymbol{h_0(t)e^{\\beta}}\\)\nRisk Score (aka Base Hazard Rate) given by not smoking: (\\(\\boldsymbol{x=0}\\)): \\(h_0(t)e^{\\beta \\cdot x} = h_0(t)e^{\\beta \\cdot 0} = \\boldsymbol{h_0(t)}\\)\nRelative Risk (aka Hazard Ratio) is the risk given by smoking divided by the risk given by not smoking: \\(\\frac{h_0(t)e^\\beta}{h_0(t)} = \\boldsymbol{e^\\beta}\\)",
    "crumbs": [
      "Regression",
      "Survival"
    ]
  },
  {
    "objectID": "qmd/regression-survival.html#sec-reg-surv-ml",
    "href": "qmd/regression-survival.html#sec-reg-surv-ml",
    "title": "Survival",
    "section": "ML",
    "text": "ML\n\nMisc\n\nSplit data so partitions have the same censoring distribution.\n\nThe censoring distribution might be obtained from a Kaplan-Meier estimator applied to the data.\n\nDynamic AUC is a recommended metric\n\n\n\nRandom Survival Forests\n\nThe main difference from a standard RF lies in the metric used to assess the quality of a split: log-rank (see Misc) which is typically used when comparing survival curves among two or more groups.\nPackages\n\n{{sklearn}}\n{aorsf} - Optimized software to fit, interpret, and make predictions with oblique random survival forests (ORSFs)\n\nInstead of using one variable to split the data, use a weighted combination of variables, i.e.¬†\\(\\text{instead of}\\;\\; x_1 &lt; \\text{cutpoint (left), use}\\;\\; c_1x_1 + c_2x_2 &lt; \\text{cutpoint (right)}\\)\n\nPredictions of Standard RF vs Oblique RF\n\n\n\n\n\n\n\n\nStandard Random Forest\n\n\n\n\n\n\n\nOblique Random Forest\n\n\n\n\n\n\n\nIn the standard rf, the decision boundaries are essentially perpendicular while the oblique rf boundaries are more angular. This should make the oblique model more flexible.\n\nKaplan-Meir Curves are fit in the leaves of the trees\n\n\nTime is on the x-axis and probability of survival on the y-axis\n\n\n\nExample: {aorsf}\n\nFrom Machine Learning for Risk Prediction using Oblique Random Survival Forests (Video; Slides & Code)\nVia package\n# equivalent syntaxes\nfit_orsf &lt;- orsf(data = pbc_orsf, \n                 formula = Surv(time + status) ~ . - id)\nfit_orsf &lt;- orsf(data = pbc_orsf, \n                 formula = time + status ~ . - id)\n\nTop model is fit with the typical survival::coxph syntax\ntime: time to event\nstatus: dummy variable indicating whether event occurred\nid: unit or patient id which is excluded\n\nVia {tidymodels}\nlibrary(parsnip)\nlibrary(censored) # must be version 0.2.0 or higher\nrf_spec &lt;- \n  rand_forest(trees = 200) %&gt;%\n  set_engine(\"aorsf\") %&gt;% \n  set_mode(\"censored regression\") \nfit_tidy &lt;- \n  rf_spec %&gt;% \n  parsnip::fit(data = pbc_orsf, \n               formula = Surv(time, status) ~ . - id)\nEstimated Expected Risk via Partial Dependence (PD)\n\nPD and importance rank for variables\norsf_summarize_uni(fit_orsf, n_variables = 1)\n## \n## -- bili (VI Rank: 1) ----------------------------\n## \n##         |---------------- risk ----------------|\n##   Value      Mean    Median     25th %    75th %\n##  &lt;char&gt;     &lt;num&gt;     &lt;num&gt;      &lt;num&gt;     &lt;num&gt;\n##    0.80 0.2343668 0.1116206 0.04509389 0.3729834\n##     1.4 0.2547884 0.1363122 0.05985486 0.4103148\n##     3.5 0.3698634 0.2862611 0.16196924 0.5533383\n## \n##  Predicted risk at time t = 1788 for top 1 predictors\n\nComputes expected risk (predicted probability) at different quantiles as a predictor variable varies.\n\nValue is 3 values of the predictor which are the 25th, 50th, and 75th quantile.\n\nn_variables says how many ‚Äúimportant‚Äù variables to look at\n\ne.g.¬†n_variables = 2 would look at the top 2 variables in terms of variable importance.\nVI Rank: 1 indicates the bili is ranked first in variable importance\n\nbili: serum bilirubin (mg/dl); continuous predictor variable\nIt choses time = 1788 because that‚Äôs the median\nAlso see Diagnostics, Model Agnostic &gt;&gt; DALEX &gt;&gt; Dataset Level &gt;&gt; Partial Dependence Profiles\n\nPD at specified predictor values and time values\npd_by_gender &lt;- orsf_pd_oob(fit_orsf, \n                  pred_spec = list(sex = c(\"m\", \"f\")),\n                  pred_horizon = 365 * 1:5)\npd_by_gender %&gt;% \n  dplyr::select(pred_horizon, sex, mean) %&gt;% \n  tidyr::pivot_wider(names_from = sex, values_from = mean) %&gt;% \n  dplyr::mutate(ratio = m / f)\n\n## # A tibble: 5 x 4\n##   pred_horizon      m      f ratio\n##          &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n## 1          365 0.0768 0.0728  1.06\n## 2          730 0.125  0.111   1.13\n## 3         1095 0.230  0.195   1.18\n## 4         1460 0.298  0.251   1.19\n## 5         1825 0.355  0.296   1.20\n\norsf_pd_oob - Computes expected risk using out-of-bag only\n\nBoth values (m,f) for sex are specified\npred_horizon specifies the time values\n\nHere, time is in days, so these values specify expected risk (predicted probabilities) at years 1 through 5.\n\n\nratio is the risk ratio of males compared to females.\nOthers\n\norsf_pd_inb - Computes expected risk using all training data\norsf_pd_new - Computes expected risk using new data\n\n\n\n\n\n\n\nGradient Boosting Survival Trees\n\nLoss Functions\n\nPartial likelihood loss of Cox‚Äôs proportional hazards model\nSquared regression loss\nInverse probability of censoring weighted least squares error.\n\nAllows the model to accelerate or decelerate the time to an event by a constant factor. It is known as the Accelerated Failure Time (AFT). It contrasts with the Cox proportional hazards model where only the features influence the hazard function.\n\n\nPackages\n\n{{sklearn}}\n\n\n\n\nSurvival Support Vector Machine\n\nPredictions cannot be easily related to the standard quantities of survival analysis, that is, the survival function and the cumulative hazard function.\nPackages\n\n{{sklearn}}",
    "crumbs": [
      "Regression",
      "Survival"
    ]
  },
  {
    "objectID": "qmd/regression-survival.html#sec-reg-surv-diag",
    "href": "qmd/regression-survival.html#sec-reg-surv-diag",
    "title": "Survival",
    "section": "Diagnostics",
    "text": "Diagnostics\n\nMisc\n\nNotes from How to Evaluate Survival Analysis Models\nPackages\n\n{survex} - Explainable Machine Learning in Survival Analysis\n\nFrom Dalex group\n\n{yardstick}\n\nArticles\n\nDynamic Performance Metrics for Event Time Data\nAccounting for Censoring in Performance Metrics for Event Time Data\n\nConcordance index on the survival time via concordance_survival()\nBrier score on the survival probability and its integrated version via brier_survival() and brier_survival_integrated()\nROC curve and the area under the ROC curve on the survival probabilities via roc_curve_survival() and auc_roc_survival() respectively\n\n\nThe likelihood-ratio test, Wald test, and score logrank statistics are asymptotically equivalent. For large enough N, they will give similar results. For small N, they may differ somewhat. The Likelihood ratio test has better behavior for small sample sizes, so it is generally preferred.\n\nConcordance Index (C-Index, Harrell‚Äôs C)\n\nConsider a pair of patients \\((i, j)\\). Intuitively, a higher risk should result in a shorter time to the adverse event. Therefore, if a model predicts a higher risk score for the first patient (\\(\\eta_i \\gt \\eta_j\\)), we also expect a shorter survival time in comparison with the other patient (\\(T_i \\lt T_j\\)).\nEach pair (\\(i, j\\)) that fulfills this expectation (\\(\\eta_i \\gt \\eta_j \\\\: T_i \\lt T_j\\) or \\(\\eta_i \\lt \\eta_j \\\\: T_i \\gt T_j\\)) as concordant pair, and discordant otherwise.\n\nA high number of concordant pairs is an evidence of the quality of the model, as the predicted higher risks correspond to an effectively shorter survival time compared to lower risks\n\nFormula\n\\[\n\\begin{aligned}\nC &= \\frac{\\operatorname{Number of Concordant Pairs}}{\\operatorname{Number of Comparable Pairs}}\\\\\n&= \\frac{\\operatorname{Number of Concordant Pairs}}{\\operatorname{Number of Concordant Pairs} + \\operatorname{Number of Disconcordant Pairs}}\n\\end{aligned}\n\\]\n\nIf both patients \\(i\\) and \\(j\\) are censored, we have no information on \\(T_i\\) and \\(T_j\\), hence the pair is discarded.\nIf only one patient is censored, we keep the pair only if the other patient experienced the event prior to the censoring time. Otherwise, we have no information on which patient might have experienced the event first, and the pair is discarded\n\nProgrammatic Formula\n\\[\nC = \\frac{\\sum_{i,j} I(|T_i \\gt T_j) \\cdot I(\\eta_j \\gt \\eta_i) \\cdot \\Delta_j}{\\sum_{i,j} I(T_i \\gt T_j) \\cdot \\Delta_j}\n\\]\n\nWhere the variable \\(\\Delta_j\\) indicates whether \\(T_j\\) has been fully observed (\\(\\Delta_j = 1\\)) or not (\\(\\Delta_j = 0\\)). Therefore, the multiplication by \\(\\Delta_j\\) discards noncomparable pairs, because the smaller survival time is censored (\\(\\Delta_j = 0\\)).\n\nGuidelines\n\n\\(C = 1\\): Perfect concordance between risks and event times.\n\\(C = 0\\): Perfect anti-concordance between risks and event times.\n\\(C = 0.5\\): Random assignment. The model predicts the relationship between risk and survival time as well as a coin toss.\nDesirable values range between 0.5 and 1.\n\nThe closer to 1, the more the model differentiates between early events (higher risk) and later occurrences (lower risk).\n\n\nIssues\n\nThe C-index maintains an implicit dependency on time.\nThe C-index becomes more biased (upwards) the more the amount of censoring (see Uno‚Äôs C below)\n\n\nUno‚Äôs C\n\n** Preferable to Harrell‚Äôs C in the presence of a higher amount of censoring. **\nVariation of Harrell‚Äôs C that includes the inverse probability of censoring weighting\n\nWeights based on the estimated censoring cumulative distribution\nUses the Kaplan-Meier estimator for the censoring distribution\n\nSo the disribution of censored units should be independent of the covariate variables\n\nIn the paper, Uno showed through simulation this measure is still pretty robust even when the censoring is dependent on the covariates\n\n\n\n\nDynamic AUC\n\nAUC where the False Positive Rates (FPR) and True Positive Rates (TPR) are time-dependent\n\nSince a unit is a True Negative until the event then becomes a True Positive\nRecommended for tasks when you want to measure performance over a specific period of time (e.g.¬†predicting churn in the first year of subscription).\n\nFormula\n\\[\n\\widehat{AUC}(t) = \\frac{\\sum_{i=1}^n \\sum_{j=1}^n I(y_j \\gt t)\\cdot I(y_i \\le t)\\cdot \\hat \\omega_i \\cdot I(\\hat f(\\boldsymbol{x}_j) \\le \\hat f(\\boldsymbol{x}_i))}{(\\sum_{i=1}^n I(y_i \\gt t)) \\;\\cdot\\; (\\sum_{i=1}^n I(y_i \\le t) \\; \\hat \\omega_i)}\n\\]\n\n\\(\\hat f\\) are predicted risk scores\n\\(\\hat \\omega\\) is the inverse probability of censoring weight (see Uno‚Äôs C)\n\\(I(y_{i,j} \\gt t)\\) indicates whether the unit pair‚Äôs, \\(i\\) and \\(j\\), event time is greater or less the time, \\(t\\). (I think)\n\n\n\n\n\n\nStandard Random Forest\nOblique Random Forest",
    "crumbs": [
      "Regression",
      "Survival"
    ]
  },
  {
    "objectID": "qmd/vs-code.html",
    "href": "qmd/vs-code.html",
    "title": "VS Code",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "VS Code"
    ]
  },
  {
    "objectID": "qmd/vs-code.html#sec-vsc-misc",
    "href": "qmd/vs-code.html#sec-vsc-misc",
    "title": "VS Code",
    "section": "",
    "text": "Press ‚Äú.‚Äù inside any page in github to create a vscode instance that opens that file.",
    "crumbs": [
      "VS Code"
    ]
  },
  {
    "objectID": "qmd/vs-code.html#sec-vsc-shcts",
    "href": "qmd/vs-code.html#sec-vsc-shcts",
    "title": "VS Code",
    "section": "Shortcuts",
    "text": "Shortcuts\n\n\n\n\n\n\n\nShortcut\nDescription\n\n\n\n\nCTRL + ` (backtick)\nSwitch focus to terminal\n\n\nCTRL+ Enter\nInsert a new line directly below, regardless of where you are in the current line\n\n\nALT+Shift + Up/Down\nDuplicate your current row up or down\n\n\nALT + Up/Down\nMove the current row up or down\n\n\nALT + Shift + Right\nHit this twice to select everything within a current bracket(this option is called smartSelect.grow , if it needs to be re-mapped)\n\n\nCTRL + /\nComment out the current line\n\n\nCTRL + [ or ]\nIndent lines inward or outward",
    "crumbs": [
      "VS Code"
    ]
  },
  {
    "objectID": "qmd/vs-code.html#preferences",
    "href": "qmd/vs-code.html#preferences",
    "title": "VS Code",
    "section": "Preferences",
    "text": "Preferences\n\nSetting shortcut for running a line of powershell code to ctrl + enter\n\nPress Ctrl+Shift+P, type ‚ÄúPreferences: Open Keyboard Shortcuts (JSON)‚Äù, and press Enter.\nAdd this code\n{\n    \"key\": \"ctrl+enter\", // Replace with your preferred shortcut\n    \"command\": \"workbench.action.terminal.runSelectedText\",\n    \"when\": \"editorTextFocus && editorLangId == 'powershell'\"\n}",
    "crumbs": [
      "VS Code"
    ]
  },
  {
    "objectID": "qmd/aws.html",
    "href": "qmd/aws.html",
    "title": "AWS",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "AWS"
    ]
  },
  {
    "objectID": "qmd/aws.html#sec-aws-misc",
    "href": "qmd/aws.html#sec-aws-misc",
    "title": "AWS",
    "section": "",
    "text": "Notes from Skillshare: Absolute Beginners Introduction to Amazon Web Services\nHow to choose the right GPU for Deep Learning\nOptimizations\n\nIncreased the number of threads that the AWS CLI uses to some large number (the default is 10) with¬†aws configure set default.s3.max_concurrent_requests 50\nIf downloading data is more of a bottleneck than cpu power, use a network speed optimized ec2 instance (‚Äún‚Äù in the name) such as c5n.4xl.",
    "crumbs": [
      "AWS"
    ]
  },
  {
    "objectID": "qmd/aws.html#sec-aws-creman",
    "href": "qmd/aws.html#sec-aws-creman",
    "title": "AWS",
    "section": "Create and Manage Account",
    "text": "Create and Manage Account\n\naws.amazon.com¬† ‚Äì create a free account (top right)\n\ne: &lt;email&gt;\np: &lt;password&gt;\nacct name:\nall info needs entered:\n\ntick personal,\nfull name,phone number, country, address, city, state, postal code,\ntick you‚Äôve read user agreement\n\n\nPayment info\n\ncredit card, exp date, name\n\nVerify phone\n\ncountry, phone number (again), the ‚Äúi am not a robot‚Äù thing, click call me now\n\n4 digit code pops up on screen, computer calls you, and you enter code\nclick continue\n\n\nSelecting the Basic/free acct option (1-year),\nyou can skip the Personalize your experience stuff. Hit ‚Äúsign into the console‚Äù (mid-right)\n\nenter email and password\n\nBasic account page set-up\n\nUpper right corner ‚Äì&gt; name of the account ‚Äì&gt; dropdown menu ‚Äì&gt; My Account\n\nMain Body ‚Äì&gt; Alternate Contact info ‚Äì&gt; right side, click edit\nMain Body ‚Äì&gt; Security Challenge Questions ‚Äì&gt; click edit\nMain Body ‚Äì&gt; IAM User and Role Access ‚Äì&gt;¬† click edit ‚Äì&gt; check Activate box ‚Äì&gt; click update\n\nthink this is just for generating roles that have access to billing\n\nMain Body ‚Äì&gt; Manage Communication preferences ‚Äì&gt; edit\n\nnewsletters, services info, etc.\n\nMain Body ‚Äì&gt; Manage AWS support plan\n\nalready selected free earlier\nwhere you can upgrade from free tier to a paid plan\n\n(nothing to fill out) left panel ‚Äì&gt;¬†Dashboard, Bills, Cost Explorer, Reports, allocation tags, credits, misc accounting stuff\n\nstats on your spending\ninvoice summary of this month‚Äôs usage\nVisual breakdown of the cost/services you‚Äôre incurring\nreports\ncreate tags to designate which departments/projects are using which services\nwhere to input promo codes\n\nCreate Spending Alerts\n\nleft panel ‚Äì&gt; Budgets ‚Äì&gt; Create Budget\nselect type: Cost, usage, reservation, utilization\n\nuse cost\n\n$20\n\n\nGive budget a name\nPeriod: monthly, etc.\n\nuse monthly, it‚Äôs the smallest period available\n\nselect start date and end date\nAmount\nCan set limits per service\nNotifications\n\nactual, greater than, 50% of budget amount\n\nother option besides actual is forecasted\n\nemail address\noption for SNS service but didn‚Äôt discuss what that is\ncreate new notification\n\nadd additional email alert for 75%\n\n\nclick create budget¬†(bottom right)\n\nleft panel ‚Äì&gt; Preferences\n\nset notifications for when you exceed the free tier services\nget emailed billing invoice\nBilling alerts, reports\n\n\n\nIdentity Access Management\n\nhome console ‚Äì top search box under AWS Services, type ‚ÄúIAM‚Äù\n\nselect Identity and access management\n\nChange User Sign-in URL to something more memorable\n\nexample: root account name\n\nercbk\n\nclick customize (mid-right) and type name\nURL will be changed to .signin.aws.amazon.com/console\n\nSecurity Status section (main body\n\nActivate multi-factor authentication (MFA)\n\nThis is for root user, see change password section below if you‚Äôre a user that‚Äôs part of a group\nphysical MFA (e.g.¬†usb drive) or virtual MFA\n\nchoose virtual\n\nclick next twice to the bar code\nuse authenticator app to scan bar code\n\nclick + in app, then click bar code, and scan bar code on screen\n\nAsks for 2 consecutive pins that display on the app\nClick finish\n(may need to refresh page on main screen in order to see green tick mark)\n\n\nCreate individual IAM users\n\nAllows you to distribute various permissions to people with different roles in your org\nNeed to set up an admin user to get access keys so you can use AWS programmatically\nleft panel ‚Äì&gt; Users ‚Äì&gt; add users ‚Äì&gt; enter user name ‚Äì select access types\n\nchoose access type\n\ntick programmatic and management console\n\nenter console password\nrequire password reset: nope (untick box)\nclick next (bottom right) to set permissions\n\nSet permissions\n\nif you‚Äôre creating user groups (left panel), you can skip this and set the permissions group-wide\nSelect Attach existing policies directly\n\ntick box ‚ÄúAdministratorAccess‚Äù ‚Äì&gt; click next (bottom right)\n\ndude did this through groups and policies, so may have to go that route if AdministratorAccess isn‚Äôt available, then add the user to the group. Also there‚Äôs a button to attach additional ‚Äúpolicies‚Äù later on if needed\n\n\n\nReview and click create User\na lot more to this‚Ä¶ groups, policies¬† (see vid for details)¬†\n\nSelect password policy\n\ntick allow users to set password and untick everything else ‚Äì&gt; apply policy\nactivate/deactivate security token regions - he didn‚Äôt go into this but all US and EU regions were activated by default, so probably doesn‚Äôt need to be messed with.\n\nCopy URL, log out as root user, Now you can starting going to URL and sign in under the administrator acct you made\n\nTo change password\n\nleft panel ‚Äì&gt; Users ‚Äì&gt; select yourself ‚Äì&gt; security credentials tab ‚Äì&gt; Console password\nCan also create MFA in same tab\n\n\n\nCloud Trail\n\nlog of actions taken on account\ntype cloud trail in search box\nCreate trail -¬† lets you store the logs in a S3 bucket",
    "crumbs": [
      "AWS"
    ]
  },
  {
    "objectID": "qmd/aws.html#sec-aws-basserv",
    "href": "qmd/aws.html#sec-aws-basserv",
    "title": "AWS",
    "section": "Overview of Basic Services",
    "text": "Overview of Basic Services\n\nCompute\n\nEC2 - Elastic Cloud Compute - Computer Clusters\n\nClosely related to on-premise set-ups\n\nElastic Container Service (ECS) - Spin up containers on top of EC2\nLambda - Serverless Architecture\n\nComputing resources can scale and descend automatically based on real-time demands.\nHandles security patches and OS updates automatically\nIssue with ‚Äútimeouts‚Äù - not optimal for long running applications\n\nA timeout defines the maximum amount of time a Lambda function can execute before it‚Äôs terminated.\nDefault timeout is set at 3 seconds, but you can configure it in the AWS console in increments of 1 second, with a maximum of 15 minutes (900 seconds).\n\nAWS charges for Lambda usage in increments of 100 milliseconds (0.1 seconds). Setting the timeout in 1-second increments helps ensure you‚Äôre not billed for extra time if your function runs slightly longer than expected.\nPrice optimization entails tuning this timeout parameter to be as close to your run time as possible. Need to take into account timeouts of other services included in your lambda function. (Guide)\n\nMonitor your Lambda functions for timeouts using CloudWatch metrics.\n\nDependency limit at 50 MB, can add 512 MB more to a tmp file after function has executed\nSpins down when not in use, so you don‚Äôt pay for downtime, but takes 5 or more secs to spin back up\n\nCan ping server from time to time to keep it ‚Äúwarm‚Äù\n\nLess tweakable than EC2, if problems occur, less flexible in terms of your team handling it\nThe code you run on AWS Lambda is uploaded as a ‚ÄúLambda function‚Äù. Each function has associated configuration information, such as its name, description, entry point, and resource requirements. The code must be written in a ‚Äústateless‚Äù style i.e.¬†it should assume there is no affinity to the underlying compute infrastructure. Local file system access, child processes, and similar artifacts may not extend beyond the lifetime of the request, and any persistent state should be stored in Amazon S3, Amazon DynamoDB, or another Internet-available storage service. Lambda functions can include libraries, even native ones.\n\n\nStorage\n\nS3\n\nDatabase -automanaged by AWS\n\nrelational db service (RDS)\ndynamoDB - noSQL\nElasticCache - redis\nRedshift - cadillac data warehouse service\n\nManagement Tools\n\nCloudWatch\n\nmonitor cpu usage\nlatency\nset alarms around metrics",
    "crumbs": [
      "AWS"
    ]
  },
  {
    "objectID": "qmd/aws.html#sec-aws-cli",
    "href": "qmd/aws.html#sec-aws-cli",
    "title": "AWS",
    "section": "CLI",
    "text": "CLI\n\ngoogle ‚Äúdownload aws cli‚Äù\nGet keys\n\nlog into aws\nsearch IAM ‚Äì&gt; goto iam\nleft panel ‚Äì&gt; users ‚Äì&gt; your usesrname ‚Äì&gt; security credentials tab ‚Äì&gt; create access key ‚Äì&gt; 2 keys are created ‚Äì&gt; click download csv file\n\naccess key id (kind of like a username)\nsecret access key (kind of like a password)\n\n\nSet-up\n\nConfigure profile\n\naws configure --profile &lt;name&gt;\n\nchoose a name, can be anything\n\nused ercbk\n\n\nYou‚Äôll be asked for your\n\n‚Äúaccess key id‚Äù and ‚Äúsecret access key‚Äù, enter those\n\nused eb_admin values\n\noptional: default region: us-east-2 or hit enter to skip\n\nused us-east-2\n\noptional: default output format: just hit enter (he didn‚Äôt go into this)\n\n\n\ncommands\n\nHelp commands\n\ngives description, flags, inputs, etc.\naws help\naws &lt;service&gt; help\n\neg aws iam help\n\naws &lt;service&gt; &lt;command&gt; help\n\neg aws iam list-users help\n\nCan also go to AWS documentation at website",
    "crumbs": [
      "AWS"
    ]
  },
  {
    "objectID": "qmd/aws.html#sec-aws-ec2",
    "href": "qmd/aws.html#sec-aws-ec2",
    "title": "AWS",
    "section": "EC2",
    "text": "EC2\n\nOn home screen, in search window, type ‚Äúec2‚Äù, click link\n\n\nEC2 Page\n\nleft panel ‚Äì Dashboard\n\nMain Body ‚Äì Under Resources\n\nInstances - which cluster instances you have running\nVolumes - which storage services you have\nkey pairs - you get a key pair for each running instance\nOther stuff‚Ä¶\n\n\nleft panel ‚Äì Instances\n\ninstances\n\nwhere you launch on-demand instances\nno bidding, you pay full price\n\nlaunch templates\n\nstored instance configurations\nalternate method to launch instances\n\nspot instances (requests)\n\nbid for available instances for a cheaper price\nIf the Spot price increases above your bid price, capacity is no longer available, or the spot request has constraints that can‚Äôt be met, then the Spot Instance can be ‚Äúinterrupted.‚Äù\n\nhttps://aws.amazon.com/premiumsupport/knowledge-center/spot-instance-terminate/\nEBS volumes can be attached, snapshots taken, or results sent to s3 buckets to prepare for a potential terminations (see above link)\n\n\nreserved instances\n\nyou can make a reservation for an instance to guarantee it‚Äôs availability at the time you specify. Can be cheaper than on-demand.\n\ndedicated host\n\nWhen you buy instances you share hosts (servers) with other people. Here you can guarantee only you are on the host with your instances.\n\n\nleft panel ‚Äì images\n\nAMI\n\nscenario: you launch an ec2 and load more packages onto it and want to save that AMI to reuse at a later time. Here you can take a snapshot of that image.\nyou can share these images with other users, sell the image in the marketplace\n\n\nleft panel ‚Äì Elastic Block Store\n\nvolumes\n\nhard drives - hdd (cold or optimized, ssd, iops ssd )\n\nsnapshots\n\nback-up of the volume\ncan be used to launch another instance if the current one is terminated\n\n\nleft_panel ‚Äì Network and Security\n\nSecurity Groups (also see docker notebook ‚Äì aws ‚Äì running a task)\n\nfirewall you place in front of an EC2 instance\nspecify allowed ports and allowed ip connections\ncan‚Äôt restrict to inbound or outbound traffic; open is open\n\nElastic IPs\n\nallows you to fix static ips (up to 5) to your instances\n\none less thing you‚Äôd have to configure if you‚Äôre starting and stopping ec2 instances a lot.\n\n\nPlacement Groups\n\ndecreases latency by placing all of your instances in the same or closely placed hosts\n\nKey Pairs\n\na form of tags that you can use to access your instance more easily,\n\nuse values as descriptors of use case for instance to organize and monitor\nCan use to SSH into your instance\npairs are key:value. Example: key= sales, value¬† = sales_forecast\n\n\nNetwork Interfaces\n\ngives a network card to your instance so it can connect to the internet\n\n\nleft panel ‚Äì Load Balancing\n\nscaling managed by aws\nroutes traffic to different instances\nLoad Balancers (also see Docker notebook, aws create load balancer section)\n\ntypes: application, network\n\nTarget Groups\n\nleft panel ‚Äì auto scaling\n\nadds instances if triggers set in load balancers (latency, cpu usage, memory)\nshuts down instances once load decreases\nkeeps cost lower\n\nleft panel ‚Äì systems manager\n\nallows you to run a command across multiple instances by specifying tags, ids, etc\ncan use scripts that update packages\n\n\n\n\nLaunch\n\nSearch ec2 in the search window\nSteps\n\ntop-right header ‚Äì choose a region\n\nwhere are the users located (latency)\n\nleft panel ‚Äì Instances ‚Äì instances (or Spot Instances)\n\nclick launch instances\nchoose an AMI, click select on right-hand side\nchoose compute option by ticking box on the left and click next on bottom right\n\neg t2 - micro (free tier option)\n\nconfigure instance details\n\nselect how many instances you want to launch\nnetwork, subnet, auto-assign public ip\n\ndefaults should be fine\n\nIAM role - mentions something about databases\nshutdown behavior: ‚Äústop‚Äù terminates instance when you stop it\nenable termination protection: protects against accidental termination\n\nthink this might have to do with spot instances being terminated\n\nmonitoring - enable CloudWatch ($)(see¬†Overview of basic services section)\n\nfree version updates metrics every 5 min\ndetailed version updates metrics every 1 min\n\nT2 unlimited\n\nenabling says if your t2 instance cpu goes over 20% usage, then start using my credits, and once my credits are used, then bill me for the rest\nguessing if this is unticked then your t2 is throttled below 21% to remain totally free\n\n\nAdd storage\n\nsee Elastic Block Store above for details on available drives\ncomes with a ‚Äúroot‚Äù drive by default\n\nChoose HD type, size of storage, and whether to delete on termination (volume gets deleted)\nGeneral Purpose SSD and up to 30 GB available for free tier.\nAlso saw a snapshot ID I think but he didn‚Äôt discuss\n\nclick add volume to add multiple HDs\n\nAdd tags\n\nadd tag\nenter key and value (different from key pair below)\ntick which resources you want to use the tag for: instance and/or volume\n\ncan create unique tags for different resources\n\n\nConfigure security group\n\nAssign a new security group or select an existing group\n\ntick select existing ‚Äì&gt; tick default\n\n\nClick review and launch\n\nshows all the options you selected above\nHit launch\n\nChoose existing or create new key pair\n\nfile that‚Äôs necessary to ssh (linux) or log into (windows)\nCreate new key pair ‚Äì&gt; enter name ‚Äì&gt; download key pair file\n\nor you can select previous key pair and use the same file\n\nLaunch instance\n\nLaunch Status\n\nClick View Instances\n\nWatch status column, usually takes a couple minutes to launch\nscreen is split in half, use divider to increase size of lower screen if you want to view the details of the instance you started\nI think you‚Äôre in¬†left panel ‚Äì instances ‚Äì instances\n\n\n\n\n\n\n\nConnect to/ Terminate Instance\n\nAlso see Docker, AWS &gt;&gt; Create Cluster: EC2 with SSH Access\nleft panel ‚Äì instances ‚Äì instances\n\nhighlight instance you want to connect to\nIn bottom of split EC2 screen, copy IPv4 Public IP and save it in notebook++ or somewhere\n\nOpen an inbound port to instance\n\nleft panel ‚Äì¬†Network and Security ‚Äì Security Groups\n\nclick on security group you selected during launch of instance\nlower half of screen ‚Äì inbound tab\n\n‚Äúall traffic‚Äù means full communication allowed between all instances within this security group\nclick edit ‚Äì add rule\n\nType ‚Äì click dropdown ‚Äì choose SSH\n\nafter choosing SSH, it also inputs port 22\n\nSource ‚Äì ditto ‚Äì choose ‚ÄúMy IP‚Äù\n\nautomatically finds your ip¬† and inputs it\n\nDescription -¬† ‚ÄúSSH for  ip‚Äù or whatever you want\n\nfor Windows Server AMI instance\n\nType ‚Äì click dropdown ‚Äì choose RDP\n\nchooses port 3389\n\nSource ‚Äì ditto ‚Äì choose ‚ÄúMy IP‚Äù\n\nautomatically finds your ip¬† and inputs it\n\nDescription - ‚ÄúRDP for  ip‚Äù or whatever you want\n\n\n\n\nIf on a linux machine (locally), click instance, choose connect, follow instructions\nIf on windows (locally):\n\nOpen Puttygen (key generator)\n\nClick load ‚Äì find and select the key pair file (.pem) you downloaded before launching the instance\nClick Save private key\n\nIt‚Äôll ask if you‚Äôre sure you don‚Äôt want to attach a passphrase ‚Äì click yes\nenter name of .ppk file (no need to add extension) ‚Äì Click Save\n\n\nOpen Putty\n\nConfiguration window opens\n\nIn Host Name (IP address) box ‚Äì paste IPv4 Public IP (that you copied from earlier)\nleft panel ‚Äì Connections ‚Äì SSH ‚Äì Auth\n\nOptions-for-controlling-SSH-authentication window opens\n\nprivate-key-for-authentication box ‚Äì click browse\n\nselect the .ppk file you made\n\n\n\nClick Open (bottom right) to open connection\n\n¬†Windows pop-up ‚Äì click yes\n\n\n\nPutty CLI opens\n\n‚Äúlogin as‚Äù¬†\n\ntype username for the AMI you used\n\nexample was a basic linux AMI with username ‚ÄúEC2-user‚Äù\nFor RStudio AMI, should be ‚Äúubuntu‚Äù\n\n\nHit enter and should be connected\n\n\nTerminate\n\nleft panel ‚Äì instances ‚Äì instances\n\nselect instance you want to terminate ‚Äì right click instance_id (or anywhere on row) ‚Äî instance state ‚Äì terminate\n\n\nConnect to a Windows Server AMI instance\n\nWindows Search ‚ÄúRemote Desktop Connection‚Äù\n\nopen it\n\nFor ‚ÄúComputer:‚Äù, type in the IPv4 public ip ‚Äì click connect\nFor base ami\n\nusername: administrator\npassword\n\nleft panel ‚Äì instance ‚Äì instance\n\nright click instance row ‚Äì Get Windows Password\n\nclick Choose FIle ‚Äì select key pair file .pem file (not .ppk)\nclick Decrypt Password (bottom right)\nCopy password, paste into Remote Desktop Connection\n\n\nClick OK\n\n\na window with opens up with Windows OS on it (takes a minute or two to completely load)\n\n\n\n\nConfigure Load Balancer and Application Ports\n\nAlso see Docker, AWS &gt;&gt; Create Application Load Balancer (ALB)\nleft panel ‚Äì Network and Security ‚Äì Security Groups¬†(also see docker notebook ‚Äì aws ‚Äì running a task)\nExample: We want the Application Instances to only talk to the load balancer (inbound) and the load balancer talks to the public (inbound) and application instances (outbound) (Reminder all inbound ports are also outbound ports)\n\nCreate security group for EC2 Instance\n\nCreate Security Group (blue button, upper left)\n\nEnter name and description\nclick create\n\nInbound tab (lower half)\n\nclick edit\n\nType: HTTP (auto-inputs protocol TCP and port 80)\nSource: type name of Load Balancer security group (long box)\n\nautocomplete will help\nafter clicking name, a group id is entered into the field\n\nWeird, but id resembles the group id listed in the top half of screen but doesn‚Äôt exactly match. Should be correct though.\n\ndrop down should be ‚Äúcustom‚Äù\n\nclick add rule\nType: HTTPS\n\nsame thing but auto-inputs port 443\n\nClick Save\n\n\noutbound tab\n\nBy default all outbound traffic goes out on the inbound ports, but you can specify additional ports\nclick edit\n\nType: all traffic (auto-inputs protocol all, port range 0-65535)\nSource: same as for inbound\n\n\n\nCreate security group for Load Balancer\n\nCreate Security Group\n\nSame procedure as above, different name\n\nInbound tab\n\nclick edit\n\nType: HTTP\n\nkeep Source as is\n\n0.0.0.0 means accept traffic from everywhere\n\n\nclick add rule\nType: HTTPS\n\nsame\n\nclick Save\n\n\nOutbound tab\n\nclick edit\n\nType: HTTP¬†(auto-inputs protocol TCP and port 80)\nSource: type the application security group name (long box)\n\nsee load balancer section for other details\n\nclick add rule\nType: HTTP\n\nsame\n\nclick Save",
    "crumbs": [
      "AWS"
    ]
  },
  {
    "objectID": "qmd/aws.html#sec-aws-s3",
    "href": "qmd/aws.html#sec-aws-s3",
    "title": "AWS",
    "section": "S3",
    "text": "S3\n\nSearch S3\nautoscales,, replicates your data to prevent total loss, ability to version data, max file size 5TB\ncharged for what you use (GB/mo)\n3 different classes\n\nstandard\n\nhighest availability (99.999%), durability (replicated across hosts multiple times)\nmost expensive of the classes\n\ninfrequently accessed\n\nless durability (replicates), should be data that doesn‚Äôt end your world if lost\n\nglacial\n\nfor archiving purposes\n\n\nSecurity\n\nIAM\n\ngive certain persons or departments permissions\n\nS3 policies\n\nmake certain buckets public, private, etc.\n\n\nBuckets\n\nmust have unique name across all AWS\n\nassume this is taken care of by some auto-generated id\nCan‚Äôt contain periods (dns-compliant)\n\nregion-specific\ncross-region replication\n\ncopy objects from one bucket to another for a fee\n\n\nObjects\n\nfiles, artifacts, etc\nkey:value look-up\n\nthe keys are essentially just file paths\n\nbucket_name/folder/file.txt\nbucket_name/folder/*\n\ngets everything\n\n\nthe values are the objects\n\nYou can directly download files from the console but not folders\n\nselect file, click on ‚Äúmore‚Äù button (top left), select ‚Äúdownload as‚Äù, follow directions\n\nTo make publicly available through a web link you have to specify that policy (see below)\n\n¬†Create a basic bucket\n\nClick Create Bucket (top left)\n\nOpens Wizard\n\nName and Region\n\nEnter unique DNS compliant name (no periods)\nEnter Region\nClick next\n\nSet Properties\n\nkeep defaults\nclick next\n\nSet Permissions\n\nkeep defaults\nclick next\n\nReview\n\nclick create bucket\n\n\n\nUpload to bucket\n\nUpload via aws console\n\nClick bucket name\nclick upload button (top left)\nOpens Wizard\n\nSelect files\n\nclick add files or drag and drop folders into the window\nclick next\n\nSet Permissions\n\nkeep defaults\nclick next\n\nSet Properties\n\nStorage class\n\nstandard, standard-ia (infrequently accessed), reduced redundancy\n\nencryption\nkeep defaults\nclick next\n\nReview\n\nclick upload\n\n\n\nSee below for uploading via CLI\n\n\nCreate Bucket Policy to allow for public download (from a weblink)\n\nclick bucket name\nclick Permissions tab (top mid)\nBucket policy requires a json expression\n\nclick policy generator (bottom left)\n\nopens up new browser tab\n\nSelect type of policy\n\nS3 Bucket Policy\n\neffect (allow or deny)\n\nselect allow\n\nPrincipal\n\ntype ‚Äú*‚Äù (asterisk) to give everyone access\n\nActions\n\ncan select more than one action if you want\nselect ‚ÄúGetObject‚Äù\n\nAmazon Resource Name\n\nThe format of the required expression is given below the box\nyour substituting your bucket_name/key_name into the expression\n\nfor key name he used * to signify ‚Äúeverything‚Äù, but I think this would be your path to the resources (not including bucket_name)\n\ne.g.¬†folder1/folder2/file.csv or folder1/folder2/*\n\n\nClick add statement\nClick generate policy\ncopy json expression\n\n\nGo back to previous browser tab with the permissions tab and paste the expression into the window\nClick Save (mid right)\n\nThere‚Äôs also a DELETE button if you want to remove a policy from the bucket\n\nShould see a ‚Äúpublic‚Äù tag on the permissions tab confirming the policy has been set\n\nDownload from bucket\n\nGet download link for a file that has a public permission set\n\ntick box of selected file\n\nwindow with info about the file should open on the right side\n\ncopy download link in overview section, paste in browser or use programatically\n\nDownload via console\n\nclick file name ‚Äì overview tab\n\n¬†various downloading options\n\n\nProgramatically: see section Upload to bucket via CLI below (uses copy command)\n\nGive bucket viewing (and downloading) access via IAM (need to have administrator permissions)\n\nsearch IAM\nleft panel ‚Äì Policies\nclick create policy (top left)\nclick choose service (mid)\n\ntype or select ‚ÄúS3‚Äù\n\nclick select actions\n\nUnder Access Level\n\nList\n\nselect all (4)\n\nRead\n\nView policy\n\nGetBucketAcl, GetBucketCORS, GetBucketLocation, GetBucketLogging, GetBucketPolicy, GetBucketTagging, GetObjectAcl, ListBucketByTags, ListBucketVersions\n\nDownload policy (should be a separate policy from View)\n\nGetObject\n\n\nResources\n\nyou have to select resources (e.g.¬†buckets, objects (i.e.¬†folders, files) that the actions above affect\n\nView policy\n\nchoose all resources (which is buckets and objects)\n\nDownload policy\n\nchoose specific\nclick ARN\n\nEnter the bucket name you want to give access to (or tick ‚Äúany‚Äù box to give access to all buckets)\nenter object name (or tick ‚Äúany‚Äù box for access to all objects)\n\nClick add\n\n\n\nClick Review Policy (bottom right)\n\nEnter a Name\n\neg ListAllBucketsObjsS3\n\nClick Create Policy (bottom right)\nSelect or click newly created policy (should be back to left panel ‚Äì policies console\n\nnew policy name should be in a clickable banner at top of screen\nOr choose it from list of policies that‚Äôs displayed\n\nGo to Attached entities tab\n\nclick attach\nselect users or groups you want the policy to apply to\nclick attach (bottom right)\n\n\n\nView buckets you have permissions with: aws s3 ls or aws s3api list-buckets --output text\nUpload to bucket via CLI\n\nAlso see above for uploading via aws console\naws s3 help, aws s3  help\naws s3 cp     can copy files via:\n\nbucket to bucket\nlocal to bucket\nbucket to local\n\nlocal to bucket\n\naws s3 cp &lt;C:\\\\Users\\\\path\\\\to\\\\file.csv&gt; &lt;s3://&lt;bucket/path/to/folder/name.csv&gt; --region &lt;bucket? region&gt; --profile &lt;profile name&gt;\nsee cli section above on how to create profile\nrefresh console if you already have it open to see the file\n\nbucket to local¬†\n\naws s3 cp &lt;s3://&lt;bucket/path/to/folder/name.csv&gt; &lt;C:\\\\Users\\\\path\\\\to\\\\file.csv&gt;¬†--region &lt;bucket? region&gt; --profile &lt;profile name&gt;\nsame thing as before, just reversing the  and  URIs\n\n\nVersioning\n\nIf you enable versioning and then disable it, then the versioned objects will remain but new objects won‚Äôt be versioned\n\n¬†Would have to manually delete the versioned objects manually\n\nbucket name ‚Äì properties tab\n\nclick on ‚ÄúVersioning‚Äù card\ntick Enable versioning¬†\nclick save\n\nTo see the different file versions\n\nbucket name ‚Äì folder\nclick versions: show button (upper left)\n\nOnce versions are displayed you can download and delete files/versions in various menus\n\nclick file name ‚Äì ‚Äúlatest version‚Äù drop down (next to file name, top left)\n\nshows all versions with options to download or delete\n\nclick file name ‚Äì overview tab\n\nvarious download options\n\ntick file name/version box ‚Äì click more button (upper left) ‚Äì select delete ‚Äì click delete button",
    "crumbs": [
      "AWS"
    ]
  },
  {
    "objectID": "qmd/cloud-services.html",
    "href": "qmd/cloud-services.html",
    "title": "4¬† Cloud Services",
    "section": "",
    "text": "4.1 Misc",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Cloud Services</span>"
    ]
  },
  {
    "objectID": "qmd/cloud-services.html#misc",
    "href": "qmd/cloud-services.html#misc",
    "title": "4¬† Cloud Services",
    "section": "",
    "text": "See Cloud Costs Every Programmer Should Know for various service estimates in order to perform back-of-the-napkin calculations of project costs\nFor ‚ÄúStead-State Workloads‚Äù requiring HPC, cloud compute doesn‚Äôt make economic sense\n\nSteady-State workloads are projects that are run near constantly\n\nSee Thread for discussion on scenarios, issues, and risks of your data center (DC) in the Cloud vs on-prem.\nExamples:\n\nAcademia: Where academics are in a queue to run experiments on the a HPC cluster\nWeather Forecasting: Forecasts are required nearly in real time, so these models run constantly\nFinancial transaction processing at a bank: The bank‚Äôs systems handle a constant stream of transactions\nInventory management system for a manufacturing plant: The system constantly receives updates on raw materials, production output, and finished goods.\nOthers: Week or two long analysis runs at hedge funds, genomic analysis jobs, a swath of AI training / fine tuning, Oil and Gas where they are plowing through seismic data constantly\n\n\n\nRStudio Server on your docker image allows you to access an ide connected to the server through a browser. Useful so you can make sure the correct packages are installed.\nServerless computing is a method of providing backend services on an as-used basis.\n\nA serverless provider allows users to write and deploy code without the hassle of worrying about the underlying infrastructure\nCharged based on their computation and do not have to reserve and pay for a fixed amount of bandwidth or number of servers, as the service is auto-scaling\ne.g.¬†AWS Lambda (i.e.¬†resources only get spun-up when an event is triggered)\n\nNVIDIA GPU Guide (thread)\n\nRTX 20-series or 30-series GPUs are forbidden from inclusion in data centers\nGeneral Recommendations (Oct 2022)\n\nA100 for model training\nT4 for inference workloads\n\nK80\n\nReleased in 2015, the K80 contained a lot of VRAM for the time (24 GB)\nCame before tensor cores and is relatively weak by today‚Äôs standards\nOnly okay for learning purposes\n\nP4\n\nReleased in 2016\nValue came from its low power consumption\nMay find it priced higher than its upgraded version (the T4), so recommended to avoid it\n\nT4\n\nReleased in 2018\nSignificant upgrade for inference workloads compared to the P4\nExtremely low power consumption, tensor cores, and plenty (16GB) of VRAM\nCheap, so if you have an inference workload, recommended to strongly consider a T4\n\nP100\n\nBig improvement for model training workloads over the K80 when released\nLess RAM (16GB) than K80\nWay more compute¬† than K80\n\nCan see memory savings from using mixed-precision training\n\nNo tensor cores\n\nV100\n\nHuge upgrade over the P100\nSame VRAM as P100 many but more CUDA cores\nIntroduces Tensor Cores\nMore cost-efficient than the P100\n\nA100\n\nnewest data center GPU\nupgraded tensor cores\nmost benchmarks show 3x+ faster training compared to the V100\n80GB VRAM\nPrice tag might be big, but it‚Äôs usually worth it over the V100",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Cloud Services</span>"
    ]
  },
  {
    "objectID": "qmd/cloud-services.html#price-management",
    "href": "qmd/cloud-services.html#price-management",
    "title": "4¬† Cloud Services",
    "section": "4.2 Price Management",
    "text": "4.2 Price Management\n\nspot instances for cheaper machines\nautoscaling (kubernetes?) to handle peak usage times (spin-up more machines) while saving during slow times (spin down excess machines)\nUse opensource project management tools (dvc, airflow, etc)\nGoogle\n\nThe Google Kubernetes Engine (GKE) control plane is free, whereas Amazon‚Äôs (EKS) costs $0.20 an hour.\n\nAWS\n\nWith a well-defined framework of tag keys and values applied across different AWS resources, billing breakdowns by tag prove extremely useful for greater insight on the source of AWS charges ‚Äî especially if resources are tagged by department, or team, or different layers of organizational granularity.\nReserved Instances - commit to specific configurations for one or three years at reduced cost\nSpot Instances - pay significantly lower costs but potential for applications to be interrupted\nSavings Plans\n\nEC2 Instance Savings Plans to reduce compute charges for specific instance types and AWS regions\n\nSavings of up to 72%\n\nCompute Savings Plans to reduce compute costs irrespective of type and region.\n\nSavings up to 66% and extends to ECS Fargate and Lambda functions.\n\n\nImage Management\n\nData Lifecycle Manager - automates the creation, retention, and deletion of images\n\nWill not manage images and snapshots created by other means, and it also excludes instance store-backed images.\nEC2 Recycle Bin - serves as a safety net to avoid the accidental deletion of resources ‚Äî retaining images and snapshots for a configurable time where we may restore them before they are deleted permanently.\n\n\nLambda\n\nCloudwatch - Lambda automatically creates log groups for its functions, unless a group already exists matching the name /aws/lambda/{functionName}. These default groups do not configure a log retention period, leaving logs to accumulate indefinitely and increasing CloudWatch costs.\n\nExplicitly configure groups with matching names and a retention policy to maintain a manageable volume of logs.\n\nMemory Optimization - AWS Lambda Power Tuning can help to identify optimizations, albeit with notable initial costs given the underlying use of AWS Step Functions.\n\nLambda charges based on compute time in GB-seconds, where the duration in seconds is measured from when function code executes until it either returns or otherwise terminates, rounded up to the nearest millisecond. To reduce these times, we desire optimal memory configuration.\n\n\nS3 Lifecycle Configuration\n\nCharged for how much data stored, but also which S3 storage classes are utilized.\n\nStandard (default) class is the most expensive, permitting regular access to objects with high availability and short access times.\nInfrequent Access (IA) classes offer reduced cost for data which requires limited access (usually once per month)\nArchival options via Glacier deliver further cost reductions.\n\nConfiguring the lifecycle allows you to automatically transfer data to different storage classes and thereafter permanently delete it, X and Y days respectively after data creation",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Cloud Services</span>"
    ]
  },
  {
    "objectID": "qmd/cloud-services.html#kaggle",
    "href": "qmd/cloud-services.html#kaggle",
    "title": "4¬† Cloud Services",
    "section": "4.3 Kaggle",
    "text": "4.3 Kaggle\n\nFree\n\n4-core CPU instances w/30 GB RAM\n2-core CPU, 2xT4 GPU w/13GB RAM\n\nT means tensor cores\n1 hour spent using 2xT4‚Äôs takes the same amount of your quota as a P100 (old free gpu offering)\n\nMeans 30-40 hours of free, multi-GPU compute per week",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Cloud Services</span>"
    ]
  },
  {
    "objectID": "qmd/cloud-services.html#saturn-cloud",
    "href": "qmd/cloud-services.html#saturn-cloud",
    "title": "4¬† Cloud Services",
    "section": "4.4 Saturn Cloud",
    "text": "4.4 Saturn Cloud\n\nSaturn Cloud Recipes\n\nJSON files that specify your environment\nGood for keeping track of server dependencies (e.g.¬†linux libraries)\n\nDunno about R packages",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Cloud Services</span>"
    ]
  },
  {
    "objectID": "qmd/cloud-services.html#google-cloud-platform-gcp",
    "href": "qmd/cloud-services.html#google-cloud-platform-gcp",
    "title": "4¬† Cloud Services",
    "section": "4.5 Google Cloud Platform (GCP)",
    "text": "4.5 Google Cloud Platform (GCP)\n\nBigQuery sandbox is Google‚Äôs GCP free tier cloud SQL database. It‚Äôs free but your data only lasts 60 days at a time.\nGCP allows users to run deep learning workloads on TPUs\nSince data expires after 60 days, back-up the model coefficients and performance score tables to Google Sheets. Article suggested this is possible through WebUI.\nAs of Nov.19, regression, logistic regression, and k-nn are the only models available to be run with the sql query editor\nhttps://cloud.google.com/free/\n\n$300 credit for 12 months\nAlways free:\n\n2M requests for containers\n1 GB storage\n\nScalable NoSQL document database.\n50,000 reads, 20,000 writes, 20,000 deletes per day\n\nFunctions\n\n1 f1-micro instance per month (Available only in region: us-west1, Iowa: us-central1, South Carolina: us-east1)\n30 GB-months HDD\n5 GB-months snapshot in select regions\n1 GB network egress from North America to all region destinations per month (excluding China and Australia)\n\nKubernetes\n\nOne-click container orchestration via Kubernetes clusters, managed by Google.\nNo cluster management fee for clusters of all sizes\nEach user node is charged at standard Compute Engine pricing\n\nApp Engine\n\n28 instance hours per day\n5 GB Cloud Storage\nShared memcache\n1,000 search operations per day, 10 MB search indexing\n100 emails per day\n\nBigQuery\n\nFully managed, petabyte scale, analytics data warehouse.\n1 TB of querying per month\n10 GB of storage\n\nOther Stuff\n\nYour free trial credit applies to all GCP resources, with the following exceptions:\n\n* You can‚Äôt have more than 8 cores (or virtual CPUs) running at the same time.\n* You can‚Äôt add GPUs to your VM instances.\n* You can‚Äôt request a quota increase. For an overview of Compute Engine quotas, see Resource quotas.\n* You can‚Äôt create VM instances that are based on Windows Server images.\n\nYou must upgrade to a paid account to use GCP after the free trial ends. To take advantage of the features of a paid account (using GPUs, for example), you can upgrade before the trial ends. When you upgrade, the following conditions apply:\n\n* Any remaining, unexpired free trial credit remains in your account.\n* Your credit card on file is charged for resources you use in excess of what‚Äôs covered by any remaining credit.\nYou can upgrade your account at any time after starting the free trial. The following conditions apply depending on when you upgrade:\n* If you upgrade before the trial is over, your remaining credit is added to your paid account. You can continue to use the resources you created during the free trial without interruption.\n* If you upgrade within 30 days of the end of the trial, you can restore the resources you created during the trial.\n* If you upgrade more than 30 days after the end of the trial, your free trial resources are lost.\n\nSpot Instances (Preemptible VM)\n\nusage capped at 24 hrs\npricing is fixed and not market-driven\n\nGoogle price calculator:¬†https://cloud.google.com/products/calculator/#id=3115f19f-4ff0-4c57-9028-69cb994fe7ca\nExample\n\ncreating a cluster with:\n\n1 x Dataproc cluster node with 30 GB of RAM\n3 x Dataproc worker nodes with 15 GB of RAM\nUsing less than 5 GB of disk space in a bucket\nAnd running the cluster for only 4 hrs\nWould cost only around $5 at the end of the month\n\n\nFree Tier\n\nincludes a 12-month free trial with $300 credit to use with any GCP services and an Always Free benefit, which provides limited access to many common GCP resources\nUse to test out, but KEEP EVERYTHING SMALL (data, hardware, etc). Need to upgrade it to see the true benefit. Free tier resources look like my desktop computer. Whatever cash is leftover should transfer to account.\nhttps://cloud.google.com/free/docs/gcp-free-tier#how-to-upgrade\nupgrade it from the free trial to a paid account through the GCP Console clicking the Upgrade button at the top of the page\n\n\n\nSteps for new project\n\nGo to interface¬†https://console.cloud.google.com/\ncreate a project. ‚Äúselect a project‚Äù on top bar ‚Äì&gt; ‚Äúnew project‚Äù on top right ‚Äì&gt; choose name (optionally a folder/organization if you have one) ‚Äì&gt; create\n(article wasn‚Äôt very reliable and went on talk about a python implementation so I stopped here\n\nTips\n\nApp Engine\n\nDon‚Äôt use App Engine Standard environments ‚Äî big brother G wants you to use rather Flex environments, otherwise, they‚Äôll punish you.\nReview cost analysis regularly to make sure there are no surprising costs.\nMake sure you clean up redundant App Engine application versions to prevent G from robbing you.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Cloud Services</span>"
    ]
  },
  {
    "objectID": "qmd/cloud-services.html#microsoft-azure",
    "href": "qmd/cloud-services.html#microsoft-azure",
    "title": "4¬† Cloud Services",
    "section": "4.6 Microsoft Azure",
    "text": "4.6 Microsoft Azure\n\nhttps://azure.microsoft.com/en-us/free/?WT.mc_id=Revolutions-blog-davidsmi\nhttps://visualstudio.microsoft.com/dev-essentials/\n\nstarts azure trial but gives you free sql server developer edition\n\nWon‚Äôt be charged until you choose to upgrade.\n12 months access to $ services for free\n$200 credit for any service for 30 days\n\nAt the end of the 30 days, I think the remainder goes into your account after you change to a pay-to-play account\n\nAccess to the services that are always free\n\nAzure Kubernetes Service (AKS)\nFunctions\n\n1,000,000¬†requests per month\na solution for easily running small pieces of code in the cloud. You can write just the code you need for the problem at hand, without worrying about a whole application or the infrastructure to run it.\nExample use case:¬†for handling WebAPI requests and sending the different data and results to where it needs to go.\n\nApp Service\n\n10¬†web, mobile, or API apps\n\nActive Directory B2C (identity)\n\n50,000¬†authentications per month\n\nMachine Learning Server\n\nDevelop and run R and Python models on your platform of choice.\n\nSQL Server 2017 Developer Edition\n\nBuild, test, and demostrate applications in a non-production environment.\n\nOther stuff\n\nBlob storage\n\nobject storage solution for the cloud\noptimized for storing massive amounts of unstructured data\n\nSpot Instances (Low Priority VM)\n\nnot time limit on instance usage\nno warning on termination by Azure\n\nTips\n\nIf you can‚Äôt create a service, because Azure servers are under maintenance for more than a couple of minutes ‚Äî check out your permissions and registrations under the ‚ÄúResource providers‚Äù panel.\nIf you see any strange errors on the Azure Portal ‚Äî just change the filters‚Äô values.\nIf you use Azure Machine Learning, and your scoring function cannot locate your source code ‚Äî deliver the code as a Model and add it explicitly to the sys.path in the init function.\nIf you use Azure Machine Learning, don‚Äôt use Batch Endpoints ‚Äî it looks like they are not ready yet ‚Äî just use the regular Published Pipelines. In fact, ‚ÄúBatch endpoint‚Äù is just a wrapper around a published pipeline.\nDon‚Äôt include flask in your Azure conda environment specification.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Cloud Services</span>"
    ]
  },
  {
    "objectID": "qmd/cloud-services.html#aws",
    "href": "qmd/cloud-services.html#aws",
    "title": "4¬† Cloud Services",
    "section": "4.7 AWS",
    "text": "4.7 AWS\n\nInstance types\n\nc-type instances are compute heavy\nr-type instances are RAM heavy\nm-type instances are balanced\n‚ÄúEach thread is represented as a virtual CPU (vCPU) on the instance. An instance has a default number of CPU cores, which varies according to instance type. For example, an m5.xlarge instance type has two CPU cores and two threads per core by default‚Äîfour vCPUs in total.‚Äù\nspot prices from 03/24/2020, all calculations over the previous month\ngen purpose\n\nm6g.8xlarge\n\ngen purpose, 32 vcpu, 128 gb\nnewer graviton, didn‚Äôt see any specs, but supposed to be much better than the xenon 1st gen\n\nm5.8xlarge\n\ngen purpose, 32 vcpu, 128 gb\nolder 3.1 ghz, xenon\non-demand $1.54/hr\n\nm5a.8xlarge\n\ngen purpose 32 vcpu, 128 gb\n2.4 ghz, slower processor speed than m5\n\nm5n.8xlarge\n\ngen purpose 32 vcpu, 128 gb\n3.1 ghz, xenon specialized for neural networks, ML tasks\nn.virg, 71% savings,¬†&lt;5% interruption\nohio,¬†83% savings, &lt;5% interruption\non-demand $1.90/hr\npotential spot price = $0.32\n\nm5dn.8xlarge\n\nsame but with 2 ssd hard drives\n\nm4.10xlarge\n\ngen purpose 40 vcpu, 160 gb\n2.4 ghz\nsmaller write-up, get the sense these are older processors/instances\n\n\ncompute optimized\n\nRequires HVM AMIs that include drivers for ENA (network adaptor) and NVMe (ssd hard drives)\n\nseems standard on a lot of instances (gen purpose and here), shouldn‚Äô t be an issue\n\nc5.9xlarge\n\n36 vcpu, 72 gb\n3.4 ghz\non-demand $1.53/hr\n\nc5d.9xlarge\n\nsame but with ssd\n\nc5n.9xlarge\n\n36 vcpu, 96 gb\n3.0 ghz, built for task needing high throughput for networking\non-demand, $1.94/hr\n\nc4.8xlarge\n\n36 vcpu, 60 gb\n2.9 ghz\n67% savings, &lt;5% interruption\non-demand $1.59/hr\npotential spot price = $0.52\n\n\nmemory optimized\n\nr5.8xlarge\n\n32 vcpu, 256 gb\n3.1 ghz\nn.virg, 72% savings, 5-10% interruption\nn.cal,¬†76% savings, &lt;5% interruption\non-demand $2.02/hr\npotential spot price = $0.48\n\nr5a.8xlarge\n\n32 vpu, 256 gb\n2.5 ghz\n\nr5n.8xlarge\n\n32 vcpu, 256 gb\n3.1 ghz, neural network optimized\nus.west. oregon 76% savings, 5-10% interruption\non-demand $2.38/hr\npotential spot price = $0.57\n\nr4.8xlarge\n\n32 vcpu, 244 gb\n2.3 ghz\n\nz1d.6xlarge\n\n24 vcpu, 192 gb\n4.0 ghz\non-demand $2.23\n\n\naccelerated computing\n\ninf1.6xlarge\n\n24 vcpu, 48 gb\nbuilt for ML\non-demand $1.91/hr\n\n\n\nFree Tier (12 months after sign-up)\n\naws.amazon.com¬†‚Äì pricing (top) ‚Äì free tier (mid) ‚Äì create a free account (mid)\nEC2\n\n750 hrs/mo of t2-micro instance usage\n\nfor Linux, Windows, RHEL, SLES AMIs\n\n\nElastic Block Storage (EBS)\n\n30 GB\ncan be connected to an ec2\n\nElastic Container Registry\n\n500 MB per month\n\nfor storing and retrieving Docker images\nexample in course was a basic nginx image and it was 50MB\n\n\nS3\n\n5 GB of standard storage (high availability/ high durability)\n20,000 Get Requests,¬†2000 Put Requests per month\n\nElastic Load Balancing\n\n750 hrs per month shared between classic and application load balancers\n\nno idea what the differences are between classic and application\n\n\n\nPricing\n\nPrice per GPU as of 29-06-2023\n\n\nExamples\n\nr3.4xlarge 16 CPUs, 122 GB RAM,¬†1 x 320 SSD,¬†Spot Price: $0.1517/h\n\nTrained H2O GBM, RF, XGBoost, DeepLearning. Cluster ran for 2 hr 40 min. Total Cost = around $0.42\nhttps://www.daeconomist.com/post/2019-01-15-partii/\n\n\nStorage\n\nS3\n\ncharged by amount stored\n\n$0.023/GB for standard (for first 50 TB)\n0.004/GB for glacier and 0.00099/GB deep glacier\n\ntakes longer to retrieve and not always available\n\n\nfree inbound transfer\nfree transfer between aws services (e.g.¬†S3 to EC2) within the same region\n\nAurora\n\nstorage + inbound/outbound: $0.20 per million requests\n\n\nConsolidated Biling\n\na separate account. All company individual accounts (marketing, sales, etc.) bills are pooled into this account\nhas no access to services\nhas no permissions to access services in other accounts\npooled bill counted towards potential discount billing\n\nCalculators\n\nTotal Cost of Ownership (TCO) calculator\n\ncompares cost of running a project on-premises to aws cloud\n\naws pricing calculator\n\ncalculates price of running a cloud application\ncalculator.aws.com\nestimates cost per service, per service group, and total infrastructure\nhelps find right ec2 instance and region\n\n\nBilling and Cost Management console\n\ncost explorer\n\nview and analysis costs and usage\n\n\n\n\nSpot Instances\n\nSummary\n\nGo to spot advisor and find instances that fit budget and compute requirements\nPrepare strategy for interruption\nOther services\n\nAs of Jan 01, 2019, cloudyr‚Äôs aws.ec2 PKG didn‚Äôt support all spot instances.\nno time limit on instance usage\nAWS gives a 2 min warning when it decides it needs your spot instance\npricing is market driven depending on capacity levels at the time\nAvailable actions when Amazon ‚Äúinterrupts‚Äù your instance:\n\nHibernation:\n\n‚Äúlike closing your laptop display‚Äù\nsaves data and memory and reboots once instance is available again\nRight before interruption, a daemon on the instance freezes the memory and stores it in Elastic Block Store (EBS) root volume\nYour EC2 will retain this root volume and any other EBS data volumes\nOnce market price falls below bid price, instance resumes with memory restored from disk to RAM\nYou aren‚Äôt charged while instance is in hibernation, but EBS volumes do cost $.\nAvailable for instance types: C3, C4, M4, R3, and R4 with &lt; 100 GB RAM on Amazon‚Äôs Linux, Ubuntu, and Windows\nAll this is done by something called the EC2 Hibernation Agent which sound like its just the name of the program on the servers\n\nStop\n\n‚Äúlike shutting down your computer to be turned on later‚Äù\nlose whatever is in RAM but retain EBS data volumes ($)\nrestores once bid price &lt; market price\n\nTerminate\n\n***default option***\neverything deleted\n\n\nSpot Advisor\n\n**always use this before spinning up spot instances **\nhttps://aws.amazon.com/ec2/spot/instance-advisor\nInput\n\nvCPUs\nMemory size\nPlatform (linux?)\navailability zone (region?)\namount required (number of instances?)\n\noutput\n\ninstance type\nvCPUs\nMemory (GB)\nSavings over On-Demand (%)\nFrequency of termination (%)\n\nliklihood your instance will get terminated\n\n\n\nRunInstance API\n\nFor requesting a spot instance through CLI I think\nLooks like you send something that looks like a python dict with max price, type, region, etc. to this API\n\nSpot Blocks\n\nallows you to set a finite duration that your instance will run for\n\n1 to 6 hrs\nno interruption during that time\n\ntypically 30 to 45% cheaper than on-demand and maybe an additional 5% cheaper during non-peak hours for the region\nrecommended for batch runs\n\nStrategy\n\nUse regions with largest pools of spot instances\n\nLargest pools\n\nus.east.1 (north.virginia)\neu.west.1(ireland)\n\nThese regions have most types/most instances available\nTypically can go uninterrupted for weeks\nless price fluctuation = more certainty\n\n\nSmallest pools\n\neu.central.1 (frankfort)\nap.south.1 (mumbai)\nap.southeast.1 (singapore)\n\ntypically get interrupted within days\n\n\n\nRun groups of instances that come from multiple spot pools\n\nTo used different compute types, jobs/tasks need to be in containers\nspot pools are instances with same region, type, OS, etc.\napplications running on instances from a least 5 different pools can cut interruptions by up to 80%\n\n\nManaging/preparing for interruptions\n\nOnly use for jobs that are short lived\n\ndevelopment and staging environments, short data processing, proof-of-concept, etc.\n\nBuild internal management system that automatically handles interruptions\n\nlook at spot pool historical prices for past 90 days\n\nlooking for least volatile pools\nolder generation (e.g.¬†c-family, m-family) tend to be most stable\n\n\nUse 3rd party platform that manages spot instances and interruptions\n\nSpotinst - uses ML to choose and manage instances that optimizes price and provide continuous activity for apps that are without a single point of failure.\n\nUses on-demand as a fall-back.\nSLA guarantees 99.9% availability.\nSnapshots volumes to migrate data to new instances in case of interruption.\nworks with other services and platforms (kubernetes, codedeploy, etc.)\n\nSpot Fleet - aws service, automanages groups of spot instances according to either of the following strategies:\n\nstrategy options\n\nlowest price - lowest price instances\ndiversified - spread instances across pools\n\nAfter receiving 2 min warning,\n\ntake snapshots of AMI and any attached EBS volumes and use them to launch a new instance.\n\nsnapshot of AMI\n\non EC2 dashboard ‚Äì left panel ‚Äì instances ‚Äì instances\n\nright-click instance ‚Äì image ‚Äì create AMI\n\nimage is in left -panel ‚Äì Images ‚Äì AMIs\n\n\n\nActually both snapshots might be able to taken in left panel ‚Äì spot requests\n\nsee AWS note ‚Äì EC2 for further details\n\n\n\n\n\n\nneed to drain and detach instance from elastic load balancer if one is used\nIf using auto-scaling, need to create an on-demand group and a spot instance group\n\n\nKubernetes\n\nAfter receiving 2 minute interruption warning from AWS:\n\nDetach instance from elastic load balancer (ELB) is one is being used\nMark instance as unschedulable (?)\n\nprevents new pods (group of containers on an instance that performs a job) from being scheduled on that node\nunderlying compute capacity and scheduling of resources of the pods needs to be monitored. Compute capacity and pod resource requirements need to match.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Cloud Services</span>"
    ]
  },
  {
    "objectID": "qmd/cloud-services.html#comparison",
    "href": "qmd/cloud-services.html#comparison",
    "title": "4¬† Cloud Services",
    "section": "4.8 Comparison",
    "text": "4.8 Comparison\n\nMisc\n\nNotes from\n\nThe Top Clouds Evaluated Such That You Don‚Äôt Need to Repeat Our Mistakes\nAWS vs GCP reliability is wildly different\n\nNo services for blockchain development, quantum computing, and graph databases in GCP (May 2022)\nhttps://cloud-gpus.com/ - tool for comparing gpu compute prices across vendors\n\nData centers\n\nCloser the resources are to your business, the less latency\n(May 2022) GCP has caught up and surpassed AWS in the number of data centers and regions that are available\n\nCompute\n\nCheapest vCPU\n\nGCP ‚Äúe2-micro-preemptible‚Äù with 2 vCPU and 1 GB memory.\n\n48% lower than ‚Äút4g.nano‚Äù from AWS\n5 times lower than ‚ÄúA0‚Äù from Azure.\n\nAWS is in-between GCP and Azure in terms of price (i.e.¬†Azure most expensive for cheap vCPUs)\n\nMore performant GCP instances usually cost approximately the same as their analogs from other cloud providers\n\nAzure servers cost the same or slightly less than AWS\n\nGCP: dedicated PostgreSQL server\n\nCheapest instances are 25% lower than the competitors\n\nGPU on-demand availability\n\nConclusion: Assuming you need on-demand boxes to succeed right when you need them, the consensus seems to clearly point to AWS. If you can stand to wait or be redundant to spawn failures, maybe Google‚Äôs hardware acceleration customizability can win the day.\nStats\n\nAWS consistently spawned a new GPU in under 15 seconds (average of 11.4s).\nGCP on the other hand took closer to 45 seconds (average of 42.6s).\nAWS encountered one valid launch error in these two weeks whereas GCP had 84\n\nCaveats\n\nGCP allows you to attach a GPU to an arbitrary VM as a hardware accelerator - you can separately configure quantity of the CPUs as needed.\nAWS only provisions defined VMs that have GPUs attached\n\n\n\nRecommendations\n\nAzure\n\nYou use the Microsoft Office stack (Word, Teams, OneDrive, SharePoint, etc.) and/or C# programming language.\nYou head neither for the cheapest servers nor for the most expensive ones ‚Äî you need something in the middle.\nYou need a memory-optimized solution rather than a general-purpose or a compute-optimized one.\nYou read about the current bugs and inconsistencies in Azure, and it does not scare you.\n\nAWS\n\nYou are rich.\nYou have AWS experts in your team.\nYou build an enterprise-level long-term project.\nOR you just want to rent a cheap virtual machine, and you don‚Äôt care about all the other facilities.\n\nGCP\n\nYou are a start-up company.\nYou can‚Äôt invest much time in learning AWS and dealing with Azure bugs.\nYou don‚Äôt need much flexibility and configuration facilities from the cloud.\nYou are ready to accept the approaches dictated by the platform.\nYou need either a general-purpose or a compute-optimized solution, but not a memory-optimized one.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Cloud Services</span>"
    ]
  },
  {
    "objectID": "qmd/db-engineering.html",
    "href": "qmd/db-engineering.html",
    "title": "Engineering",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Databases",
      "Engineering"
    ]
  },
  {
    "objectID": "qmd/db-engineering.html#sec-db-eng-misc",
    "href": "qmd/db-engineering.html#sec-db-eng-misc",
    "title": "Engineering",
    "section": "",
    "text": "If you‚Äôre developing an application, a good rule of thumb is to write your frequently run queries in such a way that they return a response within 500 ms\nColumn storage files (parquet) are more lightweight, as adequate compression can be made for each column. Row storage doesn‚Äôt work in that way, since a single row can have multiple data types.\n\n\n(See below) Apache Avro is smaller file size than most row format file types (e.g.¬†csv)\n\n{pins}\n\nConvenient storage method\nUse when:\n\nObject is less than a 1 Gb\n\nUsed {butcher} for large model objects\n\nSome model objects store training data\n\n\n\nBenefits\n\nJust need the pins board name and name of pinned object\n\nThink the set-up is supposed to be easy\n\nEasy to share; don‚Äôt need to understand databases",
    "crumbs": [
      "Databases",
      "Engineering"
    ]
  },
  {
    "objectID": "qmd/db-engineering.html#sec-db-eng-terms",
    "href": "qmd/db-engineering.html#sec-db-eng-terms",
    "title": "Engineering",
    "section": "Terms",
    "text": "Terms\n\nACID - A database transaction, by definition, must be atomic, consistent, isolated and durable. These are popularly known as ACID properties.¬† These properties can ensure the concurrent execution of multiple transactions without conflict. Guarantees data validity despite errors and ensure that data does not become corrupt because of a failure of some sort.\n\nCrucial to business use cases that require a high level of data integrity such as transactions happening in banking.\n\nBatch processing - performing an action on data, such as ingesting it or transforming it, at a given time interval.\nBTEQ - Batch Teradata Query (like SQL) is simply a utility and query tool for Teradata which is a relational database system Creating a BTEQ script to load data from a flat-file.\nConcurrency - multiple computations are happening at the same time\nData Dump - A file or a table containing a significant amount of data to be analysed or transferred. A table containing the ‚Äúdata dump‚Äù of all customer addresses.\nData Mart - A subset of a data warehouse, created for a very specific business use case. Finance data mart storing all the relevant financial information required by the Accounting team to process their month-end cycles.\nData Integration - Usually, the hardest part of the project, where multiple sources of data are integrated into a singular application/data warehouse. Integrating finance and customer relationship systems integrating into an MS SQL server database.\nData Lake - A repository for all kinds of structured and unstructured data. Mainly based on Hadoop storage technology. Called a lake as it is flexible enough to store anything from raw data to unstructured email files. Hadoop Data Lake. Storing logs of all customers called into the inbound call centre including call duration.\nData Mesh - Decentralized design where data is owned and managed by teams across the organisation that understands it the most, known as domain-driven ownership. tl;dr - Each department controls they‚Äôre own data from ingestion to ‚Äúdata products.‚Äù This data product is then made a available to the other departments for them to use in their projects. Each department has their own engineers, scientists, and analysts.\n\nEach business unit or domain aims to infuse product thinking to create quality and reusable data products ‚Äî a self-contained and accessible data set treated as a product by the data‚Äôs producers ‚Äî which can then published and shared across the mesh to consumers in other domains and business units ‚Äî called nodes on the mesh.\nEnables teams to work independently with greater autonomy and agility, while still ensuring that data is consistent, reliable and well-governed.\nYou don‚Äôt have to figure out who‚Äôs in charge of what data, who gets to access it, who needs to protect it and what controls and monitoring is in place to ensure things don‚Äôt go wrong.\nExample: Banking\n\nCredit risk domain‚Äôs own data engineers can independently create and manage their data pipelines, without relying on a centralised ingestion team far removed from the business and lacking in credit expertise. This credit team will take pride in building and refining high-quality, strategic, and reusable data products that can be shared to different nodes (business domains) across the mesh.\n\n\nData Models - A way of organising the data in a way that it can be understood in a real-world scenario. Taking a huge amount of data and logically grouping it into customer, product and location data.\nData Quality - A discipline of measuring the quality of the data to improve and cleanse it. Checking Customer data for completeness, accuracy and validity.\nData Replication - There are multiple ways to do this, but mainly it is a practice of replicating data to multiple servers to protect an organisation against data loss. Replicating the customer information across two databases, to make sure their core details are not lost.\nDenormalization - database optimization technique in which we add redundant data to one or more tables. Designers use it to tune the performance of systems to support time-critical operations. Done in order to avoid costly joins. Me: Seems like it‚Äôs kind of like a View except a View might have calculated columns in it.\nDimensions - A data warehousing term for qualitative information. Name of the customer or their country of residence.\nDistributed SQL -¬† a single logical database deployed across multiple physical nodes in a single data center or across many data centers if need be; all of which allow it to deliver elastic scale and resilience. Billions of transactions can be handled in a globally distributed database.\nEDW - The same as a data warehouse except it includes all the data within an organisation. This means that the entire enterprise can rely on this warehouse for their business decisions. Organising sales, customer, marketing and finance data in an enterprise data warehouse to be able to create several key management reports.\nEmbedded aka In-Process\n\nEmbedded database as in a database system particularly designed for the ‚Äúembedded‚Äù space (mobile devices and so on.) This means they perform reasonably in tight environments (memory/CPU wise.)\nEmbedded database as in databases that do not need a server, and are embedded in an application (like SQLite.) This means everything is managed by the application.\n\nFacts - A data warehousing term for quantitative information. The number of orders placed by a customer.\nFlat File - Commonly used to transfer data due to their basic nature; flat files are a single table storing data in a plain text format. All customer order numbers stored in a comma-separated value (.csv) file\nHorizontal Scaling - Refers to the process of adding more nodes or instances to the database cluster to increase its capacity and performance. This is achieved by distributing the data and workload across multiple servers or nodes, rather than increasing the resources (aka Vertical Scaling) such as CPU, RAM, or storage of a single server. By adding more nodes to the cluster, the system can handle more concurrent connections, queries, and data processing operations.\nHTAP - Hybrid Transactional Analytical Processing - System that attempts be good at both OLAP and OLTP\nMaster Data - This is data that is the best representation of a particular entity in the business. This gives you a 360 view of that data entity by generally consolidating multiple data sources. Best customer data representation from multiple sources of information.\nMulti-Master - allows data to be stored by a group of computers, and updated by any member of the group. All members are responsive to client data queries. The multi-master replication system is responsible for propagating the data modifications made by each member to the rest of the group and resolving any conflicts that might arise between concurrent changes made by different members.\n\nAdvantages\n\nAvailability: If one master fails, other masters continue to update the database.\nDistributed Access: Masters can be located in several physical sites, i.e.¬†distributed across the network.\n\nDisadvantages\n\nConsistency: Most multi-master replication systems are only loosely consistent, i.e.¬†lazy and asynchronous, violating ACID properties. (mysql‚Äôs multi-master is acid compliant)\nPerformance: Eager replication systems are complex and increase communication latency.\nIntegrity: Issues such as conflict resolution can become intractable as the number of nodes involved rises and latency increases.\n\nCan be contrasted with primary-replica replication, in which a single member of the group is designated as the ‚Äúmaster‚Äù for a given piece of data and is the only node allowed to modify that data item. Other members wishing to modify the data item must first contact the master node. Allowing only a single master makes it easier to achieve consistency among the members of the group, but is less flexible than multi-master replication.\n\nNiFi - It is an open-source extract, transform and load tool (refer to ETL), this allows filter, integrating and joining data. Moving postcode data from a .csv file to HDFS using NiFi.\nNormalization - A method of organizing the data in a granular enough format that it can be utilised for different purposes over time. Organizing according to data attributes reduces or eliminates data redundancy (i.e.¬†having the same data in multiple places). Usually, this is done by normalizing the data into different forms such as 1NF (normal form) or 3NF (3rd normal form) which is the most common. (See DB, Relational &gt;&gt; Normalization)\n\nTaking customer order data and creating granular information model; order in one table, item ordered in another table, customer contact in another table, payment of the order in another table. This allows for the data to be re-used for different purposes over time.\n\nNULL indexes - These are the indexes that contain a high ratio of NULL values\nObject-Relational Mapping (ORM) - Allows you to define your data models in Python classes, which are then used to create and interact with the database. See {{SQLAlchemy}}\nODS - Operational data store generally stores limited and current information to help simple queries. Unable to handle historical or complex data queries. An ODS for daily stock fluctuations in a warehouse help the warehouse manager decide what to prioritise in the next order delivery.\nOLAP - Online Analytical Processing - large chunks of tables are read to create summaries of the stored data\n\nUse chunked-columnar data representation\n\nOLTP - Online Transactional Processing - rows in tables are created, updated and removed concurrently\n\ntraditionally use a row-based data representation\npostgres excels at this type of processing\n\nRDBMS - Relational database management system. All of the above examples are RDBMS, meaning they store data in a structured format using rows and columns.\n\nA Microsoft SQL server database.\n\nReal-Time Processing (aka Event Streaming) - each new piece of data that is picked up triggers an event, which is streamed through the data pipeline continuously\nReverse ETL - Instead of ETL where data is transformed before it‚Äôs stored or ELT where data is stored and transformed while in storage, Reverse ETL performs transformations in the pipeline between Storage and the Data Product.\n\nSCD Type 1‚Äì6 - A method to deal with changes in the data over time in a data warehouse. Type 1 is when history is overwritten whereas Type 2 (most common) is when history is maintained each time a change occurs.\n\nWhen a customer changes their address; SCD Type 1 would overwrite the old address with the new one, whereas Type 2 would store both addresses to maintain history.\n\nSchemas - A term for a collection of database objects. These are generally used to logically separate data within the database and apply access controls.\n\nStoring HR data in HR schema allows logical segregation from other data in the organisation.\n\nSharding - Horizontal Partitioning ‚Äî divides the data horizontally and usually on different database instances, which reduces performance pressure on a single server.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRequires a sharding key and a hash function. Then, the logic uses the hash function to map a key with an associated record to a shard, and then the shard to a database node.\n\nStaging - The name of a storage area that is temporary in nature; to allow for processing of ETL jobs (refer to ETL). Typically data is loaded from a source database into the staging area database where it is transformed. Once transformed, it‚Äôs loaded into the production database where analytics can be performed on it.\n\nA staging area in an ETL routine to allow for data to be cleaned before loading into the final tables.\n\nTransactional Data - This is data that describes an actual event.\n\nOrder placed, a delivery arranged, or a delivery accepted.\n\nUnstructured Data - Data that cannot be nicely organised in a tabular format, like images, PDF files etc.\n\nAn image stored on a data lake cannot be retrieved using common data query languages.",
    "crumbs": [
      "Databases",
      "Engineering"
    ]
  },
  {
    "objectID": "qmd/db-engineering.html#sec-db-eng-datqual",
    "href": "qmd/db-engineering.html#sec-db-eng-datqual",
    "title": "Engineering",
    "section": "Data Quality",
    "text": "Data Quality\n\nAlso see Production, Data Validation\nAccuracy - addresses the correctness of data, ensuring it represents real-world situations without errors. For instance, an accurate customer database should contain correct and up-to-date addresses for all customers.\nCompleteness - extent your datasets have all the required information on every record\n\nMonitor: missingness\n\nConsistency - extent that no contradictions in the data received from different sources. Data should be consistent in terms of format, units, and values. For example, a multinational company should report revenue data in a single currency to maintain consistency across its offices in various countries.\nTimeliness - Data should be available at the time it‚Äôs required in the system\nValidity - ensuring that data adheres to the established rules, formats, and standards.\n\nMonitor: variable types/classes, numeric variable: ranges, number of decimal places, categorical variable: valid categories, spelling\n\nUniqueness - no replication of the same information twice or more. They appear in two forms; duplicate records and information duplication in multiple places.\n\nMonitor: duplicate rows, duplicate columns in multiple tables",
    "crumbs": [
      "Databases",
      "Engineering"
    ]
  },
  {
    "objectID": "qmd/db-engineering.html#sec-db-eng-costopt",
    "href": "qmd/db-engineering.html#sec-db-eng-costopt",
    "title": "Engineering",
    "section": "Cost Optimization",
    "text": "Cost Optimization\n\nAlso see\n\nPage 53 in SQL notebook\nGoogle, BigQuery &gt;&gt; Optimization\nSQL &gt;&gt; Best Practices\n\nAvoid disk operations, make sure that you look out for hints & information in the EXPLAIN PLAN of your query. (e.g.¬†using SORT without an index)\n\n\nWhen you see filesort, understand that it will try to fit the whole table in the memory in many chunks.\n\nIf the table is too large to fit in memory, it will create a temporary table on disk.\n\nLook out for a using filesort with or without a combination of using temporary.\n\nLoading data in chunks or streaming it record by record for ETL jobs helps to optimize memory usage.\nSplit tables with many columns Might be efficient to split the less-frequently used data into separate tables with a few columns each, and relate them back to the main table by duplicating the numeric ID column from the main table.\n\nEach small table can have a primary key for fast lookups of its data, and you can query just the set of columns that you need using a join operation.\n\nPrimary keys should be global integers.\n\nIntegers consume less memory than strings, and they are faster to compare and hash\n\nJoins\n\nWith correlated keys\n\nThe query planner won‚Äôt recognize the correlated keys and do nested loop join when a hash join is more efficient\nI don‚Äôt fully understand what correlated keys on a join are, but see SQL &gt;&gt; Terms &gt;&gt; Correlated/Uncorrelated queries\n\nIn the example below, a group of merge_commit_ids will only be from 1 repository id, so the two keys are associated in a sort of traditional statistical sense.\n\nSolutions\n\nUse LEFT_JOIN instead of INNER_JOIN\nUse extended statistics\nCREATE STATISTICS ids_correlation ON repository_id, merge_commit_id FROM pull_requests;\n\n‚Äúrepository_id‚Äù and ‚Äúmerge_commit_id‚Äù are the correlated keys\nI‚Äôm not sure if ‚Äúids_correlation‚Äù is a function or just a user-defined name\nPostgreSQL ‚â•13 will recognize correlation and the query planner will make the correct calculation and perform a hash join\n\n\n\n\nPre-join data before loading it into storage\n\nIf a group of tables is frequently joined and frequently queried, then pre-joining will reduce query costs\ncan be done using an operational transform system such as Spark, Flow, or Flink (dbt can parallelize runs and work w/Spark)\n\nIndexes{#sec-db-eng-costopt-index}\n\nIndexes help in filtering data faster as the data is stored in a predefined order based on some key columns.\n\nIf the query uses those key columns, the index will be used, and the filter will be faster.\n\nSuitable for any combination of columns that are used in filter, group, order, or join\nMySQL Docs\nDon‚Äôt use indexes with LIKE\nCluster a table according to an index\n\nAlso see Google, BigQuery &gt;&gt; Optimization &gt;&gt; Partition and Cluster\nRearranges the rows of a table on the disk\nDoesn‚Äôt stay ‚Äúclustered‚Äù if table is updated\n\nSee pg_repack for a solution\n\nExample\n-- create index\nCREATE INDEX pull_requests_repository_id ON pull_requests (repository_id, number)\n-- cluster table\nCLUSTER pull_requests USING pull_requests_repository_id\n\n\nUseful for queries such as\nSELECT *\nFROM pull_requests\nWHERE repository_id IN (...) AND number &gt; 1000\nBest Pactices\n\nAvoid too many indexes\n\nA copy of the indexed column + the primary key is created on disk\nIndexes add to the cost of inserts, updates, and deletes because each index must be updated\nBefore creating an index, see if you can repurpose an existing index to cater to an additional query\nCreate the least possible number of indexes to cover most of your queries (i.e.¬†Covering Indexes).\n\nMakes effective use of the index-only scan feature\nAdd INCLUDE to the create index expression\nExample\n-- query\nSELECT y FROM tab WHERE x = 'key';\n-- covering index, x\nCREATE INDEX tab_x_y ON tab(x) INCLUDE (y);\n-- if the index, x, is unique\nCREATE UNIQUE INDEX tab_x_y ON tab(x) INCLUDE (y);\n\ny is called a non-payload column\n\nDon‚Äôt add too many non-payload columns to an index. Each one duplicates data from the index‚Äôs table and bloat the size of the index.\n\n\nExample: Query with function\n-- query\nSELECT f(x) FROM tab WHERE f(x) &lt; 1;\n-- covering index, x\nCREATE INDEX tab_f_x ON tab (f(x)) INCLUDE (x);\n\nWhere f() can be MEAN, MEDIAN, etc.\n\n\n\nFix unusable indexes\n\nIssues related to data types, collation (i.e.¬†how it‚Äôs sorted), character set (how the db encodes characters), etc\nSometimes you can make the indexes work by explicitly forcing the optimizer to use them. (?)\n\nRepurpose or delete stale indexes\n\nIndexes are designed to serve an existing or a future load of queries on the database\nWhen queries change, some indexes originally designed to serve those queries might be completely irrelevant now\nAutomate stale index removal. Dbs keep statistics. Write a script to either notify you or just delete the index if it‚Äôs older and not been used past a certain threshold\n\nUse the most cost efficient index type\n\nExample: If your use case only needs a regular expression search, you‚Äôre better off having a simple index than a Full Text index.\n\nFull Text indexes occupy much more space and take much more time to update\n\n\nDon‚Äôt index huge tables (&gt; 100M rows), partition instead\n\nThen prune the partitions (partition pruning) you don‚Äôt need and create indexes for the partitioned tables you do keep.\n\n\nPartitioning\n\nAlso see Google, BigQuery &gt;&gt; Optimization &gt;&gt; Partition and Cluster\nSplits your table into smaller sub-tables under the hood\n\nNot viewable unless you check the table directory to see the multiple files that have been created\n\nThe same goes for indexes on that table.\n\n\nUse on tables with at least 100 million rows (BigQuery recommends &gt; 1 GB) Partitioning helps reduce table size and, in turn, reduces index size, which further speeds up the Data Warehouse (DWH) operations. But, partitioning also introduces complexity in the queries and increases the overhead of managing more data tables, especially backups. So try a few of the other performance techniques before getting to Sharding.\nPartition columns should always be picked based on how you expect to use the data, and not depending on which column would evenly split the data based on size.\n\nExample: partition on county because your analysis or transformations will largely be done by county even though since some counties may be much larger than others and will cause the partitions to be substantially imbalanced.\n\n\nUse ELT (e.g.¬†load data from on-prem server to cloud, then transform) instead of ETL (transform data while on-prem, then load to cloud) for data pipelines\n\nMost of the time you have a lot of joins involved in the transformation step\n\nSQL joins are one of the most resource-intensive commands to run. Joins increase the query‚Äôs runtime exponentially as the number of joins increases.\nExample\n\nRunning 100+ pipelines with some pipelines having over 20 joins in a single query.\nEverything facilitated by airflow (see bkmk for code)\nETL: postgres on-prem server, sql queries with joins, tasks ran 12+ hours, then the transformed data is loaded to google storage\n\n13+ hrs for full pipeline completion\n\nELT: running the queries with the joins, etc. with bigquery sql on the data after it‚Äôs been loaded into google storage.\n\n6+ hrs for full pipeline completion\n\n\n\n\nUse Materialized Views\n\nA smaller data object that contains the subset of data resulting from a specific query\nWhereas a query happens after data is loaded, a materialized view is a precomputation\nThe computation is done once, and changes to the data are incorporated as they occur, making subsequent updates to the view much cheaper and more efficient than querying the entire database from scratch\n\nFetching a large table will be slower if you try to use multiple cores.\n\nYou have to divide up the table and recombine it. Plus setting up parallel network processes takes time.\nThe time used to fetch some data from the internet depends massively on the internet bandwidth available on your router/network.\n\nUse Random Access via http range header + sparse-hilbert index to optimize db for query searches\nCITEXT extension makes it so you don‚Äôt have use lower or upper which are huge hits on performance (at least they are in WHERE expressions) GIN custom indexes for LIKE and ILIKE\nCREATE EXTENSION IF NOT EXISTS btree_gin;\nCREATE EXTENSION IF NOT EXISTS pg_trgm;\nCREATE INDEX index_users_on_email_gin ON users USING gin (email gin_trgm_ops);\n\nCREATE EXTENSION adds btree and pg_trgm extensions\nindex_users_on_email_gin is the name of the index\nusers is the table\nUSING gin (email gin_trgm_ops)\n\ngin specifies that it‚Äôs a gin index\nemail is the field\ngin_trgm_ops is from the pg_trgm extension. It splits the index into trigrams which is necessary for the gin index to work with LIKE or ILIKE\n\nSlower to update than the standard ones. So you should avoid adding them to a frequently updated table.\n\nGiST indexes are very good for dynamic data and fast if the number of unique words (lexemes) is under 100,000, while GIN indexes will handle 100,000+ lexemes better but are slower to update.\n\n\nNULLS LASTputs the NULLS in a field in any sorting operations at the end\n\nThe default behavior of ORDER BY will put the NULLS first, so if you use LIMIT , you might get back a bunch of NULLS.\nUsing NULLS LAST fixes this behavior but its slow even on an indexed column\n\nExample: ORDER BY email DESC NULLS LAST LIMIT 10\n\nInstead use two queries\nSELECT *\nFROM users\nORDER BY email DESC\nWHERE email IS NOT NULL LIMIT 10;\n\nSELECT *\nFROM users\nWHERE email IS NULL LIMIT 10;\n\nThe first one would fetch the sorted non-null values. If the result does not satisfy the LIMIT, another query fetches remaining rows with NULL values.\n\n\nRebuild Null Indexes\nDROP INDEX CONCURRENTLY users_reset_token_ix;\nCREATE INDEX CONCURRENTLY users_reset_token_ix ON users(reset_token)\nWHERE reset_token IS NOT NULL;\n\nDrops and rebuilds an index to only include NOT NULL rows\nusers_reset_token_ix is the name of the index\nusers is the table\nI assume ‚Äúreset_token has to be the field\n\nWrap multiple db update queries into a single transaction\n\nImproves the write performance unless the database update is VERY large.\nA large-scale update performed by a background worker process could potentially timeout web server processes and cause a user-facing app outage\nFor large db updates, add batching\n\nExample: db update has a 100K rows, so update 10K at a time.\nUPDATE messages SET status = 'archived'\n¬† WHERE id IN\n¬† (SELECT ID FROM messages ORDER BY ID LIMIT 10000 OFFSET 0);\nUPDATE messages SET status = 'archived'\n¬† WHERE id IN\n¬† (SELECT ID FROM messages ORDER BY ID LIMIT 10000 OFFSET 10000);\nUPDATE messages SET status = 'archived'\n¬† WHERE id IN\n¬† (SELECT ID FROM messages ORDER BY ID LIMIT 10000 OFFSET 20000);\n\nmessages is the table name\nI guess OFFSET is what‚Äôs key here.",
    "crumbs": [
      "Databases",
      "Engineering"
    ]
  },
  {
    "objectID": "qmd/db-engineering.html#sec-db-eng-ets",
    "href": "qmd/db-engineering.html#sec-db-eng-ets",
    "title": "Engineering",
    "section": "Event Tracking Systems",
    "text": "Event Tracking Systems\n\nEvents are queued, then batch inserted into your db.\n\nStreaming events does not scale very well and is not fault tolerant.\n\nCommercial Services\n\nSegment\n\nMost popular option\nVery expensive\nSusceptible to ad blockers\nOnly syncs data once per hour or two\nMissing a few key fields in the schema it generates (specifically, session and page ids).\n\nFreshpaint is a newer commercial alternative that aims to solve some of these issues.\n\nOpen Source (each with a managed offering if you don‚Äôt feel like hosting it yourself)\n\nSnowplow is the oldest and most popular, but it can take a while to setup and configure.\nRudderstack is a full-featured Segment alternative.\nJitsu is a pared down event tracking library that is laser focused on just getting events into your warehouse as quickly as possible.",
    "crumbs": [
      "Databases",
      "Engineering"
    ]
  },
  {
    "objectID": "qmd/db-engineering.html#sec-db-eng-stream",
    "href": "qmd/db-engineering.html#sec-db-eng-stream",
    "title": "Engineering",
    "section": "Streaming",
    "text": "Streaming\n\nStreaming or near real-time (i.e.¬†micro-batch) data\nQuestions\n\nWhat would be the data flow rate in that pipeline?\nDo you require real-time analytics, or is near-real-time sufficient?¬†\n\nData Characteristics\n\nIt is ingested near-real-time.\nUsed for real-time reporting and/or calculating near-real-time aggregates. Aggregation queries on it are temporal in nature so any aggregations defined on the data will be changed over time as the data comes.\nIt is append-only data but can have high ingestion rates so needs support for fast writes.\nHistorical trends can be analyzed to forecast future metrics.\n\nRelational databases can‚Äôt handle high ingestion rates and near-real-time aggregates without extensions.\nSteaming is the most expensive way to process the data in the majority of cases. Typically batch ingesting into warehouses is free, but streaming may not be.\nUse Cases: anomaly detection and fraud prevention, real-time personalized marketing and internet of things.\nTools:\n\nApache Kafka - Flexible, connects to app servers, other microservices, databases, sensor networks, financial networks, etc. and can feed the data to same types of systems including analytical tools.\n\nUtilizes a publish-subscribe model where producers (i.e.¬†sources) publish data to topics and consumers (e.g.¬†DBs, BI tools, Processing tools) subscribe to specific topics to receive relevant data.\nHighly scalable due to its distributed architecture, allowing data handling across multiple nodes.\nConfluent‚Äôs Kafka Connect - Open source and Commerical Connectors\n\nApache Flume - Similar to Kafka but easier to manage, more lightweight, and built to output to storage (but not as flexible as Kafka)\n\nLess scalable as data ingestion is handled by individual agents, limiting horizontal scaling.\nIts lightweight agents and simple configuration make it ideal for log collection\nCan also handle Batch workloads\nAble to perform basic preprocessing, e.g.¬†filtering specific log types or converting timestamps to a standard format\n\nAmazon Kinesis - A managed, commercial alternative to Kafka. Charges based on data throughput and storage. Additional features include data firehose for delivery to data stores and Kinesis analytics for real-time analysis.\nApache Flink - Processes streaming data with lower latency than Spark Streaming, especially at high throughputs. Less likely to duplicate data. Uses SQL. Steeper learning curve given its more advanced features.\nApache Spark Streaming - See Apache, Spark &gt;&gt; Streaming\nGoogle Pub/Sub - Uses Apache Beam programming API to construct processing pipelines\n\nGoogle Dataflow can create processing pipelines using streaming data from Pub/Sub. Developers write their pipelines using Beam‚Äôs API, and then Beam translates them into specific instructions for Flink or Spark to execute.\n\n{{temporian}} can interact with Beam to perform various time series preprocessing\n\nIf you have existing workflows around Hadoop or Spark or expertise in those frameworks, then Google Dataproc allows you to reuse that code. It also allows you to used other libraries that aren‚Äôt available in Dataflow. Supports various languages like Java, Python, and Scala.\nFor short-lived batch jobs, Dataproc might be more cost-effective. Although, Dataflow‚Äôs serverless nature avoids idle resource charges while Dataproc clusters incur costs even when idle.\n\n\nArchitectures\n\nNotes from\n\nData Pipeline Design Patterns\n\nETL\n\n\nKinesis collects data from a server (e.g.¬†app) and continuously feeds it to a lambda function for transformation. Transformed data is deposited into a S3 bucket, queried using Athena, and visualized using Quicksight.\n\nHybrid (Streaming and Batch)\n\n\nKinesis streams data to S3 and when a threshold is reached, a lambda trigger activates a transformation/batch load to the BQ warehouse\n\n\nTimeScale DB\n\nOpen source extension for postgresql\nSupport all things postgresql like relational queries, full SQL support(not SQL-like) as well as the support of real-time queries\nSupports an ingestion of 1.5M+ metrics per second per server\nNear-real-time aggregation of tables\nProvides integration with Kafka, kinesis, etc for data ingestion.\nCan be integrated with any real-time visualization tool such as Graphana\n\nPipeline DB\n\nOpen source extension for postgresql\nSimilar features as TimeScale DB\nEfficiency comes from it not storing raw data\n\nUsually, it‚Äôs recommended to store raw data",
    "crumbs": [
      "Databases",
      "Engineering"
    ]
  },
  {
    "objectID": "qmd/db-engineering.html#sec-db-eng-otools",
    "href": "qmd/db-engineering.html#sec-db-eng-otools",
    "title": "Engineering",
    "section": "Other Tools",
    "text": "Other Tools\n\nDataFold monitors your warehouse and alerts you if there are any anomalies (e.g.¬†if checkout conversion rate drops suddenly right after a deploy).\nHightouch lets you sync data from your warehouse to your marketing and sales platforms.\nWhale is an open source tool to document and catalog your data.¬†\nRetool lets you integrate warehouse data into your internal admin tools.\nGrowth Book that plugs into your data warehouse and handles all of the complicated querying and statistics required for robust A/B test analysis.",
    "crumbs": [
      "Databases",
      "Engineering"
    ]
  },
  {
    "objectID": "qmd/db-lakes.html",
    "href": "qmd/db-lakes.html",
    "title": "Lakes",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Databases",
      "Lakes"
    ]
  },
  {
    "objectID": "qmd/db-lakes.html#sec-db-lakes-misc",
    "href": "qmd/db-lakes.html#sec-db-lakes-misc",
    "title": "Lakes",
    "section": "",
    "text": "Data is stored in structured format or in its raw native format without any transformation at any scale.\n\nHandling both types allows all data to be centralized which means it can be better organized and more easily accessed.\n\nOptimal for fit for bulk data types such as server logs, clickstreams, social media, or sensor data.\nIdeal use cases\n\nBackup for logs\nRaw sensor data for your IoT application,\nText files from user interviews\nImages\nTrained machine learning models (with the database simply storing the path to the object)\n\nTools\n\nRclone - A command-line program to manage files on cloud storage. It is a feature-rich alternative to cloud vendors‚Äô web storage interfaces. Over 70 cloud storage products support rclone including S3 object stores\n\nLower storage costs due to their more open-source nature and undefined structure\nOn-Prem set-ups have to manage hardward and environments\n\nIf you wanted to separate stuff like test data from production data, you also probably had to set up new hardware.\nIf you had data in one physical environment that had to be used for analytical purposes in another physical environment, you probably had to copy that data over to the new replica environment.\n\nHave to keep a tie to the source environment to ensure that the stuff in the replica environment is still up-to-date, and your operational source data most likely isn‚Äôt in one single environment. It‚Äôs likely that you have tens ‚Äî if not hundreds ‚Äî of those operational sources where you gather data.\n\nWhere on-prem set-ups focus on isolating data with physical infrastructure, cloud computing shifts to focus on isolating data using security policies.\n\nObject Storage Systems\n\nCloud data lakes provide organizations with additional opportunities to simplify data management by being accessible everywhere to all applications as needed\nOrganized as collections of files within directory structures, often with multiple files in one directory representing a single table.\n\nPros: highly accessible and flexible\nMetadata Catalogs are used to answer these questions:\n\nWhat is the schema of a dataset, including columns and data types\nWhich files comprise the dataset and how are they organized (e.g., partitions)\nHow different applications coordinate changes to the dataset, including both changes to the definition of the dataset and changes to data\n\nHive Metastore (HMS) and AWS Glue Data Catalog are two popular catalog options\n\nContain the schema, table structure and data location for datasets within data lake storage\n\n\nIssues:\n\nDoes not coordinate data changes or schema evolution between applications in a transactionally consistent manner.\n\nCreates the necessity for data staging areas and this extra layer makes project pipelines brittle",
    "crumbs": [
      "Databases",
      "Lakes"
    ]
  },
  {
    "objectID": "qmd/db-lakes.html#sec-db-lakes-brands",
    "href": "qmd/db-lakes.html#sec-db-lakes-brands",
    "title": "Lakes",
    "section": "Brands",
    "text": "Brands\n\nHadoop\n\nTraditional format for data lakes\n\nAmazon S3\n\nTry to stay &lt;1000 entries per level of hierarchy when designing the partitioning format. Otherwise there is paging and things get expensive.\nAWS Athena ($5/TB scanned)\n\nAWS Athena is serverless and intended for ad-hoc SQL queries against data on AWS S3\n\n\nMicrosoft Azure Data Lake Storage (ADLS)\nMinio\n\nOpen-Source alternative to AWS S3 storage.\nGiven that S3 often stores customer PII (either inadvertently via screenshots or actual structured JSON files), Minio is a great alternative to companies mindful of who has access to user data.\n\nOf course, AWS claims that AWS personnel doesn‚Äôt have direct access to customer data, but by being closed-source, that statement is just a function of trust.\n\n\nDatabricks Delta Lake -\nGoogle Cloud Storage\n\n5 GB of US regional storage free per month, not charged against your credits.\n\nApache Hudi - A transactional data lake platform that brings database and data warehouse capabilities to the data lake. Hudi reimagines slow old-school batch data processing with a powerful new incremental processing framework for low latency minute-level analytics.",
    "crumbs": [
      "Databases",
      "Lakes"
    ]
  },
  {
    "objectID": "qmd/db-lakes.html#sec-db-lakes-iceb",
    "href": "qmd/db-lakes.html#sec-db-lakes-iceb",
    "title": "Lakes",
    "section": "Apache Iceberg",
    "text": "Apache Iceberg\n\nOpen source table format that addresses the performance and usability challenges of using Apache Hive tables in large and demanding data lake environments.\n\nOther currently popular open table formats are Hudi and Delta Lake.\n\nInterfaces\n\nDuckDB can query Iceberg tables in S3 with an extension, docs\nAthena can create Iceberg Tables\nGoogle Cloud Storage has something called BigLake that can create Iceberg tables\n\nFeatures\n\nTransactional consistency between multiple applications where files can be added, removed or modified atomically, with full read isolation and multiple concurrent writes\nFull schema evolution to track changes to a table over time\nTime travel to query historical data and verify changes between updates\nPartition layout and evolution enabling updates to partition schemes as queries and data volumes change without relying on hidden partitions or physical directories\nRollback to prior versions to quickly correct issues and return tables to a known good state\nAdvanced planning and filtering capabilities for high performance on large data volumes\nThe full history is maintained within the Iceberg table format and without storage system dependencies\n\nComponents\n\nIceberg Catalog - Used to map table names to locations and must be able to support atomic operations to update referenced pointers if needed.\nMetadata Layer (with metadata files, manifest lists, and manifest files) - Stores instead all the enriching information about the constituent files for every different snapshot/transaction\n\ne.g.¬†table schema, configurations for the partitioning, etc.\n\nData Layer - Associated with the raw data files\n\nSupports common industry-standard file formats, including Parquet, ORC and Avro\nSupported by major data lake engines including Dremio, Spark, Hive and Presto\nQueries on tables that do not use or save file-level metadata (e.g., Hive) typically involve costly list and scan operations\nAny application that can deal with parquet files can use Iceberg tables and its API in order to query more efficiently\nComparison",
    "crumbs": [
      "Databases",
      "Lakes"
    ]
  },
  {
    "objectID": "qmd/docker-aws.html",
    "href": "qmd/docker-aws.html",
    "title": "AWS",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Docker",
      "AWS"
    ]
  },
  {
    "objectID": "qmd/docker-aws.html#sec-docker-aws-misc",
    "href": "qmd/docker-aws.html#sec-docker-aws-misc",
    "title": "AWS",
    "section": "",
    "text": "Notes from Linkedin Learning Docker on AWS\n\nThe example used in this class is for a web server",
    "crumbs": [
      "Docker",
      "AWS"
    ]
  },
  {
    "objectID": "qmd/docker-aws.html#sec-docker-aws-summ",
    "href": "qmd/docker-aws.html#sec-docker-aws-summ",
    "title": "AWS",
    "section": "Summary",
    "text": "Summary\n\nPush docker image to ECR\nPush app code and build instructions (buildspec yaml) to CodeCommit\nCreate CodeBuild project that executes image building instructions\nCreate a Pipeline that triggers CodeBuild (automatic image build when new code is committed)\nChoose a Cluster method (fargate or manual EC2), then create and start cluster instances\n\nonly able to specify number of instances to create with the EC2 method\nfargate handles most of the configuration (cost extra?)\n\nCreate a task or a service\n\nA task is for short running jobs, no load balancer or autoscaling. Its definition details the container configuration; how much of the resources you want your workloads (e.g.¬†app) to be able to use; communication between containers, etc.\nA service is for long running jobs. Creates tasks and autoscales number of instances and load balances traffic¬†\n\nAdd a storage container and update task definition to include a shared volume",
    "crumbs": [
      "Docker",
      "AWS"
    ]
  },
  {
    "objectID": "qmd/docker-aws.html#sec-docker-aws-gloss",
    "href": "qmd/docker-aws.html#sec-docker-aws-gloss",
    "title": "AWS",
    "section": "Glossary of AWS Services Used",
    "text": "Glossary of AWS Services Used\n\nECR stores your images that you build\nCodeCommit (CC) is like a github (code storage)\nCodeBuild sets up the process of using a yaml file in your CC repo as instructions to build the images\nPipeline is the CI/CD part. Triggers an image build every time there‚Äôs a new push to CodeCommit¬†\nRoute 53 takes your domain name (www.store.com/app), creates a DNS ip address and reroutes traffic from that domain to your load balancer.\nContainer Networking Models\n\nHost - direct mapping to host networking (EC2)\n\nuse when performance is prime concern\nonly 1 container per task per port\n\nAwsvpc - ENI per task, required for fargate\n\nmost flexibility\nrecommended for most use cases\n\nBridge - ‚ÄúClassic‚Äù docker networking\n\ndidn‚Äôt get discussed\n\nNone - multi-container localhost and storage\n\nonly local connectivity (i.e.¬†communication between containers within a task)\n\nSecurtiy groups available for Host and AWSvpc\n\nsecurity groups allow for tracking container performance and limiting access",
    "crumbs": [
      "Docker",
      "AWS"
    ]
  },
  {
    "objectID": "qmd/docker-aws.html#sec-docker-aws-meths",
    "href": "qmd/docker-aws.html#sec-docker-aws-meths",
    "title": "AWS",
    "section": "Two Methods For Running Containers on AWS",
    "text": "Two Methods For Running Containers on AWS\n\nManaging the EC2 instances yourself (see section below for set-up instructions)\n\nif you understand the capacity you need and want greater control, this might be better\nYou pay for unused capacity\nBilling is like the standard billing for using an EC2 instance¬†\n\nFargate (see section below for set-up instructions)\n\nManaged by Amazon, less control, less to deal with\nYou don‚Äôt have to deal with starting, stopping, choosing compute sizes, capacities etc. of EC2 instances\nBilled by CPU/Memory and Time that‚Äôs used by your container",
    "crumbs": [
      "Docker",
      "AWS"
    ]
  },
  {
    "objectID": "qmd/docker-aws.html#sec-docker-aws-ecr",
    "href": "qmd/docker-aws.html#sec-docker-aws-ecr",
    "title": "AWS",
    "section": "Elastic Container Registry (ECR)",
    "text": "Elastic Container Registry (ECR)\n\nCreate an ECR account\n\nLog into your account\nsearch ecr\nClick create ‚Äúget started‚Äù under Create a Repository (mid right)\n\nSays you pay for the amount of data you store in the repository and data transferred to the internet.\n\nReason for doing this is latency. The repo is regional and you want your image/app to be close to the host\n\n\nAssign a name\n\nfirst part is a hash + region + amazon.com\nyou add a name. whatever you want\n\nmutable/immutable\n\nIf you‚Äôre going to be storing multiple versions of the same image, you should choose mutable.\n\nClick create repository (bottom right)\n\nPush image to ECR repo\n\ncopy the URI for your repo from the ecr console (ecr ‚Äì left panel ‚Äì repositories ‚Äì images)\n\nsave it to registry-tag.txt file in your local image directory\nAlso include it as the tag to your docker image\n\ndocker build . -t &lt;URI&gt;\n\nauto-appends ‚Äú:latest‚Äù\n\n\n\nIn terminal\n\n(aws ecr get-login --no-include-email --region &lt;region&gt;}\n\n*with parentheses\nregion is whatever you have in your profile e.g.¬†us-east-2\ngets login from the aws profile you‚Äôve already set-up\nprints some kind of warning, he didn‚Äôt act like it was meaningful\n\ndocker push &lt;tag&gt;\n\nwhich is your URI:\n\nin the example, the version is ‚Äúlatest‚Äù\n\n\nIf something doesn‚Äôt work, the instructions are at aws\n\nIn repository console\n\nclick repo name\nclick view push commands (top right)\nshows how to connect to repository and ‚Äúpush‚Äù instance\n\n\n\nIn console, hit refresh (mid-right) to see that the image is loaded into the repo",
    "crumbs": [
      "Docker",
      "AWS"
    ]
  },
  {
    "objectID": "qmd/docker-aws.html#sec-docker-aws-codcom",
    "href": "qmd/docker-aws.html#sec-docker-aws-codcom",
    "title": "AWS",
    "section": "CodeCommit",
    "text": "CodeCommit\n\nCreate a CodeCommit git repository - benefit is having (image/app) code live near hosting service, less latency for CI/CD processes\n\nDeveloper tools ‚Äì CodeCommit ‚Äì (maybe left panel ‚Äì Source ‚Äì Repositories)\nClick create repository (top right)\n\nEnter name\n\nDoesn‚Äôt have to match the name of the image repo, but might be worth doing\nalso a box for entering a description\n\nClick create\n\nConnection Steps\n\nhttps or ssh\n\nclick ssh\n\nfollow these directions to gitbash and create SSH keys for windows, enter them into config file, clone repository, etc. etc.\n\nhttps://docs.aws.amazon.com/codecommit/latest/userguide/setting-up-ssh-windows.html\nHe cloned the repo, one directory above the .ssh directory\n\n\n\nPush Container Code to CodeCommit repo\n\ncd to cloned repo directory\ncopy code files to that directory\ngit add *, git commit -m ‚Äúblah blah‚Äù, git push -u origin master\n\n-u is for upstream\n‚Äú-u origin master‚Äù¬† necessary for a first push\n\nFiles should be present in developer tools ‚Äì CodeCommit ‚Äì Source ‚Äì Repositories ‚Äì repo",
    "crumbs": [
      "Docker",
      "AWS"
    ]
  },
  {
    "objectID": "qmd/docker-aws.html#sec-docker-aws-codcomproj",
    "href": "qmd/docker-aws.html#sec-docker-aws-codcomproj",
    "title": "AWS",
    "section": "Create a CodeBuild Project",
    "text": "Create a CodeBuild Project\n\nBuild the CodeCommit repo into a docker container\nbuildspec.yml (see hostname folder in exercise files)\n\nyaml script that automates building docker, logging into ECR, building an image, and pushing it to ECR\ncodebuild version used was 2.0 (which is at the top of the yaml script)\n\ncodebuild must be some aws tool you can use to do this\n\nadd, commit, push to CodeCommit repo\n\ndeveloper tools ‚Äî codecommit ‚Äì left panel ‚Äì build ‚Äì build projects\n\nclick create build project (upper right)\n\nanything not listed below, just used defaults\n\nenter project name\n\nhe gave same name as CC repo\n\nUnder Source, make sure it says CodeCommit, enter repo name in box\nMake sure Manage Image box is ticked\nOperating System\n\nhe used Ubuntu\n\nRuntime\n\nselect Standard\n\nImage\n\nstandard 2.0\n\nPriviledged\n\ntick box ‚ÄúEnable this flag if you want to build Docker images or want your builds to have elevated priviledges‚Äù\n\nLogs\n\nUsing cloudwatch\n\ngroup name - codebuild\nStream Name\n\nhe used the name of the CC repo\n\n\n\nClick Create build project (bottom right)\n\nGoto IAM console ‚Äì left panel ‚Äì Roles\n\nWhen the ‚Äúbuild project‚Äù was created a role was also created\n\nUnder Role name -¬† click ‚Äúcodebuild--service-role‚Äù\nclick attach policy (mid left)\nSearch for AmazonEC2ContainerRegistryPowerUser\ntick box to select it\nclick attach policy (bottom right)\n\n\nGoto Developer tools ‚Äì CodeBuild ‚Äì left panel ‚Äì Build ‚Äì Build Project ‚Äì project name\n\nClick Start Build (top right)\nkeep all defaults, Click Start Build (bottom right)\n\nProject builds and under Build Status, status should say ‚Äúsucceeded‚Äù when it finishes\n\nWhich means there are now two images in the ECR repo\n\noriginal push and image built from this project build process (duplicate)\n\n\nAutomate building container when new code is pushed (CI/CD)\n\ndeveloper tools ‚Äì codebuild ‚Äì left panel ‚Äì pipeline ‚Äì pipelines\nclick create pipeline\n\nenter pipeline name\n\nhe named it the CC repo name, hostname\nclick next\n\nAdd source stage\n\nchoices\n\nCodeCommit\nECR\nS3\nGithub\nChoose codecommit\n\nSelect repo name\n\nexample: ‚Äúhostname‚Äù\n\nSelect branch\n\nexample ‚Äúmaster‚Äù\n\nDetection option\n\nselect CloudWatch\n\nClick next\n\nAdd build stage\n\nCodeBuild or Jenkins\n\nchoose CodeBuild\n\nRegion\n\nexample US East - (Ohio)\n\nProject Name\n\nname of the build project from last section\nexample hostname\n\nClick next\n\nAdd deploy stage\n\nskipped, because something I didn‚Äôt understand. Sound like another level of automation that might be used in the future\nclick skip deploy stage\n\nReview\n\nClick create pipeline (bottom right)\n\n\nOnce created, it will start building the pipeline from the CodeCommit source\n\nProcess takes a few minutes\ndetects the buildspec.yml in CC and executes it\nunder Build Section, there will be a details link, you can right-click and open it in a new tab\n\nShould result in a 3rd image (duplicate images) in the ECR repo\n\nSo anytime a new commit is pushed to CodeCommit, an image will be built and stored in ECR",
    "crumbs": [
      "Docker",
      "AWS"
    ]
  },
  {
    "objectID": "qmd/docker-aws.html#sec-docker-aws-ec2user",
    "href": "qmd/docker-aws.html#sec-docker-aws-ec2user",
    "title": "AWS",
    "section": "Create Cluster: EC2 (User-Managed)",
    "text": "Create Cluster: EC2 (User-Managed)\n\nCreate Cluster: Set-up instructions for running containers using EC2 method\n\nSearch for ECS\nleft panel ‚Äì Under Amazon ECS: Clusters\n\nClick create cluster\nChoose Linux + Networking\n\nWindows + Networking and Networking-only (Fargate see below) options also available\nclick next (bottom right)\n\nConfigure Cluster\n\nEnter Cluster name\n\nexample: ecs-ec2\n\nProvisioning\n\nOn demand instance\nspot instance\n\nEC2 instance type (size of compute)\n\nexample: t2.medium\n\nNumber of instances\n\nhe chose 1\n\nEC2 AMI id\n\nLinux-1, linux-2\n\nhe chose linux-2; didn‚Äôt give a reason\n\n\nDefaults kept for Virtual Private Cloud (VPC), Security Group, storage, etc.\nCloudWatch container insights\n\ntick enable container insights\nso you can monitor stats in Cloudwatch and help you tune compute resources in the future\n\nClick create\n\ntakes a minute or two to spin up the instance",
    "crumbs": [
      "Docker",
      "AWS"
    ]
  },
  {
    "objectID": "qmd/docker-aws.html#sec-docker-aws-ec2ssh",
    "href": "qmd/docker-aws.html#sec-docker-aws-ec2ssh",
    "title": "AWS",
    "section": "Create Cluster: EC2 with SSH Access",
    "text": "Create Cluster: EC2 with SSH Access\n\nCreate Cluster using EC2 method with SSH access (not possible with a Fargate cluster) and connect to it\nSteps\n\nFind your ssh public key\n\ngo into git bash and type ‚Äúcat ~/.ssh/id_ed25519.pub‚Äù\n\noptions\n\nid_rsa.pub\nid_ecdsa.pub\nid_ed25519.pub\n\nI have 2, rsa that I created when linking rstudio to github and ed25519 when I created gitlab acct\n\nCopy everything (including the ssh-filename beginning part) all the way until your email (don‚Äôt include)\n\nGoto EC2 services page (open new tab)\n\nUnder Resources (mid), click Key Pairs\n\nClick import\npaste key into Public Key Contents box\nenter a name\n\nexample ecs-ec2-key\n\nclick import\n\n\nGo back to the ECS services page and create another cluster\n\nSame as before. (create cluster - EC2 method above) except:\n\ncluster name - ecs-ec2-ssh\nkey pair - chose newly imported key pair\nNetworking\n\nvpc\n\ndrop down\n\nchoose vpc created by prev. cluster (some big long hash)\n\n\nsubnets\n\ndropdown\n\nchoose subnet created by prev. cluster\nspawns another dropdown to add another subnet\n\ndropdown\n\nchoose second subnet created by prev.cluster\n\nShould only be 2, they‚Äôre names should gray-out after you choose them\n\nsecurity group\n\nchoose the one created by the prev. cluster\n\n\nHaving SSH available will allow us to go into the container and view docker ressource\n\n\nCopy public ip address and open SSH port (also see AWS notebook ‚Äì EC2 ‚Äì connect/terminate instance)\n\nClick on Cluster name\nclick on ECS instances tab (mid left)\nright-click EC2 Instance id and open in new tab\n\ncopy IPv4 Public IP (lower right)\nclick security group link (lower left)\n\nclick inbound tab (lower left)\nclick edit\n\nclick add rule\nunder Type, click dropdown and select SSH\n\nautomatically chooses port 22\n\nSource\n\nkept 0.0.0.0¬† (‚Äúv4 address‚Äù, guess 4 is for the 4 numbers in the address)\n\nDescription\n\nkept default\n\nclick save\n\n\n\n\nOpen terminal\n\nssh -i ~/.ssh/id_rsa ec2-user@\n\nasks if you‚Äôre sure, say yes\n\nCheck container status on instance\n\nsudu su -\n\nswitches to being a root user\n\ndocker ps\n\nshows container id and name, image, status, etc.\n\n\nexec into container\n\ndocker exec -it  sh\n\nonly works if linux image has a shell environment\ninstead of sh, can try bash\nor docker run¬† ‚Äìrm ‚Äìname linux -it alpine:latest sh\nctrl + d\n\nleave shell\n\nexit\n\nto exit as root user\n\nexit\n\nleaves instance, closes connection",
    "crumbs": [
      "Docker",
      "AWS"
    ]
  },
  {
    "objectID": "qmd/docker-aws.html#sec-docker-aws-farg",
    "href": "qmd/docker-aws.html#sec-docker-aws-farg",
    "title": "AWS",
    "section": "Create Cluster: Fargate (AWS-Managed)",
    "text": "Create Cluster: Fargate (AWS-Managed)\n\nSet-up instructions for running containers using Fargate method\n\nSearch for ECS\nleft panel ‚Äì Under Amazon ECS: Clusters\n\nClick create cluster\n\nChoose Network-only (amazon fargate)\nEnter Cluster name\n\nexample ecs-fargate\n\ntick box for Enable Container insights (cloudwatch)\nclick create (bottom right)\n\n\nCluster created instantaneously\n\nclick view cluster",
    "crumbs": [
      "Docker",
      "AWS"
    ]
  },
  {
    "objectID": "qmd/docker-aws.html#sec-docker-aws-catd",
    "href": "qmd/docker-aws.html#sec-docker-aws-catd",
    "title": "AWS",
    "section": "Creating a Task Defintion",
    "text": "Creating a Task Defintion\n\nA task definition is a blueprint for your tasks, specifying what container image to use, how much CPU and memory is needed, and other configurations.\nIf using load balancer goto the ‚Äúcreate appplication load balancer‚Äù and¬† ‚Äúadd ecs service and task‚Äù below\ndetails the images to use, the CPU and memory to allocate, environment variables, ports to expose, and how the containers interact.\n\nassociates the cluster created in the previous section with the app or workload\n\n1 task definition can be used in multiple containers\nsearch for ECS\nleft panel ‚Äì Clusters ‚Äì task definitions\n\nclick create new task definition\nselect cluster method (Fargate or EC2)\n\nchoose fargate\nclick next step\n\ncreate task-definition name\n\neg hostname-fargate\n\ntask role\n\nused if workload creates other resources inside aws\nleave blank\nsome kind of warning about the network settings, he ignored it.\n\ntask execution iam role\n\ngives permission for cluster to use task defintion\nkeep default\n\ntask size\n\nmemory size choice effects available choices for cpu\ndepends on your application needs, if running multiple containers with this definition, etc.\n\nhe chose the smallest for each just because this is for illustrative purposes\n\n\ncontainer definitions\n\nassigns which containers will be using this definition\nclick add container\n\ncontainer name\n\nwhatever you want, he chose hostname\n\nimage\n\ngoto services (top left) (open new tab) ‚Äì left panel ‚Äì ecr ‚Äì left panel ‚Äì repositories\n\nclick image repo name\ncopy image uri that you want to associate with the definition\ngo back to the task definitions tab\n\npaste uri into the box\n\nIf you want to always use the latest image and your uri has build version tag, replace the build tag with ‚Äúlatest‚Äù\n\nbuild tag starts at ‚Äúbuild‚Äù and goes to the end of the uri\n\n\n\nauthentication only necessary if ecr repo is private\nsoft memory limit\n\nspecify a memory limit for the container\nleaving blank says only limit will be the memory size of the task definition (see 6.)\n\nport mappings\n\nwhatever is specified in dockerfile/image\nexample nginx image exposes 80 tcp\n\nAdvanced options\n\nhealthcheck\n\nsome cli code that allows you to check if your container is running properly at the container level\nhe already has this in his buildspec.yml code (see create CodeBuild project section)\n\nhealthcheckz is a check at the system level\n\n\nEnvironment, network settings, volumes\n\nseems like a bunch of stuff that would be used in a docker run command or in a docker-compose file\nall left blank\n\n\nclick add\n\n\nvolumes\n\nexternal volume to be shared\nleft blank\n\nclick create\n\nTakes you to launch status\n\nclick view task definition",
    "crumbs": [
      "Docker",
      "AWS"
    ]
  },
  {
    "objectID": "qmd/docker-aws.html#sec-docker-aws-utdtsdvbc",
    "href": "qmd/docker-aws.html#sec-docker-aws-utdtsdvbc",
    "title": "AWS",
    "section": "Update Task Definition to Share Data Volumes Between Containers",
    "text": "Update Task Definition to Share Data Volumes Between Containers\n\nFargate cluster example\n\nAlso see the Data Volumes, Sharing Data between containers, Docker-Compose sections of part 1 of this note\n\nthink a lot of what happens in those sections is automated by using the this task definition\n\nSearch ECS ‚Äì Left panel ‚Äì Task Definitions\n\nclick on fargate task definition\n\nclick on latest revision of the definition\n\ndefinitions are versioned\nclick on create new revision\n\nscroll down to click on add container\n\ncontainer name\n\nwhatever, fargate-storage (he called his hostname-v2)\n\nimage\n\nadd image uri (see creating task definition above)\nhe used the same image as the first container. This becomes a problem because he has two containers using the same port since both nginx containers are using 80. See troubleshooting section below. Also mentioned in part 1 ‚Äì running containers ‚Äì flags ‚Äì p\nThink for data science we‚Äôd use a postgressql, redis, etc. image\n\nEnvironment\n\nThink this was for display purposes. He added one just so when he went to the webpage and it displayed the container names, we could tell the difference. The first container said version 1 and this one says version two.\nenvironment variables\n\nkey\n\nexample VERSION\n\nvalue\n\nexample versionTwo\n\n\n\nclick add\n\nVolumes\n\nclick add volume\n\nname\n\nwhatever\nexample shared-volume\n\nclick add\n\n\nGO BACK to container section\n\nDo this for each container: click on the container name\n\nStorage and Logging\n\nmount points\n\nsource volume\n\nclick dropdown and select volume name\n\nexample from above: shared-volume\n\n\ncontainer path\n\nhe added the shared folder path and it was the same for both containers\nsee part 1 Data Volumes and Sharing Data between containers sections\n\nI think using that example, we‚Äôd specify¬†‚Äú/app/public/‚Äù (no quotes) for the app container. *** This dude said to add a trailing ‚Äú/‚Äù to the paths ***\nfor storage container, example redis, it‚Äôd be ‚Äú/data/‚Äù which is designated by the redis image authors.\n\n\n\n\nclick update\n\n\nClick create\n\nNote the revision number that‚Äôs given\n\n\n\nleft panel ‚Äì Clusters\n\nclick on fargate cluster\n\nclick services tab (mid left)\n\nclick service name (example is hostname) using the task definition\n\nThis was created in the Add ECS service and task section below\nclick update button (top right)\n\nConfigure Service\n\nTask Definition\n\nrevision\n\nselect revision number of the updated definition\n\n\nclick next\n\nclick next all the way to review\n\nclick update service\n\nclick view service\n\n\n\nclick tasks tab (mid left)\n\nrefresh (mid right) and watch new task start up with ‚Äúprovisioning‚Äù status and then ‚Äúrunning‚Äù\n\n\n\ncan go to ip address with volume path appended to the address to see that the volumes are up and running\n\nnot sure if this would work with a database example or not",
    "crumbs": [
      "Docker",
      "AWS"
    ]
  },
  {
    "objectID": "qmd/docker-aws.html#sec-docker-aws-ratwaad",
    "href": "qmd/docker-aws.html#sec-docker-aws-ratwaad",
    "title": "AWS",
    "section": "Running a Task with an Available Definition",
    "text": "Running a Task with an Available Definition\n\nleft panel ‚Äì Clusters\n\nClick Cluster your using for the task definition\n\nhe used the fargate one he created\n\nclick tasks tab (mid left)\nclick run new task\n\ntick fargate launch type\ncluster vpc,¬†subnets\n\nclick dropdown boxes\nit‚Äôll show the ones that were made during cluster creation\n\nchoose vpc and both subnets\n\n\nSecurity group\n\ncreates one for you with default rules which you can keep\n\nAlso can manipulate after created by going to EC2 ‚Äì left panel ‚Äì Network and Security ‚Äì Security Groups\n\nOr click edit button to specify ports, choose existing security group, etc\n\nadd additional port\n\ntype\n\nselect custom with tcp protocol\n\nport range\n\n81-90\ncontainer must be configured to be able to listen on the range of ports\n\nSource\n\ncan choose a group that allows you to connect with other tasks in the environment and limit access\nhe kept Anywhere\n\nclick save\n\n\n\nclick run task (bottom right)\n\nclick the task hash under Task column\n\nat the bottom, you can watch the status turn from ‚Äúpending‚Äù to ‚Äúrunning‚Äù\n\nrefresh button (right)\n\ncopy the public ip under Network section\n\npaste into address bar + port\n\nexample 3.15.13.43:80\nexample: for his nginx server, it just displayed the private ip and the image name\n\n\nSimple way for a minor scale up the access to the application is to duplicate the task definition (also see autoscaling section below)\n\nleft panel ‚Äì Clusters\n\nselect the same cluster\nclick tasks tab again\nclick task hash id again\n\nclick ‚Äúrun more like this‚Äù (top right)\n\ntick fargate again\nselect the same vpc and subnets again\nclick run task\n\nget the public ip the same way as before\nnow two ips are available for the app",
    "crumbs": [
      "Docker",
      "AWS"
    ]
  },
  {
    "objectID": "qmd/docker-aws.html#sec-docker-aws-calb",
    "href": "qmd/docker-aws.html#sec-docker-aws-calb",
    "title": "AWS",
    "section": "Create Application Load Balancer (ALB)",
    "text": "Create Application Load Balancer (ALB)\n\nAlso see AWS &gt;&gt; EC2 &gt;&gt; Configure Load Balancer and Application Ports\nsearch ec2\nleft panel ‚Äì load balancing ‚Äî load balancers\n\nclick create load balancer (top left)\n\nConfigure Load Balancer\n\nselect type\n\napplication, network or classic\n\napplication is for http, https\n\nguess this is for internet traffic coming into (and out of?) application\n\nnetwork is for tcp, tls, udp\n\nguess this would be for communication between containers\n\nclassic is for http, https, and tcp\n\nsomething about an app running on an ec2 classic network\n\nhe chose application\n\n\ngive it a name\n\nexample ecs-alb\n\nip address type\n\nipv4 (default)\n\nscheme\n\ninternal or internet facing\n\nkept internet-facing (default)\n\n\nListeners\n\nhttp, port 80\ncan add other ports if you want\n\nfor production should add a https, 80\n\n\nAvailability zones\n\nvpc, subnets\n\nselect those asscociated with the cluster\nsubnets have region specification (us-east-2a, b)\n\n\nclick next: configure security settings (bottom right)\n\nif you haven‚Äôt add https port, it‚Äôll give you a warning\n\nclick next if don‚Äôt care about https\n\n\n\nConfigure Security Settings\n\ntick box that has the name of the security group that was created during the cluster creation\n\nExample: EC2ContainerServic-ecs-ec2-EcsSecurityGroup-somehash\n\ndescription: ECS Allowed Ports\n\n\nclick Next\n\nConfigure Routing\n\ntarget group (backend of load balancer)\n\nnew target group (default)\n\nName\n\n(literally) ‚Äúdefault‚Äù\n\ntarget type\n\nInstance, IP, Lambda\nchose IP\n\nsomething about being able to use on EC2 and Fargate\n\n\nprotocol\n\nkept http\n\nport\n\nkept 80\n\nhealth check\n\nkept defaults\n\nclick next\n\nRegister targets\n\nkeep defaults\ngoing to specify this info through ecs in the next section\nclick next to review\n\nReview\n\nclick create\n\n\n\nEdit the default forwarding target\n\nA listener rule is comprised of a target (or group of targets) and conditions. When the load balancer receives a request, it checks it against the conditions in the listener rules. For whichever condition the request meets, the load balancer then sends the request to the target (e.g.¬†ip address(instance) or lambda function (code scripts)) associated with that condition.\nSearch ec2 ‚Äì left panel ‚Äì load balancing ‚Äì load balancer¬†\n\ntick the load balancer you want\nclick listener tab (mid left)\nFor listener id = http 80 and under the Rules column it will say Default: forwarding to default\n\nclick view/edit rules\n\nunder the IF column (ie the condition) it says, Requests otherwise not routed. Which means any request that doesn‚Äôt meet any of the other conditions\nclick the edit pencil icon (top left) ‚Äì click pencil icon next to http 80: default action\n\nUnder the THEN column ‚Äì click the trash can to delete ‚Äúforward to default‚Äù\nclick add action\nselect return fixed response\n\nkeep response code 503\n\nmeans server had an issue responding\n\nin Response body, type message\n\nexample: sorry, no one is home right now\nclick check mark\nclick update (top right)\n\n\n\nclick back arrow (top left)\n\n\ntest it out by clicking description tab (mid left)\n\nGet DNS (Domain Name Service) name\n\nExample blahblahamazonaws.com\nNormally, you take your domain name (www.store.com/app) and give it the Amazon Route 53 service which translates your domain name into an ip address.\nYou then reroute traffic from your domain ip address to this dns name.\n\npaste it in the browser and the error message displays",
    "crumbs": [
      "Docker",
      "AWS"
    ]
  },
  {
    "objectID": "qmd/docker-aws.html#sec-docker-aws-cesat",
    "href": "qmd/docker-aws.html#sec-docker-aws-cesat",
    "title": "AWS",
    "section": "Create ECS Service and Task",
    "text": "Create ECS Service and Task\n\nAlso see create task definition and run task sections above\nSteps\n\nsearch ecs ‚Äì left panel ‚Äì clusters ‚Äì click fargate cluster\nclick tasks tab (mid left) ‚Äì select task that was created in Create task section ‚Äì click stop button (mid left)\nclick services tab (mid left) ‚Äì click create service\n\nConfigure Service\n\nConfigure Service\n\nselect launch type\n\nchoose fargate\n\nTask Definiton and Cluster\n\nkept the ones created in sections above\n\nenter a service name\n\nexample hostname\n\nnumber of tasks\n\nexample 2\n\nclick next step\n\nDeployments\n\nkeep default, rolling update\n\nallows you to upgrade the task definition from version 1 to version 2 in a rolling fashion¬†\n\ndon‚Äôt know what he‚Äôs talking about here with versions\n\n\n\nclick next step\n\nConfigure Network\n\nService\n\ncluster vpc, subnets\n\nselect the ones that are associated with this fargate cluster\n\n¬†security group\n\nkeep default (allows traffic in)\n\nauto-assign public ip\n\nkeep default ENABLED\nwith a load balancer, we could choose to use only used private ips though\n\n\nHealth check grace period\n\nset to 60 (in seconds)\ngives the container/cluster a chance to get up an running before it tests it to see if everything is working\n\nLoad Balancing\n\ntick application load balancer\ncontainer to load balancer\n\nshows container name port:port\nclick add to load balancer\n\nproduction listener port\n\nclick dropdown ‚Äì select 80 HTTP\n\n80 is our port of the container and we chose http when we created the load balancer\n\n\npath pattern\n\nit was /hostname but he changed it to /* which is every pattern\n\nI think /hostname would that ‚Äú/hostname‚Äù would be ip address pattern associated with this container (i.e.¬†the condition or rule)\nand /* means route any request from  no matter what pattern is attached to it.\n\n\nevaluation order\n\nas soon as the first rule/condition is matched, traffic goes to that target and no other rules are considered. Lower the evaluation order, the sooner the rule is considered\nhe chose 1\n\nhealth check path\n\ndefault was /hostname\nsince he‚Äôs using /*, he changed it to /hostname/ so the healthcheck will get a webpage (code 200) and not a redirect (code 300 error)\n\nService Discovery\n\nEnables Route 53 to create a local network dns address for your container.\nUseful for when you have multiple applications talking to each other\nuntick box for enable service discovery integation since this is only one application\n\n\n\n\nclick next step\n\nSet Autoscaling\n\nSee next section on adding autoscaling\nThis can be added after this service has been created by updating\nclick next step\n\nReview\n\nclick create service\n\nCreates target group, rule/condition\n\nclick view service\nshould see two tasks starting up\ngoto the dns address on a webpage (see end of create load balancer section)\n\nbefore it displayed the error message (default target), now it shows a webpage (like in the Create Task section above)\nrefresh and it shows the second dns address associated with having a second task\n\n2 tasks means it can handle more traffic (just like the end of the create task section above)",
    "crumbs": [
      "Docker",
      "AWS"
    ]
  },
  {
    "objectID": "qmd/docker-aws.html#sec-docker-aws-utstaa",
    "href": "qmd/docker-aws.html#sec-docker-aws-utstaa",
    "title": "AWS",
    "section": "Updating the Service to Add Autoscaling",
    "text": "Updating the Service to Add Autoscaling\n\nsearch ecs ‚Äì left panel ‚Äì clusters\nClick on your cluster that you want to update its service\n\nservices tab (mid left) ‚Äì click service name or id\n\nclick update\n\nClick next until you get to Set Autoscaling\ntick configure service autoscaling\nminimum number of tasks\n\nhe chose 1\n\ndesired number of tasks\n\nhe chose 3\n\nmaximum number of tasks\n\nhe chose 5\n\nIAM role\n\nuse default ecsautoscalerole\nuse create new role if there isn‚Äôt already one available\n\nclick Add scaling policy\n\ntick step scaling\nenter policy name\n\nexample stepUp\n\nexecute policy when\n\ntick create new alarm\nalarm name\n\nexample upAlarm\n\nECS service metric\n\nCPU Utilization\n\nAlarm threshold\n\navg cpu utilization &gt; 10 (%)\nconsecutive period = 1\nperiod = 8 min\n\nhe chose 1 just for illustrative purposes\n\nclick save\n\n\nscaling action\n\nadd 1 task\nwhen cpu utilization &gt; 10\n\ncountdown period\n\namount of time it takes to make a decision\n30 sec\n\nclick save\n\nclick Add scaling policy (again)\n\nsame thing but for scaling down\navg cpu utilization\n\nhe chose &lt;= 10 but I‚Äôm not sure if that‚Äôs what you‚Äôd do in real life. I‚Äôd think you‚Äôd want some separation between the up and down scaling, but maybe not\n\nscaling action\n\nremove 1 task\n\n\nClick next to review\nclick update service\n\n\nClick tasks tab (mid left) to see how many task are currently running\nclick autoscaling tab to see the both upAlarm and downAlarm condition info\nTo see status of the targets (ip addresses of instances/containers)\n\nGoto EC2 ‚Äì left panel ‚Äì load balancing ‚Äì target groups\n\ntick the target group name of the load balancer\nUnder Registered Targets (Bottom)\n\nshows ips, status\n\nstatus == draining when auto-scaling taking a resource offline",
    "crumbs": [
      "Docker",
      "AWS"
    ]
  },
  {
    "objectID": "qmd/docker-aws.html#sec-docker-aws-troub",
    "href": "qmd/docker-aws.html#sec-docker-aws-troub",
    "title": "AWS",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n\nNotes:\n\nnormal for an active cluster without any running services or tasks to have 1 active container instance. It‚Äôs called the container management instance.\n\nExample: you notice your cluster is running 5 containers when you only desire 3\n\nCan see this in Clusters ‚Äì fargate ‚Äì services tab, under the desired tasks and running tasks columns\n\nits says 5 for desired but he chose that for his max in the autoscaling section, so I don‚Äôt know if he adjusted it for demonstration purposes or if this something confusing that AWS does.\n\nAnswer: he had both containers trying to bind to port 80 (see logs below)\nService level\n\nclick service name\n\ntasks tab\n\ncan see the task ids and definitions that the various active tasks are using\n\nexample: tasks are alternating between running and provisioning. Why are some shutting down and others starting in their place? The container is running for some time and then being stopped for some reason.\n\nclick task id\n\nlogs tab ‚Äì select container\n\nshows errors that have occurred\n\nDetails tab\n\nContainers (bottom)\n\nclick expand-arrow on desired container\n\nclick view logs in CloudWatch\n\ntakes you to CloudWatch console\n\nview the logs of the task\n\nable to filter log by events\n\ngo up one level to see log streams\n\ncan match containers and tasks to see if it might be a task issue\nend hash is the task id\n\n\n\n\n\n\n\n\nDetails tab\n\nLoad Balancing ‚Äì click target group name\n\ntargets tab\n\nshows the individual targets (ip addresses), ports, statuses\nstatus by region (if you have resources in different zones)\n\nexample: there were 2 zones -¬† us.east.2a and 2b and one had all healthy and the other had zero healthy, but he didn‚Äôt mention anything about it. Think that‚Äôs just how nodes are taken on and offline and not that there‚Äôs a regional issue.\n\n\nhealth checks tab\n\nhealthy threshold\n\nnumber of code 200s i.e.¬†healthy responses required in order for a node to be considered healthy\n\nunhealthy threshold\n\nnumber of code 300s i.e.¬†error responses required in order for the node to be considered unhealthy\n\n\n\n\nLogs tab\n\nshows the aggregate of the logs for each container (all tasks included)\nselect a container from the dropdown\n\ntimestamp, message, task id\nexample: shows a ‚Äúbind‚Äù error that says the container can‚Äôt a bind to port 80 because it‚Äôs already in use.¬† In the¬†Update task definition to share data volumes section, the second container he added was a duplicate of the nginx container and both were trying to bind to port 80. Hence the error\n\n\n\n\nCluster metrics\n\nClusters ‚Äì EC2 cluster ‚Äì metrics tab\n\nonly useful for EC2 clusters\ncompute and memory resources being used\n\nshows time series of min, max, and average percent usage\n\n\nClusters ‚Äì fargate cluster ‚Äì services tab\n\nclick service name ‚Äì metrics tab\n\nsame stuff as EC2 metrics tab\ncan click on the different metrics (cpu, memory utilization) and create custom metric functions, change period length, etc.\n\nleft panel has alarms (autoscaling trigger history), events, logs, and settings",
    "crumbs": [
      "Docker",
      "AWS"
    ]
  },
  {
    "objectID": "qmd/geospatial-processing.html",
    "href": "qmd/geospatial-processing.html",
    "title": "Preprocessing",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Geospatial",
      "Preprocessing"
    ]
  },
  {
    "objectID": "qmd/geospatial-processing.html#sec-geo-proc-misc",
    "href": "qmd/geospatial-processing.html#sec-geo-proc-misc",
    "title": "Preprocessing",
    "section": "",
    "text": "Beware statistical computations of tibbles/sf_tibbles with geometry columns\n\nCould result in an expensive union operation over identical geometries and an R session crash\n\nExample with 100K rows crashed R.\n\nNotes from thread\nOption 1 (slower): Set do_union = FALSE in summarize\ntx_income_groups &lt;- \n  get_acs(\n    geography = \"tract\",\n    table = \"B19001\",\n    state = \"TX\",\n    year = 2020,\n    geometry = TRUE\n  ) |&gt; \n  filter(variable != \"B19001_001\") |&gt; \n  mutate(bracket = case_when(\n    variable &gt; \"B19001_012\" ~ \"Above $100k\",\n    TRUE ~ \"Below $100k\"\n  )) |&gt; \n  group_by(GEOID, bracket) |&gt; \n  summarize(n_households = sum(estimate, na.rm = TRUE),\n            do_union = FALSE)\nOption 2 (faster): Perform calculation without geometries then join\ntx_tracts &lt;- tracts(\"TX\", cb = TRUE, year = 2020) |&gt; \n  select(GEOID)\n\ntx_income_groups &lt;- \n  get_acs(\n    geography = \"tract\",\n    table = \"B19001\",\n    state = \"TX\",\n    year = 2020,\n    geometry = TRUE\n  ) |&gt; \n  filter(variable != \"B19001_001\") |&gt; \n  mutate(bracket = case_when(\n    variable &gt; \"B19001_012\" ~ \"Above $100k\",\n    TRUE ~ \"Below $100k\"\n  )) |&gt; \n  group_by(GEOID, bracket) |&gt; \n  summarize(n_households = sum(estimate, na.rm = TRUE))\n\ntx_income_groups &lt;- tx_tracts |&gt; \n  left_join(tx_income_groups, by = \"GEOID\")\n\n{tidycensus} has an arg to bypass d/ling the geometries, geometry = FALSE and a separate tracts function to get the census tract geometries",
    "crumbs": [
      "Geospatial",
      "Preprocessing"
    ]
  },
  {
    "objectID": "qmd/geospatial-processing.html#sec-geo-proc-filtyp",
    "href": "qmd/geospatial-processing.html#sec-geo-proc-filtyp",
    "title": "Preprocessing",
    "section": "File Types",
    "text": "File Types\n\nPMTiles - A single-file archive format for tiled data. A PMTiles archive can be hosted on a commodity storage platform such as S3, and enables low-cost, zero-maintenance map applications that are ‚Äúserverless‚Äù - free of a custom tile backend or third party provider. (Docs)\n\nRun your interactive, smooth-zooming vector map from any storage like S3 that supports http requests; a Caddy server running on your Wi-Fi router, or even GitHub pages (if tiles &lt; 1GB).\nCloudflare R2 is the recommended storage platform for PMTiles because it does not have bandwidth fees, only per-request fees: see R2 Pricing.\n\nShape Files\n\nD/L and Load a shapefile\nMay need API key from Census Bureau (see {tigris} docs)\nExample: Counties in California\ntbl &lt;- tigris::counties(state = \"CA\") %&gt;%\n¬† ¬† st_set_crs(4326)\n{tigris} - US data\nlibrary(tigris)\n\nus_states &lt;- states(resolution = \"20m\", year = 2022, cb = TRUE)\n\nlower_48 &lt;- us_states %&gt;%\n¬† filter(!(NAME %in% c(\"Alaska\", \"Hawaii\", \"Puerto Rico\")))\n{rnaturalearth} - World data\n# Via URL\n# Medium scale data, 1:50m Admin 0 - Countries\n# Download from https://www.naturalearthdata.com/downloads/50m-cultural-vectors/\nworld_map &lt;- read_sf(\"ne_50m_admin_0_countries/ne_50m_admin_0_countries.shp\") %&gt;%\n¬† filter(iso_a3 != \"ATA\")¬† # Remove Antarctica\n\n# Via Package\nlibrary(rnaturalearth)\n\n# rerturnclass = \"sf\" makes it so the resulting dataframe has the special\n# sf-enabled geometry column\nworld_map &lt;- ne_countries(scale = 50, returnclass = \"sf\") %&gt;%\n¬† filter(iso_a3 != \"ATA\")¬† # Remove Antarctica\n\nGeoJSON\n\nWrite data to geojson\ndata %&gt;%\n¬† ¬† st_write(\"mb_shapes.geojson\")",
    "crumbs": [
      "Geospatial",
      "Preprocessing"
    ]
  },
  {
    "objectID": "qmd/geospatial-processing.html#sec-geo-proc-proj",
    "href": "qmd/geospatial-processing.html#sec-geo-proc-proj",
    "title": "Preprocessing",
    "section": "Projections",
    "text": "Projections\n\nWGS 84\n\nGoogle ‚Äúepsg code‚Äù + ‚Äúyour region name‚Äù to find a reasonable projection code to use\n\nStandard projection is 4326 aka WGS84 (required by leaflet)\nTransform shapefile\nmb_shapes &lt;- read_sf(download_folder)\nmb_shapes %&gt;%\n¬† st_transform(4326)\n\n\nTransform latitude and longitude then visualize\nnew_tbl &lt;- old_tbl # contains latitude and longitude variables\n¬† ¬† # convert to simple features object\n¬† ¬† sf::st_as_sf(\n¬† ¬† ¬† ¬† coords = c(\"&lt;longitude_var&gt;\", \"&lt;latitude_var&gt;\"), # order matters\n¬† ¬† ¬† ¬† crs = 4326 # standard crs\n¬† ¬† ) %&gt;%\n¬† ¬† mapviw::mapview()\nWGS 84 projection, which is what Google Maps (and all GPS systems) use\nus_states &lt;- us_states %&gt;% # df with geometries\n¬† sf::st_transform(st_crs(\"EPSG:4326\"))¬† # WGS 84\nNAD83, Albers, Mercator, Robinson\n\nlibrary(patchwork)\n\np1 &lt;- ggplot() +\n¬† geom_sf(data = lower_48, fill = \"#0074D9\", color = \"white\", linewidth = 0.25) +\n¬† coord_sf(crs = st_crs(\"EPSG:4269\")) +¬† # NAD83\n¬† labs(title = \"NAD83 projection\") +\n¬† theme_void() +\n¬† theme(plot.title = element_text(hjust = 0.5, family = \"Overpass Light\"))\n\np2 &lt;- ggplot() +\n¬† geom_sf(data = lower_48, fill = \"#0074D9\", color = \"white\", linewidth = 0.25) +\n¬† coord_sf(crs = st_crs(\"ESRI:102003\")) +¬† # Albers\n¬† labs(title = \"Albers projection\") +\n¬† theme_void() +\n¬† theme(plot.title = element_text(hjust = 0.5, family = \"Overpass Light\"))\n\np3 &lt;- ggplot() +\n¬† geom_sf(data = world_map, fill = \"#FF4136\", color = \"white\", linewidth = 0.1) +\n¬† coord_sf(crs = st_crs(\"EPSG:3395\")) +¬† # Mercator\n¬† labs(title = \"Mercator projection\") +\n¬† theme_void() +\n¬† theme(plot.title = element_text(hjust = 0.5, family = \"Overpass Light\"))\n\np4 &lt;- ggplot() +\n¬† geom_sf(data = world_map, fill = \"#FF4136\", color = \"white\", linewidth = 0.1) +\n¬† coord_sf(crs = st_crs(\"ESRI:54030\")) +¬† # Robinson\n¬† labs(title = \"Robinson projection\") +\n¬† theme_void() +\n¬† theme(plot.title = element_text(hjust = 0.5, family = \"Overpass Light\"))\n\n(p1 | p2) / (p3 | p4)",
    "crumbs": [
      "Geospatial",
      "Preprocessing"
    ]
  },
  {
    "objectID": "qmd/geospatial-processing.html#sec-geo-proc-py",
    "href": "qmd/geospatial-processing.html#sec-geo-proc-py",
    "title": "Preprocessing",
    "section": "Python",
    "text": "Python\n\nExample: Filter Data based on a polygon using latitude and longitude data\n\nGet California‚Äôs polygon\nimport osmnx\nimport geopandas as gpd\n\nplace = \"California, USA\"\ngdf = osmnx.geocode_to_gdf(place)\n# Get the target geometry\ngdf = gdf[[\"geometry\", \"bbox_north\", \"bbox_south\", \"bbox_east\", \"bbox_west\"]]\nFilter data according the polygon geometry\nfrom shapely.geometry import Point\n\n# Convert to a GeoDataFrame with Point geometry\ngeometry = [Point(xy) for xy in zip(df['Longitude'], df['Latitude'])]\nearthquake_gdf = gpd.GeoDataFrame(df, geometry=geometry, crs='EPSG:4326')\n\n# Filter to keep only points within the California bounding box\npoints_within_california = gpd.sjoin(earthquake_gdf, gdf, how='inner', predicate='within')\n\n# Select latitude, longitude etc. columns\ndf = points_within_california[['id', 'Latitude', 'Longitude', 'datetime', 'properties.mag']]\n\nLatitude and longitude are converted to point geometry to match the polygon point geometry\nAn inner join is used on the data and california polygon to get the points that are only in California.",
    "crumbs": [
      "Geospatial",
      "Preprocessing"
    ]
  },
  {
    "objectID": "qmd/job-management-leadership.html",
    "href": "qmd/job-management-leadership.html",
    "title": "29¬† Management / Leadership",
    "section": "",
    "text": "29.1 Misc",
    "crumbs": [
      "Job",
      "<span class='chapter-number'>29</span>¬† <span class='chapter-title'>Management / Leadership</span>"
    ]
  },
  {
    "objectID": "qmd/job-management-leadership.html#misc",
    "href": "qmd/job-management-leadership.html#misc",
    "title": "29¬† Management / Leadership",
    "section": "",
    "text": "Try really hard not to send messages outside of work hours\nEmphasize unplugging during vacations\nProvide immediate feedback - positive and negative\nDedicate time to freeform exploration\n\nSometimes the rest of the business doesn‚Äôt know what to ask of your data org. That‚Äôs why you need to give your team time to explore.\nTeam members can come to new and exciting conclusions when they‚Äôre given time to explore the data for fun. They can apply their talents to looking for patterns that no one has requested, and have the space to uncover new discoveries. This freeform exploration can lead to game-changing innovations that no business stakeholder would have imagined were possible.\nHelps keep your most valuable team members engaged and satisfied in their work.\n\nWhen first starting, request documentation\n\nRelevant server locations & descriptions\nLocations of our documentation and dashboards\nA list of tools/software that are available to be used\nA list of relevant stakeholders/gatekeepers that I‚Äôd need to make contact with\n\nRemote Teams\n\nVideo calls too easily become transactional and with little time for the chitchat that builds a proper human relationship. Without those deeper bonds, misunderstandings fester into serious relationship difficulties, and teams can get tangled in situations that would be effectively resolved if everyone were able to talk in person.\nSome organizations may balk at the costs of travel and accommodation for a team assembly like this, but they should think of it as an investment in the team‚Äôs effectiveness. Neglecting these face-to-faces leads to teams getting stuck, heading off in the wrong direction, plagued with conflict, and people losing motivation. Compared to this, saving on airplanes and hotels is a false economy.\nFrequency\n\nGet together for a week every two or three months\nAfter the team has become seasoned they may then decide to reduce the frequency, but I would worry if a team isn‚Äôt having at least two face-to-face meetings a year.\nIf a team is all in the same city, but using a remote-first style to reduce commuting, then they can organize shorter gatherings, and do them more frequently.\n\nSchedule\n\nSet a full day of work, focusing on those tasks that benefit from the low-latency communication that comes from being together. tasks that require lots of input from many people with rapid feedback\nWe should then include what feels like too much time for breaks, informal chatter, and opportunities to step outside the office.",
    "crumbs": [
      "Job",
      "<span class='chapter-number'>29</span>¬† <span class='chapter-title'>Management / Leadership</span>"
    ]
  },
  {
    "objectID": "qmd/job-management-leadership.html#terms",
    "href": "qmd/job-management-leadership.html#terms",
    "title": "29¬† Management / Leadership",
    "section": "29.2 Terms",
    "text": "29.2 Terms\n\nFocus Time - uninterrupted time, usually refers to a period of time (e.g.¬†2 hrs) where people can work without any distractions\nReport (aka Individual Contributor (IC))- People who report to the manager",
    "crumbs": [
      "Job",
      "<span class='chapter-number'>29</span>¬† <span class='chapter-title'>Management / Leadership</span>"
    ]
  },
  {
    "objectID": "qmd/job-management-leadership.html#getting-the-promotion",
    "href": "qmd/job-management-leadership.html#getting-the-promotion",
    "title": "29¬† Management / Leadership",
    "section": "29.3 Getting the Promotion",
    "text": "29.3 Getting the Promotion\n\nYou don‚Äôt get a promotion and THEN start to perform at the next level; you perform at the next level IN ORDER TO get a promotion.\n\nSo when you notice a gap somewhere, even if it doesn‚Äôt necessarily fall into your current role description, don‚Äôt be afraid to bring it up to your manager and discuss whether you can/should take initiative to help plug the gap.\nThe best way to notice gaps is to be a good listener and constantly communicate with your partners & stakeholders about their teams‚Äô work and pain points.\n\nMentor a peer\n\nIf you have new members joining the team, offer to be an onboarding buddy to guide them through their first few weeks.\nbrainstorm with team members when then need help\n\nStep out of your immediate scope\nGet involved in team-level activities\n\nHelp out with things such as sprint planning, quarterly planning, etc.\n\nAllows you to gain knowledge about other team members‚Äô work and other teams‚Äô requests for your team\nGives you some exposure to the manager‚Äôs plan and vision for the team\n\nVolunteering for culture initiatives is a great way to practice thinking about the team as a whole\nTake on projects that help the whole team\n\nproduct design doc for the data product\nSLA agreement with partner teams (?)\nCodify the best practices you use in your own work\n\n\nHave open, timely feedback conversations with your manager\n\nAsk for the leveling guide when you have the initial career development conversation with your manager.\n\nAnd make sure you mention your aspiration to be a manager as soon as possible (don‚Äôt be shy) as well as your aspired timeline that you are working towards.\n\nAsk your manager for candid feedback with regards to their assessment of your readiness to become a manager, and any gaps that they think you need to address.\nIn followup career development check-ins, ask your manager to provide feedback for you against the leveling guide.",
    "crumbs": [
      "Job",
      "<span class='chapter-number'>29</span>¬† <span class='chapter-title'>Management / Leadership</span>"
    ]
  },
  {
    "objectID": "qmd/job-management-leadership.html#managerial-hats",
    "href": "qmd/job-management-leadership.html#managerial-hats",
    "title": "29¬† Management / Leadership",
    "section": "29.4 Managerial Hats",
    "text": "29.4 Managerial Hats\n\nPeople manager - Learn what makes your direct reports (aka people that you manage) tick, identify their career aspirations, and point out opportunities for progress.\nResource manager - Determine what resources are needed and acquire them. Mostly this means recruiting, hiring, and onboarding, but it also means advocating for money for training and team activities.\nProject manager - Collect and triage projects and project requirements, set timetables and schedules, assigned tasks, and have the final say about when work was ‚Äúdone‚Äù.\nCommunications manager - Make sure the team‚Äôs work was being shared with the rest of the organization, and that everyone on the team knew what was going on outside.\nProcess manager - Help design the team‚Äôs processes to make sure we could identify, allot, do, and communicate work across the team.\nTechnical mentor and coach - A technical expert who reviews code, answers technical questions, and gives work feedback to my team.",
    "crumbs": [
      "Job",
      "<span class='chapter-number'>29</span>¬† <span class='chapter-title'>Management / Leadership</span>"
    ]
  },
  {
    "objectID": "qmd/job-management-leadership.html#preparation-for-a-managerial-role",
    "href": "qmd/job-management-leadership.html#preparation-for-a-managerial-role",
    "title": "29¬† Management / Leadership",
    "section": "29.5 Preparation for a Managerial Role",
    "text": "29.5 Preparation for a Managerial Role\n\nTake notes on the time needed to do difficult tasks, easy quick-wins, common roadblocks, and their solutions.\n\nThis will help estimate deadlines for new projects\n\nPractice verbal and written communication\nGather information on ‚Äúbig picture‚Äù strategy of your company and that applies to data projects\nTake notes of every data team member‚Äôs strengths and weaknesses\nListen to your colleagues.\n\nComplaints on a day-to-day basis.\nPraise about the workplace in general.\nPay attention to pet projects of your teammates: these are the areas they actively pursue outside their usual work.",
    "crumbs": [
      "Job",
      "<span class='chapter-number'>29</span>¬† <span class='chapter-title'>Management / Leadership</span>"
    ]
  },
  {
    "objectID": "qmd/job-management-leadership.html#meetings",
    "href": "qmd/job-management-leadership.html#meetings",
    "title": "29¬† Management / Leadership",
    "section": "29.6 Meetings",
    "text": "29.6 Meetings\n\nMeetings are bad when they:\n\nResult in calendar fragmentation.\n\nTry to schedule sometime after a Focus Time\nLimit size and number of meetings\n\n1-1s (1 on 1), team-wide update, or decision-making meetings\nLarge (&gt; 4 ppl) brainstorming meetings don‚Äôt work\n\nBetter to circulate a memo of come-up with options then debate those options during a meeting\n\n\n\nFeel useless to attendees\n\nKeep focus on the meetings agenda\n\nGroup meetings (manager‚Äôs agenda)\n1-1s (1 on 1) (report‚Äôs agenda)",
    "crumbs": [
      "Job",
      "<span class='chapter-number'>29</span>¬† <span class='chapter-title'>Management / Leadership</span>"
    ]
  },
  {
    "objectID": "qmd/job-management-leadership.html#tools-for-servant-leadership",
    "href": "qmd/job-management-leadership.html#tools-for-servant-leadership",
    "title": "29¬† Management / Leadership",
    "section": "29.7 Tools for Servant Leadership",
    "text": "29.7 Tools for Servant Leadership\n\nTeaching - As a leader you often have more context and more experience than your team members.\n\nTeach the team which situations different models work in, how those models are perceived in your organization and the red-flags to watch out for during development.\n\nReflecting - Make time to think back to events within your team.\n\nWhat caused success? What led to failure? Are we setting expectations appropriately for our models and analyses?\n\nDebate - Encourage debate.\n\nThe team is trying to use data to understand the world, and as in any form of science, there will be competing hypotheses\nTake advantage of the diversity (all forms) within our teams to minimize the impact of those personal biases\n\nProcess - Leaders will have to deal with ambiguity, but for the wider team we need to ensure there are steps to follow that support consistency across the team and alignment on the team‚Äôs over-arching goals.\nFeedback - To maintain team members‚Äô morale, the balance between negative and positive feedback has to tilt heavily towards the positive\n\nIf you can‚Äôt find that balance, then you need to consider whether the team member should continue on your team. If you want them to remain, then you must figure out how to articulate their positives back to them, otherwise you can expect them to leave.",
    "crumbs": [
      "Job",
      "<span class='chapter-number'>29</span>¬† <span class='chapter-title'>Management / Leadership</span>"
    ]
  },
  {
    "objectID": "qmd/job-management-leadership.html#nonviolent-communication-nvc",
    "href": "qmd/job-management-leadership.html#nonviolent-communication-nvc",
    "title": "29¬† Management / Leadership",
    "section": "29.8 Nonviolent Communication (NVC)",
    "text": "29.8 Nonviolent Communication (NVC)\n\nMisc\n\nNotes from How to deliver constructive feedback in difficult situations\nOther methods\n\nSBI (Situation-Behavior-Impact) Useful for giving better feedback by removing emotions from it and making it clear and concise\n\nSteps\n\nSituation - Start the feedback with a specific situation that occurred which serves as a common reference point and is specific.\nBehavior - Refer to a specific behavior that you observed and want to talk about. Make sure to not give any judgments and leave the interpretation out of it.\nImpact - Talk about the impact that behavior had and what you think and feel about it. Feel free to address what other people think and how it impacted things.\nIntent - Ask about the person‚Äôs intention and try to uncover if the person is aware of what he did and why he did it. Then, work together with the person to see how things can be made better and how to overcome issues.\n\n\n\n\nAt the core of NVC is a straightforward communication pattern:\n\n‚ÄúWhen ____[observation], I feel ____[emotion] because I‚Äôm needing some ____[universal needs]. Would you be able to ____[request]?‚Äù\nExamples\n\nTo a co-founder: ‚ÄúWhen you said, ‚ÄòI‚Äôm not happy with your work,‚Äô to me in front of the team, I felt embarrassed because it didn‚Äôt meet my need for trust and recognition. Please, could we set up a weekly one-on-one session to share feedback in private?‚Äù\nTo an investor: ‚ÄúI haven‚Äôt received any responses from the last three monthly updates. I‚Äôm feeling concerned because I need input. Please, would you mind getting back to me with responses to my questions in the last update?‚Äù\nTo a teammate: ‚ÄúYou arrived 10 minutes late to the last three team meetings. I am frustrated because, as a team, we have a need for efficiency. Please, could you help me understand what‚Äôs happening?‚Äù\n\n\nObservations (vs evaluations)\n\nExamples\n\nEvaluation: ‚ÄúYou are lazy‚Äù (which is a character attack). Observation: ‚ÄúYou said that you‚Äôd send the document last week, and I haven‚Äôt received it.‚Äù\nEvaluation: ‚ÄúYour work is sloppy‚Äù (which is a criticism). Observation: ‚ÄúThree of the numbers in the report were inaccurate.‚Äù\nEvaluation: ‚ÄúYou‚Äôre always late,‚Äù (which is a generalization). Observation: ‚ÄúYou arrived 10 minutes late to the meeting this morning.‚Äù\nEvaluation: ‚ÄúYou ignored me.‚Äù (which implies intent). Observation: ‚ÄúI sent you two emails, and I haven‚Äôt received a response.‚Äù\n\nCheck\n\nask yourself, ‚ÄúWhat did I actually see or hear?‚Äù\n\n\nEmotions (vs thoughts, vs evaluations)\n\nUsing an evaluation or thought instead of an emotion, can result in a defensive reply\nExamples\n\nEmotion: ‚ÄúI feel frustrated.‚Äù Thought: ‚ÄúI feel that you aren‚Äôt taking this seriously.‚Äù\nEvaluation: ‚ÄúI feel judged.‚Äù Impact: ‚ÄúI feel resentful.‚Äù\n\ndefensive reply: ‚ÄúI didn‚Äôt judge you.‚Äù\n\nEvaluation: ‚ÄúI feel misunderstood.‚Äù Impact: ‚ÄúI feel frustrated.‚Äù\nEvaluation: ‚ÄúI feel rejected.‚Äù Impact: ‚ÄúI feel hurt.‚Äù\n\nCheck\n\nFor thoughs, if you can substitute ‚ÄúI feel‚Äù with ‚ÄúI think‚Äù and the phrase still works ‚Äî because it‚Äôs a thought, not an emotion.\n\n\nUniversal Need (vs strategy for obtaining a need)\n\nExamples\n\nStrategy: ‚ÄúI need you to copy me into every email.‚Äù Universal Need: ‚ÄúI need some transparency.‚Äù\nUniversal: ‚Äú‚ÄúI need support.‚Äù NOT Universal: ‚ÄúI need support from you.‚Äù\n\nNOT Universal is more easily interpreted as a veiled accusation and implication that ‚ÄúYou aren‚Äôt supporting me.‚Äù\n\n\n\nRequests (vs demands)\n\nrequests are invitations for another person to meet our needs ‚Äî but only if it doesn‚Äôt conflict with one of their needs.\nCharacteristics of a good request\n\nMake them specific\n\n‚ÄúI request that you arrive to meetings on time.‚Äù instead of ‚ÄúI request that you be more respectful of everyone‚Äôs time.‚Äù\n\nSay what you want, not what you don‚Äôt want\n\nDon‚Äôt want: ‚ÄúI request that you don‚Äôt dismiss other people‚Äôs ideas straightaway‚Äù\nWant: ‚ÄúI request that when a team member shares an idea, you ask two or three probing questions before sharing your conclusion.‚Äù\n\nStay curious\n\nBe optimistic that everyone‚Äôs needs can be met.\nTreat ‚Äúno‚Äù to a request or a defensive reply as an invitation to explore the needs stopping someone from saying ‚Äúyes.‚Äù\nThink about how the other person is feeling and consider what unmet needs may be stopping them from saying ‚Äúyes.‚Äù\n\nAre you feeling hurt because you need some understanding?\nAre you feeling angry because you need your hard work to be recognized?\nIs there more you‚Äôd like to say?\n\nSimilarly, if you‚Äôre on the receiving end of a request and have to say ‚Äúno,‚Äù state the underlying need that stops you from saying ‚Äúyes.‚Äù\n\n\n\nDiplomatically confirm communication if needed\n\n‚ÄúJust so we know we‚Äôre on the same page, could you play back what I‚Äôm asking of you?‚Äù\n\n40-word rule\n\nDuring difficult conversations, it‚Äôs important to be extremely concise. Aim to describe your observations, feelings, needs, and requests in fewer than 40 words. Using more words suggests you‚Äôre justifying your needs, and that decreases their power.\n\nFace-to-Face is better\n\nNVC loses some of its power when it‚Äôs in an email.\n\nConsequences should be protective, not punitive\n\nAs a manager, you are responsible for the effectiveness of your team ‚Äî and every team needs effectiveness. If deadlines continue to be missed (the boundary), you might have to switch their responsibilities or move them on (the consequence). It‚Äôs not personal, it‚Äôs just what you‚Äôll do to protect your need for effectiveness.",
    "crumbs": [
      "Job",
      "<span class='chapter-number'>29</span>¬† <span class='chapter-title'>Management / Leadership</span>"
    ]
  },
  {
    "objectID": "qmd/job-management-leadership.html#promoting-your-teams-projects",
    "href": "qmd/job-management-leadership.html#promoting-your-teams-projects",
    "title": "29¬† Management / Leadership",
    "section": "29.9 Promoting Your Team‚Äôs Projects",
    "text": "29.9 Promoting Your Team‚Äôs Projects\n\nAnnouncement emails\n\nUnlike ‚Äúsharing‚Äù emails (brief description, link), announcement emails have more pomp associated with them\nCharacteristics\n\nUse catchy subject lines ‚Äî e.g.¬†‚ÄòRetention Dashboard is here!‚Äô or ‚ÄòIntroducing Retention Dashboard‚Äô\nIn addition to stating what the dashboard contains, tie it to key insights, recommendations and next steps\nUse icons & visuals ‚Äî Adding relevant icons and visuals makes the email easier to consume and provides a nice break from all the heavy text. Caution: Do not overuse!\n\nExample\n\n\nReadouts\n\nAn analysis that is packaged in a way that is easy to read through\nTypically a one-time analysis\n\n(deep dive) e.g.¬†what drives customer retention\n(root cause analysis) e.g.¬†why did top of funnel conversion decline or analyzing an experiment / launch / campaign performance\n\nAlso reoccurring\n\nCould be weekly or monthy, depending on topics important to your stakeholders\nActively sharing summarized findings from dashboards to the stakeholders can change perception that these dashboards are just another source of data\n\nMonthly or Quarterly Business Reviews\n\nPresentations where you review health of business based on trends in key metrics (month over month, quarter over quarter)\n\nAutomate frequent requests from Marketing managers, Product managers, and Operations managers\n\nProduce a readout that covers insights from multiple dashboards that managers are frequently asking about.\n\n\nNewletter\n\nHighlight goals for ongoing work-streams and outcomes for those completed, always connecting to business outcomes or stakeholder needs\nSample Layout\n\nSummary ‚Äî Key Wins & What‚Äôs Coming\nDetailed updates by themes\nNewsletter FAQ‚Äôs ‚Äî\n\nGoals: What is the goal of this newsletter. e.g.¬†Providing visibility and aligning on prioritization\nCadence: Weekly / Bi-weekly / Monthly\nAudience: Sr.¬†Leadership of company\nTeam members\nPOC: Who should they reach out to if they have questions\n\n\nExample",
    "crumbs": [
      "Job",
      "<span class='chapter-number'>29</span>¬† <span class='chapter-title'>Management / Leadership</span>"
    ]
  },
  {
    "objectID": "qmd/production-tools.html",
    "href": "qmd/production-tools.html",
    "title": "34¬† Tools",
    "section": "",
    "text": "34.1 Misc",
    "crumbs": [
      "Production",
      "<span class='chapter-number'>34</span>¬† <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "qmd/production-tools.html#sec-prod-tools-misc",
    "href": "qmd/production-tools.html#sec-prod-tools-misc",
    "title": "34¬† Tools",
    "section": "",
    "text": "The 2024 MAD (Machine Learning, AI & Data) Landscape\n\nOverview article with links to previous versions\nInteractive Version (link) with links to company pages and cards with descriptions of companies.\n\npricelevel - What companies actually pay for software. PriceLevel gives you visibility into the price hidden behind ‚ÄúContact Us‚Äù.\nOverview of some 2021 tools Descriptions in article \nAWS Batch - Managed service for computational jobs. Alternative to having to maintain a kubernetes cluster\n\nTakes care of keeping a queue of jobs, spinning up EC2 instances, running code and shutting down the instances.\nScales up and down depending on how many jobs submitted.\nAllows you to execute your code in a scalable fashion and to request custom resources for compute-intensive jobs (e.g., instances with many CPUs and large memory) without requiring us to maintain a cluster\nSee bkmks: Hosting &gt;&gt; AWS &gt;&gt; Batch\nPackages:\n\n{crew.aws.batch}",
    "crumbs": [
      "Production",
      "<span class='chapter-number'>34</span>¬† <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "qmd/production-tools.html#sec-prod-tools-stckcomprnk",
    "href": "qmd/production-tools.html#sec-prod-tools-stckcomprnk",
    "title": "34¬† Tools",
    "section": "34.2 Stack Component Rankings",
    "text": "34.2 Stack Component Rankings\n\nDB format\n\narrow files\n\nELT Operations\n\n*dbt\n\nGoogle‚Äôs alternative is Dataform\nAWS‚Äôs alternative is Databrew\n\n*Spark\n*Google Big Query SQL\n*AWS Athena\n\nOrchestration and monitoring\n\n*Targets\n\n+ {cronR} for orchestration + scheduling\n\n*Mage-AI\n*AWS Glue\nPrefect\nAirflow\n\nData Ingestion\n\nAirbyte (data ingestion)\nfivetran (data ingestion)\n\nCan ‚Äúprocess atomic REST APIs to extract data out of SAAS silos and onto your warehouse‚Äù\n\nterraform (multi-cloud management)\n\nTracking/Versioning for Model Building\n\n*DVC\nMLFlow\n\nReporting\n\nblastula (email), xaringan (presentation), RMarkdown (reports), flexdashboard (dashboards),\nRStudio Connect (publishing platform to stakeholders)\n\ndashboards, apps\non-demand and scheduled reports\npresentations\nAPIs (?)\nPublish R and Python\nEnterprise security\nCan stay in RStudio\n\n\nVisualization Platforms\n\nLooker*\nPowerBI, DataStudio",
    "crumbs": [
      "Production",
      "<span class='chapter-number'>34</span>¬† <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "qmd/production-tools.html#sec-prod-tools-depman",
    "href": "qmd/production-tools.html#sec-prod-tools-depman",
    "title": "34¬† Tools",
    "section": "34.3 Dependency Management",
    "text": "34.3 Dependency Management\n\nR\n\nr2u for linux installations\n\n‚Äúfor Ubuntu 20.04 and 22.04 it provides _all_ of CRAN (and portion of BioConductor) as binary #Rstats packages with full, complete and automatic resolution of all dependencies for full system integration. If you use `bspm` along with it you can even use this via `install.packages()` and friends. Everything comes from a well connected mirror‚Äù",
    "crumbs": [
      "Production",
      "<span class='chapter-number'>34</span>¬† <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "qmd/production-tools.html#data-versioning",
    "href": "qmd/production-tools.html#data-versioning",
    "title": "34¬† Tools",
    "section": "34.4 Data versioning",
    "text": "34.4 Data versioning\n\nFlat Table by Github\n\nHas a Github action associated with it\nHas a datetime commit message\nLists as a feature that it tracks differences from one commit to the next, but doesn‚Äôt a normal data commit doe the same thing?\n\nLumberjack R package\n\nAdd functions to your processing script\ntracks using a log file\noptions for changes you want to track",
    "crumbs": [
      "Production",
      "<span class='chapter-number'>34</span>¬† <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "qmd/production-tools.html#sec-prod-tools-dating",
    "href": "qmd/production-tools.html#sec-prod-tools-dating",
    "title": "34¬† Tools",
    "section": "34.5 Data Ingestion",
    "text": "34.5 Data Ingestion\n\nFiveTran\n\nFree-tier\nSync raw data sources\n\nevery 1hr for starter plan, every 15 minutes both standard plans, every 5 min for enterprise plan",
    "crumbs": [
      "Production",
      "<span class='chapter-number'>34</span>¬† <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "qmd/production-tools.html#sec-prod-tools-orch",
    "href": "qmd/production-tools.html#sec-prod-tools-orch",
    "title": "34¬† Tools",
    "section": "34.6 Orchestration",
    "text": "34.6 Orchestration\n\n34.6.1 Airflow\n\nWebpage\nOpen-source platform for authoring, scheduling, and executing data pipelines.\n\nFeatures for managing and monitoring data pipelines, including integration with various data storage and processing technologies. Similar to the Unix cron utility ‚Äî you write scripts and schedule them to run every X minutes.\nAirflow can be used for any sort of scheduling task, but is often used for scheduling data modeling. schedule, run and monitor the refresh of our data warehouse\nMonitoring on-prem checking Airflow logs is not user-friendly (better in AWS MWAA)\n\ndifferent types of logs for task, web server, scheduler, worker, and DAGs\nhave to SSH into the server and run commands which becomes more complicated when you want to use distributed servers for scalability.\n\nRequires you to create a central logging storage and make additional setup to make all servers write logs into that single place\n\n\nServer-based remains active even when not running jobs ‚Äì&gt; continually incurring cost\n\nNo latency since servers are always running\n\nProblems\nLong feedback loop\n\nWhile programming, instant feedback of your DAG becomes crucial when you want a sanity check before your code goes too far.\nTo see the graph view, which is mainly for visualizing dependencies in DAGs, your code needs to be in the folder of an Airflow scheduler that can be picked up. The airflow scheduler also takes time to render and parse your DAG until it shows up.\nMakes debugging difficult during the development cycle, so some engineers write more lines of code and test them all together. If the lines of code become unmanageable on one screen, you might vaguely remember what to validate and what dependencies to check.\n\nDifficult with local development\n\na docker image can be used to inject as much production-related information as possible. But it‚Äôs still not 100% copy, and it takes tremendous effort to develop and maintain that docker image.\nEven if you set up dev, staging, and production environments for running Airflow, they aren‚Äôt totally isolated and developers can end-up interfering with one another. Services/Extensions Astronomer offers a managed Airflow service.\nAmazon Managed Workflows for Apache Airflow (MWAA) - managed Airflow service\n\nOrchestrate jobs in EMR, Athena, S3, or Redshift\n\nGlue\n\nAirflow has the glue operator\n\nCloudFormation can be used to configure and manage\n\nallows for autoscaling which saves on costs by scaling down when usage is low\nstill needs a server running even when not running jobs\nmonitoring much easier since all the logs are written into CloudWatch search certain logs using Logs Insights\n\nhave a dashboard that displays usage of server resources like CPU, memory, and network traffic.\nmonitor numerous other Airflow-specific metrics.\nset up alerts and manage notification recipients programmatically.\n\n\nCost factors\n\nInstance size\nAdditional worker instance\nAdditional scheduler instance\nMeta database storage\n\nPotential Issues:\n\nResources are shared on multiple jobs so performance can suffer if:\n\nDon‚Äôt distribute trigger times evenly\nMisconfigure your maximum worker count\n\n\nOperate through AWS SDK\n\nCan\n\ncreate, update, and delete MWAA environments and retrieve their environment information that includes logging policies, number of workers, schedulers\nrun Airflow‚Äôs internal commands to control DAGs\n\nCan‚Äôt\n\nSome of Airflow‚Äôs native commands like backfill (check this AWS document), dags list, dags list-runs, dags next-execution, and more\n\n\n\n\n\n\n\n34.6.2 AWS Glue\n\nCloud-based data integration service that makes it easy to move data between data stores.\n\nIncludes a data catalog for storing metadata about data sources and targets, as well as a ETL (extract, transform, and load) engine for transforming and moving data.\nIntegrates with other AWS services, such as S3 and Redshift, making it a convenient choice for users of the AWS ecosystem. Serverless (i.e.¬†costs only incurred when triggered by event) Each job triggers separate resources, so if one job overloads resources, it doesn‚Äôt affect other jobs\nJobs experience latency since instances have to spin-up and install packages\nCost Charged by Data Processing Unit (DPU) multiplied by usage hours (Pricing)\n\nJob types:\n\nPython shell: you can choose either 0.0625 or 1 DPU.\nApache Spark: you can use 2 to 100 DPUs.\nSpark Streaming: you can use 2 DPUs to 100 DPUs.\n\n\n\nCan run Spark\nExpensive for longer running ETL tasks. So, setting up your own container and deploying it on ECS with Fargate makes sense, both in terms of efficiency and cost.\nMonitoring\n\nCloudwatch\nGlueStudio within Glue Clicking number sends you to Cloudwatch where you can drill down into jobs\nCloudFormation can be used to configure and manage\nGlue SDK available\n\n\n\n\n34.6.3 Prefect\n\nEasier to manage for smaller data engineer teams or a single data engineer\nmore user friendly than Airflow; Better UI; more easily discover location and time of errors\npurely python\nMisc\n\nadd slack webhook for notifications\nHas slack channel to get immediate help with issues or questions\n\nautomatic versioning for every flow, within every project\n\nalso document the models deployed with each version in the README they provide with every flow\n\nComponents\n\nTasks - individual jobs that do one unit of work\n\ne.g.¬†a step that syncs Fivetran data or runs a dbt model\n\nFlows - functions that consist of a bunch of smaller tasks, or units of work, that depend on one another\n\ne.g.¬†1 flow could be multiple tasks running Fivetran syncs and dbt models\n\nExample:\nfrom prefect import flow, task\n@flow(name=\"Create a Report for Google Trends\")\ndef create_pytrends_report(\n¬† ¬† keyword: str = \"COVID\", start_date: str = \"2020-01-01\", num_countries: int = 10\n):\n\nThese flows are then scheduled and run by whatever types of agents you choose to set up.\n\nSome options include AWS ECS, GCP Vertex, Kubernetes, locally, etc.\n\nDeployments (docs)\n\nAlso see Create Robust Data Pipelines with Prefect, Docker, and GitHub\nDefintions\n\nSpecify the execution environment infrastructure for the flow run\nSpecify how your flow code is stored and retrieved by Prefect agents\nCreate flow runs with custom parameters from the UI\nCreate a schedule to run the flow\n\nSteps\n\nBuild the deployment definition file and optionally upload your flow to the specified remote storage location\nCreate the deployment by applying the deployment definition\n\nSyntax: prefect deployment build [OPTIONS] &lt;path-to-your-flow&gt;:&lt;flow-name&gt;\nExample:\nprefect deployment build src/main.py:create_pytrends_report \\\n¬† -n google-trends-gh-docker \\\n¬† -q test\n\nDeployment for the flow create_pytrends_report (see flow example) from the file, ‚Äúsrc/main.py‚Äù\n-n google-trends-gh-docker specifies the name of the deployment to be google-trends-gh-docker.\n-q test specifies the work queue to be test . A work queue organizes deployments into queues for execution.\nOutput\n\n‚Äúcreate_pytrends_report-deployment.yaml‚Äù file and a ‚Äú.prefectignore‚Äù created in the current directory.\n\n‚Äúcreate_pytrends_report-deployment.yaml‚Äù:¬† specifies where a flow‚Äôs code is stored and how a flow should be run.\n‚Äú.prefectignore‚Äù:¬† prevents certain files or directories from being uploaded to the configured storage location.\n\n\n\n\n\n34.6.4 Azure Data Factory\n\nAllows users to create, schedule, and orchestrate data pipelines for moving and transforming data from various sources to destinations.\nData Factory provides a visual designer for building pipelines, as well as a range of connectors for integrating with various data stores and processing technologies.\nExample: Demand Planning Project\n\n\n\n34.6.5 Mage-AI\n\nEnables users to define DAG regardless of the choice of languages (python/SQL/R)\nWeb-based IDE, so its mobility allows working from different devices, and sharing becomes more straightforward.\n\nUI layout feels like using RStudio. It has many sections divided into different areas.\nOne of the areas is the DAG visualization which provides instant feedback to the user on the task relationship.\n\nDAGs\n\nThe pipeline or DAG is constructed with modular blocks‚Äîa block maps to a single file.\nBlock Options\n\nExecution with upstream blocks: this triggers all upstream blocks to get the data ready for the current block to run\nExecute and run tests defined in the current block: this focuses on the current block to perform testing.\nSet block as dynamic: this changes the block type into the dynamic block, and it fits better to create multiple downstream blocks at runtime.\n\nManipulate dependencies via drag and drop\n\nmage-ai keeps track of the UI changes the user made and automatically builds the dependencies DAG into the YAML file. (./pipelines/{your_awesome_pipeline_name}/metadata.yaml)\n\nVisualize data in each block\n\nHelpful for inspecting your input data and further validating the transformation.\nOnce the chart has been created, it will also be attached to the current block as the downstream_blocks.\n\n\nR\n\nAllows users to write the main ETL (Extraction, Transformation, and Loading) blocks using R.\n\n\n\n\n34.6.6 kestra\n\nPopular orchestration libraries such as Airflow, Prefect, and Dagster require modifications to the Python code to use their functionalities. You may need to modify the data science code to add orchestration logic\nKestra, an open-source library, allows you to develop your Python scripts independently and then ‚Äã‚Äãseamlessly incorporate them into data workflows using YAML files.",
    "crumbs": [
      "Production",
      "<span class='chapter-number'>34</span>¬† <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "qmd/production-tools.html#sec-prod-tools-eltetl",
    "href": "qmd/production-tools.html#sec-prod-tools-eltetl",
    "title": "34¬† Tools",
    "section": "34.7 ELT/ETL Operations",
    "text": "34.7 ELT/ETL Operations\n\n34.7.1 Misc\n\ndbt - see DB, dbt\nGoogle Dataform - Docs, Best Practices\n\n\n\n34.7.2 AWS DataBrew\n\nFeatures to clean and transform the data to ready it for further processing or feeding to machine learning models\n\nNo coding; pay for what you use; scales automatically\nover 250 transformations\nAllows you to add custom transformations with lambda functions",
    "crumbs": [
      "Production",
      "<span class='chapter-number'>34</span>¬† <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "qmd/production-tools.html#sec-prod-tools-modexptrk",
    "href": "qmd/production-tools.html#sec-prod-tools-modexptrk",
    "title": "34¬† Tools",
    "section": "34.8 Model Experimentation/Version Tracking",
    "text": "34.8 Model Experimentation/Version Tracking\n\n34.8.1 DVC\n\nTracks data and models while model building\nStore code and track changes in a Git repository while data/models are in AWS/GCP/Azure/etc. storage\nTracking changes\n\nSteps\n\nhashes every file in the directory data,\nadds it to .gitignore and\ncreates a small file data.dvc that is added to Git.\n\nBy comparing hashes, DVC knows when files change and which version to restore.\n\nInitial Steps\n\nGoto project directory -cd &lt;path to local github repo&gt;\nInitialize DVC - dvc init\nAdd a data path/uri - dvc remote add -d remote path/to/remote\n\ncan be Google Drive, Amazon S3, Google Cloud Storage, Azure Storage, or on your local machine\ne.g.¬†Google Drive: dvc remote add -d remote gdrive://&lt;hash&gt;\n\nThe hash will the last part of the URL, e.g.¬†‚Äúhttps://drive.google.com/drive/u/0/folders/1v1cBGN9vS9NT6-t6QhJG‚Äù\n\nConfirm data set-up: dvc config -l\n\nThe config file is located inside ‚Äú.dvc/‚Äù\nTo version your config on github: git add .dvc/config\n\n\nAdd data/ to .gitignore\n\nExample showed adding every file in the repo manually but this seems easier\n\nAdd, commit, and push all files to repo\n\nMain differences to regular project initialization\n\ndata/ directory doesn‚Äôt get pushed to github\ndata.dvc file gets pushed to github\n\n\nSet-up DVC data cache\n\nCan be local directory/s3/gs/gdrive/etc\nExample: S3\n¬† ¬† ¬† ¬† ¬† ¬† dvc remote add -d myremote s3://mybucket/path\n¬† ¬† ¬† ¬† ¬† ¬† git add .dvc/config\n¬† ¬† ¬† ¬† ¬† ¬† git commit -m \"Configure remote storage\"\n¬† ¬† ¬† ¬† ¬† ¬† git push\n¬† ¬† ¬† ¬† ¬† ¬† dvc push\n\n\nI‚Äôm guessing .dvc/config is created with dvc remote add¬† and wasn‚Äôt there before. Otherwise in steps 3 and 4, I need to add the files manually.",
    "crumbs": [
      "Production",
      "<span class='chapter-number'>34</span>¬† <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "qmd/production-tools.html#sec-prod-tools-modmon",
    "href": "qmd/production-tools.html#sec-prod-tools-modmon",
    "title": "34¬† Tools",
    "section": "34.9 Model/Data Drift Monitoring",
    "text": "34.9 Model/Data Drift Monitoring\n\nArize AI\n\nDocs\nAccessed through Rest API, Python SDK, or Cloud Storage Bucket\n\nFiddler AI Monitoring: fiddler.ai has a suite of tools that help in making the AI explainable, aid in operating ML models in production, monitor ML models and yes data & model drift detection is one of them\nEvidently: EvidentlyAI is another open-source tool, which helps in evaluating and monitoring models in production. If you are not using Azure ML and looking for a non-commercial tool that is simple to use, evidentlyai is a good place to start.\nAzure ML\n\nMonitors data; uses wasserstein distance\n\nAWS Glue DataBrew\n\nmonitors features\ncalculates full suite of summary stats + entropy\n\nCan be exported to a bucket and then download to measure change over time\n\nAccessed through console or programmatically\nGenerates reports that can be viewed in console or be exported in html, pdf, etc.",
    "crumbs": [
      "Production",
      "<span class='chapter-number'>34</span>¬† <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "qmd/production-tools.html#sec-prod-tools-appclustmon",
    "href": "qmd/production-tools.html#sec-prod-tools-appclustmon",
    "title": "34¬† Tools",
    "section": "34.10 App/Cluster Monitoring",
    "text": "34.10 App/Cluster Monitoring\n\n34.10.1 Prometheus\n\nDon‚Äôt use for ML monitoring (from article)(maybe for apps?)\n\nNeed to use multiple Prometheus Metric types for cross-component monitoring\nNeed to define histogram buckets up front for single-component monitoring\nCorrectness of query results depending on scraping interval\nInability to handle sliding windows\nDisgusting-looking PromQL queries\nHigh latency for cross-component metrics (i.e., high-cardinality joins)\n\nMisc\n\nPrometheus is not a time series database (TSDB). It merely leverages a TSDB.\nBecause Prometheus scrapes values periodically, some Metric types (e.g., Gauges) can lose precision if the Metric value changes more frequently than the scraping interval. This problem does not apply to monotonically increasing metrics (e.g., Counters).\nMetrics can be logged with arbitrary identifiers such that at query time, users can filter Metrics by their identifier value.\nPromQL is flexible ‚Äì users can compute many different aggregations (basic arithmetic functions) of Metric values over different window sizes, and these parameters can be specified at query time.\n\nMetric values (Docs):\n\nCounter: a cumulative Metric that monotonically increases. Can be used to track the number of predictions served, for example.\nGauge: a Metric that represents a single numerical value that can arbitrarily change. Can be used to track current memory usage, for example.\nHistogram: a Metric that categorizes observed numerical values into user-predefined buckets. This has a high server-side cost because the server calculates quantiles at query time.\nSummary: a Metric that tracks a user-predefined quantile over a sliding time window. This has a lower server-side cost because quantiles are configured and tracked at logging time. Also, the Summary Metric doesn‚Äôt generally support aggregations in queries.\n\nProcess\n\n\nUsers instrument their application code to log Metric values.\nThose values are scraped and stored in a Prometheus server.\nThe values can be queried using PromQL and exported to a visualization tool like Grafana\n\nThe are R packages that might make querying these metrics easier so you don‚Äôt have to learn PromQL",
    "crumbs": [
      "Production",
      "<span class='chapter-number'>34</span>¬† <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "qmd/production-tools.html#sec-prod-tools-oth",
    "href": "qmd/production-tools.html#sec-prod-tools-oth",
    "title": "34¬† Tools",
    "section": "34.11 Other",
    "text": "34.11 Other\n\nTerraform\n\nProvision infrastructure across 300+ public clouds and services using a single workflow through yaml files\n\nAutomates and makes these workflows reproducible\nArticle on using it with R\n\n\nDatadog - Monitor servers in all cloud hosts in one place ‚Äî alerts, metrics, logs, traces, security incidents, etc.\nPagerDuty - Automated incident management ‚Äî alerts, notifications, diagnostics, logging, etc.",
    "crumbs": [
      "Production",
      "<span class='chapter-number'>34</span>¬† <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "qmd/quarto-rmarkdown.html",
    "href": "qmd/quarto-rmarkdown.html",
    "title": "Quarto",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Quarto"
    ]
  },
  {
    "objectID": "qmd/quarto-rmarkdown.html#sec-quarto-misc",
    "href": "qmd/quarto-rmarkdown.html#sec-quarto-misc",
    "title": "Quarto",
    "section": "",
    "text": "Packages\n\n{quarto}\n\nResources\n\nDocs\nReference\nTroubleshooting\n\nquarto --version - Must be in RStudio Terminal\nquarto check - Must be in RStudio Terminal - versions and engine checks\n$ quarto check\n[&gt;] Checking versions of quarto binary dependencies...\n¬† ¬† ¬† Pandoc version 3.1.1: OK\n¬† ¬† ¬† Dart Sass version 1.55.0: OK\n[&gt;] Checking versions of quarto dependencies......OK\n[&gt;] Checking Quarto installation......OK\n¬† ¬† ¬† Version: 1.3.340\n¬† ¬† ¬† Path: C:\\Users\\tbats\\AppData\\Local\\Programs\\Quarto\\bin\n¬† ¬† ¬† CodePage: 1252\n[&gt;] Checking basic markdown render....OK\n[&gt;] Checking Python 3 installation....OK\n¬† ¬† ¬† Version: 3.8.1 (Conda)\n¬† ¬† ¬† Path: C:/Users/tbats/Miniconda3/python.exe\n¬† ¬† ¬† Jupyter: 4.9.1\n¬† ¬† ¬† Kernels: python3\n(\\) Checking Jupyter engine render....2023-04-28 10:18:15,018 - traitlets - WARNING - Kernel\nProvisioning: The 'local-provisioner' is not found.¬† This is likely due to the presence of multiple jupyter_client distributions and a        previous distribution is being used as the source for entrypoints - which does not include 'local-provisioner'.¬† That distribution should     be removed such that only the version-appropriate distribution remains (version &gt;= 7).¬† Until then, a 'local-provisioner' entrypoint will     be automatically constructed and used.\nThe candidate distribution locations are: ['C:\\\\Users\\\\tbats\\\\Miniconda3\\\\lib\\\\site-packages\\\\jupyter_client-5.3.4.dist-info',                'C:\\\\Users\\\\tbats\\\\Miniconda3\\\\lib\\\\site-packages\\\\jupyter_client-7.0.6.dist-info']\n[&gt;] Checking Jupyter engine render....OK\n[&gt;] Checking R installation...........OK\n¬† ¬† ¬† Version: 4.2.3\n¬† ¬† ¬† Path: C:/PROGRA~1/R/R-42~1.3\n¬† ¬† ¬† LibPaths:\n¬† ¬† ¬† ¬† - C:/Users/tbats/AppData/Local/R/win-library/4.2\n¬† ¬† ¬† ¬† - C:/Program Files/R/R-4.2.3/library\n¬† ¬† ¬† knitr: 1.42\n¬† ¬† ¬† rmarkdown: 2.20\n[&gt;] Checking Knitr engine render......OK\nCLI\n\nquarto render to compile a document\nquarto preview to render a live preview that automatically updates when the source files are saved\n\nUsing a development verison of Quarto\n\nFirst Usage\n\nChange directories to where you want to store the dev version\nClone repo and change to the cloned directory\ngit clone https://github.com/quarto-dev/quarto-cli\ncd quarto-cli\nDisable Anti-Virus\nRun Configuration Script\n\nWindows Command Prompt\ncmd /k configure.cmd\n\n\\k keeps the window open in case it errors\n\nPowershell\nInvoke-Item configure.cmd\nLinux/MacOS\n./configure.sh\nThis will take a minute or two as it checks versions, installs dependencies like pandoc, etc.\n\nAdd path to quarto.cmd to PATH\n\nAfter the configuration file runs, it will output the path you need to put on PATH, e.g.¬†\"C:\\Users\\erc\\Documents\\Quarto\\quarto-cli\\package\\dist\\bin\"\n\nEnable Anti-Virus\nShould be able to use in RStudio\n\nI was not able to use the RStudio terminal for quarto commands (e.g.¬†quarto check) though.\nTo find the version, I just opened powershell and ran quarto ‚Äìversion just to make sure it was running and on PATH.\n\nNot sure if they use this every time but it was 99.9.9 instead of the verion in the changelog.\n\nI also rendered a qmd file using quarto-cmd from the root directory of quarto-cli to see if it matched the output from RStudio. (cd qmd then quarto preview forecasting-statistical.qmd --to html --no-watch-inputs --no-browse)\n\n\nSubsequent Development Versions\n\nChange directory to quarto-cli and git pull\n\n\nShortcuts\n\nNew R chunk: ctrl + alt + i\nBuild whole book: ctrl+shift b\nRender page and preview book: ctrl+shift k\n\nUsing yaml style for chunk options\n\nConvert Rmd chunk options to Quarto: knitr::convert_chunk_header(\"doc.rmd\", \"doc.qmd\")\nAnchor Link - A link, which allows the users to flow through a website page. It helps to scroll and skim-read easily. A named anchor can be used to link to a different part of the same page (like quickly navigating) or to a specific section of another page.\n\nThis is the ‚Äú#sec-moose‚Äù id that can be added to headers which it allows to be referenced within the document or in other documents.\n\nMathJax commands\n\nFont Size: \\tiny{ }, \\scriptsize{ }, \\small{ }, \\normal{ }, \\large{ }, \\Large{ }, \\LARGE{ }, \\huge{ }, \\Huge{ }\n\nLightbox\n\nDocs\nGrouping images for lightbox carousel: ![A Lovely Image](mv-1.jpg){group=\"my-gallery\"}\n\nFoldable (non-executable) Code Blocks\n\nHTML Tags\n![](image.png)\n\n&lt;details&gt;\n\n&lt;summary&gt;Code: My non-executable code&lt;/summary&gt;\n\n``` r\n# code\n```\n\n&lt;/details&gt;",
    "crumbs": [
      "Quarto"
    ]
  },
  {
    "objectID": "qmd/quarto-rmarkdown.html#sec-quarto-syntax",
    "href": "qmd/quarto-rmarkdown.html#sec-quarto-syntax",
    "title": "Quarto",
    "section": "Syntax",
    "text": "Syntax\n\nInline code\n-   Total number of counties: **`{r} polling_places |&gt; filter(state == \"Alabama\") |&gt; distinct(county_name) |&gt; count()`**\n-   Total number of polling places: **`{r} polling_places |&gt; filter(state == \"Alabama\") |&gt; count()`**\n-   Election Day: **`{r} polling_places |&gt; filter(state == \"Alabama\") |&gt; pull(election_date) |&gt; unique()`**\n\nTo escape backticks in inline code, you have to use double-backticks instead of single backticks\n\nExample: To get \"`\"\\$(\\$file.FullName)`\"\"\n`` \"`\"\\$(\\$file.FullName)`\"\" ``\n\n\nAlign code chunk under bullet and add indented comment below chunk\n-   [Example]{.ribbon-highlight} (using a SQL Query; method 1)\n\n    ``` r\n    # open dataset\n    ds &lt;- arrow::open_dataset(dir_out, partitioning = \"species\")\n    # open connection to DuckDB\n    con &lt;- dbConnect(duckdb::duckdb())\n    # register the dataset as a DuckDB table, and give it a name\n    duckdb::duckdb_register_arrow(con, \"my_table\", ds)\n    # query\n    dbGetQuery(con, \"\n      SELECT sepal_length, COUNT(*) AS n\n      FROM my_table\n      WHERE species = 'species=setosa'\n      GROUP BY sepal_length\n    \")\n\n    # clean up\n    duckdb_unregister(con, \"my_table\")\n    dbDisconnect(con)\n    ```\n\n    -   filtering using a partition, the WHERE format is '\\&lt;partition_variable\\&gt;=\\&lt;partition_value\\&gt;'\n\nSpace between bullet and top ticks\nSpace between bottom ticks and bullet\nNote alignment of text\n\nAdd Code Annotations\n-   [Partition a large file and write to arrow format]{.underline}\n\n    ``` r\n    lrg_file &lt;- open_dataset(&lt;file_path&gt;, format = \"csv\") # &lt;1&gt;\n    lrg_file %&gt;%\n        group_by(var) %&gt;% # &lt;2&gt;\n        write_dataset(&lt;output_dir&gt;, format = \"feather\") # &lt;3&gt;\n    ```\n\n    1.  Pass the file path to `open_dataset()`\n\n    2.  Use `group_by()` to partition the Dataset into manageable chunks\n\n    3.  Use `write_dataset()` to write each chunk to a separate Parquet file---all without needing to read the full CSV file into R\n\n    -   `open_dataset` is fast because it only reads the metadata of the file system to determine how it can construct queries\nFootnote\nwords [^1]\n\n[^1]: Data from https://github.com/rfordatascience/tidytuesday\nFor PDF output, you need pagebreaks:\n{{&lt; pagebreak &gt;}}",
    "crumbs": [
      "Quarto"
    ]
  },
  {
    "objectID": "qmd/quarto-rmarkdown.html#sec-quarto-yaml",
    "href": "qmd/quarto-rmarkdown.html#sec-quarto-yaml",
    "title": "Quarto",
    "section": "YAML",
    "text": "YAML\n\nSet global chunk options in yaml\n\n\nFor code cells\nexecute:\n  echo: false\n  message: false\n  warning: false\n\nEnable Margin Notes\n---\n# YAML front matter\nreference-location: margin\n---\n!expr to render code within chunk options\n\ne.g.¬†figure caption: #| fig-cap: !expr glue::glue(\"The mean temperature was {mean(airquality$Temp) |&gt; round()}\")\n\ncolumn: screen-inset yaml markup is used to show a very wide table\nIf you haven‚Äôt set your Quarto document to be self-contained, then the images have also already been saved for you - probably in a folder called documentname_files/figure-html/\nformat: \n  html:\n    embed-resources: true\nDate first published and date modified using the current date:\n---\ndate: 2024-01-01\ndate-modified: today\n---\nYAML Examples\n\nExample",
    "crumbs": [
      "Quarto"
    ]
  },
  {
    "objectID": "qmd/quarto-rmarkdown.html#sec-quarto-chunk",
    "href": "qmd/quarto-rmarkdown.html#sec-quarto-chunk",
    "title": "Quarto",
    "section": "Chunk Options",
    "text": "Chunk Options\n\nGraphics\n\nCode Chunk\n#| label: \"fig-statemap\"\n#| dpi: 300\n#| fig.height: 7.2\n#| fig.width: 3.6\n#| dev: \"png\"\n#| echo: false\n#| warning: false\n#| message: false\n\nExample shows settings for a graph for mobile\nfig.height and fig.width are always given in inches\n\nReference Figure\n1 See polling place locations in @fig-statemap.\n\nConditional Code Chunk Evaluation\n\nExample: document output type\n\nSet value in a code chunk\n```{r setup}\n# Include in first chunk of .qmd\n# Get output file type\nout_type &lt;- knitr::opts_knit$get(\"rmarkdown.pandoc.to\")\n```\nUse !expr sytax to determine evaluation status\n\nExample: eval chunk based on output type\n```{r}\n#| eval: !expr out_type == \"html\"\n\n# code to create interactive {plotly}\n```\n\n```{r}\n#| eval: !expr out_type == \"docx\"\n\n# code to create static {ggplot2}\n```\n\n\nExample: Use parameterization to set value\n---\ntitle: \"test\"\nformat: html\nparams:\n  my_value: false\n---\n\nmy_value can then be used throughout the document to determine chunk evaluation status\n\n\nKnitr Hooks\n\nNotes from Writing knitr hooks\n\nAlso has a knitr hook example that alters cell output (e.g.¬†only prints 4 lines of a vector)\n\nChunk Hooks\n\nChunk hooks get called twice: once before knitr executes the code in the chunk, and once again afterwards\nThe function can take up to four arguments, all of which are optional:\n\nbefore: A logical value indicating whether the function is being called before or after the code chunk is executed\noptions: The list of chunk options\nenvir: The environment in which the code chunk is executed\nname: The name of the code chunk option that triggered the hook function\n\nThe chunk hook is called for its side effects not the return value. However, if it returns a character output, knitr will add that output to the document output as-is.\nExample: Chunk Timer\n\nCode\ncreate_timer_hook &lt;- function() {\n  start_time &lt;- NULL\n  function(before, options) {\n    if (before) {\n      start_time &lt;&lt;- Sys.time()\n    } else {\n      stop_time &lt;- Sys.time()\n      elapsed &lt;- difftime(stop_time, start_time, units = \"secs\")\n      paste(\n        \"&lt;div style='font-size: 70%; text-align: right'&gt;\",\n        \"Elapsed time:\", \n        round(elapsed, 2), \n        \"secs\",\n        \"&lt;/div&gt;\"\n      )\n    }\n  }\n}\nknitr::knit_hooks$set(timer = create_timer_hook())\n\nThe hook is triggered the first time (with before = TRUE) to record the system time somewhere (e.g., in a variable called start_time). Then, when the hook is triggered the second time (with before = FALSE), it records the system time again (e.g., as stop_time), and computes the difference in time.\n\nUse in a cell\n```{r}\n#| timer: true\nrunif(10000)\n```\nOutput",
    "crumbs": [
      "Quarto"
    ]
  },
  {
    "objectID": "qmd/quarto-rmarkdown.html#sec-quarto-rpy",
    "href": "qmd/quarto-rmarkdown.html#sec-quarto-rpy",
    "title": "Quarto",
    "section": "R and Python",
    "text": "R and Python\n\nIf only R or R and Python, the notebook is rendered by {knitr}\nIf only Python, the notebook is rendered by jupyter\nSet-up\n\n{reticulate} automatically comes loaded in Quarto and it knows to use it when it sees a python block, so you don‚Äôt need to load the package\nQuarto will select a version of Python using the Python Launcher on Windows or system PATH on MacOS and Linux. You can override the version of Python used by Quarto by setting the QUARTO_PYTHON environment variable.\n\nIn CLI on Windows, type py is see which version the Python Launcher , and therefore Quarto, is using and py ‚Äìlist to see which versions are installed.\n\n\nR\n```{r}\n#| label: read-data\n#| echo: true\n#| message: false\n#| cache: true\nlemurs &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-08-24/lemur_data.csv')\n```\nPython\n```{python}\n#| label: modelling \n#| echo: true \n#| message: false\n\nlemur_data_py = r.lemur_data \nimport statsmodels.api as sm \ny = lemur_data_py[[\"Weight\"]] \nx = lemur_data_py[[\"Age\"]] \nx = sm.add_constant(x) \nmod = sm.OLS(y, x).fit() \nlemur_data_py[\"Predicted\"] = mod.predict(x) \nlemur_data_py[\"Residuals\"] = mod.resid`\n```\n\nUse r. to access the data in the R chunk\nThe first execution of a python cell starts reticulate::repl_python() in the terminal\n\n(back to) R\n```{r}\n#| label: plotting \n#| echo: true \n#| output-location: slide \n#| message: false \n#| fig-align: center \n#| fig-alt: \"Scatter plot of predicted and residual values for the fitted linear model.\" \n\nlibrary(reticulate) \nlibrary(ggplot2) \nlemur_residuals &lt;- py$lemur_data_py \nggplot(data = lemur_residuals, aes(x = Predicted, y = Residuals)) +\n  geom_point(colour = \"#2F4F4F\") +\n  geom_hline(yintercept = 0,\n            colour = \"red\") +\n  theme(panel.background = element_rect(fill = \"#eaf2f2\", colour = \"#eaf2f2\"),\n        plot.background = element_rect(fill = \"#eaf2f2\", colour = \"#eaf2f2\"))\n```\n\nUse py$ to access the data in the Python chunk *\nMust call library(reticulate) in order for Quarto to recognize py$",
    "crumbs": [
      "Quarto"
    ]
  },
  {
    "objectID": "qmd/quarto-rmarkdown.html#sec-quarto-lay",
    "href": "qmd/quarto-rmarkdown.html#sec-quarto-lay",
    "title": "Quarto",
    "section": "Layouts",
    "text": "Layouts\n\n2 cols (1 col: text, 1 col: image)\n\n::: {layout=\"[50,50]\"}\n\n::: column\nEvery Quarto project starts with a Quarto file that has the extension `.qmd`.\n\n\nThis particular one analyzes children's early words, but every `.qmd` includes the same three basic elements inside:\n\n\n- A block of metadata at the top, between two fences of `---`s. This is written in [YAML](https://learnxinyminutes.com/docs/yaml/). \n- Narrative text, written in [Markdown](https://commonmark.org/help/tutorial/). \n- Code chunks in gray between two fences of ```` ``` ````, written with R or another programming language.\n\n\nYou can use all three elements to develop your code and ideas in one reproducible document.\n:::\n\n![](img/01-source.png)\n:::\n2 figures, 2 columns (i.e.¬†side-by-side) with captions at the top\n---\nfig-cap-location: top\n---\n\n-   Words\n    -   Predictions of Standard RF vs Oblique RF\n\n        ::: {layout-ncol=\"2\"}\n        ![Standard Random Forest](_resources/Regression,_Survival.resources/ml-rf-obl-vs-axis-axpred-1.png){fig-align=\"left\" width=\"432\"}\n\n        ![Oblique Random Forest](_resources/Regression,_Survival.resources/ml-rf-obl-vs-axis-oblpred-1.png){fig-align=\"left\" width=\"432\"}\n        :::\n\n        -   Words  \n\nfig-cap-location: bottom is default;\nfig-cap-location: margin is buggy, at least in for project type book. Captions are added to the margins but bullet points mysteriously disappear during rendering to html\n\n2 charts side-by-side extending past body margins\n```{r}\n#| label: my-figure\n#| layout-ncol: 2\n#| column: page\nggplot() + ...\nggplot() + ...\n```\n\n‚Äúlayout-ncol‚Äù says 2 side-by-side columns\n‚Äúcolumn: page‚Äù says extend column width to the width of the page\n\nNested Tabs\n\n\nCallout Blocks\n\nDocs\nExample: Todo\n::: {.callout-note collapse=\"true\"}\n## todo\n\n1.  Finish video\n    -   Currently working on: multinomial\n2.  Figure out exactly what $e$ is in the weight formula\n3.  Organize note\n4.  Transfer PS Matching section from econometrics-general\n:::\nExample: Caution\n::: callout-caution\nThis note is unfinished. I still have more to add, and the concepts need to be organized.\n:::",
    "crumbs": [
      "Quarto"
    ]
  },
  {
    "objectID": "qmd/quarto-rmarkdown.html#sec-quarto-auto",
    "href": "qmd/quarto-rmarkdown.html#sec-quarto-auto",
    "title": "Quarto",
    "section": "Automation",
    "text": "Automation\n\nIteration and Parameterization\n\nNotes from\n\nVel√°squez R-Ladies Nairobi: Code, Slides, Video\n\nIt involves having a ‚Äúchild‚Äù document as a template and running it repeatedly with different parameters\nThe ‚Äúmain‚Äù document includes the output from the child document\nRendering Options\n\nCLI: e.g.¬†quarto render polling-places-report.qmd -P state:'California'\n{quarto}:\nquarto::quarto_render(\n  input = here::here(\"polling-places-report.qmd\"),\n  execute_params = list(state = \"California\")\n)\n\nExample: Create a report for each parameter value. In each report, use the parameter value (e.g.¬†state) to iterate through a template file that makes a tables (1 for each county) based on that value.\nMain Report Document\n---\ntitle: \"Polling Places Report - `r params$state`\"\nparams:\n  state: \"California\"\n---\n\n```{r}\n#| results: hide\n\nlibrary(dplyr)\n\ncounties &lt;- polling_places |&gt; \n  filter(state == params$state) |&gt; \n  distinct(county_name) |&gt; \n  pull()\n\nexpanded_child &lt;- \n  counties |&gt; \n    purrr::map(\\(county) {\n      knitr::knit_expand(\"../_template.qmd\", \n                         current_county = county))\n      }|&gt; \n    purrr::flatten()\n\nparsed_child &lt;- knitr::knit_child(text = unlist(expanded_child))\n```\n\n`{r} parsed_child`\n\nThe document that gets published, emailed, etc.\nparams specified in YAML\n\nValue can also be used in the title of the document via inline R code\n\nEach county is iterated through the child document (_template.qmd) via current_county variable and knit_expand\nparsed_child is a list of the template file outputs.\nThen, parsed_child is converted to a character vector by unlist and all the results are printed in the document by the inline R code\n\nChild Document (i.e.¬†Template)\n### {{current_county}} COUNTY\n\n-   Total Polling Places: `{r} polling_places |&gt; filter(state == params$state, county_name == \"{{current_county}}\") |&gt; count()`\n-   Example Locations:\n\n```{r}\npolling_places |&gt; \n  filter(state == params$state, \n         county_name == \"{{current_county}}\") |&gt; \n  head(6) |&gt; \n  select(name, address.x) |&gt; \n  kbl(format = \"markdown\")\n```\n\nParameter value is used to get county data and create tables for each.\nNo YAML is necessary in child document\n\nparams values are automatically available through knitr::knit_expand that‚Äôs executed in the Main document\n\nThe county variable is utilized by the template file using the double curly braces, {{current_county}}\nkbl outputs in markdown format so the table is correctly rendered in the Main document.\n\nRendering Script\npolling_places &lt;-\n  readr::read_csv(here::here(\"data\", \"geocoded_polling_places.csv\"))\n\n# create quarto::render arguments df\npolling_places_reports &lt;-\n  polling_places |&gt;\n  dplyr::distinct(state) |&gt;\n  dplyr::slice_head(n = 5) |&gt;\n  dplyr::mutate(\n    output_format = \"html\",\n    output_file = paste0(tolower(state),\n                         \"-polling-places\"),\n    execute_params = purrr::map(state,\n                                \\(state) list(state = state))\n  ) |&gt;\n  # default output is html, so that variable not selected\n  dplyr::select(output_file, execute_params) \n\n# iterate through args and create reports\npurrr::pwalk(\n  .l = polling_places_reports,\n  .f = quarto::quarto_render,\n  input = here::here(\"main_report_document.qmd\"),\n  .progress = TRUE\n)\n\nCreates a report for each params value (e.g.¬†state)\nGenerates a dataframe for each set of arguments to be fed to quarto::quarto_render.",
    "crumbs": [
      "Quarto"
    ]
  },
  {
    "objectID": "qmd/quarto-rmarkdown.html#sec-quarto-webr",
    "href": "qmd/quarto-rmarkdown.html#sec-quarto-webr",
    "title": "Quarto",
    "section": "WebR",
    "text": "WebR\n\nSet-Up\n\nInstall the extension alongside your blog post by running quarto add coatless/quarto-webr\nAdd the extension to your blog by adding filters: [\"webr\"] to your post‚Äôs frontmatter\nInstead of {r} code chunks, use {webr-r} ones\n\nInstall CRAN packages on page load\nfilters:\n  - \"webr\"\nwebr:\n  packages:\n  - \"dplyr\"\n  - \"tidyr\"\n  - \"purrr\"\n  - \"tibble\"\n  - \"crayon\"\n\nAdd to frontmatter\n\nInstall R-Universe Package\n```{webr-r}\n#| context: setup\nwebr::install(\"collateral\", repos = c(\"https://jimjam-slam.r-universe.dev\"))\n```\n\nR-Universe packages must be installed in code cells",
    "crumbs": [
      "Quarto"
    ]
  },
  {
    "objectID": "qmd/clustering-general.html",
    "href": "qmd/clustering-general.html",
    "title": "General",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Clustering",
      "General"
    ]
  },
  {
    "objectID": "qmd/clustering-general.html#sec-clust-gen-misc",
    "href": "qmd/clustering-general.html#sec-clust-gen-misc",
    "title": "General",
    "section": "",
    "text": "Also see\n\nNotebook, pgs 57-58\nDiagnostics, Clustering\n\nFor static data, i.e., if the values do not change with time, clustering methods are usually divided into five major categories:\n\nPartitioning (or Partitional)\nHierarchical\nDensity-Based\nGrid-Based\nModel-Based Methods",
    "crumbs": [
      "Clustering",
      "General"
    ]
  },
  {
    "objectID": "qmd/clustering-general.html#sec-clust-gen-terms",
    "href": "qmd/clustering-general.html#sec-clust-gen-terms",
    "title": "General",
    "section": "Terms",
    "text": "Terms\n\nCluster Centroid - The middle of a cluster. A centroid is a vector that contains one number for each variable, where each number is the mean of a variable for the observations in that cluster. The centroid can be thought of as the multi-dimensional average of the cluster.\nHard (or Crisp) Clustering - each point belongs to a single cluster\nSoft (or Fuzzy) Clustering - each point is allowed to belong to multiple clusters",
    "crumbs": [
      "Clustering",
      "General"
    ]
  },
  {
    "objectID": "qmd/clustering-general.html#sec-clust-gen-clustdesc",
    "href": "qmd/clustering-general.html#sec-clust-gen-clustdesc",
    "title": "General",
    "section": "Cluster Descriptions",
    "text": "Cluster Descriptions\n\nPackages\n\n{parameters} - provides various functions for describing, analyzing, and visualizing clusters for various methods\n{clustereval} - compute the statistical association between the features and the detected cluster labels and whether they are significant.\n\nCategorical: Chi-Square, Fisher‚Äôs Exact, or Hypergeometric tests\nContinuous: Mann-Whitney-U test\n\n\nExamine variable values at the centroids of each cluster\n\nA higher absolute value indicates that a certain variable characteristic is more pronounced within that specific cluster (as compared to other cluster groups with lower absolute mean values).\n\nDistributional statistics for each cluster\n\nNumeric variables: mean and sd for each variable in that cluster\nCategorical variables:\n\nbinary: percent where event = 1\nmultinomial: most prominent category\n\n\nRun a decision tree on clusters\n\n\nEach color (orange, blue, green, purple) represents a cluster\nExplains how clusters were generated\n{treeheatr}\n\n\nRadar charts\n\n\n3 clusters: blue (highlighted), red, green\nGuessing the mean values for each variable are the points\n\nScatter\n\nUse clustering variables of interest for a scatter plot then label the points with cluster id",
    "crumbs": [
      "Clustering",
      "General"
    ]
  },
  {
    "objectID": "qmd/clustering-general.html#sec-clust-gen-gmm",
    "href": "qmd/clustering-general.html#sec-clust-gen-gmm",
    "title": "General",
    "section": "Gaussian Mixture Models (GMM)",
    "text": "Gaussian Mixture Models (GMM)\n\nMisc\n\nSoft clustering algorithm\nNotes from\n\nSerrano video: https://www.youtube.com/watch?v=q71Niz856KE&ab_channel=LuisSerrano\nPackages\n\n{otrimle}\n\nUses Improper Maximum Likelihood Estimator Clustering (IMLEC) method\nHyperparameters automatically tuned; Outliers removed\nRobust gaussian mixture clustering algorithm\nWebpage has links to paper, Coretto and Hennig, 2016\n\n\n\n\nComponents of the Algorithm\n\n‚ÄúColor‚Äù points according to gaussians (clusters)\n\nThe closer a point is to the center of a gaussian the more intensely it matches the color of that gaussian\nPoints in between gaussians are a mixture or proportion of the colors of each gaussian\n\n\nFitting a Gaussian\n\nFind the center of mass\n\n2-dim: calculate the mean of x and the mean of y and that‚Äôs the coordinates of your center of mass\n\nFind the spread of the points\n\n2-dim: calculate the x-variance, y-variance, and covariance\n\n\nFirst Equation: Height of Gaussian (Multivariate Gaussian distribution equation).\nSecond Equation: 1-D gaussian equation that‚Äôs just being used for reference\n\nPartially ‚Äúcolored‚Äù points affect spread and center of mass calculations\n\nFully colored points ‚Äúweigh‚Äù more than partially colored points and pull the center of mass and change the orientation\n\n\n\n\n\nSteps\n\nStart with random Gaussians\n\nEach gaussian has random means, variances\n\nColor points according to distance to the random gaussians\n\nThe heights in the distributions pic above\n\n(Forget about old gaussians) Calculate new gaussians based on the colored points\n(Forget about old colors) Color points according to distance to the new gaussians\nRepeat until some threshold is reached (i.e.¬†gaussians or colors don‚Äôt change much)\n\nTuning\n\nInitial Conditions (i.e.¬†Good starting points for the random gaussians at the beginning)\nLimits on the mean and variance calculations\nNumber of gaussians, k, can be chosen by minimizing the Davies-Bouldin score\n\nSee Diagnostics, Clustering &gt;&gt; Spherical/Centroid Based &gt;&gt; Davies-Bouldin Index\n\nRunning algorithm multiple times\n\nLike CV grid search algs or bootstrapping",
    "crumbs": [
      "Clustering",
      "General"
    ]
  },
  {
    "objectID": "qmd/clustering-general.html#sec-clust-gen-lpa",
    "href": "qmd/clustering-general.html#sec-clust-gen-lpa",
    "title": "General",
    "section": "Latent Profile Analysis (LPA)",
    "text": "Latent Profile Analysis (LPA)\n\nSort of like k-means + GMM\nk number of profiles (i.e.¬†clusters) are chosen\nModel outputs probabilities that an observation belongs to any particular cluster\nGOF metrics available\n‚ÄúAs with Exploratory Factor Analysis (EFA )(and other latent-variable models), the assumption of LPA is that the latent (unobserved) factor‚Äùcauses‚Äù (I‚Äôm using the term loosely here) observed scores on the indicator variables. So, to refer back to my initial hypothetical example, a monster being a spell caster (the unobserved class) causes it to have high intelligence, low strength, etc. rather than the inverse. This is a worthwhile distinction to keep in mind, since it has implications for how the model is fit.‚Äù\nBin variables that might dominate the profile. This way the profiles will represent a latent variable and not gradations of the dominate variable (e.g.¬†low, middle, high values of the dominate variable).\nCenter other variable observations according to dominant variable bin those observations are in. (e.g.¬†subtract values in bin1 from bin1‚Äôs mean)\n# From D&D article where challenge_rating is a likely dominant variable\nmons_bin &lt;- mons_df %&gt;%\n¬† mutate(cr_bin = ntile(x = challenge_rating, n = 6))\nab_scores &lt;- c(\"strength\", \"dexterity\", \"constitution\", \"intelligence\", \"wisdom\", \"charisma\")¬†\nmons_bin &lt;- mons_bin %&gt;%\n¬† group_by(cr_bin) %&gt;%\n¬† mutate(across(.cols = ab_scores, .fns = mean, .names = \"{.col}_bin_mean\")) %&gt;%\n¬† ungroup()",
    "crumbs": [
      "Clustering",
      "General"
    ]
  },
  {
    "objectID": "qmd/clustering-general.html#sec-clust-gen-tsne",
    "href": "qmd/clustering-general.html#sec-clust-gen-tsne",
    "title": "General",
    "section": "tSNE",
    "text": "tSNE\n\nPackages\n\n{Rtsne}\n\nt-Distributed Stochastic Neighbor Embedding\nLooks at the local distances between points in the original data space and tries to reproduce them in the low-dimensional representation\n\nBoth UMAP and tSNE attempt to do this but fails (Lior Pachter paper thread, Doesn‚Äôt preserve local structure, No theorem says that it preserves topology)\n\nResults depend on a random starting point\nTuning parameters: perplexity",
    "crumbs": [
      "Clustering",
      "General"
    ]
  },
  {
    "objectID": "qmd/clustering-general.html#sec-clust-gen-umap",
    "href": "qmd/clustering-general.html#sec-clust-gen-umap",
    "title": "General",
    "section": "UMAP",
    "text": "UMAP\n\nPackages:\n\n{umap}\n{scDEED} (article) - Detects Dubious t-SNE and UMAP Embeddings and Optimizes Hyperparameters\n\nscDEED assigns a reliability score to each 2D embedding to indicate how much the data point‚Äôs mid-range neighbors change in the 2D space. Observations whose 2D embedding neighbors have been drastically changed through the embedding process are called ‚Äòdubious.‚Äô\n\n\nUniform Manifold Approximation and Projection\nSee tSNE section for Lior Pachter threads on why not to use tSNE or UMAP\nPreprocessing\n\nOnly for numeric variables\nStandardize\n\nProjects variables to a nonlinear space\nVariation of tSNE\n\nRandom starting point has less of an impact\n\nCan be supervised (give it an outcome variable)\nComputationally intensive\nLow-dimensional embedding cannot be interpreted\n\nNo rotation matrix plot like in PCA\n\nTry pca - linear method (fast)\n\nIf successful (good separation between categories), then prediction may be easier\nIf not, umap, tsne needed\n\nUMAP can taking training model and apply it to test data or new data (tSNE can‚Äôt)\nTuning parameter: neighbors\n\nExample used 500 iterations (n_epochs) as limit for convergence",
    "crumbs": [
      "Clustering",
      "General"
    ]
  },
  {
    "objectID": "qmd/clustering-general.html#sec-clust-gen-kmeans",
    "href": "qmd/clustering-general.html#sec-clust-gen-kmeans",
    "title": "General",
    "section": "K-Means",
    "text": "K-Means\n\nSeeks to assign n points to k clusters and find cluster centers so as to minimize the sum of squared distances from each point to its cluster center.\nFor choosing the number of clusters, elbow method (i.e.¬†WSS) is usually awful if there are more than few clusters. Recommended: Calinski-Harabasz Index and BIC then Silhouette Coefficient or Davies-Bouldin Index (See Diagnostics, Clustering &gt;&gt; Spherical/Centroid Based (article)\nBase R kmeans uses the Hartigan-Wong algorithm\n\nFor large k and larger n, the density of cluster centers should be proportional to the density of the points to the power (d/d+2). In other words the distribution of clusters found by k-means should be more spread out than the distribution of points. This is not in general achieved by commonly used iterative schemes, which stay stuck close to the initial choice of centers.\n\n{tidyclust}\n\nEngines\n\nstats and ClusterR run classical K-means\nlaR¬†runs K-Modes models which are the categorical analog to K-means, meaning that it is intended to be used on only categorical data\nclustMixType¬†to run K-prototypes which are the more general method that works with categorical and numeric data at the same time.\n\nExample: Mixed K-Means\nlibrary(tidymodels)\nlibrary(tidyclust)\n\ndata(\"ames\", package = \"modeldata\")\n\nkproto_spec &lt;- k_means(num_clusters = 3) %&gt;%\n  set_engine(\"clustMixType\")\n\nkproto_fit &lt;- kproto_spec %&gt;%\n  fit(~ ., data = ames)\n\nkproto_fit %&gt;%\n  extract_centroids() %&gt;%\n  select(11:20) %&gt;%\n  glimpse()\n#&gt; Rows: 3\n#&gt; Columns: 10\n#&gt; $ Lot_Config     &lt;fct&gt; Inside, Inside, Inside\n#&gt; $ Land_Slope     &lt;fct&gt; Gtl, Gtl, Gtl\n#&gt; $ Neighborhood   &lt;fct&gt; College_Creek, North_Ames, Northridge_Heights\n#&gt; $ Condition_1    &lt;fct&gt; Norm, Norm, Norm\n#&gt; $ Condition_2    &lt;fct&gt; Norm, Norm, Norm\n#&gt; $ Bldg_Type      &lt;fct&gt; OneFam, OneFam, OneFam\n#&gt; $ House_Style    &lt;fct&gt; Two_Story, One_Story, One_Story\n#&gt; $ Overall_Cond   &lt;fct&gt; Average, Average, Average\n#&gt; $ Year_Built     &lt;dbl&gt; 1989.977, 1953.793, 1998.765\n#&gt; $ Year_Remod_Add &lt;dbl&gt; 1995.934, 1972.973, 2003.035",
    "crumbs": [
      "Clustering",
      "General"
    ]
  },
  {
    "objectID": "qmd/clustering-general.html#sec-clust-gen-ann",
    "href": "qmd/clustering-general.html#sec-clust-gen-ann",
    "title": "General",
    "section": "Approximate Nearest Neighbor (ANN)",
    "text": "Approximate Nearest Neighbor (ANN)\n\nkNN runs at O(N*K), where N is the number of items and K is the size of each embedding. Approximate nearest neighbor (ANN) algorithms typically drop the complexity of a lookup to O(log(n)).\nMisc\n\nAlso see Maximum inner product search using nearest neighbor search algorithms\n\nIt shows a preprocessing transformation that is performed before kNN to make it more efficient\nIt might already be implemented in ANN algorithms\n\n\nCommonly used in Recommendation algs to cluster user-item embeddings at the end. Also, any NLP task where you need to do a similarity search of one character embedding to other character embeddings.\nGenerally uses one of two main categories of hashing methods: either data-independent methods, such as locality-sensitive hashing (LSH); or data-dependent methods, such as Locality-preserving hashing (LPH)\nLocality-Sensitive Hashing (LSH)\n\nHashes similar input items into the same ‚Äúbuckets‚Äù with high probability.\nThe number of buckets is much smaller than the universe of possible input items\nHash collisions are maximized, not minimized, where a collision is where two distinct data points have the same hash.\n\nSpotify‚Äôs Annoy\n\nUses a type of LSH, Random Projections Method (RPM) (article didn‚Äôt explain this well)\nL RPM hashing functions are chosen. Each data point, p, gets hashed into buckets in each of the L hashing tables. When a new data point, q, is ‚Äúqueried,‚Äù it gets hash into buckets like p did. All the hashes in the same buckets of p are pulled and the hashes within a certain threshold, c*R, are considered nearest neighbors.\n\nWiki article on LSH and RPM clears it up a little, but I‚Äôd probably have to go to Spotify‚Äôs paper to totally make sense of this.\n\nAlso the Spotify alg might bring trees/forests into this somehow\n\nFacebook AI Similarity Search (FAISS)\n\nHierarchical Navigable Small World Graphs (HNSW)\nHNSW has a polylogarithmic time complexity (O(logN))\nTwo approximations available Embeddings are clustered and centroids are calculated. The k nearest centroids are returned.\n\nEmbeddings are clustered into veroni cells. The k nearest embeddings in a veroni cell or a region of veroni cells is returned.\n\nBoth types of approximations have tuning parameters.\n\nInverted File Index + Product Quantization (IVFPQ)(article)",
    "crumbs": [
      "Clustering",
      "General"
    ]
  },
  {
    "objectID": "qmd/clustering-general.html#sec-clust-gen-dbscan",
    "href": "qmd/clustering-general.html#sec-clust-gen-dbscan",
    "title": "General",
    "section": "DBSCAN",
    "text": "DBSCAN\n\n\nMisc\n\nNotes from:\n\nUnderstanding DBSCAN and Implementation with Python\nClustering with DBSCAN, Clearly Explained video\n\nPackages\n\n{dbscan}\n{parameters}\n\nn_clusters_dbscan - Given a ‚Äúmin_size‚Äù (aka minPts?), the function estimates the optimal ‚Äúeps‚Äù\ncluster_analysis - Shows Sum of Squares metrics and the (standardized) mean value for each variable within each cluster.\n\n\nHDBSCAN is the hierarchical density-based clustering algorithm\nUse Cases\n\nGeospatially Clustering Earthquakes\n\nEvents can occur in irregular shaped clusters (i.e., along faults of different orientations).\nEvents can occur in different densities (i.e.¬†some fault zones are more active than others).\nEvents can occur far away from fault zones (i.e.¬†outliers)\n\n\n\nTuning\n\neps - The maximum distance between two samples for one to be considered to be connected to the other\n\nLarge eps tend to include more points within a cluster,\nToo-large eps will include everything in the same single cluster\nToo-small eps will result in no clustering at all\n\nminPts (or min_samples) - The minimum number of samples in a neighborhood for a point to be considered as a core point\n\nToo-small minPts is not meaningful because it will regard every point as a core point.\nLarger minPts can be better to deal with noisy data\n\n\nAlgorithm\n\nFor each data point, find the points in the neighborhood within eps distance, and define the core points as those with at least minPts neighbors.\n\n\nThe orange circle represents the eps area\nIf minPts = 4, then the top 4 points are core points because they have at least 4 points overlapping the eps area\n\nDefine groups of connected core points as clusters.\n\n\nAll the green points have been labelled as core points\n\nAssign each non-core point to a nearby cluster if it‚Äôs directly reachable from a neighboring core point, otherwise define it as an outlier.\n\n\nThe black points are non-core points but are points that overlap the eps area for the outer-most core points.\nAdding these black points finalizes the first cluster\n\nThis process is repeated for the next group of core points and continues until all that‚Äôs left are outliers.\n\nAdvantages\n\nDoesn‚Äôt require users to specify the number of clusters.\nNot sensitive to outliers.\nClusters formed by DBSCAN can be any shape, which makes it robust to different types of data.\n\nExample: Nested Cluster Structure\n\nK-Means\n\n\nK-Means wants spherical clusters which makes it grab groups of points it shouldn‚Äôt\n\nDBSCAN\n\n\nAble correctly identify the oblong shaped cluster\n\n\n\n\nDisadvantages\n\nIf the data has a very large variation in densities across clusters because you can only use one pair of parameters, eps and MinPts, on one dataset\nIt could be hard to define eps without the domain knowledge of the data\nClusters not totally reproducible. Clusters are defined sequentially so depending on which group of core points the algorithm starts with and hyperparameter values, some non-core points that are within the eps area of multiple clusters may be assigned to different clusters on different runs of the algorithm.",
    "crumbs": [
      "Clustering",
      "General"
    ]
  },
  {
    "objectID": "qmd/db-lakes.html#sec-db-lakes-lkhs",
    "href": "qmd/db-lakes.html#sec-db-lakes-lkhs",
    "title": "Lakes",
    "section": "Lakehouse",
    "text": "Lakehouse\n\n\nThe key idea behind a Lakehouse is to be able to take the best of a Data Lake and a Data Warehouse.\n\nData Lakes can in fact provide a lot of flexibility (e.g.¬†handle structured and unstructured data) and low storage cost.\nData Warehouses can provide really good query performance and ACID guarantees.",
    "crumbs": [
      "Databases",
      "Lakes"
    ]
  },
  {
    "objectID": "qmd/db-postgres.html",
    "href": "qmd/db-postgres.html",
    "title": "Postgres",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Databases",
      "Postgres"
    ]
  },
  {
    "objectID": "qmd/db-postgres.html#sec-db-pstgr-misc",
    "href": "qmd/db-postgres.html#sec-db-pstgr-misc",
    "title": "Postgres",
    "section": "",
    "text": "Notes from\n\nCourse: linkedin.learning.postgresql.client.applications\nCourse: Linux.Academy.PostgreSQL.Administration.Deep.Dive\nPostgres is eating the database world\n\nResources\n\nDocs - All on one page so you can just ctrl + f\nPostgreSQL is Enough - Links to various applications/extensions resources\n\nWhen you don‚Äôt use the open-source distribution of PostgreSQL, and instead, utilize PostgreSQL as a managed service by subscribing to a provider like Amazon RDS, you are limited to that service provider‚Äôs list of supported extensions. These services usually offer all the core PostgreSQL capabilities, but may not support an extension you might need in the future.\nEverything is case sensitive, so use lowercase for db and table names\nCheck postgres sql version - psql --version or -V\nSee flag options - psql --help\nIf there‚Äôs a ‚Äú#‚Äù in the prompt after logging into a db, then that signifies you are a super-user\nMeta commands (i.e.¬†commands once you‚Äôre logged into the db)\n\n\\du - list roles (aka users + permissions)\n\\c  - switches databases\n\\password  - assign a password to a user (prompt will ask for the password twice)\n\nCan also use ALTER ROLE for this but the password will then be in the log\n\n\nUnlogged Table - Data written to an unlogged table will not be logged to the write-ahead-log (WAL), making it ideal for intermediate tables and considerably faster. Note that unlogged tables will not be restored in case of a crash, and will not be replicated.\nPigsty\n\nOpen Source RDS alternative\nAims to harness the collective power of PostgreSQL ecosystem extensions and democratize access to high-quality database services.",
    "crumbs": [
      "Databases",
      "Postgres"
    ]
  },
  {
    "objectID": "qmd/db-postgres.html#sec-db-gstgr-ext",
    "href": "qmd/db-postgres.html#sec-db-gstgr-ext",
    "title": "Postgres",
    "section": "Extensions",
    "text": "Extensions\n\npg_analytics\n\nIntro, Repo\nArrow and Datafusion integrated with Postgres\nDelta Lake tables behave like regular Postgres tables but use a column-oriented layout via Apache Arrow and utilize Apache DataFusion, a query engine optimized for column-oriented data\nData is persisted to disk with Parquet\nThe delta-rs library is a Rust-based implementation of Delta Lake. This library adds ACID transactions, updates and deletes, and file compaction to Parquet storage. It also supports querying over data lakes like S3, which introduces the future possibility connecting Postgres tables to cloud data lakes.\n\npg_bm25\n\nIntro, Repo\nRust-based extension that significantly improves Postgres‚Äô full text search capabilities\n\nBuilt to be an Elasticsearch inside of a postgres db\n\nPerformant on large tables, adds support for operations like fuzzy search, relevance tuning, or BM25 relevance scoring (same algo as Elasticsearch), real-time search ‚Äî new data is immediately searchable without manual reindexing\n\nQuery times over 1M rows are 20x faster compared to tsquery and ts_ran (built-in search and sort)\n\nCan be combined with PGVector for semantic fuzzy search\n\nCitus\n\nWebsite\nDistributed Postgres\nTransforms a standalone cluster into a horizontally partitioned distributed database cluster.\nScales Postgres by distributing data & queries. You can start with a single Citus node, then add nodes & rebalance shards when you need to grow.\nCan combine with PostGIS for a distributed geospatial database, PGVector for a distributed vector database, pg_bm25 for a distributed full-text search database, etc.\nyugabytedb is also an option for distributed postgres\n\nduckdb_fdw\n\nRepo\nForeign Data Wrapper (FDW) to connect PostgreSQL to DuckDB database file.\nA Foreign Data Wrapper (FDW) in PostgreSQL is a mechanism that allows you to access and query data stored in external data sources (e.g.¬†duckdb) from within a PostgreSQL database.\n\nplprql\n\nRepo\nEnables you to run PRQL queries. PRQL has a syntax that is similar to {dplyr}\nBuilt in Rust so you have to have pgrx installed. Repo has directions.\n\npgrx\n\nRepo\nFramework for developing PostgreSQL extensions in Rust\nTo install extensions built in Rust, you need to have this extension installed\n\npg_sparse\n\nIntro, Repo\nEnables efficient storage and retrieval of sparse vectors using HNSW\n\nSPLADE outputs sparse vectors with over 30,000 entries. Sparse vectors can detect the presence of exact keywords while also capturing semantic similarity between terms.\n\nFork of pgvector with modifications\nCompatible alongside both pg_bm25 and pgvector\n\npgvector\n\nRepo\nAlso see Databases, Vector Databases for alternatives and comparisons\nEnables efficient storage and retrieval of dense vectors using HNSW\n\nOpenAI‚Äôs text-embedding-ada-002 model outputs dense vectors with 1536 entries\n\nExact and Approximate Nearest Neighbor search\nL2 distance, Inner Product, and Cosine Distance\nSupported inside AWS RDS\n\npg_vectorize\n\nRepo\nWorkflows for both vector search and RAG\nIntegrations with OpenAI‚Äôs embeddings and chat-completion endpoints and a self-hosted container for running Hugging Face Sentence-Transformers\nAutomated creation of Postgres triggers to keep your embeddings up to date\nHigh level API - one function to initialize embeddings transformations, and another function to search",
    "crumbs": [
      "Databases",
      "Postgres"
    ]
  },
  {
    "objectID": "qmd/db-postgres.html#sec-db-pstgr-dock",
    "href": "qmd/db-postgres.html#sec-db-pstgr-dock",
    "title": "Postgres",
    "section": "Docker",
    "text": "Docker\n\nSteps\n\nStart docker desktop\nStart powershell\ndocker run --name pg_database -p 5432:5432 -e POSTGRES_PASSWORD=ericb2022 -d postgres:latest\n\n1st 5432 is local computer port\n2nd 5432 is the required postgres image port\n-e is for defining an environment variable; here its the db password that I set to ericb2022\n-d\n\nRuns the container in the background\nAllows you to run commands in the same terminal window that you used the container run command in\n\n‚Äúpostgres:latest‚Äù is the name of the image to build the container from\n\nClose powershell\nIn docker desktop, the ‚Äúpg_database‚Äù container should be running\n\nConnect to the db\n\nSteps\n\npsql should be in your list of path environment variables\n\nRight-click Start &gt;&gt; System &gt;&gt; advanced settings (right panel) &gt;&gt; environment variables &gt;&gt; highlight path &gt;&gt; edit\n‚ÄúC:\\Program Files\\PostgreSQL\\14\\bin‚Äù\n\n** Note the ‚Äú14‚Äù in the path which is the current version. Therefore, when postgres is updated, this path will have to be updated **\n\n\npsql --host localhost --port 5432 --dbname postgres --username postgres\n\nNote these are all default values, so this is equivalent to psql -U postgres\n‚Äìhost (-h) is the ip address or computer name that you want to connect to\n\nlocalhost is for the docker container that‚Äôs running\n\n5432 is the default ‚Äìport (-p) for a postgres container\n‚Äìdbname (-d) is the name of the database on the server\n\n‚Äúpostgres‚Äù is a db that ships with postgres\n\n‚Äìusername (-U) is a username that has permission to access the db\n\n‚Äúpostgres‚Äù is the default super-user name\n\n\nA prompt will then ask you for that username‚Äôs password\n\nThe container above has the password ericb2022\n\nThis didn‚Äôt work for me, needed to use my postgres password that I set-up when I installed postgres and pgAdmin.\nMy local postgres server and the container are listening on the same port, so maybe if I changed the first port number to something else, it would connect to the container.\n\n\nTo exit db, \\q\n\n\nCreate a db\n\nSteps\n\ncreatedb -h localhost -p 5432 -U postgres -O eric two_trees\n\n-U is the user account used to create the db\n-O is used to assign ownership to another user account\n\n‚Äúrole‚Äù (i.e.¬†user account) must already exist\n\n‚Äútwo_trees‚Äù is the name of the new db\nYou will be prompted for user‚Äôs password\n\nList of dbs on the server\n\npsql -h localhost -p 5432 -U postgres -l\n\n-l lists all dbs on server\nYou will be prompted for user‚Äôs password\n\n\n\n\nRun a sql script\n\npsql -d acweb -f test.sql\n\n-d is for the database name (e.g.¬†acweb)\n-f is for running a file (e.g.¬†test.sql)\n\n\nAdd users\n\nCreate user/role (once inside db)\nCREATE USER &lt;user name1&gt;;\nCREATE ROLE &lt;user name2&gt;;\nALTER ROLE &lt;user name2&gt; LOGIN\n\nCREATE USER will give the user login attribute/permission while CREATE ROLE will not\n\nALTER ROLE gives the user attributes/permissions (e.g.¬†login permission)\n\nCreate user/role (at the CLI) - createuser &lt;user name&gt;",
    "crumbs": [
      "Databases",
      "Postgres"
    ]
  },
  {
    "objectID": "qmd/db-postgres.html#sec-db-pstgr-pgadm",
    "href": "qmd/db-postgres.html#sec-db-pstgr-pgadm",
    "title": "Postgres",
    "section": "pgAdmin",
    "text": "pgAdmin\n\nCreate a server\n\nRight-click on servers &gt;&gt; create &gt;&gt; server\n\nGeneral tab &gt;&gt; enter name\nConnection tab\n\nHost name/address: computer name or ip address where the server is running\n\nlocal: localhost or 127.0.0.1\n\nPort: default = 5432\nMaintenance database: db you want to connect to\n\nIf you haven‚Äôt created it yet, just use default ‚Äúpostgres‚Äù which autmatically created during installation\n\nusername/password\n\nu: default is postgres\np: installation password\nTick Save password\n\n\nClick Save\n\n\nCreate a db\n\nRight-click databases &gt;&gt; create &gt;&gt; databases &gt;&gt; enter name (lowercase) and click save\n\nCreate a table\n\nVia gui\n\nClick db name &gt;&gt; schema &gt;&gt; public &gt;&gt; right-click tables &gt;&gt; create &gt;&gt; tables\nGeneral tab\n\nEnter the table name (lower case)\n\nColumns tab\n\nEnter name, data type, whether there should be a ‚ÄúNot Null‚Äù constraint, and whether it‚Äôs a primary key\nAdd additional column with ‚Äú+‚Äù icon in upper right\nIf you‚Äôre going to fill the table with a .csv file, make sure the column names match\n\nClick save\nTable will be located at db name &gt;&gt; schema &gt;&gt; public &gt;&gt; tables\n\nVia sql\n\nOpen query tool\n\nRight-click  or Schemas or Tables &gt;&gt; query tool\nClick Tools menu dropdown (navbar) &gt;&gt; query tool\n\nRun CREATE TABLE statement\n\nIf you don‚Äôt include the schema as part of the table name, pgadmin automatically places it into the ‚Äúpublic‚Äù schema directory (e.g.¬†public.table_name)\n\n\n\nImport csv into an empty table\n\nMake sure the column names match\nRight-click table name &gt;&gt; import/export\nOptions tab\n\nMake sure import is selected\nSelect the file\nIf you have column names in your csv, select Yes for Header\nSelect ‚Äú,‚Äù for the Delimiter\n\nColumns tab\n\nCheck to make sure all the column names are there\n\nClick OK\n\nQuery Table\n\nRight-click table &gt;&gt; query editor\nQuery editor tab\n\nType query &gt;&gt; click ‚ñ∂ to run query",
    "crumbs": [
      "Databases",
      "Postgres"
    ]
  },
  {
    "objectID": "qmd/db-postgres.html#sec-db-pstgr-rds",
    "href": "qmd/db-postgres.html#sec-db-pstgr-rds",
    "title": "Postgres",
    "section": "AWS RDS",
    "text": "AWS RDS\n\nMisc\n\nNotes from Create an RDS Postgres Instance and connect with pgAdmin\nList of extensions supported by AWS RDS\nAmazon Aurora Limitless is a sharding solution for Postgres for AWS users. However, you should assess the TCO (Total Cost of Ownership) carefully, as many who have evaluated this new service suggest that ‚ÄúLimitless‚Äù might also imply limitless costs.\n\nSteps\n\nSearch AWS services for ‚ÄúRDS‚Äù (top left navbar)\nCreate Database\n\nClick ‚ÄúCreate Database‚Äù\n\nCreate Database\n\nChoose Standard create or Easy Create\n\nEasy Create - uses ‚Äúbest practices‚Äù settings\n\nSelect postgres\n\nAlso available: Amazon Aurora, MySQL, MariaDB, Oracle, Microsoft SQL Server\n\nTemplates\n\nProduction\n\nMulti-AZ Deployment - Multiple Availability Zones\nProvisioned IOPS Storage - Increased output\n\nDev/Test\nRree tier\n\n750 hrs of Amazon RDS in a Single-AZ db.t2.micro Instance.\n20 GB of General Purpose Storage (SSD).\n20 GB for automated backup storage and any user-initiated DB Snapshots.\n\nRDS pricing page\n\nSettings\n\nDB Instance Identifier - enter name\nSet master username, master username password\n\nDB Instance\n\ndb.t3.micro or db.t4g.micro for free tier\n\ndev/test, production has many other options\n\n\nStorage\n\nDefaults: SSD with 20GB\nAutoscaling can up the storage capacity to a default 1000GB",
    "crumbs": [
      "Databases",
      "Postgres"
    ]
  },
  {
    "objectID": "qmd/db-postgres.html#sec-db-pstgr-py",
    "href": "qmd/db-postgres.html#sec-db-pstgr-py",
    "title": "Postgres",
    "section": "Python",
    "text": "Python\n\n{{psycopg2}}\n\nMisc\n\nNotes from Fastest Way to Load Data Into PostgreSQL Using Python\ntl;dr\n\nLarge Data: use copy_to\nMedium to Small Data:\n\nTime and memory isn‚Äôt an issue: Use extract_values or maybe copy_to if you don‚Äôt have JSON.\n\n\n\nConnect to db\nimport psycopg2\n\nconnection = psycopg2.connect(\n    host=\"localhost\",\n    database=\"testload\",\n    user=\"haki\",\n    password=None,\n)\nconnection.autocommit = True\nCreate a table\ndef create_staging_table(cursor) -&gt; None:\n    cursor.execute(\"\"\"\n        DROP TABLE IF EXISTS staging_beers;\n        CREATE UNLOGGED TABLE staging_beers (\n            id                  INTEGER,\n            name                TEXT,\n            tagline             TEXT,\n            first_brewed        DATE,\n            description         TEXT,\n            image_url           TEXT,\n            abv                 DECIMAL,\n            ibu                 DECIMAL,\n            target_fg           DECIMAL,\n            target_og           DECIMAL,\n            ebc                 DECIMAL,\n            srm                 DECIMAL,\n            ph                  DECIMAL,\n            attenuation_level   DECIMAL,\n            brewers_tips        TEXT,\n            contributed_by      TEXT,\n            volume              INTEGER\n        );\n    \"\"\")\n\nwith connection.cursor() as cursor:\n  create_staging_table(cursor)\n\nThe function receives a cursor and creates a unlogged table called staging_beers.\n\nInsert many rows at once\n\nNotes from Fastest Way to Load Data Into PostgreSQL Using Python\nThe best way to load data into a database is using the copy command (last method in this section). The issue here is that copy needs a .csv file and not json.\n\nThis might be an issue just because of psycopg2 library doesn‚Äôt support json or that there is a postgres extension that isn‚Äôt supported by the library. This also might not be a problem in the future.\n\nData\nbeers = iter_beers_from_api()\nnext(beers)\n{'id': 1,\n 'name': 'Buzz',\n 'tagline': 'A Real Bitter Experience.',\n 'first_brewed': '09/2007',\n 'description': 'A light, crisp and bitter IPA brewed...',\n 'image_url': 'https://images.punkapi.com/v2/keg.png',\n 'abv': 4.5,\n 'ibu': 60,\n 'target_fg': 1010,\n...\n}\nnext(beers)\n{'id': 2,\n 'name': 'Trashy Blonde',\n 'tagline': \"You Know You Shouldn't\",\n 'first_brewed': '04/2008',\n 'description': 'A titillating, ...',\n 'image_url': 'https://images.punkapi.com/v2/2.png',\n 'abv': 4.1,\n 'ibu': 41.5,\n ...\n }\n\nData is from beers api\niter_beers_from_api is a udf that takes the json from the api and creates a generator object that iterates through each beer.\n\nInsert data in db using execute_values (low memory usage and still pretty fast)\ndef insert_execute_values_iterator(\n    connection,\n    beers: Iterator[Dict[str, Any]],\n    page_size: int = 100,\n) -&gt; None:\n    with connection.cursor() as cursor:\n        create_staging_table(cursor)\n        psycopg2.extras.execute_values(cursor, \"\"\"\n            INSERT INTO staging_beers VALUES %s;\n        \"\"\", ((\n            beer['id'],\n            beer['name'],\n            beer['tagline'],\n            parse_first_brewed(beer['first_brewed']),\n            beer['description'],\n            beer['image_url'],\n            beer['abv'],\n            beer['ibu'],\n            beer['target_fg'],\n            beer['target_og'],\n            beer['ebc'],\n            beer['srm'],\n            beer['ph'],\n            beer['attenuation_level'],\n            beer['brewers_tips'],\n            beer['contributed_by'],\n            beer['volume']['value'],\n        ) for beer in beers), page_size=page_size)\n\ninsert_execute_values_iterator(page_size=1000)\n\nparse_first_brewed is a udf that transforms a date string to datetime type.\nbeer[‚Äòvolume‚Äô][‚Äòvalue‚Äô]: Data is in json and the value for volume is subsetted from the nested field.\nBenchmark: At page_size = 1000, 1.468s, 0.0MB of RAM used\nThe generator((bear['id'], ‚Ä¶ , bear['volume']['value'], for beer in beers) keeps data from being stored in memory during transformation\npage_size: maximum number of¬†arglist items to include in every statement. If there are more items the function will execute more than one statement.\n\nHere arglist is the data in the form of generator\n\n\nInsert data in db using copy_from (Fast but memory intensive)\nimport io\n\ndef clean_csv_value(value: Optional[Any]) -&gt; str:\n    if value is None:\n        return r'\\N'\n    return str(value).replace('\\n', '\\\\n')\n\ndef copy_stringio(connection, beers: Iterator[Dict[str, Any]]) -&gt; None:\n    with connection.cursor() as cursor:\n        create_staging_table(cursor)\n        csv_file_like_object = io.StringIO()\n        for beer in beers:\n            csv_file_like_object.write('|'.join(map(clean_csv_value, (\n                beer['id'],\n                beer['name'],\n                beer['tagline'],\n                parse_first_brewed(beer['first_brewed']),\n                beer['description'],\n                beer['image_url'],\n                beer['abv'],\n                beer['ibu'],\n                beer['target_fg'],\n                beer['target_og'],\n                beer['ebc'],\n                beer['srm'],\n                beer['ph'],\n                beer['attenuation_level'],\n                beer['contributed_by'],\n                beer['brewers_tips'],\n                beer['volume']['value'],\n            ))) + '\\n')\n        csv_file_like_object.seek(0)\n        cursor.copy_from(csv_file_like_object, 'staging_beers', sep='|')\n\nclean_csv_value: Transforms a single value\n\nEscape new lines: some of the text fields include newlines, so we escape \\n -&gt; \\\\n.\nEmpty values are transformed to \\N: The string \"\\N\" is the default string used by PostgreSQL to indicate NULL in COPY (this can be changed using the NULL option).\n\ncsv_file_like_object: Generate a file like object using io.StringIO. A StringIO object contains a string which can be used like a file. In our case, a CSV file.\ncsv_file_like_object.write: Transform a beer to a CSV row\n\nTransform the data: transformations on first_brewed and volume are performed here.\nPick a delimiter: Some of the fields in the dataset contain free text with commas. To prevent conflicts, we pick ‚Äú|‚Äù as the delimiter (another option is to use QUOTE).\n\n\nInsert data (streaming) in db using copy_from (Fastest and low memory but complicated, at least with json)\n\nBuffering function\nfrom typing import Iterator, Optional\nimport io\n\nclass StringIteratorIO(io.TextIOBase):\n    def __init__(self, iter: Iterator[str]):\n        self._iter = iter\n        self._buff = ''\n\n    def readable(self) -&gt; bool:\n        return True\n\n    def _read1(self, n: Optional[int] = None) -&gt; str:\n        while not self._buff:\n            try:\n                self._buff = next(self._iter)\n            except StopIteration:\n                break\n        ret = self._buff[:n]\n        self._buff = self._buff[len(ret):]\n        return ret\n\n    def read(self, n: Optional[int] = None) -&gt; str:\n        line = []\n        if n is None or n &lt; 0:\n            while True:\n                m = self._read1()\n                if not m:\n                    break\n                line.append(m)\n        else:\n            while n &gt; 0:\n                m = self._read1(n)\n                if not m:\n                    break\n                n -= len(m)\n                line.append(m)\n        return ''.join(line)\n\nThe regular io.StringIO creates a file-like object but is memory-heavy. This function creates buffer that will feed each line of the file into a buffer, stream it to copy, empty the buffer, and load the next line.\n\nCopy to db\ndef clean_csv_value(value: Optional[Any]) -&gt; str:\n    if value is None:\n        return r'\\N'\n    return str(value).replace('\\n', '\\\\n')\n\ndef copy_string_iterator(connection, beers: Iterator[Dict[str,\nAny]]) -&gt; None:\n    with connection.cursor() as cursor:\n        create_staging_table(cursor)\n        beers_string_iterator = StringIteratorIO((\n            '|'.join(map(clean_csv_value, (\n                beer['id'],\n                beer['name'],\n                beer['tagline'],\n                parse_first_brewed(beer['first_brewed']).isoformat(),\n                beer['description'],\n                beer['image_url'],\n                beer['abv'],\n                beer['ibu'],\n                beer['target_fg'],\n                beer['target_og'],\n                beer['ebc'],\n                beer['srm'],\n                beer['ph'],\n                beer['attenuation_level'],\n                beer['brewers_tips'],\n                beer['contributed_by'],\n                beer['volume']['value'],\n            ))) + '\\n'\n            for beer in beers\n        ))\n        cursor.copy_from(beers_string_iterator, 'staging_beers', sep='|')\n\nSimilar to other code above",
    "crumbs": [
      "Databases",
      "Postgres"
    ]
  },
  {
    "objectID": "qmd/db-postgres.html#distributed-architectures",
    "href": "qmd/db-postgres.html#distributed-architectures",
    "title": "Postgres",
    "section": "Distributed Architectures",
    "text": "Distributed Architectures\n\nMisc\n\nNotes from An Overview of Distributed PostgreSQL Architectures\nFeatures to achieve single node availability, durability, and performance - Replication - Place copies of data on different machines - Distribution - Place partitions of data on different machines - Decentralization - Place different DBMS activities on different machines\nIf transactions take on average 20ms, then a single (interactive) session can only do 50 transactions per second. You then need a lot of concurrent sessions to actually achieve high throughput. Having many sessions is not always practical from the application point-of-view, and each session uses significant resources like memory on the database server. Most PostgreSQL set ups limit the maximum number of sessions in the hundreds or low thousands, which puts a hard limit on achievable transaction throughput when network latency is involved.\n\nNetwork-Attached Block Storage (e.g.¬†EBS)\n\n\nCommon technique in cloud-based architectures\nDatabase server typically runs in a virtual machine in a Hypervisor, which exposes a block device to the VM. Any reads and writes to the block device will result in network calls to a block storage API. The block storage service internally replicates the writes to 2-3 storage nodes.\nPros\n\nHigher durability (replication)\nHigher uptime (replace VM, reattach)\nFast backups and replica creation (snapshots)\nDisk is resizable\n\nCons\n\nHigher disk latency (~20Œºs -&gt; ~1000Œºs)\nLower IOPS (~1M -&gt; ~10k IOPS)\nCrash recovery on restart takes time\nCost can be high\n\nGuideline: The durability and availability benefits of network-attached storage usually outweigh the performance downsides, but it‚Äôs worth keeping in mind that PostgreSQL can be much faster.\n\nRead Replicas\n\n\nThe most common way of using a replica is to set it up as a hot standby that takes over when the primary fails in a high availability set up.\nHelps you scale read throughput when reads are CPU or I/O bottlenecked by load balancing queries across replicas, which achieves linear scalability of reads and also offloads the primary, which speeds up writes!\n\nThe primary usually does not wait for replication when committing a write, which means read replicas are always slightly behind. That can become an issue when your application does a read that, from the user‚Äôs perspective, depends on a write that happened earlier.\nFor example, a user clicks ‚ÄúAdd to cart‚Äù, which adds the item to the shopping cart and immediately sends the user to the shopping cart page. If reading the shopping cart contents happens on the read replica, the shopping cart might then appear empty. Hence, you need to be very careful about which reads use a read replica.\n\nWhen load balancing between different nodes, clients might repeatedly get connected to different replica and see a different state of the database\nPowerful tool for scaling reads, but you should consider whether your workload is really appropriate for it.\nPros\n\nRead throughput scales linearly\nLow latency stale reads if read replica is closer than primary\nLower load on primary\n\nCons\n\nEventual read-your-writes consistency\nNo monotonic read consistency\nPoor cache usage\n\nGuideline: Consider using read replicas when you need &gt;100k reads/sec or observe a CPU bottleneck due to reads, best avoided for dependent transactions and large working sets.\n\nDBMS-Optimized Cloud Storage\n\n\nWhere DBMS is Database Management Software. (e.g.¬†Aurora)\nPostgreSQL is not optimized for this architecture\nWhile the theory behind DBMS-optimized storage is sound. In practice, the performance benefits are often not very pronounced (and can be negative), and the cost can be much higher than regular network-attached block storage. It does offer a greater degree of flexibility to the cloud service provider, for instance in terms of attach/detach times, because storage is controlled in the data plane rather than the hypervisor.\nPros\n\nPotential performance benefits by avoiding page writes from primary\nReplicas can reuse storage, incl.¬†hot standby\nCan do faster reattach, branching than network-attached storage\n\nCons\n\nWrite latency is high by default\nHigh cost / pricing\nPostgreSQL is not designed for it, not OSS\n\nGuideline: Can be beneficial for complex workloads, but important to measure whether price-performance under load is actually better than using a bigger machine.\n\nActive-Active (e.g.¬†BDR)\n\n\nAny node can locally accept writes without coordination with other nodes.\nIt is typically used with replicas in multiple sites, each of which will then see low read and write latency, and can survive failure of other sites.\nActive-active systems do not have a linear history, even at the row level, which makes them very hard to program against.\nPros\n\nVery high read and write availability\nLow read and write latency\nRead throughput scales linearly\n\nCons\n\nEventual read-your-writes consistency\nNo monotonic read consistency\nNo linear history (updates might conflict after commit)\n\nGuideline: Consider only for very simple workloads (e.g.¬†queues) and only if you really need the benefits.\n\nTransparent Sharding (e.g.¬†Citus)\n\n\nTables distributed and/or replicated across multiple primary nodes using a ‚Äúshard key .‚Äù\n\nEach node shows the distributed tables as if they were regular PostgreSQL tables and queries\n\nData are located in ‚Äúshards‚Äù which are regular PostgreSQL tables. Joins and foreign keys that include the shard key can be performed locally.\nScaling out transactional workloads is most effective when queries have a filter on the shard key, such that they can be routed to a single shard group (e.g.¬†single tenant in a multi-tenant app) or compute-heavy analytical queries that can be parallelized across the shards (e.g.¬†time series / IoT).\nWhen loading data, use COPY, instead of INSERT, to avoid waiting for every row.\nPros\n\nScale throughput for reads & writes (CPU & IOPS)\nScale memory for large working sets\nParallelize analytical queries, batch operations\n\nCons\n\nHigh read and write latency\nData model decisions have high impact on performance\nSnapshot isolation concessions\n\nGuideline: Use for multi-tenant apps, otherwise use for large working set (&gt;100GB) or compute heavy queries.\n\nDistributed Key-Value Stores With SQL (e.g.¬†Yugabyte)\n\n\nA bunch of complicated stuff I don‚Äôt understand üòÖ\nTables are stored in the key-value store, with the key being a combination of the table ID and the primary key.\nBetter to use PostgresSQL without this architecture.\nPros\n\nGood read and write availability (shard-level failover)\nSingle table, single key operations scale well\nNo additional data modeling steps or snapshot isolation concessions\n\nCons\n\nMany internal operations incur high latency\nNo local joins in current implementations\nNot actually PostgreSQL, and less mature and optimized\n\nGuideline: Just use PostgreSQL. For simple applications, the availability and scalability benefits can be useful.",
    "crumbs": [
      "Databases",
      "Postgres"
    ]
  },
  {
    "objectID": "qmd/missingness.html",
    "href": "qmd/missingness.html",
    "title": "Missingness",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Missingness"
    ]
  },
  {
    "objectID": "qmd/missingness.html#sec-missing-misc",
    "href": "qmd/missingness.html#sec-missing-misc",
    "title": "Missingness",
    "section": "",
    "text": "Missing data can reduce the statistical power of a study and can produce biased estimates, leading to invalid conclusions\nAlso see\n\nEDA &gt;&gt; Missingness\nModel Building, tidymodels &gt;&gt; Recipe &gt;&gt; Imputation\n\nBagging and knn methods for cross-sectional data\nRolling method for time series data\n\n\nResources\n\nHarrell RMS 3.5 Strategies for Developing an Imputation Model\nFlexible Imputation of Missing Data\nIntroduction to Regression Methods for Public Health Using R: Chapter 9 Multiple Imputation of Missing Data\n\nPackages\n\n{mice} (Multivariate Imputation by Chained Equations) - Imputes mixes of continuous, binary, unordered categorical and ordered categorical data\n\nBased on Fully Conditional Specification, where each incomplete variable is imputed by a separate model.\nImpute continuous two-level data, and maintain consistency between imputations by means of passive imputation.\nMany diagnostic plots are implemented to inspect the quality of the imputations.\n\n{naniar} - Tidyverse compliant methods to summarize, visualize, and manipulate missing data.\n{simputation} - Model-based, multivariate, donar, and simple stat methods available\n{NPBayesImputeCat}: Non-Parametric Bayesian Multiple Imputation for Categorical Data\n\nProvides routines to i) create multiple imputations for missing data and ii) create synthetic data for statistical disclosure control, for multivariate categorical data, with or without structural zeros\nImputations and syntheses are based on Dirichlet process mixtures of multinomial distributions, which is a non-parametric Bayesian modeling approach that allows for flexible joint modeling\nVignette\n\n\n‚ÄúBut more precisely, even having the correct model of the analysis stage does not absolve the analyst of considering the relationship between the imputation stage variables, the causal model, and the missingness mechanism. It turns out that in this simple example, imputing with an analysis-stage collider is innocuous (so long as it is excluded at the analysis stage). But imputation-stage colliders can wreck MI even if they are excluded from the analysis stage.‚Äù\n\nSee Multiple Imputation with Colliders\n\n**Don‚Äôt impute missing values before your training/test split\nImputing Types full-information maximum likelihood\n\nMultiple imputation\nOne-Step Bayesian imputation\n\nMissness Types (MCAR, MAR, and MNAR)\n\nMultivariate Imputation with Chained Equation (MICE) assumes MAR\n\nMethod entails creating multiple imputations for each missing value as opposed to just one. The algorithm addresses statistical uncertainty and enables users to impute values for data of different types.\n\nStochastic Regression Imputation is problematic\n\nPopular among practitioners though\nIssues\n\nStochastic regression imputation might lead to implausible values (e.g.¬†negative incomes).\nStochastic regression imputation has problems with heteroscedastic data\n\nBayesian PMM handles these issues\n\nMissingness in RCT due dropouts (aka loss to follow-up)\n\nNotes from To impute or not: the case of an RCT with baseline and follow-up measurements\n\n{mice} used for imputation\n\nBias in treatment effect due to missingness\n\nIf there are adjustment variables that affect unit dropout then bias increases as variation in treatment effect across units increases (aka hetergeneity)\n\nIn the example, a baseline measurement of the outcome variable, used an explanatory variable, was also causal of missingness. Greater values of this variable resulted in greater bias\nUsing multiple imputation resulted in less bias than just using complete cases, but still underestimated the treatment effect.\n\nIf there are no such variables, then there is no bias due to hetergeneous treatment effects\n\nComplete cases of the data can be used\n\n\nLast observation carried forward\n\nSometimes used in clinical trials because it tends to be conservative, setting a higher bar for showing that a new therapy is significantly better than a traditional therapy.\nMust assume that the previous value (e.g.¬†2008 score) is similar to the ahead value (e.g.¬†2010 score).\nInformation about trajectories over time is thrown away.\n\n\nAssessment of Imputations\n\nSee {naniar} vignette - Expanding Tidy Data Principles to Facilitate Missing Data Exploration, Visualization and Assessment of Imputations | Journal of Statistical Software",
    "crumbs": [
      "Missingness"
    ]
  },
  {
    "objectID": "qmd/missingness.html#sec-missing-caim",
    "href": "qmd/missingness.html#sec-missing-caim",
    "title": "Missingness",
    "section": "Choosing an Imputation Method",
    "text": "Choosing an Imputation Method\n\n** Don‚Äôt use this. Just putting it here to be aware of **) Standard Procedure for choosing an imputation method\n\nIssues\n\nSome methods will be favored based on the metric used\n\nConditional means methods (RMSE)\nConditional medians methos (MAE) Chosen methods tend to artificially strengthen the association between variables. As a consequence, statistical estimation and inference techniques applied to the so-imputed data set can be invalid.\n\n\nSteps\n\nSelect some observations\nSet their status to missing\nImpute them with different methods\nCompare their imputation accuracy\n\nFor numeric variables, RMSE or MAE typically used\nFor categoricals, percentage of correct predictions (PCP)\n\n\n\nInitial Considerations\n\nIf a dataset‚Äôs feature has missing data in more than 80% of its records, it is probably best to remove that feature altogether.\nIf a feature with missing values is strongly correlated with other missing values, it‚Äôs worth considering using advanced imputation techniques that use information from those other features to derive values to replace the missing data.\nIf a feature‚Äôs values are missing not at random (MNAR), remove methods like MICE from consideration. I-Score {Iscores}, Paper\nA proper scoring rule metric\nConsistent for MCAR, but MAR requires additional assumptions\n\n‚Äúvalid under missing at random (MAR) if we restrict the random projections in variable space to always include all variables, which in turn requires access to some complete observations‚Äù\n\nKinda complicated. I need to read the paper",
    "crumbs": [
      "Missingness"
    ]
  },
  {
    "objectID": "qmd/missingness.html#sec-missing-bayes",
    "href": "qmd/missingness.html#sec-missing-bayes",
    "title": "Missingness",
    "section": "Bayesian",
    "text": "Bayesian\n\nPredictive Mean Matching (PMM)\n\nNotes from:\n\nPredictive Mean Matching Imputation (Theory & Example in R)\nPredictive Mean Matching Imputation in R (mice Package Example)\n\nUses a bayesian regression to predict a missing value, then randomly picks a value from a group of observed values that are closest to the predicted value.\nSteps\n\nEstimate a linear regression model:\n\nUse the variable we want to impute as Y.\nUse a set of good predictors as \\(X\\) (Guidelines for the selection of \\(X\\) can be found in van Buuren, 2012, p.¬†128).\nUse only the observed values of \\(X\\) and \\(Y\\) to estimate the model.\n\nDraw randomly from the posterior predictive distribution of \\(\\hat \\beta\\) and produce a new set of coefficients \\(\\beta^*\\).\n\nThis bayesian step is needed for all multiple imputation methods to create some random variability in the imputed values.\n\nCalculate predicted values for observed and missing \\(Y\\).\n\nUse \\(\\hat \\beta\\) to calculate predicted values for observed \\(Y\\).\nUse \\(\\beta^*\\) to calculate predicted values for missing \\(Y\\).\n\nFor each case where \\(Y\\) is missing, find the closest predicted values among cases where \\(Y\\) is observed.\n\nExample:\n\n\\(Y_i\\) is missing. Its predicted value is 10 (based on \\(\\beta^*\\)).\nOur data consists of five observed cases of \\(Y\\) with the values 6, 3, 22, 7, and 12.\nIn step 3, we predicted the values 7, 2, 20, 9, and 13 for these five observed cases (based on \\(\\hat \\beta\\)).\nThe predictive mean matching algorithm selects the closest observed values (typically three cases) to our missing value \\(Y_i\\). Hence, the algorithm selects the values 7, 9, and 13 (the closest values to 10).\n\n\nDraw randomly one of these three close cases and impute the missing value \\(Y_i\\) with the observed value of this close case.\n\nExample: Continued\n\nThe algorithm draws randomly from 6, 7, and 12 (the observed values that correspond to the predicted values 7, 9, and 13).\nThe algorithm chooses 12 and substitutes this value to \\(Y_i\\).\n\n\nIn case of multiple imputation (strongly advised), steps 1-5 are repeated several times.\n\nEach repetition of steps 1-5 creates a new imputed data set.\nWith multiple imputation, missing data is typically imputed 5 times.\n\n\nExample\ndata_imp &lt;- \n  complete(mice(data,\n           m = 5,\n           method = \"pmm\"))\n\nm is the number of times to impute the data\ncomplete formats the data into different shapes according to an action argument\nRunning parmice instead of mice imputes in parallel",
    "crumbs": [
      "Missingness"
    ]
  },
  {
    "objectID": "qmd/missingness.html#sec-missing-multimp",
    "href": "qmd/missingness.html#sec-missing-multimp",
    "title": "Missingness",
    "section": "Multiple Imputation Fit",
    "text": "Multiple Imputation Fit\n\nAKA ‚Äúmultiply‚Äù imputed data\nThe key difficulty multiple imputation is that the result of multiple imputation is K replicated datasets corresponding to different estimated values for the missing data in the original dataset.\nPackages\n\n{merTools} - Tools for aggregating results for multiply imputed Mixed Effects model data\n\nFitting a regression model with multiply imputed data\n\nSee If you fit a model with multiply imputed data, you can still plot the line\nMethods\n\nPredict then Combine (PC)\nCombine then Predict (CP)",
    "crumbs": [
      "Missingness"
    ]
  },
  {
    "objectID": "qmd/missingness.html#sec-missing-ts",
    "href": "qmd/missingness.html#sec-missing-ts",
    "title": "Missingness",
    "section": "Time Series",
    "text": "Time Series\n\nResources\n\nForward/Backward Filling, Linear/Spline Interpolation - Handle Missing Values in Time Series For Beginners\n\nIf seasonality is present, mean, median, mode, random assignment, or previous value methods shouldn‚Äôt be used.",
    "crumbs": [
      "Missingness"
    ]
  },
  {
    "objectID": "qmd/llms-general.html",
    "href": "qmd/llms-general.html",
    "title": "General",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "LLMs",
      "General"
    ]
  },
  {
    "objectID": "qmd/llms-general.html#misc",
    "href": "qmd/llms-general.html#misc",
    "title": "General",
    "section": "",
    "text": "What chatGPT is:\n\n‚ÄúWhat would a response to this question sound like‚Äù machine Researchers build (train) large language models like GPT-3 and GPT-4 by using a process called ‚Äúunsupervised learning,‚Äù which means the data they use to train the model isn‚Äôt specially annotated or labeled. During this process, the model is fed a large body of text (millions of books, websites, articles, poems, transcripts, and other sources) and repeatedly tries to predict the next word in every sequence of words. If the model‚Äôs prediction is close to the actual next word, the neural network updates its parameters to reinforce the patterns that led to that prediction.\nConversely, if the prediction is incorrect, the model adjusts its parameters to improve its performance and tries again. This process of trial and error, though a technique called ‚Äúbackpropagation,‚Äù allows the model to learn from its mistakes and gradually improve its predictions during the training process. As a result, GPT learns statistical associations between words and related concepts in the data set.\nIn the current wave of GPT models, this core training (now often called ‚Äúpre-training‚Äù) happens only once. After that, people can use the trained neural network in ‚Äúinference mode,‚Äù which lets users feed an input into the trained network and get a result. During inference, the input sequence for the GPT model is always provided by a human, and it‚Äôs called a ‚Äúprompt.‚Äù The prompt determines the model‚Äôs output, and altering the prompt even slightly can dramatically change what the model produces.Iterative prompting is limited by the size of the model‚Äôs ‚Äúcontext window‚Äù since each prompt is appended onto the previous prompt.  ChatGPT is different from vanilla GPT-3 because it has also been trained on transcripts of conversations written by humans. ‚ÄúWe trained an initial model using supervised fine-tuning: human AI trainers provided conversations in which they played both sides‚Äîthe user and an AI assistant,‚Äù\nChatGPT has also been tuned more heavily than GPT-3 using a technique called ‚Äúreinforcement learning from human feedback,‚Äù or RLHF, where human raters ranked ChatGPT‚Äôs responses in order of preference, then fed that information back into the model. This has allowed the ChatGPT to produce coherent responses with fewer confabulations than the base model. The prevalence of accurate content in the data set, recognition of factual information in the results by humans, or reinforcement learning guidance from humans that emphasizes certain factual responses.\nTwo major types of falsehoods that LLMs like ChatGPT might produce. The first comes from inaccurate source material in its training data set, such as common misconceptions (e.g., ‚Äúeating turkey makes you drowsy‚Äù). The second arises from making inferences about specific situations that are absent from its training material (data set); this falls under the aforementioned ‚Äúhallucination‚Äù label.\nWhether the GPT model makes a wild guess or not is based on a property that AI researchers call ‚Äútemperature,‚Äù which is often characterized as a ‚Äúcreativity‚Äù setting. If the creativity is set high, the model will guess wildly; if it‚Äôs set low, it will spit out data deterministically based on its data set. If creativity is set low, ‚Äú[It] answers ‚ÄòI don‚Äôt know‚Äô all the time or only reads what is there in the Search results (also sometimes incorrect). What is missing is the tone of voice: it shouldn‚Äôt sound so confident in those situations.‚Äù\nIn some ways, ChatGPT is a mirror: It gives you back what you feed it. If you feed it falsehoods, it will tend to agree with you and ‚Äúthink‚Äù along those lines. That‚Äôs why it‚Äôs important to start fresh with a new prompt when changing subjects or experiencing unwanted responses.\n‚ÄúOne of the most actively researched approaches for increasing factuality in LLMs is retrieval augmentation‚Äîproviding external documents to the model to use as sources and supporting context,‚Äù said Goodside. With that technique, he explained, researchers hope to teach models to use external search engines like Google, ‚Äúciting reliable sources in their answers as a human researcher might, and rely less on the unreliable factual knowledge learned during model training.‚Äù Bing Chat and Google Bard do this already by roping in searches from the web, and soon, a browser-enabled version of ChatGPT will as well. Additionally, ChatGPT plugins aim to supplement GPT-4‚Äôs training data with information it retrieves from external sources, such as the web and purpose-built databases.\nOther things that might help with hallucination include, ‚Äúa more sophisticated data curation and the linking of the training data with ‚Äòtrust‚Äô scores, using a method not unlike PageRank‚Ä¶ It would also be possible to fine-tune the model to hedge when it is less confident in the response.‚Äù (arstechnica article)\n\nOpenAI models\n\ndavinci (e.g.¬†davinci-003) text-generation models are 10x more expensive than their chat counterparts (e.g.¬†gpt-3.5-turbo)\nFor lower usage in the 1000‚Äôs of requests per day range ChatGPT works out cheaper than using open-sourced LLMs deployed to AWS. For millions of requests per day, open-sourced models deployed in AWS work out cheaper. (As of April 24th, 2023.) (article)\n\nUsed AWS Lambda for deployment\n\ndavinci hasn‚Äôt been trained using reinforcement learning from human feedback (RLHF}\nchatgpt 3.5 turbo models\n\nPros\n\nPerforms better on 0 shot classification tasks than Davinci-003\nOutperforms Davinci-003 on sentiment analysis\nSignificantly better than Davinci-003 at math\ncheaper than davinci\n\nCons\n\nTends to produce longer responses than Davinci-003, which may not be ideal for all use cases\nIncluding k-shot examples can lead to inefficient resource usage in multi-turn use cases\n\n\ndavinci-003\n\nPros\n\nPerforms slightly better than GPT-3.5 Turbo with k-shot examples\nProduces more concise responses than GPT-3.5 Turbo, which may be preferable for certain use cases\n\nCons\n\nLess accurate than GPT-3.5 Turbo on 0 shot classification tasks and sentiment analysis\nPerforms significantly worse than GPT-3.5 Turbo on math tasks\n\n\n\nUse Cases\n\nUnderstanding code (Can reduce cognative load)(article)\n\nDuring code reviews or onboarding new programmers\nunder-commented code\n\nGenerating the code scaffold for a problem where you aren‚Äôt sure where or how to start solving it.\nLLMs don‚Äôt require removing stopwords during preprocessing of documents\n\nCost\n\nFor lower usage in the 1000‚Äôs of requests per day range ChatGPT works out cheaper than using open-sourced LLMs deployed to AWS. For millions of requests per day, open-sourced models deployed in AWS work out cheaper. (article, April 24th, 2023.)\n\nMethods for giving chatGPT data\n\nThink you can upload a file\nThrough prompt\n\nSee bizsci video\n\npaste actual data\npaste column names and types (glimse() with no values)\n\nGenerate a string for each row of data that contains the column name and value\n\nExample\n\n‚ÄúThe  is . The  is . ‚Ä¶‚Äù\n‚ÄúThe fico_score is 578.0. The load_amount is 6000.0.¬† The annual income is 57643.54.‚Äù\n\n\n\n\nEvolution of LLMs",
    "crumbs": [
      "LLMs",
      "General"
    ]
  },
  {
    "objectID": "qmd/llms-agent-chains.html",
    "href": "qmd/llms-agent-chains.html",
    "title": "Agent Chains",
    "section": "",
    "text": "LangChain",
    "crumbs": [
      "LLMs",
      "Agent Chains"
    ]
  },
  {
    "objectID": "qmd/llms-agent-chains.html#langchain",
    "href": "qmd/llms-agent-chains.html#langchain",
    "title": "Agent Chains",
    "section": "",
    "text": "Framework for developing applications powered by language models; connect a language model to other sources of data; allow a language model to interact with its environment\nImplementation of the paper¬†ReAct: Synergizing Reasoning and Acting in Language Models which demonstrates a prompting technique to allow the model to ‚Äúreason‚Äù (with a chain-of-thoughts) and ‚Äúact‚Äù (by being able to use a tool from a predefined set of tools, such as being able to search the internet). This combination is shown to drastically improve output text quality and give large language models the ability to correctly solve problems.\nMisc\n\nNotes from:\n\nA Gentle Intro to Chaining LLMs, Agents, and utils via LangChain\n\nSee article for workflow for multi-chains\nAlso shows some diagnostic methods that are included in the library\n\n\nAvailable vector stores for document embeddings\n\nAlso see Databases, Vector Databases\n\nSee article for description and link to code for a manual, more controlled parsing of markdown files to get text and code blocks\n\nComponents\n\nDocument loader: Facilitate the data loading from various sources, including CSV files, SQL databases, and public datasets like Wikipedia.\nAgent: Use the language model as a reasoning engine to determine which actions to take and in which order. It repeats through a continuous cycle of thought-action-observation until the task is completed.\nChain: Different from agents, they consist of predetermined sequences of actions, which are hard coded. It addresses complex and well-defined tasks by guiding multiple tools with high-level directions.\nMemory: Currently the beta version supports accessing windows of past messages, this provides the application with a conversational interface\n\nIn general, chains are what you get by sequentially connecting one or more large language models (LLMs) in a logical way.\nChains can be built using LLMs or ‚ÄúAgents‚Äù.\n\nAgents provide the ability to answer questions that require recent or specialty information the LLM hasn‚Äôt been trained on.\n\ne.g.¬†‚ÄúWhat will the weather be like tomorrow?‚Äù\nAn agent has access to an LLM and a suite of tools for example Google Search, Python REPL, math calculator, weather APIs, etc. (list of supported agents)\nLLMs will use tools to interact with Agents (list of tools)\n\nTool process\n\nUses a LLMChain for building the API URL based on our input instructions and makes the API call.\nUpon receiving the response, it uses another LLMChain that summarizes the response to get the answer to our original question.\n\n\nReAct (Reason + Act) is a popular agent that picks the most usable tool (from a list of tools), based on what the input query is.\n\nOutput Components: observation, a thought, or it takes an action. This is mainly due to the ReAct framework and the associated prompt that the agent is using (See example with ZERO_SHOT_REACT_DESCRIPTION)\n\nserpapi is useful for answering questions about current events.\n\n\nChains can be simple (i.e.¬†Generic) or specialized (i.e.¬†Utility).\n\nGeneric ‚Äî A single LLM is the simplest chain. It takes an input prompt and the name of the LLM and then uses the LLM for text generation (i.e.¬†output for the prompt).\n\nGeneric chains are more often used as building blocks for Utility chains\n\nUtility ‚Äî These are specialized chains, comprised of many LLMs to help solve a specific task. For example,\n\nLangChain supports some end-to-end chains (such as AnalyzeDocumentChain for summarization, QnA, etc) and some specific ones (such as GraphQnAChain for creating, querying, and saving graphs). Programme Aided Language Model reads complex math problems (described in natural language) and generates programs (for solving the math problem) as the intermediate reasoning steps, but offloads the solution step to a runtime such as a Python interpreter.\n2-Chain Examples\n\nChain 1 is used to clean the prompt (remove extra whitespaces, shorten prompt, etc) and chain 2 is used to call an LLM with this clean prompt. (link)\nChain 1 is used to generate a synopsis for a play and chain is used to write a review based on this synopsis. (link)\n\n\n\nDocument Loaders (docs)- various helper functions that take various formats and types of data and produce a document output\n\nFormats like like markdown, word docs, text, PowerPoint, images, HTML, PDF, csvs, AsciiDoc (adoc), etc.\nExamples\n\nGitLoader function clones the repository and load relevant files as documents\nYoutubeLoader - gets subtitles from videos\nDataFrameLoader - converts text columns in panda dfs to documents\n\nAlso, tons of other functions for googledrive or dbs like bigquery, duckdb or cloud storage like s3 or confluence or email or discord, etc.\n\nText Spitters (Docs) - After loading the documents, they‚Äôre usually fed to text splitter to create chunks of text due to LLM context constraints. The chunks of text can then be transformed into embeddings.\n\nFrom ‚ÄúSales and Support Chatbot‚Äù article in example below\n\n\n# Define text chunk strategy\nsplitter = CharacterTextSplitter(\n¬† chunk_size=2000,\n¬† chunk_overlap=50,\n¬† separator=\" \"\n)\n# GDS guides\ngds_loader = GitLoader(\n¬† ¬† clone_url=\"https://github.com/neo4j/graph-data-science\",\n¬† ¬† repo_path=\"./repos/gds/\",\n¬† ¬† branch=\"master\",\n¬† ¬† file_filter=lambda file_path: file_path.endswith(\".adoc\")\n¬† ¬† and \"pages\" in file_path,\n)\ngds_data = gds_loader.load()\n# Split documents into chunks\ngds_data_split = splitter.split_documents(gds_data)\nprint(len(gds_data_split)) #771\n\nEmbeddings\n\nOpenAI‚Äôs text-embedding-ada-002 model is easy to work with, achieves the highest performance out of all of OpenAI‚Äôs embedding models (on the BEIR benchmark), and is also the cheapest ($0.0004/1K tokens).\nHuggingFace‚Äôs sentence-transformers , which reportedly has better performance than OpenAI‚Äôs embeddings, but that involves downloading the model and running it on your own server.\n\nExample: Generic\n\nBuild Prompt\n\n\nfrom langchain.prompts import PromptTemplate\nprompt = PromptTemplate(\n¬† ¬† input_variables=[\"product\"],\n¬† ¬† template=\"What is a good name for a company that makes [{product}]{style='color: #990000'}?\",\n)\nprint(prompt.format(product=\"podcast player\"))\n# OUTPUT\n# What is a good name for a company that makes podcast player?\n\nIf using multiple variables, then you need, e.g.¬†print(prompt.format(product=\"podcast player\", audience=\"children‚Äù), to get the updated prompt.\nCreate LLMChain instance and run\n\nfrom langchain.llms import OpenAI\nfrom langchain.chains import LLMChain\nllm = OpenAI(\n¬† ¬† ¬† ¬† ¬† model_name=\"text-davinci-003\", # default model\n¬† ¬† ¬† ¬† ¬† temperature=0.9) #temperature dictates how whacky the output should be\nllmchain = LLMChain(llm=llm, prompt=prompt)\nllmchain.run(\"podcast player\")\n\nIf you had more than one input_variables, then you won‚Äôt be able to use run. Instead, you‚Äôll have to pass all the variables as a dict.\n\ne.g., LLMchain({‚Äúproduct‚Äù: ‚Äúpodcast player‚Äù, ‚Äúaudience‚Äù: ‚Äúchildren‚Äù}).\n\nUsing the less expensive chat models: chatopenai = ChatOpenAI(model_name=\"gpt-3.5-turbo\")\nExample: Multiple Chains and Multiple Input Variables\n\nGoal: create an age-appropriate gift generator\nChain 1: Find age\n\n\n# Chain1 - solve math problem, get the age\nfrom langchain.agents import initialize_agent\nfrom langchain.agents import AgentType\nfrom langchain.agents import load_tools\n\nllm = OpenAI(temperature=0)\ntools = load_tools([\"pal-math\"], llm=llm)\nagent = initialize_agent(tools,\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† llm,\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† verbose=True)\n\npal-math is a math-solving tool\nReact agent uses the tool to answer the age problem\nChain 2: Recommend a gift\n\ntemplate = \"\"\"You are a gift recommender. Given a person's age,\\n\nit is your job to suggest an appropriate gift for them. If age is under 10,\\n\nthe gift should cost no more than [{budget}]{style='color: #990000'} otherwise it should cost atleast 10 times [{budget}]{style='color: #990000'}.\nPerson Age:\n[{output}]{style='color: #990000'}\nSuggest gift:\"\"\"\nprompt_template = PromptTemplate(input_variables=[\"output\", \"budget\"], template=template)\nchain_two = LLMChain(llm=llm, prompt=prompt_template)\n\n‚Äú{output}‚Äù is the name of the output from the 1st chain\n\nFind the name of the output of a chain: print(agent.agent.llm_chain.output_keys)\n\nThe prompt includes a conditional that transforms {budget} (more below)\nLLMchain is used when there are multiple variable in the template\nCombine Chains and Run\n\noverall_chain = SequentialChain(\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† input_variables=[\"input\"],\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† memory=SimpleMemory(memories={\"budget\": \"100 GBP\"}),\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† chains=[agent, chain_two],\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† verbose=True)\noverall_chain.run(\"If my age is half of my dad's age and he is going to be 60 next year, what is my current age?\")\n\nThe prompt is only for the 1st chain, and it‚Äôs output, Age, will be input for the second chain.\nSimpleMemory is used pass the variable for the second prompt which adds some additional context to the second chain ‚Äî the {budget} for the gift.\nOutput\n\n#&gt; Entering new SequentialChain chain...\n#&gt; Entering new AgentExecutor chain...\n# I need to figure out my dad's current age and then divide it by two.\n#Action: PAL-MATH\n#Action Input: What is my dad's current age if he is going to be 60 next year?\n#Observation: 59\n#Thought: I now know my dad's current age, so I can divide it by two to get my age.\n#Action: Divide 59 by 2\n#Action Input: 59/2\n#Observation: Divide 59 by 2 is not a valid tool, try another one.\n#Thought: I can use PAL-MATH to divide 59 by 2.\n#Action: PAL-MATH\n#Action Input: Divide 59 by 2\n#Observation: 29.5\n#Thought: I now know the final answer.\n#Final Answer: My current age is 29.5 years old.\n#&gt; Finished chain.\n# For someone of your age, a good gift would be something that is both practical and meaningful. Consider something like a nice watch, a piece of jewelry, a nice leather bag, or a gift card to a favorite store or restaurant.\\nIf you have a larger budget, you could consider something like a weekend getaway, a spa package, or a special experience.'}\n#&gt; Finished chain.\n\nExample: Sales and Support Chatbot (article)\n\nCreate embeddings from text data sources and store in Chroma vector store\n\n\n# Define embedding model\nOPENAI_API_KEY = \"OPENAI_API_KEY\"\nembeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\nsales_data = medium_data_split + yt_data_split\nsales_store = Chroma.from_documents(\n¬† ¬† sales_data, embeddings, collection_name=\"sales\"\n)\nsupport_data = kb_data + gds_data_split + so_data\nsupport_store = Chroma.from_documents(\n¬† ¬† support_data, embeddings, collection_name=\"support\"\n)\n\nSales data is from Medium articles and YouTube subtitles\nSupport data is from docs in a couple github repos and stackoverflow\nInstantiate chatgpt\n\nllm = ChatOpenAI(\n¬† ¬† model_name=\"gpt-3.5-turbo\",\n¬† ¬† temperature=0,\n¬† ¬† openai_api_key=OPENAI_API_KEY,\n¬† ¬† max_tokens=512,\n)\n\nSales prompt template\n\nsales_template = \"\"\"As a Neo4j marketing bot, your goal is to provide accurate¬†\nand helpful information about Neo4j, a powerful graph database used for¬†\nbuilding various applications. You should answer user inquiries based on the¬†\ncontext provided and avoid making up answers. If you don't know the answer,¬†\nsimply state that you don't know. Remember to provide relevant information¬†\nabout Neo4j's features, benefits, and use cases to assist the user in¬†\nunderstanding its value for application development.\n[{context}]{style='color: #990000'}\nQuestion: [{question}]{style='color: #990000'}\"\"\"\nSALES_PROMPT = PromptTemplate(\n¬† ¬† template=sales_template, input_variables=[\"context\", \"question\"]\n)\nsales_qa = RetrievalQA.from_chain_type(\n¬† ¬† llm=llm,\n¬† ¬† chain_type=\"stuff\",\n¬† ¬† retriever=sales_store.as_retriever(),\n¬† ¬† chain_type_kwargs={\"prompt\": SALES_PROMPT},\n)\n\n‚Äú{context}‚Äù in the template is the data stored in the vector store\n‚Äúretriever‚Äù arg points to vector store (e.g.¬†sales_store) and uses the as_retriever method to get the embeddings\nSupport prompt template\n\nsupport_template = \"\"\"\nAs a Neo4j Customer Support bot, you are here to assist with any issues¬†\na user might be facing with their graph database implementation and Cypher statements.\nPlease provide as much detail as possible about the problem, how to solve it, and steps a user should take to fix it.\nIf the provided context doesn't provide enough information, you are allowed to use your knowledge and experience to offer you the best possible assistance.\n[{context}]{style='color: #990000'}\nQuestion: [{question}]{style='color: #990000'}\"\"\"\nSUPPORT_PROMPT = PromptTemplate(\n¬† ¬† template=support_template, input_variables=[\"context\", \"question\"]\n)\nsupport_qa = RetrievalQA.from_chain_type(\n¬† ¬† llm=llm,\n¬† ¬† chain_type=\"stuff\",\n¬† ¬† retriever=support_store.as_retriever(),\n¬† ¬† chain_type_kwargs={\"prompt\": SUPPORT_PROMPT},\n)\n\nSee Sales template\nAdd React agent to determine whether LLM should use Sales or Support templates and contexts\n\ntools = [\n¬† ¬† Tool(\n¬† ¬† ¬† ¬† name=\"sales\",\n¬† ¬† ¬† ¬† func=sales_qa.run,\n¬† ¬† ¬† ¬† description=\"\"\"useful for when a user is interested in various Neo4j information,¬†\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† use-cases, or applications. A user is not asking for any debugging, but is only\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† interested in general advice for integrating and using Neo4j.\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† Input should be a fully formed question.\"\"\",\n¬† ¬† ),\n¬† ¬† Tool(\n¬† ¬† ¬† ¬† name=\"support\",\n¬† ¬† ¬† ¬† func=support_qa.run,\n¬† ¬† ¬† ¬† description=\"\"\"useful for when when a user asks to optimize or debug a Cypher statement or needs\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† specific instructions how to accomplish a specified task.¬†\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† Input should be a fully formed question.\"\"\",\n¬† ¬† ),\n]\n\nagent = initialize_agent(\n¬† ¬† tools,¬†\n¬† ¬† llm,¬†\n¬† ¬† agent=\"zero-shot-react-description\",¬†\n¬† ¬† verbose=True\n)\nagent.run(\"\"\"What are some GPT-4 applications with Neo4j?\"\"\")\n\nIn this example, the tools used by the Agent are custom data sources and prompt templates",
    "crumbs": [
      "LLMs",
      "Agent Chains"
    ]
  },
  {
    "objectID": "qmd/llms-fine-tuning.html",
    "href": "qmd/llms-fine-tuning.html",
    "title": "Fine Tuning",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "LLMs",
      "Fine Tuning"
    ]
  },
  {
    "objectID": "qmd/llms-fine-tuning.html#sec-nlp-fintun-misc",
    "href": "qmd/llms-fine-tuning.html#sec-nlp-fintun-misc",
    "title": "Fine Tuning",
    "section": "",
    "text": "Tuning an LLM\n\nNotes from:\n\nHacker‚Äôs Guide to Language Models (Howard)\n\nStages\n\nLM Pre-Training - Trained on a large corpus (e.g.¬†much of the internet) to predict the next word in a sentence or to fill in a word in a sentence\nLM Fine-Tuning - Trained on a specific task (e.g.¬†solve problems, answer questions). Instruction Tuning is often used. OpenOrca is an example of a Q&A dataset to train a LM to answer questions. Still predicting the next word, like in Pre-Training, but more target-based on a specific task.\nClassifier Fine-Tuning - Reinforcement Learning from Human Feedback (RLHF)is often used. The LLM being trained gives a few answers to a question and then a human or better LLM will pick which one is best.\n\nPre-Trained LLMs are ones that are typically the open source ones being released and available for download\n\nThey will need to be Fine-Tuned, but not necessarily Classifier Fine-Tuned. Often times, LM Fine-Tuning is enough.\n\n\nWorkflow Example (paper)\n\nRaschka - An introduction to the core ideas and approaches to Finetuning Large Language Models - by Sebastian Raschka\nWith ChatGPT, you can have it answer questions from context that contains thousands of documents.\n\n\nStore all these documents as small chunks of text (allowable size of the context window) in a database.\nCreate embeddings of documents and question\nThe documents of relevance can then be found by computing similarities between the question and the document chunks. This is done typically by converting the chunks and question into word embedding vectors, and computing cosine similarities between chunks and question, and finally choosing only those chunks above a certain cosine similarity as relevant context.\nFinally, the question and context can be combined into a prompt as below, and fed into an LLM API like ChatGPT: prompt=f\"Answer the question. Context: {context}\\n Question: {question}\"",
    "crumbs": [
      "LLMs",
      "Fine Tuning"
    ]
  },
  {
    "objectID": "qmd/llms-prompt-engineering.html",
    "href": "qmd/llms-prompt-engineering.html",
    "title": "Prompt Engineering",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "LLMs",
      "Prompt Engineering"
    ]
  },
  {
    "objectID": "qmd/llms-prompt-engineering.html#sec-nlp-prompt-misc",
    "href": "qmd/llms-prompt-engineering.html#sec-nlp-prompt-misc",
    "title": "Prompt Engineering",
    "section": "",
    "text": "Resources\n\nAnthropic Prompt Library\n\nDefinition\n\nAsking the right question\n‚ÄúPrompt engineering is the process of designing and optimizing prompts to LLMs for a wide variety of applications and research topics. Prompts are short pieces of text that are used to guide the LM‚Äôs output. They can be used to specify the task that the LM is supposed to accomplish, provide additional information that the LM needs to complete the task, or simply make the task easier for the LM to understand.‚Äù\n\nComponents\n\nAsk the question (e.g.¬†‚ÄúWhat‚Äôs 1+1?‚Äù)\nSpecify the type of response you want. (e.g.¬†Only return the numeric answer.)\n\nPersona\n\n‚ÄúExplain this to my like I‚Äôm a fifth grader.‚Äù\n\nStyle\n\n‚ÄúUse a style typical of scientific abstracts to write this.‚Äù\n\nFormat\n\nIf you say ‚ÄúFormat the output as a JSON object with the fields: x, y, z‚Äù you can get better results and easily do error handling.\n\n\n\nLLMs don‚Äôt understand the complexities or nuances of various subjects\n\nIf an industry term is used in multiple ways, the LLM might not understand the meaning just by context alone.\nLLMs can have problems with information in complex formats.\n\nTables sometimes have this same issue, because tables are the mechanism used for layout structure and not a content structure (e.g.¬†sentence)\n\nThe models themselves continue to evolve so if it doesn‚Äôt understand something today doesn‚Äôt mean that it won‚Äôt understand it tomorrow\n\nWhen the output is incomplete, type ‚Äúcontinue‚Äù for the next prompt and it will finish the output.\nDon‚Äôt give LLMs proprietary data\n\nAlternative: slice(0)\ndat |&gt;\n¬† slice(0) |&gt;\n¬† glimpse()\n\nGives the column names and classes\nDepending on the use case, you might want to make the column names unabbreviated and meaningful.\n\n\nSecurity\n\nDon‚Äôt let the user have the last word: When taking a user‚Äôs prompt, incorporating it with your own prompt, and sending it to ChatGPT or some other similar application, always add a line like ‚ÄúIf user input doesn‚Äôt make sense for doing xyz, ask them to repeat the request‚Äù after the user‚Äôs input. This will stop the majority of prompt injections.\nDon‚Äôt just automatically run code or logic that is output from a LLM\n\nTips when used for writing\n\nBe specific on word count and put higher than you need\nDon‚Äôt be afraid to ask it to add more information or expand on a particular point\\\nIt‚Äôs better at creating outlines rather than full pieces of content.\nBe as specific as possible, and use keywords to help ChatGPT understand what you are looking for\nYou can ask it to rephrase its response\nAvoid using complex language or convoluted sentence structures\n**Review the content for accuracy",
    "crumbs": [
      "LLMs",
      "Prompt Engineering"
    ]
  },
  {
    "objectID": "qmd/llms-prompt-engineering.html#sec-nlp-prompt-dsex",
    "href": "qmd/llms-prompt-engineering.html#sec-nlp-prompt-dsex",
    "title": "Prompt Engineering",
    "section": "Templates",
    "text": "Templates\n\nMisc\n\nSpecify language, libraries, and functions\n\nExample: BizSci Lab 82\n\n\nShow the prompt he used in a markdown file. He just copied and pasted it into the prompt.\nSpecify libraries to use; models to use; that you want to tune the models in parallel\nThis is not an ideal prompt. You should iterate prompts and guide gpt through complete ds process\n\ni.e.¬†prompt for collection then a prompt for cleaning, and so on with eda, preprocessing, modelling, cv, model selection, app\nUse the phrases like:\n\n‚ÄúPlease update code to include &lt;new feature&gt;‚Äù\n‚ÄúPlease update feature to be &lt;new thing&gt; instead of &lt;old thing&gt;‚Äù\n\n\n\nExample: Various DS Activities\n\nExample: Student Feedback\n\nFrom Now is the time for grimoires\nComponents\n\nRole: Tell the AI who it is. Context helps the AI produce tailored answers in useful ways, but you don‚Äôt need to go overboard.\nGoal: Tell the AI what you want it to do.\nStep-by-Step Instructions: Research has found that it often works best to give the AI explicit instructions that go step-by-step through what you want.\n\nOne approach, called Chain of Thought prompting, gives the AI an example of how you want it to reason before you make your request\nYou can also give it step-by-step directions the way we do in our prompts.\n\nConsider Examples: Few-shot prompting, where you give the AI examples of the kinds of output you want to see, has also proven very effective in research.\nAdd Personalization: Ask the user for information to help tailor the prompt for them.\nAdd Your Own Constraints: The AI often acts in ways that you may not want. Constraints tell it to avoid behaviors that may come up in your testing.\nFinal Step: Check your prompt by trying it out, giving it good, bad, and neutral input. Take the perspective of your users‚Äì is the AI helpful? Does the process work? How might the AI be more helpful? Does it need more context? Does it need further constraints? You can continue to tweak the prompt until it works for you and until you feel it will work for your audience.\n\nPrompt",
    "crumbs": [
      "LLMs",
      "Prompt Engineering"
    ]
  },
  {
    "objectID": "qmd/code-snippets.html",
    "href": "qmd/code-snippets.html",
    "title": "Snippets",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Code",
      "Snippets"
    ]
  },
  {
    "objectID": "qmd/code-snippets.html#sec-code-snippits-misc",
    "href": "qmd/code-snippets.html#sec-code-snippits-misc",
    "title": "Snippets",
    "section": "",
    "text": "Check whether an environment variable is empty\nnzchar(Sys.getenv(\"blopblopblop\"))\n#&gt; [1] FALSE\nwithr::with_envvar(\n  new = c(\"blopblopblop\" = \"bla\"),\n  nzchar(Sys.getenv(\"blopblopblop\"))\n)\nUse a package for a single instance using {withr::with_package}\n\n\nUsing library() will keep the package loaded during the whole session, with_package() just runs the code snippet with that package temporarily loaded. This can be useful to avoid namespace collisions for example\n\nRead .csv from a zipped file\n# long way\ntmpf &lt;- tempfile()\ntmpd &lt;- tempfile()\ndownload.file('https://website.org/path/to/file.zip', tmpf)\nunzip(tmpf, exdir = tmpd)\ny &lt;- data.table::fread(file.path(tmpd,\n                       grep('csv$',\n                            unzip(tmpf, list = TRUE)$Name,\n                            value = TRUE)))\nunlink(tmpf)\nunlink(tmpd)\n\n# quick way\ny &lt;- data.table::fread('curl https://website.org/path/to/file.zip | funzip')\nLoad all R scripts from a directory: for (file in list.files(\"R\", full.names = TRUE)) source(file)\nView dataframe in View as html table using {kableExtra}\ndf_html &lt;- kableExtra::kbl(rbind(head(df, 5), tail(df, 5)), format = \"html\")\nprint(df_html)",
    "crumbs": [
      "Code",
      "Snippets"
    ]
  },
  {
    "objectID": "qmd/code-snippets.html#sec-code-snippits-opts",
    "href": "qmd/code-snippets.html#sec-code-snippits-opts",
    "title": "Snippets",
    "section": "Options",
    "text": "Options\n\n{readr}\noptions(readr.show_col_types = FALSE)",
    "crumbs": [
      "Code",
      "Snippets"
    ]
  },
  {
    "objectID": "qmd/code-snippets.html#sec-code-snippits-cleaning",
    "href": "qmd/code-snippets.html#sec-code-snippits-cleaning",
    "title": "Snippets",
    "section": "Cleaning",
    "text": "Cleaning\n\nRemove all objects except: rm(list=setdiff(ls(), c(\"train\", \"validate\", \"test\")))\nRemove NAs\n\ndataframes\ndf %&gt;% na.omit\ndf %&gt;% filter(complete.cases(.))\ndf %&gt;% tidyr::drop_na()\nvariables\ndf %&gt;% filter(!is.na(x1))\ndf %&gt;% tidyr::drop_na(x1)\n\nFind duplicate rows\n\n{datawizard} - Extract all duplicates, for visual inspection. Note that it also contains the first occurrence of future duplicates, unlike duplicated or dplyr::distinct. Also contains an additional column reporting the number of missing values for that row, to help in the decision-making when selecting which duplicates to keep.\ndf1 &lt;- data.frame(\n  id = c(1, 2, 3, 1, 3),\n  year = c(2022, 2022, 2022, 2022, 2000),\n  item1 = c(NA, 1, 1, 2, 3),\n  item2 = c(NA, 1, 1, 2, 3),\n  item3 = c(NA, 1, 1, 2, 3)\n)\n\ndata_duplicated(df1, select = \"id\")\n#&gt;   Row id year item1 item2 item3 count_na\n#&gt; 1   1  1 2022    NA    NA    NA        3\n#&gt; 4   4  1 2022     2     2     2        0\n#&gt; 3   3  3 2022     1     1     1        0\n#&gt; 5   5  3 2000     3     3     3        0\n\ndata_duplicated(df1, select = c(\"id\", \"year\"))\n#&gt; 1   1  1 2022    NA    NA    NA        3\n#&gt; 4   4  1 2022     2     2     2        0\ndplyr\ndups &lt;- dat %&gt;%¬†\n¬† group_by(BookingNumber, BookingDate, Charge) %&gt;%¬†\n¬† filter(n() &gt; 1)\nbase r\ndf[duplicated(df[\"ID\"], fromLast = F) | duplicated(df[\"ID\"], fromLast = T), ]\n\n##¬† ¬† ¬† ¬† ID value_1 value_2 value_1_2\n## 2¬† ID-003¬† ¬† ¬† 6¬† ¬† ¬† 5¬† ¬† ¬† 6 5\n## 3¬† ID-006¬† ¬† ¬† 1¬† ¬† ¬† 3¬† ¬† ¬† 1 3\n## 4¬† ID-003¬† ¬† ¬† 1¬† ¬† ¬† 4¬† ¬† ¬† 1 4\n## 5¬† ID-005¬† ¬† ¬† 5¬† ¬† ¬† 5¬† ¬† ¬† 5 5\n## 6¬† ID-003¬† ¬† ¬† 2¬† ¬† ¬† 3¬† ¬† ¬† 2 3\n## 7¬† ID-005¬† ¬† ¬† 2¬† ¬† ¬† 2¬† ¬† ¬† 2 2\n## 9¬† ID-006¬† ¬† ¬† 7¬† ¬† ¬† 2¬† ¬† ¬† 7 2\n## 10 ID-006¬† ¬† ¬† 2¬† ¬† ¬† 3¬† ¬† ¬† 2 3\n\ndf[duplicated(df[\"ID\"], fromLast = F) doesn‚Äôt include the first occurence, so also counting from the opposite direction will include all occurences of the duplicated rows\n\n\nRemove duplicated rows\n\n{datawizard} - From all rows with at least one duplicated ID, keep only one. Methods for selecting the duplicated row are either the first duplicate, the last duplicate, or the ‚Äúbest‚Äù duplicate (default), based on the duplicate with the smallest number of NA. In case of ties, it picks the first duplicate, as it is the one most likely to be valid and authentic, given practice effects.\ndf1 &lt;- data.frame(\n  id = c(1, 2, 3, 1, 3),\n  item1 = c(NA, 1, 1, 2, 3),\n  item2 = c(NA, 1, 1, 2, 3),\n  item3 = c(NA, 1, 1, 2, 3)\n)\n\ndata_unique(df1, select = \"id\")\n#&gt; (2 duplicates removed, with method 'best')\n#&gt;   id item1 item2 item3\n#&gt; 1  1     2     2     2\n#&gt; 2  2     1     1     1\n#&gt; 3  3     1     1     1\nbase R\ndf[!duplicated(df[c(\"col1\")]), ]\ndplyr\ndistinct(df, col1, .keep_all = TRUE)\n\nShowing all combinations present in the data and creating all possible combinations\n\nFuzzy Join (alt to case_when)\nref.df &lt;- data.frame(\n¬† ¬† ¬† ¬† ¬† ¬† bucket = c(‚ÄúHigh‚Äù, ‚ÄúMedium-High‚Äù, ‚ÄúMedium-Low‚Äù, ‚ÄúLow‚Äù),\n¬† ¬† ¬† ¬† ¬† ¬† value.high = c(max(USArrests$Assault), 249, 199, 149),\n¬† ¬† ¬† ¬† ¬† ¬† value.low = c(250, 200, 150, min(USArrests$Assault)))\nUSArrests %&gt;%¬†\n¬† fuzzy_join(ref.df,¬†\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† by = c(\"Assault\"=\"value.low\",\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† \"Assault\" = 'value.high'),¬†\n¬† ¬† ¬† ¬† ¬† ¬† match_fun = c(`&gt;=`,`&lt;=`)) %&gt;%¬†\n¬† select(-c(value.high, value.low))\n\nAlso does partial matches\n\n\n\n\nRemove elements of a list by name\npurrr::discard_at(my_list, \"a\")\nlistr::list_remove",
    "crumbs": [
      "Code",
      "Snippets"
    ]
  },
  {
    "objectID": "qmd/code-snippets.html#sec-code-snippits-func",
    "href": "qmd/code-snippets.html#sec-code-snippits-func",
    "title": "Snippets",
    "section": "Functions",
    "text": "Functions\n\nggplot\nviz_monthly &lt;- function(df, y_var, threshhold = NULL) {\n\n¬† ggplot(df) +\n¬† ¬† aes(\n¬† ¬† ¬† x = .data[[\"day\"]],\n¬† ¬† ¬† y = .data[[y_var]]\n¬† ¬† ) +\n¬† ¬† geom_line() +\n¬† ¬† geom_hline(yintercept = threshhold, color = \"red\", linetype = 2) +\n¬† ¬† scale_x_continuous(breaks = seq(1, 29, by = 7)) +\n¬† ¬† theme_minimal()\n}\n\naes is on the outside\n\nThis was a function for a shiny module\nIt‚Äôs peculier. Necessary for function or module?\n\n\nCreate formula from string\nanalysis_formula &lt;- 'Days_Attended ~ W + School'\nestimator_func &lt;-¬† function(data) lm(as.formula(analysis_formula), data = data)\nRecursive Function\n\nExample\n# Replace pkg text with html\nreplace_txt &lt;- function(dat, patterns) {\n  if (length(patterns) == 0) {\n    return(dat)\n  }\n\n  pattern_str &lt;- patterns[[1]]$pattern_str\n  repl_str &lt;- patterns[[1]]$repl_str\n  replaced_txt &lt;- dat |&gt;\n    str_replace_all(pattern = pattern_str, repl_str)\n\n  new_patterns &lt;- patterns[-1]\n  replace_txt(replaced_txt, new_patterns)\n}\n\nArguments include the dataset and the iterable\nTests whether function has iterated through pattern list\nRemoves 1st element of the list\nreplace_text calls itself within the function with the new list and new dataset\n\nExample: Using Recall and tryCatch\nload_page_completely &lt;- function(rd) {\n  # load more content even if it throws an error\n  tryCatch({\n      # call load_more()\n      load_more(rd)\n      # if no error is thrown, call the load_page_completely() function again\n      Recall(rd)\n  }, error = function(e) {\n      # if an error is thrown return nothing / NULL\n  })\n}\n\nload_more is a user defined function\nRecall is a base R function that calls the same function it‚Äôs in.",
    "crumbs": [
      "Code",
      "Snippets"
    ]
  },
  {
    "objectID": "qmd/code-snippets.html#sec-code-snippits-calcs",
    "href": "qmd/code-snippets.html#sec-code-snippits-calcs",
    "title": "Snippets",
    "section": "Calculations",
    "text": "Calculations\n\nCompute the running maximum per group\n(df &lt;- structure(list(var = c(5L, 2L, 3L, 4L, 0L, 3L, 6L, 4L, 8L, 4L),\n¬† ¬† ¬† ¬† ¬† ¬† ¬† group = structure(c(1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L),\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† .Label = c(\"a\", \"b\"), class = \"factor\"),\n¬† ¬† ¬† ¬† ¬† ¬† ¬† time = c(1L, 2L, 3L, 4L, 5L, 1L, 2L, 3L, 4L, 5L)),\n¬† ¬† ¬† ¬† ¬† .Names = c(\"var\", \"group\",\"time\"),\n¬† ¬† ¬† ¬† ¬† class = \"data.frame\", row.names = c(NA, -10L)))\n\ndf[order(df$group, df$time),]\n#¬† ¬† var group time\n# 1¬† ¬† 5¬† ¬† a¬† ¬† 1\n# 2¬† ¬† 2¬† ¬† a¬† ¬† 2\n# 3¬† ¬† 3¬† ¬† a¬† ¬† 3\n# 4¬† ¬† 4¬† ¬† a¬† ¬† 4\n# 5¬† ¬† 0¬† ¬† a¬† ¬† 5\n# 6¬† ¬† 3¬† ¬† b¬† ¬† 1\n# 7¬† ¬† 6¬† ¬† b¬† ¬† 2\n# 8¬† ¬† 4¬† ¬† b¬† ¬† 3\n# 9¬† ¬† 8¬† ¬† b¬† ¬† 4\n# 10¬† 4¬† ¬† b¬† ¬† 5\n\ndf$curMax &lt;- ave(df$var, df$group, FUN=cummax)\ndf\nvar¬† |¬† group¬† |¬† time¬† |¬† curMax\n5¬† ¬† ¬† a¬† ¬† ¬† ¬† 1¬† ¬† ¬† ¬† 5\n2¬† ¬† ¬† a¬† ¬† ¬† ¬† 2¬† ¬† ¬† ¬† 5\n3¬† ¬† ¬† a¬† ¬† ¬† ¬† 3¬† ¬† ¬† ¬† 5\n4¬† ¬† ¬† a¬† ¬† ¬† ¬† 4¬† ¬† ¬† ¬† 5\n0¬† ¬† ¬† a¬† ¬† ¬† ¬† 5¬† ¬† ¬† ¬† 5\n3¬† ¬† ¬† b¬† ¬† ¬† ¬† 1¬† ¬† ¬† ¬† 3\n6¬† ¬† ¬† b¬† ¬† ¬† ¬† 2¬† ¬† ¬† ¬† 6\n4¬† ¬† ¬† b¬† ¬† ¬† ¬† 3¬† ¬† ¬† ¬† 6\n8¬† ¬† ¬† b¬† ¬† ¬† ¬† 4¬† ¬† ¬† ¬† 8\n4¬† ¬† ¬† b¬† ¬† ¬† ¬† 5¬† ¬† ¬† ¬† 8\n\n\nTime Series\n\nBase-R\n\nIntervals\n\nDifference between dates\n# Sample dates\nstart_date &lt;- as.Date(\"2022-01-15\")\nend_date &lt;- as.Date(\"2023-07-20\")\n\n# Calculate time difference in days\ntime_diff_days &lt;- end_date - start_date\n\n# Convert days to months\nmonths_diff_base &lt;- as.numeric(time_diff_days) / 30.44  # average days in a month\n\ncat(\"Number of months using base R:\", round(months_diff_base, 2), \"\\n\")\n#&gt; Number of months using base R: 18.1 \n\n\n\n\n{lubridate}\n\nDocs\nIntervals\n\nLubridate‚Äôs interval functions\nNotes from: Wrangling interval data using lubridate\nDifference between dates\n# Load the lubridate package\nlibrary(lubridate)\n\n# Sample dates\nstart_date &lt;- ymd(\"2022-01-15\")\nend_date &lt;- ymd(\"2023-07-20\")\n\n# Calculate months difference using lubridate\nmonths_diff_lubridate &lt;- interval(start_date, end_date) %/% months(1)\n\ncat(\"Number of months using lubridate:\", months_diff_lubridate, \"\\n\")\n#&gt; Number of months using lubridate: 18 \n\n%/% is used for floor division by months. For decimals, just use /\n\nData\n(house_df &lt;- tibble(\n  person_id  = factor(c(\"A10232\", \"A10232\", \"A10232\", \"A39211\", \"A39211\", \"A28183\", \"A28183\", \"A10124\")),\n  house_id   = factor(c(\"H1200E\", \"H1243D\", \"H3432B\", \"HA7382\", \"H53621\", \"HC39EF\", \"HA3A01\", \"H222BA\")),\n  start_date = ymd(c(\"20200101\", \"20200112\", \"20211120\", \"19800101\", \"19900101\", \"20170303\", \"20190202\", \"19931023\")),\n  end_date   = ymd(c(\"20200112\", \"20211120\", \"20230720\", \"19891231\", \"20170102\", \"20180720\", \"20230720\", \"20230720\"))\n))\n\n#&gt;   A tibble: 8 √ó 4\n#&gt;   person_id house_id start_date end_date  \n#&gt;   &lt;fct&gt;     &lt;fct&gt;    &lt;date&gt;     &lt;date&gt;    \n#&gt; 1 A10232    H1200E   2020-01-01 2020-01-12\n#&gt; 2 A10232    H1243D   2020-01-12 2021-11-20\n#&gt; 3 A10232    H3432B   2021-11-20 2023-07-20\n#&gt; 4 A39211    HA7382   1980-01-01 1989-12-31\n#&gt; 5 A39211    H53621   1990-01-01 2017-01-02\n#&gt; 6 A28183    HC39EF   2017-03-03 2018-07-20\n#&gt; 7 A28183    HA3A01   2019-02-02 2023-07-20\n#&gt; 8 A10124    H222BA   1993-10-23 2023-07-20\nCreate interval column\nhouse_df &lt;- \n  house_df |&gt; \n  mutate(\n    # create the interval\n    int = interval(start_date, end_date), \n    # drop the start/end columns\n    .keep = \"unused\"                      \n  )\n\nhouse_df\n#&gt;   A tibble: 8 √ó 3\n#&gt;   person_id house_id int                           \n#&gt;   &lt;fct&gt;     &lt;fct&gt;    &lt;Interval&gt;                    \n#&gt; 1 A10232    H1200E   2020-01-01 UTC--2020-01-12 UTC\n#&gt; 2 A10232    H1243D   2020-01-12 UTC--2021-11-20 UTC\n#&gt; 3 A10232    H3432B   2021-11-20 UTC--2023-07-20 UTC\n#&gt; 4 A39211    HA7382   1980-01-01 UTC--1989-12-31 UTC\n#&gt; 5 A39211    H53621   1990-01-01 UTC--2017-01-02 UTC\n#&gt; 6 A28183    HC39EF   2017-03-03 UTC--2018-07-20 UTC\n#&gt; 7 A28183    HA3A01   2019-02-02 UTC--2023-07-20 UTC\n#&gt; 8 A10124    H222BA   1993-10-23 UTC--2023-07-20 UTC\nIntersection Function\n\nint_intersect &lt;- function(int, int_limits) {\n  int_start(int) &lt;- pmax(int_start(int), int_start(int_limits))\n  int_end(int)   &lt;- pmin(int_end(int), int_end(int_limits))\n  return(int)\n}\n\nThe red dashed line is the reference interval and the blue solid line is the interval of interest\nThe function creates an interval thats the intersection of both intervals (segment between black parentheses)\n\nProportion of the Reference Interval\n\nint_proportion &lt;- function(dat, reference_interval) {\n\n  # start with the housing data\n  dat |&gt; \n    # only retain overlapping rows, this makes the following\n    # operations more efficient by only computing what we need\n    filter(int_overlaps(int, reference_interval)) |&gt; \n    # then, actually compute the overlap of the intervals\n    mutate(\n      # use our earlier truncate function\n      int_sect = int_intersect(int, reference_interval),\n      # then, it's simple to compute the overlap proportion\n      prop = int_length(int_sect) / int_length(reference_interval)\n    ) |&gt; \n    # combine different intervals per person\n    summarize(prop_in_nl = sum(prop), .by = person_id)\n\n}\n\nExample\nint_2017  &lt;- interval(ymd(\"20170101\"), ymd(\"20171231\"))\nprop_2017 &lt;- \n  int_proportion(dat = house_df, \n                 reference_interval = int_2017)\n\nprop_2017\n\n#&gt; # A tibble: 3 √ó 2\n#&gt;   person_id prop_in_nl\n#&gt;   &lt;fct&gt;          &lt;dbl&gt;\n#&gt; 1 A39211       0.00275\n#&gt; 2 A28183       0.832  \n#&gt; 3 A10124       1",
    "crumbs": [
      "Code",
      "Snippets"
    ]
  },
  {
    "objectID": "qmd/code-snippets.html#sec-code-snippits-paral",
    "href": "qmd/code-snippets.html#sec-code-snippits-paral",
    "title": "Snippets",
    "section": "Parallelization",
    "text": "Parallelization\n\nMaking a cluster out of SSH connected machines (Thread)\n\nBasic\npacman::p_load(parallely, future, furrr)\nnodes = c(\"host1\", \"host2\")\nplan(cluster, workers = nodes)\nfuture_map(...)\nWith {renv}\npacman::p_load(parallely, future, furrr)\nnodes = c(\"host1\", \"host2\")\nplan(cluster, workers = nodes, rscript_libs = .libPaths())\nfuture_map(...)",
    "crumbs": [
      "Code",
      "Snippets"
    ]
  },
  {
    "objectID": "qmd/bayes-troubleshooting-hmc.html",
    "href": "qmd/bayes-troubleshooting-hmc.html",
    "title": "Troubleshooting HMC",
    "section": "",
    "text": "Divergent Transitions",
    "crumbs": [
      "Bayes",
      "Troubleshooting HMC"
    ]
  },
  {
    "objectID": "qmd/bayes-troubleshooting-hmc.html#sec-trblhmc-divtrans",
    "href": "qmd/bayes-troubleshooting-hmc.html#sec-trblhmc-divtrans",
    "title": "Troubleshooting HMC",
    "section": "Divergent Transitions",
    "text": "Divergent Transitions\n\n‚Äúdivergent transitions after warmup‚Äù\nSee Taming Divergences in Stan Models and Identifying non-identifiability\nSee Statistical Rethinking &gt;&gt; Ch.9 MCMC &gt;&gt; Issues\nDivergent Transition - A rejected proposed parameter value in the posterior during the sampling process\n\nToo many DTs could indicate a poor exploration of the posterior by the sampling algorithm and possibly biased estimates.\n\nIf the DTs are happening in the same region of the posterior then that region isn‚Äôt being sampled by the HMC algorithm\n\nIf there are ‚Äústeep‚Äù areas in the posterior, these areas can break the sampling process resulting in a ‚Äúbad‚Äù proposed parameter value.\n\nSolutions\n\nadjust priors from flat to weakly informative\nNeed more data\nIncrease adapt_delta closer to 1 (default: 0.8)\nReparameterize the model",
    "crumbs": [
      "Bayes",
      "Troubleshooting HMC"
    ]
  },
  {
    "objectID": "qmd/bayes-troubleshooting-hmc.html#sec-trblhmc-chnmix",
    "href": "qmd/bayes-troubleshooting-hmc.html#sec-trblhmc-chnmix",
    "title": "Troubleshooting HMC",
    "section": "Chains Not Mixing",
    "text": "Chains Not Mixing\n\nMisc\n\nNotes from When MCMC fails: The advice we‚Äôre giving is wrong. Here‚Äôs what we you should be doing instead. (Hint: it‚Äôs all about the folk theorem.)\n\nPotential issues\n\nPriors on some parameters are weak or nonexistent or the data are too weak to identify all the parameters in the model.\n\nSigns: Chains are exploring extreme regions of the parameter space. Check out the y-axis range in trace and see how high or low the values are.\nExamples: elasticity parameters of -20 or people with 8 kg livers\n\nCoding mistake\n\nStan examples:\n\nYou can forget to use a log link or set a prior using variance instead sd\nArray indices and for loops don‚Äôt match\n\n\nMinor modes in the tails of the posterior distribution\n\nYour posterior is multimodal and all but one of the modes have near-zero mass\nSigns: different chains will cluster in different places\nSolutions:\n\nUsing starting values near the main mode (brms init arg; see [Statistical Rethinking &gt;&gt; Ch 4]((https://ercbk.github.io/Statistical-Rethinking-Notebook/qmd/chapter-4.html){style=‚Äúcolor: green‚Äù}\n\nCan also be used in general cases where you‚Äôre getting bad mixing from you chains\n\ne.g.¬†divergent transitions, large numbers of transitions, high R-hat values, and/or very low effective sample size estimates\n\nDiagnostic example\n\nCheck the intercept warm-up (For real-world models, it‚Äôs good to look at the trace plots for all major model parameters)\n\ngeom_trace &lt;- function(subtitle = NULL,¬†\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† xlab = \"iteration\",¬†\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† xbreaks = 0:4 * 500) {\nlist(\nannotate(geom = \"rect\",¬†\n¬† ¬† ¬† ¬† xmin = 0, xmax = 1000, ymin = -Inf, ymax = Inf,\n¬† ¬† ¬† ¬† fill = fk[16], alpha = 1/2, size = 0),\ngeom_line(size = 1/3),\nscale_color_manual(values = fk[c(3, 8, 27, 31)]),\nscale_x_continuous(xlab, breaks = xbreaks, expand = c(0, 0)),\nlabs(subtitle = subtitle),\ntheme(panel.grid = element_blank())\n)¬† ¬†\n}\np1 &lt;-\nggmcmc::ggs(fit1, burnin = TRUE) %&gt;%\nfilter(Parameter == \"b_Intercept\") %&gt;%¬†\nmutate(chain = factor(Chain),\n¬† ¬† intercept = value) %&gt;%¬† ¬†\nggplot(aes(x = Iteration, y = intercept, color = chain)) +\ngeom_trace(subtitle = \"fit1 (default settings)\") +\nscale_y_continuous(breaks = c(0, 650, 1300), limits = c(NA, 1430))¬†\np1\n\n\n\n\n\nOne of our chains eventually made its way to the posterior, three out of the four stagnated near their starting values (lines near zero).\nSet starting values manually. Same values to all 4 chains.\ninits &lt;- list(\nIntercept = 1300,\nsigma¬† ¬† = 150,\nbeta¬† ¬† ¬† = 520\n)\nlist_of_inits &lt;- list(inits, inits, inits, inits)\nfit2 &lt;- brm(\ndata = dat,\nfamily = exgaussian(),\nformula = rt ~ 1 + (1 | id),\ncores = 4, seed = 1,\ninits = list_of_inits\n)\n\nMuch mo‚Äô better, but not evidence of chain convergence since all started at the same value\nNo practical for large models with many parameters\nSet starting values (somewhat) randomly by function\nset_inits &lt;- function(seed = 1) {¬† ¬†\nset.seed(seed)\nlist(\n# posterior for the intercept often looks gaussian\nIntercept = rnorm(n = 1, mean = 1300, sd = 100),\n# posteriors for sigma and beta need to nonnegative (alt:rgamma)\nsigma¬† ¬† = runif(n = 1, min = 100, max = 200),\nbeta¬† ¬† ¬† = runif(n = 1, min = 450, max = 550)\n)¬† ¬†\n}\n\nlist_of_inits &lt;- list(\n# different seed values will return different results\nset_inits(seed = 1),\nset_inits(seed = 2),\nset_inits(seed = 3),\nset_inits(seed = 4)\n)\n\n\nChains are mixing and evidence of convergence since we started at different starting values\nNeed to also check sigma and beta\n\nFitting multiple models and averaging predictions (stacking). Don‚Äôt fully understand but this is from a section 5.6 of Gelman‚Äôs Bayesian Workflow paper\n\n‚Äúdivide the model into pieces by introducing a strong mixture prior and then fitting the model separately given each of the components of the prior. Other times the problem can be addressed using strong priors that have the effect of ruling out some of the possible modes‚Äù\n\nAlso ‚ÄúSoon we should have Pathfinder implemented and this will get rid of lots of these minor modes automatically‚Äù\nThe model can be reparameterized\n\nThink this had to do with Divergent Transitions\nSigns: You have coefficients like 0.000002\nSolutions:\n\nUse variables that have been log transformed or scaled (e.g.¬†per million). For some reason, itt‚Äôs difficult for the sampler when parameters values are on vastly different scales.\nReparameterize to ‚Äúunit scale‚Äù\n\nI think scale in ‚Äúunit scale‚Äù refers to scale as a distribution parameter like sd is the scale parameter in a Normal distribution, and ‚Äúunit scale‚Äù is scale = 1 (e.g.¬†sd = 1 in standardization). But there‚Äôs more to this, and I haven‚Äôt read S.R. ch 13 yet\n\n\n\nCommon (misguided?) solutions\n\nIncrease iterations\nTweak adapt_delta and max_treedepth parameters to make it explore the space more carefully\n\nOther\n\nSequential Monte Carlo (SMC) is a potential solution multimodal posterior problem (Stan‚Äôs NUTS sampler may already do this to some extent. See Ch. 9 Statistical Rethinking)\n\nBayesTools PKG implements a SMC sampler\nAlgorithm details - https://docs.pymc.io/notebooks/SMC2_gaussians.html?highlight=smc",
    "crumbs": [
      "Bayes",
      "Troubleshooting HMC"
    ]
  },
  {
    "objectID": "qmd/bayes-troubleshooting-hmc.html#sec-trblhmc-reparam",
    "href": "qmd/bayes-troubleshooting-hmc.html#sec-trblhmc-reparam",
    "title": "Troubleshooting HMC",
    "section": "Reparameterization",
    "text": "Reparameterization\n\n\nNote: ‚ÄúRT‚Äôs‚Äù in the image is probably refering to a response variable (e.g.¬†Response Time, Reaction Time, etc.)\nMisc\n\nResources\n\nSee Stan User Guide\nSee Ch 13.4 in Statistical Rethinking\n\nNotes from Thread\nIn the paper, Strategies for fitting nonlinear ecological models in R, AD Model Builder, and BUGS, it suggests scaling; eliminating correlation; making contours elliptical.(?)\n\nGamma from {shape,scale} to {log-mean, log-shape} is often good.\n\n\nParameters with distributions such as cauchy, student-t, normal, or any distribution in the location-scale family are good reparameterization candidates\nNeal‚Äôs Funnel\n\nTechnique for efficiently sampling random effects or latent variables in hierarchical Bayesian models.\n\nExample: McElreath\n\\[\nx \\sim \\mathbb{exponential}(\\lambda) \\\\\n\\text{-same as-} \\\\\nz \\sim \\mathbb{exponential(1)} \\\\\nx = \\frac{z}{\\lambda}\n\\]\n\nFactoring out scale parameters",
    "crumbs": [
      "Bayes",
      "Troubleshooting HMC"
    ]
  },
  {
    "objectID": "qmd/bayes-troubleshooting-hmc.html#sec-bayes-trblhmc-divtrans",
    "href": "qmd/bayes-troubleshooting-hmc.html#sec-bayes-trblhmc-divtrans",
    "title": "Troubleshooting HMC",
    "section": "",
    "text": "‚Äúdivergent transitions after warmup‚Äù\nSee Taming Divergences in Stan Models and Identifying non-identifiability\nSee Statistical Rethinking &gt;&gt; Ch.9 MCMC &gt;&gt; Issues\nDivergent Transition - A rejected proposed parameter value in the posterior during the sampling process\n\nToo many DTs could indicate a poor exploration of the posterior by the sampling algorithm and possibly biased estimates.\n\nIf the DTs are happening in the same region of the posterior then that region isn‚Äôt being sampled by the HMC algorithm\n\nIf there are ‚Äústeep‚Äù areas in the posterior, these areas can break the sampling process resulting in a ‚Äúbad‚Äù proposed parameter value.\n\nSolutions\n\nAdjust priors from flat to weakly informative\nNeed more data\nIncrease adapt_delta closer to 1 (default: 0.8)\nReparameterize the model",
    "crumbs": [
      "Bayes",
      "Troubleshooting HMC"
    ]
  },
  {
    "objectID": "qmd/bayes-troubleshooting-hmc.html#sec-bayes-trblhmc-chnmix",
    "href": "qmd/bayes-troubleshooting-hmc.html#sec-bayes-trblhmc-chnmix",
    "title": "Troubleshooting HMC",
    "section": "Chains Not Mixing",
    "text": "Chains Not Mixing\n\nMisc\n\nNotes from When MCMC fails: The advice we‚Äôre giving is wrong. Here‚Äôs what we you should be doing instead. (Hint: it‚Äôs all about the folk theorem.)\n\n\n\nMinor Issues\n\nPriors on some parameters are weak or nonexistent or the data are too weak to identify all the parameters in the model.\n\nSigns: Chains are exploring extreme regions of the parameter space. Check out the y-axis range in trace and see how high or low the values are.\nExamples: elasticity parameters of -20 or people with 8 kg livers\n\nCoding mistake\n\nStan examples:\n\nYou can forget to use a log link or set a prior using variance instead sd\nArray indices and for loops don‚Äôt match\n\n\n\n\n\nMulti-Modal Posterior\n\nMinor modes in the tails of the posterior distribution\nYour posterior is multimodal and all but one of the modes have near-zero mass\nWhen the posterior distribution is multimodal, it means that there are multiple distinct regions of high probability density, separated by regions of low probability density. This can lead to poor mixing of the MCMC chains, as the chains may get stuck in one of the modes and have difficulty exploring the other modes.\nSigns: different chains will cluster in different places\n\n\nSolutions\n\nPathfinder\n\nPaper, Github\nGets rid of lots of these minor modes automatically\nA Variational Inference (VI) method that locates approximations to the target density along a quasi-Newton optimization path. Starting from a random initialization in the tail of the posterior distribution, the quasi-Newton optimization trajectory can quickly move from the tail, through the body of the distribution, to a mode or pole.\n\nVariational inference searches for a tractable approximate distribution that minimizes Kullback-Leibler (KL) divergence to the posterior and is typically faster than Monte Carlo sampling.\n\n\nUse starting values near the main mode\n\n{brms} init argument (See Statistical Rethinking &gt;&gt; Ch 4)\nCan also be used in general cases where you‚Äôre getting bad mixing from you chains\n\ne.g.¬†divergent transitions, large numbers of transitions, high R-hat values, and/or very low effective sample size estimates\n\nExample:\n\nDiagnose Issue: Check the intercept warm-up (For real-world models, it‚Äôs good to look at the trace plots for all major model parameters)\n\ngeom_trace &lt;- \n  function(subtitle = NULL,¬†\n¬† ¬† ¬† ¬† ¬† ¬†xlab = \"iteration\",¬†\n¬† ¬† ¬† ¬† ¬† ¬†xbreaks = 0:4 * 500) {\n    list(\n      annotate(geom = \"rect\",¬†\n      ¬† ¬† ¬† ¬†  xmin = 0, \n               xmax = 1000, \n               ymin = -Inf, \n               ymax = Inf,\n      ¬† ¬† ¬† ¬†  fill = fk[16], \n               alpha = 1/2, \n               size = 0),\n      geom_line(size = 1/3),\n      scale_color_manual(values = fk[c(3, 8, 27, 31)]),\n      scale_x_continuous(xlab, \n                         breaks = xbreaks, \n                         expand = c(0, 0)),\n      labs(subtitle = subtitle),\n      theme(panel.grid = element_blank())\n    )¬† ¬†\n}\np1 &lt;-\n  ggmcmc::ggs(fit1, burnin = TRUE) %&gt;%\n  filter(Parameter == \"b_Intercept\") %&gt;%¬†\n  mutate(chain = factor(Chain),\n¬† ¬†      intercept = value) %&gt;%¬† ¬†\n  ggplot(aes(x = Iteration, \n             y = intercept, \n             color = chain)) +\n  geom_trace(subtitle = \"fit1 (default settings)\") +\n  scale_y_continuous(breaks = c(0, 650, 1300), \n                     limits = c(NA, 1430))¬†\np1\n\nOne of our chains eventually made its way to the posterior, three out of the four stagnated near their starting values (lines near zero).\n\nSet starting values manually. Same values to all 4 chains\n\ninits &lt;- list(\n  Intercept = 1300,\n  sigma¬† ¬† = 150,\n  beta¬† ¬† ¬† = 520\n)\nlist_of_inits &lt;- list(inits, inits, inits, inits)\nfit2 &lt;- brm(\n  data = dat,\n  family = exgaussian(),\n  formula = rt ~ 1 + (1 | id),\n  cores = 4, seed = 1,\n  inits = list_of_inits\n)\n\nMuch mo‚Äô better, but not evidence of chain convergence since all started at the same value\nNo practical for large models with many parameters\n\nSet starting values (somewhat) randomly by function\n\nset_inits &lt;- function(seed = 1) {\n  set.seed(seed)\n  list(\n    # posterior for the intercept often looks gaussian\n    Intercept = rnorm(n = 1, mean = 1300, sd = 100),\n    # posteriors for sigma and beta need to nonnegative (alt:rgamma)\n    sigma¬† ¬† = runif(n = 1, min = 100, max = 200),\n    beta¬† ¬† ¬† = runif(n = 1, min = 450, max = 550)\n  )\n}\n\nlist_of_inits &lt;- list(\n  # different seed values will return different results\n  set_inits(seed = 1),\n  set_inits(seed = 2),\n  set_inits(seed = 3),\n  set_inits(seed = 4)\n)\n\nChains are mixing and evidence of convergence since we started at different starting values\nNeed to also check sigma and beta\n\n\n\nFit multiple models and average predictions (stacking). Don‚Äôt fully understand but this is from a section 5.6 of Gelman‚Äôs Bayesian Workflow paper\n\n‚ÄúDivide the model into pieces by introducing a strong mixture prior and then fitting the model separately given each of the components of the prior. Other times the problem can be addressed using strong priors that have the effect of ruling out some of the possible modes‚Äù\n\nReparameterize the Model\n\nThink this had to do with Divergent Transitions\nSigns: You have coefficients like 0.000002\nSolutions:\n\nUse variables that have been log transformed or scaled (e.g.¬†per million). For some reason, itt‚Äôs difficult for the sampler when parameters values are on vastly different scales.\nReparameterize to ‚Äúunit scale‚Äù\n\nI think scale in ‚Äúunit scale‚Äù refers to scale as a distribution parameter like sd is the scale parameter in a Normal distribution, and ‚Äúunit scale‚Äù is scale = 1 (e.g.¬†sd = 1 in standardization). But there‚Äôs more to this, and I haven‚Äôt read S.R. ch 13 yet\n\n\n\nCommon (misguided?) Solutions\n\nIncrease iterations\nTweak adapt_delta and max_treedepth parameters to make it explore the space more carefully\n\nOther\n\nSequential Monte Carlo (SMC) is a potential solution multimodal posterior problem (Stan‚Äôs NUTS sampler may already do this to some extent. See Ch. 9 Statistical Rethinking)\n\nAlgorithm details - https://docs.pymc.io/notebooks/SMC2_gaussians.html?highlight=smc",
    "crumbs": [
      "Bayes",
      "Troubleshooting HMC"
    ]
  },
  {
    "objectID": "qmd/bayes-troubleshooting-hmc.html#sec-bayes-trblhmc-reparam",
    "href": "qmd/bayes-troubleshooting-hmc.html#sec-bayes-trblhmc-reparam",
    "title": "Troubleshooting HMC",
    "section": "Reparameterization",
    "text": "Reparameterization\n\n\nNote: ‚ÄúRT‚Äôs‚Äù in the image is probably refering to a response variable (e.g.¬†Response Time, Reaction Time, etc.)\nUsed to help solve HMC issues\nMisc\n\nResources\n\nSee Stan User Guide\nSee Ch 13.4 in Statistical Rethinking\n\nAlso see {makemypriors} in Bayes, Priors &gt;&gt; Misc &gt;&gt; Packages - Recommended in the Thread below as method for handling the same issues that reparameterization solves.\nNotes from Thread\nIn the paper, Strategies for fitting nonlinear ecological models in R, AD Model Builder, and BUGS, it suggests scaling; eliminating correlation; making contours elliptical.(?)\n\nGamma from {shape,scale} to {log-mean, log-shape} is often good.\n\n\nParameters with distributions such as cauchy, student-t, normal, or any distribution in the location-scale family are good reparameterization candidates\nNeal‚Äôs Funnel\n\nTechnique for efficiently sampling random effects or latent variables in hierarchical Bayesian models.\nZ-Scores Random Effects\n\nExample: McElreath\n\\[\n\\begin{align}\n&x \\sim \\text{Exponential}(\\lambda) \\\\\n&\\quad\\quad\\text{-same as-} \\\\\n&z \\sim \\text{Exponential(1)} \\\\\n&x = \\frac{z}{\\lambda}\n\\end{align}\n\\]\n\nFactoring out scale parameters",
    "crumbs": [
      "Bayes",
      "Troubleshooting HMC"
    ]
  },
  {
    "objectID": "qmd/bayes-priors.html",
    "href": "qmd/bayes-priors.html",
    "title": "Priors",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Bayes",
      "Priors"
    ]
  },
  {
    "objectID": "qmd/bayes-priors.html#sec-bayes-priors-misc",
    "href": "qmd/bayes-priors.html#sec-bayes-priors-misc",
    "title": "Priors",
    "section": "",
    "text": "Resources\n\nStan/brms prior distributions (mc-stan/function-ref)\n\nDistributions are towards the bottom of the guide\n{brms} should have all the distributions available in Stan according to their docs (see Details section)\n\n\nPackages\n\n{makemyprior} (paper)- Intuitive construction of Joint priors for variance parameters.\n\nGUI can be used to choose the joint prior, where the user can click through the model and select priors.\nUsing a hierarchical variance decomposition, a joint variance prior is formulated that takes the whole model structure into account. In this way, existing knowledge can intuitively be incorporated at the level it applies to.\n\nAlternatively, independent variance priors can be used for each model component in the latent Gaussian model.\n\n\n\nUsing meta-analyis or previous studies to create informed priors\n\n‚ÄúSystematic use of informed studies leads to more precise, but more biased estimates (due to non-linear information flow in the literature). Critical comparison of informed and skeptical priors can provide more nuanced and solid understanding of our findings.‚Äù (thread + paper)\n\ni.e.¬†try both and compare the result\n\n\nPrior sensitivity analysis\n\n{priorsense}\n\nVideo, Thread\n\n{BayesSenMC}\n\nFor binary exposure and a dichotomous outcome\nGenerates different posterior distributions of adjusted odds ratio under different priors of sensitivity and specificity, and plots the models for comparison. It also provides estimations for the specifications of the models using diagnostics of exposure status with a non-linear mixed effects model.\nVignette\n\n\nStatistical Rethinking\n\nThe ‚Äúflatness‚Äù of a Normal prior is controlled by the size of the s.d. value\n\nnot in logistic regression (see examples below)\n\nFlat priors result in poor frequency properties (i.e.¬†consistently give bad inferences) in realistic settings where studies are noisy and effect sizes are small. (Gelman post)\nWeakly informative priors: they allow some implausibly strong relationships but generally bound the lines to possible ranges of the variables. (fig 5.3, pg 131)\nWe want our priors to be skeptical of large differences [in treatment effects], so that we reduce overfitting. Good priors hurt fit to sample but are expected to improve prediction. (pg 337)\nWe don‚Äôt formulate priors based on the sample data. We want the prior predictive distribution to live in the plausible outcome space, not fit the sample.\nFor logistic regression and poisson regression, a flat prior in the logit space is not a flat prior in the outcome probability space (pg 336)\nAs long as the priors are vague, minimizing the sum of squared deviations to the regression line is equivalent to finding the posterior mean. pg 200\n‚ÄúAs always in rescaling variables, the goals are to create focal points that you might have prior information about, prior to seeing the actual data values. That way we can assign priors that are not obviously crazy, and in thinking about those priors, we might realize that the model makes no sense. But this is only possible if we think about the relationship between measurements and parameters, and the exercise of rescaling and assigning sensible priors helps us along that path. Even when there are enough data that choice of priors is not crucial, this thought exercise is useful.‚Äù pg 258\nComparing the posteriors with the priors",
    "crumbs": [
      "Bayes",
      "Priors"
    ]
  },
  {
    "objectID": "qmd/bayes-priors.html#sec-bayes-priors-preproc",
    "href": "qmd/bayes-priors.html#sec-bayes-priors-preproc",
    "title": "Priors",
    "section": "Preprocessing",
    "text": "Preprocessing\n\nCentering the predictor\n\nMakes the posterior easier to sample\nReduces covariance among the parameter posterior distributions\nMakes it easier to define the prior on average temperature in the center of the time range (instead defining prior for temperature at year 0).\nLinks to Gelman posts about centering your predictors (article)\n\nIf you standardize your predictors, you can use a mean of 0 for the prior on your intercept\n\nWith flat priors, this doesn‚Äôt make much of difference",
    "crumbs": [
      "Bayes",
      "Priors"
    ]
  },
  {
    "objectID": "qmd/bayes-priors.html#sec-bayes-priors-getprecs",
    "href": "qmd/bayes-priors.html#sec-bayes-priors-getprecs",
    "title": "Priors",
    "section": "Get Prior Recommendations",
    "text": "Get Prior Recommendations\n\nExample: Fitting a spline\n# get recommended prior specifications\n# s is the basis function brms imports from mgcv pkg\nbrms::get_prior(data = d2,¬†\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† family = gaussian,¬†\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† doy ~ 1 + s(year))\n##¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† prior¬† ¬† class¬† ¬† coef group resp dpar nlpar bound¬† ¬† ¬† source¬†\n##¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† (flat)¬† ¬† ¬† ¬† b¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† default¬†\n##¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† (flat)¬† ¬† ¬† ¬† b syear_1¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† (vectorized)¬†\n##¬† student_t(3, 105, 5.9) Intercept¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† default¬†\n##¬† ¬† student_t(3, 0, 5.9)¬† ¬† ¬† sds¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† default¬†\n##¬† ¬† student_t(3, 0, 5.9)¬† ¬† ¬† sds s(year)¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† (vectorized)¬†\n##¬† ¬† student_t(3, 0, 5.9)¬† ¬† sigma¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† default\n\n# applying the recommendations\n# multi-level method for spline fitting\nb4.11 &lt;- brm(data = d2,¬†\n¬† ¬† ¬† ¬† ¬† ¬† family = gaussian,¬†\n¬† ¬† ¬† ¬† ¬† ¬† # k = 19, corresponds to 17 basis functions I guess ::shrugs::¬†\n¬† ¬† ¬† ¬† ¬† ¬† # The default for s() is to use what‚Äôs called a thin plate regression spline¬†\n¬† ¬† ¬† ¬† ¬† ¬† # bs uses a basis spline¬†\n¬† ¬† ¬† ¬† ¬† ¬† temp ~ 1 + s(year, bs = \"bs\", k = 19),¬†\n¬† ¬† ¬† ¬† ¬† ¬† prior = c(prior(normal(100, 10), class = Intercept),¬†\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† prior(normal(0, 10), class = b),¬†\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† prior(student_t(3, 0, 5.9), class = sds),¬†\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† prior(exponential(1), class = sigma)),¬†\n¬† ¬† ¬† ¬† ¬† ¬† iter = 2000, warmup = 1000, chains = 4, cores = 4,¬†\n¬† ¬† ¬† ¬† ¬† ¬† seed = 4,¬†\n¬† ¬† ¬† ¬† ¬† ¬† control = list(adapt_delta = .99))\nExample: Multinomial Logistic Regression\n# Outcome categorical variable has k = 3 levels. We fit k-1 models. Hence the 2 intercept priors\n# intercept model\nget_prior(data = d,¬†\n¬† ¬† ¬† ¬† ¬† # refcat sets the reference category to the 3rd level\n¬† ¬† ¬† ¬† ¬† family = categorical(link = logit, refcat = 3),\n¬† ¬† ¬† ¬† ¬† # just an intercept model\n¬† ¬† ¬† ¬† ¬† career ~ 1)\n##¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† prior¬† ¬† class coef group resp dpar nlpar bound¬† source\n##¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† (flat) Intercept¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† default\n##¬† student_t(3, 3, 2.5) Intercept¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† mu1¬† ¬† ¬† ¬† ¬† ¬† default\n##¬† student_t(3, 3, 2.5) Intercept¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† mu2¬† ¬† ¬† ¬† ¬† ¬† default\n\nb11.13io &lt;-\n¬† brm(data = d,¬†\n¬† ¬† ¬† family = categorical(link = logit, refcat = 3),\n¬† ¬† ¬† career ~ 1,\n¬† ¬† ¬† prior = c(prior(normal(0, 1), class = Intercept, dpar = mu1),\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† prior(normal(0, 1), class = Intercept, dpar = mu2)),\n¬† ¬† ¬† iter = 2000, warmup = 1000, cores = 4, chains = 4,\n¬† ¬† ¬† seed = 11,\n¬† ¬† ¬† file = \"fits/b11.13io\")\n\nAs of brms 2.12.0, ‚Äúspecifying global priors for regression coefficients in categorical models is deprecated.‚Äù Meaning ‚Äî if we want to use the same prior for both, we need to use the dpar argument for each",
    "crumbs": [
      "Bayes",
      "Priors"
    ]
  },
  {
    "objectID": "qmd/bayes-priors.html#sec-bayes-priors-finterp",
    "href": "qmd/bayes-priors.html#sec-bayes-priors-finterp",
    "title": "Priors",
    "section": "Formulating an Intercept Prior",
    "text": "Formulating an Intercept Prior\n\nExample SR 6.3.1 pgs 182-83\n\nA thought process on how to set a predictor prior based on its relationship to the outcome and an intercept prior.\n\nExample SR 7.1.1 pg 200\n\nOutcome variable was scaled, outcome/max(outcome)\n\nValues now between 0 and 1\nUseful for when 0 is a meaningful boundary\n\nNow able to center the intercept prior on mean of outcome, Œ± ‚àº Normal(0.5, 1)\n\nSays that the average species with an average body mass (predictor variable) has a brain volume (outcome variable) with an 89% credible interval (¬± 1.5 sd) from about ‚àí1 to 2.\n\nBody mass was centered, so it‚Äôs at its average is when its value is zero.\n\n\n\nExample SR 8.3.2 pg 259\n\nSimilar to 7.1.1 example except there‚Äôs the observation that a sd = 1 for the intercept prior is too large given that the outcome is bdd between 0 and 1 (after scaling)\na &lt;- rnorm( 1e4 , 0.5 , 1 )\nsum( a &lt; 0 | a &gt; 1 ) / length( a )\n[1] 0.6126\n\n61% of the prior is outside the bounds for the outcome which makes no sense\n\nIf it‚Äôs 0.5 units from the mean to zero, then a standard deviation of 0.25 should put only 5% of the mass outside the valid range.\na &lt;- rnorm( 1e4 , 0.5 , 0.25 )\nsum( a &lt; 0 | a &gt; 1 ) / length( a )\n[1] 0.0486\n\nNot sure why you want 5% outside the valid range of the outcome variable\n\n\nExample (Ch 11 pg 335-6)\n\nWith logistic regression, flat Normal priors aren‚Äôt priors with a high sd.\n\n\nThe Normal prior on the logit scale with the large sd says that the probabilty of an event is either 0 or 1 which usually isn‚Äôt reasonable.\n\nlogit(pi) = Œ±\n\nŒ± ~ Normal(0, 1.5) ‚Äî the curve for the probability of an event is very flat, looks like a mesa\nŒ± ~ Normal(0, 1.0) ‚Äîthe curve for the probability of an event is a fat hill shape. A little more skeptical of extreme probabilities\n\n\nExample\n\nAlso have ggplot code in Documents &gt;&gt; R &gt;&gt; Code &gt;&gt; Simulations &gt;&gt; sim-prior-predictive-distr.R\nPoisson regression (pg 356)\nIn poisson regression, flat normal priors aren‚Äôt priors with high s.d.\n\n# prior predictive distribution\ncurve( dlnorm( x , 3 , 0.5 ) , from=0 , to=100 , n=200 )\n\nSince poisson regression uses a log link, the outcome is log-normal. We‚Äôre simulating the effect of a normal prior on a log-normal outcome which is why the simulation code uses dlnorm.\n‚Äúnumber of tools‚Äù is the outcome variable\nThe prior with s.d. 10 has almost all the probability density at zero and huge mean\na &lt;- rnorm(1e4,0,10)\nlambda &lt;- exp(a)\nmean(lambda)\n[1] 9.622994e+12\nThis usually doesn‚Äôt make sense for a prior\nThe prior with s.d. 0.5 has a mean around 20 and a more spread out probability density which makes much more sense given the literature on the subject.",
    "crumbs": [
      "Bayes",
      "Priors"
    ]
  },
  {
    "objectID": "qmd/bayes-priors.html#sec-bayes-priors-fslopp",
    "href": "qmd/bayes-priors.html#sec-bayes-priors-fslopp",
    "title": "Priors",
    "section": "Formulating a Slope Prior",
    "text": "Formulating a Slope Prior\n\nExample SR pg 259\n\nslopes centered on zero, imply no prior information about direction\nHow big could can the slopes be in theory?\n\nAfter centering, range of each predictor is 2‚Äîfrom ‚àí1 to 1 is 2 units.\nTo take us from the theoretical minimum of outcome variable = 0 on one end to the observed maximum of 1‚Äîa range of 1 unit‚Äîon the other would require a slope of 0.5 from either predictor variable‚Äî0.5 √ó 2 = 1.\n\nAssign a standard deviation of 0.25, then 95% of the prior slopes are from ‚àí0.5 to 0.5, so either predictor could in principle account for the entire range, but it would be unlikely\n\nExample SR pg 336-7\n\nWith logistic regression, flat Normal priors aren‚Äôt priors with a high sd.\n\n\nShows difference between two levels of the treatment effect (i.e.¬†2 different treatments) on the 0/1 outcome\nThe prior with large sd has all the probability density massed at 0 and 1\n\nSays that the 2 treatments are completely alike or completely different\n\nThe prior with the small sd (e.g.¬†Normal(0, 0.5) is concentrated from about 0 to about 0.4\n\nAlthough 0 difference in treatments has the highest probability, the mean is at a difference around 0.10\n\nPrior says that large differences between treatments are very unlikely, but if the data contains strong evidence of large differences, they will shine through\n\nPairs nicely with an intercept prior, Œ± ~ Normal(0, 1.5)\nAn example of a weakly informative prior that reduces overfitting the sample data\n\n\n\nExample: pg 357\n\nset.seed(11)\n## TOP ROW\n\n# how many lines would you like?\nn &lt;- 100\n# simulate and wrangle\ntibble(i = 1:n,\n       a = rnorm(n, mean = 3, sd = 0.5)) %&gt;%\n  mutate(`beta%~%Normal(0*', '*10)` = rnorm(n, mean = 0 , sd = 10),\n         `beta%~%Normal(0*', '*0.2)` = rnorm(n, mean = 0 , sd = 0.2)) %&gt;%\n  pivot_longer(contains(\"beta\"),\n               values_to = \"b\",\n               names_to = \"prior\") %&gt;%\n  expand(nesting(i, a, b, prior),\n         x = seq(from = -2, to = 2, length.out = 100)) %&gt;%\n\n  # plot\n  ggplot(aes(x = x, y = exp(a + b * x), group = i)) +\n  geom_line(size = 1/4, alpha = 2/3,\n            color = wes_palette(\"Moonrise2\")[4]) +\n  labs(x = \"log population (std)\",\n       y = \"total tools\") +\n  coord_cartesian(ylim = c(0, 100)) +\n  facet_wrap(~ prior, labeller = label_parsed)\n\n## BOTTOM ROW\nprior &lt;-\n  tibble(i = 1:n,\n         a = rnorm(n, mean = 3, sd = 0.5),\n         b = rnorm(n, mean = 0, sd = 0.2)) %&gt;%¬†\n  expand(nesting(i, a, b),\n         x = seq(from = log(100), to = log(200000), length.out = 100))\n# left\np1 &lt;-\n  prior %&gt;%\n  ggplot(aes(x = x, y = exp(a + b * x), group = i)) +\n  geom_line(size = 1/4, alpha = 2/3,\n            color = wes_palette(\"Moonrise2\")[4]) +\n  labs(subtitle = expression(beta%~%Normal(0*', '*0.2)),\n       x = \"log population\",\n       y = \"total tools\") +\n  coord_cartesian(xlim = c(log(100), log(200000)),\n                  ylim = c(0, 500))\n# right\np2 &lt;-\n  prior %&gt;%\n  ggplot(aes(x = exp(x), y = exp(a + b * x), group = i)) +\n  geom_line(size = 1/4, alpha = 2/3,\n            color = wes_palette(\"Moonrise2\")[4]) +\n  labs(subtitle = expression(beta%~%Normal(0*', '*0.2)),\n       x = \"population\",\n       y = \"total tools\") +\n  coord_cartesian(xlim = c(100, 200000),\n                  ylim = c(0, 500))\n# combine\np1 | p2\n\nWith poisson regression, flat Normal priors aren‚Äôt priors with a high sd.\nOutcome: total_tools, predictor: log_population\nBottom row fig titles have a typo. Should be a ~ dnorm(3, 0.5) since it‚Äôs the Intercept prior\nVariables have been standardized; total_tools simulated with intercept + predictor priors. So the y axis is simulating the potential fitted values.\nTop Left (0 is mean of log_population):\n\nlarge sd: mostly results in explosive growth of tools just after mean of log_population or explosive decline just before mean log_population (unlikely)\n\nTop Right (0 is mean of log_population)\n\nsmall sd (flatter): most results are around the mean of the intercept prior results (see above) but still allows for more extreme estimates. (reasonable)\n\nBottom Left\n\n100 trend lines between total tools and un-standardized log population\n\nViewing prior predictive trends with un-standardized variables is more natural to see what‚Äôs happening\n\n100 total tools is probably the most we expect to ever see in these data\n\nLooks like 80-85% of the trend lines are under 100. still keeps some explosive possibilities.\n\n\nBottom Right\n\n100 trend lines between total tools and un-standardized, un-logged population\n\nViewing prior predictive trends with un-standardized, un-transformed variables is even more natural to see what‚Äôs happening\n\nWhen a predictor variable is logged in a regression with a log-link (i.e.¬†log-log), this means we are assuming diminishing returns for the raw predictor variable.\n\nEach additional person contributes a smaller increase in the expected number of tools\nDiminishing returns as a predictor value continues to increase makes sense in many situations which is why logging predictors is a popular transformation\n\n\nThoughts\n\nBottom-right seems like the right way to visualize the prior to think about the association between the outcome and predictor\nTop row and bottom-left seem to give a better sense of how many explosive possibilities and their patterns that your allowing for with different transformations",
    "crumbs": [
      "Bayes",
      "Priors"
    ]
  },
  {
    "objectID": "qmd/bayes-priors.html#sec-bayes-priors-fsigp",
    "href": "qmd/bayes-priors.html#sec-bayes-priors-fsigp",
    "title": "Priors",
    "section": "Formulating a Sigma Prior",
    "text": "Formulating a Sigma Prior\n\nCommon to start with exponential(1)\nTightening the spread of the Exponential distribution by using a Gamma distribution (Thread)\n\n\nYou can keep ‚Äúmean = 1‚Äù (aka exponential(1) and adjust the ‚Äúsd‚Äù.\n\nSee Distributions &gt;&gt; Gamma for details on the process\n\nAlso allows you to move most of the mass of the prior a littler further away from 0.\nAnother alternative is the Weibull distribution",
    "crumbs": [
      "Bayes",
      "Priors"
    ]
  },
  {
    "objectID": "qmd/bayes-priors.html#sec-bayes-priors-eapfam",
    "href": "qmd/bayes-priors.html#sec-bayes-priors-eapfam",
    "title": "Priors",
    "section": "Extracting a Prior From a Model",
    "text": "Extracting a Prior From a Model\n\nExample: Logistic Regression (SR sect 11.1.1 pg 336)\n\nIntercept\n\n# prior_samples and inv_logit_scaled are brms functions\n# theme is from ggthemes\nprior_samples(b11.1) %&gt;%\n  mutate(p = inv_logit_scaled(Intercept)) %&gt;%\n\n  ggplot(aes(x = p)) +\n  geom_density(fill = wes_palette(\"Moonrise2\")[4],\n               size = 0, adjust = 0.1) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  xlab(\"prior prob pull left\")",
    "crumbs": [
      "Bayes",
      "Priors"
    ]
  },
  {
    "objectID": "qmd/bayes-priors.html#sec-bayes-priors-simppd",
    "href": "qmd/bayes-priors.html#sec-bayes-priors-simppd",
    "title": "Priors",
    "section": "Simulating a Prior Predictive Distribution",
    "text": "Simulating a Prior Predictive Distribution\n\nExample: SR pg 176\n# log-normal prior\nsim_p &lt;- rlnorm( 1e4 , 0 , 0.25 )\n\n# \"this prior expects anything from 40% shrinkage up to 50% growth\"\nrethinking::precis( data.frame(sim_p) )\n# 'data.frame': 10000 obs. of 1 variables:\n#       mean    sd  5.5% 94.5% histogram\n# sim_p 1.03  0.26  0.67  1.48 ‚ñÅ‚ñÉ‚ñá‚ñá‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n# tidy-way\nsim_p &lt;-\n  tibble(sim_p = rlnorm(1e4, meanlog = 0, sdlog = 0.25))sim_p %&gt;%\n    mutate(`exp(sim_p)` = exp(sim_p)) %&gt;%\n    gather() %&gt;%\n    group_by(key) %&gt;%\n    tidybayes::mean_qi(.width = .89) %&gt;%\n    mutate_if(is.double, round, digits = 2)\n\n## # A tibble: 2 x 7\n##  key         value .lower .upper .width .point .interval\n##  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;chr&gt;     &lt;chr&gt;\n## 1 exp(sim_p)  2.92   1.96   4.49   0.89   mean        qi\n## 2 sim_p       1.03   0.67    1.5   0.89   mean        qi\n\nVisualize with ggplot\n\n# wrangle\nsim_p %&gt;%\n  mutate(`exp(sim_p)` = exp(sim_p)) %&gt;%\n  gather() %&gt;%\n  # plot\n  ggplot(aes(x = value)) +\n  geom_density(fill = \"steelblue\") +\n  scale_x_continuous(breaks = c(0, .5, 1, 1.5, 2, 3, 5)) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  coord_cartesian(xlim = c(0, 6)) +\n  theme(panel.grid.minor.x = element_blank()) +\n  facet_wrap(~key, scale = \"free_y\", ncol = 1)\n\nExample:\n\nPossible Intercept Priors\n\ngrid &lt;- seq(-3, 3, \n             length.out = 1000) # evenly spaced values from -3 to 3\nb0_prior &lt;- \n   map_dfr(.x = c(0.5, 1, 2), # .x represents the three sigmas \n           ~ data.frame(grid = grid,\n                        b0 = dnorm(grid, mean = 0, sd = .x)),\n                        .id = \"sigma_id\")\n# Create Friendlier Labels\nb0_prior &lt;- b0_prior %&gt;%\n  mutate(sigma_id = factor(sigma_id, \n         labels = c(\"normal(0, 0.5)\",\n                    \"normal(0, 1)\",\n                    \"normal(0, 2)\")))\nggplot(b0_prior, aes(x = grid, y = b0)) +\n  geom_area(fill = \"cadetblue4\", color = \"black\", alpha = 0.90) +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0), limits = c(0, 0.85)) +\n  labs(x = NULL,\n       y = \"probability density\",\n       title = latex2exp::TeX(\"Possible $\\\\beta_0$ (intercept) priors\")) +\n       facet_wrap(~sigma_id, nrow = 3)\nSampling of lines from \\(\\beta_0\\) and \\(\\beta_1\\) priors\n\nb0b1 &lt;- \n  map2_df(.x = c(0.5, 1, 2), \n          .y = c(0.25, 0.5, 1), \n          ~ data.frame(\n              b0 = rnorm(100, mean = 0, sd = .x),\n              b1 = rnorm(100, mean = 0, sd = .y)), \n          .id = \"sigma_id\"\n  )\n\n# Create friendlier labels\nb0b1 &lt;- \n  b0b1 %&gt;%\n    mutate(sigma_id = factor(sigma_id, \n                             labels = c(\"b0 ~ normal(0, 0.5); b1 ~ normal(0, 0.25)\",\n                                        \"b0 ~ normal(0, 1); b1 ~ normal(0, 0.50)\",\n                                        \"b0 ~ normal(0, 2); b1 ~ normal(0, 1)\")))\n\nggplot(b0b1) +\n  geom_abline(aes(intercept = b0, slope = b1), color = \"cadetblue4\", alpha = 0.75) +\n  scale_x_continuous(limits = c(-2, 2)) +\n  scale_y_continuous(limits = c(-3, 3)) +\n  labs(x = \"x\",\n       y = \"y\",\n  title = latex2exp::TeX(\"Sampling of lines from $\\\\beta_0$ and $\\\\beta_1$ priors\")) +\n  facet_wrap(~sigma_id, nrow = 3)",
    "crumbs": [
      "Bayes",
      "Priors"
    ]
  },
  {
    "objectID": "qmd/bayes-priors.html#sec-bayes-priors-conjp",
    "href": "qmd/bayes-priors.html#sec-bayes-priors-conjp",
    "title": "Priors",
    "section": "Conjugate Priors",
    "text": "Conjugate Priors\n\nIf the posterior distributions p(Œ∏ | x) are in the same probability distribution family as the prior probability distribution p(Œ∏), the prior and posterior are then called conjugate distributions, and the prior is called a conjugate prior for the likelihood function p(x | Œ∏)\nBenefits\n\nBayesian updates no longer need to compute the product of the likelihood and prior (only addition is needed).\n\nThis product is computationally expensive and sometimes not feasible.\nOtherwise numerical integration may be necessary\n\nMay give intuition, by more transparently showing how a likelihood function updates a prior distribution.\n\nAll members of the exponential family have conjugate priors.\nList\n&lt;Beta posterior&gt;\nBeta prior * Bernoulli likelihood ‚Üí Beta posterior\nBeta prior * Binomial likelihood ‚Üí Beta posterior\nBeta prior * Negative Binomial likelihood ‚Üí Beta posterior\nBeta prior * Geometric likelihood ‚Üí Beta posterior\n\n&lt;Gamma posterior&gt;\nGamma prior * Poisson likelihood ‚Üí Gamma posterior\nGamma prior * Exponential likelihood ‚Üí Gamma posterior\n\n&lt;Normal posterior&gt;¬†\nNormal prior * Normal likelihood (mean) ‚Üí Normal posterior\n\n&lt;Others&gt;\nDirichlet prior * Multinomial likelihood ‚Üí Dirichlet posterior",
    "crumbs": [
      "Bayes",
      "Priors"
    ]
  },
  {
    "objectID": "qmd/bayes-priors.html#sec-bayes-priors-pelic",
    "href": "qmd/bayes-priors.html#sec-bayes-priors-pelic",
    "title": "Priors",
    "section": "Prior Elicitation",
    "text": "Prior Elicitation\n\nTranslating expert opinion to a probability density than you can use as a prior\nMisc\n\nNotes from:\n\nVideo: On using expert information in Bayesian statistics (R &gt;&gt; Videos)\nPaper: Methods for Eliciting Informative Prior Distributions (R &gt;&gt; Documents &gt;&gt; Bayes)\n\nPackages\n\n{SHELF} - shiny apps for eliciting for various distributions and via various methods\n\nwebpage also has links to papers\n\n\nPrior sensitivity analysis should done especially if you‚Äôre using expert-informed priors.\n\nBest Practice\n\nUse domain experts to set constraints for your priors (e.g.¬†upper and lower limits) instead of formulating a prior\n\nBest to use data, previous studies, etc. to formulate priors\n\nUse domain experts to inform you on relationships between variables\n\nElicitation process is resource intensive for what is probably minimal gain in comparison to data\n\nIt takes a lot of your time and their time to get this right\nExperts may not have the statistical knowledge to understand what you need or how to convey the information\n\nIn this case, you‚Äôll need to school them on basic statistical concepts\n\n\nWhen to use expert information to formulate a prior\n\nIf your DAG specifies a model that requires more data than you have.\n\nWhile it might be necessary to augment your data, be aware that expert knowledge has been shown to much less useful in complex models rather than simpler models.\n\nIf the your data is really noisy\nIf experts know something that isn‚Äôt represented by your data or can‚Äôt be captured by the data\nIf domain expertise is required given your research question\n\nInterviews with experts\n\nQuality Control: If your subject matter allows, try to create ‚Äúcalibrating‚Äù questions to weed-out the experts that aren‚Äôt really experts\n\nShould be questions that you are certain of the answer and are things any expert should know\n\nThis can be difficult for some subject matter.\n\nMaybe consult with an expert that you‚Äôre confident is an expert to help come up with some questions.\n\nQuestions should be standardized, so you know that the results from each expert are consistent.\nFace-to-face elicitation produces greater quality results, because the facilitator can clarify the questions from the experts if needed.\nTry to keep experts from biasing the information they give you\n\nDon‚Äôt use experts that have seen the results of your model\n\nIf they‚Äôve seen your raw data that‚Äôs okay. (I dunno about this, even if they‚Äôve seen eda plots?)\n\nDon‚Äôt provide them with any estimates you may have from previous studies or other experts\nDon‚Äôt let them fixate on outlier scenarios they may have encountered\n\nRecord conversations with video and/or audio\n\nIf problems surface when evaluating the expert‚Äôs information, these can be useful to go back over the information collection process\n\nWas there a misunderstanding between you and the expert on what information you wanted\nWas the information biased? (see above)\nIf using mulitiple experts, maybe subgroups have different viewpoints/experiences which is causing a divergence in opinion (e.g nurses vs psychologists treating PTSD)\n\n\nIf problems surface when evaluating the expert‚Äôs information, it can be useful to gather specific experts that differ and have them discuss why they hold their substantially differing opinions. Afterwards, they may adjust their opinions and you‚Äôll have a greater consensus.\nProcess\n\nElicit location parameter (e.g.¬†via Trial Roulette) from the expert\n\nTrial Roulette (see paper in Misc for details)\n\nRequires the expert to have sufficient statistical knowledge to be able to place the blocks to form an appropriate distribution\nUser should be aware that distribution output may be inappropriate for sample data\n\nExample: Algorithm may output a Gamma distribution which is inappropriate for percentage data since the upper bound can be greater than one\n\nParameter space is split into subsections (e.g.¬†quantiles)\nUser assigns blocks to each subsection\nExample From MATCH website which was an earlier implementation of {SHELF}\n\nTop chart is a histogram where each cell is a ‚Äúblock‚Äù (called ‚Äúchips‚Äù at the bottom). The right panel shows the options for setting the axis ranges and number of bins\nBottom chart evidently estimates distribution parameters from the histogram in the top chart which are your prior‚Äôs parameters\n\n\nWith experts with less statistical training, it may be better for you to give them scenarios (e.g.¬†combinations of quantiles of the predictor variables) and have them predict the outcome variable.\n\nCompute the statistics given their answers. Show them the results. Ask them to give an uncertainty range around that statistic.\nExample: From their predictions, you calculate the mean. Then you present them with this average and ask them about their uncertainty?\n\ni.e.¬†What is the range around this value they expect the average to be in?\n\n\nAlso try combinations of methods\nSee paper in Misc for other options\n\nFeedback session\n\nExplain to the expert how you‚Äôre interpreting their information. Do they agree with your interpretation? Refine information based on their feedback.\n\nElicit scale and shape parameters (upper and lower bounds)\nFeedback session\nEvaluate distribution\n\n\nEvaluating Expert Distribution(s)\n\nMisc\n\nMight be better to use another measure instead of K-L divergence (see Inspect the distributions visually section below)\n\ne.g Jensen-Shannon Divergence, Population Stability Index (see Production, ML Monitoring for details)\n\n\nCalculate K-L divergence between the expert distribution and the computed posterior using the expert distribution as a prior\n\nSmaller K-L divergence means the 2 distributions are more similar\nLarger K-L divergence means the 2 distributions are more different\n\nCreate a benchmark distribution\n\nShould be a low information distribution as compared to the sample data distribution\ne.g.¬†uniform distribution\n\nCalculate K-L divergence between the benchmark distribution and the computed posterior using the benchmark distribution as the prior\nCalculate ratio of K-L divergences (expert K-L/benchmark K-L)\n\nGreater than 1 is bad. Indicates a ‚Äúprior data conflict‚Äù and it may be better to drop this expert‚Äôs distribution\nLess than 1 is good. Potentially an informative prior\n\nInspect the distributions visually (Expert prior distributions and computed posterior from benchmark prior)\n\nK-L divergence penalyzes more certain distributions (i.e.¬†skinny, tall) than less certain distribtutions (fatter, shorter) even if they have the same mean/median and mostly the same information\n\nSo, an expert that is more certain may have a disqualifying ratio of K-L difference while a less certain expert with a very similar distribution has a qualifying ratio.\n\nAfter inspecting the distributions, you may determine that distributions really are too different and the expert is far too certain to keep.\n\n\n\n\nAggregate distributions if you‚Äôre eliciting from multiple experts\n\nAverage the distributions (i.e.¬†equal weights for all experts)\nRank experts (e.g.¬†by K-L ratio), weight them, then calculate a weighted average distribution\nUse aggregated distribution(s) as your prior(s)",
    "crumbs": [
      "Bayes",
      "Priors"
    ]
  },
  {
    "objectID": "qmd/css-general.html",
    "href": "qmd/css-general.html",
    "title": "General",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "CSS",
      "General"
    ]
  },
  {
    "objectID": "qmd/css-general.html#sec-css-gen-misc",
    "href": "qmd/css-general.html#sec-css-gen-misc",
    "title": "General",
    "section": "",
    "text": "Resources\n\nOnline Interactive Cheat Sheet\nhttps://css-tip.com/\nWidget testing parameter values for css styling a div box\n\nColumn Widths in CSS Grid\n\nCSS comment - /* comment */\nSelector formats\n\nSyntax: #&lt;class&gt;.&lt;id&gt;&lt;additional-stuff&gt;\nExample:\n\nCSS\n#header.fluid-row::before{\n}\nHTML\n&lt;div class=\"fluid-row\" id=\"header\"&gt; == $0\n::before\n&lt;/div&gt;\n\n\nInclude css styling directly into a html page\n\nExample: Via HTML style tag\n&lt;style&gt;\nbody {\n¬† padding: 50px 25px 0px 25px;\n¬† font-family: 'Roboto', sans-serif;\n¬† font-size: 19px;\n}\n&lt;/style&gt;\nExample: Via R chunk\nhtmltools::tags\\$link(href = \"https://fonts.googleapis.com/css2?family=Libre+Baskerville:ital,wght\\@0,400;0,700;1,400&display=swap\",\n                      rel = \"stylesheet\")\nExample: styling of a legend html div\n&lt;style type='text/css'&gt;\n¬† .my-legend .legend-title {\n¬† ¬† text-align: left;\n¬† ¬† margin-bottom: 8px;\n¬† ¬† font-weight: bold;\n¬† ¬† font-size: 90%;\n¬† ¬† }\n¬† .my-legend .legend-scale ul {\n¬† ¬† margin: 0;\n¬† ¬† padding: 0;\n¬† ¬† float: left;\n¬† ¬† list-style: none;\n¬† ¬† }\n¬† .my-legend .legend-scale ul li {\n¬† ¬† display: block;\n¬† ¬† float: left;\n¬† ¬† width: 50px;\n¬† ¬† margin-bottom: 6px;\n¬† ¬† text-align: center;\n¬† ¬† font-size: 80%;\n¬† ¬† list-style: none;\n¬† ¬† }\n¬† .my-legend ul.legend-labels li span {\n¬† ¬† display: block;\n¬† ¬† float: left;\n¬† ¬† height: 15px;\n¬† ¬† width: 50px;\n¬† ¬† }\n¬† .my-legend .legend-source {\n¬† ¬† font-size: 70%;\n¬† ¬† color: #999;\n¬† ¬† clear: both;\n¬† ¬† }\n¬† .my-legend a {\n¬† ¬† color: #777;\n¬† ¬† }\n&lt;/style&gt;\n\nSee link for details on the legend div element that uses this CSS",
    "crumbs": [
      "CSS",
      "General"
    ]
  },
  {
    "objectID": "qmd/css-general.html#centering",
    "href": "qmd/css-general.html#centering",
    "title": "General",
    "section": "Centering",
    "text": "Centering\n\nThere are also instructions for placing elements in different positions (e.g.¬†right edge)\nNotes from How To Center a Div\n\nThere‚Äôs also code/explainer for centering elements (e.g.¬†images) that have to stacked on top of each other\n\n\n\nElements\n\nCenter Horizontally with auto-margins\n.element {\n  max-width: fit-content;\n  margin-left: auto;\n  margin-right: auto;\n  /* margin-inline: auto*/\n}\n\nUse when you want to horizontally center a single element without disturbing any of its siblings\nmax-width is used because if width is used instead, it would lock it to that size, and the element would overflow when the container is really narrow.\nIncluding only margin-left: auto will force the div flush with the right side and vice verse with margin-right\nmargin-inline: auto can replace both margin-left and margin-right to center the div\n\nCentering Vertically and Horizontally\n.container {\n  align-content: center;\n}\n.element {\n  max-width: fit-content;\n  margin-inline: auto;\n}\nCenter Vertically and Horizontally with Flexbox\n/* single element */\n.container {\n  display: flex;\n  justify-content: center;\n  align-items: center;\n}\n/* multiple elements */\n.container {\n  display: flex;\n  flex-direction: row;\n  justify-content: center;\n  align-items: center;\n  gap: 4px;\n}\n\nThe most versatile method; it can be used to center one or multiple children, horizontally and/or vertically, whether they‚Äôre contained or overflowing.\nflex-direction controls the direction in which the items are aligned, and it can have other values: column, row-reverse, column-reverse\n\nText\ncontainer {\n  display: flex;\n  justify-content: center;\n  align-items: center;\n  text-align: center;\n}\n\nBlocks of text can be treated as one element and can be centered using the previous methods. This code (text-align) is for centering the rows of text within a element‚Äôs block.\n\n\n\n\nViewports\n\nUseful for elements like dialogs, prompts, and GDPR banners need to be centered within the viewport. (Think pop-ups)\nCentering With Known Sizes\n.element {\n  position: fixed;\n  inset: 0px;\n  width: 12rem;\n  height: 5rem;\n  max-width: 100vw;\n  max-height: 100dvh;\n  margin: auto;\n}\n\nComplex and has more settings that depend on the element. See article for details but there are four main concepts:\n\nFixed positioning\nAnchoring to all 4 edges with inset: 0px\nConstrained width and height\nAuto margins\n\nOmitting top: 0px will anchor the element to the bottom\n\nUse calc with max-width to make sure theres a buffer around the element\nmax-width: calc(\n    100vw - 8px * 2\n  );\n\n\nCentering Elements With Unknown Sizes\n.element {\n  position: fixed;\n  inset: 0;\n  width: fit-content;\n  height: fit-content;\n  margin: auto;\n}\n\nfit-content is doing the work",
    "crumbs": [
      "CSS",
      "General"
    ]
  },
  {
    "objectID": "qmd/html.html",
    "href": "qmd/html.html",
    "title": "HTML",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "HTML"
    ]
  },
  {
    "objectID": "qmd/html.html#sec-html-misc",
    "href": "qmd/html.html#sec-html-misc",
    "title": "HTML",
    "section": "",
    "text": "Resources\n\nOnline Interactive Cheat Sheet\n\nSome JS libraries use custom attributes in html tags that have hyphens in their name. R hates hypens, but you can just put the attribute name in quotes and it works (e.g.¬†data-sub-html).\n# see data-sub-html\ntags$a(\n  href = paste0(\"images/gallery/large/\", l),\n  \"data-sub-html\" = \"&lt;h4&gt;Photo by - &lt;a href='https://unsplash.com/@entrycube' &gt;Diego Guzm√°n &lt;/a&gt;\",\n  tags$img(src = paste0(\"images/gallery/thumbnails/\", t))\n  )\nglue(\"&lt;b style='background-color:{color}; font-family:Roboto; font-size:15px'&gt;{value}&lt;/b&gt;\")\n\ncolor and value are variables\n\nglue(\"&lt;b style= 'font-family:Roboto; font-size:15px'&gt;{name}&lt;/br&gt;Combined Indicator&lt;/b&gt;: {value_text}\")\n\nname and value_text are variables\n\nhtml coment - &lt;!-- comment --&gt;\nwithTags - Instead of needing to specify tags each time a tag function is used, as in tags\\(div() and tags\\)p(), code inside withTags is evaluated with tags searched first, so you can simply use div() and p().\ntagList - takes a list of tag objects and combines them into html code\nExample: From my website gallery\n\nhtml\n&lt;!---- withTags part ----&gt;\n&lt;div class=\"row\" id=\"lightgallery\"&gt;\n  &lt;!---- tagsList part ----&gt;\n  &lt;a href=\"_gallery/img/images/gallery/large/excess-death-col.png\"&gt;\n    &lt;img src=\"_gallery/img/images/gallery/thumbnails/thumb-excess-death-col.png\"/&gt;\n  &lt;/a&gt;\n  &lt;a href=\"_gallery/img/images/gallery/large/pos-policy-one-2021-06-20.png\"&gt;\n    &lt;img src=\"_gallery/img/images/gallery/thumbnails/thumb-pos-policy-one-2021-06-20.png\"/&gt;\n  &lt;/a&gt;\n&lt;/div&gt;\n{htmltools}\n\nCreate list of tags\n# images_thumb, images_full_size are paths to png files\nmoose &lt;- \n  purrr::map2(images$images_thumb, images$images_full_size, \n    function(t, l) {\n      tags$a(\n        href = paste0(\"_gallery/img/\", l),\n                      tags$img(src = paste0(\"_gallery/img/\", \n                      t))\n      )\n    })\n\n#&gt; [[1]]\n#&gt; &lt;a href=\"_gallery/img/images/gallery/large/excess-death-col.png\"&gt;\n#&gt; &lt;img src=\"_gallery/img/images/gallery/thumbnails/thumb-excess-death-col.png\"/&gt;\n#&gt; &lt;/a&gt;\n\n#&gt; [[2]]\n#&gt; &lt;a href=\"_gallery/img/images/gallery/large/pos-policy-one-2021-06-20.png\"&gt;\n#&gt; &lt;img src=\"_gallery/img/images/gallery/thumbnails/thumb-pos-policy-one-2021-06-20.png\"/&gt;\n#&gt; &lt;/a&gt;\nConvert list of tags to code with tagsList\nsquirrel &lt;- tagsList(moose)\n\n#&gt; &lt;a href=\"_gallery/img/images/gallery/large/excess-death-col.png\"&gt;\n#&gt; &lt;img src=\"_gallery/img/images/gallery/thumbnails/thumb-excess-death-col.png\"/&gt;\n#&gt; &lt;/a&gt;\n#&gt; &lt;a href=\"_gallery/img/images/gallery/large/pos-policy-one-2021-06-20.png\"&gt;\n#&gt; &lt;img src=\"_gallery/img/images/gallery/thumbnails/thumb-pos-policy-one-2021-06-20.png\"/&gt;\n#&gt; &lt;/a&gt;\nInsert into a div frame with withTags\nwithTags(\n  div(\n    class = \"row\",\n    id = \"lightgallery\",\n    squirrel\n  )\n)\n#&gt; &lt;div class=\"row\" id=\"lightgallery\"&gt;\n#&gt;    &lt;a href=\"_gallery/img/images/gallery/large/excess-death-col.png\"&gt;\n#&gt;        &lt;img src=\"_gallery/img/images/gallery/thumbnails/thumb-excess-death-col.png\"/&gt;\n#&gt;    &lt;/a&gt;\n#&gt;    &lt;a href=\"_gallery/img/images/gallery/large/pos-policy-one-2021-06-20.png\"&gt;\n#&gt;        &lt;img src=\"_gallery/img/images/gallery/thumbnails/thumb-pos-policy-one-2021-06-20.png\"/&gt;\n#&gt;    &lt;/a&gt;\n#&gt; &lt;/div&gt;",
    "crumbs": [
      "HTML"
    ]
  },
  {
    "objectID": "qmd/information-theory.html",
    "href": "qmd/information-theory.html",
    "title": "Information Theory",
    "section": "",
    "text": "Kullback-Lieber Divergence",
    "crumbs": [
      "Information Theory"
    ]
  },
  {
    "objectID": "qmd/information-theory.html#sec-infothy-kldiv",
    "href": "qmd/information-theory.html#sec-infothy-kldiv",
    "title": "Information Theory",
    "section": "",
    "text": "Figure by Erik-Jan van Kesteren\n\n\n\nMeasures the similarity between the joint probability density function and the product of the individual density functions\n\nIf they‚Äôre the same, then both variables are independent\n\nAlso see Statistical Rethinking &gt;&gt; Chapter 7\nExample: Measuring Segregation (link)\n\\[\nL_u = \\sum_{g=1}^G p_{g|u} \\log \\frac{p_{g|u}}{p_g}\n\\]\n\n\\(p_{g|u}\\) is the proportion of a racial group, \\(g\\), in a neighborhood, \\(u\\)\n\\(p_g\\) is the overall proportion of that racial group in the metropolitan area\nThis is a sum of scores across all racial groups of a neighborhood, \\(u\\)",
    "crumbs": [
      "Information Theory"
    ]
  },
  {
    "objectID": "qmd/information-theory.html#sec-infothy-mi",
    "href": "qmd/information-theory.html#sec-infothy-mi",
    "title": "Information Theory",
    "section": "Mutual Information",
    "text": "Mutual Information\n\nMeasures how dependent two random variables are on one another\n\nAccounts for linear and non-linear dependence\n\nIf the mutual information is 0, the variables are independent, otherwise there is some dependence.\nExample: Measuring Segregation\n\\[\nM = \\sum_{u=1}^U p_uL_u\n\\]\n\n\\(L_u\\) : See example in Kullback-Lieber Divergence section\n\\(p_u\\) : Described as the ‚Äúsize of the neighborhood‚Äù\n\nNot sure if this is a count or a proportion of the population of the neighborhood to the population of the metropolitan area. Both may end up in the same place.\n\nThis is a sum of scores across all neighborhoods in a metropolitan area\n\nSo the neighborhood scores are weighted by neighborhood population and summed for an overall metropolitan score\n\\(L_u\\) is affected by the smallest racial proportion (see article) for that metropolitan area, so unless these are the same, you can‚Äôt compare metropolitan areas with this number. But you can use these numbers to see how a metro‚Äôs (or neighborhood‚Äôs) diversity has changed over time.\n\n\n\n\n\n\nFigure by Erik-Jan van Kesteren",
    "crumbs": [
      "Information Theory"
    ]
  },
  {
    "objectID": "qmd/teaching.html",
    "href": "qmd/teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Teaching"
    ]
  },
  {
    "objectID": "qmd/teaching.html#sec-teach-misc",
    "href": "qmd/teaching.html#sec-teach-misc",
    "title": "Teaching",
    "section": "",
    "text": "Also see\n\nBkmks:\n\nScience and Mathematics &gt;&gt; Education\nData Science &gt;&gt; Modeling &gt;&gt; Reporting, Sharing, Publishing &gt;&gt; Concepts &gt;&gt; Teaching\n\n\ntl;dr for writing¬†article or preparing a talk Audience\n\nKnow their level of knowledge\nWhy they should want to know this\nHow does that reason get them closer to their goal\n(Maybe those last 2 are the same?)\n\nBooks\n\nTeaching what you dont know\nThe discussion book -¬†50 great ways to get people talking\n\nOn average, 7 \\(\\pm\\) 2 things can be kept in short term memory\nTeach chunks of your concept map. Each new chunk should be adjacent to the previous chunk.\nSacrifice truth for clarity to give learner actionable concept\nExplaining an unclear concept from the homework or reading (Gelman)\n\nBetter to work through an example than to try to clarify a definition or restate it, etc.\nThen, ask the students to get into pairs and explain to each other the meaning of each of the concepts in question\nThen, if students want to ask questions on the concept, we could do it in the context of this example that they‚Äôve just been talking about. We could also loop back to their homework assignment.",
    "crumbs": [
      "Teaching"
    ]
  },
  {
    "objectID": "qmd/teaching.html#sec-teach-steps",
    "href": "qmd/teaching.html#sec-teach-steps",
    "title": "Teaching",
    "section": "Steps",
    "text": "Steps\n\nState goalpost\nSplit goalpost into concepts\nConnect concepts to form map\nIf more than 7 \\(\\pm\\) 2 concepts, then group in chunks\nThe next chunk of concepts you present should be adjacent to previous chunk\nSummarize",
    "crumbs": [
      "Teaching"
    ]
  },
  {
    "objectID": "qmd/teaching.html#sec-teach-chtaud",
    "href": "qmd/teaching.html#sec-teach-chtaud",
    "title": "Teaching",
    "section": "Characterize the Audience",
    "text": "Characterize the Audience\n\nGeneral background\nrelevant experience\nperceived needs\nspecial consideration",
    "crumbs": [
      "Teaching"
    ]
  },
  {
    "objectID": "qmd/teaching.html#sec-teach-mentmod",
    "href": "qmd/teaching.html#sec-teach-mentmod",
    "title": "Teaching",
    "section": "Mental Model",
    "text": "Mental Model\n\nDraw a concept map. Concepts and connections between them\nExamples:¬†Venn¬†diagrams, flow charts",
    "crumbs": [
      "Teaching"
    ]
  },
  {
    "objectID": "qmd/teaching.html#sec-teach-ass",
    "href": "qmd/teaching.html#sec-teach-ass",
    "title": "Teaching",
    "section": "Assessments",
    "text": "Assessments\n\nSummative\n\nSummary of what you want to be learned. Goalpost\nGuide to creating¬†formative assessments by working backwards from the endpoint\n\nFormative\n\nIs the learning working?¬†\nTypes of mistakes or questions are clues to the types of misconceptions that learners are thinking and what you should say next.\nDiagnosing misconceptions by checking in every few minutes with questions. Think about what those answers might be and they mean.\nQuestions should have diagnostic power\nTells you if its okay to move on to the next lesson.",
    "crumbs": [
      "Teaching"
    ]
  },
  {
    "objectID": "qmd/teaching.html#sec-teach-forces",
    "href": "qmd/teaching.html#sec-teach-forces",
    "title": "Teaching",
    "section": "Forces for Learning",
    "text": "Forces for Learning\n\nPositive\n\nIntrinsic Motivation - Learner isnt being made to learn something, theyre choosing to learn\nUtility - Moves them towards their goals\nCommunity - Not alone in learning, connection to peers, more comfortable about not knowing someting\n\nNegative\n\nUnpredictability - ‚ÄúWhat i do doesnt seem to affect the outcome‚Äù, learned helplessness\nUnfairness - Teacher Bias\nIndifference - Feeling that the teacher doesnt care about your problem",
    "crumbs": [
      "Teaching"
    ]
  },
  {
    "objectID": "qmd/surveys-census-data.html#sec-surv-cens-tidyc",
    "href": "qmd/surveys-census-data.html#sec-surv-cens-tidyc",
    "title": "Census Data",
    "section": "tidycensus",
    "text": "tidycensus\n\nGet an API key\n\nRequest a key, then activate the key from the link in your email.(https://api.census.gov/data/key_signup.html)\n\nRequired for hitting the census API over 500 times per day which isn‚Äôt as hard as you‚Äôd think.\n\nSet as an environment variable: census_api_key(\"&lt;api key&gt;\", install = TRUE)\n\nOr add this line to .Renviron file, CENSUS_API_KEY=‚Äò&lt;api key‚Äô\n\n\nSearch Variables\n\nColumns\n\nName - ID of the variable (Use this in the survey functions)\nLabel - Detailed description of the variable\nContext - Subject of the table that the variable is located in.\n\nPrefixes (Variables can have combinations of prefixes)\n\nP: i.e.¬†Person; Data available at the census block and larger\nCT: Data available at the census track and larger\nH: Data available at the Housing Unit level\n\nI think housing unit is an alternatve unit. So instead of the unit being a person, which I assume is the typical unit, it‚Äôs a housing unit (~family).\nNot affected by Differential Privacy (i.e.¬†no noise added; true value)\nExample: Total Deleware housing units at census block level\ndp_households &lt;- \n      get_decennial(\n            geography = \"block\",\n            variables = \"H1_001N\",\n            state = \"DE\",\n            sumfile = \"dhc\",\n            year = 2020\n      )\n\n\nExample: DHC data in census for 2020\n\nvars &lt;- load_variables(2020, \"dhc\")\n\nView(vars)\n\nView table, click filter, and then search for parameters (e.g.¬†Age, Median, etc.) with the Label, Context boxes, and overall search box\n\n\nsummary_var - Argument for supplying an additional variable that you need to calculate some kind of summary statistic\n\nExample: Race Percentage per Congressional District\n\nrace_vars &lt;- c(\n  Hispanic = \"P5_010N\", # all races identified as hispanic\n  White = \"P5_003N\", # white not hispanic\n  Black = \"P5_004N\", # black not hispanic\n  Native = \"P5_005N\", # native american not hispanic\n  Asian = \"P5_006N\", # asian not hispanic\n  HIPI = \"P5_007N\" # hawaiian, islander not hispanic\n)\n\ncd_race &lt;- \n  get_decennial(\n    geography = \"congressional district\",\n    variables = race_vars,\n    summary_var = \"P5_001N\", # total population for county\n    year = 2020,\n    sumfile = \"cd118\"\n)\n\ncd_race_percent &lt;- \n  cd_race %&gt;%\n    mutate(percent = 100 * (value / summary_value)) %&gt;% \n    select(NAME, variable, percent)",
    "crumbs": [
      "Surveys",
      "Census Data"
    ]
  },
  {
    "objectID": "qmd/surveys-census-data.html#sec-surv-cens-map",
    "href": "qmd/surveys-census-data.html#sec-surv-cens-map",
    "title": "Census Data",
    "section": "Mapping",
    "text": "Mapping\n\nMisc\n\nUse geometry = TRUE for any of the get_* {tidycensus} functions, and it‚Äôll join the shapefile with the census data. Returns a SF (Simple Features) dataframe for mapping.\nIf you only want the shape files without the demographic data, see {tigris}\nFor examples with {tmap}, see Chapter 6.3 of Analyzing US Census Data\n{mapview} along with some other packages gives you some tools for comparing maps (useful for eda or exploratory reports, etc.) (m1 and m2 are mapview objects)\n\nm1 + m2 - Creates layers that allows you click the layers button and cycle through multiple maps. So I assume you could compare more than two maps here.\nm1 | m2 - Creates swipe map (need {leaflet.extras2}). There will be a vertical slider that you can interactively slide horizontally to gradually expose one map or the other.\nsync(m1, m2) - Uses {leafsync} to create side by side maps. Zooming and cursor movement are synced on both maps.\n\n\n\n\nPreprocessing\n\nRemove water from geographies\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnyc_income_tiger &lt;- \n  get_acs(\n    geography = \"tract\",\n    variables = \"B19013_001\",\n    state = \"NY\",\n    county = c(\"New York\", \"Kings\", \"Queens\",\n               \"Bronx\", \"Richmond\"),\n    year = 2022,\n    cb = FALSE,\n    geometry = TRUE\n)\n\nlibrary(tigris)\nlibrary(sf)\nsf_use_s2(FALSE)\n\nnyc_erase &lt;- \n  erase_water(\n    nyc_income_tiger,\n    area_threshold = 0.5,\n    year = 2022\n)\n\nmapview(nyc_erase, zcol = \"estimate\")\n\nThe left figure is before and the right figure is after water removal\n\nAt the center-bottom, you can see how a sharp point is exposed where it was squared off before\nAt the top-left, the point along the border which are piers/docks are now exposed.\nAt the upper-middle, extraneous boundary lines have been removed and islands in the waterway are more clearly visible.\n\nWorks only with regular tigris shapefiles from the US census bureau ‚Äî so not OpenStreetMaps, etc. For other shapefiles, you‚Äôd need to do the manual overlay, see Chapter 7.1 in Analyzing US Census Data for details.\nCan take a couple minutes to run\ncb = FALSE says get the regular tigris line files which avoid sliver polygons which are caused by slight misalignment of layers (?)\narea_threshold = 0.5 says that water areas below the 50th percentile in terms of size are removed. Probably a value you‚Äôll have to play around with.\n\n\n\n\nChoropleths\n\nBest for continuous data like rates and percentages, but you can use for discrete variables\n\nYou can create a discrete color palette with the at argument in the mapview function.\n\nExample\n# check min and max of your data to select range of bins\nmin(iowa_over_65, \n    na.rm = TRUE) # 0\nmax(iowa_over_65, \n    na.rm = TRUE) # 38.4\n\nm1 &lt;- \n  mapview(\n    iowa_over_65, \n    zcol = \"value\",\n    layer.name = \"% age 65 and up&lt;br&gt;Census tracts in Iowa\",\n    col.regions = inferno(100, direction = -1),\n    at = c(0, 10, 20, 30, 40)\n  )\n\nThis will result in a discrete palette with bins of 0-10, 10-20, etc. Looks like an overlap, so I‚Äôm sure which bin contains the endpoints.\n\n\n\nExample: Over 65 in Iowa by census tract\n\nlibrary(mapviw); library(viridisLite)\n\niowa_over_65 &lt;- \n  get_decennial(\n    geography = \"tract\",\n    variables = \"DP1_0024P\",\n    state = \"IA\",\n    geometry = TRUE,\n    sumfile = \"dp\",\n    year = 2020\n  )\nm1 &lt;- \n  mapview(\n    iowa_over_65, zcol = \"value\",\n    layer.name = \"% age 65 and up&lt;br&gt;Census tracts in Iowa\",\n    col.regions = inferno(100, \n                          direction = -1))\n\n{mapview} is interactive and great for exploration of data\n\nExport as an HTML file\nhtmlwidgets::saveWidget(m1@map, \"iowa_over_65.html\")\n\nCan embed it elsewhere (html report or website) by adding it as an asset\n\nIn {ggplot}\ntexas_income_sf &lt;- \n  get_acs(\n    geography = \"county\",\n    variables = \"B19013_001\",\n    state = \"TX\",\n    year = 2022,\n    geometry = TRUE\n)\n\nplot(texas_income_sf['estimate'])\n\n\n\nCircle Maps\n\n‚ÄúGraduated Symbol‚Äù maps are better for count data. Even though using a choropleth is not as bad at the census tract level since all tracts have around 4000 people, the sizes of the tracts can be substantially different which can influence the interpretation. Using circles or bubbles, etc. focuses the user on the size of the symbol and less on the size of the geography polygons.\nExample: Hispanic Counts in San Diego County at the Census Tract Level\n\n\nsan_diego_race_counts &lt;- get_acs(\n  geography = \"tract\",\n  variables = c(\n    Hispanic = \"DP05_0073\",\n    White = \"DP05_0079\",\n    Black = \"DP05_0080\",\n    Asian = \"DP05_0082\"\n  ),\n  state = \"CA\",\n  county = \"San Diego\",\n  geometry = TRUE,\n  year = 2022\n)\n\nsan_diego_hispanic &lt;- \n  filter(\n    san_diego_race_counts, \n    variable == \"Hispanic\"\n  )\n\ncentroids &lt;- st_centroid(san_diego_hispanic)\n\ngrad_symbol &lt;- \n  ggplot() + \n  geom_sf(\n    data = san_diego_hispanic, \n    color = \"black\", \n    fill = \"lightgrey\") + \n  geom_sf(\n    data = centroids, \n    aes(size = estimate),\n    alpha = 0.7, \n    color = \"navy\") + \n  theme_void() + \n  labs(\n    title = \"Hispanic population by Census tract\",\n    subtitle = \"2018-2022 ACS, San Diego County, California\",\n    size = \"ACS estimate\") + \n  scale_size_area(max_size = 6)\n\nst_centroid finds the center point of geography polygons which will be the location of the symbols. If you look at the geometry column it will say POINT, which only has a latitude and longitude, instead of POLYGON, which as multiple coordiates.\nscale_size_area scales the size of the circle according to the count value.\n\nmax_size is the maximum diameter of the circle which you‚Äôll want to adjust to be large enough so that you can differentiate the circles but small enough so you have the least amount of overlap between circles in neighboring geographies (although this is probably inevitable).\n\n\n\n\n\nDot Density\n\nUseful to show heterogeneity and mixing between groups versus plotting group facet maps.\nExample: Population by Race in San Diego County\n\nsan_diego_race_dots &lt;- \n  as_dot_density(\n    san_diego_race_counts, # see circle maps example for code\n    value = \"estimate\", # group population\n    values_per_dot = 200,\n    group = \"variable\" # races\n  )\n\ndot_density_map &lt;- \n  ggplot() + \n  annotation_map_tile(type = \"cartolight\", \n                      zoom = 9) + \n  geom_sf(\n    data = san_diego_race_dots, \n    aes(color = variable), \n    size = 0.01) + \n  scale_color_brewer(palette = \"Set1\") + \n  guides(color = guide_legend(override.aes = list(size = 3))) + \n  theme_void() + \n  labs(\n    color = \"Race / ethnicity\",\n    caption = \"2018-2022 ACS | 1 dot = approximately 200 people\")\n\nas_dot_density scatters the dots randomly within a geography. values_per_dotsays each dot is 200 units (e.g.¬†people or households). Without shuffling, ggplot will layer each group‚Äôs dots on top of each other.\nannotation_map_tile from {ggspatial} applies a base map layer as a reference for the user. Base maps have land marks and popular features labeled in the geography and surrounding areas to help the user identify the area being shown.",
    "crumbs": [
      "Surveys",
      "Census Data"
    ]
  },
  {
    "objectID": "qmd/causal-inference.html",
    "href": "qmd/causal-inference.html",
    "title": "Causal Inference",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Causal Inference"
    ]
  },
  {
    "objectID": "qmd/causal-inference.html#sec-causinf-misc",
    "href": "qmd/causal-inference.html#sec-causinf-misc",
    "title": "Causal Inference",
    "section": "",
    "text": "Notes from\n\nhttps://fabiandablander.com/r/Causal-Inference.html\n\nStatistical models measure associations (e.g.¬†linear, non-linear) which is mutual information among the variables\n\ne.g.¬†wind and leaves moving in a tree (doesn‚Äôt answer whether the leaves moving creates the wind or the wind creates leaving moving)\n\nCausal inference predicts the conseqences after an intervention (i.e.¬†action)\n\nYou must know the direction of causation in order to predict the conseqences of an intervention (unlike measuring associations)\nAnswers the question, ‚ÄúWhat happens if I do this?‚Äù\n\nCausal inference is able to reconstruct unobserved counterfactual outcomes.\n\nAnswers the question, ‚ÄúWhat happens if I had done something else?‚Äù\n\nCausal assumptions are necessary in order to make causal inferences\n\nmultiple regression does not distinguish causes from confounds\np-values are not causal statements\n\nDesigned to control type I error rate\n\nAIC, etc are purely predictive\n\nCausal Experiment Assumptions\n\nsee tlverse workshop notes and ebook for listing of assumptions and definitions,¬† https://tlverse.org/acic2019-workshop/intro.html#identifiability\n\nThe tlverse Project seeks to use ML models to calculate causal effects. Uses Super Learner ensembling and Targeted Maximum Likelihood Estimation (TMLE) which they call Targeted Learning.\n\nIgnorability - By randomly assigning treatment, researchers can ensure that the potential outcomes are independent of treatment assignment, so that the average difference in outcomes between the two groups can only be attributable to treatment\n\nEngineering outcome variables using potential adjustment variables does not automatically adjust for those variables in your model\n\nNotes from There Are No Magic Outcome Variables\nExample\n\n\nP is population density\nX is the variable of interest\nGDP and P have been used to create GDP/P\nP influences X and provides a backdoor path to GDP/P, so P must be adjusted for\nEven if P doesn‚Äôt influence X, the point is that constructing GDP/P using P doens‚Äôt automatically adjust for P\n\n\nRandomized experiments remove all paths from the treatment variable, X\n\n\nAdjusting for Z, B, and C can add precision to measurement of the treatment effect since they are causal to Y, but they aren‚Äôt necessary to get an unbiased estimate of the treatment effect.\n\nTable 2 fallacy (Notes from McElreath video, 2022 SR Lecture 6)\n\n\nThe 2nd table presented in a paper is usually a summary of all the effects of a regression. The fallacy is that the coefficient of each variable is treated as causal.\nExample: The effect of HIV on Stroke\n\nThe model is lm(Stroke ~ HIV + Smoke + Age)\n\nOnly the coefficient of the HIV variable should be treated as causal and none of the other adjustment variables (Smoke, Age)\n\nThe effects for Smoke and Age are only partial.\nThere are likely unobserved confounding variables, U, on the effect of Smoking on Stroke (e.g.¬†other lifestyle variables).\n\nSmoke is confounded so it‚Äôs causal estimate is biased\nAge is also confounded since Smoke is now a collider and has been conditioned upon. This opens the non-causal path, Age-Smoke-U-Stroke.\n\nAge-Smoke is frontdoor, but the backdoor path, Smoke-U, also becomes a backdoor path for Age once Smoke is conditioned upon. (aka sub-backdoor path)\nSo any open path that contains a backdoor path must also be closed\n\n\n\nSolutions\n\nDon‚Äôt include effect estimates of adjustment variables\nExplicitly interpret each effect estimate according to the causal model\n\nSee 2022 SR at the end of Lecture 6 where McElreath breaks down the interpretation of each adjustment variable estimated effect.\n\n\n\nPartial Identification (Handling Unobserved Confounds)\n\nMisc\n\nAlso see\n\nPaper: Hidden yet quantifiable: A lower bound for confounding strength using randomized trials (code)\n\nUsing RCT results and Observational data, this paper proposes a statistical test and a method for determining the lower bound confounder strength.\nIn the context of pharmacuticals, RCT results are evidently often released after FDA approval, but this method can be used in any field where there‚Äôs a combination of RCT and observational studies..\n\n\n\nSometimes the confounding paths of a DAG model can be not be resolved.\n\nFor confounders that influence the treatment and outcome, see:\n\nStructural Causal Models &gt;&gt; Bayesian examples\nIf there‚Äôs a mediator, see Other Articles &gt;&gt; Frontdoor Adjustment\n\nMeasure proxies for the unobserved confound if it‚Äôs not practical/ethical to measure\n\ni.e.¬†If the confound is ability, then test scores, letters of recommendation, etc. could be proxies.\n\nExample: 2022 SR Lecture 10 video, code\n\n\nA: Admitted to Grad School, G: Gender, D: Dept, u: Ability, T1,2,3: 3 Test Scores\n\nAbility is latent variable/unobserved confounder\nTest Scores are proxies for Ability\n\nBoth models are fit simultaneously\nCouldn‚Äôt find a way to use {brms} to code this and Kurz didn‚Äôt included it in his brms SR book.\n\n\n\nA biased estimate is better than no estimate. It can provide an upper bound\nFind a natural experiment or design one\nSensitivity Analysis\n\nAfter the analyis, you should be able to make the statement, ‚ÄúIn order for the confound to be responsible for the entire causal effect, it was have to be .‚Äù\n\n\nPackages\n\n{tipr} - Tools for tipping point sensitivity analyses\n{konfound} (vignette): Utilizes a sensitivity analysis approach that extends from dichotomous outcomes and omitted variable sensitivity of previous approaches to continuous outcomes and evaluates both changes in estimates and standard errors.\n\nITCV (Impact Threshold of a Confounding Variable) - Generates statements about the correlation of an omitted, confounding variable with both a predictor of interest and the outcome. The ITCV index can be calculated for any linear model.\nRIR (Robustness of an Inference to Replacement) - Assesses how replacing a certain percentage of cases with counterfactuals of zero treatment effect could nullify an inference. The RIR index is more general than the ITCV index.\n\n\nSteps for using sensitivity analysis\n\nPerform a sensitivity analysis to determine plausibly how much of the causal effect is due to confounding paths\n\nAssume the confound exists, model it‚Äôs consequences for different strengths/kinds of influence\nExample: 2022 SR Lecture 10 video, code \n\nA: Admitted to Grad School, G: Gender, D: Dept, u: Unobserved Confounder\nBoth models are fit simultaneously\nValues for Œ≤ and Œ≥ are specified and u is estimated as a parameter\nI think Gender (G) is an interaction in both models which I didn‚Äôt think was possible given there are no arrows of influence from gender to u.\n\nSince gender is a moderator it wouldn‚Äôt necessarily have to be an influence arrow, it would only need to be an arrow from G to the effect of u on D (see Moderator Analysis), so maybe this is kosher\nCould also be that I‚Äôm misunderstanding McElreath‚Äôs code he uses to specify his models with {Rethinking}.\n\nCouldn‚Äôt find a way to use {brms} to code this and Kurz didn‚Äôt included it in his brms SR book.\n\n\nUse previous studies that have effect strengths of those potential confounding variables\nCompare the strengths from the previous studies to the strength determined from the sensitivity analysis. The difference is a good guess for the strength of the causal effect of your treatment variable.",
    "crumbs": [
      "Causal Inference"
    ]
  },
  {
    "objectID": "qmd/causal-inference.html#sec-causinf-causdes",
    "href": "qmd/causal-inference.html#sec-causinf-causdes",
    "title": "Causal Inference",
    "section": "Causal Design",
    "text": "Causal Design\n\nNotes from McElreath video\nMisc\n\nWhen trying to determine the relationship (e.g.¬†linear, nonlinear) between variables and remove inconsequential variables, the Double Debiased ML procedure might be useful.\n\nDouble Debiased Machine Learning - basic concepts, links to papers, videos\nEconML (Microsoft) and causalml (Uber) has included the method in their libraries\n\n\nWhen trying to infer causal relationships, we should not blindly enter all variables into a regression in order to ‚Äúcontrol‚Äù for them, but think carefully about what the underlying causal DAG could look like. Otherwise, we might induce spurious associations (e.g.¬†confounding such as collider bias).\nOverview\n\nMake a causal model (i.e.¬†DAG)\n\nNeed background information in order to make the causal assumptions represented in the DAG\nDAGs only show whether or not a variable influences another, not how the influence occurs (e.g.¬†DAGs can‚Äôt show interactions between variables or whether the association is non-linear)\n\nUse it to design data collection and statistical procedures\n\nSteps:\n\nDetermine two variables of interest (exposure, outcome) that you want to determine if a causal relationship exists and what effect the exposure has.\nUse domain knowledge or prior scholarship to determine the relevant variable and the likely associations between all variables in data\nCreate the DAG\n\nIdentify the direct causal path between exposure and outcome\nIdentify other explanatory variables and label their directions of influence with each other, the exposure, and the outcome variable\nConsider which variables (especially the exposure and the outcome) have unobserved variables influencing them.\n\nAnalyze the DAG\n\nIdentify colliders and use d-separation to determine conditional independencies\nIdentify additional paths (backdoor paths, sub-backdoor paths) between exposure and outcome\nUse the backdoor criterion to determine the set of variables that need to be adjusted for in order to block all backdoor paths with only the direct causal path remaining open.\nAdd additional adjustment variables that are causal to the outcome variable (but don‚Äôt confound the treatment effect) in order to add precision to the estimate of the treatment effect\n\nCreate simulated data that fits the DAG (i.e.¬†a generative model)\nPerform statistical analysis (i.e.¬†SCMs) on the simulated data¬† to make sure you can measure the causal effect.\nDesign experiment and collect the data\nRun the statistical analysis on the collected data and calculate the average causal effect (ACE) under the assumptions that your DAG and model specifications are correct.\nBased on your results, revise the DAG and SCM as necessary and repeat as necessary\n\nBad Adjustment Variables (Code and more details included in 2022 SR, Lecture 6)\n\nFor all examples, Z is the adjustment variable that‚Äôs being considered; X is the treatment and Y is the outcome\n\nIn each scenario, including Z produces a biased estimate of X, so the correct model is Y ~ X.\n\nM-bias\n\n\nZ doesn‚Äôt have a direct causal influence on the either X or Y, but when it‚Äôs conditioned upon it becomes a collider due to unobserved confounds that have a direct causal influence on X and Y.\nCommon issue in Political Science and network analysis\nExample\n\nY: Health of Person 2\nX: Health of Person 1\nZ: Friendship status\n\nPre-treatment variable (tend to be open to collider paths) since they could be friends before the exposure\n\nU: Hobbies of Person 1\nV: Hobbies of Person 2\n\n\nPost-Treatment Bias\n\n\nZ is a mediator and conditioning upon Z blocks the path from X to Y, but opens the backdoor path through the unobserved confound, U.\nCommon in medical studies¬†¬†\nExample\n\nY: Lifespan\nX: Win Lottery\nZ: Happiness\nU: Contextual Confounds\n\n\nSelection Bias\n\n\nSame as collider bias\n\nThis version adds an unobserved confounder\n\nExample\n\nY: Income\nX: Education\nZ: Values\nU: Family\n\n\nCase-Control Bias\n\n\nZ is a descendent. Since Z has information about Y, conditioning on it will narrow the variation of Y and distort the measured effect of X.\nAlso see Association &gt;&gt; Single Path DAGs &gt;&gt; Descendent\nExample\n\nY: Occupation\nX: Education\nZ: Income\n\n\nPrecision Parasite\n\n\n2 versions: with and without U\n\nWithout U, conditioning on Z removes variation from X and lessens (but doesn‚Äôt bias) the precision of the estimated effect of X on Y (i.e.¬†inflated std.error)\nWith U, the effect of X is biased and that bias is amplified when Z is included.\n\n\nPeer Bias\n\n\nClassic DAG of the Berkley Admission-Race-Department study\nAlso see Structural Causal Models &gt;&gt; Example (Bayesian Peer Bias)\nX is race, E is department, Q is unobserved (e.g.¬†student quality), Y is Admission\nDepartment cannot be conditioned upon because it‚Äôs a collider with Q and would bias the estimate of X through a sub-backdoor path, X-E-Q-Y\nOnly the total effect of X on Y can be estimated (Y ~ X) since E cannot be conditioned upon but that‚Äôs not interesting and maybe not precise",
    "crumbs": [
      "Causal Inference"
    ]
  },
  {
    "objectID": "qmd/causal-inference.html#sec-causinf-terms",
    "href": "qmd/causal-inference.html#sec-causinf-terms",
    "title": "Causal Inference",
    "section": "Terms",
    "text": "Terms\n\nAverage Causal Efffect (ACE) - average population effect that‚Äôs calculated from an intervention (see Counterfactual definition for info on Individual Causal Effects)\n\nIf X is binary, then  ¬†is the average causal effect¬†(see Simpson‚Äôs Paradox example)\n\nCalculated from a contingency table\n\nAlso, \n\nThis looks like the interpretation of the slope in a regression model.\n\n\nBackdoor Criterion - A valid causal estimate is available if it is possible to condition on variables such that all backdoor paths are closed\n\nGiven two nodes, X and Y, an adjustment set,¬†L, fulfills the backdoor criterion if¬†\n\nno member in¬†L¬†is a descendant of X and\nmembers in¬†L¬†block all backdoor paths (‚Äúshutting the backdoor‚Äù) between X and Y.\n\nAdjusting for¬†L¬†thus yields the causal effect of X‚ÜíY.\nAfter executing an intervention, the conditional distribution in the observational DAG (seeing) will correspond to the interventional distribution (doing) when blocking the spurious path. (see Simpson‚Äôs Paradox example)\n\nBackdoor Path - A non-causal path that enters a causal variable in a DAG rather than exits it.\n\ne.g.¬†the path that connects a collider to a causal variable points from the collider to the causal variable\nSub-backdoor Path - this path begins with a frontdoor path but through conditioning on a variable, it opens a connecting backdoor path which biases the treatment effect\n\nsee Misc &gt;&gt; Table 1 Fallacy and Causal Design &gt;&gt; Bad Adjustment Variables &gt;&gt; Peer Bias\n\n\nThe causal effect is the distribution of Y when we change x, averaged over the distributions of the adjustment variables (Z)\nCausal Hierarchy (lowest to highest)\n\nAssociation\n\nassociated action: Seeing - observational; observing the value of Y when X = x\n\n , observational distribution; What values Y would likely take on if X happened to equal x.\n\n\nIntervention\n\nassociated action (do-Calculus): Doing -¬† experimental; observing the value of Y after setting X = x\n\n , interventional distribution; What values Y would likely take on if X would be set to x.\nUsing the do operator allows us to make inferences about the population but not individuals.\ndo(X) means to cut all of the backdoor paths into X, as if we did a manipulative experiment. The do-operator changes the graph, closing the backdoors.\nThe do-operator defines a causal relationship, because Pr(Y|do(X)) tells us the expected result of manipulating X on Y, given a causal graph.\n\nWe might say that some variable X is a cause of Y when Pr(Y|do(X)) &gt; Pr(Y|do(not-X)).\n\n(makes more sense to me with a binary outcome, Pr(Y = 1|do(X), but maybe Y as a continuous variable can be defined a subset. ‚Ä¶I dunno)\n\n\nThe ordinary conditional probability comparison, Pr(Y|X) &gt; Pr(Y|not-X), is not the same. It does not close the backdoor.\nNote that what the do-operator gives you is not just the direct causal effect. It is the total causal effect through all forward paths.\n\nTo get a direct causal effect, you might have to close more backdoors.\n\nThe do-operator can also be used to derive causal inference strategies even when some backdoors cannot be closed.\n\n\nCounterfactual\n\nassociated action: Imagining¬†- what would be the outcome if the alternative would‚Äôve happened.\nIndividual Causal Effects can be calculated but it requires stronger assumptions and deeper understanding of the causal mechanisms\n\nNeed to research this part further.\nIf the underlying SCM is linear then the ICE = ACE.\n\n\n\nA collider along a path blocks that path. However, conditioning on a collider (or any of its descendants) unblocks that path\n\nWhen a collider is conditioned upon, the change in the association between the two nodes it separates is called collider bias.\n\ne.g.¬†if Z is a collider between X and Y, conditioning upon Z will induce an association between X and Y.\n\n\nA conditioning set, \\(L\\), is the set of nodes we condition on (it can be empty).\nConfounding is the situation where a (possibly unobserved) common cause obscures the causal relationship between two or more variables.\n\nThere is more than one causal path between two nodes.\nA causal effect of X on Y is confounded if¬† \nCollider bias is a type of confounding. When a collider is controlled for, a second (or more) path opens, and the effect is confounded\n\nX and Y are d-separated by¬†[L¬†if conditioning on all members in¬†[L¬†blocks all paths between the nodes, X and Y.\n\nTool for checking the conditional independencies which are visualized in DAGs.\n\nA descendant is a node connected to a parent node by that parent node‚Äôs outgoing arrow.\nFrontdoor Adjustment - In a causal chain with three nodes X‚ÜíZ‚ÜíY, we can estimate the effect of X on Y indirectly by combining two distinct quantities: (Useful for when unobserved confounders prevent direct causal estimation)\n\nThe estimate of the effect of X on Z, P(Z|do(X))\nThe estimate of the effect of Z on Y, P(Y|do(Z), X)\n\nFrontdoor Path - a path that exits a causal variable in a DAG rather than enters it.\n\ne.g.¬†the path that connects a causal variable, X, to an outcome variable, Y, has an arrow that points from X to Y.\n\nMarkov Equivalence - A set of DAGs, each with the same conditional independencies\nMediation Analysis -¬†seeks to identify and explain the mechanism or process that underlies an observed relationship between an independent variable and a dependent variable via the inclusion of a third hypothetical variable, known as a mediator variable (z-variable in the DAGs of ‚Äúpipes‚Äù below)\n\nIncluding a mediator and the independent variable in a regression will result in the independent variable not being signficant and the mediator being significant.\n\nModeration Analysis - Like mediation analysis, it allows you to test for the influence of a third variable, Z, on the relationship between variables X and Y, but rather than testing a causal link between these other variables, moderation tests for when or under what conditions an effect occurs.\nA node is a parent of another node if it has an outgoing arrow to that node\nA path from X to Y is a sequence of nodes and edges such that the start and end nodes are X and Y, respectively.\nResidual Confounding occurs when a confounding variable is measured imperfectly or with some error and the adjustment using this imperfect measure does not completely remove the effect of the confounding variable.\n\nExample: Women who smoke during pregnancy have a decreased risk of having a Down syndrome birth.\n\nThis is puzzling, as smoking is not often thought of as a good thing to do. Should we ask women to start smoking during pregnancy?\nIt turns out that there is a relationship between age and smoking during pregnancy, with younger women being more likely to indulge in this bad habit. Younger women are also less likely to give birth to a child with Down syndrome. When you adjust the model relating smoking and Down syndrome for the important covariate of age, then the effect of smoking disappears. But when you make the adjustment using a binary variable (age&lt;35 years, age &gt;=35 years), the protective effect of smoking appears to remain.\n\n\nStructural Causal Models (SCMs) - relate causal and probabilistic statements; each equation is a causal statement\n\n\n\n‚Äú:=‚Äù is the assignment operator\nX is a direct cause of Y which it influences through the function f( )\n\nwhere f is a statistical model\n\nThe noise variables, œµX and œµY, are assumed to be independent.\n\nThere are Stochastic and Deterministic SCMs. Deterministic SCMs presented in article.",
    "crumbs": [
      "Causal Inference"
    ]
  },
  {
    "objectID": "qmd/causal-inference.html#sec-causinf-assoc",
    "href": "qmd/causal-inference.html#sec-causinf-assoc",
    "title": "Causal Inference",
    "section": "Association",
    "text": "Association\n\n\n\nFar left: lm(Y ~ X); X and Y show a linear correlation when Z is NOT conditioned upon\nLeft: lm(Y ~ X + Z); X and Y show NO linear correlation when Z is conditioned upon\nRight: lm(Y ~ X); X and Y show NO linear correlation when Z is NOT conditioned upon\nFar Right:¬† lm(Y ~ X + Z); X and Y show a linear correlation when Z is conditioned upon",
    "crumbs": [
      "Causal Inference"
    ]
  },
  {
    "objectID": "qmd/causal-inference.html#sec-causinf-singpath",
    "href": "qmd/causal-inference.html#sec-causinf-singpath",
    "title": "Causal Inference",
    "section": "Single path DAGs",
    "text": "Single path DAGs\n\n\nFor each of these DAGs, Z would be the only member of the conditioning set.\nThe first 3 DAGs represent the scatter plots above\n\nZ only blocks the path between X and Y when it‚Äôs conditioned upon.\n\nX and Y are associated (e.g.¬†linear correlation, mutual information, etc.) when Z is ignored\nConditioning on Z results in X and Y no longer being associated (i.e.¬†conditional independence)\n\nThe first and second DAGs are elemental confounds or relations called ‚ÄúPipes.‚Äù\n\nThe left one\n\nIn general, DO NOT add these variables to your model\n\nThese paths are causal so they shouldn‚Äôt be blocked\nIf your goal isn‚Äôt causal inference, then adding these variables might provide predictive information\ne.g.¬†If there was a causal arrow from X to Y, the far left DAG would NOT have a backdoor path and therefore Z would not¬† be conditioned upon to block the path, X-Z-Y\n\nThe path from X to Z is a frontdoor path since the arrow exits X.\n\n\nSometimes you DO condition on these variables\n\nDuring mediation analysis, you condition on these variables as part of the process to determine how much of the effect goes through Z.\nThe mediation path can have an important interpretation depending on your research question\n\ne.g.¬†indirect descrimination\n\nSee Statistical Rethinking &gt;&gt; Chapter 11 &gt;&gt; Conclusion of Berkeley Admissions example\n\nalso Lecture 9 2022 video\n\n\n\n\n\nThe right one is a backdoor path and should be conditioned on.\nEverything you can learn about Y from X (or vice versa) happens through Z, therefore learning about X separately provides no additional information\nZ is traditionally labelled a mediator\n\nThe third DAG is an elemental confound¬† or relation called a ‚ÄúFork.‚Äù\n\nIn general, add these variables to your model\nThese are backdoor paths and are NOT causal\nX and Y have a common cause in Z and some of the mutual information about Z they each contain, overlaps, and creates an association (when Z isn‚Äôt conditioned upon).\n\n\nThe fourth DAG is an elemental confound or relation called a ‚ÄúCollider.‚Äù\n\n\nIn general, do NOT add these variables to your model\nZ blocks the path between X and Y unless conditioned upon.\nAn association between X and Y is induced¬† by conditioning on Z, lm(Y ~ X + Z)\n\nX and Y are independent causes of Z. Z contains information about both X and Y, but X doesn‚Äôt contain any information about Y and vice versa.\nA small X and a sufficiently large Y (and vice versa) can produce a Z = 1. So X and Y have compensatory relationship in causing Z.\n\ni.e.¬†For a given value of Z, learning something about X tells us what Y might have been.\n\n\n\nThe last elemental confound or relation is called a ‚ÄúDescendent.‚Äù\n\n\nConditioning on a descendent variable, D, is like conditioning on the variable, Z itself, but weaker. A descendent is a variable influenced by another variable.\nControlling for D will also control, to a lesser extent, for Z. The reason is that D has some information about Z. This will (partially) open the path from X to Y, because Z is a collider. The same holds for non-colliders. If you condition on a descendent of Z in the pipe, it‚Äôll still be like (weakly) closing the pipe.",
    "crumbs": [
      "Causal Inference"
    ]
  },
  {
    "objectID": "qmd/causal-inference.html#sec-causinf-dualpath",
    "href": "qmd/causal-inference.html#sec-causinf-dualpath",
    "title": "Causal Inference",
    "section": "Dual path DAGs",
    "text": "Dual path DAGs\n\n\nCausal paths do not flow against arrows but associations can.\nTwo examples of DAGs representing confounding\n\nThese are the 2 middle DAGs above with an additional path from X to Y\nIf Z is NOT conditioned on (i.e.¬†top path is not blocked), then the causal effect of X on Y would be confounded.\n\n\n\n\nThe paths from X to Y:\n\nThe path through Z matches the first DAG.\n\nTherefore X and Y are conditionally independent given Z.\n\nThe path through W matches the fourth DAG\n\nTherefore X and Y are conditionally dependent given W.\n\n\nThe path through W (collider) is blocked unless W is conditioned upon\nThe path through Z is open unless Z is conditioned upon\nIf Z and W are conditioned upon, then the path between X and Y is open through W and an association is present.",
    "crumbs": [
      "Causal Inference"
    ]
  },
  {
    "objectID": "qmd/causal-inference.html#sec-causinf-interv",
    "href": "qmd/causal-inference.html#sec-causinf-interv",
    "title": "Causal Inference",
    "section": "Intervention",
    "text": "Intervention\n\n\nSince actual interventions are usually unfeasible, we want to be able to determine causality with observational data. This requires two assumptions:\n\nThe intervention occurs locally. Which means that only the variable we target is the one that receives the intervention.\nThe mechanism by which variables interact do not change through interventions; that is, the mechanism by which a cause brings about its effects does not change whether this occurs naturally or by intervention\n\nThe Doing row of DAGs (aka manipulated DAGs) represents setting X = x\n\nFor DAGs 1 and 4, Y is still affected\n\nMoving from seeing to doing didn‚Äôt change anything\n\n\nFor DAGs 2 and 3, Y is now UNaffected\n\nUsing the assumptions and some mathematical manipulation (See article for details):\n\n\n\nThus, the interventional distribution we care about is equal to the (observational) conditional distribution of Y given X when we adjust for Z\n\n\n\n\nThe rule: After an intervention, incoming arrows are cut from the node where the intervention took place.",
    "crumbs": [
      "Causal Inference"
    ]
  },
  {
    "objectID": "qmd/causal-inference.html#sec-causinf-confound",
    "href": "qmd/causal-inference.html#sec-causinf-confound",
    "title": "Causal Inference",
    "section": "Confounding",
    "text": "Confounding\n\n\nThe backdoor criterion tells us which variable we need to adjust for in order to for our model to yield a causal relationship between two variables (i.e.¬†graphically, nodes)\n\nBlocks all spurious, that is, non-causal paths between X and Y.\nLeaves all directed paths from X to Y unblocked\nCreates no spurious paths\n\nExample\n\nCausal effect of Z on U is confounded by X¬†because in addition to the legitimate causal path Z‚ÜíY‚ÜíW‚ÜíU, there is also an unblocked path Z‚ÜêX‚ÜíW‚ÜíU which confounds the causal effect\n\nSince X‚Äôs arrow enters the causal variable of interest, Z, it‚Äôs arrow is a backdoor path and needs to be blocked/closed\nThere are some descendant nodes that make the confounding a little difficult to parse out, but this graph is essentially\n\n\nwhich is the same as the second example DAG for confounding in the Association section\n\n\nThe backdoor criterion would have us condition on X, which blocks the spurious path and renders the causal effect of Z on U unconfounded.\n\nThe reduced, confounding DAG above is the same as the third DAG (without the path from Z to U) in the Association section. Conditioning on Z in that example blocked the path between X and Y, so it makes sense that conditioning on X in the reduced DAG would block the Z to X to U path. And therefore, the¬†Z‚ÜêX‚ÜíW‚ÜíU would also be blocked in the complete DAG.\n\nNote that conditioning on W would also block this spurious path; however, it would also block the causal¬†path, Z‚ÜíY‚ÜíW‚ÜíU.\n\n\nIf we breakdown the complete DAG into the modular components involving W, we can see these are the same as the first example DAG in the Association section.\nW is also collider for X and Y, but I don‚Äôt think that has any bearing when discussing the causal effect of Z on U.",
    "crumbs": [
      "Causal Inference"
    ]
  },
  {
    "objectID": "qmd/causal-inference.html#sec-causinf-appsimp",
    "href": "qmd/causal-inference.html#sec-causinf-appsimp",
    "title": "Causal Inference",
    "section": "Application: Simpson‚Äôs Paradox Example",
    "text": "Application: Simpson‚Äôs Paradox Example\n\nSex as the adjustment variable¬†¬† ¬†¬†¬† ¬†¬†¬† \n\nPatients CHOOSE whether or not to take a drug to cure some disease.\nMen choosing to take the drug recover at a higher percentage that those that didn‚Äôt\nWomen choosing to take the drug recover at a higher percentage that those that didn‚Äôt\nBut overall, those that chose to take the drug recovered at a lower percentage than those that didn‚Äôt.\nSo should a doctor prescribe the drug or not?\nSuppose we know that women are more likely to take the drug, that being a woman has an effect on recovery more generally, and that the drug has an effect on recovery.¬†\nCreate DAGs\n\n\nS=1 as being female,\nD=1 as having chosen to take the drug\nR=1 as having recovered\nThe right DAG indicates either forcing everyone to either take the drug or not take the drug\nNotice that  ¬†therefore our calculated effect will be confounded.\n\nBackdoor criterion says the manipulated DAG (right) will correspond to the observational DAG (left) if we condition on Sex.\n\n\nUse intervention formula from Intervention section\n\n\nAverage Causal Effect = 0.832 - 0.782 = 0.050. So the drug has a positive effect on average.\n\n\nBlood Pressure as the adjustment variable \n\nBlood Pressure instead of sex is used as the adjustment. Blood Pressure is a post-treatment variable.\nRelatively same observations as before. High or Low Blood Pressure with the drug produces better results than those that chose not to take the drug. Yet overall, those that chose the drug recovered at a lower percentage.\n\nSince Blood Pressure (B) is post-treatment, it has no effect on whether the patient takes the drug or not (D).\nTaking or not taking the drug (D) has an indirect effect on recovery (R) through Blood Pressure (B) along with a direct effect.  ¬†so our calculated effect will be unconfounded.\n\nSo with BP as the adjustment variable, the drug now has a small, negative effect (harmful), 0.78 - 0.83 = -0.05\n\nThe unconfounded, average causal effect for the population is negative, therefore the doctor should NOT prescribe the drug.",
    "crumbs": [
      "Causal Inference"
    ]
  },
  {
    "objectID": "qmd/causal-inference.html#sec-causinf-scms",
    "href": "qmd/causal-inference.html#sec-causinf-scms",
    "title": "Causal Inference",
    "section": "Structural Causal Models (SCMs)",
    "text": "Structural Causal Models (SCMs)\n\nYou add additional assumptions to your DAG to derive a causal estimator\n‚ÄúFull Luxury‚Äù Bayesian approach\n\n‚ÄúFull Luxury‚Äù is just a term coined by McElreath; it‚Äôs just a bayesian model but bayesian models can fully model a DAG where standard regression approachs can fail (see examples)\nNo other approach will find something that the bayesian approach doesn‚Äôt\n\nMain disadvantage is that it can be computationally intensive (same with all baysian models)\n\nProvides ways to add ‚Äúcauses‚Äù for missingness and measurement error\n\nExample (2 Moms)\n\nNotes from McElreath video\nHypothesis: a mother‚Äôs family size is causal to her daughter‚Äôs family size\n\nTruth: no relationship\n\nVariables:\n\nM - Mother‚Äôs family size (i.e.¬†number of children the birth)\nD - Daughter‚Äôs family size\nB1 - Mother‚Äôs birth order; binary, first born or not\nB2 - Daughters‚Äô birth order; binary, first born or not\nU - unobserved confounds¬† (shown as curved dotted line)\n\n\n\nUnobserved confounds (economic status, education, cultural background, etc.) are causal to both Mother and Daughter (curved dotted line) which makes regression, D ~ M, impossible\n\nSee Baysian Two Moms example below for results of a typical regression\nStill possible to calculate the effect of M on D with SCMs\n\n\nAssumptions: Relationships are linear (i.e.¬†linear system)\nCausal Effects\n\n\nWe want m which is the causal effect of M on D\nAssumes causal effect of birth order is the same on mother and daughter\nAside: There is no arrow/coefficient from M to B2 because it‚Äôs not germane to the calculation of m\n\nCalculate linear effect (i.e.¬†regression coefficient) without a regression model using a linear system of equations\n\nNote: a regression coefficent, Œ≤ = cov(X,Y) / var(X)\nWe can‚Äôt calculate the covariance of M and D directly because it depends on unobserved confounders but we can calculate the covariance between B1 and D and use that to get m.\nThe covariance for each path is the product of the path coefficients and the variance of the originating causal variable.\nPath B1 ‚Üí M: cov(B1, M) = b*var(B1)\nPath B1 ‚Üí D: cov(B1, D) = b*m*var(B1)\n2 equations and 2 unknowns, m and b\nSolve for b in the first equation, substitute b into the second equation, and solve for m\n\nm = cov(B1, D) / cov(B1, M)\n\nStill need an uncertainty of this value (e.g.¬†bootstrap)\n\n\nExample (Bayesian 2 Moms)\n\nSee previous example for link, hypothesis, and definition of the variables\n\nFunctions (right side)\n\nEach variable‚Äôs function‚Äôs inputs are variables that are causal influences (i.e.¬†have arrows pointing at the particular variable\n\ne.g.¬†M has two arrows pointing at it in the DAG: B1 and u\n\n\nCode\n\nThe assumption is that this is a lineary system, so M and D have Normal distributions for their functions with means as linear regression equations\nB1 and B2 are binary so they get bernoulli distributions\nU gets a standard normal prior\n\nAside: evidently this is a typical prior for latent variables in psychology\n\np, intercepts, sd, k get typical priors for bayesian regressions\n\nResults\n\n\nTruth: no effect\n1st 3 lm models shows how the unobserved confound biases the estimate when using a typical regression model to estimate the causal effect\n\nIncluding B2 adds precision to the biased estimate since it is causal to the outcome D while adding B1 increases the bias\n\nBayesian model isn‚Äôt fooled because U is specified as an input to the functions for M and D\n\nInterpretation: There is no reliable estimate of an effect. The most likely effect is a moderately positive one but it could also be negative.\nAdding more simulated data to this example will move the point estimate towards zero\n\n\n\nExample (Bayesian Peer Bias)\n\nAlso see Causal Design &gt;&gt; Bad Adjustment Variables &gt;&gt; Peer Bias\nHypothesis: racial discrimination in acceptance of applicatioon to Berkeley grad schools\n\nTruth: moderate negative effect, -0.8\n\nVariables:\n\nX is race, E is department, Q is an unobserved confound (latent variable: student quality), Y is binary; Admission/No Admission\nR1 and R2 are proxy variables for Q (e.g.¬†test scores, lab work, extracurriculars, etc.)\n\nAssumptions: System is linear\nDAG and Code\n\n\nXX is the race variable with X as the coefficient in the code\n\nThis code uses his {rethinking} package so some of this syntax is unfamiliar\n\nR1 and R2 are shown in the DAG to be influenced by student quality, Q\nEvery prior is normal except for Q‚Äôs coefficient\n\nResults\n\n\nTruth: -0.8\n1st 3 glm models shows how the unobserved confound, Q, biases the estimate when using a typical logistic regression model to estimate the causal effect\nBayesian model isn‚Äôt fooled because Q is specified as an input to the function for Y\n\nInterpretation: There is a reliably negative effect (no 0 in the CI). The most likely effect is a moderately negative one.\nNot quite equal to the truth but reliably negative and the point estimate is closer than the glms\n\n\n\nExample\n\nAssumptions: Relationships between variables are linear and error terms are independent\nEquations\n\n,¬† \n\n\nDAG 1 (left) shows the association DAG which represents the SCM\nmanipulated DAG 1 (middle) shows intervention where z is set to a constant\n\nincoming causal arrows get cutoff the intervening variable\n\nmanipulated DAG 1 (right) shows intervention where x is set to a constant\n\nSimulation of the SCM (n = 1000) (code in article)\n\n\nZ is more predictive of Y than X\n\nSimulate interventions (code in article)\n\n\nLeft - histogram of SCM for Y without an intervention\nMiddle - Intervention on Z\n\nconfirms the DAG which shows no effect on Y and Z is not causal\n\nRight - intervention on X\n\nconfirms the DAG which shows an intervention on X produces an effect on Y and X is causal\n\nAverage Causal Effect (ACE) can be determined by subtracting the expected values of interventions where¬† X = x +1 and¬† X = x",
    "crumbs": [
      "Causal Inference"
    ]
  },
  {
    "objectID": "qmd/causal-inference.html#sec-causinf-ctrfact",
    "href": "qmd/causal-inference.html#sec-causinf-ctrfact",
    "title": "Causal Inference",
    "section": "Counterfactuals",
    "text": "Counterfactuals\n\nExample(code in article): Test whether Grandma‚Äôs home remedy can speed recovery time for the common cold\n\nSCM\n\n\nT is 1/0, i.e.¬†whether patient receives Grandma‚Äôs treatment, with p = 0.5; \nR is recovery time\nŒº is the intercept\nŒ≤ is the average causal effect, since\n\n\nwhere¬†\n\n\nFrom fitting the model, we find Œº = 7, Œ≤ = -2, Œ§ = 0, Œµ1 = 0.78\n\nTherefore, the Individual Causal Effect for patient 1\n\n\nJust plug and chug where we substitute T = 1 into the SCM and we already have the T = 0 part from the model\n\n\nIn this case, the SCM is linear, so the ICE = ACE.",
    "crumbs": [
      "Causal Inference"
    ]
  },
  {
    "objectID": "qmd/causal-inference.html#sec-causinf-medanal",
    "href": "qmd/causal-inference.html#sec-causinf-medanal",
    "title": "Causal Inference",
    "section": "Mediation Analysis",
    "text": "Mediation Analysis\n\n\nFigure\n\nc‚Äô is the direct effect of X on the outcome after the indirect path has been removed (i.e.¬†conditioned upon, outcome ~ X + mediator)\nc is the to total effect (outcome ~ X)\nc - c‚Äô equals the indirect effect\nSee definitions below\n\nAllows you to test for the influence of a third variable, the mediator, on the relationship between (i.e.¬†effect of) the treatment variable, X, and the Outcome variable, Y.\nMisc\n\nNotes from: Mediation Models\n\nOverview of packages (Aug 2020)\n\n{brms} very flexible in terms of models. You‚Äôll just have to calculate the effects by hand unless some outside package (e.g.¬†sjstats) takes a brms model and does it for you.\n\nSee below for formulas. {mediation} papers should have other formulas for other types of models (e.g.¬†poisson, binomial)\n\n{mediation} handles a lot for you. Method isn‚Äôt bayesian but is very similar to it in a frequentist-bootstrappy-simulation way.\n\nPackage has been substantially updated since that article was written.\n\n\nAlso see\n\nOther Articles &gt;&gt; Frontdoor Adjustment\nStatistical Rethinking &gt;&gt; Chapter 11 &gt;&gt; Conclusion of Berkeley Admissions example\n\nalso Lecture 9 2022 video\n\nebook (w/brms) Introduction to Mediation, Moderation, and Conditional Process Analysis\n\nPackages\n\n{lcmmtp} - Efficient and flexible causal mediation with longitudinal mediators, treatments, and confounders using modified treatment policies\n\nPaper: Identification and estimation of mediational effects of longitudinal modified treatment policies\n\n\nIncluding a mediator and the independent variable in a regression will result in the independent variable not being signficant and the mediator being significant.\n\nExample: Causal effect of education on income\n\nSay occupation is your mediator. Education has a big impact on your occupation, which in turn has a big impact on your income. You don‚Äôt want to control for a mediator if you are interested in the full effect of X on Y! Because a huge part of how X impacts on Y is precisely through the mediation of C, in our case choice of and access to occupation, given a certain level of education. If you ‚Äòcontrol‚Äô for occupation you will be greatly underestimating the importance of education.\n\n\nWhen would you want to only measure the Direct Effect?\n\nExample: Determining the amount of remuneration for discrimination\n\nFrom Simulating confounders, colliders and mediators\nVariables\n\nOutcome: Pay Gap\nTreatment: Gender\n\nIn this case, this variable is actually ‚Äúgender discrimination in the current workplace in making a pay decision‚Äù (for which we use actual, observed Gender as a proxy)\n\nMediators: Occupation and Experience\n\nWhen determining whether a type of descrimination exists, you don‚Äôt want to condtion on the mediators, because the effect of gender will be underestimated. So, you‚Äôd want the total effect. But here, discrimation is already determined and Gender is now a proxy variable. Under Gender‚Äôs new definition, Occupation and Experience might influence the amount of ‚Äúgender discrimiation,‚Äù so they can‚Äôt be definitively labelled mediators any more.\nSo if you want to estimate that final ‚Äúequal pay for equal work‚Äù step of the chain then yes it is legitimate to control for occupation and experience.\n\n\nShould always compare a mediation model to a model without mediation\n\nAn unnecessary mediation model will almost certainly be weaker and probably more confusing than the model you would otherwise have.\n\nAverage Causal Mediation Effect (ACME) (aka Indirect Effect)- the expected difference in the potential outcome when the mediator took the value that it would have under the treatment condition as opposed to the control condition, while the treatment status itself is held constant.\n\nIf this isn‚Äôt significant, there isn‚Äôt a mediation effect\nIt is possible that the ACME takes different values depending on the baseline treatment status. Shown by analyzing the interaction between the treatment variable and the mediator\nŒ¥(t) = E[Y (t, M(t1)) ‚àí Y (t, M(t0))]\n\nwhere\n\nt, t1, t0 are particular values of the treatment T such that t1 ‚â† t0,\nM(t) is the potential mediator\nY (t, m) is the potential outcome variable\n\n\n\nAverage Direct Effect (ADE) - the expected difference in the potential outcome when the treatment is changed but the mediator is held constant at the value that it would have if the treatment equals t.\n\nŒ∂(t) = E[Y (t1, M(t)) ‚àí Y (t0, M(t))]\n\nThe Total Effect of the treatment on the outcome is ACME + ADE.\n\nConditions where you likely do NOT need mediation analysis :\n\nIf you cannot think of your model in temporal or physical terms, such that X necessarily leads to the mediator, which then necessarily leads to the outcome.\nIf you could see the arrows going either direction.\nIf when describing your model, everyone thinks you‚Äôre talking about an interaction (a.k.a. moderation).\nIf there is NO strong correlation between key variables (variables of interest) and mediator, and if there is NO strong correlation between mediator and the outcome.\n\nSobel test - tests whether the suspected mediator‚Äôs influence on the independent variable is significant.\n\nPerforming the test in R via bda::mediation.test - article\n\nMethods\n\nBaron & Kenny‚Äôs (1986) 4-step indirect effect method has low power\nProduct-of-Paths (or difference in coefficients)\n\nc - c‚Äô = a*b (see figure at start of this section) where c - c‚Äô is the indirect effect (aka ACME)\n\nif either a or b are nearly zero, then the indirect effect can only be nearly zero\nFormula only appropriate for the analysis of causal mediation effects when both the mediator and outcome models are linear regressions where treatment (IV) and moderator enter the models additively (e.g.¬†without interaction)\n\nEffect formulas for models with an interaction between treatment and moderator (Paper)\n\nmediator: M = Œ±2 + Œ≤2Ti + ŒæT2Xi + Œµi2(T~i`)\noutcome: Y = Œ±~3 + Œ≤3Ti + Œ≥Mi + Œ∫TiMi + ŒæT3Xi + Œµi3(Ti, Mi)\nACME = Œ≤2(Œ≥ + Œ∫t) where t = 0,1\nADE = Œ≤3 + Œ∫{Œ±2 + Œ≤2t + ŒæT2Œï(Xi)}\nATE = Œ≤2Œ≥ + Œ≤3 +Œ∫{Œ±2 + Œ≤2 + ŒæT2Œï(Xi)}\n\nAlternatively, fit Y = Œ±1 + Œ≤1Ti + ŒæT1Xi + Œ∑TTiXi + Œµi1\n\nThen ATE = Œ≤1 + Œ∑TE(Xi)\n\n\nNotes\n\nVariables\n\nT is treatment, M is mediator, X is a set of adjustment variables\n\nThe exponentiated T in ŒæT is to let you know it can be a set of coefficients for a set of adjustment variables (I guess)\n\n\nCouldn‚Äôt figure out why curly braces are being used\nACME with have two estimates (t=0, t=1)\nATE (average total effect)\nŒï(Xi) is the sample average of each adjustment variable and it‚Äôs multiplied by its associated Œæ2 coefficient\nSee paper for other types of models\n\n\n{lavaan}, {brms}\n\nTingley, Yamamoto, Hirose, Keele, & Imai, 2014\n\nQuasi-bayesian approach (paper ,esp Appendix D, for details)\n\nFits the mediation and outcome models (see 1st example)\nTakes the coefficients and vcov matrices from both models\n\nUses the coefs (means) and vcovs (variances) as inputs to a mvnorm function to simulate distributions for the coefficients.\nI do not understand what these are used for‚Ä¶ would have to look at the code.\n\nSamples predictions of each model K times for treatment = 1, then for treatment = 0\nCalcs difference between predictions for each set of samples, then averages to get the ACME\n\nAssumes Sequential Ignorability\n\nRequires treatment randomization or an equivalent assignment mechanism\nmediator is also ignorable given the observed treatment and pre-treatment confounders. This additional assumption is quite strong because it excludes the existence of (measured or unmeasured) post-treatment confounders as well as that of unmeasured pretreatment confounders. This assumption, therefore, rules out the possibility of multiple mediators that are causally related to each other (see Section 6 for the method that is designed to deal with such a scenario).\nCan‚Äôt be tested but a sensitivity analysis can be conducted using mediation::medsens (see vignette)\n\n{mediation} (vignette)\n\nMultiple types of models for both mediator and outcome\n\nincluding multilevel model functions from {lme4} supported\n\nMethods for:\n\n‚Äòmoderated‚Äô mediation\n\nthe magnitude of the ACME depends on (or is moderated by) a pre-treatment covariate. Such a pre-treatment covariate is called a moderator. (see Moderator Analysis)\nACME can depend on treatment status (i.e.¬†interaction between treatment and mediator), but this situation is talking about a separate variable moderating the effect of the treatment on the mediator.\n\nmultiple mediators (which violates sequential ingnorability but can be handled)\nvarious experimental designs (e.g.¬†parallel, crossover)\ntreatment non-compliance\n\nUses MASS (so may have conflicts with dplyr)\nNo latent variable capabilities\n\n\nEtsy article calculates generalized average causal mediation effect (GACME) and generalized average direct effect (GADE) and uses a known mediator to measure the direct causal effect even when the DAG has multiple unknown mediators (paper, video, R code linked in article)\n\nExample: Tingley, 2014 Method\n\nEquations\n\n\n\nPredictions for ‚Äújob_seek‚Äù in the mediator model (top) are used as predictor values in the outcome model (bottom).\n\nData: data(jobs, package = 'mediation')\n\ndepress2: outcome, numeric: Measure of depressive symptoms post-treatment. The outcome variable.\ntreat: treatment, binary: whether participant was randomly selected for the JOBS II training program.\n\n1 = assignment to participation.\n\njob_seek: mediator, ordinal: measures the level of job-search self-efficacy with values from 1 to 5.\necon_hard: adjustment, ordinal: Level of economic hardship pre-treatment with values from 1 to 5.\nsex: adjustment, binary: 1 = female\nage: adjustment, numeric: Age in years\n\n{mediation}\nmodel_mediator &lt;- lm(job_seek ~ treat + econ_hard + sex + age, data = jobs)\nmodel_outcome¬† &lt;- lm(depress2 ~ treat + econ_hard + sex + age + job_seek, data = jobs)\n\n# Estimation via quasi-Bayesian approximation¬†\nmediation_result &lt;- mediate(\n¬† model_mediator,¬†\n¬† model_outcome,¬†\n¬† sims = 500,\n¬† treat = \"treat\",\n¬† mediator = \"job_seek\"\n)\n\nSummary - summary(mediation_result)\n\n\nerror bar plot also available via plot(mediation_result)\nSays ACME isn‚Äôt significant, therefore no mediation effect detected.\n‚ÄúProp Mediated‚Äù is supposed to be the ratio of the indirect effect to the total.\n\nHowever this is not a proportion, and can even be negative, and so ‚Äúit is mostly a meaningless number.‚Äù\n\n\n\n\nExample: product-of-paths (or difference in coefficients)\n\n{lavaan}\nsem_model = '\n¬† job_seek ~ a*treat + econ_hard + sex + age\n¬† depress2 ~ c*treat + econ_hard + sex + age + b*job_seek\n¬† # direct effect\n¬† direct := c\n¬† # indirect effect\n¬† indirect := a*b\n¬† # total effect\n¬† total := c + (a*b)\n'\nmodel_sem = sem(sem_model, data=jobs, se='boot', bootstrap=500)\nsummary(model_sem, rsq=T)¬† # compare with ACME in mediation\nDefined Parameters:\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† Estimate¬† Std.Err¬† z-value¬† P(&gt;|z|)\n¬† ¬† direct¬† ¬† ¬† ¬† ¬† -0.040¬† ¬† 0.045¬† -0.904¬† ¬† 0.366\n¬† ¬† indirect¬† ¬† ¬† ¬† -0.016¬† ¬† 0.012¬† -1.324¬† ¬† 0.185\n¬† ¬† total¬† ¬† ¬† ¬† ¬† ¬† -0.056¬† ¬† 0.046¬† -1.224¬† ¬† 0.221\n\nAlso outputs the typical summary regression estimates, std.errors, pvals, R2 etc.\nBootstraps std.errors\nSame results for ‚Äúindirect‚Äù here as with {mediation} ACME estimate\nR2s are poor for both regression models which could be why no mediation effect is detected.\n\n{brms}\nmodel_mediator &lt;- bf(job_seek ~ treat + econ_hard + sex + age)\nmodel_outcome¬† &lt;- bf(depress2 ~ treat + job_seek + econ_hard + sex + age)\nmed_result = brm(\n¬† model_mediator + model_outcome + set_rescor(FALSE),¬†\n¬† data = jobs\n)\nsummary(med_result) # regression results\n# using brms we can calculate the indirect effect as follows\nhypothesis(med_result, 'jobseek_treat*depress2_job_seek = 0')\n\nExact same brms syntax (except priors are specified) as in Statistical Rethinking &gt;&gt; Chapter 5 &gt;&gt; Counterfactual Plots\nExample has a mediator DAG as well.\nhypothesis tests H0: a*b == 0\n\npval &lt; 0.05 says there is a mediation effect.\n\n\n{sjstats}\n\nsjstats::mediation(med_result) %&gt;% kable_df()\n\nmediator (b): the effect of ‚Äújob_seek‚Äù on ‚Äúdepress2‚Äù\nindirect (c-c‚Äô): ACME\ndirect (c‚Äô): ADE\nproportion mediated: See {mediation} example",
    "crumbs": [
      "Causal Inference"
    ]
  },
  {
    "objectID": "qmd/causal-inference.html#sec-causinf-modanal",
    "href": "qmd/causal-inference.html#sec-causinf-modanal",
    "title": "Causal Inference",
    "section": "Moderation Analysis",
    "text": "Moderation Analysis\n\n\nMisc\n\nAlso see Introduction to Mediation, Moderation, and Conditional Process Analysis\n\nLike mediation analysis, it allows you to test for the influence of a third variable, Z (moderator), on the relationship between variables X and Y, but rather than testing a causal link between these other variables, moderation tests for when or under what conditions an effect occurs.\n\nModerators are conceptually different from mediators (‚Äúwhen‚Äù (moderator) vs ‚Äúhow/why‚Äù (mediator)).\n\nThere can be moderated mediation effect though. (see Mediation Analysis &gt;&gt; Methods &gt;&gt; {mediation})\n\nModerators can stengthen, weaken, or reverse the nature of a relationship.\nSome variables may be a moderator or a mediator depending on your question.\n\nAssumption: assumes that there is little to no measurement error in the moderator variable and that the DV did not CAUSE the moderator.\n\nIf moderator error is likely to be high, researchers should collect multiple indicators of the construct and use SEM to estimate latent variables.\nThe safest ways to make sure your moderator is not caused by your DV are to experimentally manipulate the variable or collect the measurement of your moderator before you introduce your IV.\n\nModeration can be tested by interacting variables of interest (moderator x IV) and plotting the simple slopes of the interaction, if present.\n\nSee Regression, Interactions for simple slopes/effects analysis\nMean center both your moderator and your IV to reduce multicolinearity and make interpretation easier. (‚Äúc‚Äù in variable names indicates variable was centered)\n\nExample: academic self-efficacy (moderator)(confidence in own‚Äôs ability to do well in school) moderates the relationship between task importance (independent variable (IV)) and the amount of test anxiety (outcome) a student feels (Nie, Lau, & Liau, 2011).\n\nStudents with high self-efficacy experience less anxiety on important tests (task importance) than students with low self-efficacy while all students feel relatively low anxiety for less important tests.\nSelf-efficacy (Z) is considered a moderator in this case because it interacts with task importance (X), creating a different effect on test anxiety (Y) at different levels of task importance.\n\nExample: What is the relationship between the number of hours of sleep (X, independent variable (IV)) a graduate student receives and the attention that they pay to this tutorial (Y, outcome) and is this relationship influenced by their consumption of coffee (Z, moderator)\nmod &lt;- lm(Y ~ Xc + Zc + Xc*Zc)\nsummary(mod)\n## Coefficients:\n##¬† ¬† ¬† ¬† ¬† ¬† Estimate Std. Error t value Pr(&gt;|t|)¬† ¬†\n## (Intercept) 48.54443¬† ¬† 1.17286¬† 41.390¬† &lt; 2e-16 ***\n## Xc¬† ¬† ¬† ¬† ¬† 5.20812¬† ¬† 0.34870¬† 14.936¬† &lt; 2e-16 ***\n## Zc¬† ¬† ¬† ¬† ¬† 1.10443¬† ¬† 0.15537¬† 7.108 2.08e-10 ***\n## Xc:Zc¬† ¬† ¬† ¬† 0.23384¬† ¬† 0.04134¬† 5.656 1.59e-07 ***\n\nSince we have significant interactions in this model, there is no need to interpret the separate main effects of either our IV or our moderator\nPlot the simple slopes (1 SD above and 1 SD below the mean) of the moderating effect\n\n\nFor details on this plot and analysis, see Regression, Interactions &gt;&gt; OLS &gt;&gt; numeric:numeric &gt;&gt; Calculate simple slopes for the IV at 3 representative values for the moderator variable\nInterpretation\n\nThose who drank less coffee (moderator, black line) paid more attention (outcome) with the more sleep (IV) that they got last night but paid less attention overall than average (the red line).\nThose who drank more coffee (moderator, green line) paid more attention (outcome) when they slept more (IV) as well and paid more attention than average.\nThe difference in the slopes for those who drank more or less coffee (moderator) shows that coffee consumption moderates the relationship between hours of sleep and attention paid",
    "crumbs": [
      "Causal Inference"
    ]
  },
  {
    "objectID": "qmd/causal-inference.html#sec-causinf-sr",
    "href": "qmd/causal-inference.html#sec-causinf-sr",
    "title": "Causal Inference",
    "section": "Statistical Rethinking",
    "text": "Statistical Rethinking\n\nMisc\n\nArrows indicate directions of influence\nArrows in DAGs ‚Äúcreate‚Äù correlations\n\ni.e.¬†if arrow, then correlation\nThe direction it points determines whether its association is causal or not.\n\nUnlike a statistical model, a DAG, if it is correct, will tell you the consequences of intervening to change a variable.\n** The data alone can never tell us when a DAG is right. But the data can tell us when a DAG is wrong. **\nMany dynamical systems cannot be usefully represented by DAGs, because they have complex behavior that is sensitive to initial conditions. But these models can still be analyzed and causal interventions designed from them.\nA DAG path means any series of variables you could walk through to get from one variable to another, ignoring the directions of the arrows.\nThe variable, U, in DAGs represents one or more unobserved variables\n\nUsually has circle around the U or is just represented by a dashed line\n\n‚ÄúConditioned upon,‚Äù ‚Äúadjusted for,‚Äù or ‚Äúcontrolled for‚Äù is all the same thing\n‚Äúa‚Äù or ‚ÄúŒ±‚Äù is used in bayesian formulas to represent the intercept\nNotation\n\nX is not independent of Y, i.e \nconditional independence: Y is not associated with some variable X, after conditioning on some other variable Z, i.e.¬†\n\nthey are statements of which variables should be associated with one another (or not) in the data.\nthey are statements of which variables become dis-associated when we condition on some other set of variables.\nThere is no other path of influence from X to Y except through Z\n\n\n(Total ) Causal Effect and Direct Causal Effect\n\n\nWeight (W) is the outcome, Height (H) and Sex (S) are explanatory\n(Total) Causal Effect is simply, W ~ S\nDirect Causal Effect shuts the backdoor paths, W ~ S + H\n\nSometimes we want the total causal effect and not the direct causal effect. (e.g.¬†if H is a post-treatment variable, see SR, Ch.6)\n\n\n\n\n\nTestable Implications\n\nDiffering associations between plausible DAGs that are testable through statistical models\nAny DAG may imply that some variables are independent of others under certain conditions.\nNO conditional independencies ‚Üí NO testable implications\nwww.dagitty.net - Enter DAG and it will give you the Adjustment Set and Testable Implications\nExample\n\nQuestion: What is the causal relationship between Divorce Rate (D), Marriage Rate (M), and Median Age at Marriage (A)\nData:\n\n2 regressions are fit\n\nD ~ Œ± + Œ≤M\n\nShows that M is positively correlated with D\n\nD ~ Œ± + Œ≤A\n\nShows that A is negatively correlated with D\n\n\n\nPlausible DAGs (note: marriage cannot influence your age‚Ä¶ technically)\n\n\n\nA directly influences D\nM directly influences D\nA directly influences M\nReasoning: First, Age can have a direct effect, perhaps because younger people change faster than older people and are therefore more likely to grow incompatible with a partner. Second, it can have an indirect effect by influencing the marriage rate. If people get married earlier, then the marriage rate may rise, because there are more young people. Consider for example if an evil dictator forced everyone to marry at age 65. Since a smaller fraction of the population lives to 65 than to 25, forcing delayed marriage will also reduce the marriage rate. If marriage rate itself has any direct effect on divorce, maybe by making marriage more or less normative, then some of that direct effect could be the indirect effect of age at marriage.\n\n\n\nSimilar to 1 except M does not directly influence D\nReasoning This DAG is plausible even though there‚Äôs a correlation between M and D (regression 1). It could be that M derives it‚Äôs correlation with D through it‚Äôs association with A.\n\nThe direction of influence doesn‚Äôt prevent a correlation between M and D\n\n\n\nTestable implications\n\nDAG 1\n\nThe DAG shows all three are associated to each other, i.e.¬†\nIt would be natural to think about measuring correlation and if a pair shows no correlation you could discard the DAG, but it is NOT a good test since there are many ways two variables can show correlation yet not be directly associated. (see reasoning under DAG 2 above and under DAG2 below)\nDAG1 has NO conditional independencies and therefore, NO testable implications\n\nDAG 2\n\nThis DAG also shows all three variables are associated with each other.\nD and M are associated with one another, because A influences them both. They share a cause, and this leads them to be correlated with one another through that cause. But suppose we condition on A. All of the information in M that is relevant to predicting D is in A. So once we‚Äôve conditioned on A, M tells us nothing more about D\nThe testable implication is that D is independent of M, conditional on A, i.e.¬†\n\n(Conditioning on A does not make D independent of M, because M really influences D all by itself in this model.)\n\ni.e A and M are marginally dependent\n\n\n\nOnly difference between both DAGs is the conditional independence in DAG2.\n\nTest\n\nRun a multiple regression D ~ Œ± + Œ≤MM + Œ≤AA\nIf the effect measured from regression 1 disappears in the multiple regression, then we can discard DAG 1. If the effect remains, then we discard DAG 2.\n\n\nDAGs that are consistent with the data associations (M & N are associated but the causal relationship isn‚Äôt known)\n\nwhere U is an unknown variable. Unobserved variables are circled.\n\nAll three DAGs have no conditional independencies and therefore not testable implications\n\nA set of DAGs, each with the same conditional independencies known as a Markov Equivalence\n\nData cannot eliminate any of these DAGS. Domain knowledge must be used to reduce the number of Markov Equivalent DAGs.\n\n\n\n\n‚ÄúShutting the backdoor‚Äù to potential confounding paths\n\nSection 6.4\nwww.dagitty.net - Enter DAG and it will give you the Adjustment Set and Testable Implications\nRecipe\n\nList all of the paths connecting X (the potential cause of interest) and Y (the outcome).\nClassify each path by whether it is open or closed. A path is open unless it contains a collider.\nClassify each path by whether it is a backdoor path. A backdoor path has an arrow entering X.\nIf there are any backdoor paths that are also open, decide which variable(s) to condition on to close it.\n\nIf you have a choice between two variables where conditioning on either will close a backdoor path and one of them is causal to the outcome variable, then condition on the variable that is causal to the outcome variable. It will add precision to the estimate of the treatment effect.\nAny frontdoor paths that lead to backdoor paths must also be closed (see Misc &gt;&gt; Table 2 fallacy)\n\n\nExamples:\n\n\n\nProblem: We want to measure the causal effect of X ‚Äì&gt; Y\nPotential confounding paths: XUAC, XUBC\n\nXUAC doesn‚Äôt have a collider so a variable needs conditioned on (aka adjusted for)\n\nU is unobserved, so either A or C. C directly influences Y, so it‚Äôs more efficient and will ‚Äúaid in precision.‚Äù\n\nXUBC has a collider, B. So, no need to condition on any variable\n\nSolution: Y ~ a + X + C\n\nlibrary(dagitty)\ndag_6.1 &lt;- dagitty( \"dag {¬†\n¬† ¬† U [unobserved]\n¬† ¬† X -&gt; Y\n¬† ¬† X &lt;- U &lt;- A -&gt; C -&gt; Y\n¬† ¬† U -&gt; B &lt;- C\n}\")\nadjustmentSets( dag_6.1 , exposure=\"X\" , outcome=\"Y\" )\n#&gt; { C }\n#&gt; { A }\n\n\nProblem: We want to measure the causal effect of the number of Waffle Houses, W, on Divorce, D.\nPotential confounding paths: WSM, WSA, WSMA (Also WSAM but McElreath on says there are 3. Maybe a combo of same letters is equivalent?)\n\nWSM doesn‚Äôt have a collider and therefore either S or M needs conditioned on\nWSA doesn‚Äôt have a collider and therefor either S or A needs conditioned on\nWSMA has a collider, M. So that path is blocked\nM is a choice for WSM but it‚Äôs a collider so it‚Äôs out. S is in both WSM and WSA, so conditioning on it kills two birds.\n\nSolution: D ~ a + W + S\n\nlibrary(dagitty)\ndag_6.2 &lt;- dagitty( \"dag {\n¬† ¬† A -&gt; D\n¬† ¬† A -&gt; M -&gt; D\n¬† ¬† A &lt;- S -&gt; M\n¬† ¬† S -&gt; W -&gt; D\n}\")\nadjustmentSets( dag_6.2 , exposure=\"W\" , outcome=\"D\" )\n#&gt; { A, M }\n#&gt; { S }\n\nEvidently conditioning on A and M is also a solution\n\nConditioning on M does close WSM but would then open WSMA. So, by then conditioning on A which is on a fork (or pipe depending on the path) it closes WSMA.\n\nIn his brms ebook, Kurz fits these regressions and a couple others for comparison. There wasn‚Äôt a consensus point estimate for W in the regressions that adjust for S and A + M.\n\nMcElreath mentions, ‚ÄúThis DAG is obviously not satisfactory‚Äìit assumes there are no unobserved confounds, which is very unlikely for this sort of data.‚Äù\nThe inconsistent point estimates are probably do to an omitted variable(s) that is confounding the regression.\n\nConditional independencies:\nimpliedConditionalIndependencies( dag_6.2 )\n#&gt; A _||_ W | S\n#&gt; D _||_ S | A, M, W\n#&gt; M _||_ W | S",
    "crumbs": [
      "Causal Inference"
    ]
  },
  {
    "objectID": "qmd/causal-inference.html#sec-causinf-othart",
    "href": "qmd/causal-inference.html#sec-causinf-othart",
    "title": "Causal Inference",
    "section": "Other Articles",
    "text": "Other Articles\n\nFrontdoor Adjustment\n\nFrom http://arelbundock.com/posts/frontdoor/\nUseful when an unobserved confounder creates a backdoor path that prevents direct causal estimation\nIn a causal chain with three nodes X‚ÜíZ‚ÜíY, we can estimate the effect of X on Y indirectly by combining two distinct quantities:\n\nThe estimate of the effect of X on Z, P(Z|do(X))\nThe estimate of the effect of Z on Y, P(Y|do(Z), X)\n\nAssumptions\n\nFull mediation: there is no direct path from X to Y, except through Z.\nUn-confoundedness 1: There is no open backdoor from X to Z.\nUn-confoundedness 2: All backdoors from Z to Y are blocked by X\n\nExample: 1\n\nOur goal is to estimate P(Y|do(X)). Unfortunately, this relationship between X and Y is confounded by the unobserved variable U, via this backdoor path: X‚ÜêU‚ÜíY. Therefore, we cannot estimate the causal quantity of interest directly.\n\n\ncause X, a mediator Z, an outcome Y, and an unobserved confounder U\n\nlibrary(data.table)\nset.seed(731460)¬†\nN = 1e5\nU = rbinom(N, 1, prob = .2)\nX = rbinom(N, 1, prob = .1 + U * .6)\nZ = rbinom(N, 1, prob = .3 + X * .5)\nY = rbinom(N, 1, prob = .1 + U * .3 + Z * .5)\ndat = data.table(X, Z, Y)\n\n# truth\ncoef(lm(Y ~ X + U))[\"X\"]\n## 0.2549541\nEstimate the effect of X on Z, P(Z|do(X))\nstep1 = lm(Z ~ X, dat)\nEstimate the effect of Z on Y, P(Y|do(Z), X)\nstep2 = lm(Y ~ Z + X, dat)\nCombine both estimates by multiplication\ncoef(step1)[\"X\"] * coef(step2)[\"Z\"]\n## 0.2496002\n\nExample 2\n\nSame as first example but using {dosearch} package\nlibrary('dosearch')\n   data1 &lt;- \"P(X, Y, Z)\"\nquery1 &lt;- \"P(Y | do(X))\"\ngraph1 &lt;- \"U -&gt; X\n¬† ¬† ¬† ¬† ¬† U -&gt; Y\n¬† ¬† ¬† ¬† ¬† X -&gt; Z\n¬† ¬† ¬† ¬† ¬† Z -&gt; Y \"\n   # compute\n   frontdoor &lt;- dosearch(data1, query1, graph1)\n   frontdoor\n\nOutput:\n\nEstimate the causal effect\ndat[, `P(X)`¬† ¬† := fifelse(X == 1, mean(X), 1 - mean(X)) ][\n¬† ¬† , `P(Z|X)`¬† := mean(Z), by = X¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ][\n¬† ¬† , `P(Y|Z,X)` := mean(Y), by = .(Z, X)¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ][\n¬† ¬† , `P(Z|X)`¬† := mean(Z), by = X¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ][\n¬† ¬† , Y := NULL¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ]\ndat = unique(dat)\ndat[, `P(Y|do(Z))` := sum(`P(Y|Z,X)` * `P(X)`), by = Z]\n`P(Y|do(X=0))` = with(dat[X == 0],¬†\n¬† `P(Z|X)`¬† ¬† ¬† ¬† ¬† [Z == 1] *¬†\n¬† `P(Y|do(Z))`¬† ¬† ¬† [Z == 1] +\n¬† (1 - `P(Z|X)`)¬† ¬† [Z == 0] *¬†\n¬† `P(Y|do(Z))`¬† ¬† ¬† [Z == 0]\n)\n`P(Y|do(X=1))` = with(dat[X == 1], {\n¬† `P(Z|X)`¬† ¬† ¬† ¬† ¬† [Z == 1] *¬†\n¬† `P(Y|do(Z))`¬† ¬† ¬† [Z == 1] +\n¬† (1 - `P(Z|X)`)¬† ¬† [Z == 0] *¬†\n¬† `P(Y|do(Z))`¬† ¬† ¬† [Z == 0]\n})\n`P(Y|do(X=1))` - `P(Y|do(X=0))`\n## 0.249766\nComparison\n\nTruth: 0.2549541\nlm: 0.2496002\ndosearch: 0.249766",
    "crumbs": [
      "Causal Inference"
    ]
  },
  {
    "objectID": "qmd/model-building-sklearn.html",
    "href": "qmd/model-building-sklearn.html",
    "title": "sklearn",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Model Building",
      "sklearn"
    ]
  },
  {
    "objectID": "qmd/model-building-sklearn.html#sec-modbld-sklearn-misc",
    "href": "qmd/model-building-sklearn.html#sec-modbld-sklearn-misc",
    "title": "sklearn",
    "section": "",
    "text": "Extensions\n\n{{DeepChecks}} - For model diagnostics\n{{feature-engine}} - Multiple transformers to engineer and select features to use in machine learning models.\n{{permetrics}} (vignette) - 111 diagnostic metrics for regression, classification, and clustering.\n\nPreventing data leakage in sklearn\n\nUse fit_transform on the train data - this ensures that the transformer learns from the train set only and transforms it simultaneously. Then, call the transformmethod on the test set to transform it based on the information learned from the training data (i.e mean and variance of the training data).\n\nPrevents data leakage\nSomehow the transform parameters calculated on the training data have to saved, so they can be applied to the production data. This can be done with Pipelines (see Pipelines &gt;&gt; Misc) by serializing the Pipeline objects.\n\nu_transf = LabelEncoder()\nitem_transf = LabelEncoder()\n# encoding\ndf['user'] = u_transf.fit_transform(df['user'])\ndf['item'] = item_transf.fit_transform(df['item'])\n# decoding\ndf['item'] = item_transf.inverse_transform(df['item'])\ndf['user'] = u_transf.inverse_transform(df['user'])\n\nA more robust way is to use sklearn‚Äôs Pipelines (see Pipelines below)\n\n\nTuning\n\nPick a metric for GridSearchCV and RandomizedSearchCV\n\nDefault is accuracy for classification which is rarely okay\nUsing metric from sklearn\ngs = GridSearchCV(estimator=svm.SVC(),¬†\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† param_grid={'kernel':('linear', 'rbf'), 'C':[1, 10]},\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† scoring=‚Äòf1_micro‚Äô)\n\nList of metrics available\n\nUsing a custom metric\nfrom sklearn.metrics import fbeta_score, make_scorer\ncustom_metric = make_scorer(fbeta_score, beta=2)\ngs = GridSearchCV(estimator=svm.SVC(),\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† param_grid={'kernel':('linear','rbf'), 'C':[1,10]},\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† scoring=custom_metric)\nAlso see Diagnostics, Classification &gt;&gt; Scores &gt;&gt; Lift Score\n\n\nScore model\n\nAlso see Pipelines &gt;&gt; Tuning a Pipeline\nBasic\nmodel = RandomForestClassifier(max_depth=2, random_state=0, warm_start=True, n_estimators=1)\nmodel.fit(X_train, y_train)\nmodel.score(X_test, y_test)\n\nClassification\nfrom sklearn.metrics import classification_report\nrep = classification_report(y_test, y_pred, output_dict = True)\n\nSee Diagnostics, Classification &gt;&gt; Multinomial for definitions of multinomial scores\n\nMultiple metrics function\nfrom sklearn.metrics import precision_recall_fscore_support\ndef score(y_true, y_pred, index):\n¬† ¬† \"\"\"Calculate precision, recall, and f1 score\"\"\"\n\n¬† ¬† metrics = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n¬† ¬† performance = {'precision': metrics[0], 'recall': metrics[1], 'f1': metrics[2]}\n¬† ¬† return pd.DataFrame(performance, index=[index])\npredict_proba outputs probabilities for classification models",
    "crumbs": [
      "Model Building",
      "sklearn"
    ]
  },
  {
    "objectID": "qmd/model-building-sklearn.html#sec-modbld-sklearn-opt",
    "href": "qmd/model-building-sklearn.html#sec-modbld-sklearn-opt",
    "title": "sklearn",
    "section": "Optimization",
    "text": "Optimization\n\nMisc\n\nFor fast iteration and if you have access to GPUs, it‚Äôs better to use {{XGBoost, LightGBM or CatBoost}} since sklearn is only CPU capable.\n\nFastest to slowest: CaBoost, LightGBM, XGBoost\n\n\n{{sklearnex}}\n\nIntel¬Æ Extension for Scikit-learn that dynamically patches scikit-learn estimators to use Intel(R) oneAPI Data Analytics Library as the underlying solver\n\nRequirements\n\nThe processor must have x86 architecture.\nThe processor must support at least one of SSE2, AVX, AVX2, AVX512 instruction sets.\nARM* architecture is not supported.\nIntel¬Æ processors provide better performance than other CPUs.\n\n\nMisc\n\nDocs\nBenchmarks\nAlgorithms\nCurrently extension does not support multi-output and sparse data for the Random Forest and K-Nearest Neighbors\n\nNon-interactively\npython -m sklearnex my_application.py\nInteractively for all algorithms\nfrom sklearnex import patch_sklearn\npatch_sklearn()\n# then import sklearn estimators\nInteractively for specific algorithms\nfrom sklearnex import patch_sklearn\npatch_sklearn([\n¬† ¬† 'RandomForestRegressor,\n¬† ¬† 'SVC',\n¬† ¬† 'DBSCAN'\n])\nUnpatch unpatch_sklearn()\nNeed to reimport estimators after executing\n\nSpeeding up retraining a model (article)\n\nPotentially useful for slow training models that need to be retrained often.\nNot all models have these methods available\nwarm_start = True permits the use of the existing fitted model attributes to initialize a new model in a subsequent call to fit\n\nDoesn‚Äôt learn new parameters so shouldn‚Äôt be used to fix concept drift\n\ni.e.¬†The new data should have the same distribution as the original data which maintains the same relationship with the output variable\n\nExample\n# original fit\nmodel = RandomForestClassifier(max_depth=2, random_state=0, warm_start=True, n_estimators=1)\nmodel.fit(X_train, y_train)\n\n# fit with new data\nmodel.n_estimators+=1\nmodel.fit(X2, y2)\n\n\nPartial Fit\n\nDoes learn new model parameters\nExample\n# original fit\nmodel = SGDClassifier()¬†\nmodel.partial_fit(X_train, y_train, classes=np.unique(y))\n\n# fit with new data\nmodel.partial_fit(X2, y2)",
    "crumbs": [
      "Model Building",
      "sklearn"
    ]
  },
  {
    "objectID": "qmd/model-building-sklearn.html#sec-modbld-sklearn-preproc",
    "href": "qmd/model-building-sklearn.html#sec-modbld-sklearn-preproc",
    "title": "sklearn",
    "section": "Preprocessing",
    "text": "Preprocessing\n\nMisc\n\nResources\n\nSciKit-Learn Transformation Docs\n\ntrain_test_split doesn‚Äôt choose random rows to be in each partition by default. For example, a 90-10 split has the first 90% of the rows in Train which leaves the last 10% of the rows to be in Test\n\n‚Äúshuffle=True‚Äù will randomly shuffle the rows before it partitions which would be equivalent to randomly selecting rows for each partition\n\n\nStratified train/test splits\nnp.random.seed(2019)\n# Generate stratified split\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random.state=42)\n# Train set class weights\n&gt;&gt;&gt; pd.Series(y_train).value_counts(normalize=True)\n1¬† ¬† 0.4\n0¬† ¬† 0.4\n2¬† ¬† 0.2\ndtype: float64\n# Test set class weights\n&gt;&gt;&gt; pd.Series(y_test).value_counts(normalize=True)\n1¬† ¬† 0.4\n0¬† ¬† 0.4\n2¬† ¬† 0.2\n\nOr you can use StratifiedKFold which stratifies automatically.\n\nStratified Train/Validate/Test splits\nX_train, X_, y_train, y_ = train_test_split(\n¬† ¬† df['token'], tags, train_size=0.7, stratify=tags, random_state=RS\n)\n\nX_val, X_test, y_val, y_test = train_test_split(\n¬† ¬† X_, y_, train_size=0.5, stratify=y_, random_state=RS\n)\n\nprint(f'train: {len(X_train)} ({len(X_train)/len(df):.0%})\\n'\n¬† ¬† ¬† f'val: {len(X_val)} ({len(X_val)/len(df):.0%})\\n'\n¬† ¬† ¬† f'test: {len(X_test)} ({len(X_test)/len(df):.0%})')\nCheck proportions of stratification variable (e.g.¬†‚Äútags‚Äù) in splits\nsplit = pd.DataFrame({\n¬† ¬† 'y_train': Counter(', '.join(i for i in row) for row in mlb.inverse_transform(y_train)),\n¬† ¬† 'y_val': Counter(', '.join(i for i in row) for row in mlb.inverse_transform(y_val)),\n¬† ¬† 'y_test': Counter(', '.join(i for i in row) for row in mlb.inverse_transform(y_test))\n}).reindex(tag_dis.index)\n\nsplit = split / split.sum(axis=0)\n\nsplit.plot(\n¬† ¬† kind='bar',¬†\n¬† ¬† figsize=(10,4),¬†\n¬† ¬† title='Tag Distribution per Split',¬†\n¬† ¬† ylabel='Proportion of observations'\n);\nBin numerics: KBinsDiscretizer(n_bins=4)\nImpute Nulls/Nas\n\nSimpleImputer (Docs)\ncol_transformer = ColumnTransformer(\n¬† ¬† # Transformer name, Transformer Object and columns\n¬† ¬† [(\"ImputPrice\", SimpleImputer(strategy=\"median\"), [\"price\"])],\n¬† ¬† # Any other columns are ignored\n¬† ¬† remainder= SimpleImputer(strategy=\"constant\", fill_value=-1)\n¬† )\n\nTakes ‚Äúprice‚Äù and replaces Nulls with median; all other columns get constant, -1, to replace their Nulls\n‚Äúmost frequent‚Äù also available\n\nIterativeImputer (Docs) - Multivariate imputer that estimates values to impute for each feature with missing values from all the others.\nKNNImputer (Docs) - Multivariate imputer that estimates missing features using nearest samples.\n\nLog: FunctionTransformer(lambda value: np.log1p(value))\nStandardize\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nstandardized_data = scaler.fit_transform(df)\nstandardized_df = pd.DataFrame(standardized_data, columns=df.columns)\n\nCheck: standardized_df.apply([\"mean\", \"std\"]\n\nScaling\n\nMin/Max\nfrom sklearn.preprocessing import MinMaxScaler\n# Create a scaler object\n\nmm_scaler = MinMaxScaler()\n\n# Transform the feature values\nchurn[[\"Customer_Age\", \"Total_Trans_Amt\"]] = mm_scaler.fit_transform(churn[[\"Customer_Age\", \"Total_Trans_Amt\"]])\n\n# check the feature value range after transformation\nchurn[\"Customer_Age\"].apply([\"min\", \"max\"])\n\n‚Äúfeature_range=(0,1)‚Äù parameter allows you to change the range\n\ndefault range for the MinMaxScaler is [0,1]\n\n\n\nOrdinal\nencode_cat = ColumnTransformer(\n¬† [('cat', OrdinalEncoder(), make_column_selector(dtype_include='category'))],\n¬† remainder='passthrough'\n)\nCategorical DType (like R factor type)\n\nAssign predefined unordered values so that whenever some value for some column does not exist in the training set, receiving such a value in the test set would not lead to incorrectly assigned labels or any other logical error.\n\nWith multiple categorical columns, it can be difficult to stratify them in the training/test splits\nMust know all possible categories for each categorical variable\n\nimport pandas as pd\nfrom pandas import CategoricalDtype\n\ndef transform_data(df: pd.DataFrame, target: pd.Series, frac: float = 0.07,\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† random_state: int = 42) -&gt; pd.DataFrame:\n\n¬† ¬† \"Transform non-numeric columns into categorical type and clean data.\"\n\n¬† ¬† categories_map = {\n¬† ¬† ¬† ¬† ¬† 'gender': {1: 'male', 2: 'female'},\n¬† ¬† ¬† ¬† ¬† 'education': {1: 'graduate', 2: 'university', 3: 'high_school',\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† 4: 'others', 5: 'others', 6: 'others', 0: 'others'},\n¬† ¬† ¬† ¬† ¬† 'marital_status': {1: 'married', 2: 'single', 3: 'others', 0: 'others'}\n¬† ¬† ¬† ¬† }\n\n¬† ¬† for col_id, col_map in categories_map.items():\n¬† ¬† ¬† ¬† ¬† ¬† df[col_id] = df[col_id].map(col_map).astype(\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† CategoricalDtype(categories=list(set(col_map.values())))\n¬† ¬† ¬† ¬† ¬† ¬† )\n\nOne-hot encoding\n\n** Don‚Äôt use pandas get_dummies because it doesn‚Äôt handle categories that aren‚Äôt in your test/production set **\n\nWonder if get_dummies creates 1 less dummy than the number of categories like it should\n\nBasic\n# Create a one-hot encoder\nonehot = OneHotEncoder()\n\n# Create an encoded feature\nencoded_features = onehot.fit_transform(churn[[\"Marital_Status\"]]).toarray()\n\n# Create DataFrame with the encoded features\nencoded_df = pd.DataFrame(encoded_features, columns=onehot.categories_)\n\nPipeline\nfrom sklearn.preprocessing import OneHotEncoder\n# one hot encode categorical features\nohe = OneHotEncoder(handle_unknown='ignore')\n\nfrom sklearn.pipeline import Pipeline\n# store one hot encoder in a pipeline\ncategorical_processing = Pipeline(steps=[('ohe', ohe)])\n\nfrom sklearn.compose import ColumnTransformer\n# create the ColumnTransormer object\npreprocessing = ColumnTransformer(transformers=[('categorical', categorical_processing, ['gender', 'job'])],\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† remainder='passthrough')\nClass Imbalance\n\nSMOTE with {{imblearn}}\nfrom sklearn.datasets import make_classification\nfrom imblearn.over_sampling import SMOTE\nX_train, y_train = make_classification(n_samples=500, n_features=5, n_informative=3)\nX_res, y_res = SMOTE().fit_resample(X_train, y_train)",
    "crumbs": [
      "Model Building",
      "sklearn"
    ]
  },
  {
    "objectID": "qmd/model-building-sklearn.html#sec-modbld-sklearn-pip",
    "href": "qmd/model-building-sklearn.html#sec-modbld-sklearn-pip",
    "title": "sklearn",
    "section": "Pipelines",
    "text": "Pipelines\n\nMisc\n\nHelps by creating more maintainable and clearly written code. Reduces mistakes by transforming train/test sets automatically.\nPipeline objects are estimators and can be serialized and saved like any other estimator\nimport joblib\n\n#saving the pipeline into a binary file\njoblib.dump(pipe, 'wine_pipeline.bin')\n\n#loading the saved pipeline from a binary file\npipe = joblib.load('wine_pipeline.bin')\n\n\n\nBasic Feature Transforming and Model Fitting Pipeline\n\nformat: Pipeline(steps = [(‚Äò&lt;step1_name&gt;‚Äô, function), (‚Äò&lt;step2_name&gt;‚Äô, function)])\nExample\nnp.random.seed(2019)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n\n# creating the pipeline with its different steps\n# fitting the pipeline with data\n# making predictions\npipe = Pipeline([\n¬† ¬† ('feature_selection', VarianceThreshold(threshold=0.1)),\n¬† ¬† ('scaler', StandardScaler()),\n¬† ¬† ('model', KNeighborsClassifier())\n])\npipe.fit(X_train, y_train)\npredictions = pipe.predict(X_test)\n\n#checking the accuracy\naccuracy_score(y_test, predictions) #sklearn function; multi-class: accuracy, binary: jaccard index (similarity)\n\nPipeline transforms according to the sequence of the steps inserted into the list of tuples\n\n\n\n\nColumn Transformers\n\nExample: transform by column type\n# creating pipeline for numerical features\nnumerical_pipe = Pipeline([\n¬† ¬† ('imputer', SimpleImputer(missing_values=np.nan, strategy='mean')),\n¬† ¬† ('scaler', StandardScaler())\n])\n\n# creating pipeline for categorical features\ncategorical_pipe = Pipeline([\n¬† ¬† ('imputer', SimpleImputer(missing_values=np.nan, strategy='most_frequent')),\n¬† ¬† ('one_hot', OneHotEncoder(handle_unknown='ignore'))\n])\npreprocessor = ColumnTransformer([\n¬† ¬† ('numerical', numerical_pipe, make_column_selector(dtype_include=['int', 'float'])),\n¬† ¬† ('categorical', categorical_pipe, make_column_selector(dtype_include=['object', 'category'])),\n])\npipe = Pipeline([\n¬† ¬† ('column_transformer', preprocessor),\n¬† ¬† ('model', KNeighborsClassifier())\n])\n\npipe.fit(X_train, y_train)\npredictions = pipe.predict(X_test)\n\nColumnTransformertakes list of tuples: name, transformer, columns\n\nn_jobs, verbose args also available\nremainder=‚Äúpassthrough‚Äù says all other columns not listed are ignored (might be a default)\n\nCan also provide a transformer object\n\nmethods: fit_transform, get_feature_names_out, get_params, etc\n\nmake_column_selector allows your to select the type of column to apply the transformer (docs)\n\nExample: Apply sequence of transformations to a column\ncol_transformer = ColumnTransformer(\n¬† ¬† [\n¬† ¬† ¬† (\n¬† ¬† ¬† ¬† ¬† \"PriceTransformerPipeline\",\n¬† ¬† ¬† ¬† ¬† # Pipeline -&gt; multiple transformation steps\n¬† ¬† ¬† ¬† ¬† Pipeline([\n¬† ¬† ¬† ¬† ¬† ¬† (\"MeanImputer\"¬† ¬† ¬† , SimpleImputer(strategy=\"median\")),\n¬† ¬† ¬† ¬† ¬† ¬† (\"LogTransformation\", FunctionTransformer(lambda value: np.log1p(value)) ),\n¬† ¬† ¬† ¬† ¬† ¬† (\"StdScaler\",¬† ¬† ¬† ¬† StandardScaler() ),\n¬† ¬† ¬† ¬† ¬† ]),\n¬† ¬† ¬† ¬† ¬† [\"price\"]\n¬† ¬† ¬† ¬† ),\n¬† ¬† ],\n¬† ¬† remainder=SimpleImputer(strategy=\"constant\", fill_value=-1)\n¬† )\n\nFor the ‚Äúprice‚Äù columns, it replaces Nulls with median, then log transforms, then standardizes. All other columns get their Nulls replaces with -1.\n\n\n\n\nFunction Transformers\n\nMisc\n\nIf using a method in the sklearn.preprocessing module, then able to use fit, transform, and fit_transform methods (I think)\nData must be the first argument of the function\ninverse_func argument for FunctionTransformer allows you to include a back-transform function\n\nSteps\n\nCreate function that transforms data\nCreate FunctionTransformer object using function as the argument\nAdd function-tranformer to the pipeline by including it as an argument to make_pipeline\n\nExample: Make numerics 32-bit instead 64-bit to save memory\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import FunctionTransformer\n\ndef reduce_memory(X: pd.DataFrame, y=None):\n¬† ¬† \"\"\"Simple function to reduce memory usage by casting numeric columns to float32.\"\"\"\n¬† ¬† num_cols = X.select_dtypes(incluce=np.number).columns\n¬† ¬† for col in num_cols:\n¬† ¬† ¬† ¬† X[col] = X.astype(\"float32\")\n¬† ¬† return X, y\n\nReduceMemoryTransformer = FunctionTransformer(func = reduce_memory)\n\n# Plug into a pipeline\n&gt;&gt;&gt; make_pipeline(SimpleImputer(), ReduceMemoryTransformer)\n\nData goes through the SimpleImputer first then the ReduceMemoryTransformer\n\n\n\n\nCustom Transformers Classes\n\nMisc\n\nFor more complex transforming tasks\n\nSteps\n\nCreate a class that inherits from BaseEstimator and TransformerMixin classes of sklearn.base\n\nInheriting from these classes allows Sklearn pipelines to recognize our classes as custom estimators and automatically adds fit_transform to your class\n\nAdd transforming methods to Class\n\nClass Skeleton\nclass CustomTransformer(BaseEstimator, TransformerMixin):\n¬† ¬† def __init__(self):\n¬† ¬† ¬† ¬† pass\n¬† ¬† def fit(self):\n¬† ¬† ¬† ¬† pass\n¬† ¬† def transform(self):\n¬† ¬† ¬† ¬† pass\n¬† ¬† def inverse_transform(self):\n¬† ¬† ¬† ¬† pass\nExample: Log transforming outcome variable\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.preprocessing import PowerTransformer\n\nclass CustomLogTransformer(BaseEstimator, TransformerMixin):\n\n¬† ¬† def __init__(self):\n¬† ¬† ¬† ¬† self._estimator = PowerTransformer()¬† # init a transformer\n\n¬† ¬† def fit(self, X, y=None):\n¬† ¬† ¬† ¬† X_copy = np.copy(X) + 1¬† # add one in case of zeroes\n¬† ¬† ¬† ¬† self._estimator.fit(X_copy)\n¬† ¬† ¬† ¬† return self\n\n¬† ¬† def transform(self, X):\n¬† ¬† ¬† ¬† X_copy = np.copy(X) + 1\n¬† ¬† ¬† ¬† return self._estimator.transform(X_copy)¬† # perform scaling\n\n¬† ¬† def inverse_transform(self, X):\n¬† ¬† ¬† ¬† X_reversed = self._estimator.inverse_transform(np.copy(X))\n¬† ¬† ¬† ¬† return X_reversed - 1¬† # return subtracting 1 after inverse transform\n\nreg_lgbm = lgbm.LGBMRegressor()\nfinal_estimator = TransformedTargetRegressor(\n¬† ¬† regressor=reg_lgbm, transformer=CustomLogTransformer()\n)\nfinal_estimator.fit(X_train, y_train)\n\nfit returns the tranformer itself since it returns self\n\nEstimates the optimal parameter lambda for each feature\n\ntransform returns transformed features\n\nApplies the power transform to each feature using the estimated lambdas in fit\n\nfit_transform does both at once\ncustom_log.fit(tps_df)¬†\ntransformed_tps = custom_log.transform(tps_df)\n# or\ntransformed_tps = custom_log.fit_transform(tps_df)\ninverse_transform returns the back-transformed features\nTransformedTargetRegressor transforms the targets y before fitting a regression model. The predictions are mapped back to the original space via an inverse transform. It takes as an argument the regressor that will be used for prediction, and the transformer that will be applied to the target variable\n\nThe regressor parameter accepts both regressors or pipelines that end with regressors\nIf the transformer is a function, like np.log, you can pass it to func argument\n\n\nExample: Dummy transformer + FeatureUnion (article)\n\nThis classes (below) that inherit this class with get these methods along with fit_transform\nclass DummyTransformer(BaseEstimator, TransformerMixin):\n  \"\"\"\n  Dummy class that allows us to modify only the methods that interest us,\n  avoiding redudancy.\n  \"\"\"\n  def __init__(self):\n  ¬† ¬† return None\n\n  def fit(self, X=None, y=None):\n  ¬† ¬† return self\n\n  def transform(self, X=None):\n  ¬† ¬† return self\nTransformer classes\nclass Preprocessor(DummyTransformer):\n\"\"\"\n  Class used to preprocess text\n\"\"\"\ndef __init__(self, remove_stopwords: bool):\n¬† ¬† self.remove_stopwords = remove_stopwords\n¬† ¬† return None\n\ndef transform(self, X=None):\n¬† ¬† preprocessed = X.apply(lambda x: preprocess_text(x, self.remove_stopwords)).values\n¬† ¬† return preprocessed\n\nclass SentimentAnalysis(DummyTransformer):\n\"\"\"\n¬† ¬† Class used to generate sentiment\n¬† ¬† \"\"\"\n¬† ¬† def transform(self, X=None):\n     ¬† ¬† sentiment = X.apply(lambda x: get_sentiment(x)).values\n     ¬† ¬† return sentiment.reshape(-1, 1) # &lt;-- note the reshape to transfor\n\nEach class inherits the dummy transformer and its methods\npreprocess_text and get_sentiment are user functions that are defined earlier in the article\n\nCreate pipeline\nvectorization_pipeline = Pipeline(steps=[\n  ('preprocess', Preprocessor(remove_stopwords=True)), # the first step is to preprocess the text\n  ('tfidf_vectorization', TfidfVectorizer()), # the second step applies vectorization on the preprocessed text\n  ('arr', FromSparseToArray()), # the third step converts a sparse matrix into a numpy array in order to show it in a            dataframe\n])\n\nTfidVectorizer and FromSparseArray are other classes in the article that I didn‚Äôt include in the Transformer classes chunk to save space\n\nCombine transformed features\n# vectorization_pipeline is a pipeline within a pipeline\nfeatures = [\n  ('vectorization', vectorization_pipeline),\n  ('sentiment', SentimentAnalysis()),\n  ('n_chars', NChars()),\n  ('n_sentences', NSententences())\n]\ncombined = FeatureUnion(features) # this is where we merge the features together\n\n# Get col names: subsets the second step of the second object in the vectorization_pipeline to retrieve¬†\n# the terms generated by the tf-idf then adds the other three column names to it\ncols = vectorization_pipeline.steps[1][1].get_feature_names() + [\"sentiment\", \"n_chars\", \"n_sentences\"]\nfeatures_df = pd.DataFrame(combined.transform(df['corpus']), columns=cols)\n\nPipelines are combined with FeatureUnion, features are transformed, and coerced into a pandas df which can be used to train a model\nNChars and NSentences are other classes in the article that I didn‚Äôt include in the Transformer classes chunk to save space\n\n\n\n\n\nTuning a Pipeline\n\nExample\nparameters = {\n¬† ¬† 'column_transformer__numerical__imputer__strategy': ['mean', 'median'],\n¬† ¬† 'column_transformer__numerical__scaler': [StandardScaler(), MinMaxScaler()],\n¬† ¬† 'model__n_neighbors': [3, 6, 10, 15],\n¬† ¬† 'model__weights': ['uniform', 'distance'],\n¬† ¬† 'model__leaf_size': [30, 40]\n}\n\n# defining a scorer and a GridSearchCV instance\nmy_scorer = make_scorer(accuracy_score, greater_is_better=True)\nsearch = GridSearchCV(pipe, parameters, cv=3, scoring=my_scorer, n_jobs=-1, verbose=1)\n\n# search for the best hyperparameter combination within our defined hyperparameter space\nsearch.fit(X_train, y_train)\n\n# changing pipeline parameters to gridsearch results\npipe.set_params(**search.best_params_)\n\n# making predictions\npredictions = pipe.predict(X_test)\n\n# checking accuracy\naccuracy_score(y_test, predictions)\n\nSee Column Transformer section example for details on this pipeline\nNote the double underscore used in the keys of the parameter dict\n\nDouble underscores separate names of steps inside a nested pipeline with last name being the argument of the transformer function being tuned\nExample: ‚Äòcolumn_transformer__numerical__imputer__strategy‚Äô\n\ncolumn_transformer (pipeline step name) &gt;&gt; numerical (pipeline step name) &gt;&gt; imputer (pipeline step name) &gt;&gt; strategy (arg of SimpleImputer function)\n\n\nView tuning results\nbest_params = search.best_params_\nprint(best_params)\n\n# Stores the optimum model in best_pipe\nbest_pipe = search.best_estimator_\nprint(best_pipe)\n\nresult_df = DataFrame.from_dict(search.cv_results_, orient='columns')\nprint(result_df.columns)\n\n\n\n\nDisplay Pipelines in Jupyter\n\nfrom sklearn import set_config¬†\nset_config(display=\"diagram\")\ngiant_pipeline",
    "crumbs": [
      "Model Building",
      "sklearn"
    ]
  },
  {
    "objectID": "qmd/model-building-sklearn.html#sec-modbld-sklearn-algs",
    "href": "qmd/model-building-sklearn.html#sec-modbld-sklearn-algs",
    "title": "sklearn",
    "section": "Algorithms",
    "text": "Algorithms\n\nMisc\nHistogram-based Gradient Boosting Regression Tree\n\nHistogram-based models are more efficient since they bin the continuous features\nInspired by LightGBM. Much faster than GradientBoostingRegressor for big datasets (n_samples &gt;= 10 000).\n\nsklearn.ensemble.HistGradientBoostingRegressor\n\nuser guide\n\nStochastic Gradient Descent (SGD)\n\nalgorithm\nNot a class of models, just merely an optimization technique\n\nSGDClassifier(loss='log') results in logistic regression, i.e.¬†a model equivalent to LogisticRegression which is fitted via SGD instead of being fitted by one of the other solvers in LogisticRegression.\nSGDRegressor(loss='squared_error', penalty='l2') and Ridge solve the same optimization problem, via different means.\n\nPenalyzed regression hyperparameters are labelled different than in R\n\nlambda (R) is alpha (py)\nalpha (R) is 1 - L1_ratio (py)\n\nCan be successfully applied to large-scale and sparse machine learning problems often encountered in text classification and natural language processing Advantages:\n\nEfficiency.\nEase of implementation (lots of opportunities for code tuning). Disadvantages:\nSGD requires a number of hyperparameters such as the regularization parameter and the number of iterations.\nSGD is sensitive to feature scaling.\n\nProcessing\n\nMake sure you permute (shuffle) your training data before fitting the model or use shuffle=True to shuffle after each iteration (used by default).\nFeatures should be standardized using e.g.¬†make_pipeline(StandardScaler(), SGDClassifier())\n\n\nBisectingKMeans\n\nCentroid is picked progressively (instead of simultaneously) based on the previous cluster. We would split the cluster each time until the number of K is achieved\nAdvantages\n\nIt would be more efficient with a large number of clusters\nCheaper computational costs\nIt does not produce empty clusters\nThe clustering result was well ordered and would create a visible hierarchy.\n\n\nXGBoost with GPU\ngbm = xgb.XGBClassifier(\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† n_estimators=100000,\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† max_depth=6,\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† objective=\"binary:logistic\",\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† learning_rate=.1,\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† subsample=1,\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† scale_pos_weight=99,\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† min_child_weight=1,\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† colsample_bytree=1,\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† tree_method='gpu_hist',\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† use_label_encoder=False\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† )\neval_set=[(X_train,y_train),(X_val,y_val)]¬†\nfit_model = gbm.fit(¬†\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† X_train, y_train,¬†\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† eval_set=eval_set,\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† eval_metric='auc',\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† early_stopping_rounds=20,\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† verbose=True¬†\n¬† ¬† ¬† ¬† ¬† ¬† ¬† )\n\ntree_method='gpu_hist' specifies the use of GPU\nPlot importance\nfig,ax2 = plt.subplots(figsize=(8,11))\nxgb.plot_importance(gbm, importance_type='gain', ax=ax2)\n\nGBM with Quantile Loss (PIs)\ngbm_lower = GradientBoostingRegressor(\n¬† ¬† loss=\"quantile\", alpha = alpha/2, random_state=0\n)\ngbm_upper = GradientBoostingRegressor(\n¬† ¬† loss=\"quantile\", alpha = 1-alpha/2, random_state=0\n)\ngbm_lower.fit(X_train, y_train)\ngbm_upper.fit(X_train, y_train)\ntest['gbm_lower'] = gbm_lower.predict(X_test)\ntest['gbm_upper'] = gbm_upper.predict(X_test)",
    "crumbs": [
      "Model Building",
      "sklearn"
    ]
  },
  {
    "objectID": "qmd/model-building-sklearn.html#sec-modbld-sklearn-cv",
    "href": "qmd/model-building-sklearn.html#sec-modbld-sklearn-cv",
    "title": "sklearn",
    "section": "CV/Splits",
    "text": "CV/Splits\n\nK-Fold\n\nfrom sklearn.model_selection import KFold\ncv = KFold(n_splits=7, shuffle=True)\n\nShuffling: minimizes the risk of overfitting by breaking the original order of the samples\n\n\nStratifiedKFold\n\nfrom sklearn.model_selection import StratifiedKFold\ncv = StratifiedKFold(n_splits=7, shuffle=True, random_state=1121218)\n\nFor classification, class ratios are held to the same ratios in both the training and test sets.\n\nclass ratios are preserved across all folds and iterations.\n\n\nLeavePOut: from sklearn.model_selection import LeaveOneOut, LeavePOut\n\nData is so limited that you have to perform a CV where you set aside only a few rows of data in each iteration\nLeaveOneOut is P = 1 for LeavePOut\n\nShuffleSplit, StratifiedShuffleSplit\n\nfrom sklearn.model_selection import ShuffleSplit, StratifiedShuffleSplit\ncv = ShuffleSplit(n_splits=7, train_size=0.75, test_size=0.25)\ncv = StratifiedShuffleSplit(n_splits=7, test_size=0.5)\n\nNot a CV, just repeats the train/test split process multiple times\nUsing different random seeds should resemble a robust CV process if done for enough iterations\n\nTimeSeriesSplit\n\nfrom sklearn.model_selection import TimeSeriesSplit\ncv = TimeSeriesSplit(n_splits=7)\n\nWith time series data, the ordering of samples matters.\n\nGroup Data\n\nData is not iid (e.g.¬†multi-level data)\nOptions\n\nGroupKFold\nStratifiedGroupKFold\nLeaveOneGroupOut\nLeavePGroupsOut\nGroupShuffleSplit\n\nWorks just like the non-group methods but with a group arg for the grouping variable",
    "crumbs": [
      "Model Building",
      "sklearn"
    ]
  },
  {
    "objectID": "qmd/outliers.html",
    "href": "qmd/outliers.html",
    "title": "Outliers",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Outliers"
    ]
  },
  {
    "objectID": "qmd/outliers.html#sec-outliers-eda",
    "href": "qmd/outliers.html#sec-outliers-eda",
    "title": "Outliers",
    "section": "",
    "text": "Also see Anomaly Detection for ML methods\nPackages\n\n{robustmatrix} (vignette) - Robust covariance estimation for matrix-valued data and data with Kronecker-covariance structure using the Matrix Minimum Covariance Determinant (MMCD) estimators and outlier explanation using and Shapley values.\n\nExamples of matrix data would be image resolution and repeated measueres (e.g different time points, different spatial locations, different experimental conditions, etc)\n\n\nResources\n\nNeed to examine this article more closely, Taking Outlier Treatment to the Next Level\n\nDiscusses detailed approach to diagnosing outliers , eda, diagnostics, robust regression, winsorizing, nonlinear approaches for nonrandom outliers.\n\nFor Time Series, see bkmks, pkgs in time series &gt;&gt; cleaning/processing &gt;&gt; outliers",
    "crumbs": [
      "Outliers"
    ]
  },
  {
    "objectID": "qmd/outliers.html#eda",
    "href": "qmd/outliers.html#eda",
    "title": "Outliers",
    "section": "EDA",
    "text": "EDA\n\nIQR\n\nObservations above \\(q_{0.75} + (1.5 \\times \\text{IQR})\\) are considered outliers\nObservations below \\(q_{0.25} - (1.5 \\times \\text{IQR})\\) are considered outliers\nWhere \\(q_{0.25}\\) and \\(q_{0.75}\\) correspond to first and third quartile respectively, and IQR is the difference between the third and first quartile\n\nHampel Filter\n\nObservations above \\(\\text{median} + (3 \\times \\text{MAD})\\) are considered outliers\nObservations below \\(\\text{median} - (3 \\times \\text{MAD})\\) are considered outliers\nUse mad(vec, constant = 1)¬† for the MAD",
    "crumbs": [
      "Outliers"
    ]
  },
  {
    "objectID": "qmd/outliers.html#sec-outliers-tests",
    "href": "qmd/outliers.html#sec-outliers-tests",
    "title": "Outliers",
    "section": "Tests",
    "text": "Tests\n\n** All tests assume data is from a Normal distribution **\nSee the EDA section for ways to find potential outliers to test\nGrubbs‚Äôs Test\n\nTest either a maximum or minimum point\n\nIf you suspect multiple points, you have remove the max/min points above/below the suspect point. Then test the subsetted data. Repeat as necessary\n\nH0: There is no outlier in the data.\nHa: There is an outlier in the data.\nTest statistics\n\\[\nG = \\frac {\\bar{Y} - Y_{\\text{min}}}{s}G = \\frac {Y_{\\text{max}} - \\bar{Y}}{s}\n\\]\n\nStatistics for whether the minimum or maximum sample value is an outlier\n\nThe maximum value is outlier if\n\\[\nG &gt; \\frac {N-1}{\\sqrt{N}} \\sqrt{\\frac {t^2_{\\alpha/(2N),N-2}}{N-2+t^2_{\\alpha/(2N),N-2}}}\n\\]\n\n‚Äú&lt;‚Äù for minimum\nt is denotes the critical value of the t distribution with (N-2) degrees of freedom and a significance level of Œ±/(2N).\nFor testing either the maximum or minimum value, use a significance level of level of Œ±/N\n\nRequirements\n\nNormally distributed\nMore than 7 observations\n\noutliers::grubbs.test(x, type = 10, opposite = FALSE, two.sided = FALSE)\n\nx: a numeric vector of data values\ntype=10: check if the maximum value is an outlier, 11 = check if both the minimum and maximum values are outliers, 20 = check if one tail has two outliers.\nopposite:\n\nFALSE (default): check value at maximum distance from mean\nTRUE: check value at minimum distance from the mean\n\ntwo-sided: If this test is to be treated as two-sided, this logical value indicates that.\n\nsee bkmk for examples\n\nDixon‚Äôs Test\n\nTest either a maximum or minimum point\n\nIf you suspect multiple points, you have remove the max/min points above/below the suspect point. Then test the subsetted data. Repeat as necessary.\n\nMost useful for small sample size (usually n‚â§25)\nH0: There is no outlier in the data.\nHa: There is an outlier in the data.\noutliers::dixon.test\n\nWill only accept a vector between 3 and 30 observations\n‚Äúopposite=TRUE‚Äù to test the maximum value\n\n\nRosner‚Äôs Test (aka generalized (extreme Studentized deviate) ESD test) Tests multiple points\n\nAvoids the problem of masking, where an outlier that is close in value to another outlier can go undetected.\nMost appropriate when n‚â•20\nH0: There are no outliers in the data set\nHa: There are up to k outliers in the data set\nres &lt;- EnvStats::rosnerTest(x,k)\n\nx: numeric vector\nk: upper limit of suspected outliers\nalpha: 0.05 default\nThe results of the test, res , is a list that contains a number of objects\n\nres$all.stats shows all the calculated statistics used in the outlier determination and the results\n\n‚ÄúValue‚Äù shows the data point values being evaluated\n‚ÄúOutlier‚Äù is True/False on whether the point is determined to be an outlier by the test\nRs are the test statistics\nŒªs are the critical values",
    "crumbs": [
      "Outliers"
    ]
  },
  {
    "objectID": "qmd/outliers.html#sec-outliers-meth",
    "href": "qmd/outliers.html#sec-outliers-meth",
    "title": "Outliers",
    "section": "Methods",
    "text": "Methods\n\nRemoval\n\nAn option if there‚Äôs sound reasoning (e.g.¬†data entry error, etc.)\n\nWinsorization\n\nA typical strategy is to set all outliers (values beyond a certain threshold) to a specified percentile of the data\nExample: A 90% winsorization would see all data below the 5th percentile set to the 5th percentile, and data above the 95th percentile set to the 95th percentile.\nPackages\n\n({DescTools::Winsorize})\n({datawizard::winsorize})",
    "crumbs": [
      "Outliers"
    ]
  },
  {
    "objectID": "qmd/outliers.html#sec-outliers-mod",
    "href": "qmd/outliers.html#sec-outliers-mod",
    "title": "Outliers",
    "section": "Models",
    "text": "Models\n\nBayes has different distributions for increasing uncertainty\nIsolation Forests - See Anomaly Detection &gt;&gt; Isolation Forests\nSupport Vector Regression (SVR) - See Algorithms, ML &gt;&gt; Support Vector Machines &gt;&gt; Regression\nExtreme Value Theory approaches\n\nfat tail stuff (need to finish those videos)\n\nRobust Regression (see bkmks &gt;&gt; Regression &gt;&gt; Other &gt;&gt; Robust Regression)\n\n{robustbase}\nCRAN Task View\n\nHuber Regression\n\nSee Loss Functions &gt;&gt; Huber Loss\nSee bkmks, Regression &gt;&gt; Generalized &gt;&gt; Huber\n\nTheil-Sen estimator\n\n{mblm}",
    "crumbs": [
      "Outliers"
    ]
  },
  {
    "objectID": "qmd/mathematics-statistics.html",
    "href": "qmd/mathematics-statistics.html",
    "title": "Statistics",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Mathematics",
      "Statistics"
    ]
  },
  {
    "objectID": "qmd/mathematics-statistics.html#sec-math-statc-",
    "href": "qmd/mathematics-statistics.html#sec-math-statc-",
    "title": "Statistics",
    "section": "",
    "text": "Age Adjustment of per 100K rate\n\n\nAllows communities with different age structures to be compared\nThe crude (unadjusted)1994 cancer mortality rate in New York State is 229.8 deaths per 100,000 men. The age-adjusted rate is 214.7 deaths per 100,000 men.\n\nNotice that 214.7 isn‚Äôt 229.8*1 of course but the sum of all the individual age-group weighted rates.\n\nProcess (Formulas in column headers)\n\nCalculate the disease‚Äôs rate per 100K for each age group\nMultiply the age-specific rates of disease by age-specific weights\n\nThe weights are the proportion of the US population within each age group. (e.g.¬†0-14 year olds are 28.4% of the 1970 US population)\n\nThe weighted rates are then summed over the age groups to give the (total) age-adjusted rate",
    "crumbs": [
      "Mathematics",
      "Statistics"
    ]
  },
  {
    "objectID": "qmd/mathematics-statistics.html#sec-math-statc-terms",
    "href": "qmd/mathematics-statistics.html#sec-math-statc-terms",
    "title": "Statistics",
    "section": "Terms",
    "text": "Terms\n\nCoefficient of Variation (CV) - aka Relative Standard Deviation (RSD) - aka Dispersion Parameter - Measure of the relative dispersion of data points in a data series around the mean. Usually expressed as a percentage.\n\\[\nCV = \\frac{\\sigma}{\\mu}\n\\]\n\nWhile most often used to analyze dispersion around the mean, a quartile, quintile, or decile CV can also be used to understand variation around the median or 10th percentile, for example.\nShould only be used with variables that have minimum at zero (e.g.¬†counts, prices) and not interval data (e.g celsius or fahrenheit)\nWhen the mean value is close to zero, the coefficient of variation will approach infinity and is therefore sensitive to small changes in the mean. This is often the case if the values do not originate from a ratio scale.\nA more robust possibility is the quartile coefficient of dispersion, half the interquartile range divided by the average of the quartiles (the midhinge)\n\\[\nCV_q = \\frac{0.5(Q_3 - Q_1)}{0.5(Q_3 + Q_1)}\n\\]\nFor small samples (Normal Distribution)\n\\[\nCV_*= (1 + \\frac{1}{4n}) CV\n\\]\nFor log-normal distribution\n\\[\nCV_{LN} = \\sqrt{e^{\\sigma^2_{ln}} - 1}\n\\]\n\nWhere \\(\\sigma_{ln}\\) is the standard deviation after a \\(\\ln\\) transformation of the data\n\n\nCovariance - Between two random variables is a measure of how correlated are their variations around their respective means\nGrand Mean - The mean of group means\n\nUse Cases\n\nHierarchical Group Comparison: Allows you to see how the average of each group relates to the overall average across all groups.\n\nExample: Imagine you collect data on the growth of various species of plants in different types of soil. Calculating the grand mean for plant height of each species allows you to see if any specific soil type consistently produces taller plants compared to the overall average.\n\nIdentifying Outliers: If the mean of a specific group deviates significantly from the grand mean, it might indicate the presence of outliers in that group.\nANOVA: It provides a baseline against which the individual group means are compared to assess if there are statistically significant differences between them.\n\nIt‚Äôs the intercept value when you use Sum-to-Zero (used in ANOVA) or Deviation contrasts (See Regression, Linear &gt;&gt; Contrasts &gt;&gt; Sum-to-Zero, Deviation)\n\nKernel Smoothing - Essence is the simple concept of a local average around a point, x; that is, a weighted average of some observable quantities, those of which closest to x being given the highest weights\nMargin of Error (MoE) - The range of values below and above the sample statistic in a confidence interval.\n\\[\n\\text{MOE}_\\gamma = z_\\gamma \\sqrt{\\frac{\\sigma^2}{n}}\n\\]\n\nZ-Score with confidence level Œ≥ ‚®Ø Standard Error\nIn general, for small sample sizes (under 30) or when you don‚Äôt know the population standard deviation, use a t-score to get the critical value. Otherwise, use a z-score.\n\nSee Null Hypothesis Significance Testing (NHST) &gt;&gt; Misc &gt;&gt; Z-Statistic Table for an example\n\nExample: a 95% confidence interval with a 4 percent margin of error means that your statistic will be within 4 percentage points of the real population value 95% of the time.\nExample: a Gallup poll in 2012 (incorrectly) stated that Romney would win the 2012 election with Romney at 49% and Obama at 48%. The stated confidence level was 95% with a margin of error of ¬± 2. We can conclude that the results were calculated to be accurate to within 2 percentages points 95% of the time.\n\nThe real results from the election were: Obama 51%, Romney 47%. So the Obama result was outside the range of the Gallup poll‚Äôs margin of error (2 percent).\n\n48 ‚Äì 2 = 46 percent\n48 + 2 = 50 percent\n\n\n\nNormalization - Rescales the values into a specified range, typically [0,1]. This might be useful in some cases where all parameters need to have the same positive scale. However, the outliers from the data set are lost.\n\\[\n\\tilde{X} = \\frac{X-X_{\\text{min}}}{X_{\\text{max}}-X_{\\text{min}}}\n\\]\n\nSome functions have the option to normalize with range [-1, 1]\nBest option if the distribution of your data is unknown.\n\nParameter - Describes an entire population (also see statistic)\nP-Value - \\(\\text{p-value}(y) = \\text{Pr}(T(y_{\\text{future}}) &gt;= T(y) | H)\\)\n\n\\(H\\) is a ‚Äúhypothesis,‚Äù a generative probability model\n\\(y\\) is the observed data\n\\(y_{\\text{future}}\\) are future data under the model\n\\(T\\) is a ‚Äútest statistic,‚Äù some pre-specified specified function of data\n\nSampling Error - The difference between population parameter and the statistic that is calculated from the sample (such as the difference between the population mean and sample mean).\nStandard Error of the Mean (SEM) - Measures how far the sample mean (average) of the data is likely to be from the true population mean (Also see Fundamentals &gt;&gt; Interpreting s.d., s.e.m, and CI Bars)\n\\[\n\\text{SEM} = \\frac{\\text{SD}}{\\sqrt{n}}\n\\]\n\nAssumes a simple random sample with replacement from an infinite population\n\nStandardization rescales data to fit the Standard Normal Distribution which has a mean (Œº) of 0 and standard deviation (œÉ) of 1 (unit variance).\n\\[\n\\tilde X = \\frac{X-\\mu}{\\sigma}\n\\]\n\nRecommended for PCA and if your data is known to come from a Gaussian distribution.\n\nStatistic - Describes a sample (also see parameter)\nVariance (\\(\\sigma^2\\))- Measures variation of a random variable around its mean.\nVariance-Covariance Matrix - Square matrix containing variances of the fitted model‚Äôs coefficient estimates and the pair-wise covariances between coefficient estimates.\n\\[\n\\begin{align}\n&\\text{Cov}(\\hat\\beta) = (X^TX)^{-1} \\cdot \\text{MSE}\\\\\n&\\text{where}\\;\\; \\text{MSE} = \\frac{\\text{SSE}}{\\text{DSE}} = \\frac{\\text{SSE}}{n-p}\n\\end{align}\n\\]\n\nDiagnonal is the variances, and the rest of the values are covariances\nThere‚Äôs also a variance/covariance matrix for error terms\nExample\nx &lt;- sin(1:100)\ny &lt;- 1 + x + rnorm(100)\nMSE &lt;- sum(residuals(lm(y ~ x))^2)/98 # where 98 is n-2\nvcov_mat &lt;- MSE * solve(crossprod(cbind(1, x)))\n\nvcov_mat is the same as vcov(lm(y ~ x))",
    "crumbs": [
      "Mathematics",
      "Statistics"
    ]
  },
  {
    "objectID": "qmd/mathematics-statistics.html#sec-math-statc-nhst",
    "href": "qmd/mathematics-statistics.html#sec-math-statc-nhst",
    "title": "Statistics",
    "section": "Null Hypothesis Significance Testing (NHST)",
    "text": "Null Hypothesis Significance Testing (NHST)\n\nMisc\n\nWhy would we not always use a non-parametric test so we do not have to bother about testing for normality? The reason is that non-parametric tests are usually less powerful than corresponding parametric tests when the normality assumption holds. Therefore, all else being equal, with a non-parametric test you are less likely to reject the null hypothesis when it is false if the data follows a normal distribution. It is thus preferred to use the parametric version of a statistical test when the assumptions are met.\nIf your statistic value is greater than the critical value, then it‚Äôs significant and you reject H0\n\n\nThink in terms of a distribution with statistic values on the x-axis, and greater than the critical value means you‚Äôre in the tail (one-sided)\n\nT-Statistic Table\nZ-Statistic Table\n\nExample: 95% CI ‚Üí Œ± = 100% - 95% = 0.05 ‚Üí Œ±/2 (1-tail) = 0.025\n\n1 - 0.025 = 0.975 (subtract from 1 because the z-score table cells are for the area left of the critical value\nThe z-score is 1.96 for a 95% CI\n\n\nZ-score comes from adding the row value with the column value that has the cell value of our area (e.g.¬†0.975) left of the critical value\nIf the area was between 0.97441 and 0.97500, then the z-score would be the row value, 1.9, added to the column value that‚Äôs half way between 0.05 and 0.06, which results in a z-score of 1.955\n\n\n\n\nType I Error - False-Positive; occurs if an investigator rejects a null hypothesis that is actually true in the population\n\nThe models perform equally well, but the A/B test still produces a statistically significant result. As a consequence, you may roll out a new model that doesn‚Äôt really perform better.\nYou can control the prevalence of this type of error with the p-value threshold. If your p-value threshold is 0.05, then you can expect a Type I error in about 1 in 20 experiments, but if it‚Äôs 0.01, then you only expect a Type I error in only about 1 in 100 experiments. The lower your p-value threshold, the fewer Type I errors you can expect.\n\nType II Error - False-Negative; occurs if the investigator fails to reject a null hypothesis that is actually false in the population\n\nThe new model is in fact better, but the A/B test result is not statistically significant.\nYour test is underpowered, and you should either collect more data, choose a more sensitive metric, or test on a population that‚Äôs more sensitive to the change.\n\nType S Error (Sign Error): The A/B test shows that the new model is significantly better than the existing model, but in fact the new model is worse, and the test result is just a statistical fluke. This is the worst kind of error, as you may roll out a worse model into production which may hurt the business metrics.\n\n{retrodesign} - Provides tools for working with Type S (Sign) and Type M (Magnitude) errors. (Vignette)\n\nType M error (Magnitude Error): The A/B test shows a much bigger performance boost from the new model than it can really provide, so you‚Äôll over-estimate the impact that your new model will have on your business metrics.\n\n{retrodesign} - Provides tools for working with Type S (Sign) and Type M (Magnitude) errors. (Vignette)\n\nFalse Positive Rate (FPR)(\\(\\alpha\\)) - ; Probability of a type I error; Pr(measured effect is significant | true effect is ‚Äúnull‚Äù)\n\\[\n\\text{FPR} = \\frac{v}{m_0}\n\\]\n\n\\(v\\): Number of times there‚Äôs a false positive\n\\(m_0\\): Number of non-significant variables\n\nFalse Discovery Rate (FDR) - Pr(measured effect is null | true effect is significant)\n\n\\[\n\\text{FDR} = \\frac{\\alpha \\pi_0}{\\alpha \\pi_0 + (1-\\beta)(1-\\pi_0)}\n\\]\n\n\\(\\alpha\\) - Type I error rate (False Positive Rate)\n\\(\\beta\\) - Type II error rate (False Negative Rate)\n\\(1-\\beta\\) - Power\n\\(\\pi_0\\) - Count of true null effects\n\\(1‚àí\\pi_0\\) - Count of true non-null effects\n\nPower - 1-Œ≤ where beta is the Probability of a type II error\nFamily-Wise Error Rate (FWER) - the risk of at least one false positive in a family of S hypotheses.\n\\[\nFWER = \\frac{v}{R}\n\\]\n\n\\(v\\): Number of times there‚Äôs a false positive\n\\(R\\): Number of times we claim Œ≤ ‚â† 0\ne.g.¬†Using the same data and variables to fit multiple models with different outcome variables (i.e.¬†different hypotheses)\n\nRomano and Wolf‚Äôs Ccorrection\n\nAccounting for the dependence structure of the p-values (or of the individual test statistics) produces more powerful procedures than Bonferroi and Holms. This can be achieved by applying resampling methods, such as bootstrapping and permutations methods.\n\nPermutation tests of regression coefficients can result in rates of Type I error which exceed the nominal size, and so these methods are likely not ideal for such applications\n\nSee Stata docs of the procedure\nPackages\n\n{wildrwolf}: Implements Romano-Wolf multiple-hypothesis-adjusted p-values for objects of type fixest and fixest_multi from the fixest package via a wild cluster bootstrap.",
    "crumbs": [
      "Mathematics",
      "Statistics"
    ]
  },
  {
    "objectID": "qmd/mathematics-statistics.html#sec-math-statc-boot",
    "href": "qmd/mathematics-statistics.html#sec-math-statc-boot",
    "title": "Statistics",
    "section": "Bootstrapping",
    "text": "Bootstrapping\n\nMisc\n\nPost-Hoc Analysis, General &gt;&gt; Frequentist &gt;&gt; Bootstrap\nDo NOT bootstrap the standard deviation (article)\n\nBootstrap is ‚Äúbased on a weak convergence of moments‚Äù\nif you use an estimate based standard deviation of the bootstrap, you are being overly conservative (i.e.¬†you‚Äôre overestimating the sd and CIs are too wide)\n\nBootstrapping uses the original, initial sample as the population from which to resample, whereas Monte Carlo simulation is based on setting up a data generation process (with known values of the parameters of a known distribution). Where Monte Carlo is used to test drive estimators, bootstrap methods can be used to estimate the variability of a statistic and the shape of its sampling distribution\nUse bias-corrected bootstrapped CIs (article)\n\n‚Äúpercentile and BCa methods were the only ones considered here that were guaranteed to return a confidence interval that respected the statistic‚Äôs sampling space. It turns out that there are theoretical grounds to prefer BCa in general. It is‚Äùsecond-order accurate‚Äù, meaning that it converges faster to the correct coverage. Unless you have a reason to do otherwise, make sure to perform a sufficient number of bootstrap replicates (a few thousand is usually not too computationally intensive) and go with reporting BCa intervals.‚Äù\n\nPackages\n\n{rsample}\n{DescTools::BootCI}\nboot and boot.ci\n{ebtools::get_boot_ci}\n\n\nSteps\n\nResample with replacement\nCalculate statistic of resample\nStore statistic\nRepeat 10K or so times\nCalculate mean, sd, and quantiles for CIs across all collected statistics\n\nBayesian Bootstrapping (aka Fractional Weighted Bootstrap)\n\nMisc\n\nNotes from\n\nThe Bayesian Bootstrap\nThread\n\nPackages\n\n{fwb}\n\n\nDescription\n\nDoesn‚Äôt resample the dataset, but samples a set of weights from the Uniform Dirichlet distribution and computes weighted averages (or whatever statistic)\nWeights sum to ‚Äòn‚Äô but may be non-integers\nEach row gets a frequency weight based on the number of times they appear\nIn this way, every row is included in the analysis but given a fractional weight that represents its contribution to the statistic\n\nIn a traditional bootstrap, some rows of data may not be sampled and therefore excluded from the calculation of the statistic\n\nParticularly useful with rare events, where a row excluded from a traditional bootstrap sample might cause the whole estimation to explode (e.g., in a rare-events logistic regression where one sample has no events!)\n\n\n\nShould be faster and consume less RAM\nPython implementation\ndef classic_boot(df, estimator, seed=1):\n¬† ¬† df_boot = df.sample(n=len(df), replace=True, random_state=seed)\n¬† ¬† estimate = estimator(df_boot)\n¬† ¬† return estimate\n\ndef bayes_boot(df, estimator, seed=1):\n¬† ¬† np.random.seed(seed)\n¬† ¬† w = np.random.dirichlet(np.ones(len(df)), 1)[0]\n¬† ¬† result = estimator(df, weights=w)\n¬† ¬† return result\n\nfrom joblib import Parallel, delayed\ndef bootstrap(boot_method, df, estimator, K):\n¬† ¬† r = Parallel(n_jobs=8)(delayed(boot_method)(df, estimator, seed=i) for i in range(K))\n¬† ¬† return r\n\ns1 = bootstrap(bayes_boot, dat, np.average, K = 1000)",
    "crumbs": [
      "Mathematics",
      "Statistics"
    ]
  },
  {
    "objectID": "qmd/mathematics-statistics.html#sec-math-statc-desc",
    "href": "qmd/mathematics-statistics.html#sec-math-statc-desc",
    "title": "Statistics",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics\n\nMeans\n\nGeometric Mean\nsummarize_revenue &lt;- function(tbl) {\n¬† ¬† tbl %&gt;%\n¬† ¬† ¬† ¬† summarize(geom_mean_revenue = exp(mean(revenue)),\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† n = n())\n}\n\n\n\nProportions\n\nVariance of a proportion\n\nAssume that p applies equally to all n subjects\n\\[\n\\sigma^2_p = \\frac{p(1-p)}{n}\n\\]\n\nExample\n\nSample of 100 subjects where there are 40 females and 60 males\n10 of the females and 30 of the males have the disease\n\nMarginal estimate of the probability of disease is (30+10)/100 = 0.4\n\nVariance of the estimator assuming constant risk (i.e.¬†assuming risk for females = risk for males)\n\n(prob_of_disease √ó prob_not_disease) / n = (0.4 √ó 0.6) / 100 = 0.0024\n\np = (10 + 30) / 100 = 0.40\n\n\n\n\nAssume p depends on a variable (e.g.¬†sex)\n\\[\n\\sigma^2_p = \\frac{p^2_{1,n} \\cdot p_1(1-p_1)}{n_1} + \\frac{p^2_{2,n} \\cdot p_2(1-p_2)}{n_2}\n\\]\n\nExample\n\nDescription same as above\nAdjusted marginal estimate of the probability of disease is\n\n(prop_female √ó prop_disease_female) + (prop_male √ó prop_disease_male)\n(0.4 √ó 0.25) + (0.6 √ó 0.5) = 0.4\nSame marginal estimate as before\n\nVariance of the estimator assuming varying risk (i.e.¬†assumes risk for females \\(\\neq\\) risk for males)\n\n1st half of equation:\n\nprop_female2 √ó (prop_disease_female √ó prop_not_disease_female) / n_female = [0.42 √ó (0.25 √ó 0.74) / 40]\n\n2nd half of equation\n\nprop_male2 √ó (prop_disease_male √ó prop_not_disease_male) / n_male = [0.62 √ó (0.5 √ó 0.5) / 60]\n\n1st half + 2nd half = 0.00224\n\nVariance is smaller than before\n\n\n\n\n\nCIs\n\nPackages:\n\n{binomCI} - 12 confidence intervals for one binomial proportion or a vector of binomial proportions are computed\n\nJeffrey‚Äôs Interval\n# probability of event\n# n_rain in the number of events (rainy days)\n# n is the number of trials (total days)\nmutate(pct_rain = n_rain / n, \n          # jeffreys interval\n          # bayesian CI for binomial proportions\n          low = qbeta(.025, n_rain + .5, n - n_rain + .5), \n          high = qbeta(.975, n_rain + .5, n - n_rain + .5))\n\n\n\n\nSkewness\n\nPackages:\n\n{moments} - Standard algorithm\n{e1071} - 3 alg options\n{DescTools::Skew} - Same algs but with bootstraps CIs\n\nFrom the paper referenced in e1071, b1 (type 3) is better for non-normal population distributions and G1 (type 2) is better for normal population distributions\nSymmetric: Values between -0.5 to 0.5\nModerated Skewed data: Values between¬†-1 and -0.5¬†or between¬†0.5 and 1\nHighly Skewed data: Values¬†less than -1¬†or¬†greater than 1\nRelationship between Mean and Median under different skewness\n\n\n\n\nKurtosis\n\nA high kurtosis distribution has a sharper peak and longer fatter tails, while a low kurtosis distribution has a more rounded peak and shorter thinner tails.\nTypes\n\n\nMesokurtic: kurtosis = ~3\n\nExamples: normal distribution. Also binomial distribution when p = 1/2 +/- sqrt(1/12)\n\nLeptokurtic: This distribution has fatter tails and a sharper peak. Excess kurtosis &gt; 3\n\nExamples: Student‚Äôs t-distribution, Rayleigh distribution, Laplace distribution, exponential distribution, Poisson distribution and the logistic distribution\n\nPlatykurtic: The distribution has a lower and wider peak and thinner tails. Excess kurtosis &lt; 3\n\nExamples: continuous and discrete uniform distributions, raised cosine distribution, and especially the Bernoulli distribution\n\nExcess Kurtosis is the kurtosis value - 3\n\n\n\n\nUnderstanding CI, SD, and SEM Bars\n\narticle\nP-values test whether the sample means are different from each other\nsd bars: Show the population spread around each sample mean. Useful as predictors of the range of new sample.\n\nNever seen these and it seems odd to mix a sample statistic with a population parameter and that the range is centered on the sample mean (unless the sample size is large I guess).\n\ns.e.m. is the ‚Äústandard error of the mean‚Äù (See Terms)\n\nIn large samples, the s.e.m. bar can be interpreted as a 67% CI.\n95% CI ‚âà 2 √ó s.e.m. (n &gt; 15)\n\nFigure 1\n\n\nEach plot shows 2 points representing 2 sample means\nPlot a: bars of both samples touch and are the same length\n\nsem bars intepretation: Commonly held view that ‚Äúif the s.e.m. bars do not overlap, the difference between the values is statistically significant‚Äù is NOT correct. Bars touch here but don‚Äôt overlap and the difference in sample means is NOT significant.\n\nPlot b: p-value = 0.05 is fixed\n\nsd bar interpretation: Although the means differ, and this can be detected with a sufficiently large sample size, there is considerable overlap in the data from the two populations.\nsem bar intepretation: For there to be a significant difference in sample means, sem bars have to much further away from each other than there just being a recognizable space between the bars.\n\n\nFigure 2\n\n\nPlot a: shows how a\n\n95% CI captures the population mean 95% of the time but as seen here, only 18 out of 20 sample CIs (90%) contained the population mean (i.e.¬†this is an asymptotic claim)\nA common misconception about CIs is an expectation that a CI captures the mean of a second sample drawn from the same population with a CI% chance. Because CI position and size vary with each sample, this chance is actually lower.\n\nPlot b:\n\nHard to see at first but the outer black bars are the 95% CI and the inner gray bars are the sem.\nBoth the CI and sem shrink as n increases and the sem is always encompassed by the CI\n\n\nFigure 3\n\n\nsem bars must be separated by about 1 sem (which is half a bar) for a significant difference to be reached at p-value = 0.05\n95% CI bars can overlap by as much as 50% and still indicate a significant difference at p-value = 0.05\n\nIf 95% CI bars just touch, the result is highly significant (P = 0.005)",
    "crumbs": [
      "Mathematics",
      "Statistics"
    ]
  },
  {
    "objectID": "qmd/mathematics-statistics.html#sec-math-statc-desc-pvfun",
    "href": "qmd/mathematics-statistics.html#sec-math-statc-desc-pvfun",
    "title": "Statistics",
    "section": "P-Value Function",
    "text": "P-Value Function\n\nNotes from https://ebrary.net/72024/health/value_confidence_interval_functions\n{concurve} creates these curves\nGives a more complete picture than just stating the p-value (strength and precision of the estimate)\n\nShows level of precision of the point estimate via shape of the curve\n\nnarrow-based, spikey curves = more precise\n\nVisualizes strength of the effect along the x-axis. Helps in showing ‚Äúsignificant‚Äù effect is not necessarily a meaningful effect.\n\nShows other estimate(s) that are also consistent with that p-value\nShows p-values associated with other estimates for the Null Hypothesis\n\nsee the end of the article for discussion on using this fact in an interpretation context\n\nThe P-value function is closely related to the set of all confidence intervals for a given estimate. (see example 3)\nExample 1\n\n\np-value of the point estimate is (always?) 1 which says, ‚Äúgiven a null hypothesis =  is true (i.e.¬†the true risk ratio = ),¬† the probability of seeing data produce this estimate or this estimate with more strength (ie smaller std error) is 100%.‚Äù\n\nI.e. the pt est is the estimate most compatible with the data.\nThis pval language is mine. The whole ‚Äúthis data or data more extreme‚Äù has never sit right with me. I think this is more correct if my understanding is right.\n\nThe pval for the data in this example is at 0.08 for a H0 of 1. So unlikely, but typically not unlikely enough in order to reject the null hypothesis.\nA pval of 0.08 is identical for a pt est = 1 or pt est = 10.5\nWide base of the curve indicates the estimate is imprecise. There‚Äôs potentially a large effect or little or no effect.\n\nExample 2\n\n\nmore data used for the second curve which indicates a precise point estimate.\npt est very close to H0\npval = 0.04 (not shown in plot)\n\nso the arbitrary pval = 0.05 threshold is passed and says a small effect is probably present\nIs that small of an effect meaningful even if it‚Äôs been deemed statistically present?\n\nIn this case a plot with the second curve helps show that ‚Äústatistically significant‚Äù doesn‚Äôt necessarily translate to meaningful effect\n\nExample 3\n\n\nThe different confidence intervals reflect the same degree of precision (i.e.¬†the curve width doesn‚Äôt change when moving from one CI to another).\nThe three confidence intervals are described as nested confidence intervals. The P-value function is a graph of all possible nested confidence intervals for a given estimate, reflecting all possible levels of confidence between 0% and 100%.",
    "crumbs": [
      "Mathematics",
      "Statistics"
    ]
  },
  {
    "objectID": "qmd/cli-linux.html#sec-cli-lin-fun",
    "href": "qmd/cli-linux.html#sec-cli-lin-fun",
    "title": "Linux",
    "section": "Functions",
    "text": "Functions\n\nBasic\nsay_hello() {\n  echo \"hello\"\n}\nsay_hello\nUsing Return\nfailing_func () {\n  return 1\n}\n\nreturn cannot take strings ‚Äî only numbers 1 to 255\n\nWith arguments\nsay_hello() {\n  echo \"Hello $1 and $2\"\n}\nsay_hello \"Ahmed\" \"Layla\"\nDeclaring local and global variables\nsay_hello() {\n  local x\n  x=$(date)\n  y=$(date)\n}\n\nlocal is a keyword\nx is local and y is global\n\nSuppress errors\nlocal x=$(moose)\n\nWhen local is used in the same line as the variable declaration, then the variable never errors. e.g.¬†Even if moose doesn‚Äôt exist, this line won‚Äôt trigger an error",
    "crumbs": [
      "CLI",
      "Linux"
    ]
  },
  {
    "objectID": "qmd/spreadsheets.html",
    "href": "qmd/spreadsheets.html",
    "title": "Spreadsheets",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Spreadsheets"
    ]
  },
  {
    "objectID": "qmd/spreadsheets.html#sec-spdsht-misc",
    "href": "qmd/spreadsheets.html#sec-spdsht-misc",
    "title": "Spreadsheets",
    "section": "",
    "text": "Resources\n\nSpreadsheet Munging Strategies\n\nSome Excel files are binaries and in order to use download.file, you must set mode = ‚Äúwb‚Äù\ndownload.file(url, \n              destfile = glue(\"{rprojroot::find_rstudio_root_file()}/data/cases-age.xlsx\"), \n              mode = \"wb\")\nIndustry studies show that 90 percent of spreadsheets containing more than 150 rows have at least one major mistake.",
    "crumbs": [
      "Spreadsheets"
    ]
  },
  {
    "objectID": "qmd/spreadsheets.html#sec-spdsht-cats",
    "href": "qmd/spreadsheets.html#sec-spdsht-cats",
    "title": "Spreadsheets",
    "section": "Catastrophes",
    "text": "Catastrophes\n\nReleasing confidential information\n\nIrish police accidently handed out officers private information when sharing sheets with statistics due to a freedom of information request. (link)\n\nErrors when combining sheets\n\nWales dismissed anaesthesiologists after mistakenly deeming them ‚Äúunappointable.‚Äù Spreadsheets from different areas lacked standardization in formatting, naming conventions, and overall structure. To make matters worse, data was manually copied and pasted between various spreadsheets, a time-consuming and error-prone process. (link)\nWhen consolidating assets from different spreadsheets, the spreadsheet data was not ‚Äúcleaned‚Äù and formatted properly. The Icelandic bank‚Äôs shares were subsequently undervalued by as much as ¬£16 million. (link)\n\nData entry errors\n\nCryto.com accidentally transferred $10.5 million instead of $100 into the account of an Australian customer due to an incorrect number being entered on a spreadsheet. (link)\nNorway‚Äôs $1.5tn sovereign wealth fund lost $92M, on an error relating to how it calculated its mandated benchmark. A person used the wrong date, December 1st instead of November 1st. (link)",
    "crumbs": [
      "Spreadsheets"
    ]
  },
  {
    "objectID": "qmd/spreadsheets.html#sec-spdsht-bprac",
    "href": "qmd/spreadsheets.html#sec-spdsht-bprac",
    "title": "Spreadsheets",
    "section": "Best Practices",
    "text": "Best Practices\n\nNotes from Data organization in spreadsheets\n\nBe consistent\nWrite dates like YYYY-MM-DD\nDon‚Äôt leave any cells empty\nPut just one thing in a cell\nOrganize the data as a single rectangle (with subjects as rows and variables as columns, and with a single header row)\nCreate a data dictionary\nDon‚Äôt include calculations in the raw data files\nDon‚Äôt use font color or highlighting as data\nChoose good names for things\nMake backups\nUse data validation to avoid data entry errors\nSave the data in plain text files.",
    "crumbs": [
      "Spreadsheets"
    ]
  },
  {
    "objectID": "qmd/spreadsheets.html#sec-spdsht-transspr",
    "href": "qmd/spreadsheets.html#sec-spdsht-transspr",
    "title": "Spreadsheets",
    "section": "Transitioning from Spreadsheet to DB",
    "text": "Transitioning from Spreadsheet to DB\n\nMisc\n\nWhen you start to have multiple datasets or when you want to make use of several columns in one table and other columns in another table you should consider going the local database route.\nUse db ‚Äúnormalization‚Äù to figure out a schema\nAlso see\n\nDatabases, Normalization\nDatabases, Warehouses &gt;&gt; Design a Warehouse\n\n\nDB advantages over spreadsheets:\n\nEfficient analysis: Relational databases allow information to be retrieved quicker to then be analyzed with SQL (Structured Query Language), to then run queries.\n\nOnce spreadsheets get large, they can lag or freeze when opening, editing, or performing simple analyses in them.\n\nCentralized data management: Since relational databases often require a certain type or format of data to be input into each column of a table, it‚Äôs less likely that you‚Äôll end up with duplicate or inconsistent data.\nScalability: If your business is experiencing high growth, this means that the database will expand, and a relational database can accommodate an increased volume of data.\n\nStart documenting the spreadsheets\n\nFile Names, File Paths\nUnderstand where values are coming from\n\nSource (e.g.¬†department, store, sensor), Owner\n\nHow rows of data are being generated\n\nWho/What is inputting the data\n\nHow does each spreadsheet/notebooks/set of spreadsheets fit in the company‚Äôs business model\n\nHow are they being used and by whom\n\nMap the spreadsheets relationships to one another\n\nSee Databases, Warehouses &gt;&gt; Design a Warehouse",
    "crumbs": [
      "Spreadsheets"
    ]
  },
  {
    "objectID": "qmd/econometrics-fixed-effects.html",
    "href": "qmd/econometrics-fixed-effects.html",
    "title": "Fixed Effects",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Econometrics",
      "Fixed Effects"
    ]
  },
  {
    "objectID": "qmd/econometrics-fixed-effects.html#sec-econ-fe-misc",
    "href": "qmd/econometrics-fixed-effects.html#sec-econ-fe-misc",
    "title": "Fixed Effects",
    "section": "",
    "text": "Model with independent intercepts for each time point and/or case, which are called ‚Äúfixed effects‚Äù\n\nThe effects the omitted variables have on the subject at one time, they will also have the same effect at a later time; hence their effects will be constant, or ‚Äúfixed.‚Äù\nA ‚Äúfixed effect‚Äù in statistics is a non-random regression term, while a ‚Äúfixed effect‚Äù in econometrics means that the coefficients in a regression model are time-invariant\n\nNotes from\n\nhttps://www.econometrics-with-r.org/10-rwpd.html\nhttps://www.robertkubinec.com/post/fixed_effects/\n\nPackages\n\n{plm}\n\nFunctions for model estimation, testing, robust covariance matrix estimation, panel data manipulation and information.\n\n{fixest}\n\nFast estimation, has parallel option, glm option and many other features\n\n{estimatr}\n\nProviding a range of commonly-used linear estimators, designed for speed and for ease-of-use. Users can easily recover robust, cluster-robust, and other design appropriate estimates.\nUsers can choose an estimator to reflect cluster-randomized, block-randomized, and block-and-cluster-randomized designs.\n\n{{panelsplit}} (article)- CV for panel data prediction. It seems to take an expanded window approach. Also see Cross-Validation &gt;&gt; Time Series.\n\nIf you used {plm} + {coeftest} and want stata errors, then vcov = vcovCL",
    "crumbs": [
      "Econometrics",
      "Fixed Effects"
    ]
  },
  {
    "objectID": "qmd/econometrics-fixed-effects.html#sec-econ-fe-terms",
    "href": "qmd/econometrics-fixed-effects.html#sec-econ-fe-terms",
    "title": "Fixed Effects",
    "section": "Terms",
    "text": "Terms\n\nCoarse Clustering - Grouping the data into larger clusters or units. Each cluster represents a broader and more aggregated subset of observations (as compared to Fine Clustering).\n\nCan lead to lower variance in the estimated standard errors because it captures less of the within-cluster variation.\nMay be used when there is less within-cluster heteroscedasticity or correlation, or when computational efficiency is a concern.\n\nFine Clustering - Grouping the data into small clusters or units. Each cluster represents a relatively small and specific subset of observations in the dataset.\n\nCan lead to higher variance in the estimated standard errors because it captures more of the within-cluster variation.\nAppropriate when there is a substantial degree of heteroscedasticity or correlation within these small clusters.\n\nFixed Panel - When the same set of units/people/cases is tracked throughout the study\nHomogeneous (or Pooled) - Panel data models that assume the model parameters are common across individuals.\nHeterogeneous - Panel models allow for any or all of the model parameters to vary across individuals.\n\nFixed effects and random effects models are both examples of heterogeneous panel data models.\n\nRotating Panel - When the units/people/cases change during the study",
    "crumbs": [
      "Econometrics",
      "Fixed Effects"
    ]
  },
  {
    "objectID": "qmd/econometrics-fixed-effects.html#sec-econ-fe-consid",
    "href": "qmd/econometrics-fixed-effects.html#sec-econ-fe-consid",
    "title": "Fixed Effects",
    "section": "Considerations",
    "text": "Considerations\n\nFixed Effects or Random Effects (aka mixed effects model)?\n\nIf there‚Äôs likely correlation between unobserved group/cases variables (e.g.¬†individual talent) and treatment variable (i.e.¬†E(Œ±|x) != 0) AND there‚Äôs substantial variance between group units, then FE is a better choice (see 1-way assumptions or Econometrics, Mixed Effects, Frequentist &gt;&gt; Assumptions for more details)\nIf cases units change little, or not at all, across time, a fixed effects model may not work very well or even at all (SEs for a FE model will be large)\n\nThe FE model is for analyzing within-units variance\n\nDo we wish to estimate the effects of variables whose values do not change across time, or do we merely wish to control for them?\n\nFE: these effects aren‚Äôt estimated but adjusted for by explicitly including a separate intercept term for each individual (Œ±i) in the regression equation\nRE: estimates these effects (might be biased if RE assumptions violated)\nThe RE model is for analyzing between-units variance\n\nThe amount of within-unit variation relative to between-unit variation has important implications for these two approaches\n\nArticle with simulated data showed that within variation around sd &lt; 0.5 didn‚Äôt detect the effect of explanatory variable but ymmv (depends on # of units, observations per unit, N)\n\nDurbin‚ÄìWu‚ÄìHausman test (plm::phtest)\n\nIf H0 is not rejected, then both FE and RE are consistent but only RE is efficient. ‚Äì&gt; use RE but if you have a lot of data, then FE is also fine.\nIf H0 is rejected, then only FE is consistent ‚Äì&gt; use FE\n\n\nValid research questions for using a fixed effect for:\n\nCases/Units (e.g.¬†State, school, individuals, stores) - ‚ÄúHow much does a case unit change relative to other case units?‚Äù\nTime (e.g.¬†Year) - ‚ÄúHow much does a case change in relation to itself over time?‚Äù\n\nHow much each case varies around its average. The larger this coefficient the more cases fluctuate in their outcomes\nExample: Survey data with individual incomes over time\n\nHow the measure is different in a particular year compared to the individual average (e.g., do they have a lower or higher income compared to their normal income).\n\n\nExamples\n\nWhether obtaining more education leads to higher earnings.\nWhether wealthier countries tend to be more democratic than poorer countries\n\n\nFixed Effects or First Difference Estimator (FD)?\n\nTaking the first difference is an alternative to the demeaning step in the FE model\nIf the error terms are homoskedastic with no serial correlation, the fixed effects estimator is more efficient than the first difference estimator.\nIf the error follows a random walk, however, the first difference estimator is more efficient. If T=2, then they are numerically equivalent, and for T &gt; 2, they are not.\n\nIs the panel data balanced?\n\nplm::is.pbalanced(&lt;data&gt;, index = c(\"&lt;id_var&gt;\", \"&lt;time_var&gt;\"))\nBalanced - Has the same number of observations for all groups/units at each time point\nUnbalanced - At least one group/unit is not observed every period\n\ne.g.¬†Have missing values at some time observations for some of the groups/units.\n\nCertain panel data models are only valid for balanced datasets.\n\nFor such models, data will need to be condensed to include only the consecutive periods for which there are observations for all individuals in the cross section.\n\n\nOmitted variable bias\n\nMultiple regression can correct for observable omitted variable bias, however, this cannot account for omitted unobservable factors that differ (e.g.¬†from state to state)¬†\n\nThis refers to doing two multivariable regression models - one for each time period\n\nFE models control for any omitted variables that are constant over time but vary between individuals by explicitly including a separate intercept term for each individual (\\(\\alpha_i\\)) in the regression equation\nYou can difference the outcome and difference predictor variables from period 1 to period 2 in order to remove the effects of unobserved omitted variables that are constant between the time periods\nFrom Kubinec differs regarding omitted variables\n\nAny statistical model should have, as its first requirement, that it match the researcher‚Äôs question. Problems of omitted variables are important, but necessarily secondary.\nFixed effects models do not control for omitted variables. What fixed effect models do is isolate one dimension of variance in the model. As a result, any variables that don‚Äôt vary on that dimension are by definition removed from the model. This side-effect is trumpeted as the great inferential benefit of fixed effect models, but it has nothing to do with inference. Fixed effects (or their cousin, random effects/hierarchical models) are simply about selecting which part of the panel dataset is most germane to the analysis.",
    "crumbs": [
      "Econometrics",
      "Fixed Effects"
    ]
  },
  {
    "objectID": "qmd/econometrics-fixed-effects.html#sec-econ-fe-pit",
    "href": "qmd/econometrics-fixed-effects.html#sec-econ-fe-pit",
    "title": "Fixed Effects",
    "section": "Pitfalls",
    "text": "Pitfalls\n\nBickel: If you performed matching on your sample, don‚Äôt condition on any of the matching variables\n\nCan result in collider bias and opening up a previously closed backdoor\nMe: Matching makes sense because FE model has ‚ÄúCommon Trends‚Äù assumption\n\nKubinec says\n\n2-way fixed models have big problems\n\nSlope Interpretation\n\nCases and time points are nested and we end up making comparisons across both dimensions simultaneously. There is no clear research question that matches this model.\nThe one known use of the model is for difference-in-difference estimation, but only with two time points. Says to read his paper for more details.\n\nIs this what the eor book is describing for unobserved omitted variables? (see above)\n\n\nSlope Value Unreliable\n\nOnly identifiable if there‚Äôs a different effect of¬†x¬†on¬†y¬†for each time point/case\n\nI think he‚Äôs saying if there is no variation in one of your fixed effects and you fit a two-way model anyways, the calculated effect is unreliable. He says the data looks normal and you wouldn‚Äôt recognize what happened necessarily.\n\nWhen this model is unidentifiable, R fixes the problem by deleting the last dummy variable (created by factor(fixed_effect_var)) and spits out the estimate.\n\nThe coefficient estimate for the removed dummy variable shows-up as an NA in the summary\n\n\n\nIt‚Äôs best to choose whether within-case or between-case effect is more important and fit the 1-way model.\n\ni.e.¬†It is important to think about which dimension is more relevant, and then go with that dimension.\nAssumptions for a model with just an cases fixed effect\n\nResiduals have mean = 0 (i.e.¬†errors uncorrelated with X)\n\nif violated, then omitted variable bias\n\nX (variable of interest) is i.i.d\n\nwithin-cases, autocorrelation is allowed (e.g.¬†states)\n\nlarge outliers unlikely\nno perfect multicollinearity between variables\n\n\n\nPotential danger of biased effects when treatment is assigned during different periods for each group\n\nExample: group 1 is untreated at periods 1 and 2 and treated at period 3, while group 2 is untreated at period 1 and treated both at periods 2 and 3\nWhen the treatment effect is constant across groups and over time, FE regressions estimate that effect under the standard ‚Äúcommon trends‚Äù assumption.\n\nRequires that the expectation of the outcome without treatment follow the same evolution over time in every group\n\nEstimates can be severely biased ‚Äì and may even be incorrectly signed ‚Äì when treatment effects change over time within treated units (aka hetergeneous treatment effects)\nFundamentally, the main reason TWFE estimates get weird and biased with differently-timed treatments is because of issues with weights‚Äîin TWFE settings, treated observations often get negative weights and vice versa\nAccording to Jakiela (2021, 5), negative weights in treated observations are more likely in (1) early adopter countries, since the country-level treatment mean is high, and (2) later years, since the year-level treatment mean is higher.\n\n\nSo, in general, the bias comes from entity variable categories that received the treatment early and the biased weight estimates occur on observations with later time values. This is because of the extreme treatment imbalance during these ranges/intervals, and its effect on the outcome variable.\n\nHaving negative weights on treated observations isn‚Äôt necessarily bad! It‚Äôs often just a mathematical artefact, and if you have (1) enough never-treated observations and (2) enough pre-treatment data, and if (3) the treatment effects are homogenous across all countries, it won‚Äôt be a problem. But if you don‚Äôt have enough data, your results will be biased and distorted for later years and for early adopters.\nDiagnostics\n\nDo any treated units get negative weight when calculating¬†Œ≤TWFE? Check this by looking at the weights\nCan we reject the hypothesis that the treatment effects are homogenous? Check this by looking at the relationship between¬†Yit and¬†Dit. The slope shouldn‚Äôt be different.\n\ntreatment effect homogeneity implies a linear relationship between residualized outcomes and residualized treatment after removing the fixed effects\n\n\nComments\n\nShe states that she‚Äôs only looking for linearity between the two sets of residuals, but actually breaks it down further by checking whether the relationship varies by treatment. This whole procedure is computing a partial correlation except instead of the last step of measuring the correlation between the two sets of residuals (e.g.¬†cor.test(treatment_resid, out_resid) and getting the p-value, she looks at an interaction.\nI don‚Äôt understand the homogeneity check in 3.2 though. She says that if the linearity relationship varies by treatment then this breaks assumptions for TWFE models. I‚Äôve only looked at her paper and the Chaisemartin paper, and the only assumptions I saw for TWFE models in general was the ‚Äúcommon trends‚Äù and the ‚Äústrong exogeneity‚Äù assumption. I think this is more likely to be about the ‚Äúcommon trends‚Äù assumption, and my understanding of that one is that it pertains to the effect across time for a particular group. I‚Äôm guessing there‚Äôs a connection between those two concepts, but I‚Äôm not seeing it.",
    "crumbs": [
      "Econometrics",
      "Fixed Effects"
    ]
  },
  {
    "objectID": "qmd/econometrics-fixed-effects.html#sec-econ-fe-clus",
    "href": "qmd/econometrics-fixed-effects.html#sec-econ-fe-clus",
    "title": "Fixed Effects",
    "section": "Clusters",
    "text": "Clusters\n\nMisc\n\nNotes from Cluster-robust inference: A guide to empirical practice (Paper)\n\nSections: 4.2 (level of clustering), 4.3 (level of clustering), 4.3.2 (influential clusters), 8.1 (infl clusters), 8.2 (placebo regression)\n\nAlso see Econometrics, General &gt;&gt; Standard Errors &gt;&gt; HC and HAC vcov estimators\n\nCluster-Robust Variance Estimators (CRVE)\n\n‚ÄúRandom-effects model is the only model within the class of factor models for which including cluster fixed effects can remove all intra-cluster dependence‚Äù\n\nThink this says that HC or HAC ({sandwich}) should be used for 2FE but not RE models\n\n‚ÄúEven very small intra-cluster correlations can have a large effect on standard errors when the clusters are large‚Äù\n‚ÄúIt has become quite standard in modern empirical practice both to include cluster fixed effects (and perhaps other fixed effects as well) and also to employ cluster-robust inference.‚Äù\n\nLevel of Clustering\n\n‚Äúone or more fine clusters nested within each of the coarse clusters‚Äù\n‚ÄúClustering at too fine a level generally leads to serious over-rejection, which becomes worse as the sample size increases with the numbers of clusters at all levels held constant‚Äù\n‚ÄúClustering at too coarse a level also leads to both some over-rejection and some loss of power, especially when the number of clusters is small.‚Äù\nIssues for Certain Rules of Thumb\n\nJust cluster at the coarsest feasible level\n\nMay be attractive when the number of coarse clusters G is reasonably large, but it can be dangerous when G is small, or when the clusters are heterogeneous in size or other features\n\nCluster at whatever level yields the largest standard error(s) for the coefficient(s) of interest\n\nWill often lead to the same outcome as the first one, but not always. When the number of clusters, G, is small, cluster-robust standard errors tend to be too small, sometimes much too small. Hence, the second rule of thumb is considerably less likely to lead to severe over-rejection than the first one. However, because it is conservative, it can lead to loss of power (or, equivalently, confidence intervals that are unnecessarily long).\n\n\nRecommended: Cluster at the treatment level\n\ne.g.¬†If the treatment is assigned by classroom then cluster by classroom\nBut if there‚Äôs concern of significant spillover effects, then cluster at a coarser level than the treatment level (e.g.¬†schools)\n\n\nDiagnostics\n\nStatistical testing for the correct level of clustering\n\nHard to tell but I don‚Äôt think any of the tests were recommended in the paper\n\nChecking for influential clusters\n\nInfluential Cluster - Estimates change a lot when it‚Äôs deleted.\n‚ÄúIn a few extreme cases, there may be a cluster \\(h\\) for which it is impossible to compute \\(Œ≤_j^{(h)}\\). If so, then the original estimates should probably not be believed.\n\nThis will happen, for example, when cluster \\(h\\) is the only treated one. Inference is extremely unreliable in that case.‚Äù\n\n\nPlacebo Regressions\n\nProcess\n\nAdd a random dummy variable to the model\nfit model check if dummy variable is significant\nrepeat many times\n\nBecause a placebo regressor is artificial, we would expect valid significance tests at level Œ± to reject the null close to Œ±% of the time when the experiment is repeated many times.\nExample:\n\nClustering at levels below state-level leads to rejection rates far greater than Œ±\nUsing a state-level CRVE is important for survey data that samples individuals from multiple states. If we fail to do so, we will find, with probability much higher than Œ±, that nonsense regressors apparently belong in the model.\n\ni.e.¬†placebo regressors are significant &gt; 5% of the time\n\n\nA placebo-regressor experiment should lead to over-rejection whenever both the regressor and the residuals display intra-cluster correlation at a coarser level than the one at which the standard errors are clustered. (e.g.¬†&lt; 5%)\nIf the placebo regressor is clustered at the coarse level, we would expect significance tests based on heteroskedasticity-robust standard errors to over-reject whenever the residuals are clustered at either level. Similarly, we would expect significance tests based on finely-clustered standard errors to over-reject whenever the residuals are clustered at the coarse level. Table 4 in Section 8.2 displays both of these phenomena.",
    "crumbs": [
      "Econometrics",
      "Fixed Effects"
    ]
  },
  {
    "objectID": "qmd/econometrics-fixed-effects.html#sec-econ-fe-owfe",
    "href": "qmd/econometrics-fixed-effects.html#sec-econ-fe-owfe",
    "title": "Fixed Effects",
    "section": "One-Way Fixed Effects",
    "text": "One-Way Fixed Effects\n\nOnly compares different periods within the same cases category and discards the between-cases variance Steps\nSteps\n\nRemove endogeneity (resulting from omitted variable bias)\n\nFirst, the error is broken into 2 parts\n\\[\n\\begin{align}\n&y_{it} = \\beta x_{it}+ \\nu_{it} \\\\\n&\\text{where}\\: \\nu_{it} = \\alpha_i + \\epsilon_{it} = 0\n\\end{align}\n\\]\n\n\\(\\alpha\\) is the cases/units-specific or between part of the error\n\nunit-specific heterogeneity, the error component that is constant over time\nIt‚Äôs the unit fixed effect, the unit-specific intercept.\n\n\\(\\epsilon\\) is time-varying or within part of the error\n\nIdiosyncratic, varying both over units and over time\n\n\nThen, each group (aka cases) is centered by each group‚Äôs mean\n\\[\n\\begin{align}\ny_{it}-\\bar y_i &= \\beta(x_{it} - \\bar x_i) + (\\alpha_i - \\alpha_i) + (\\epsilon_{it} - \\bar \\epsilon_i) \\\\\n\\tilde y_{it} &= \\beta \\tilde x_{it} + \\tilde \\epsilon_{it}\n\\end{align}\n\\]\n\nThe centering eliminates all between-group variance, including the person-specific part of the error term (\\(\\alpha_i\\)), and leaves only the within-group variability to analyze\n\n\\(\\alpha_i\\) is a constant so it‚Äôs mean is equal to itself\n\n\n\nOLS is performed after the endogeneity is removed.\n\nAssumptions\n\nFunctional Form\n\nAdditive fixed effect\nConstant and contemporaneous treatment effect (aka homogeneous treatment effects)\nLinearity in covariates\n\nTime-constant unobserved heterogeneity is allowed (not the case for Mixed Effects models)\n\ni.e.¬†\\(\\mathbb{E}(\\alpha|x) \\neq 0\\) or correlation between unobserved unit variables that are constant across time and \\(x\\) is allowed\n\nThis correlation is seen in the figure at the top of section\n\nEach group‚Äôs \\(x\\) values get larger from left to right as each group‚Äôs \\(\\alpha\\) (aka \\(y\\)-intercepts) for each unit get larger time-constant, unobserved variablesexplain variation between cases units\n\n\n\nStrong (strict) Exogeneity\n\n\\(\\mathbb{E}(\\epsilon|x,\\alpha)=0\\)\nTime-varying unobserved heterogeneity biases the estimator\nAlso see Pitfalls &gt;&gt; Kubinec\n\n\nExample\ne2 &lt;- plm(wage ~ marriage, data = df2,\n¬† ¬† ¬† ¬† ¬† index = c(\"id\", \"wave\"),\n¬† ¬† ¬† ¬† ¬† effect = \"individual\", model = \"within\")\n\nWhere marriage is the variable of interest, id is the cases variable and wave is the time variable\nUsing effect = ‚Äúindividual‚Äù, model = ‚Äúwithin‚Äù specifies a one-way fixed effects model",
    "crumbs": [
      "Econometrics",
      "Fixed Effects"
    ]
  },
  {
    "objectID": "qmd/econometrics-fixed-effects.html#sec-econ-fe-twfe",
    "href": "qmd/econometrics-fixed-effects.html#sec-econ-fe-twfe",
    "title": "Fixed Effects",
    "section": "Two-Way Fixed Effects (TWFE)",
    "text": "Two-Way Fixed Effects (TWFE)\n\nAdds a time variable that is constant (fixed effect) across cases but varies over time\n\\[\ny_{it} = \\beta x_{it} + \\alpha_i + \\zeta_t + \\epsilon_{it}\n\\]\n\nWhere \\(\\zeta\\) is the time fixed effect\n\nSteps\n\nRemove endogeneity (resulting from omitted variable bias)\n\nFirst, the error is broken into 2 parts: \\(\\nu_{it} = \\alpha_i + \\epsilon_{it} = 0\\)\n\nWhere \\(\\alpha\\) is the cases-specific or between part of the error and \\(\\epsilon\\) is time-varying or within part of the error\n\nThen,\n\\[\n(y_{it} - \\bar y_i -\\bar y_t + \\bar y) = \\beta(x_{it} - \\bar x_i - \\bar x_t + \\bar x) +(\\epsilon_{it} = \\bar \\epsilon_i - \\bar \\epsilon_t + \\bar \\epsilon)\n\\]\n\nFor each group/case, variables are centered by that group‚Äôs mean\nFor each period, variables are centered by that time period‚Äôs mean\nThe grand mean is added back\n\n\nOLS is performed after the endogeneity is removed.\n\nAssumptions\n\nTime-constant unobserved heterogeneity is allowed (See 1-way FE assumptions)\nFunctional Form\n\nAdditive fixed effect\nConstant and contemporaneous treatment effect\nLinearity in covariates\n\nStrong (strict) Exogeneity (also see 1-way FE assumptions)\n\n\\(Œµ \\perp D_{is}, X_{i}, \\alpha_i, \\zeta_t\\)\n\nThis implies the below statement\n\nTreatment assignment, \\(D_i\\), for a given unit, \\(i\\), in time, \\(s\\), is independent of the potential outcomes for that unit in that time period\n\\[\n{Y_{it}(0), Y_{it}(1)} \\perp D_{is}\\;|\\; \\boldsymbol{X}_i^{1:T}, \\alpha_i, \\boldsymbol{f}^{1:T} \\quad \\quad \\forall\\; i, t, s\n\\]\n\ne.g A policy (i.e.¬†treatment) doesn‚Äôt get enacted in region because it experiences negative economic shocks and we‚Äôre measuring some economic metric\nAs a result, if we only had observed outcomes (which of course is all we have), we can substitute either \\(Y_{it}(0)\\) or \\(Y_{it}(1)\\) depending on whether we observe \\(D_{is}= 1\\) or \\(D_{is}= 0\\) and we can still, at least theoretically, get an unbiased estimate of the treatment effect.\n\n\\(D\\) is the treatment variable so it‚Äôs \\(x_{it}\\) in the other equations above and here, \\(X\\) is probably other adjustment variables\n\\(f\\) is the time fixed effect\n\n\nImplies treatment status is assigned randomly or at one shot, not sequentially\n\n\nCommon Trends\n\nSee Fixed Effects with Individual Slopes (FEIS) section for models that relax this assumption\nFor \\(t \\geq 2, \\mathbb{E}(Y_{g,t}(0) ‚àí Y_{g,t‚àí1}(0))\\) does not vary across group, \\(g\\)\n\n\\(Y_{g,t}(0)\\) denotes average potential outcomes without treatment in group \\(g\\) at period \\(t\\).\n\\(Y_{g,t}(1)\\) would denote average potential outcomes with treatment in group \\(g\\) at period \\(t\\).\ni.e.¬†For each period after the first period, the expected change in outcome doesn‚Äôt vary across group \\(g\\)\n\nExample\n\n\nBefore treatment (getting married), wages for the treatment group (top 2 lines) were growing at a substantially faster rate than the control group (bottom two lines). This violates the Common Trends assumption\n\n\n\nExample:\nfe3 &lt;- \n  plm(wage ~ marriage, data = df2,\n¬† ¬† ¬† index = c(\"id\", \"wave\"),\n¬† ¬† ¬† effect = \"twoways\", \n¬† ¬† ¬† model = \"within\")\n\nWhere marriage is the variable of interest, id is the cases variable and wave is the time variable\nUsing effect = ‚Äútwoways‚Äù, model = ‚Äúwithin‚Äù specifies a two-way effects model\n\nExample:\n\nModel\n\\[\n\\begin{align}\nY_{it} &= \\beta_0 + \\beta_1 X_{it} + \\gamma_2 D2_i + \\cdots + \\gamma_n DT_i + \\delta_2B2_t + \\cdots + \\delta_n BT_t + u_{it} \\\\\n\\text{FatilityRate}_{it} &= \\beta_1 \\text{BeerTax}_{it} + \\text{StateEffects} + \\text{TimeFixedEffect} + u_{it}\n\\end{align}\n\\]\n\nIncluding the intercept would allow for a change in the mean fatality rate in the time between the years 1982 and 1988 in the absence of a change in the beer tax.\nThe variable of interest is Beer Tax and it‚Äôs effect on Fatality Rate\n\nBeer Tax is a continuous variable with a value for each unit and for each year\n\nThe state and time fixed effects are the dummy variables in the formal equation\nTheir coefficients start at 2 because the intercept coefficient is considered the first coefficient\n\nCode\n# Two ways to fit the model\n\nlm(fatal_rate ~ beertax + state + year - 1, data = Fatalities)\n\nfatal_tefe_mod &lt;- \n  plm::plm(fatal_rate ~ beertax,\n¬† ¬† ¬† ¬† ¬† ¬†data = Fatalities,\n¬† ¬† ¬† ¬† ¬† ¬†index = c(\"state\", \"year\"),\n¬† ¬† ¬† ¬† ¬† ¬†# fixed effects estimator is also called the 'within' estimator\n¬† ¬† ¬† ¬† ¬† ¬†model = \"within\",\n¬† ¬† ¬† ¬† ¬† ¬†effect = \"twoways\") # twoways required for \"entities\" and \"time\" fixed effects\n\n# only calcs for variable of interest\n# if needed, dof = nrow(dat) - 1\ncoeftest(fatal_tefe_mod, vcov = vcovHC, type = \"HC1\")\n#&gt; t test of coefficients:\n#&gt;¬†\n#&gt;¬† ¬† ¬† ¬† Estimate Std. Error t value Pr(&gt;|t|)¬†\n#&gt; beertax -0.63998¬† ¬† 0.35015 -1.8277¬† 0.06865 .\n\n# moar adjustment vars\nfatalities_mod6 &lt;- \n  plm::plm(fatal_rate ~ beertax + year + drinkage\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† + punish + miles + unemp + log(income),\n¬† ¬† ¬† ¬† ¬† ¬†index = c(\"state\", \"year\"),\n¬† ¬† ¬† ¬† ¬† ¬†model = \"within\",\n¬† ¬† ¬† ¬† ¬† ¬†effect = \"twoways\",\n¬† ¬† ¬† ¬† ¬† ¬†data = Fatalities)\n\nstate and year variables need to be factors\nIntercept removed because it has no meaning in this context\n\n\nExample: {estimatr}\nmodel_lm_robust &lt;- \n    estimatr::lm_robust(primary ~ treatment,\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† fixed_effects = ~ country + year,\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† data = fpe_primary,\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† clusters = country, se_type = \"stata\")\n\ntidy(model_lm_robust)\n##¬† ¬† ¬† ¬† term estimate std.error statistic p.value conf.low conf.high df outcome\n## 1 treatment¬† ¬† 20.4¬† ¬† ¬† 9.12¬† ¬† ¬† 2.24¬† 0.0418¬† ¬† 0.867¬† ¬† ¬† ¬† 40 14 primary\n\nglance(model_lm_robust)\n##¬† r.squared adj.r.squared statistic p.value df.residual nobs se_type\n## 1¬† ¬† 0.768¬† ¬† ¬† ¬† 0.742¬† ¬† ¬† ¬† NA¬† ¬† ¬† NA¬† ¬† ¬† ¬† ¬† 14¬† 490¬† stata\nExample: {fixest}\nmodel_feols &lt;- \n  fixest::feols(primary ~ treatment | country + year,\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† data = fpe_primary,\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† cluster = ~ country,\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† dof = dof(fixef.K = \"full\"))\n\ntidy(model_feols)\n## # A tibble: 1 √ó 5\n##¬† term¬† ¬† ¬† estimate std.error statistic p.value\n##¬† &lt;chr&gt;¬† ¬† ¬† ¬† &lt;dbl&gt;¬† ¬† &lt;dbl&gt;¬† ¬† &lt;dbl&gt;¬† &lt;dbl&gt;\n## 1 treatment¬† ¬† 20.4¬† ¬† ¬† 9.12¬† ¬† ¬† 2.24¬† 0.0418\n\nglance(model_feols)\n## # A tibble: 1 √ó 9\n##¬† r.squared adj.r.squared within.r.squared pseudo.r.squared sigma¬† nobs¬† AIC¬† BIC logLik\n##¬† ¬† ¬† &lt;dbl&gt;¬† ¬† ¬† ¬† &lt;dbl&gt;¬† ¬† ¬† ¬† ¬† ¬† &lt;dbl&gt;¬† ¬† ¬† ¬† ¬† ¬† &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;¬† &lt;dbl&gt;\n## 1¬† ¬† 0.768¬† ¬† ¬† ¬† 0.742¬† ¬† ¬† ¬† ¬† ¬† 0.111¬† ¬† ¬† ¬† ¬† ¬† ¬† NA¬† 14.7¬† 490 4071. 4280. -1985.\n\n# Standard print,summary output from a fixest model (from vignette)\nprint(fixest_pois_mod)\n#&gt; Poisson estimation, Dep. Var.: Euros\n#&gt; Observations: 38,325¬†\n#&gt; Fixed-effects: Origin: 15,¬† Destination: 15,¬† Product: 20,¬† Year: 10\n#&gt; Standard-errors: Clustered (Origin)¬†\n#&gt;¬† ¬† ¬† ¬† ¬† ¬† ¬† Estimate Std. Error t value¬† Pr(&gt;|t|)¬† ¬†\n#&gt; log(dist_km) -1.52787¬† 0.115678 -13.208 &lt; 2.2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:¬† 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; Log-Likelihood: -7.025e+11¬† Adj. Pseudo R2: 0.764032\n#&gt;¬† ¬† ¬† ¬† ¬† ¬† BIC:¬† 1.405e+12¬† ¬† Squared Cor.: 0.612021\n\n# With clustered SEs\nsummary(fixest_pois_mod, vcov = \"twoway\")\n#&gt; Poisson estimation, Dep. Var.: Euros\n#&gt; Observations: 38,325¬†\n#&gt; Fixed-effects: Origin: 15,¬† Destination: 15,¬† Product: 20,¬† Year: 10\n#&gt; Standard-errors: Clustered (Origin & Destination)¬†\n#&gt;¬† ¬† ¬† ¬† ¬† ¬† ¬† Estimate Std. Error¬† t value¬† Pr(&gt;|t|)¬† ¬†\n#&gt; log(dist_km) -1.52787¬† 0.130734 -11.6869 &lt; 2.2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:¬† 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; Log-Likelihood: -7.025e+11¬† Adj. Pseudo R2: 0.764032\n#&gt;¬† ¬† ¬† ¬† ¬† ¬† BIC:¬† 1.405e+12¬† ¬† Squared Cor.: 0.612021",
    "crumbs": [
      "Econometrics",
      "Fixed Effects"
    ]
  },
  {
    "objectID": "qmd/econometrics-fixed-effects.html#sec-econ-fe-feis",
    "href": "qmd/econometrics-fixed-effects.html#sec-econ-fe-feis",
    "title": "Fixed Effects",
    "section": "Fixed Effects with Individual Slopes (FEIS)",
    "text": "Fixed Effects with Individual Slopes (FEIS)\n\nFixed effects model that relaxes the Common Trends assumption (see 2-way FE assumptions above)\n\nGives each case (e.g.¬†State, school, individual, store) it‚Äôs own intercept and slope\nData are not cases ‚Äúdemeaned‚Äù like with a FE estimator, but ‚Äúdetrended‚Äù by the predicted individual slope of each cases unit\n\nMisc\n\nNotes from https://ruettenauer.github.io/Panel-Data-Analysis/Panel_part2.html#Fixed_Effects_Individual_Slopes\n{feisr}\n** Each additional slope variable requires more observations per cases category **\n\nEach cases unit needs at least q+1 observations to contribute to the model. If not, they are dropped.\n\nWhere q number of slope parameters (including a constant)\n\nMost likely this refers to the number of slope variables + constant\nExample: Slope variables are exp + I(exp^2)\n\nq = number_of_slope_vars + constant = 2 + 1 = 3 observations for each unit are required.\n\n\n(Probably not this) Example (Based on the feisr vignette): Slope variables are exp + I(exp^2)\n\nq = number_of_cases_units * (number_of_slope_vars + constant)\nq = number_of_ids * 3\nThis is the actual number of slope parameters estimated but this could be huge, so I doubt it‚Äôs this.\n\n\n\n\nModel Equation: \\(y_i = \\beta X_i + \\alpha_i W_i + \\epsilon_i\\)\n\n\\(W\\) is a matrix of slope variables\n\\(\\alpha\\) is a vector of estimated parameters for the slope variables\n\nProcess\n\nIt‚Äôs equivalent to a typical lm model except with dummies of your cases variable (e.g.¬†‚Äúid‚Äù below) and 2-way interaction terms for all combinations of dummies \\(\\times\\) each slope variable\nActual process (more efficient) (see article for more mathematical detail)\n\nEstimate the individual-specific predicted values for the dependent variable and each covariate based on an individual intercept and the additional slope variables of \\(W_i\\)\nDetrend the original data by these individual-specific predicted values\nRun an OLS model on the residual (‚Äòdetrended‚Äô) data\n\n\nExample: Does marrying increase (log) wages\nwages.feis &lt;- \n  feis(lnw ~ marry + enrol + yeduc + as.factor(yeargr)\n¬† ¬† ¬† ¬†| exp + I(exp^2), \n¬† ¬† ¬† ¬†data = mwp, \n¬† ¬† ¬† ¬†id = \"id\",\n¬† ¬† ¬† ¬†robust = TRUE)\n\nsummary(wages.feis)\n## Coefficients:\n##¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† Estimate Std. Error t-value¬† Pr(&gt;|t|)¬† ¬†\n## marry¬† ¬† ¬† ¬† ¬† ¬† ¬† 0.0134582¬† 0.0292771¬† 0.4597¬† 0.64579¬† ¬†\n## enrol¬† ¬† ¬† ¬† ¬† ¬† ¬† -0.1181725¬† 0.0235003 -5.0286 5.325e-07 ***\n## yeduc¬† ¬† ¬† ¬† ¬† ¬† ¬† -0.0020607¬† 0.0175059 -0.1177¬† 0.90630¬† ¬†\n## as.factor(yeargr)2 -0.0464504¬† 0.0378675 -1.2267¬† 0.22008¬† ¬†\n## as.factor(yeargr)3 -0.0189333¬† 0.0524265 -0.3611¬† 0.71803¬† ¬†\n## as.factor(yeargr)4 -0.1361305¬† 0.0615033 -2.2134¬† 0.02697 *¬†\n## as.factor(yeargr)5 -0.1868589¬† 0.0742904 -2.5152¬† 0.01196 *¬†\n## ---\n## Signif. codes:¬† 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n##¬†\n## Cluster robust standard errors\n## Slope parameters:¬† exp, I(exp^2)\n\nExperience (exp) is used for the slope variables.\nTo estimate the slope parameters, the relationship with wage (lnw) is assumed to be non-linear (exp + I(exp^2))\nInterpretation: Marrying doesn‚Äôt reliably affect wages (p-value = 0.64579)",
    "crumbs": [
      "Econometrics",
      "Fixed Effects"
    ]
  },
  {
    "objectID": "qmd/gnu-make.html",
    "href": "qmd/gnu-make.html",
    "title": "GNU Make",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "GNU Make"
    ]
  },
  {
    "objectID": "qmd/gnu-make.html#sec-make-misc",
    "href": "qmd/gnu-make.html#sec-make-misc",
    "title": "GNU Make",
    "section": "",
    "text": "Notes from\n\narticle\nproject\n\nGood example of an advanced makefile for a practical data science project\n\nVideo\n\nGoes through the differect components of executing a python project with make (e.g.¬†testing, clean-up, defining variables, setting up the virtual environment, etc.)\n\n\nResources\n\nDocs - All on one page so you can just ctrl + f\nNice little tutorial\nAnother tutorial\n\nHaven‚Äôt read it, but looks pretty thorough\n\nDocs for variable types\nDocs for functions\n\nProject orchestration system that only builds steps that have changed\n\n{drake}/{targets} are based on this system\n\nAssuming that you‚Äôve named the file ‚Äúmakefile‚Äù or ‚ÄúMakefile‚Äù or something like that, simply typing make at the command line while inside your project‚Äôs directory will execute the build process.\n\nmake -B --recon shows all the commands used to build the project (i.e.¬†kind of like a DAG)\nmake -B rebuilds the entire project even if no targets have changed",
    "crumbs": [
      "GNU Make"
    ]
  },
  {
    "objectID": "qmd/gnu-make.html#sec-make-gen",
    "href": "qmd/gnu-make.html#sec-make-gen",
    "title": "GNU Make",
    "section": "General",
    "text": "General\n\nSyntax\ntargets: prerequisites\ncommand\ncommand\ncommand\nThe prerequisites are also file names, separated by spaces. These files need to exist before the commands for the target are run. These are also called dependencies.\n.PHONY - Helpful to avoid conflicts between target names and file names\n\nConsidered best practice to use\nExample\n.PHONY: install\ninstall:\n        python3.9 -m venv venv && source venv/bin/activat && pip install -r requirements-dev.txt\n\n.PHONY: dbsetup\ndbsetup:\n        source venv/bin/activate && python -m youtube.db\n\n.PHONY: lint\nlint:\n        flake8 emojisearcher tests",
    "crumbs": [
      "GNU Make"
    ]
  },
  {
    "objectID": "qmd/gnu-make.html#sec-make-rul",
    "href": "qmd/gnu-make.html#sec-make-rul",
    "title": "GNU Make",
    "section": "Rules",
    "text": "Rules\n\nMakefiles are made-up of rules. Each rule is a code chunk.\nExample\n# Inside Makefile\ndata/raw/NIPS_1987-2015.csv:\ncurl -o $@ https://archive.ics.uci.edu/ml/machine-learning-databases/00371/NIPS_1987-2015.csv\n\nDownloads a file\n‚Äúdata/raw/NIPS_1987-2015.csv‚Äù is the file path and target for this rule.\n$@¬† is a Make automatic variable that fills in the target name for the file name arg in the curl command.\nThere is no prerequisite required for this command. So, this syntax is just target : command.",
    "crumbs": [
      "GNU Make"
    ]
  },
  {
    "objectID": "qmd/gnu-make.html#sec-make-tar",
    "href": "qmd/gnu-make.html#sec-make-tar",
    "title": "GNU Make",
    "section": "Targets",
    "text": "Targets\n\nThe targets are file names, separated by spaces. Typically, there is only one per rule.\nDummy Targets - A target with no commands directly associated with it (it is sort of a meta-target).\n\nUseful if you want to only rebuild part of the project\nExample: if you have a couple of scripts that involve data acquisition and cleaning, another few that involve data analysis, and a few that involve the presentation of results (paper, plot), then you might define a dummy for each of them.\nall: data model paper\ndata: raw.csv\nmodel: model.rds\npaper: plot.png paper.pdf\n\nExecuting make paper in the CLI and in the project directory will call the commands that built ‚Äúplot.png‚Äù and ‚Äúpaper.pdf‚Äù",
    "crumbs": [
      "GNU Make"
    ]
  },
  {
    "objectID": "qmd/gnu-make.html#sec-make-var",
    "href": "qmd/gnu-make.html#sec-make-var",
    "title": "GNU Make",
    "section": "Variables",
    "text": "Variables\n\nExpanded Variables\n\nValues are accessed using $(x) or ${x})\n‚ÄúRecursively Expanded‚Äù Variables are defined using = operator\nx = hello\ny = $(x)\n# Both $(x) and $(y) will now yield \"hello\"\nx = world\n# Both $(x) and $(y) will now yield \"world\"\n\nAny functions referenced in the definition will be executed every time the variable is expanded\nCan cause infitinite loops\n\n‚ÄúSimply Expanded‚Äù Variables are defined using the := or ::= operator\nx := hello\ny := $(x)\n# Both $(x) and $(y) will now yield \"hello\"\nx := world\n# $(x) will now yield \"world\", and $(y) will yield \"hello\"\n\nAutomatic Variables\n\n$@ is a Make variable that ‚Äúexpands‚Äù into the (first?) target name\n$^ is a Make variable that ‚Äúexpands‚Äù into all of the prerequisites\n$&lt; is a Make variable that ‚Äúexpands‚Äù into the first prerequisite\n$? is a Make variable that ‚Äúexpands‚Äù into any prerequisites which have a time stamp more recent than the target\n% is a wildcard; looks for any targets in the makefile that matches it‚Äôs pattern or files in the project directory (also see abstraction section below)\nfoo%.o: %.c\n    $(CC) $(CFLAGS) -c $&lt; -o $@\n\nWill match target lib/foobar.o, with:\n\nStem ($*): lib/bar\nTarget name ($@): lib/foobar.o\nPrerequisites ($&lt;, $^): lib/foobar.c\n\n\n$*is a Make variable that ‚Äúexpands‚Äù the ‚Äústem‚Äù (i.e.¬†value) of wildcard",
    "crumbs": [
      "GNU Make"
    ]
  },
  {
    "objectID": "qmd/gnu-make.html#sec-make-com",
    "href": "qmd/gnu-make.html#sec-make-com",
    "title": "GNU Make",
    "section": "Commands",
    "text": "Commands\n\nThe commands are a series of steps typically used to make the target(s). These need to start with a tab character, not spaces.\nSee command used to generate a target\n# CLI\n&gt;&gt; make --recon &lt;target&gt;\nUpdate a specific target\n# CLI\n&gt;&gt; make data/raw/NIPS_1987-2015.csv\n\nThis will re-run the rule that created the file. In this case, it‚Äôs the curl command in the ‚ÄúDownload a file‚Äù section\nIf you run this command again, you‚Äôll receive this message: make:data/raw/NIPS_1987-2015.csv‚Äô is up to date.`",
    "crumbs": [
      "GNU Make"
    ]
  },
  {
    "objectID": "qmd/gnu-make.html#sec-make-exe",
    "href": "qmd/gnu-make.html#sec-make-exe",
    "title": "GNU Make",
    "section": "Execute a script",
    "text": "Execute a script\n\nExample\n\nMakefile\ndata/processed/NIPS_1987-2015.csv : src/data/transpose.py data/raw/NIPS_1987-2015.csv\n    $(PYTHON_INTERPRETER) $^ $@\n\n‚Äú$(PYTHON_INTERPRETER)‚Äù is an environment variable set in the Make file for python3 interpreter\nThe function in this example has 2 args: input file path and output file path\n$^ fills in the prerequisites which takes care of &lt;script&gt; &lt;arg1&gt;\n$@ fills in the target name for &lt;arg2&gt;\n\nCLI\n&gt;&gt; make --recon data/processed/NIPS_1987-2015.csv\npython3 src/data/transpose.py data/raw/NIPS_1987-2015.csv data/processed/NIPS_1987-2015.csv\n\nmake --recon shows us the translation of command line in the Make file\nBasic format for executing a python script in the cli is python3 &lt;script&gt; &lt;arg1&gt; &lt;arg2&gt; ... &lt;argn&gt;",
    "crumbs": [
      "GNU Make"
    ]
  },
  {
    "objectID": "qmd/gnu-make.html#sec-make-abs",
    "href": "qmd/gnu-make.html#sec-make-abs",
    "title": "GNU Make",
    "section": "Abstraction",
    "text": "Abstraction\n\nExample\nall: models/10_topics.png models/20_topics.png\n\nmodels/%_topics.png : src/models/fit_lda.py data/processed/NIPS_1987-2015.csv src/models/prodlda.py\n    $(PYTHON_INTERPRETER) $&lt; $(word 2, $^) $@ --topics $*\n\n% matches both targets in the dummy target, ‚Äúall‚Äù and takes the stem 10 and 20\n\nSo this rule runs twice: once with the value 10 then with the value 20.\n\n$&lt; is an autmatic variable that expands into ‚Äúsrc/models/fit_lda.py‚Äù\nBuilt-in Make text function, $(word n,text) , returns the nth word of text. (see Misc &gt;&gt; Resources for function docs)\n\nThe legitimate values of n start from 1. If n is bigger than the number of words in text, the value is empty\nIn this example, it‚Äôs used to return the 2nd prerequisite to become the 1st argument of the fit_lda.py script\n\n1st arg is the input file path\n\n\n$@ is an automatic variable that expands into the target name which becomes the 2nd argument of the fit_lda.py script\n\n2nd arg is the output file path\n\n--topics is a function option for fit_ldy.py which is defined in the script using decorators from {{click}}\n\n$* is the stem of the wildcard which is a numeric in this case and provides the value the topics flag\n\n\nExample Clean text files in data directory\ndata/processed/%.txt: data/raw/%.txt\nsed 1,20d $^ &gt; $@\n\nTakes all text files in the raw directory, removes some rows (sed 1,20d), outputs (&gt;) the processed file into target with the same file name ($@)",
    "crumbs": [
      "GNU Make"
    ]
  },
  {
    "objectID": "qmd/cli-general.html#sec-cli-gen-vim",
    "href": "qmd/cli-general.html#sec-cli-gen-vim",
    "title": "General",
    "section": "Vim",
    "text": "Vim\n\nCommand-line based text editor\nCommon Usage\n\nEdit text files while in CLI\nLogging into a remote machine and need to make a code change there. vim is a standard program and therefore usually available on any machine you work on.\nWhen running git commit, by default git opens vim for writing a commit message. So at the very least you‚Äôll want to know how to write, save, and close a file.\n\nResources\n\nThe minimum vi(m) you need to know\nVim Visual Cheat Sheet\nVim Cheatsheet\n\n2 modes: Navigation Mode; Edit Mode\n\nWhen Vim is launched you‚Äôre in Navigation mode\nPress i to start edit mode, in which you can make changes to the file.\nPress Esc key to leave edit mode and go back to navigation mode.\n\nCommands\n\nx deletes a character\ndd deletes an entire row\nb (back) goes to the previous word\nn (next) goes to the next word\n:wq saves your changes and closes the file\n:q! ignores your changes and closes the file\nh is \\(\\leftarrow\\)\nj is \\(\\downarrow\\)\nk is \\(\\uparrow\\)\nl (i.e.¬†lower L) is \\(\\rightarrow\\)",
    "crumbs": [
      "CLI",
      "General"
    ]
  },
  {
    "objectID": "qmd/feature-engineering-splines.html",
    "href": "qmd/feature-engineering-splines.html",
    "title": "Splines",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Feature Engineering",
      "Splines"
    ]
  },
  {
    "objectID": "qmd/feature-engineering-splines.html#sec-feat-eng-spl-misc",
    "href": "qmd/feature-engineering-splines.html#sec-feat-eng-spl-misc",
    "title": "Splines",
    "section": "",
    "text": "Knots are placed at several places within the data range with (usually) low-order polynomials that are chosen to fit the data between two consecutive knots.\n\nChoices\n\nNumber of knots\nTheir positions\nDegree of polynomial to be used between the knots (a straight line is a polynomial of degree 1)\n\nThe type of polynomial and the number and placement of knots is what defines the type of spline.\n\ne.g.¬†cubic splines are created by using a cubic polynomial in an interval between two successive knots.\n\nIncreasing the number of knots may overfit the data and increase the variance, whilst decreasing the number of knots may result in a rigid and restrictive function that has more bias.\n\nNotes from A review of spline function procedures in R (paper)\nAlso see:\n\nFeature Engineering, General &gt;&gt; Continuous &gt;&gt; Binning &gt;&gt; Harrell on the benefits of using splines vs binning\nFeature Engineering, Time Series &gt;&gt; Engineering &gt;&gt; Calendar features\nStatistical Rethinking &gt;&gt; (end of ) Ch 4\nFeature Engineering, Geospatial &gt;&gt; Cyclic Smoothing Spline\nHarrell‚Äôs RMS\nModel Building, tidymodels &gt;&gt; Recipe &gt;&gt; Transformations &gt;&gt; Splines\n\nCommon variables: trend, calendar features, age, cardinal directions (N, S, E, W, etc.)\nPackage Comparison\n\nDefault types: {mgcv} uses thin plate splines (see smoothing splines) as a default for it‚Äôs s() which makes it‚Äôs spline more flexible (i.e.¬†curvy) than the default splines for {gam}, {VGAM}, and {gamlss} which use cubic smoothing splines.\n\n{gamlss} doesn‚Äôt use s but instead has specific functions for specific types of splines\n\nP-Splines: {mgcv} and {gamlss} are very similar, and the differences can be attributed to the different way that two packages optimize the penalty weight, Œª.\n\n{mgcv}: option, ‚Äúps‚Äù within s will create a cubic p-spline basis on a default of 10 knots, with a third order difference penalty.\n\nThe penalty weight, Œª, is optimized with generalized cross validation.\n\n{gamlss}: pb defines cubic p-splines functions with 20 interior knots and a second order difference penalty.\n\nThe smoothing parameter is estimated using local maximum likelihood method, but there are also other options based on likelihood methods, AIC, generalized cross validation and more.\nMultiple other functions available for p-splines with various attributes.\n\n\nDependencies: {mgcv} creates its own spline functions while {gam}, {VGAM}, and {gamlss} use the base R package, {splines}.\n\n{gam} and {VGAM} call the base R function smooth.spline (smoothing spline) with four degrees of freedom as default and give identical results",
    "crumbs": [
      "Feature Engineering",
      "Splines"
    ]
  },
  {
    "objectID": "qmd/feature-engineering-splines.html#sec-feat-eng-spl-terms",
    "href": "qmd/feature-engineering-splines.html#sec-feat-eng-spl-terms",
    "title": "Splines",
    "section": "Terms",
    "text": "Terms\n\nSmoothly Joined -¬† Means that for polynomials of degree n, both the spline function and its first n-1 derivatives are continuous at the knots.",
    "crumbs": [
      "Feature Engineering",
      "Splines"
    ]
  },
  {
    "objectID": "qmd/feature-engineering-splines.html#sec-feat-eng-spl-tune",
    "href": "qmd/feature-engineering-splines.html#sec-feat-eng-spl-tune",
    "title": "Splines",
    "section": "Tuning Parameters",
    "text": "Tuning Parameters\n\nB: Basis functions (e.g.¬†B-Spline)\nd: The degree of the underlying polynomials in the basis\n\nTypically d = 3 (cubic) is used (&gt;3 usuallly indistinguishable)\n\nK: Number of knots for Regression Splines\n\nUsually k = 3, 4, 5. Often k = 4\n\nHarrell (uses natural splines): ‚ÄúFor many datasets, k = 4 offers an adequate fit of the model and is a good compromise between flexibility and loss of precision caused by overfitting‚Äù\n\nIf the sample size is small, three knots should be used in order to have enough observations in between the knots to be able to fit each polynomial.\nIf the sample size is large and if there is reason to believe that the relationship being studied changes quickly, more than five knots can be used.\n\n\nThere should be at least 10‚Äì20 events per degree of freedom (Harrell, RMS)\nVariables that are thought to be more influential on the outcome or more likely to have non-linear associations are assigned more degrees of freedom (i.e.¬†more knots)\nFlexibility of fit vs.¬†n and variance\n\nLarge n (e.g.¬†n ‚â• 100): k = 5\nSmall n (e.g.¬†n &lt; 30): k = 3\n\nCan use Akaike‚Äôs information criterion (AIC) to choose k\n\nThis chooses k to maximize model likelihood ratio of œá2 ‚àí 2k.\nCross-Validation is also valid\n\nAlso option for knot positions\n\nLocations not important in most situations\nPlace knots where data exist e.g.¬†fixed quantiles of predictor‚Äôs marginal distribution (See Regression Splines &gt;&gt; B-Splines for examples)\n\nFrom Harrell‚Äôs RMS\n\n\n\n\nŒª: Penalty weight for Smoothing Splines\n\n\nCalculated by generalized cross-validation in {mgcv} which is an approximation of LOO-CV\n\nSee article or Wood‚Äôs GAM book or Elements of Statistical Learning (~pg 244) for details",
    "crumbs": [
      "Feature Engineering",
      "Splines"
    ]
  },
  {
    "objectID": "qmd/feature-engineering-splines.html#sec-feat-eng-spl-interp",
    "href": "qmd/feature-engineering-splines.html#sec-feat-eng-spl-interp",
    "title": "Splines",
    "section": "Interpretation",
    "text": "Interpretation\n\nA regression fit will result in estimated coefficients for each parameter used in the splines.\nOther than including them in technical appendices, in almost all cases, one does not present these estimated coefficients ‚Äì their interpretation is essentially meaningless.\nVisual interpretations of the predicted response vs the splined variable are useful in discovering trends or patterns.\nPredicted responses, given representative values, outlier values, or any values of interest of the splined variable, are useful in calculating various contrasts.\nEffective Coefficient\n\nIt shows how the effect of the variable on the response varies over its range\nThink this is only possible for a natural spline\nExample: Age on Survival in Titanic dataset (link)\n\nmodel_02 &lt;- \n  glm(Survived ~ SibSp + ns(Age, df = 3) + Pclass + Parch + Fare,\n      data = titanic,\n      family = binomial)\n#\n# Create a data frame for prediction: only `Age` will vary.\n#\nN &lt;- 101\nx &lt;- titanic[which.max(complete.cases(titanic)), ]\ndf &lt;- do.call(rbind, lapply(1:N, function(i) x))\ndf$Age &lt;- with(titanic, seq(min(Age, na.rm=TRUE), max(Age, na.rm=TRUE), length.out=N))\n#\n# Predict and plot.\n#\ndf$Survived.hat &lt;- predict(model_02, newdata=df) # The predicted *link,* by default\nwith(df, plot(Age, Survived.hat, type=\"l\", lwd=2, ylab=\"\", main=\"Relative spline term\"))\nmtext(\"Spline contribution\\nto the link function\", side=2, line=2)\n#\n# Plot numerical derivatives.\n#\ndAge &lt;- diff(df$Age[1:2])\ndelta &lt;- diff(df$Survived.hat)/dAge\nage &lt;- (df$Age[-N] + df$Age[-1]) / 2\nplot(age, delta, type=\"l\", lwd=2, ylab=\"Change per year\", xlab=\"Age\",\n     main=\"Spline Slope (Effective Coefficient)\")\n\nThe varying coefficient is computed by calculating the first derivatives numerically: divide the successive differences in predicted values by the successive differences in age.\nAt Age near 35 the effective slope is nearly zero, meaning small changes of Age in this range have no effect on the predicted response. Near ages of zero, the effective slope is near ‚àí0.15, indicating each additional year of Age reduces the value of the link function by about 0.15. At the oldest ages, the effective slopes are settling down to a value near ‚àí0.09, indicating each additional year of age in this age group decreases the link function by ‚àí0.09.",
    "crumbs": [
      "Feature Engineering",
      "Splines"
    ]
  },
  {
    "objectID": "qmd/feature-engineering-splines.html#sec-feat-eng-spl-reg",
    "href": "qmd/feature-engineering-splines.html#sec-feat-eng-spl-reg",
    "title": "Splines",
    "section": "Regression Splines",
    "text": "Regression Splines\n\nNo penalty function added\n\nSplined variable is just added to the regression model like any other predictor\n\nTypes\n\nTruncated Power Basis\n\nIssue: Basis functions are not supported locally but over the whole range of the data\n\nCould lead to high correlations between some basis splines, implying numerical instabilities in spline estimation\n\nExample: d = 3 (cubic) with 5 equidistant knots\n\nExample: d = 3 with 3 knots (œÑ1, œÑ2, œÑ3)\n\n\\[\nf(X) = \\beta_0 + \\beta_1 X + \\beta_2 X^2 + \\beta_3 X^3 + \\beta_4 (X - \\tau_1)^3 + \\beta_5 (X - \\tau_2)^3 + \\beta_5 (X - \\tau_3)^3\n\\]\n\n7 dof\n\n\nB-splines\n\n\nBased on a special parameterization of a cubic spline\nSee Statistical Rethinking Notebook &gt;&gt; (end of) Chapter 4\nBasis functions supported locally which leads to high numerical stability, and also in an efficient algorithm for the construction of the basis functions.\nIssue: can be erratic at the boundaries of the data (boundary knots)\nDegrees of freedom (dof) = d + K\nbs(x) will create a cubic B-spline basis with two boundary knots and one interior knot placed at the median of the observed data values\n\nBounded by the range of the data\nlm(y ~ bs(x))\n\nExample: bs(x, degree=2, knots=c(0,.5,1))\n\ndegree specifies d\nknots specifies the number of knots and their locations\n\nExample: bs(x, knots = median(x))\n\n1 interior knot created at the median\n4 dof since d + K = 3 + 1\n\nd = 3 (default)\n\n\nExample: bs(x, knots = c(min(x), median(x), max(x)))\n\n1 interior knot specified at the median and 2 boundary knots at the min and max.\n6 dof since d + K = 3 + 3\n\nd = 3 (default)\n\n\n\nNatural Cubic and Cardinal Splines\n\n\nStable at boundaries of data because of additional constraints that they are linear in the tails of the boundary knots\nDegrees of freedom (dof) = K + 1\nns(x) returns a straight line within the boundary knots\n\nlm(y ~ ns(x))\n\nExample: ns(x,df=3)\n\n‚Äúdf‚Äù specifies degrees of freedom\n‚Äúknots‚Äù: alternatively to specifying df, you can specify the knots (# and positions) like in bs\n\nCardinal splines\n\nHave an additional constraint that leads to the interpretation that each coefficient \\(\\beta_k\\) is equal to the value of the spline function at the knot \\(\\tau_k\\)",
    "crumbs": [
      "Feature Engineering",
      "Splines"
    ]
  },
  {
    "objectID": "qmd/feature-engineering-splines.html#sec-feat-eng-spl-smth",
    "href": "qmd/feature-engineering-splines.html#sec-feat-eng-spl-smth",
    "title": "Splines",
    "section": "Smoothing Splines (aka Penalized Splines)",
    "text": "Smoothing Splines (aka Penalized Splines)\n\nAutomatically handles the number of knots and knot positions by using a large number of knots and letting Œª control the amount of smoothness\n\nDifferent packages usually produce similar results. Penalties are very powerful in controlling the fit, given that enough knots are supplied into the function\n\nRequires modification of the fitting routine in order to accommodate it\n\nProbably need a GAM package to use.\n\nA special case of the more general class of thin plate splines\nFunction\n\\[\n\\hat{\\beta} = \\arg\\max_{\\beta} [l_\\beta (x_1, y_1, \\ldots, x_n, y_n) - \\lambda J_\\beta]\n\\]\n\nThe maximization of this function implies a trade-off between smoothness and model fit that is controlled by the tuning parameter Œª\nTerms\n\nlŒ≤ is the likelihood\nJŒ≤ (penalty function) is the roughness penalty (expresses the smoothness of the spline function)\n\nFor a gaussian regression this is the integrated second derivative of the spline function (see paper for more details)\n\nExample:\n\\[\n||y-f||^2 + \\lambda \\int \\left(\\frac {\\partial^2 f(\\text{log[baseline profit]})}{\\partial \\; \\text{log[baseline profit]}^2}\\right)^2 \\partial x\n\\]\n\n\nŒª is a tuning parameter that‚Äôs ‚â•0\n\n\nB-Spline basis is typically used\nNot easy to specify the degrees of freedom, since they will vary depending on the size of the penalty\n\nUsually can be restricted to a maximum number of degrees of freedom or desired degrees of freedom\n\nPenalized Regression Splines\n\nApproximation of a smoothing spline\nBest used when n is large and the variable range is covered densely by the observed data\nP-Spline\n\nBased on the cubic B-spline basis and on a ‚Äòlarge‚Äô set of equidistant knots (usually, 10‚Äì40)\nSimplifies the calculation of JŒ≤ (see paper for more details)\nPackages: {mgcv}, {gamlss} (See above, Misc &gt;&gt; Package Comparison)",
    "crumbs": [
      "Feature Engineering",
      "Splines"
    ]
  },
  {
    "objectID": "qmd/feature-engineering-splines.html#sec-feat-eng-inter",
    "href": "qmd/feature-engineering-splines.html#sec-feat-eng-inter",
    "title": "Splines",
    "section": "Interactions",
    "text": "Interactions\n\nNumeric spline varying by indicator\ns(log_profit_rug_business_b, by = treatment)\n\nCoefficient is a conditional average treatment effect (CATE)\nCreates the main effect and the interaction",
    "crumbs": [
      "Feature Engineering",
      "Splines"
    ]
  },
  {
    "objectID": "qmd/job-reports.html",
    "href": "qmd/job-reports.html",
    "title": "Reports",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Job",
      "Reports"
    ]
  },
  {
    "objectID": "qmd/job-reports.html#sec-job-reports-misc",
    "href": "qmd/job-reports.html#sec-job-reports-misc",
    "title": "Reports",
    "section": "",
    "text": "Packages\n\n{narrator} - Creates a text summarization of descriptive statistics. The outputted text can be enhanced with ChatGPT. Available in both R and Python.\n\nChicago manual of style for citations\nReread after 3 days to make sure it makes sense before publishing\n\nDancho does his labs on Wed afternoons, so maybe that‚Äôs a good time to release articles.\n\nPrint out article and highlight topic sentences\n\nDoes each topic sentence describe the paragraph. Do all the other sentences in the paragraph support the topic sentence.\nDo the topic sentences produce a good outline about the subject you wanted to discuss. Do they follow a logical data storytelling sequence.\n\nPrimary interests of business people: business question, budget, whether research is conclusive or not conclusive, and value the research or product provides.\nKeep color schemes for categoricals, metrics, etc.\n\nIf you used a color palette for male/female in an earlier section/slide, keep that same palette throughout.\n\nKeep date and other variable formats consistent throughout\nNo more than 3 dimensions on a chart\nPick the chart, graph or table that best fits with the paragraph and move on to the next point. Don‚Äôt use multiple charts that show the same thing.\nNever introduce something into the conclusion that was not analyzed or discussed earlier in the report.\nDo not include more information than is necessary to support you report objectives\nPhrases for communicating uncertainty\n\n‚ÄôHere‚Äôs something we expect to see a lot,‚Äù\n‚ÄúHere‚Äôs something we expect to see sometimes‚Äù\n‚ÄúHere‚Äôs something that could happen on rare occasions, but which is worth considering because of the high stakes.‚Äù",
    "crumbs": [
      "Job",
      "Reports"
    ]
  },
  {
    "objectID": "qmd/job-reports.html#sec-job-reports-concepts",
    "href": "qmd/job-reports.html#sec-job-reports-concepts",
    "title": "Reports",
    "section": "Concepts",
    "text": "Concepts\n\nWhat is the problem we are solving\n\ne.g.¬†Why are we losing so many customers?\n\nUnderstand the kind of story you want to tell -\n\nA one-time story: What caused the last month‚Äôs shortage?\nUpdated, ongoing story: Weekly rise and fall of sales, fraud detection\n\nKnow your audience\n\nWhat knowledge your audience brings to the story. What kind of preconceptions does the audience have.\n\nInclude the critical elements of a traditional story structure\n\nPoint of View: Someone has to ask the question that‚Äôs answered with data.\nEmpathy: Need to have human protagonist who‚Äôs solving the problem.\nAn Antagonist: Confusion or misunderstanding that makes achievement of the solution difficult.\nAn Explicit Narrative: This happened, then this happened, and then‚Ä¶\n\nDevelop the Right Hook\n\nWhat helps grab the attention of the managers? e.g.¬†newspaper lead opening, startling statistics, teaser\n\nA picture is priceless\n\nPeople like visuals but good ones are really difficult to create\n\nWhat‚Äôs your point?\n\nResolve and close what does your story advise to do? e.g A Call to Action\n\nIterate\n\nSome stories need to be retold continuously when new data arrives, good stories live on",
    "crumbs": [
      "Job",
      "Reports"
    ]
  },
  {
    "objectID": "qmd/job-reports.html#sec-job-reports-genguid",
    "href": "qmd/job-reports.html#sec-job-reports-genguid",
    "title": "Reports",
    "section": "General Guidelines",
    "text": "General Guidelines\n\nKnow your audience Don‚Äôt use technical terms when talking to non-technical people.\n\nFast-track the conversation to the technical stuff when talking to fellow data scientists.\nThe more senior the person you‚Äôre talking to, the more to the point your message has to be.\nSmall talk with long-term clients is always essential to maintain a strong relationship.\nThe CEO only wants to know the result of your analysis and what it means for their company.\n\nSimplified\n\nIntent: This is your overall goal for the project, the reason you are doing the analysis and should signal how you expect the analysis to contribute to decision-making.\nObjectives: The objectives are the specific steps you are taking or have taken to achieve your above goal. These should likely form the report‚Äôs table of contents.\nImpact: The result of your analysis has to be usable ‚Äî it must offer solutions to the problems that led to the analysis, to impact business actions. Any information which won‚Äôt trigger action, no matter how small, is useless.\n\nWhat is the¬†business question?\nWhy is it important?\n\nDoes your model enable us to better select our target audience?\nHow much better are we using your model?\nWhat will the expected response on our campaign be?\nWhat is the financial impact of using your model?\n\nWhat is the data science question?\nWhat is the data science answer?\nWhat is the business answer?\nShow a general form of the equation, definition of terms, before explaining how to fill it with values of particular to your problem\nHow would you recommend your model/results be used?\nBe direct. Communicate your thoughts in a forthright manner, otherwise the reader may begin to tune out.\nStart with an outline\n\nState your objective\nList out your main points\nNumber and underline your main points to guide the reader\nEnd with a summary.\n\nOpen with short paragraphs and short sentences\nUse short words. The goal is to reduce friction.\n\nUse adjectives and adverbs for precision, not exclamation points.\n\nCut lazy words like very, great, awfully, and basically. These do nothing for you.\n\nUse down-to-earth language and avoid jargon. Like explaining to a 6th grader\nDon‚Äôt use generalities (e.g.¬†‚ÄúOur campaign was a great success and we came in under budget‚Äù).\n\nBe specific (e.g.¬†‚ÄúWe increased click-through rates by 21% while spending 19% less than expected.‚Äù)\n\nTake the time to build down what you have to say. Then, express it confidently in simple, declarative sentences.\n\nEspecially in memos and emails, put your declaration in the subject line or as the first line",
    "crumbs": [
      "Job",
      "Reports"
    ]
  },
  {
    "objectID": "qmd/job-reports.html#sec-job-reports-narr",
    "href": "qmd/job-reports.html#sec-job-reports-narr",
    "title": "Reports",
    "section": "Narrative Structures",
    "text": "Narrative Structures\n\nDeveloping a narrative when presenting results is imperative in order for recommendations to gain traction with stakeholders\n\nBudget at least 50% of time in the project plan for insight generation, and structuring a narrative (seems a bit large)\nWith each iteration (potentially dozens) of improving your presentation, you are looking to address any insight gaps, and improve the effectiveness in conveying the insight and recommendations\nAnticipate potential follow up questions they might ask and preemptively address them\nEliminate any distractions to the key message such as ambiguous statements, or erroneous facts that can derail the presentation\nIf possible find someone with tenure in the organization, or has expertise in the business area you are analyzing to lend a critical eye to your presentation.\n\nAlso may provide insight on how best to win the trust of key decision makers and potential areas that can derail the effort\n\n\nExample 1\n\nExecutive Summary\n\nBrief Description of Problem\nApproach Taken\nModels Used\nResults\nConclusion\nRecommendations\n\nDescribe the status quo\n\nMaybe describe what each proceeding section will entail\n\nWhat‚Äôsthe problem that needs fixing or improved upon\nProposed solution\nIssues that arose during process, maybe a new path discovered not previously thought of\nSolution\n\nDescription of data\n\nRecommendations or next steps\n\nThe stakeholder must understand the expected outcome, and the levers that need to be pulled to achieve that outcome.\nAn effective analysis owner will take on the responsibility for the stakeholder‚Äôs understanding, through communicating both specific predictions and the supporting evidence in a consumable way.\n\n\nExample 2\n\nExecutive Summary\n\nBrief Description of problem\nApproach Taken\nModels Used\nResults\nConclusion\nRecommendations\n\nIntroduction\n\nQuestion\nBackground\nWhy Important\nDescribe Structure of the Report\n\nMaybe a Table of Contents\n\n\nMethodology (EDA and Models)\n\nDescribe the data you are using\nThe types of analyses you have conducted & why\n\nResults\n\nMain body of the report split into sections according to the various business questions the report attempts to answer\nThe results generated for each question.\n\nDiscussion\n\nBring together patterns seen in EDA, model interpretations\nCompare with your prior beliefs and/or other papers results\nObjective recommendations for business actions to be taken\n\nConclusion/Summary\n\nRestate Question\nSteps Taken\nAnswers to Auestions\nIssues Faced\nNext Steps",
    "crumbs": [
      "Job",
      "Reports"
    ]
  },
  {
    "objectID": "qmd/job-reports.html#sec-job-reports-lay",
    "href": "qmd/job-reports.html#sec-job-reports-lay",
    "title": "Reports",
    "section": "Layouts",
    "text": "Layouts\n\nNotes from: How I create an Analyst Style Guide\nMost important details (i.e.¬†the conclusion) always come first\n\ne.g.¬†Executive summaries at the beginning of reports; Conclusions/useful sentences for titles of sections and slide titles\nThe goal is to reduce the time required by the reader to understand what you‚Äôre trying to tell them. If they want further details, they can read on further.\n\nUse consistent layouts so your audience can get used to where different types of information will be located\n\nExample: Driver layout\n\nPlot the trend of the Goal KPI on the left side with a text description in the same box.\nUse the larger space on the right side to plot the trends of the Driver KPIs that can explain the development of the Goal KPI\n\n\nThe Goal KPI is Sales Revenue and the Driver KPIs are Leads (#), Conversion Rate (%) and Order Value (EUR)\n\n\nExample: Contrast layout\n\nUseful to highlight the difference in two or more KPIs given the same segmentation\nDivide the space equally depending on the number of the metrics I want to compare with.\n\n\nThe contrast is between the metrics\nThe segmentation is gender and age groups\nTakeaway: Females generate most revenue and cost the least to obtain",
    "crumbs": [
      "Job",
      "Reports"
    ]
  },
  {
    "objectID": "qmd/job-reports.html#sec-job-reports-eym",
    "href": "qmd/job-reports.html#sec-job-reports-eym",
    "title": "Reports",
    "section": "Explaining Your Model",
    "text": "Explaining Your Model\n\nMisc\n\nFor ML models use feature importance to pick predictors to use for partial dependence plots (with standardized predictors, these can also advise on feature importance) and go back to do descriptive/aggregated statistical explorations (box plots, bars, etc.). Explain what‚Äôs happening in the plots, potential reasons why it‚Äôs happening, and potential solutions.\n\nTypes\n\nWhen talking to a colleague or regulator you may need to give more technical explanations. In comparison, customers would expect simpler explanations. It is also unlikely that you would need to give a global explanation to a customer. This is because they would typically only be concerned with decisions that affect them personally.\nGlobal: Explain what trends are being captured by the model in general\n\n‚ÄúWhich features are the most important?‚Äù or ‚ÄúWhat relationship does feature X have with the target variable?‚Äù\n\nLocal: explain individual model predictions\n\nTypically needed to explain a decision that has resulted from a model prediction\n‚ÄúWhy did we reject this loan application?‚Äù or ‚ÄúWhy was I given this movie recommendation?‚Äù\n\n\nCharacteristics\n\n\nTrue: Include uncertainty in your explanations of your model predictions\nCorrect level: Use the language of your audience instead of DS or statistical terminology\nNo.¬†of Reasons & Significant: Only give the top features that are responsible for a prediction or trend, and those features should be responsible for a substantial contribution\nGeneral: Explain features that are important to large portion of predictions (e.g.¬†feature importance, mean SHAP)\nAbnormal: Explain features that are important to extreme predictions or a representative prediction\n\nMight be a feature that isn‚Äôt globally important but important for an individual prediction or an outlier prediction\n\nContrasting: Explain contrasting decisions made by your model\n\n‚ÄúWhy was my application rejected and theirs accepted?‚Äù\nUse important features (ranges/levels of those features) that aren‚Äôt common to both decisions",
    "crumbs": [
      "Job",
      "Reports"
    ]
  },
  {
    "objectID": "qmd/job-reports.html#sec-job-reports-bizpres",
    "href": "qmd/job-reports.html#sec-job-reports-bizpres",
    "title": "Reports",
    "section": "Business Presentation",
    "text": "Business Presentation\n\nThey‚Äôre only interested in the story the data tells and the actions it influences\nPrep\n\nCreate an outline\nSituation-Complication-Resolution Framework\n\nSituation: Facts about the current state.\nComplication: Action is required based on the situation.\nResolution: The action is taken or recommended to solve the complication.\nExample\n\n\nOne minute per slide rule\n\nIf you have a 20-minute presentation, aim for 20 slides with content\n\nTry to stick to 3 bullet points\n\nOr if you need to include more information, structure the slide with some sort of ‚Äú3‚Äù framework\n\nExample: 3 columns\n\n\nEach column has 3 bullets\n\n\n\nFocus audience attention to important words\n\nBold, italics, a different color, or size for words you want to emphasize\n\n\nUse emotional elements as hooks to grab attention before starting the introduction. They generate these emotions but also curiosity about what comes next.\n\nGreed - ‚Äúthis has the potential to double revenue‚Äù\nFear - ‚Äúlayoffs may be coming‚Äù\nPride - ‚Äúwe can do this!‚Äù\nAnger - ‚ÄúIt‚Äôs the competition‚Äôs fault!‚Äù\nSympathy - ‚Äúthey‚Äôre counting on us to help‚Äù\nSurprise - ‚Äúyou won‚Äôt believe what we found‚Äù\n\nUse meaningful sentences as slide titles.\n\nExamples\n\nInstead of ‚ÄúSales outlook‚Äù, use ‚ÄúSales outlook is promising in the next 12 months‚Äù.\nInstead of ‚ÄúAnnual Sales‚Äù, use ‚ÄúSales Up 22% In 2022‚Äù\nInstead of ‚ÄúAlgorithm Training and Validation‚Äù use ‚ÄúPredict Customer Churn with 92% Accuracy‚Äù\nInstead of ‚ÄúQ1 Conversation Rates‚Äù use ‚ÄúAccounts With Direct Contact are 5x More Likely to Purchase‚Äù\nInstead of ‚ÄúUtilizing XGBoost to Classify Accounts‚Äù use ‚ÄúMachine Learning Improves Close Rates by 22%‚Äù\n\n\nRead (only) slide titles aloud\n\nBy reading just the tile and title only as you start each slide, the audience will be able to process the message much more easily than reading the written words and listening to you simultaneously.\nFor the rest of the slide, do not read the content, especially if you use a lot of bulleted or ordered lists. Reading all of your content can be monotonous\n\nIntroduction:\n\nProblem: ‚Äúflat 4th quarter sales‚Äù and maybe a why? it happened\nGoal: ‚Äúrestore previous year‚Äôs growth‚Äù\nDescribe the presentation to come: ‚ÄúBy analyzing blah blah, we can forecast blah, blah‚Äù and maybe a teaser on how it will be solved.\nDesired outcome: ‚ÄúOur goal here today is to leave with a budget, schedule, and brainstorm some potential advertising approaches that might be more successful‚Äù\nIf analysis is negative, it‚Äôs important to frame the story or somebody else will. Could become an investigation or witchhunt. Include something about the way the forward, so keep the focus positive and about teamwork.\nInclude disclaimers/assumptions but only those that directly pertain to the specific subject matter of the presentation\nLayout Q&A ground rules (questions only after or also during the presentation?)\n\nBody\n\nInterpret all visuals. Don‚Äôt let the audience reach their own conclusions.\nBullets\n\nShould only cover key concepts so don‚Äôt read\nYour narration should add more\n\nMore context\nMore interpretation\nMore content\nMore feeling\n\n\nPresentation Pattern: Present visual \\(\\rightarrow\\) interpret visual\n\nStart with a visual that illustrates the problem \\(\\rightarrow\\) discuss problem \\(\\rightarrow\\) present hypothesis that explains a cause of the problem\nPresent visual that is evidence for your hypothesis \\(\\rightarrow\\) interpret visual\n\nRepeat\nVisuals act as a chain of evidence\n\nProvide recommendation for a course of action \\(\\rightarrow\\) present visuals or data that support this action\n\ne.g.¬†Historical results from previous instances of taking this action\n\nHow this situation mimics the successful instances\n\nForecasts that support the recommendation\n\nTalk about the uncertainty, consequences of lower and upper bounds\n\nSurvey Data\n\nInvite questions and comments about the data and visuals you shown if you have no recommendations or courses of action\n\nTake notes (yourself or assistent)\nIf you don‚Äôt have an answer:\n\n‚ÄúI don‚Äôt have an answer for that offhand but I‚Äôll get back to you after we look into that.‚Äù\n‚ÄúI don‚Äôt have the answer to that. I can reanalyze the data and see if they support that idea.‚Äù\n\n\n\n\nSatisfying Conclusion\n\nSummarize (especially if a lot was covered)\n\nConsiderations\n\nWhat does your audience care about?\nWhat are the implications of your results?\n\nHow does these results affect the business or solve the problem or clarify the problem, etc.?\n\nWhich insights from your analysis will have the biggest impact?\n\n\nIf you asked for questions or comments above, summarize them and any conclusions from the discussion, which ones require further study, etc.\nIf you provided recommendations, review them and include the rationale for them ideally tied to the data, and the expected results of such actions\n\nExample: ‚ÄúThe price reduction on  has resulted in a strong rebound in sales figures that analysis shows will increase further with additional marketing support. We recommend increasing the advertising budget for this line by 25% next quarter and would like the art department to take on design of a new campaign as their immediate action item.‚Äù\n\nDefine success metrics and what values would require a rethink of the strategy.\nDefine a timeframe\n\n‚ÄúIt is our hope that the additional 25% marketing investment in the  will result in Q4 revenue that is 50% over last year‚Äôs Q4 revenue for that line. We will review the results next January and meet again to discuss them and determine any changes in course going forward.‚Äù\n\nPotentially include consequences of not following recommendations\n\n‚Äú‚Ä¶ it is unlikely sales will recover and we‚Äôll continue to lose market share.‚Äù\n\nIf anyone made any commitments to other actions, note those.\nBring back emotional hook that you used in the intro\n\n‚Äù our analysis shows that blah, blah will justify the further investment and eliminate the need for layoffs.‚Äù\n‚Äú‚Ä¶ should lead to a return to robust sales and profitability, along with stronger profit sharing.\nIf you used greed, conclude with how rewarding the action will be\nif you used fear, end with how the action will alleviate that fear\n\n\nQ&A\n\nPlant questions with collegues about info you wanted to include but the topic didn‚Äôt fit into the presentation\nPrepare for likely questions will have tables or other slides that answer those questions\nDisagreements or questions you don‚Äôt have an answer to:\n\nDON‚ÄôT BE DISMISSIVE\n\nDon‚Äôt respone with any variant of, ‚Äúyou don‚Äôt trust data?‚Äù or blaming difficulties on someone‚Äôs lack of ‚Äúdata literacy.‚Äù\nWith so many potential sources of error or misunderstanding, it seems sensible for the data scientist to listen to concerns.\nClient questions provide an important counterweight against over-trust in data products.\n\nGive non-defensive responses\n\nA non-defensive response is helpful when you‚Äôre wrong, but pure gold when you are right (and both things will happen from time to time). If you are right, but are argumentative or dismissive, the client is likely to be upset. if you take a client‚Äôs concerns seriously and are thoughtful about addressing the situation, then turn out to be correct on top of that, you‚Äôre likely to make a very positive impression.\nPhrases\n\nThat‚Äôs a great question. We need to collect more data before we‚Äôll be able to answer that.\nThank you for bringing that to my attention\nI need to think about that\nI‚Äôm not prepared to give that the consideration it deserves, but can we make an appointment to discuss it later?\nI hadn‚Äôt thought of it that way\nAnything is possible\n\n\nAnswer a question with a question (to clarify)\n\nA great many disagreements arise due to mismatched interpretation of goals and definitions. It‚Äôs important to fully understand the nature of the concern.\nReports sometimes are outdated or refer to a different product, department, etc.\nThey can speed up finding the root cause of your own error.\n\nUse email\n\nFollowing-up emails summarizing an issue, outlining plans, and suggesting timelines for investigations, are nearly always appreciated\n\nBe careful about taking lifelines from the audience\n\nDuring a disagreement, a helpful bystander will often offer a suggestion. Their ideas are usually generous, imagining a way that the data scientist might be correct. It might be tempting to agree, but be careful! Thoughtlessly taking a lifeline is a fast way to lose credibility.\nPhrases\n\n‚ÄúThat‚Äôs a possibility, John, thanks for the suggestion!‚Äù\n‚ÄúGreat idea, Sally, but I need more time to look at the data to be sure!‚Äù\n\n\n\nDon‚Äôt let anyone hijack q&a and turn it into a one and one conversations. Cut off or defer answering a follow up question.\n\n‚ÄúThanks for your great question, but we do need to let others ask their questions. Please follow-up with me afterwards.‚Äù\n\nThank everyone for attending and leave the front of the room.\n\nFollow-up\n\nKeep Promises\n\nAnswer questions to promised to look into\nPost slide deck if you said you would\nSchedule and attend a meeting if you said you would\n\nSend summary email to participants if any actions resulted from the meeting\nSet up monitoring of success metrics. Someone could want an interim report before the settled upon timeframe has been reached.",
    "crumbs": [
      "Job",
      "Reports"
    ]
  },
  {
    "objectID": "qmd/job-reports.html#sec-job-reports-instrart",
    "href": "qmd/job-reports.html#sec-job-reports-instrart",
    "title": "Reports",
    "section": "Instructional Articles",
    "text": "Instructional Articles\n\nWhat?\n\nGiven a short description of the subject matter\n\nWhy?\n\nWhy is the subject matter important\nWhy is the subject matter useful\nWhy do it this way and not another\nState what each section will entail.\n\nBackground\n\nSome history\nContext surrounding the problem\nBusiness and Data Science interpretations of the problem or subject matter\n\nExample\n\nFramework\n\nDescribe the variables\nDescribe the model\nPotential issues/assumptions with approach\n\nAnalysis\nResults\n\nConclusion",
    "crumbs": [
      "Job",
      "Reports"
    ]
  },
  {
    "objectID": "qmd/job-reports.html#sec-job-reports-domspec",
    "href": "qmd/job-reports.html#sec-job-reports-domspec",
    "title": "Reports",
    "section": "Domain Specific",
    "text": "Domain Specific\n\nTime Series\n\nNotes from Why Should I Trust Your Forecasts?\nIn Goodwin et al.¬†(paper yet to be published, July 2021), people trusted forecasts more when they were presented as ‚Äúbest case‚Äù and ‚Äúworst case‚Äù values rather than as ‚Äúbounds of a 90% prediction interval.‚Äù\n\nWtf is ‚Äúworst case‚Äù? Outside an 80% CI? If so that has a 20% chance of happening.\n\nIn some situations, managers who are not mathematically inclined may be suspicious of forecasts presented using technical terminology and obscure statistical notation (Taylor and Thomas, 1982).\n\nSuch a manager may respect the forecast provider‚Äôs quantitative skills, but simultaneously perceive that the provider has no understanding of managers‚Äô forecasting needs ‚Äì hence the manager distrusts the provider‚Äôs forecasts\n\nI don‚Äôt understand this one either. What could possibly be the different ‚Äúforecasting need‚Äù that the manager needs?\n\n\nExplanations (i.e.¬†justifications, rationale, etc.) of the forecast can improve people‚Äôs perceptions of a forecast. The higher the perceived value of the explanations, the higher the level of acceptance of the forecast. (G√∂n√ºl et al, 2006)\n\nPeople enjoy the ‚Äústories‚Äù and it makes the forecasts more believable.\n\nProvide cues for how to evaluate the forecast in the report\nProvide accuracy metrics in relation to a reasonable benchmark\n\nExample: rolling average, naive, average for these days over the previous 5 yrs, whatever the current method is, etc.\nIn very unpredictable situations, this will help to show that relatively high forecast errors are unavoidable and not a result of the forecaster‚Äôs lack of competence.\n\nBeing transparent about assumptions, and even presenting multiple forecasts based on different assumptions, will most likely reassure the user about the integrity of the provider.",
    "crumbs": [
      "Job",
      "Reports"
    ]
  },
  {
    "objectID": "qmd/visualization-general.html",
    "href": "qmd/visualization-general.html",
    "title": "General",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Visualization",
      "General"
    ]
  },
  {
    "objectID": "qmd/visualization-general.html#sec-vis-gen-misc",
    "href": "qmd/visualization-general.html#sec-vis-gen-misc",
    "title": "General",
    "section": "",
    "text": "Notes from\n\nFriends Don‚Äôt Let Friends\n\nMicrosoft Paint 3D\n\nLocation: Start &gt;&gt; All Programs &gt;&gt; Paint 3D\nHightlight Text\n\nClick 2D Shapes (navbar) &gt;&gt; Select square (side panel)\nLeft click and hold &gt;&gt; Extend area around text you want to highlight &gt;&gt; Release\nChoose Line Type color and Sticker Opacity level (37%)\nOn area surrrounding text\n\nIf needed, make area size adjustment dragging little box-shaped icons that are along the outside\nOn the right side, click the check mark icon to finalize\n\nClick Menu (left-side on navbar) &gt;&gt; save as &gt;&gt; Image\n\nIt adds a png extension, but you just need to type the name.\n\n\n\nAlt Text\n\nThe guiding principle is to write alt text that gives disabled readers as close to the same experience as nondisabled readers as possible.\n\nHtmlwidget Packages\n\n{highcharter}\n\nDrilldown functionality\n\n{apexcharter}\n\nGood for mobile\nSycronization\n\n\nggplot2\n\nDocs\n\nTheme elements\n\nTheme Elements Cheatsheet (source)\n\nDon‚Äôt use stat calculating geoms and set axis limits with scale_y_continuous\n\n\nSee examples of the behavior in this thread\n\nDefaults for any {ggplot2} geom using the default_aes field (i.e.¬†GeomBlah$default_aes )\nTransparent Background\np +\n  theme(\n    panel.background = element_rect(fill='transparent'), #transparent panel bg\n    plot.background = element_rect(fill='transparent', color=NA), #transparent plot bg\n    panel.grid.major = element_blank(), #remove major gridlines\n    panel.grid.minor = element_blank(), #remove minor gridlines\n    legend.background = element_rect(fill='transparent'), #transparent legend bg\n    legend.box.background = element_rect(fill='transparent') #transparent legend panel\n  )\n\nggsave('myplot.png', p, bg='transparent')\ntheme_notebook\ntheme_notebook &lt;- function() {\n  theme_minimal() %+replace%\n    theme(\n      panel.background = element_rect(fill='#fffaf0'),\n      plot.background = element_rect(fill='#fffaf0', color=NA),\n      legend.background = element_rect(fill='#fffaf0'),\n      legend.box.background = element_rect(fill='#fffaf0')\n    )\n}\n\nFractional Data\n\nUse Stacked Bars instead of Pie or Circular or Donut\n\nHumans are better at judging lengths than angles (article)\n\n\nFactorial Experiments\n\nDon‚Äôt use bars factorial experiments\n\nCheck outcome ranges by group when facetting",
    "crumbs": [
      "Visualization",
      "General"
    ]
  },
  {
    "objectID": "qmd/visualization-general.html#sec-vis-gen-concepts",
    "href": "qmd/visualization-general.html#sec-vis-gen-concepts",
    "title": "General",
    "section": "Concepts",
    "text": "Concepts\n\nExploration and Analysis\n\nGoal: explore a new dataset, gertan overview, find answers to specific questions\nFast iteration of many generic charts, don‚Äôt customize or worry about color schemes, etc.\n\nExplanation\n\nGoal: help others understand a relationship in the data\nUse as few charts as possible, carefully chosen\nSequence so that they are easy to understand\nAdd interaction to help people get a better understanding\n\nPresentation\n\nGoal: walk your audience through an argument, help them come to a decision\nFocus on polishing charts: colors, legends, titles, etc.\nHighlighting of key elements (which might be considered biasing in Exploration)\nPossibly use of unusual charts for memorability\nSequence to make a specific point",
    "crumbs": [
      "Visualization",
      "General"
    ]
  },
  {
    "objectID": "qmd/visualization-general.html#sec-vis-gen-svg",
    "href": "qmd/visualization-general.html#sec-vis-gen-svg",
    "title": "General",
    "section": "SVG",
    "text": "SVG\n\nBetter for doing post-processing in Inkscape and gimp\nSVGs won‚Äôt be pixelated when you zoom in like PNGs are\nD3 outputs SVG\nsvglite PKG\n\nusing svglite instead of base::svg( ) allows you alter text in Inkscape or Illustrator\nrequires the used fonts to be present on the system it is viewed on.\n\nThe vast majority of interactive data visualizations on the web are now based on D3.js which often renders to SVG and it all seems to behave. Still, this is something to be mindful of, and a reason to use svg() if exactness of the rendered text is of prime importance\n\nFile size will be dramatically smaller",
    "crumbs": [
      "Visualization",
      "General"
    ]
  },
  {
    "objectID": "qmd/visualization-general.html#sec-vis-gen-layout",
    "href": "qmd/visualization-general.html#sec-vis-gen-layout",
    "title": "General",
    "section": "Layout",
    "text": "Layout\n\nFacetting vs Single Graph\n\nLayout based on experiement design\n\nAlign title ALL the way to the left (ggplot: plot.title.position = ‚Äúplot‚Äù)\nremove legends\n\nuse colored text in title (ggtext)\nlabel points or lines\nlast resort: place legend underneath title/subtitle\n\ngrid lines\n\nremove if possible\nsparse and faint if needed\n\naxis labels\n\nremove if obvious (e.g brands of cars)\ncreate a title that informs about the axis labels\nshould always be horizontal\n\nflip axis, don‚Äôt angle them 45 degrees\n\n\ntext\n\nleft-align most text\ncan center a subtitle if it helps with making the graph more symmetrical\nsome labels can be right-aligned\n\nRemove all borders\nMaximize white space\n\ndon‚Äôt cram visuals together\n\nWorking memory. A cognitive limitation that affects plot comprehension is the limit on working memory. Typically, working memory is limited to approximately seven (plus or minus two) items, or chunks. In practice, this means that categorical scales with more than seven categories decrease readability, increase comprehension time, and require significant attentional resources, because it is not possible to hold the legend mapping in working memory.\nThe use of redundant aesthetics that activate the same gestalt principles (such as color and shape in a scatter plot, which both activate similarity) results in higher identification of corresponding data features. In addition, dual encoding increases the accessibility of a chart to individuals who have impaired color vision or perceptual processing (e.g., dyslexia, dysgraphia). This experimental evidence directly contradicts the guidelines popularized by Tufte (1991), which suggest the elimination of any feature that is not dedicated to representing the core data, including redundant encoding and other unnecessary graphical elements.\nggplot themes\n\nCedric Sherer (article)\ntheme_set(theme_minimal(base_size = 15, base_family = \"Anybody\"))\ntheme_update(\n  axis.title.x = element_text(margin = margin(12, 0, 0, 0), color = \"grey30\"),\n  axis.title.y = element_text(margin = margin(0, 12, 0, 0), color = \"grey30\"),\n  panel.grid.minor = element_blank(),\n  panel.border = element_rect(color = \"grey45\", fill = NA, linewidth = 1.5),\n  panel.spacing = unit(.9, \"lines\"),\n  strip.text = element_text(size = rel(1)),\n  plot.title = element_text(size = rel(1.4), face = \"bold\", hjust = .5),\n  plot.title.position = \"plot\"\n)",
    "crumbs": [
      "Visualization",
      "General"
    ]
  },
  {
    "objectID": "qmd/visualization-general.html#sec-vis-gen-ar",
    "href": "qmd/visualization-general.html#sec-vis-gen-ar",
    "title": "General",
    "section": "Aspect Ratio",
    "text": "Aspect Ratio\n\nMisc\n\nGolden Rectangle\n```{{{r, fig.width = 6, fig.asp = 1.618}}}\n```\nGet consistent outputs\n\nRStudio pane displays in 72dpi which can mislead you on what your output looks like.\nThink {ragg} is supposed to have taken care of the inconsistency in terms of printing on different OSes\nUsing {camcorder}\n\nStart ‚Äúrecording‚Äù plots\ncamcorder::gg_record(\n  dir = \"imgs\",\n  width = 12,\n  height = 12*9/16,\n  dpi = 300,\n  bg = \"white\"  # Makes sure background is actually white an not transparent\n)\n\nAll plots will immediately be exported as a .png-file to the directory specified\nAll plots will be displayed in the viewer with dimensions and resolution that you specified and not in the plots pane in RStudio\n300 dpi is pretty standard and default of ggsave\n\nDo work. Export final png file in directory when done and delete the rest\nRegarding Fonts\n\nIf using {ragg}, then all is fine.\nIf using {showtext}, then you have to set resolution in options, showtext_opts(dpi = 300)\n\n\n\n\nTwitter\n\nVideo: 1105 x 1920\n\nLine Charts\n\nMatters most if two different line charts are being compared\n\nThe core idea of ‚Äúbanking‚Äù is that the slopes in a line chart are most readable if they average to 45¬∞.\nUse ggthemes::bank_slopes(x, y, method = c(\"ms\", \"as\"))\n\n2 methods (that req. no optimization) from Jeer, Maneesh who followed Cleveland‚Äôs 45¬∞ guideline\ndocs\n\n‚ÄúThe problem with banking is that sometimes you need the chart in a certain aspect ratio to fit into a page layout. Especially if banking produces portrait sized charts. But why not let the optimal chart ratio define your layout? For instance, you can put the additional information to the side of the chart. Remember that the main goal of banking is to increase the readability of the line slopes. In the following example, the slopes for Nuclear and Renewables would have been much more difficult to see, if the chart would have been ‚Äòsqueezed‚Äô to a landscape aspect.‚Äù (article)",
    "crumbs": [
      "Visualization",
      "General"
    ]
  },
  {
    "objectID": "qmd/visualization-general.html#sec-vis-gen-typo",
    "href": "qmd/visualization-general.html#sec-vis-gen-typo",
    "title": "General",
    "section": "Typography",
    "text": "Typography\n\nCSS Length Units\n\nAbsolute Lengths\n\n* Pixels (px) are relative to the viewing device. For low-dpi devices, 1px is one device pixel (dot) of the display. For printers and high resolution screens 1px implies multiple device pixels.\n\n\ncm\ncentimeters\n\n\nmm\nmillimeters\n\n\nin\ninches (1in = 96px = 2.54cm)\n\n\npx*\npixels (1px = 1/96th of 1in)\n\n\npt\npoints (1pt = 1/72 of 1in)\n\n\npc\npicas (1pc = 12 pt)\n\n\n\nRelative Lengths\n\nThe em and rem units are practical in creating perfectly scalable layout! * Viewport = the browser window size. If the viewport is 50cm wide, 1vw = 0.5cm.\n\n\n\n\n\n\nem\nRelative to the font-size of the element (2em means 2 times the size of the current font)\n\n\nex\nRelative to the x-height of the current font (rarely used)\n\n\nch\nRelative to the width of the ‚Äú0‚Äù (zero)\n\n\nrem\nRelative to font-size of the root element\n\n\nvw\nRelative to 1% of the width of the viewport*\n\n\nvh\nRelative to 1% of the height of the viewport*\n\n\nvmin\nRelative to 1% of viewport‚Äôs* smaller dimension\n\n\nvmax\nRelative to 1% of viewport‚Äôs* larger dimension\n\n\n%\nRelative to the parent element\n\n\n\n\nCSS formula to make font size responsive to screen size (article)\n:root {\n  font-size: calc(1rem + 0.25vw);\n}\nFont Weight\n\n400 is the same as normal, and 700 is the same as bold\n\nFonts\n\nAdelle\n\nA serif font that doesn‚Äôt go overboard. Good for short paragraphs.\n\nAlegreya\nBarlow\n\nSlender font\n\nFira Code Retina\n\ncode syntax highlighting\n@import url(‚Äúhttps://cdn.rawgit.com/tonsky/FiraCode/1.205/distr/fira_code.css‚Äù);\n\nLora\n\nbody\nUsed in COVID-19 project &gt;&gt; Static Charts, Hospitals\n@import url(‚Äòhttps://fonts.googleapis.com/css2?family=Lora&display=swap‚Äô);\n\nMerriweather\n\nSimilar to Adelle, but has a bit more pronounced hooks\n\nMontserrat\n\nSimple design that can handle long lines of text. I like it for minimal plots.\n\nPrata\n\nheader\nUsed in ericbook-distill\n@import url(‚Äòhttps://fonts.googleapis.com/css2?family=Cinzel&display=swap‚Äô);\n\nReforma family\n\nonly one I have is Roboto, need to import and load the rest using extrafont pkg\n\nRoboto family\n\nDancho shiny apps\n\np, body: 100 wt\nHeaders, (h1, h2, etc.): 400 wt\n\nRoboto Slab\n\nNot sure if this is exact font used but it‚Äôs very similar. Only difference I spotted was the ‚Äú3.‚Äù\n\n\nTitillium Web Bold\n\nheaders\nUsed in ebtools\n@import url(‚Äòhttps://fonts.googleapis.com/css?family=Titillium+Web&display=swap‚Äô);\n\n\nNumbers\n\nshould all have the same height (Lining)\nshould all have the same width (Tabular)\n\nUsing {showtext}\nlibrary(showtext)\n#load font\nfont_add_google(name = \"Metal Mania\", family = \"metal\")\nfont_add_google(name = \"Montserrat\", family = \"montserrat\")\nshowtext_auto()",
    "crumbs": [
      "Visualization",
      "General"
    ]
  },
  {
    "objectID": "qmd/visualization-general.html#sec-vis-gen-annot",
    "href": "qmd/visualization-general.html#sec-vis-gen-annot",
    "title": "General",
    "section": "Annotation",
    "text": "Annotation\n\nPeople love annotations (thread, paper). More text, the better.\n\nTheir takeaway from the chart is more likely to resemble the annotation if it takes the form of L2 and/or L4 and is close to the data\n\nExample: Financial Times\n\n\nTitle (L2) is used for part of the takaway message\n\nSubtitle used to describe the Y-Axis\n\nChart annotation paragraph (L4) gives contextual information\n\n\nWhen to annotate\n\na design element in your visualization that needs explaining\na data point or series that you want readers to see, like an outlier\nreaders should know something to better understand why certain data points look the way they do\n\nRemove the color key/legend and directly label your categories\n\nIf the screen is small (e.g.¬†mobile), then it‚Äôs better to keep the legend\n\nMake it obvious which units your data uses.\n\nDon‚Äôt just put units in the description, but also in axis labels, tooltips, and annotations\n\nFor large numbers (e.g.¬†20 million), try to use B, M, K instead of an annotation somewhere that says something like ‚Äúin thousand‚Äù\nTooltips\n\nConsider not just stating the numbers in tooltips, but also the category\n\ne.g.¬†‚Äú3.4% unemployed‚Äù instead of ‚Äú3.4%,‚Äù or ‚Äú+16% revenue‚Äù instead of ‚Äú+16%‚Äù\n\nUse a transparent background by setting the alpha channel of CSS background-colorto a number less than 1\n\ne.g.¬†0.3 using rgba(255, 255, 255, 0.3)\n\nWith a transparent background, text behind the tooltip can interfere with the text in the tooltip, so also apply backdrop-filter\n\nExample:\n.tooltip {\n¬† background-color: rgba(255, 255, 255, 0.3);\n¬† -webkit-backdrop-filter: blur(2px);\n¬† backdrop-filter: blur(2px);\n}\n@media (prefers-contrast: more) {\n¬† .tooltip {\n¬† ¬† background-color: white;\n¬† ¬† -webkit-backdrop-filter: none;\n¬† ¬† backdrop-filter: none;\n¬† }\n}\n\nExample shows a tooltip that has an HTML class of ‚Äútooltip‚Äù.\nblur is measured in pixels and the image size varies with screen width, so the optimal blur size here may vary for you depending on the dimensions of your browser window.\n\nApplies a Gaussian blur to the target element‚Äôs background with the standard deviation specified as the argument (e.g.¬†two pixels).\n\nAs of Mar 2023, doesn‚Äôt work on Safari, so adding -webkit-backdrop-filter allows it to work on Safari\n@media (prefers-contrast: more)checks if your user has informed their operating system or browser that they prefer increased contrast. When they do, this chunk then overrides the applied styles.\n\n\n\nTransparent backgrounds might work better with thematic maps and less with scatter plots\nDon‚Äôt center-align your text\nUse straightforward phrasings\nMove axis labels nearest the most important chart objects (e.g.¬†bars)\n\n\nIf the higher bars are what‚Äôs most important and they‚Äôre on the right, then usea right-side axis\n\nFonts for annotation\n\n\nUse what readers are most used to (e.g.¬†sans-serif regular, &gt;12px, (almost) black text\nIf you need to need a lot of words and they don‚Äôt fit, don‚Äôt use smaller font, use a tooltip instead\n\nOn mobile screens you can also hide the least important annotations, or move them below the visualization\n\n\nLead the eye with font sizes, styles, and colors\n\n\nThe biggest and boldest text with the highest contrast against the background should be reserved for the most important information.\n\nDon‚Äôt overdo it though\n\nUse only two levels of hierarchy that are clearly different from each other ‚Äî like a 12px gray and a 14px black\nEmphasize within the annotations using boldness\n\nKeep labels horizontal\n\n\nUse a text outline\n\n\nSet the stroke around your letters, using the background color of your chart.\n\nBe conversational first and precise later",
    "crumbs": [
      "Visualization",
      "General"
    ]
  },
  {
    "objectID": "qmd/visualization-general.html#sec-vis-gen-color",
    "href": "qmd/visualization-general.html#sec-vis-gen-color",
    "title": "General",
    "section": "Color",
    "text": "Color\n\nMisc\n\nWhen choosing bg and fg colors, keep in mind that it‚Äôs generally a good idea to pick colors with a similar hue but a large difference in their luminance.\nDatawrapper guide\nWhen using several subplots together to tell a story and they each have their own color scheme. Blend a color into each color scheme to produce a more unified look\n\nExample: Blending blue into a plot with green color scheme.\n\n\nBreakpoints for scales\n\nHow to choose an interpolation for your color scale\n\nCharts (see prismatic PKG to do this manipulation within ggplot)\nPalette composition methods\n\nComplimentary\n\nopposite sides of the color wheel (2 colors)\ncontrast\n\nAnalogous\n\nsame side of the color wheel (multiple)\ngradient\n\nTriadic\n\nforms triangle on the color wheel\nvibrant, contrast\n\nOthers\n\nsplit complimentary (popular)\n\nComprised of one color and two colors symmetrically placed around it. This strategy adds more variety than complementary color schemes by including three hues without being too jarring or bold. Using this method, we end up with combinations that include warm and cool hues that are more easily balanced than the complementary color schemes\n\nquadratic\n\n\nAdjustments once you chosen a color (hue) to create variations\n\nMove brightness up for lighter variations and down for darker variations\nThen, move saturation in the opposite way you moved brightness\n\nSave colors you find attractive\n\ninstant eyedropper (windows)\nThen use HSL (hue, saturation, lightness) slider for adjustments\n\nBackgrounds\n\nWhite\n\nbright, used a lot\ntry ivory or a light gray\nshades of eggshell, link\n\nAvoid black (or REALLY dark) unless situation calls for it\n\ndark is fine\n\n\nLightest and darkest colors should have meaning (e.g.¬†min, max, mean, zero) and not just some arbitrary numbers\n\nWhat to do when you have a lot of categories\n\nSimply don‚Äôt show different colors Does your chart work without colors?\n\n1 color and a discrete axis with the categories\n\nShow shades, not hues Can you make the chart less confetti-like?\n\nAlthough, consider not using shades when the parts are as or more important than the totals\n\nEmphasize Can you only use color for your most important categories?\nLabel directly Can you use the same or similar colors but label them?\nMerge categories Can you put categories together?\nGroup categories, but keep showing them Can strokes help to tell categories apart?\n\nChange the chart type Will another chart type rely less on colors?\n‚ÄúSmall multiply‚Äù it Can you split the categories into multiple charts? (i.e.¬†facet by category)\nAdd other indicators Can you add symbols, patterns, line widths, or dashes?\n\n\nDoesn‚Äôt use any color ‚Äî just opacity, thickness, and dotted lines.\n\nUse tooltips and hover effects Can smaller categories be hidden with them?\n\nColor scales should be chosen to best match the data values and plot type: If the goal is to show magnitude, a univariate color scheme is typically preferable, while a double-ended color scale is typically more effective when showing data that differ in sign and magnitude. Where possible, color scales should use a minimal number of hues, varying intensity or lightness of the color to show magnitude, and transitioning through neutral colors (white, light yellow) when utilizing a gradient. Cognitive load can also be reduced by selecting colors with cultural associations that match the data display, such as the use of blue for men and red (or pink) for women, or the use of blue for cold temperatures and red/orange for warm temperatures.\n\nIt is also important to consider the human perceptual system, which does not perceive hues uniformly: We can distinguish more shades of green than any other hue, and fewer shades of yellow, so green univariate color schemes will provide finer discriminability than other colors because the human perceptual system evolved to work in the natural world, where shades of green are plentiful.\n\n\nFigure above shows the International Commission on Illumination (CIE) 1931 color space, which maps the wavelength of a color to a physiologically based perceptual space; a significant portion of the color space is dedicated to greens and blues, while much smaller regions are dedicated to violet, red, orange, and yellow colors. This unevenness in mapping color is one reason that the multi-hued rainbow color scheme is suboptimal‚Äîthe distance between points in a given color space may not be the same as the distance between points in perceptual space. As a result of the uneven mapping between color space and perceptual space, multi-hued color schemes are not recommended.",
    "crumbs": [
      "Visualization",
      "General"
    ]
  },
  {
    "objectID": "qmd/visualization-general.html#sec-vis-gen-types",
    "href": "qmd/visualization-general.html#sec-vis-gen-types",
    "title": "General",
    "section": "Chart Types",
    "text": "Chart Types\n\nBar Graphs\n\nDon‚Äôt use bar graphs for anything except counts. Audiences have trouble with the abstraction.\nFor averages, used errorbar charts or use median + raincloud.\nGuide\n\nDesign Examples (link)\n\n\nStacked Bar\n\nReplacement for pie charts et al when dealing with fractional data\nAlways reorder stacks\n\n\nRmd tutorial for reordering optimation\n\n\nBox Plots\n\n\nSmall data - emphasize the points\nLarge data - emphasize the box\n\nLine Charts\n\nSometimes it‚Äôs appropriate not to use zero as the baseline\nHaving the y-axis not intersect the x-axis can minimize the risk of confusing the readers with a non-zero baseline chart\n\nTime Series of ordinal discrete data by category\n\n\nordinal data has 3 levels\n\n\nHeatmaps\n\nReorder rows and columns to produce a more meaningful visualization\n\n\nGuide on reordering heatmaps\nIf order is important, then this may not be possible\n\nReorder by clustering\n\n\nNetwork Graphs\n\nAlways try different multiple layout methodologies",
    "crumbs": [
      "Visualization",
      "General"
    ]
  },
  {
    "objectID": "qmd/visualization-general.html#sec-vis-gen-maps",
    "href": "qmd/visualization-general.html#sec-vis-gen-maps",
    "title": "General",
    "section": "Maps",
    "text": "Maps\n\nAbove rules also apply\nRemove as many extraneous elements as possible\n\nHard because maps have so many necessary elements\n\nBorders, Labels, etc.\n\nIn cloropleths, remove unnecessary borders (e.g.¬†along coastlines)\n\n\n‚ÄúBorders as lines‚Äù is much less cluttered\nArticle, rmapshaper::ms_innerlines() keeps only the necessary inner borders in the ‚Äúgeometry‚Äù column of the spatial dataset.\n\n\nPay close attention to typography hierarchy\n\nBold, Font size, etc\n\nUse iconography to help users identify what you want them to see\nNumeric values (thread)\n\nPalettes: use a sequential (top row) or diverging (bottom row)\n\n\nFor diverging palettes\n\n\nThe middle value should be light on a light background (top left) or dark on a dark background (bottom left)\n\n\nBackgrounds:\n\n\nLight background: darker color on the value of interest (usually the higher value) (top left)\nDark background: lighter color on the value of interest (usually the higher value) (bottom left)\n\n\nTry not to use Rainbow palettes, because they are misleading\n\nThe rainbow and jet colors are problematic as the change in color is not perceptually uniform, leading to distinct ‚Äòbands‚Äô of certain colors. This causes misleading jumps and emphasizes certain values, most likely without the intention to highlight them. (Cedric Scherer)\n(acceptable) rainbow called ‚ÄúTurbo‚Äù if you need one (article)\n\n\nCode - see comments for links to R scripts and improved versions of Turbo\n\nOther Alternatives",
    "crumbs": [
      "Visualization",
      "General"
    ]
  },
  {
    "objectID": "qmd/visualization-general.html#sec-vis-gen-area",
    "href": "qmd/visualization-general.html#sec-vis-gen-area",
    "title": "General",
    "section": "Area",
    "text": "Area\n\nIn general, these charts aren‚Äôt good for noisy data and data with many categories\n\nHave issues when values increase sharply (see video. around 50:13)\n\nExperiment with the order of the groups\n\nEvents that you‚Äôre looking for are probably only visable when there‚Äôs a particular order\nMost of the time, putting the most stable groups at the bottom produces the best results",
    "crumbs": [
      "Visualization",
      "General"
    ]
  },
  {
    "objectID": "qmd/visualization-general.html#sec-vis-gen-ts",
    "href": "qmd/visualization-general.html#sec-vis-gen-ts",
    "title": "General",
    "section": "Time Series",
    "text": "Time Series\n\nHorizon Charts\n\nSee Anomaly Detection &gt;&gt; Charts\nEspecially useful for showing data with large amplitudes in a short vertical space",
    "crumbs": [
      "Visualization",
      "General"
    ]
  },
  {
    "objectID": "qmd/visualization-general.html#sec-vis-gen-uncert",
    "href": "qmd/visualization-general.html#sec-vis-gen-uncert",
    "title": "General",
    "section": "Uncertainty",
    "text": "Uncertainty\n\nVisualizing only inferential uncertainty can lead to significant overestimates of treatment effects\n\n\nWhen possible, plot individual data points alongside statistical estimates\n\nTranslate percentages into counts (e.g.¬†‚Äúa 1 out of 5 chance‚Äù rather than ‚Äúa 20% chance‚Äù)\n\n\n{riskyr} - icon arrays and less sophisticated viz for the above chart\nicon arrays\n\nExamples\n\nbase rates and error rates (paper)\nrelative risks (paper)\n\n\nWaffle plots are similar to icon arrays\n\nquantile dotplots\n\n{ggdist} (many examples and flavors)\n\nhypothetical outcome plots\n\nConsists of multiple individual plots (frames), each of which depicts one draw from a distribution (use case for animation)\nBest suited for multivariate judgments like how reliable a perceived difference between two random variables is\nIllustration of the process\n\n\nYou create a distribution to sample from or using known distribution and parameters or bootstrapping the sample and sample from each bootstrap.\nEach sample/draw is presented on the right side of the distribution plot (fig 1) (final product)\n\nI think it would be better if after each draw the previous draw remained but was de-emphasized (i.e.¬†turned light gray)\nAnother example would McElreath‚Äôs lecture video on posterior prediction distribution.\n\nFigs 2 and 3 show a sequence of draws from a joint distribution of uncorrelated variables (fig 2) and correlated variables (fig 3)\n\nExample: NYT on interpreting jobs reports\n\n\n\n2 facets: accelerating job growth (left), steady job growth (right)\nFor each facet,\n\nthe left plot is static, and the right plot is animated showing different noisy samples of the same underlying dgp\nthe left plot shows what normals perceive the distribution to look like for the given interpretation (e.g.¬†accelerating job growth), and the right plot shows what real (i.e.¬†noisy) data with the same interpretion looks like.\n\n\n\n\nFan charts\n\n\nshows a 90% interval broken divided into 30% increments (left) or 10% increments (right)\n\nShow previous forecasts\n\n\nTruth is in dark blue with light blue branches showing previous forecasts",
    "crumbs": [
      "Visualization",
      "General"
    ]
  },
  {
    "objectID": "qmd/visualization-general.html#sec-vis-gen-mob",
    "href": "qmd/visualization-general.html#sec-vis-gen-mob",
    "title": "General",
    "section": "Mobile",
    "text": "Mobile\n\nMisc\n\nRStudio plots are displayed in 96 dpi and ggsave uses 300 dpi as default\n\ni.e.¬†viewed plots won‚Äôt look the same as the saved plots using default settings\n\n\nUse sharp color contrasts when highlighting\nMinimal readable size is 16, but 22 is recommended\nAspect ratio of 4:3 or 1024 x 768 pixels\n\nAnother article say 1:2\n\nBar Charts should be horizontal to make charts with many categories readable\n\nMobile screens are more tall than wide so labels on the y-axis makes more sense than on the x-axis\n\nR\n\nSet-up external window with aspect ratio (e.g.¬†1:2)\ndev.new(width=1080, height=2160, unit=\"px\", noRStudioGD = TRUE)\n\nnoRStudioGD = TRUE says any new plots appear in the new graphics window rather than the RStudio graphics device\nCan also use windows(), x11(), or png() from {ragg}\n\n\nUse Quarto (or Rmd) for developement\n#| dpi: 300     \n#| fig.height: 7.2     \n#| fig.width: 3.6     \n#| dev: \"png\"     \n#| echo: false     \n#| warning: false     \n#| message: false`\n\nThis way your dpi and aspect ratio are set and you can view the final output without having to save the png and viewing it separately to see how it looks\nfig.height and fig.width are always given in inches\n\nIf you haven‚Äôt set your Quarto document to be self-contained, then the images have also already been saved for you - probably in a folder called documentname_files/figure-html/",
    "crumbs": [
      "Visualization",
      "General"
    ]
  },
  {
    "objectID": "qmd/feature-engineering-general.html",
    "href": "qmd/feature-engineering-general.html",
    "title": "General",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Feature Engineering",
      "General"
    ]
  },
  {
    "objectID": "qmd/feature-engineering-general.html#sec-feat-eng-gen-misc",
    "href": "qmd/feature-engineering-general.html#sec-feat-eng-gen-misc",
    "title": "General",
    "section": "",
    "text": "Tree-based Models\n\nFrom Uber, ‚ÄúTree-based models are performing piecewise linear functional approximation, which is not good at capturing complex, non-linear interaction effects.‚Äù\n\nWith regression models, you have to be careful about encoding categoricals as ordinal (i.e.¬†integers) which means one-hot encoding is better.\n\nFor example, the raw numerical encoding (0-24) of the ‚Äúhour‚Äù feature prevents the linear model from recognizing that an increase of hour in the morning from 6 to 8 should have a strong positive impact on the number of bike rentals while a increase of similar magnitude in the evening from 18 to 20 should have a strong negative impact on the predicted number of bike rentals.\n\nModels with large numbers (100s) of features increases the opportunity for feature drift\nZero-Inflated Predictors/Features\n\nFor ML, transformations probably not necessary\nFor regression\n\nlog(x + 0.05)\n\nlarger effect on skew than sqrt\n\narcsinh(x) (see Continuous &gt;&gt; Transformations &gt;&gt; Logging)\n\napproximates a log but handles 0s\n\nsqrt maybe Yeo-Johnson (?)\n\n&gt;60% of values = 0, consider binning or binary",
    "crumbs": [
      "Feature Engineering",
      "General"
    ]
  },
  {
    "objectID": "qmd/feature-engineering-general.html#sec-feat-eng-gen-cont",
    "href": "qmd/feature-engineering-general.html#sec-feat-eng-gen-cont",
    "title": "General",
    "section": "Continuous",
    "text": "Continuous\n\nBinning\n\nBenefits\n\nReduces Noise\n\nContinuous variables tend to store information with minute fluctuations that provide no added value for the machine learning task of interest\n\nMakes the feature more intuitive\n\nIs there an important threshold value?\n\n1 value ‚Äì&gt; split into a binary\nmultiple values ‚Äì&gt; multinomial\n\n\nMinimizes outlier influence\n\nBin and Embed\n\nSteps\n\nFind bin ranges\n\nIf sufficient data, calculate quantiles of the numeric vector to find the bin ranges\nsklearn.preprocessing.KBinsDiscretizer has a few different methods\nUse some other method to find the number/ranges of bins (see R packages)\n\nUse the indices of the bins (i.e.¬†leftmost bin is 1, 2nd leftmost bin is 2) to discretize each value of the numeric\n\nMight need to be one-hot coded\n\nCreate an embedding of the discretized vector and use the embedding as features.\n\n\nDichotomizing is bad (post, list of papers)\n\nTypical arguments for splitting (even when there‚Äôs no underlying reason to do so) include: simplifies the statistical analysis and leads to easy interpretation and presentation of results\n\nExample: splitting at the median‚Äîleads to a comparison of groups of individuals with high or low values of the measurement, leading in the simplest case to a t test or œá2 test and an estimate of the difference between the groups (with its confidence interval) on another variable.\n\nUsing multiple categories (to create an ‚Äúordinal‚Äù variable) is generally preferable , and using four or five groups the loss of information can be quite small\nIssues:\n\nInformation is lost, so the statistical power to detect a relation between the variable and patient outcome is reduced.\n\nDichotomising a variable at the median reduces power by the same amount as would discarding a third of the data\n\nMay increase the risk of a positive result being a false positive\nMay seriously underestimate the extent of variation in outcome between groups, such as the risk of some event, and considerable variability may be subsumed within each group.\nIndividuals close to but on opposite sides of the cutpoint are characterised as being very different rather than very similar.\nConceals any non-linearity in the relation between the variable and outcome\nUsing a stat like median for a cutpoint means studies will have different cutpoints, therefore results cannot easily be compared, seriously hampering meta-analysis of observational studies\nAn ‚Äúoptimal‚Äù cutpoint (usually that giving the minimum P value) runs a high risk of a spuriously significant result. Effect will be overestimated and the CI too narrow\nAdjusting for the effect of a confounding variable, dichotomisation will run the risk that a substantial part of the confounding remains\n\n\nHarrell\n\nThink most of these issues are related to inference models like types of logistic regression\nA better approach that maximizes power and that only assumes a smooth relationship is to use a restricted cubic spline (regression spline; piecewise cubic polynomial) function for predictors that are not known to predict linearly. Use of flexible parametric approaches such as this allows standard inference techniques (P -values, confidence limits) to be used (See Feature Engineering, Splines)\nIssues with binning continuous variables\n\nIf cutpoints are chosen by trial and error in a way that utilizes the response, even informally, ordinary P -values will be too small and confidence intervals will not have the claimed coverage probabilities.\n\nThe correct Monte-Carlo simulations must take into account both multiple tests and uncertainty in the choice of cutpoints.\n\nUsing the ‚Äúminimum p-value approach‚Äù often results in multiple cutpoints so ¬Ø\\_(„ÉÑ)_/¬Ø plus multiple testing p-value adjustments need to be used.\n\nThis approach involves testing multiple cutpoints and choosing one that minimizes the p-value below a threshold.\n\nOptimal cutpoints often change from sample to sample\nThe optimal cutpoint for a predictor would necessarily be a function of the continuous values of all the other predictors\nYou‚Äôre losing variation (information) which causes a loss of power and precision\nAssumes that the relationship between the predictor and the response is flat within each interval\n\nthis assumption is far less reasonable than a linearity assumption in most cases\n\nPercentiles\n\nUsually estimated from the data at hand, are estimated with sampling error, and do not relate to percentiles of the same variable in a population\nValue of binned variable potentially takes on a different relationship with the outcome\n\ne.g.¬†Body Mass Index has a smooth relationship with every outcome studied, and relates to outcome according to anatomy and physiology. Binning may change that relationship to being how many subjects have a similar BMI.\n\n\nMany bins usually required to make it worth it. Therefore, many dummy variables will end up being created resulting in a loss of power and precision. (i.e.¬†more bins = more variables = more dof used)\nPoor predictive performance with Cox regression models\n\n\nThey might help with prediction using ML or DL models though\n\n‚ÄúInstead of directly using marketplace health as a continuous feature, we decided to use a form of target-encoding by splitting up the metric into buckets and taking the average historical delivery duration within that bucket as the new feature. With this approach, we directly helped the model learn that very supply-constrained market conditions are correlated with very high delivery times ‚Äî rather than relying on the model to learn those patterns from the relatively sparse data available.‚Äù\n\nImproving ETA Prediction Accuracy for Long Tail Events\nHelps to ‚Äúrepresent features in a way that makes it easy for the model to learn sparse patterns.‚Äù\n\nThis article was about modeling tail events, so maybe this is most useful for features that have an association with the tail values in the outcome variable\n\n\nXGBoost seems to like numerics much more than dummies\n\nTrees may prefer larger cardinalities. So if you do bin, you‚Äôd probably want quite a few bins\nNever really seen a binned age variable do well, so guessing more than 10 at least. Though maybe Age just wasn‚Äôt important enough.\n\n\nExamples\n\nBinary\n\nWhether a user spent more than $50 or didn‚Äôt\nIf user had activity on the weekend or not\n\nMultinomial or Discrete\n\nTimestamp to morning/afternoon/ night,\nOrder values into buckets of $10‚Äì20, $20‚Äì30, $30+\nHeight, age\n\nExample: step_discretize\ndata(ames, package = \"modeldata\")\n\nrecipe(~ Lot_Frontage + Lot_Area, data = ames) |&gt;\n  step_discretize(all_numeric_predictors(), num_breaks = 5) |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\n#&gt; # A tibble: 2,930 √ó 2\n#&gt;    Lot_Frontage Lot_Area\n#&gt;    &lt;fct&gt;        &lt;fct&gt;   \n#&gt;  1 bin5         bin5    \n#&gt;  2 bin4         bin4    \n#&gt;  3 bin5         bin5    \n#&gt;  4 bin5         bin4    \n#&gt;  5 bin4         bin5    \n#&gt;  6 bin4         bin3    \n#&gt;  7 bin2         bin1    \n#&gt;  8 bin2         bin1    \n#&gt;  9 bin2         bin1    \n#&gt; 10 bin2         bin2    \n#&gt; # ‚Ñπ 2,920 more rows\n\n\n\n\nTransformations\n\nMisc\n\nAlso see:\n\nRegression, Linear &gt;&gt; Transformations\nFeature Engineering, Splines\n\nCentering\n\nNo matter how a variable is centered (e.g.¬†around the mean, median, or other number), its linear regression coefficient will not change - only the intercept will change.\n\nGuide for choosing a scaling method for classification modeling\n\nNotes from The Mystery of Feature Scaling is Finally Solved (narrator: it wasn‚Äôt)\n\nOnly used a SVM model for experimentation so who knows if this carries over to other classifiers\n\ntldr\n\nGot time and compute resources? ‚Äì&gt; Ensemble different standardization methods using averaging\nNo time and limited compute resources ‚Äì&gt; standardization\n\nModels that are distribution independent or distance sensitive (e.g.¬†SVM, kNN, ANNs) should use standardization\n\nModels that are distribution dependent (e.g.¬†regularized linear regression, regularized logistic regression, or linear discriminant analysis) weren‚Äôt tested\n\nNo evidence that data-centric rules (e.g.¬†normal or non-normal distributed variables, outliers present)\nFeature scaling that is aligned with the data or model can be responsible for overfitting\nEnsembling by averaging (instead of using a model to ensemble) different standarization methods\n\nExperiment used robust scaler (see below) and z-score standardization\n\nWhen they added a 3rd method it created more biased results\n\nRequires predictions to be probabilities\n\nFor ML models, this takes longer because an extra CV has to be run\n\n\n\n\n\n\nStandardization\n\nThe standard method transforms feature to have mean = 0, and standard deviation = 1\n\nNot robust to outliers\n\nFeature will be skewed\n\n\nUsing the median to center and the MAD to scale makes the transformation robust to outliers\nScaling by 2 sd/MAD instead of 1 sd/MAD can be useful to obtain model coefficients of continuous parameters comparable to coefficients related to binary predictors, when applied to the predictors (not the outcome)\nNotes from\n\nWhen conducting multiple regression, when should you center your predictor variables & when should you standardize them?\n\nReasons to standardize\n\nMost ML/DL models require it\n\nMany elements used in the objective function of a learning algorithm (such as the RBF kernel of Support Vector Machines or the l1 and l2 regularizers of linear models) assume that all features are centered around zero and have variance in the same order. If a feature has a variance that is orders of magnitude larger than others, it might dominate the objective function and make the estimator unable to learn from other features correctly as expected.\n\nMost Clustering methods require it\nPCA can only be interpreted as the singular value decomposition of a data matrix when the columns have centered\nInterpreting the intercept as the mean of the outcome when all predictors are held at their means\nPredictors with large values (country populations) can have really small regression coefficients. Standardization makes the coefficients have a more managable scale.\nSome types of models are more numerically stable with the predictors have been standardized\nEasier to set priors in Bayesian modeling\nCentering fixes collinearity issues when creating powers and interaction terms\n\nCollinearity between the created terms and the main effects\n\n\nOther Reasons why you might want to:\n\nCreating a composite score\n\nWhen you‚Äôre trying to sum or average variables that are on different scales, perhaps to create a composite score of some kind. Without scaling, it may be the case that one variable has a larger impact on the sum due purely to its scale, which may be undesirable.\nOther Examples:\n\nResearch into children‚Äôs behavioral disorders - researchers might get ratings from both parents & teachers, & then want to combine them into a single measure of maladjustment.\nStudy on the activity level at a nursing home w/ self-ratings by residents & the number of signatures on sign-up sheets for activities\n\n\nTo simplify calculations and notation.\n\nA sample covariance matrix of values that has been centered by their sample means is simply X‚Ä≤X (correlation matrix)\nIf a univariate random variable, X, has been mean centered, then var(X)=E(X2) and the variance can be estimated from a sample by looking at the sample mean of the squares of the observed values.\n\n\nReasons NOT to standardize\n\nWe don‚Äôt want to standardize when the value of 0 is meaningful.\n\nstep_best_normalize\n\nRequires {bestNormalize} and has bestNormalize for use outside of {tidymodels}\nChooses the best standardization method using repeated cross-validation to estimate the Pearson‚Äôs P statistic divided by its degrees of freedom (from {nortest}) which indicates closness to the Gaussian distribution.\nPackage features the method, Ordered Quantile normalization (orderNorm, or ORQ). ORQ transforms the data based off of a rank mapping to the normal distribution.\nAlso includes: Lambert W\\(\\times\\)F, Box Cox, Yeo-Johnson, arcsinh, exponential, log, square root, and has a method to add your own.\nExample\nlibrary(bestNormalize)\n\ndata(ames, package = \"modeldata\")\n\nrecipe(Sale_Price ~ Lot_Frontage + Lot_Area, data = ames) |&gt;\n  step_best_normalize(all_numeric_predictors()) |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\n#&gt; # A tibble: 2,930 √ó 3\n#&gt;    Lot_Frontage Lot_Area Sale_Price\n#&gt;           &lt;dbl&gt;    &lt;dbl&gt;      &lt;int&gt;\n#&gt;  1        2.48     2.29      215000\n#&gt;  2        0.789    0.689     105000\n#&gt;  3        0.883    1.28      172000\n#&gt;  4        1.33     0.574     244000\n#&gt;  5        0.468    1.19      189900\n#&gt;  6        0.656    0.201     195500\n#&gt;  7       -0.702   -1.27      213500\n#&gt;  8       -0.669   -1.24      191500\n#&gt;  9       -0.735   -1.19      236500\n#&gt; 10       -0.170   -0.654     189000\n#&gt; # ‚Ñπ 2,920 more rows\n\nscale(var or matrix)\n\nDefault args: center = T, scale = T\nStandardizes each column of a matrix separately\nFYI scale(var) == scale(scale(var))\n\n{datawizard::standardize} - Can center by median and scale by MAD (robust), can scale by 2sd (Gelman)\n{{sklearn::RobustScaler}}\n\nStandardize by median and IQR instead of mean and sd\n\n(value ‚àí median) / IQR\n\nThe resulting variable has a zero mean and median and a standard deviation of 1, although not skewed by outliers and the outliers are still present with the same relative relationships to other values.\nstep_normalize has means, sd args, so it might be able to do this\n\nHarrell recommends substituting the gini mean difference for the standard deviation\n\nGini‚Äôs mean difference - the mean absolute difference between any two distinct elements of a vector.\n\n\nHmisc::GiniMd(x, na.rm = F) (doc)\nsjstats::gmd(x or df, ...) (doc)\n\nIf ‚Äúdf‚Äù then it will compute gmd for all vectors in the df\n‚Äú‚Ä¶‚Äù allows for use of tidy selectors\n\nManual\ngmd &lt;- function(x) {\n¬† n &lt;- length(x)\n¬† sum(outer(x, x, function(a, b) abs(a - b))) / n / (n - 1)\n¬† }\n\n\n\n\n\nRescaling/Normalization\n\nMisc\n\nIf the values of the feature get rescaled between 0 and 1, i.e.¬†[0,1], then it‚Äôs called normalization\nExcept in min/max, all values of the scaling variable should be &gt; 0 since you can‚Äôt divide by 0\n{datawizard::rescale} - Scales variable to a specified range\n\nMin/Max\n\nRange: [0, 1]\n\n\nMake sure the min max value are NOT outliers. If they are outliers, then the range of your data will be more constricted that it needs to be.\n\ne.g.¬†if values are in between 100 and 500 with an exceptional value of 25000, then 25000 is scaled as 1 and all the other values become very close to the lower bound of zero\n\nExample: Age is the predictor and Happiness is the outcome. Imagine a very strong relationship between age and happiness, such that happiness is at its maximum at age 18 and its minimum at age 65. It‚Äôll be easier if we rescale age so that the range from 18 to 65 is one unit. Now this new variable A ranges from 0 to 1, where 0 is age 18 and 1 is age 65. (from Statistical Rethinking section 6.3.1 pg 182)\nd2 &lt;- d[ d$age&gt;17 , ] # only adults\nd2$A &lt;- ( d2$age - 18 ) / ( 65 - 18 )\n\nRange: [a, b]\n\nAlso see notebook for code to transform more than 1 variable at a time.\n\nBy max\nscaled_var = var/max(var)\n\nExample: From Statistical Rethinking, pg 246\n\n‚Äú‚Ä¶ zero ruggedness is meaningful. So instead terrain ruggedness is divided by the maximum value observed. This means it ends up scaled from totally flat (zero) to the maximum in the sample at 1 (Lesotho, a very rugged and beautiful place).‚Äù\n\nExample: From Statistical Rethinking, pg 258\n\n‚ÄúI‚Äôve scaled blooms by its maximum observed value, for three reasons. First, the large values on the raw scale will make optimization difficult. Second, it will be easier to assign a reasonable prior this way. Third, we don‚Äôt want to standardize blooms, because zero is a meaningful boundary we want to preserve.‚Äù\n\nblooms is bloom size. So there can‚Äôt be a negative but zero makes sense.\nblooms is 2 magnitudes larger than both its predictors.\n\n\n\nBy mean\nscaled_var = var/mean(var)\n\nExample: From Statistical Rethinking, pg 246\n\n‚Äúlog GDP is divided by the average value. So it is rescaled as a proportion of the international average. 1 means average, 0.8 means 80% of the average, and 1.1 means 10% more than average.‚Äù\n\n\n\n\n\nLogging\n\nUseful for skewed variables\nIf you have zeros, then its common to add 1 to the predictor values\n\nTo backtransform: exp(logged_predictor) - 1\narcsinh(x): approximates a log (at large values of x) but handles 0s:\n\nBacktransform: log(x + sqrt(1+x^2))\n\n* Don‚Äôt use these for outcome variables (See Regression, Other &gt;&gt; Zero-Inflated/Truncated &gt;&gt; Continuous for methods, Thread for discussion and link to a paper on the alternatives)\n\nThe scale of the outcome matters. The thread links to a discussion of a paper on log transforms.\nProposals in the paper are in Section 4.1. One of the recommendations is log(E[Y(0)] + Y) where (I think) E[Y(0)] is the average value of Y when Treatment = 0 but I‚Äôm not sure. Need to read the paper.\n\n\nRemember to back-transform predictions if you transformed the target variable\n# log 10 transformed target variable\npreds_intervals &lt;- predict(\n¬† workflows::pull_workflow_fit(lm_wf),\n¬† workflows::pull_workflow_prepped_recipe(lm_wf) %&gt;% bake(ames_holdout),\n¬† type = \"pred_int\",\n¬† level = 0.90\n) %&gt;%¬†\n¬† mutate(across(contains(\".pred\"), ~10^.x))\nCombos\n\nLog + scale by mean\n\nExample From Statistical Rethinking Ch8 pg 246\n\n‚ÄúRaw magnitudes of GDP aren‚Äôt meaningful to humans. Since wealth generates wealth, it tends to be exponentially related to anything that increases it (earlier in chapter). This is like saying that the absolute distances in wealth grow increasingly large, as nations become wealthier. So when we work with logarithms instead, we can work on a more evenly spaced scale of magnitudes‚Äù\n‚ÄúLog GDP is divided by the average value. So it is rescaled as a proportion of the international average. 1 means average, 0.8 means 80% of the average, and 1.1 means 10% more than average.‚Äù",
    "crumbs": [
      "Feature Engineering",
      "General"
    ]
  },
  {
    "objectID": "qmd/feature-engineering-general.html#sec-feat-eng-gen-disc",
    "href": "qmd/feature-engineering-general.html#sec-feat-eng-gen-disc",
    "title": "General",
    "section": "Discrete",
    "text": "Discrete\n\nQuantitative variables that are countable with no in-between the values. (e.g.¬†integer value variables)\n\ne.g.¬†Age, Height (depending on your scale), Year of Birth, Counts of things\n\nMany variables can be either discrete or continuous depending on whether they are ‚Äúexact‚Äù or have been rounded (i.e.¬†their scale).\n\nTime since event, distance from location\nA zip code would not be a discrete variable since it is not quantitative (i.e.¬†don‚Äôt represent amounts of anything). The values just represent geographical locations and could just as easily be names instead of numbers. There is no inherent meaning to arithmetic operations performed on them (e.g.¬†zip_code1 - 5 has no obvious meaning)\n\nBinning\n\nSee Binning\n\nRange to Average\n\nSo numerical range variables like Age can have greater predictive power in ML/DL algorithms by just using the average value of the range\ne.g.¬†Age == 21 to 30 ‚Äì&gt; (21+30)/2 = 25.5\n\nRates/Ratios\n\nSee Domain Specific\n\nMin/Max Rescaling\n\nSee Continuous &gt;&gt; Transformations &gt;&gt; Rescaling/Normalization",
    "crumbs": [
      "Feature Engineering",
      "General"
    ]
  },
  {
    "objectID": "qmd/feature-engineering-general.html#sec-feats-general-cats",
    "href": "qmd/feature-engineering-general.html#sec-feats-general-cats",
    "title": "General",
    "section": "Categoricals",
    "text": "Categoricals\n\nMisc\n\nSee Feature Engineering, Embeddings &gt;&gt; Engineering\nOne-Hot Encode Issues:\n\nWith high cardinality, the feature space explodes ‚Äì&gt;\n\nLess power\nLikely to encounter memory problems\n\nUsing a sparse matrix is memory efficient which might make the one-hot encode feasible\n\nSparse data sets don‚Äôt work well with highly efficient tree-based algorithms like Random Forest or Gradient Boosting.\n\nModel can‚Äôt determine similarity between categories (embedding does)\nEvery kind of encoding and embedding outperforms it by a lot, especially in tree models\n\n\n\n\nCombine/Lump/Collapse\n\nCollapse categories with similar characteristics to reduce dimensionality\n\nstates to regions (midwest, northwest, etc.)\n\nLump\n\nCat vars with levels with too few counts ‚Äì&gt; lump together into an ‚ÄúOther‚Äù category\nstep_other(cat_var, threshold = 0.01) # see\n\nFor details see Model Building, tidymodels &gt;&gt; Recipe\nLevels with too few data will have large uncertainties about the effect and the bloated std.devs can cause some models to throw errors\n\n\nCombine\n\nThe feature reduction can help when data size is a concern\n\nThink this is equivalent to a cat-cat interaction.¬† ML models usually algorithmically create interactions but I guess this way you get the interaction but with fewer features.\nAlso might be useful to use the same considerations that you use to choose interactions to choose which cat variables to combine.\n\nSteps\n\nCombine var1 and var2 (e.g.¬†‚Äúdog‚Äù, ‚Äúminnesota‚Äù) to create a new feature called var3 (‚Äúdog_minnesota‚Äù).\nRemove individual features (var1 and var2) from the dataset.\nencode (one-hot, dummy, etc.) var 3\n\n\n\n\n\nEncode/Hashing\n\nCat vars with high numbers of levels need encoded\nCan‚Äôt dummy var because it creates too many additional variables ‚Äì&gt; reduces power\nNumeric: as.numeric(as.factor(char_var))\nTarget Encoding\n\n{collinear}\n\ntl;dr; I don‚Äôt see a method that stands out as theoretically better or worse than the others. The rnorm method would probably produce the most variance within the predictor.\ntarget_encoding_lab takes a df and encodes all categoricals using all or some of the methods\nRank (target_encoding_rank): Returns the rank of the group as a integer, starting with 1 as the rank of the group with the lower mean of the response variable\n\nwhite_noise argument might be able to used.\n\nMean (target_encoding_mean): Replaces each value of the categorical variable with the mean of the response across the category the given value belongs to.\n\nThe argument, white_noise, limits potential overfitting. Must be a value betwee 0 and 1. The value added depends on the magnitude of the response. If response is within 0 and 1, a white_noise of 0.25 will add to every value of the encoded variable a random number selected from a normal distribution between -0.25 and 0.25\n\nrnorm (target_encoding_rnorm): Computes the mean and standard deviation of the response for each group of the categorical variable, and uses rnorm() to generate random values from a normal distribution with these parameters.\n\nThe argument rnorm_sd_multiplier is used as a multiplier of the standard deviation to control the range of values produced by rnorm() for each group of the categorical predictor. Values smaller than 1 reduce the spread in the results, while values larger than 1 have the opposite effect.\n\nLOO (target_encoding_loo): Replaces each categorical value with the mean of the response variable across the other cases within the same group.\n\nThe argument, white_noise, limits potential overfitting.\n\n\n{{category_encoders}}\npip install category_encoders\nimport category_encoders as ce\ntarget_encoder = ce.TargetEncoder(cols=['cat_col_1', 'cat_col_2'])\ntarget_encoder.fit(X, y)\nX_transformed = target_encoder.transform(X_pre_encoded)\n\nCatboost Encoder\npip install category_encoders\nimport category_encoders as ce\ntarget_encoder = ce.CatBoostEncoder(cols=['cat_col_1', 'cat_col_2'])\ntarget_encoder.fit(X, y)\nX_transformed = target_encoder.transform(X_pre_encoded)\nBinary Encoding\n\nBenchmarks for decision trees:\n\nNumeric best (&lt; 1000 categories)\nBinary best (&gt; 1000 categories)\n\nStore N cardinalities using ceil(log(N+1)/log(2)) features\n\n\nHashing\n\nI‚Äôm not sure about this method. I‚Äôve seen examples where hashes look very different when 1 small thing changes ‚Äî but maybe it was the algorithm. Also seems like it would be poor security if things that looked similar had similar hashes.\nSee Privacy for hashing packages\nBeyond security and fast look-ups, hashing is used for similarity search.\n\ne.g.¬†Different pictures of the same thing should have similar hashes\nSo, if these hashes are being binned, you‚Äôd want something a hashing algorithm thinks is similar to actually be similar in order for this to be most effective.\n\nzip codes, postal codes, lat + long would be good\nNot countries or counties since I‚Äôd think the hashing similarity would be related to how similar they are alphabetically or maybe phonetically\nMaybe something like latin species names since those have similar roots, etc. would work. (e.g.¬†dogs are canis-whatever)\n\n\nCan‚Äôt be reversed to the original values\n\nAlthough since you have the original, it seems like you could see which cat levels are in a particular hash and maybe glean some latent variable\n\nCreates dummies for each cat but fewer of them.\n\nIt is likely that multiple levels of the column will map to the same hashed columns (even with small data sets). Similarly, it is likely that some columns will have all zeros.\n\nA zero-variance filter (via recipes::step_zv) is recommended for any recipe that uses hashed columns\n\n\ntextrecipes::step_dummy_hash - Dimension Reduction. Create dummy variables, but instead of giving each level its own column, you run the level through a hashing function (MurmurHash3) to determine the column.\n\nnum_terms: Tuning parameter tha controls the number of indices that the hashing function will map to. Since the hashing function can map two different tokens to the same index, will a higher value of num_terms result in a lower chance of collision.\nExample\ndata(ames, package = \"modeldata\")\n\nrecipe(Sale_Price ~ Neighborhood, data = ames) |&gt;\n  step_dummy_hash(Neighborhood, num_terms = 4) |&gt; # Low for example\n  prep() |&gt;\n  bake(new_data = NULL)\n\n#&gt; # A tibble: 2,930 √ó 5\n#&gt;    Sale_Price dummyhash_Neighborhood_1 dummyhash_Neighborhood_2\n#&gt;         &lt;int&gt;                    &lt;int&gt;                    &lt;int&gt;\n#&gt;  1     215000                        0                       -1\n#&gt;  2     105000                        0                       -1\n#&gt;  3     172000                        0                       -1\n#&gt;  4     244000                        0                       -1\n#&gt;  5     189900                        0                        0\n#&gt;  6     195500                        0                        0\n#&gt;  7     213500                        0                        0\n#&gt;  8     191500                        0                        0\n#&gt;  9     236500                        0                        0\n#&gt; 10     189000                        0                        0\n#&gt; # ‚Ñπ 2,920 more rows\n#&gt; # ‚Ñπ 2 more variables: dummyhash_Neighborhood_3 &lt;int&gt;,\n#&gt; #   dummyhash_Neighborhood_4 &lt;int&gt;\n\n\nLikelihood Encodings\n\nEstimate the effect of each of the factor levels on the outcome and these estimates are used as the new encoding. The estimates are estimated by a generalized linear model. This step can be executed without pooling (via glm) or with partial pooling (stan_glm or lmer). Currently implemented for numeric and two-class outcomes.\n{embed}\n\nstep_lencode_glm, step_lencode_bayes , and step_lencode_mixed\n\n\n\n\n\nOrdinal\n\nMisc\n\nIf there are NAs or Unknowns, etc.,\n\nAfter coercing into a numeric/integer, you can convert Unknowns to NA and then impute the variable\n\nAll these encodings will produce the same results for a tree model, since tree-based models rely on variable ranks rather than exact values.\n0 = ‚Äú0 Children‚Äù\n1 = ‚Äú1 Child‚Äù\n2 = ‚Äú2 Children‚Äù\n3 = ‚Äú3 Children‚Äù\n4 = ‚Äú4 or more Children‚Äù\n\n1 = ‚Äú0 Children‚Äù\n2 = ‚Äú1 Child‚Äù\n3 = ‚Äú2 Children‚Äù\n4 = ‚Äú3 Children‚Äù\n5 = ‚Äú4 or more Children‚Äù\n\n-100 = ‚Äú0 Children‚Äù\n-85¬† = ‚Äú1 Child‚Äù\n0¬† ¬† = ‚Äú2 Children‚Äù\n10¬† = ‚Äú3 Children‚Äù\n44¬† = ‚Äú4 or more Children‚Äù\n\nVia {tidymodels}\nstep_mutate(ordinal_factor_var = as_integer(ordinal_factor_var))\n# think this uses as_numeric\nstep_ordinalscore(ordinal_factor_var)\nPolynomial Contrasts\n\nSee the section Kuhn‚Äôs book\n\nRainbow Method (article)\n\nMIsc\n\nCreates an artifical ordinal variable from a nominal variable (i.e.¬†ordering colors according the rainbow, roy.g.biv)\nAt worst, it maintains the signal of a one-hot encode, but with tree models, it results in less splits and therefore a simpler, more efficient, and less overfit model.\nTest psuedo ordinal method by constructing a simple bayesian model with response ~ 0 + ordinal. Then, you extract the posterior for each constructed ordinal level. Pass these posteriors through a constraint that labels draws for that level that are less (or not) than the draws of the previous level. Lastly calculate the proportion of those that were less than in order to get a probability that the predictor is ordered (article &gt;&gt; ‚ÄúThe Model‚Äù section)\n\nCode\ngrid &lt;- data.frame(\n  Layer = c(\"B\", \"C\", \"E\", \"G\", \"I\"),\n  error = 0\n)\n\ngrid_with_mu &lt;- tidybayes::add_linpred_rvars(grid, simple_mod, value = \".mu\")\n\nis_stratified &lt;- with(grid_with_mu, {\n  .mu[Layer == \"B\"] &gt; .mu[Layer == \"C\"] &\n  .mu[Layer == \"C\"] &gt; .mu[Layer == \"E\"] &\n  .mu[Layer == \"E\"] &gt; .mu[Layer == \"G\"] &\n  .mu[Layer == \"G\"] &gt; .mu[Layer == \"I\"]\n})\n\nPr(is_stratified)\n#&gt; [1] 0.78725\n‚ÄúLayer‚Äù is the ordinal variable being tested\nadd_linpred_rvars extracts the mean response posteriors for each level of the variable\nResults strongly suggest that the levels of the variable (‚ÄúLayer‚Äù) are ordered, with a 0.79 posterior probability.\n\n\nMethods:\n\nDomain Knowledge\nVariable Attribute (see examples)\nOthers - Best to compute these on a hold out set, so as not cause data leakage\n\nAssociation with the target variable where the value of association is used to rank the categories\nProportion of the event for a binary target variable where the value of the proportion is used to rank the categories\n\n\nIf it‚Äôs possible, use domain knowledge according the project‚Äôs context to help choose the ranking of the categories.\nThere are always multiple ways to rank the categories, so it may be worthwhile to try multiple versions of the artificial ordinal variable\n\nNot recommended to use more than log‚ÇÇ(K) versions, so as to not surpass the number of variables creating using One-hot (where k is the number of categories)\n\nExample: Vehicle Type\n\nCategories\n\nC: ‚ÄúCompact Car‚Äù\nF: ‚ÄúFull-size Car‚Äù\nL: ‚ÄúLuxury Car‚Äù\nM: ‚ÄúMid-Size Car‚Äù\nP: ‚ÄúPickup Truck‚Äù\nS: ‚ÄúSports Car‚Äù\nU: ‚ÄúSUV‚Äù\nV: ‚ÄúVan‚Äù\n\nPotential attributes to order by: vehicle size, capacity, price category, average speed, fuel economy, costs of ownership, motor features, etc.\n\nExample: Occupation\n\nCategories\n\n1: ‚ÄúProfessional/Technical‚Äù\n2: ‚ÄúAdministration/Managerial‚Äù\n3: ‚ÄúSales/Service‚Äù\n4: ‚ÄúClerical/White Collar‚Äù\n5: ‚ÄúCraftsman/Blue Collar‚Äù\n6: ‚ÄúStudent‚Äù\n7: ‚ÄúHomemaker‚Äù\n8: ‚ÄúRetired‚Äù\n9: ‚ÄúFarmer‚Äù\nA: ‚ÄúMilitary‚Äù\nB: ‚ÄúReligious‚Äù\nC: ‚ÄúSelf Employed‚Äù\nD: ‚ÄúOther‚Äù\n\nPotential attributes to order by: average annual salary, by their prevalence in the geographic area of interest, or variables in a Census dataset or some other data source\n\n\n\n\n\nWeight of Evidence\n\nembed::step_woe",
    "crumbs": [
      "Feature Engineering",
      "General"
    ]
  },
  {
    "objectID": "qmd/feature-engineering-general.html#sec-feat-eng-gen-inter",
    "href": "qmd/feature-engineering-general.html#sec-feat-eng-gen-inter",
    "title": "General",
    "section": "Interactions",
    "text": "Interactions\n\nManually\n\nNumeric ‚®Ø Cat\n\nDummy the cat, then multiply the numeric times each of the dummies.",
    "crumbs": [
      "Feature Engineering",
      "General"
    ]
  },
  {
    "objectID": "qmd/feature-engineering-general.html#sec-feat-eng-gen-date",
    "href": "qmd/feature-engineering-general.html#sec-feat-eng-gen-date",
    "title": "General",
    "section": "Date",
    "text": "Date\n\nDuration\n\nDays since last purchase per customer\n\nExample: (max(invoice_date) - max_date_overall) / lubridate::ddays(1)\n\nThink ddays converts this value to a numeric\n\n\nCustomer Tenure\n\nExample: (min(invoice_date) - max_date_overall) / lubridate::ddays(1)",
    "crumbs": [
      "Feature Engineering",
      "General"
    ]
  },
  {
    "objectID": "qmd/feature-engineering-general.html#sec-feat-eng-gen-dom",
    "href": "qmd/feature-engineering-general.html#sec-feat-eng-gen-dom",
    "title": "General",
    "section": "Domain Specific",
    "text": "Domain Specific\n\nRates/Ratios\n\nPurchase per Customer\n\nTotal Spent\n\nExample: sum(total_per_invoice, na.rm = TRUE)\n\nAverage Spent\n\nExample: mean(total_per_invoice, na.rm = TRUE)\n\n\nLet the effect of Cost vary by the person‚Äôs income\n\nmutate(cost_income = cost_of_product/persons_income)\nIntuition being that the more money you have the less effect cost will have on whether purchase something.\nDividing the feature by income is equivalent to dividing the \\(\\beta\\) by income.\n\n\nPre-Treatment Baseline\n\nExample: From Modeling Treatment Effects and Nonlinearities in A/B Tests with GAMS\n\noutcome = log(profit), treatment = exposure to internation markets, group = store\nBaseline variable is log(profit) before experiment is conducted\n\nShould center this variable",
    "crumbs": [
      "Feature Engineering",
      "General"
    ]
  },
  {
    "objectID": "qmd/experiments-rct.html",
    "href": "qmd/experiments-rct.html",
    "title": "RCT",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Experiments",
      "RCT"
    ]
  },
  {
    "objectID": "qmd/experiments-rct.html#sec-exp-rct-misc",
    "href": "qmd/experiments-rct.html#sec-exp-rct-misc",
    "title": "RCT",
    "section": "",
    "text": "Reasons for not running a RCT\n\nIt‚Äôs just not technically feasible to have individual-level randomization of users as we would in a classical A/B test\n\ne.g.¬†randomizing which individuals see a billboard ad is not possible\n\nWe can randomize but expect interference between users assigned to different experiences, either through word-of-mouth, mass media, or even our own ranking systems; in short, the stable unit treatment value assumption (SUTVA) would be violated, biasing the results\n\nA gold-standard design is a 6-period 2-treatment randomized crossover study; the patient actually receives both treatments and her responses can be compared (Harrell)\nATE for RCT:\n\nNon-theoretical ATE (i.e.¬†calculated from actual data) is sample-averaged; population sampling weights are unavailable for RCT subject groups. So this ATE applies to a replication of the study with similar sampling patterns. ATE does not apply to the population and in fact may apply to no one due to lack of conditioning on patient characteristics. The ATE used in 99% of papers has nothing to do with population but uses only convenience sample weighting.¬† Some papers even blatantly call it population-averaged.‚Äù\n‚ÄúThey test causal hypotheses about a group of patients with symptoms & other ‚Äòdiagnostic‚Äô findings that form entry criteria for the RCT & may only be available in sufficient numbers in specialist centres.‚Äù\nGelman\n\n‚Äúthe drug works on some people and not others‚Äîor in some comorbidity scenarios and not others‚Äîwe realize that‚Äùthe treatment effect‚Äù in any given study will depend entirely on the patient mix. There is no underlying number representing the effect of the drug. Ideally one would like to know what sorts of patients the treatment would help, but in a clinical trial it is enough to show that there is some clear average effect. My point is that if we consider the treatment effect in the context of variation between patients, this can be the first step in a more grounded understanding of effect size.\n\nGelman regarding a 0.1 ATE for a treatment in an education study\n\n‚ÄúActually, though, an effect of 0.1 GPA is a lot. One way to think about this is that it‚Äôs equivalent to a treatment that raises GPA by 1 point for 10% of people and has no effect on the other 90%. That‚Äôs a bit of an oversimplification, but the point is that this sort of intervention might well have little or no effect on most people. In education and other fields, we try lots of things to try to help students, with the understanding that any particular thing we try will not make a difference most of the time. If mindset intervention can make a difference for 10% of students, that‚Äôs a big deal. It would be naive to think that it would make a difference for everybody: after all, many students have a growth mindset already and won‚Äôt need to be told about it.\n‚ÄúMaybe in some fields of medicine this is cleaner because you can really isolate the group of patients who will be helped by a particular treatment. But in social science this seems much harder.‚Äù\n\nMe: So, a 0.1 effect wouldn‚Äôt be large if there was no variation (i.e.¬†same size effect for everyone), but that‚Äôs very unlikely to be the case.\n\n\n\nCalculation of standard-errors is different depending on the RCT type in order that variation within arms could be validly used to estimate variation between.\nRandom Sampling vs Random Treatment Allocation (source)\n\nRandom Sampling: licenses the use of measures of uncertainty¬† for (sub)groups of sampled patients.\nRandom Treatment Allocation: licenses the use of measures of uncertainty for the differences between the allocated groups.\n\nRe RCTs:\n\nlicenses the use of measures of uncertainty for hazard ratios, odds ratios, risk ratios, median/mean survival difference, absolute risk reduction etc that measure differences between groups.\nBecause there is no random sampling, measures of uncertainty are not licensed by the randomization procedure for cohort-specific estimates such as the median survival observed in each treatment cohort.\n\nFor those, we can use descriptive measures such as standard deviation (SD), interquartile range etc. Measures of uncertainty will require further assumptions to be considered valid. Further discussion here‚Äù\n\n\n\n\nBalance - Balanced allocations are more efficient in that they lead to lower variances\n\nVariance of the Mean difference (e.g between treatment and control groups) for unbalanced design\n\\[\n\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}\n\\]\n\nFollow-Up\n\nIn-Trial Follow-Up (ITFU)\n\nWithout ITFU, the unbiased ascertainment of outcomes may be compromised and statistical power considerably reduced\nStrategies\n\nFace-to-face follow-up is widely used during the initial ‚Äúin-trial‚Äù period, but is costly if employed longer term.\nTelephone-based approaches are more practical, with the ability to contact many participants coordinated by a central trial office\nPostal follow-up has been shown to be effective.\nWeb-based techniques may become more widespread as technological advances develop.\n\n\nPost-Trial Follow-Up (PTFU)\n\nRCTs are costly and usually involve a relatively brief treatment period with limited follow-up. A treatment response restricted to this brief ‚Äúin-trial‚Äù period can potentially underestimate the long-term benefits of treatment and also may fail to detect delayed hazards.\nStrategies\n\nSee ITFU strategies\nUse of routine health records can provide detailed information relatively inexpensively, but the availability of such data and rules governing access to it varies across countries.",
    "crumbs": [
      "Experiments",
      "RCT"
    ]
  },
  {
    "objectID": "qmd/experiments-rct.html#sec-exp-rct-feats",
    "href": "qmd/experiments-rct.html#sec-exp-rct-feats",
    "title": "RCT",
    "section": "RCT Features",
    "text": "RCT Features\n\nThree valuable design features of clinical trials are concurrent control, randomization and blinding.\n\nBlinding is weak at best without Randomization\nRandomization is impossible without Concurrent Control,\nConcurrent Control is necessary for the other two, so it can be regarded as the most important of the three.\n\nBlinding or Masking - patients are unaware of the treatment they are receiving and treating physicians are unaware of the treatment they are administering\n\nPrevents differential care during follow-up, accounts for nonspecific effects associated with receiving an intervention (placebo effects), may facilitate blinding of outcome assessors, and may improve adherence.\n\nConcurrent Control - the effect of a treatment should be assessed by comparing the results of subjects treated with the intervention being studied with the results of subjects treated concurrently (i.e.¬†at the ‚Äòsame‚Äô places at the ‚Äòsame‚Äô times) with a control treatment, for example, placebo.\n\nIn reality\n\nRe ‚Äòsame time‚Äô: the idea behind concurrent control is that the times at which they are recruited will vary randomly within treatment arms in the same way as between, so that variation in outcomes arising as a result of the former can be used to judge variation in outcomes as a result of the latter.\n\nTiming matters: The time at which a patient is recruited into the trial matters, but should not be biasing if patients are randomized throughout the trial to intervention and control. It will tend to increase the variance of the treatment effect, and rightly so, but it is a component that may be possible to eliminate (partially) by modelling a trend effect.\n\nExample: 1990s AIDS studies found survival of patients who were recruited later into trials tended to be better than those recruited earlier\n\n\nRe ‚Äòsame place‚Äô: The vast majority of randomised clinical trials are run in many centers. The variations in design around this many-centers aspect is the primary difference between various types of RCTs (see below). All the types will be regarded as employing concurrent control, but have their standard errors calculated differently.\n\nConsequence of violations\n\nIf variation from center to center is ignored and patients have not been randomized concurrently, then Fisher‚Äôs exact test, Pearson‚Äôs chi-square and Student‚Äôs t will underestimate the variation\n\n\nRandomized assignment means that eligible units are randomly assigned to a treatment or comparison group. Each eligible unit has an equal chance of being selected. This tends to generate internally valid impact estimates under the weakest assumptions.\n\nRandomization also allows us to achieve statistical independence, which eliminates omitted variable bias. Statistical independence implies that the treatment variable is not correlated with the other variables. The key assumption is that randomization effectively produces two groups that are statistically identical with respect to observed and unobserved characteristics. In other words, the treatment group is the same as the control group on average.\n\ni.e.¬†randomization process renders the experimental groups largely comparable. Thus, we can attribute any differences in the final metrics between the experimental groups to the intervention.\n\nIn the absence of randomization, we might fall victim to this omitted variable bias because our treatment variable will probably be endogenous. That is, it will be probably correlated with other variables excluded from the model (omitted variable bias).",
    "crumbs": [
      "Experiments",
      "RCT"
    ]
  },
  {
    "objectID": "qmd/experiments-rct.html#sec-exp-rct-assopts",
    "href": "qmd/experiments-rct.html#sec-exp-rct-assopts",
    "title": "RCT",
    "section": "Assignment Options",
    "text": "Assignment Options\n\nRandomization\n\nIndividual Randomization - The individual or patient that is allocated to an intervention (may be more than one intervention group) or control group, and simple statistical analyses on participant outcomes are used to evaluate if the intervention was effective.\n\nThese analyses assume that all participants are completely independent (ie. unlike each other, do not influence each other, and any outcomes measured on them are influenced by the intervention or usual care in the same way)\nStratified Randomization - Study participants are first divided into strata or subgroups based on one or more relevant characteristics or variables that are thought to influence response to treatment (i.e.¬†confounders). Then, within each stratum, participants are randomly assigned to the treatment or control groups.\n\nThe purpose is to increase statistical precision and ensure balance across known confounding factors.\nStrafiication variables could be age, gender, severity of disease, or any other potentially confounding factors.\nThe purpose of stratification is to ensure that the treatment and control groups are balanced concerning these potentially confounding variables, which can improve the precision and statistical power of the study.\nTo obtain valid inference following stratified randomisation, treatment effects should be estimated with adjustment for stratification variables.\n\nIf stratification variables are originally continuous but have been discretized for purposes of stratification (e.g.¬†Age), then the continuous form of these variables should be used in the model. (Paper)\n\n\nBlock Randomization\n\nThe purpose is to control for known sources of variability and remove their effects from the experimental error\nTwitter thread presenting an example of the procedure for a block randomized RCT\nSimilar to Stratified Randomization but blocks are formed based on practical considerations, such as time, location, or specific characteristics of participants or experimental units. (Models seem to be the same though)\n\nExample: These hospitals volunteered to participate in my study and I wish to randomize individuals within groups. I have no reason to believe that a particular hospital envirnoment will affect the treatment‚Äôs efficacy on the outcome variable (i.e hospital is not a confounder), but I wish to account for it in order to reduce error.\n\nDesigns with unequal sized blocks are known as Incomplete Block Designs\n\nThe assumption of homogeneous variances across blocks is likely to be violated when block sizes differ greatly. This violates one of the standard ANOVA assumptions.\nTypically modelled by Weighted Least Squares or Mixed Effects. Random effects for blocks to account for the variability between blocks, and fixed effects for treatment effects.\n\nBlock Designs\n\nLatin Square\n\nIt‚Äôs commonly used in situations where there are two sources of variability that need to be controlled, such as when testing multiple treatments in different orders or sequences while accounting for variations in time or space.\nAppropriate when complete randomization of treatments is feasible and desirable. This design requires no natural structure or hierarchy among experimental units (see Split-Plot for comparison), and treatments can be applied independently to each unit.\nEach treatment appears once in each row and each column of the Latin square, ensuring that each treatment occurs in every possible position relative to the other treatments.\nUseful for reducing noise and increasing efficiency.\nVariations of the Latin square design, such as the Graeco-Latin square design, can be used to extend the concept to accommodate groups or blocks of experimental units. In these designs, the basic principles of the Latin square are applied within each block or group, allowing for more flexibility in experimental design while still controlling for sources of variation\nExample: Does Music Genre Affect Work Productivity\n\n\n\n\nLocation 1\nLocation 2\nLocation 3\nLocation 4\n\n\n\n\nTime 1\nGenre A\nGenre B\nGenre C\nGenre D\n\n\nTime 2\nGenre D\nGenre A\nGenre B\nGenre C\n\n\nTime 3\nGenre C\nGenre D\nGenre A\nGenre B\n\n\nTime 4\nGenre B\nGenre C\nGenre D\nGenre A\n\n\n\n\nDetermine if there are significant differences in task performance across the different music genres while controlling for potential confounding factors (time of day and office location).\nEach cell is a different study participant which receives a particular treatment at a particular location and particular time.\n\n\nSplit-Plot\n\nParticularly useful when there is a hierarchical structure to the experimental units\nHandles hard-to-randomize factors such as logistical factors\nThe whole plots are for hard-to-randomize factors and subplots are for easy-to-randomize factors. It controls variability due to the whole plot factor.\nExample: Effects of Learning Environment and Study Technique on Memory Recall\n\nMain Plots (Groups of Participatnts)\n\n\n\nPlot 1\nPlot 2\n\n\nPlot 3\nPlot 4\n\n\n\n\nEach plot consists of 20 participants\nPlot 1: Participants from Psychology Department\nPlot 2: Participants from Biology Department\nPlot 3: Participants from English Department\nPlot 4: Participants from Mathematics Department\nNote how it would be impossible to randomly assign students to each department.\n\nSubplots within each main plot\n\n\n\n\nRepeated Reading\nMind Mapping\n\n\nQuiet Room\nsubgroup 1\nsubgroup 2\n\n\nNoisy Caf√©\nsubgroup 3\nsubgroup 4\n\n\n\n\nEach subgroup has 5 participants and are randomly assigned to a combination of two treatments: Environment and Study Technique.\n\n\n\nFamily Block - Naturally occurring groups or families (e.g.¬†litters, plots with similar soil conditions, etc.).\n\n\n\nCluster Randomization - One in which intact social units, or clusters of individuals rather than individuals themselves, are randomized to different intervention groups\n\nAll participants recruited from the practice, school or workplace are allocated to either the intervention or the control group\nThe outcomes of the intervention are still measured at the individual level, but the level at which the comparison is made is the practice, school or workplace\nAdvantages\n\nMembers from intervention and control groups are less likely to have direct contact with each other and are less likely to pass on components of the intervention to the control group. (i.e.¬†contamination)\nThere may also be increased compliance due to group participation.\nClusters typically consistent in their management.\n\n\n\n\n\nTreatment Strategy\n\nParallel Group - Subjects are randomized to one or more study arms (aka treatment groups) and each study arm will be allocated a different intervention. After randomization each participant will stay in their assigned treatment arm for the duration of the study\n\nThink this is just typical randomization into treatment/control groups but can be extended to include multiple treatment arms.\n‚ÄúChange from Baseline‚Äù (aka Change Scores) should never be the outcome variable\nCentral Question: For two patients with the same premeasurement value of x, one given treatment A and the other treatment B, will the patients tend to have different post-treatment values of y?\n\nCrossover Group - Subjects are randomly allocated to study arms where each arm consists of a sequence of two or more treatments given consecutively.\n\ni.e.¬†each subject receives more than one treatment and each treatment occurs sequentially over the duration of the study.\nExample: AB/BA study - Subjects allocated to the AB study arm receive treatment A first, followed by treatment B, and vice versa in the BA arm.\nAllows the response of a subject to treatment A to be contrasted with the same subject‚Äôs response to treatment B.\n\nRemoving patient variation in this way makes crossover trials potentially more efficient than similar sized, parallel group trials in which each subject is exposed to only one treatment.\n\nIn theory treatment effects can be estimated with greater precision given the same number of subjects.\nMisc\n\nBest practice is to avoid this design if there is a reasonable chance of Carry Over\nAlso see\n\nSenn SJ. Crossover trials in clinical research. Chichester: John Wiley; 1993, 2002.\n\n‚ÄúReadable approach to the problems of designing and analysing crossover trials‚Äù\nSee R &gt;&gt; Documents &gt;&gt; Experimental Design\nKurz notes on Chapters 3 and 4 with updated R code\n\n\n\nIssue: Carry Over\n\nEffects of one treatment may ‚Äúcarry over‚Äù and alter the response to subsequent treatments.\n(Pre-experiment) Solution: introduce a washout (no treatment) period between consecutive treatments which is long enough to allow the effects of a treatment to wear off.\n\nA variation is to restrict outcome measurement to the latter part of each treatment period. Investigators then need to understand the likely duration of action of a given treatment and its potential for interaction with other treatments.\n\nTesting for Carry Over\n\nIf carry over is present the outcome on a given treatment will vary according to its position in the sequence of treatments.\nExample: Concluding that there was no carry over when an analysis of variance found no statistically significant interaction between treatment sequence and outcome.1\nHowever such tests have limited power and cannot rule out a type II error (wrongly concluding there is no carry over effect).\n\n(Post-experiment) Solution: If Carry Over is detected:\n\nOption 1: Treat the study as though it were a parallel group trial and confine analysis to the first period alone.\n\nThe advantages of the crossover are lost, with the wasted expense of discarding the data from the second period.\nMore importantly, the significance test comparing the first periods may be invalid\n\nOption 2 (applicable only to studies with at least three treatment periods, e.g.¬†ABB/BAA)\n\nModel the carry over effect and use it to adjust the treatment estimate.\nSuch approaches, while statistically elegant, are based on assumptions which can rarely be justified in practice.\nSee Senn paper above\n\n\n\n\nRandomized designs are classified as completely randomized design, complete block design, randomized block design, Latin square design, split pot design, crossover design, family block design, stepped-wedge cluster design, etc.\n\nCompletely Randomized Parallel Group trial - Any given center will have some patients randomly allocated to intervention and some to control. Randomization includes centers (i.e.¬†a patient is randomly selected either treatment/control and which center they will receive the treatment)\n\nParallel Group Blocked by Center - Randomization happens within each center (i.e.¬†each center handles their own randomization). Treatment/Control ratio is the same for each center.\n\n‚ÄúCenter‚Äù should be included as a variable in the model.\n\n\nCluster-Randomized trial - Randomly allocate some centers to dispense the intervention and some the control\n\nFundamental unit of inference becomes the center and patients are regarded as repeated measures on it\n\n\nExamples\n\nThe effects of a leading mindfulness meditation app (Headspace) on mental health, productivity, and decision-making (Paper)\n\nRCT with 2,384 US adults recruited via social media ads.\nFour-week experiment\n\nfirst group is given free access to the app (worth $13)\nsecond group receives, in addition, a $10 incentive to use the app at least four or ten separate days during the first two weeks\nthird group serves as a (waitlist) control group",
    "crumbs": [
      "Experiments",
      "RCT"
    ]
  },
  {
    "objectID": "qmd/experiments-rct.html#sec-exp-rct-sbias",
    "href": "qmd/experiments-rct.html#sec-exp-rct-sbias",
    "title": "RCT",
    "section": "Sources of Bias",
    "text": "Sources of Bias\n\nMisc\n\nNotes from\n\nBiases in randomized trials: a conversation between trialists and epidemiologists\nAlso see\n\nExperiments, A/B Testing &gt;&gt; Potential Biases\nExperiments, Planning &gt;&gt; Misc\n\n\n\nSelection Bias - Occurs when there are systematic differences between baseline characteristics of groups.\n\nIf the assignment that was not properly randomized or the randomized assignment was not sufficiently concealed (i.e.¬†allocation concealment), and so the person enrolling participants was aware of allocation sequence and influenced which patients were assigned to each group based on their prognostic factors\nExample: if groups are not comparable on key demographic factors, then between-group differences in treatment outcomes cannot necessarily be attributed solely to the study intervention.\nExample: The assignment of patients to a group is influenced by knowledge of which treatment they will receive\nSolutions:\n\nRandomized Assignment - RCTs attempt to address selection bias by randomly assigning participants to groups ‚Äì but it is still important to assess whether randomization was done well enough to eliminate the influence of confounding variables.\nBlinding - participants and investigators should remain unaware of which group participants are assigned to.\n\n\nPerformance Bias - Refers to systematic differences between groups that occur during the study. Leads to overestimated treatment effects, because of the physical component of interventions\n\nExample: if participants know that they are in the active treatment rather than the control condition, this could create positive expectations that have an impact on treatment outcome beyond that of the intervention itself.\nSolution: Blinding - participants and investigators should remain unaware of which group participants are assigned to.\n\nMore easily achieved in medication trials than in surgical trials\n\n\nDetection Bias - Refers to systematic differences in the way outcomes are determined.\n\nExample: if providers in a psychotherapy trial are aware of the investigators‚Äô hypotheses, this knowledge could unconsciously influence the way they rate participants‚Äô progress.\nSolution: Attention to conflicts of interest and Blinding (also see Performance Bias) - RCTs address this by utilizing independent outcome assessors who are blind to participants‚Äô assigned treatment groups and investigators‚Äô expectations.\n\nAttrition Bias - occurs when there are systematic differences between groups in withdrawals from a study.\n\nIt‚Äôs common for participants to drop out of a trial before or in the middle of treatment, and researchers who only include those who completed the protocol in their final analyses are not presenting the full picture.\nSolution: Intention to Treat analysis - Analyses should include all participants who were randomized into the study, and not only participants who completed some or all of the intervention.\n\nReporting Bias - Refers to systematic differences between reported and unreported data.\n\nExample: publication bias - occurs because studies with positive results are more likely to be published, and tend to be published more quickly, than studies with findings supporting the null hypothesis.\nExample: outcome reporting bias - occurs when researchers only write about study outcomes that were in line with their hypotheses.\nSolution: Requirements that RCT protocols be published in journals or on trial registry websites, which allows for confirmation that all primary outcomes are reported in study publications.\n\nOther Bias - A catch-all category that includes specific situations not covered by the above domains.\n\nIncludes bias that can occur when study interventions are not delivered with fidelity, or when there is ‚Äúcontamination‚Äù between experimental and control interventions within a study (for example, participants in different treatment conditions discussing the interventions they are receiving with each other).",
    "crumbs": [
      "Experiments",
      "RCT"
    ]
  },
  {
    "objectID": "qmd/classification.html",
    "href": "qmd/classification.html",
    "title": "Classification",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Classification"
    ]
  },
  {
    "objectID": "qmd/classification.html#sec-class-misc",
    "href": "qmd/classification.html#sec-class-misc",
    "title": "Classification",
    "section": "",
    "text": "Also see Regression, Logistic\nGuide for suitable baseline models: link\nIf you have mislabelled target classes, try AdaSampling to correct the labels (article)\nSample size requirements\n\nLogistic Regression: (Harrell, link)\n\nThese are conservative estimates. Sample size estimates assume an event probability of 0.50.\nFor just estimating the intercept and a margin of error for predicted probabilities of 0.1\n\nWith no covariates (i.e.¬†population is homogeneous), n = 96\nWith 1 categorical covariate, n = 96 for each level of the covariate\n\ne.g.¬†For gender, you need 96 males and 96 females\n\n\nFor just estimating the intercept and a margin of error for predicted probabilities of 0.05\n\nWith no covariates (i.e.¬†population is homogeneous), n = 384\nIf true probabilities of event (and non-event) are known to be extreme, i.e.¬†\\(p \\notin [0.2, 0.8]\\), n = 246\n\nFor estimating predicted probabilities with 1 continuous predictor\n\nFor a margin of error of 0.1, n = 150\nFor a margin of error of 0.07, n = 300\n\n\nRF: 200 events per candidate feature (Harrell, link)\n\nUndersampling non-events(0s) is the popular way to balance the target variable in data sets but other ma be worth exploring while building the model.\nSpline ‚Äî don‚Äôt bin continuous, baseline, adjustment variables, where ‚Äúbaseline‚Äù means the patients measurements before treatment. Lack of fit will then come only from omitted interaction effects. (Harrell)\n\ne.g.: if older males are much more likely to receive treatment B than treatment A than what would be expected from the effects of age and sex alone, adjustment for the additive propensity would not adequately balance for age and sex.\nAlso see\n\nFeature Engineering, General &gt;&gt; Continuous &gt;&gt; Binning\nFeature Engineering, General &gt;&gt; Continuous &gt;&gt; Splines\n\n\nThe best information to present to the patient is the estimated individualized risk of the bad outcome separately under all treatment alternatives. That is because patients tend to think in terms of absolute risk, and differences in risks don‚Äôt tell the whole story (Harrell)\n\nA risk difference (RD, also called absolute risk reduction) often means different things to patients depending on whether the base risk is very small, in the middle, or very large.\n\nRecommended metrics to be reported for medical studies (Harrell). This is perhaps generalizable to any RCT with a binary outcome.\n\nThe distribution of Risk Difference (RD)\nCovariate-Adjusted OR\nAdjusted marginal RD (mean personalized predicted risk as if all patients were on treatment A minus mean predicted risk as if all patients were on treatment B) (emmeans?)\nMedian RD",
    "crumbs": [
      "Classification"
    ]
  },
  {
    "objectID": "qmd/classification.html#sec-class-diag",
    "href": "qmd/classification.html#sec-class-diag",
    "title": "Classification",
    "section": "Diagnostics",
    "text": "Diagnostics\n\nAlso see\n\nDiagnostics, Classification\nRegression, Logistic &gt;&gt; Diagnostics\n\n‚ÄúDuring the initial phase of model building, a good strategy for data sets with two classes is to focus on the AUC statistics from these curves instead of metrics based on hard class predictions. Once a reasonable model is found, the ROC or precision-recall curves can be carefully examined to find a reasonable cutoff for the data and then qualitative prediction metrics can be used.‚Äù ‚Äì 3.2.2¬†Classification Metrics‚Äù Kuhn and Kjell\n‚ÄúStable‚Äù AUC requirements for 0/1 outcome:\n\nPaper: Modern modelling techniques are data hungry: a simulation study for predicting dichotomous endpoints | BMC Medical Research Methodology | Full Text\nLogistic Regression: 20 to 50 events per predictor variable\nRandom Forest and SVM: greater than 200 to 500 events per predictor variable",
    "crumbs": [
      "Classification"
    ]
  },
  {
    "objectID": "qmd/classification.html#sec-class-imbal",
    "href": "qmd/classification.html#sec-class-imbal",
    "title": "Classification",
    "section": "Class Imbalance",
    "text": "Class Imbalance\n\nMisc\n\nAlso see\n\nModel Building, tidymodel &gt;&gt; Recipe &gt;&gt; up/down-sampling\nModel Building, sklearn &gt;&gt; Preprocessing &gt;&gt; Class Imbalance\nSurveys, Analysis &gt;&gt; Modeling &gt;&gt; Tidymodels &gt;&gt; Importance weights\nPaper: Subsampling without calibration will likely be more harmful than without subsampling\n\nPackages\n\n{themis} - Extra steps for the {recipes} for dealing with unbalanced data.\n{ebmc} - Four ensemble-based methods (SMOTEBoost, RUSBoost, UnderBagging, and SMOTEBagging) for class imbalance problem are implemented for binary classification.\n{imbalance} - Novel oversampling algorithms, filtering of instances and evaluation of synthetic instances. Methods: MWMOTE (Majority Weighted Minority Oversampling Technique), RACOG (Rapidly Converging Gibbs), wRACOG (wrapper-based RACOG), RWO (Random Walk Oversampling), and PDFOS (Probability Distribution density Function estimation based Oversampling).\n{{imbalanced-learn}} - Tools when dealing with {{sklearn}} classification with imbalanced classes\n\nSubsampling can refer to up/oversampling or down/undersampling.\nIt is perfectly ok to train a model on 1M negatives and 10K positives (i.e.¬†plenty of events), as long as you avoid using accuracy as a metric\n\n1K to 10K events might be enough for the ML algorithm to learn from\nFor a RF model: 200 events per candidate feature (Harrell, link)\n\nUnless recalibration is applied, applying subsampling to correct class imbalance will lead to overpredicting the minority class (discrimination) and worse calibration when using logistic regression or ridge regression (paper)\n\nPaper used random undersampling (RUS), random oversampling (ROS), and SMOTE (Synthetic Minority Oversampling Technique)\nEvent Fractions: 1%, 10%, 30%\nN: 2500, 5000; p: 3, 6, 12, 24\n‚ÄúWe anticipate that risk miscalibration will remain present regardless of type of model or imbalance correction technique, unless the models are recalibrated. However, class imbalance correction followed by recalibration is only worth the effort if imbalance correction leads to better discrimination of the resulting models.‚Äù\n\nThey used what looked to be Platt Scaling for recalibration\n\nAlso see Model Building, tidymodels &gt;&gt;¬†Tune &gt;&gt; Tune Model with Multiple Recipes for an example of how downsampling (w/o calibration) + glmnet affects class prediction and GOF metrics\n\n\nIssues\n\nUsing Accuracy as a metric\n\nIf the positivity rate is just 1%, then a naive classifier labeling everything as negative has 99% accuracy by definition\n\nIf you‚Äôve used subsampling, then the training data is not the same as the data used in production\nLow event rate\n\nIf you only have 10 to 100 positive samples, the model may easily memorize these samples, leading to an overfit model that generalized poorly\nMay result in large CIs for your effect estimate (see Gelman post)\n\n\nCheck Imbalance\ndata %&gt;%\n¬† count(class) %&gt;%\n¬† mutate(prop = n / sum(n)) %&gt;%\n¬† pretty_print()\nDownsampling\n\nUse cases for downsampling the majority class\n\nwhen the training data doesn‚Äôt fit into memory (and your ML training pipeline requires it to be in memory), or\nwhen model training takes unreasonably long (days to weeks), causing too long iteration cycles, and preventing you from iterating quickly.\n\nUsing a domain knowledge filter for downsampling\n\na simple heuristic rule that cuts down most of the majority class, while keeping nearly all of the minority class.\n\ne.g.¬†if a rule can retain 99% of positives but only 1% of the negatives, this would make a great domain filter\n\nExamples\n\ncredit card fraud prediction: filter for new credit cards, i.e.¬†those without a purchase history.\nspam detection: filter for Emails from addresses that haven‚Äôt been seen before.\ne-commerce product classification: filter for products that contain a certain keyword, or combination of keywords.\nads conversion prediction: filter for a certain demographic segment of the user population.\n\n\n\nCV\n\nIt is extremely important that subsampling occurs inside of resampling. Otherwise, the resampling process can produce poor estimates of model performance.\n\ndata leakage: if you first upsample the data and then split the data into training (aka analysis set) and validation (aka assessment set) folds, your model can simply memorize the positives from the training data and achieve artificially strong performance on the validation data, causing you to think that the model is much better than it actually is.\nThe subsampling process should only be applied to the analysis (aka training) set. The assessment (aka validation) set should reflect the event rates seen ‚Äúin the wild.‚Äù\n\nDoes {recipe} handle this correctly?\n\n\nProcess\n\nSubsample the data only in the analysis set\nPerform CV algorithm selection and tuning using a suitable metric for class imbalance\nUse same metric to get score on the test set\n\n\nML Methods\n\nSynthetic Data Approaches\n\nTabDDPM: Modeling Tabular Data with Diffusion Models (Raschka Thread)\n\nCategorical and Binary Features: adds uniform noise via multinomial diffusion\nNumerical Features: adds Gaussian noise using Gaussian diffusion\n\nSynthetic Data Vault (Docs)\n\nData generated with variational autoencoders adapted for tabular data (TVAE) (paper)\nCan Synthetic Data Boost Machine Learning Performance?\n\n\nImbalanced Classification via Layered Learning (ICLL)\n\nFor code, see article\nA hierarchical model composed of two levels:\n\nLevel 1: A model is built to split easy-to-predict instances from difficult-to-predict ones.\n\nThe goal is to predict if an input instance belongs to a cluster with at least one observation from the minority class.\n\nLevel 2: We discard the easy-to-predict cases. Then, we build a model to solve the original classification task with the remaining data.\n\nThe first level affects the second one by removing easy-to-predict instances from the training set.\n\n\nIn both levels, the imbalanced problem is reduced, which makes the modeling task simpler.\n\n\nTuning parameters (last resort)\n\nXGBoost and LightGBM have a parameter called scale_pos_weight, which up-weighs positive samples when computing the gradient at each boosting iteration\nUnlikely to have a major effect and probably won‚Äôt generalize well.",
    "crumbs": [
      "Classification"
    ]
  },
  {
    "objectID": "qmd/classification.html#sec-class-calib",
    "href": "qmd/classification.html#sec-class-calib",
    "title": "Classification",
    "section": "Calibration",
    "text": "Calibration\n\nCalibrated- When the predicted probabilities from a model match the observed distribution of probabilities for each class.\n\nCalibration Plots visualize this comparison of distributions\n\nCalibration mesasure are important for validating a predictive model.\n\nLogistic Regression models are usually well-calibrated, but most ML and DL model predicted probabilities aren‚Äôt directly produced by the algorithms and aren‚Äôt usually calibrated\nRF models can also benefit from calibration although given enough data, they are already pretty well calibrated\nSVM, Naive Bayes, boosted tree algorithms, and DL models benefit most from calibration\n\nAn unintended consequence of applying calibration modeling can be the worsening of calibration for models that are already well calibrated\nIf a model isn‚Äôt calibrated, then the magnitude of the predicted probability cannot be interpreted as the likelihood of an event\n\ne.g.¬†If 0.66 and 0.67 are two predicted probabilities from an uncalibrated xgboost model, the 0.67 prediction cannot be interpreted as more likely to be an event than the 0.66 prediction.\nMiscalibrated predictions don‚Äôt allow you to have more confidence in a label with a higher probability than a label with a lower probability.\nSee (below) the introduction in the paper, ‚ÄúA tutorial on calibration measurements ‚Ä¶‚Äù for examples of scenarios where calibration of risk scoring model is essential. If you can‚Äôt trust the predicted probabilities (i.e.¬†risk) then decision-making is impossible.\n\nalso this paper which also explains some sources of miscalibration (e.g.¬†dataset from region with low incidence, measurement error, etc.)\n\n\nEven if a model isn‚Äôt caibrated and depending on the metric, it can still be more accurate than a calibrated model.\n\nPoor calibration may make an algorithm less clinically useful than a competitor algorithm that has a lower AUC but is well calibrated (paper)\nCalibration doesn‚Äôt affect the AUROC (does not rely on predicted probabilities) but does affect the Brier Score (does rely on predicted probabilities) (paper)(Harrell)\nSince thresholds are based on probabilities, I don‚Äôt see how a valid, optimized threshold can be established for an uncalibrated model\nWonder how this affects model-agnostic metrics, feature importance, shap, etc.\n\n\n\nMisc\n\nAlso see Diagnostics, Classification &gt;&gt; Calibration\nPackages\n\n{probably} - tidymodels calibration package\n\nCalibrating Binary Probabilities - Nice tutorial\n\n\nNotes from:\n\nHow and When to Use a Calibrated Classification Model with scikit-learn\nCan I Trust My Model‚Äôs Probabilities? A Deep Dive into Probability Calibration\n\nPython, multiclass example\n\nA tutorial on calibration measurements and calibration models for clinical prediction models (paper)\n\nCalibration curves for nested cv (post)\nFor calibrated models, sample size affects the how well they‚Äôre calibrated\n\n\nEven for a logistic model, N &gt; 1000 is desired for good calibration\nFor a rf, closer to N = 10,000 is probably needed.\n\nDistributions of predicted probabilities\n\n\nRandom Forest pushes the probabilities towards 0.0 and 1.0, while the probabilities from the logistic regression are less skewed.\nDecision Trees are even more skewed than RF\nSays how rare a prediction is.\n\nIn a rf, really low or high probability predictions aren‚Äôt rare. So, if the model gives you a 0.93, you shouldn‚Äôt interpet it the way you normally would such a high probability (i.e.¬†high certainty), because a rf inherently pushes its probabilities towards 0 and 1.\n\n\n\n\n\nBasic Workflow\n\nMisc\n\nIdeally you‚Äôd want each model (i.e.¬†tuning parameter combination) being scored in a CV or a nested CV to have its own calibration model, but it‚Äôs not practical. But also, it‚Äôs unlikely an algorithm with a slightly different parameter value combination with have a substantially different predicted probability distribution, and it‚Äôs the algorithm itself that‚Äôs the salient factor. Therefore, for now, I‚Äôd go with 1 calibration model per algorithm.\n\nProcess\n\nSplit data into Training, Calibration, and Test\nFor each algorithm, train the algorithm on the Training set, and create the calibration model using it and the Calibration set.\n\nEach algorithm with have it‚Äôs own calibration model\nSee below, Example: AdaBoost in Py using CV calibrator\n\nI like sklearn‚Äôs ideas for training a calibration model\n\n\nUse the Training set for CV or Nested CV\nFor each split during CV or Nested CV (outer loop)\n\nTrain the algorithm on the training fold, predict on the validation fold, calibrate predictions with calibration model, score the calibrated predictions for that fold\n\nThe calibration model could be used in the tuning process in the inner loop of Nested CV as well\n\nScores should include calibration metrics (See Diagnostics, Classification &gt;&gt; Calibration &gt;&gt; Basic Workflow)\n\nThe rest is normal CV/Nested CV procedure\n\ni.e.¬†average scores across the splits then select the algorithm with the best score. Predict and score algorithm on the Test set\nSee Calibration curves for nested cv for details and code on averaging calibration curves\n\n\n\n\n\nMethods\n\n\nMisc\n\nTLDR; Both Platt Scaling and Isotonic Regression methods are the essentially the same except:\n\nPlatt Scaling uses a logistic regression model as the calibration model\nIsotonic Regression uses an isotonic regression model on ordered data as the calibration model\n\n\nPlatt Scaling\n\nMisc\n\nSuitable for smaller data and for calibration curves with the S-shape.\nMay fail when model is already well calibrated (e.g.¬†logistic regression models)\nPerforms best under the assumption that the predicted probabilities are close to the midpoint and away from the extremes\n\nSo, might be bad for tree models but okay for SVM, Naive Bayes, etc.\n\n\nProcess\n\nSplit data into Training, Calibration, and Test Sets\nTrain your model on the Training Set\nGet the predicted probabilities from your model on the Test Set\nFit a logistic regression using your model‚Äôs predicted probabilities for the Calibration Set as the predictor and the outcome variable in the Calibration Set as the outcome\nCalibrated probabilities are the predicted probabilities of the LR model using your model‚Äôs predicted probabilites on the Test set as new data.\n\nExample: SVM in R\nsvm_platt_recal = svm_platt_recal_func(model_fit, calib_dat, test_preds, test_dat)\n\nsvm_platt_recal_func = function(model_fit, calib_dat, test_preds, test_dat){\n\n¬† # Predict on Calibration Set¬†\n¬† cal_preds_obj &lt;- predict(model_fit,¬†\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† calib_dat[, -which(names(calib_dat) == 'outcome_var')],¬†\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† probability = TRUE)¬†\n¬† # e1071 has a funky predict output; just getting probabilities¬†\n¬† cal_preds &lt;- as.data.frame(attr(cal_preds_obj, 'probabilities')[, 2])\n¬† # Create calibration model\n¬† cal_obs_preds_df = data.frame(y = calib_dat$outcome_var, yhat = cal_preds[, 1])\n¬† calib_model &lt;- glm(y ~ yhat, data = cal_obs_preds_df, family = binomial)¬†\n\n¬† # Recalibrate classifiers predicted probabilities on the test set¬† ¬†\n¬† colnames(test_preds) &lt;- c(\"yhat\")\n¬† recal_preds = predict(calib.model, test_preds, type='response')¬† ¬†\n\n¬† return(recal_preds)\n\n}\n\nSee Istotonic Regression for ‚Äúmodel_fit‚Äù, ‚Äúcalib_dat‚Äù, ‚Äútest_preds‚Äù, and ‚Äútest_dat‚Äù\n\nExample: AdaBoost in py\nX_, X_test, y_, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\nX_train, X_calib, y_train, y_calib = train_test_split(X_, y_, test_size=0.4, random_state=42)\n\n# uncalibrated model\nclf = AdaBoostClassifier(n_estimators=50)\ny_proba = clf.fit(X_train, y_train).predict_proba(X_test)\n\n# calibrated model\ncalibrator = LogisticRegression()\ncalibrator.fit(clf.predict_proba(X_calib), y_calib)\ny_proba_calib = calibrator.predict_proba(y_proba)\n\nIsotonic Regression\n\nMore complex, requires a lot more data (otherwise it may overfit), but can support reliability diagrams with different shapes (is nonparametric).\n\nTried on palmer penguins (n = 332) and almost all the probabilites were pushed to the edges\nTried on mlbench::PimaIndiansDiabetes (n = 768). Probabilites were mostly clumped into 3 modes.\nPaper used a simulated (n = 5000) dataset. Probabilities moved a little more towards the center but the movement was much less dramatic that the other two datasets.\nI didn‚Äôt calculate brier scores but I‚Äôd guess you‚Äôd need over a 1000 or so observations for this method to have a significantly positive effect.\n\nLacks continuousness, because the fitted regression function is a piecewise function.\n\nSo, a slight change in the uncalibrated predictions can result in dramatic difference in the calibrated predictions (i.e.¬†a change in step)\n\nProcess\n\nSplits: train (50%), Calibration (25%), Test (25%)\nTrain classifier model on train data\nGet predictions from the classifier on the test data\nCreate calibration model dataset from the calibration data\n\nGet predictions from the classifier on the calibration data\nCreate df with observed outcome of calibration data and predictions on calibration data\nOrder df rows according the predictions column\n\nLowest to largest probabilities in the paper but I‚Äôm not sure it matters\n\n\nFIt calibration model\n\nFit isotonic regression model on calibration model dataset\nCreate a step function using the isotonic regression fitted values and the predictions on the calibration data\n\nCalibrate the classifier‚Äôs predictions on the test set with the step function\n\nExample: SVM in R\nlibrary(dplyr)\nlibrary(e1071)\ndata(PimaIndiansDiabetes, package = \"mlbench\")\n\n# isoreg doesn't handle NAs\n# e1071::svm doesn't identify the event correctly in non-0/1 factor variables\ndat_clean &lt;-¬†\n¬† PimaIndiansDiabetes %&gt;%¬†\n¬† mutate(diabetes = ifelse(as.numeric(diabetes) == 2, 1, 0)) %&gt;%¬†\n¬† rename(outcome_var = diabetes) %&gt;%¬†\n¬† tidyr::drop_na()\n\n# Data splits Training, Calibration, Test (50% - 25% - 25%)\nsmp_size &lt;- floor(0.50 * nrow(dat_clean))\nval_smp_size = floor(0.25 * nrow(dat_clean))\ntrain_idx &lt;- sample(seq_len(nrow(dat_clean)), size = smp_size)\ntrain_dat &lt;- dat_clean[train_idx, ]\ntest_val_dat &lt;- dat_clean[-train_idx, ]\nval_idx &lt;- sample(seq_len(nrow(test_val_dat)), size = val_smp_size)\ncalib_dat &lt;- test_val_dat[val_idx, ]\ntest_dat &lt;- test_val_dat[-val_idx, ]\nrm(list=setdiff(ls(), c(\"train_dat\", \"calib_dat\", \"test_dat\")))\n\n# Fit classifier; predict on Test Set\n# e1071::svm needs a factor outcome, probability=T to output probabilities\nmodel_fit &lt;- svm(factor(outcome_var) ~ .,\n¬† ¬† ¬† ¬† ¬† ¬† data = train_dat,¬†\n¬† ¬† ¬† ¬† ¬† ¬† kernel = \"linear\", cost = 10, scale = FALSE, probability = TRUE)\ntest_preds_obj &lt;- predict(model_fit,\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† test_dat[, -which(names(test_dat) == 'outcome_var')],\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† probability = TRUE)\n# e1071 has a funky predict output; just getting probabilities\ntest_preds &lt;- as.data.frame(attr(test_preds_obj, 'probabilities')[, 2])\n\n# Predict on Calibration Set\ncal_preds_obj &lt;- predict(model_fit,\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† calib_dat[, -which(names(calib_dat) == 'outcome_var')],\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† probability = TRUE)\n# e1071 has a funky predict output; just getting probabilities\ncal_preds &lt;- as.data.frame(attr(cal_preds_obj, 'probabilities')[, 2])\n\n# Create Calibration Model dataset\ncal_obs_preds_mtx = cbind(y = calib_dat$outcome_var, yhat = cal_preds[, 1])\n# order training data by predicted probabilities\niso_train_mtx &lt;- cal_obs_preds_mtx[order(cal_obs_preds_mtx[, 2]), ]\n\n# Fit Calibration Model\n# (predicted probabilities, observed outcome)\ncalib_model &lt;- isoreg(iso_train_mtx[, 2], iso_train_mtx[, 1])¬†\n# yf are the fitted values of the outcome variable\nstepf_data &lt;- cbind(calib_model$x, calib_model$yf)¬†\nstep_func &lt;- stepfun(stepf_data[, 1], c(0, stepf_data[, 2]))¬†\n# recalibrate classifiers predicted probabilities on the test set\nrecal_preds &lt;- step_func(test_preds[, 1])\n\nhead(recal_preds, n = 20)\nhist(recal_preds)\nhist(test_preds[, 1])\n\nisoreg doesn‚Äôt handle NAs\nFor a binary classification model that outputs probabilities, e1071::svm needs:\n\nfactored 0/1 outcome variable\nprobability = TRUE\n\n\nExample: AdaBoost in Py using CV calibrator\nfrom sklearn import datasets\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn_evaluation import plot\nX, y = datasets.make_classification(10000, 10, n_informative=8,\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† class_sep=0.5, random_state=42)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\nclf = AdaBoostClassifier(n_estimators=100)\nclf_calib = CalibratedClassifierCV(base_estimator=clf,\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† cv=3,\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ensemble=False,\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† method='isotonic')\nclf_calib.fit(X_train, y_train)\ny_proba = clf_calib.predict_proba(X_test)\ny_proba_base = clf.fit(X_train, y_train).predict_proba(X_test)\nfig, ax = plt.subplots()\nplot.calibration_curve(y_test,\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† [y_proba_base[:, 1], y_proba[:, 1]],\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† clf_names=[\"Uncalibrated\", \"Calibrated\"],\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† n_bins=20)\nfig.set_figheight(4)\nfig.set_figwidth(6)\nfig.set_dpi(150)\n\nCalibratedClassifierCV has both platt scaling (‚Äúsigmoid‚Äù)(default) and isotonic (‚Äúisotonic‚Äù) calibration methods (Docs)\nCalibration model is built using the test fold (aka validation fold)\n\nensemble=True\n\nFor each cv split, the base estimator is fit on the training fold, and the calibration model is built using the ‚Äútest‚Äù fold (aka validation fold)\n\nThe test (aka validation) fold is the calibration data described in the isotonic and platt scaling process sections above\n\nFor prediction, predicted probabilities are averaged across these individual calibrated classifiers.\n\nensemble=False\n\nLOO CV is performed using cross_val_predict, and those predictions are used to train the calibration model.\nThe base estimator is trained using all the data (i.e.¬†training and test (aka validation)).\nFor prediction, there is only one classifier and calibration model combo.\nThe benefit is that it‚Äôs faster, and since there‚Äôs only one combo, it‚Äôs smaller in size as compared to ensemble = True which is k combos. (not as accurate or as well-calibrated though)\n\n\n\nOther forms\n\nFor logistic regression models, adjustment using the Calibration Intercept (and Calibration Slope)\n\nSeems similar to Platt Scaling, but has strong overfitting vibes so caveat emptor\nFor calculating the values, see Diagnostics, Classification &gt;&gt; Calibration &gt;&gt; Evaluation of Calibration Levels &gt;&gt; Weak &gt;&gt; Intercept, Slope\npaper, see supplemental material\nProcedure\n\nThe calibration intercept is added to the fitted model‚Äôs intercept\nThe calibration slope is multiplied times all (nonexponentiated) coefficients of the fitted model (including interactions)\nPredictions are then calculated using the new formula\n\n\nExample\n\nFrom How We Built Our COVID-19 Estimator\nTarget: Risk of Death (probabilities)\nPredictors: Age, Gender, Comorbidities\nModel: XGBoost\n‚ÄúEvery time our model makes a prediction, we compare the result to what the model would have returned for an individual of the same age and sex and only one of the listed comorbidities, or none at all. If the predicted risk is lower than the risk for a comorbidity taken on its own‚Äîsuch as, say, the estimated risk for heart disease alone being greater than the risk for heart disease and hypertension, or the risk for metabolic disorders being lower than the risk of someone with no listed comorbidities‚Äîour tool delivers the higher number instead. We also smoothed our estimates and confidence intervals, using five-year moving averages by age and gender.‚Äù",
    "crumbs": [
      "Classification"
    ]
  },
  {
    "objectID": "qmd/classification.html#sec-class-featimp",
    "href": "qmd/classification.html#sec-class-featimp",
    "title": "Classification",
    "section": "Feature Importance",
    "text": "Feature Importance\n\nMisc\n\nSee notebook and bookmarks for proper algorithms (e.g.¬†permutation importance)¬†to specify for variable importance plots\nPermutation Importance\n\nPermutation importance is generally considered as a relatively efficient technique that works well in practice\nImportance of correlated features may be overestimated\n\nIf you have highly correlated features, use partykit::varimp(conditional = TRUE)\n\nProcess\n\nTake a model that was fit to the training dataset\nEstimate the predictive performance of the model on an independent dataset (e.g., validation dataset) and record it as the baseline performance\nFor each feature i:\n\nrandomly permute feature column i in the original dataset\nrecord the predictive performance of the model on the dataset with the permuted column\ncompute the feature importance as the difference between the baseline performance (step 2) and the performance on the permuted dataset\n\n\n\nVariable Importance plots can be useful for explaining the model to clients. They should never be interpreted as causal. Ex. Real estate: variables that are most influential in determining housing price by this model are aspects of a house that could be emphasized by Realtors to their clients.\nMake sure to use ‚ÄúLossFunctionChange‚Äù importance type in Catboost.\n\nLooks at how much the loss function changes when a feature is excluded from the model.\nDefault method capable of giving importance to random noise\nRequires evaluation on a testing set\n\n\nxgboost\nxg_wf_best &lt;- xg_workflow_obj %&gt;%\nfinalize_workflow(select_best(xg_tune_obj))\nxg_fit_best &lt;- xg_wf_best %&gt;%\nfit(train)\nimportances &lt;- xgboost::xgb.importance(model = extract_fit_engine(xg_fit_best))\nimportances %&gt;%\nmutate(Feature = fct_reorder(Feature, Gain)) %&gt;%\nggplot(aes(Gain, Feature)) +\ngeom_point() +\nlabs(title = \"Importance of each term in xgboost\",\n¬† subtitle = \"Even after turning direction numeric, still not *that* important\")\nUsing vip package\nlibrary(vip)\n\n# xg_wf = xgboost workflow object\nfit(xg_wf, whole_training_set) %&gt;%¬†\npull_workflow_fit() %&gt;%¬†\nvip(num_features = 20)\nglmnet\n\nlin_trained &lt;- lin_wf %&gt;%\n¬† ¬† finalize_workflow(select_best(lin_tune)) %&gt;%\n¬† ¬† fit(train) # or split_obj, test_dat, etc.\n\nlin_trained$fit$fit %&gt;%\n¬† ¬† broom::tidy %&gt;%\n¬† ¬† top_n(50, abs(estimate)) %&gt;%\n¬† ¬† filter(term != \"(Intercept)\") %&gt;%\n¬† ¬† mutate(ter = fct_reorder(term, estimate)) %&gt;%\n¬† ¬† ggplot(aes(estimate, term, fill = estimate &gt; 0)) +\n¬† ¬† geom_col() +\n¬† ¬† theme(legend.position = \"none\")",
    "crumbs": [
      "Classification"
    ]
  },
  {
    "objectID": "qmd/db-duckdb.html",
    "href": "qmd/db-duckdb.html",
    "title": "DuckDB",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Databases",
      "DuckDB"
    ]
  },
  {
    "objectID": "qmd/db-duckdb.html#sec-db-duckdb-misc",
    "href": "qmd/db-duckdb.html#sec-db-duckdb-misc",
    "title": "DuckDB",
    "section": "",
    "text": "Also see\n\nBig Data &gt;&gt; Larger Than Memory\nJSON &gt;&gt; DuckDB\n\nHigh performance embedded database for analytics which provides a few enhancements over SQLite such as increased speed and allowing a larger number of columns\n\nFaster than sqlite for most analytics queries (sums, aggregates etc).\n\nVectorizes query executions (columnar-oriented), while other DBMSs (SQLite, PostgreSQL‚Ä¶) process each row sequentially\n\n\nUnlike some other big data tools it is entirely self-contained. (aka embedded, in-process)\n\nNo external dependencies, or server software to install, update, or maintain\n\nCan directly run queries on Parquet files, CSV files, SQLite files, postgres files, Pandas, R and Julia data frames as well as Apache Arrow sources\nResources\n\nAwesome DuckDB - Curated list of libraries, tools, and resources.\n\nExtensions\n\nDocs, List of Official Extensions\n\nTools\n\nSQL Workbench - Query parquet files locally or remotely. Can also produce charts of results. Uses DuckDB-WASM so browser based.\n\nTutorial - Along with explaining the features of the tool, it has complete normalization example and analysis.\nFor visualizations, click the configure button on the right side of the Results sections (bottom main), click Data Grid, choose a chart type, drag column names from the bottom to various areas (similar to Tableau). Click the Reset button in the toolbar close to the configure button to return to Table mode.\nFor tables, if you right-click their name in the Schema pane (far-left), you get a list of options including Summarize which gives summary stats along with uniques and null % for missing data.\nIf tables have foreign keys, data models can be visualized in a mermaid diagram by clicking Data Modes in the bottom-left of the schema panel",
    "crumbs": [
      "Databases",
      "DuckDB"
    ]
  },
  {
    "objectID": "qmd/db-duckdb.html#sec-db-duckdb-setup",
    "href": "qmd/db-duckdb.html#sec-db-duckdb-setup",
    "title": "DuckDB",
    "section": "Set-up",
    "text": "Set-up\n\nInstallation: install.packages(\"duckdb\")\nCreate db and populate table from csv\n\nExample \nExample\n# includes filename/id\nwithr::with_dir(\"data-raw/files/\", {\n  dbSendQuery(\n    con, \"\n    CREATE TABLE files AS\n    SELECT *, regexp_extract(filename, '\\\\d{7}') AS file_number\n    FROM read_csv_auto('*Control*File-*.txt', FILENAME = TRUE);\"\n  )\n})",
    "crumbs": [
      "Databases",
      "DuckDB"
    ]
  },
  {
    "objectID": "qmd/db-duckdb.html#sec-db-duckdb-dbplyr",
    "href": "qmd/db-duckdb.html#sec-db-duckdb-dbplyr",
    "title": "DuckDB",
    "section": "d/dbplyr",
    "text": "d/dbplyr\n\nExample: Connect, Read in Parallel, and Summarize\ncon &lt;- \n  dbConnect(duckdb(), \n            \":memory:\")\ndf &lt;- \n  dplyr::tbl(con, \n             paste0(\"read_csv('\",\n                    file_name,\n                    \"',\n                    parallel = true,\n                    delim = ',',\n                    header = true,\n                    columns = {\n                        'measurement': 'DOUBLE',\n                        'state': 'VARCHAR'\n                    })\"), \n             check_from = FALSE)\ndf &lt;- df |&gt;\n  summarize(\n    .by = state,\n    mean = mean(measurement),\n    min = min(measurement),\n    max = max(measurement)\n  ) |&gt;\n  collect()\ndf &lt;- NULL\ndbDisconnect(con, shutdown = TRUE)\ngc()\n\nCompetative with running the operation in SQL\n\nExample Connect to db; Write a df to table; Query it\nlibrary(dbplyr)\n\nduck = DBI::dbConnect(duckdb::duckdb(), dbdir=\"duck.db\", read_only=FALSE)\nDBI::dbWriteTable(duck, name = \"sales\", value = sales)\nsales_duck &lt;- tbl(duck, \"sales\")\n\nsales_duck %&gt;%\n  group_by(year, SKU) %&gt;%\n  mutate(pos_sales = case_when(\n          sales_units &gt; 0 ~ sales_units,\n          TRUE ~ 0)) %&gt;%\n  summarize(total_revenue = sum(sales_units * item_price_eur),\n            max_order_price = max(pos_sales * item_price_eur),\n            avg_price_SKU = mean(item_price_eur),\n            items_sold = n())\n\nDBI::dbDisconnect(duck)",
    "crumbs": [
      "Databases",
      "DuckDB"
    ]
  },
  {
    "objectID": "qmd/db-duckdb.html#sec-db-duckdb-arrow",
    "href": "qmd/db-duckdb.html#sec-db-duckdb-arrow",
    "title": "DuckDB",
    "section": "Apache Arrow",
    "text": "Apache Arrow\n\nto_duckdb() and to_arrow(): Converts between using {arrow} engine and {duckdb} engieg in workflow without paying any cost to (re)serialize the data when you pass it back and forth\n\nUseful in cases where something is supported in one of Arrow or DuckDB but not the other\n\nBenefits\n\nUtilization of a parallel vectorized execution engine without requiring any extra data copying\nLarger Than Memory Analysis: Since both libraries support streaming query results, we are capable of executing on data without fully loading it from disk. Instead, we can execute one batch at a time. This allows us to execute queries on data that is bigger than memory.\nComplex Data Types: DuckDB can efficiently process complex data types that can be stored in Arrow vectors, including arbitrarily nested structs, lists, and maps.\nAdvanced Optimizer: DuckDB‚Äôs state-of-the-art optimizer can push down filters and projections directly into Arrow scans. As a result, only relevant columns and partitions will be read, allowing the system to e.g., take advantage of partition elimination in Parquet files. This significantly accelerates query execution.\n\nExample (using a SQL Query; method 1)\n# open dataset\nds &lt;- arrow::open_dataset(dir_out, partitioning = \"species\")\n# open connection to DuckDB\ncon &lt;- dbConnect(duckdb::duckdb())\n# register the dataset as a DuckDB table, and give it a name\nduckdb::duckdb_register_arrow(con, \"my_table\", ds)\n# query\ndbGetQuery(con, \"\n  SELECT sepal_length, COUNT(*) AS n\n  FROM my_table\n  WHERE species = 'species=setosa'\n  GROUP BY sepal_length\n\")\n\n# clean up\nduckdb_unregister(con, \"my_table\")\ndbDisconnect(con)\n\nfiltering using a partition, the WHERE format is ‚Äò&lt;partition_variable&gt;=&lt;partition_value&gt;‚Äô\n\nExample (using SQL Query; method 2)\nlibrary(duckdb)\nlibrary(arrow)\nlibrary(dplyr)\n\n# Reads Parquet File to an Arrow Table\narrow_table &lt;- arrow::read_parquet(\"integers.parquet\", as_data_frame = FALSE)\n\n# Gets Database Connection\ncon &lt;- dbConnect(duckdb::duckdb())\n\n# Registers arrow table as a DuckDB view\narrow::to_duckdb(arrow_table, table_name = \"arrow_table\", con = con)\n\n# we can run a SQL query on this and print the result\nprint(dbGetQuery(con, \"SELECT SUM(data) FROM arrow_table WHERE data &gt; 50\"))\n\n# Transforms Query Result from DuckDB to Arrow Table\nresult &lt;- dbSendQuery(con, \"SELECT * FROM arrow_table\")\nExample (using dplyr)\nlibrary(duckdb)\nlibrary(arrow)\nlibrary(dplyr)\n\n# Open dataset using year,month folder partition\nds &lt;- arrow::open_dataset(\"nyc-taxi\", partitioning = c(\"year\", \"month\"))\n\nds %&gt;%\n  # Look only at 2015 on, where the number of passenger is positive, the trip distance is\n  # greater than a quarter mile, and where the fare amount is positive\n  filter(year &gt; 2014 & passenger_count &gt; 0 & trip_distance &gt; 0.25 & fare_amount &gt; 0) %&gt;%\n  # Pass off to DuckDB\n  to_duckdb() %&gt;%\n  group_by(passenger_count) %&gt;%\n  mutate(tip_pct = tip_amount / fare_amount) %&gt;%\n  summarize(\n    fare_amount = mean(fare_amount, na.rm = TRUE),\n    tip_amount = mean(tip_amount, na.rm = TRUE),\n    tip_pct = mean(tip_pct, na.rm = TRUE)\n  ) %&gt;%\n  arrange(passenger_count) %&gt;%\n  collect()\n\nIn the docs, the example has to_duckdb after the group_by. Not sure if that makes a difference in speed.¬†\n\nExample (Streaming Data)\n# Reads dataset partitioning it in year/month folder\nnyc_dataset = open_dataset(\"nyc-taxi/\", partitioning = c(\"year\", \"month\"))\n\n# Gets Database Connection\ncon &lt;- dbConnect(duckdb::duckdb())\n\n# We can use the same function as before to register our arrow dataset\nduckdb::duckdb_register_arrow(con, \"nyc\", nyc_dataset)\n\nres &lt;- dbSendQuery(con, \"SELECT * FROM nyc\", arrow = TRUE)\n# DuckDB's queries can now produce a Record Batch Reader\nrecord_batch_reader &lt;- duckdb::duckdb_fetch_record_batch(res)\n\n# Which means we can stream the whole query per batch.\n# This retrieves the first batch\ncur_batch &lt;- record_batch_reader$read_next_batch()",
    "crumbs": [
      "Databases",
      "DuckDB"
    ]
  },
  {
    "objectID": "qmd/db-duckdb.html#sec-db-duckdb-sql",
    "href": "qmd/db-duckdb.html#sec-db-duckdb-sql",
    "title": "DuckDB",
    "section": "SQL",
    "text": "SQL\n\nMisc\n\nDocs\n\nExample: Connect, Read in Parallel, and Query\nsqltxt &lt;- paste0(\n  \"select\n        state, min(measurement) as min_m,\n        max(measurement) as max_m,\n        avg(measurement) as mean_m\n  from read_csv('\", file_name, \"',\n        parallel = true,\n        delim = ',',\n        header = true,\n        columns = {\n            'measurement': 'DOUBLE',\n            'state': 'VARCHAR'\n        }\n  )\n  group by state\"\n)\ncon &lt;- \n  dbConnect(duckdb(), \n            dbdir = \":memory:\")\ndbGetQuery(con, \n           sqltxt)\ndbDisconnect(con, \n             shutdown = TRUE)\ngc()\n\nFastest method besides polars for running this operation in this benchmark\n\nStar Expressions\n\nAllows you dynamically select columns\n-- select all columns present in the FROM clause\nSELECT * FROM table_name;\n-- select all columns from the table called \"table_name\"\nSELECT table_name.* FROM table_name JOIN other_table_name USING (id);\n-- select all columns except the city column from the addresses table\nSELECT * EXCLUDE (city) FROM addresses;\n-- select all columns from the addresses table, but replace city with LOWER(city)\nSELECT * REPLACE (LOWER(city) AS city) FROM addresses;\n-- select all columns matching the given expression\nSELECT COLUMNS(c -&gt; c LIKE '%num%') FROM addresses;\n-- select all columns matching the given regex from the table\nSELECT COLUMNS('number\\d+') FROM addresses;",
    "crumbs": [
      "Databases",
      "DuckDB"
    ]
  },
  {
    "objectID": "qmd/db-duckdb.html#sec-db-duckdb-remcon",
    "href": "qmd/db-duckdb.html#sec-db-duckdb-remcon",
    "title": "DuckDB",
    "section": "Remote Connections",
    "text": "Remote Connections\n\nMisc\n\nNotes from\n\nQuery Remote Parquet Files with DuckDB\n\n\nhttpfs Extension\n\nCreate a db in memory since the data is stored remotely.\nconn &lt;- \n  DBI::dbConnect(\n    duckdb::duckdb(),\n    dbdir = \":memory:\"\n  )\nInstall and Load httpfs extension\nDBI::dbExecute(conn, \"INSTALL httpfs;\")\nDBI::dbExecute(conn, \"LOAD httpfs;\")\n\nCurrently not available for Windows\n\nQuery\nparquet_url &lt;- \"url_to_parquet_files\"\nres &lt;- DBI::dbGetQuery(\n  conn, \n  glue::glue(\"SELECT carrier, flight, tailnum, year FROM '{parquet_url}' WHERE year = 2013 LIMIT 100\")\n)\n\nQueries that needs more data and return more rows takes longer to run, especially transmitting data over the Internet. Craft carefully your queries with this in mind.\n\nTo use {dplyr}, a View must first be created\nDBI::dbExecute(conn, \n               glue::glue(\"CREATE VIEW flights AS SELECT * FROM PARQUET_SCAN('{parquet_url}')\"))\nDBI::dbListTables(conn)\n#&gt; [1] \"flights\"\n\ntbl(conn, \"flights\") %&gt;%\n  group_by(month) %&gt;%\n  summarise(freq = n()) %&gt;%\n  ungroup() %&gt;%\n  collect()\nClose connection: DBI::dbDisconnect(conn, shutdown = TRUE)\n\n{duckdbfs}\n\nCreate dataset object\nparquet_url &lt;- \"url_to_parquet_files\" #e.g. AWS S3\nds &lt;- duckdbfs::open_dataset(parquet_url)\nQuery\nds %&gt;%\n  group_by(month) %&gt;%\n  summarise(freq = n()) %&gt;%\n  ungroup() %&gt;%\n  collect()",
    "crumbs": [
      "Databases",
      "DuckDB"
    ]
  },
  {
    "objectID": "qmd/db-duckdb.html#sec-db-duckdb-ext",
    "href": "qmd/db-duckdb.html#sec-db-duckdb-ext",
    "title": "DuckDB",
    "section": "Extensions",
    "text": "Extensions\n\nVS Code extension\n\nConnect to a local DuckDB instance\nCreate new in-memory DuckDB instance\nView DuckDB tables, columns, and views\nRun SQL queries on open DuckDB connections\nAttach SQLite database files to in-memory DuckDB instances\nQuery remote CSV and Parquet data files with DuckDB HTTPFS extension\nCreate in-memory DuckDB tables from remote data sources and query results\nManage DuckDB connections in SQLTools Database Explorer\nAutocomplete SQL keywords, table names, column names, and view names on open database connections in VSCode SQL editor\nSave named SQL query Bookmarks\nUse SQL Query History\nExport SQL query results in CSV and JSON data formats\nintegrate with the equally spiffy SQL Tools extension\n\nJSON extension\n\nExample: From hrbrmstr drop\nINSTALL 'json';\nLOAD 'json';\n\nCOPY (\n  SELECT * FROM (\n    SELECT DISTINCT\n      cve_id,\n      unnest(\n        regexp_split_to_array(\n          concat_ws(\n            ',',\n            regexp_extract(case when cweId1 IS NOT NULL THEN cweId1 ELSE regexp_replace(json_extract_string(problem1, '$.description'), '[: ].*$', '') END, '^(CWE-[0-9]+)', 0),\n            regexp_extract(case when cweId2 IS NOT NULL THEN cweId2 ELSE regexp_replace(json_extract_string(problem2, '$.description'), '[: ].*$', '') END, '^(CWE-[0-9]+)', 0)\n          ),\n          ','\n        )\n      ) AS cwe_id\n    FROM (\n      SELECT \n        json_extract_string(cveMetadata, '$.cveId') AS cve_id, \n        json_extract(containers, '$.cna.problemTypes[0].descriptions[0]') AS problem1,\n        json_extract(containers, '$.cna.problemTypes[0].descriptions[1]') AS problem2,\n        json_extract_string(containers, '$.cna.problemTypes[0].cweId[0]') AS cweId1,\n        json_extract_string(containers, '$.cna.problemTypes[0].cweId[1]') AS cweId2\n      FROM \n        read_json_auto(\"/data/cvelistV5/cves/*/*/*.json\", ignore_errors = true) \n    )\n    WHERE \n      (json_extract_string(problem1, '$.type') = 'CWE' OR\n       json_extract_string(problem2, '$.type') = 'CWE')\n    )\n  WHERE cwe_id LIKE 'CWE-%'\n) TO '/data/summaries/cve-to-cwe.csv' (HEADER, DELIMETER ',')\n\nProcesses a nested json\nClones the CVE list repo, modify the directory paths and run it. It burns through nearly 220K hideous JSON files in mere seconds, even with some complex JSON operations.\n\nDBs\n\nMySQL, Postgres, SQLite\n\nMight need to use FORCE INSTALL postgres\n\nAllows DuckDB to connect to those systems and operate on them in the same way that it operates on its own native storage engine.\nUse Cases\n\nExport data from SQLite to JSON\nRead data from Parquet into Postgres\nMove data from MySQL to Postgres\nDeleting rows, updating values, or altering the schema of a table in another DB\n\nNotes from\n\nMulti-Database Support in DuckDB\n\nHas other examples including transaction operations\n\n\nExample: Open SQLite db file\nATTACH 'sakila.db' AS sakila (TYPE sqlite);\nSELECT title, release_year, length FROM sakila.film LIMIT 5;\n\nATTACH opens the db file and TYPE says that it‚Äôs a SQLite db file\nMultiple dbs without using TYPE\nATTACH 'sqlite:sakila.db' AS sqlite;\nATTACH 'postgres:dbname=postgresscanner' AS postgres;\nATTACH 'mysql:user=root database=mysqlscanner' AS mysql;\nIn python\nimport duckdb\ncon = duckdb.connect('sqlite:file.db')\n\nExample: Switch between attached dbs\nUSE sakila;\nSELECT first_name, last_name FROM actor LIMIT 5;\n\nUSE switches from the previous db to the ‚Äúsakila‚Äù db\n\nExample: View all attached dbs\nSELECT database_name, path, type FROM duckdb_databases;\nExample: Copy table from one db type to another\nCREATE TABLE mysql.film AS FROM sqlite.film;\nCREATE TABLE postgres.actor AS FROM sqlite.actor;\nExample: Joins\nSELECT first_name, last_name\nFROM mysql.film\nJOIN sqlite.film_actor ON (film.film_id = film_actor.film_id)\nJOIN postgres.actor ON (actor.actor_id = film_actor.actor_id)\nWHERE title = 'ACE GOLDFINGER';",
    "crumbs": [
      "Databases",
      "DuckDB"
    ]
  },
  {
    "objectID": "qmd/db-relational.html",
    "href": "qmd/db-relational.html",
    "title": "Relational",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Databases",
      "Relational"
    ]
  },
  {
    "objectID": "qmd/db-relational.html#sec-db-rel-misc",
    "href": "qmd/db-relational.html#sec-db-rel-misc",
    "title": "Relational",
    "section": "",
    "text": "Packages\n\n{dbplyr}\n\ncompute stores results in a remote temporary table\ncollect retrieves data into a local tibble.\ncollapse doesn‚Äôt force computation, but instead forces generation of the SQL query.\n\nsometimes needed to work around bugs in dplyr‚Äôs SQL generation.\n\n\n{dm}\n\nCan join multiple tables from a db, but keeps the meta info such as table names, primary and foreign keys, size of original tables etc.\n\n{dbx} - Convenience functions for insert, update, upsert, and delete\n\nEasy parameterization\nHigh performance batch operations\nDate and time support\nWorks well with auto-incrementing primary keys\nConnection Pooling\nConnects with DBI, so can also use with {dbplyr}\nSupports Postgres, MySQL, MariaDB, SQLite, SQL Server, and more\n\n\nBenchmarks\n\nClickBench ‚Äî a Benchmark For Analytical DBMS\n\nRelational databases do not keep all data together but split it into multiple smaller tables. That separation into sub-tables has several advantages:\n\nAll information is stored only once, avoiding repetition and conserving memory\nAll information is updated only once and in one place, improving consistency and avoiding errors that may result from updating the same value in multiple locations\nAll information is organized by topic and segmented into smaller tables that are easier to handle\n\nOptimized for a mix of read and write queries that insert/select a small number of rows at a time and can handle up to 1TB of data reasonably well.\nThe main difference between a ‚Äúrelational database‚Äù and a ‚Äúdata warehouse‚Äù is that the former is created and optimized to ‚Äúrecord‚Äù data, whilst the latter is created and built to ‚Äúreact to analytics‚Äù.\nTypes\n\nEmbedded aka In-Process (see Databases, Engineering &gt;&gt; Terms): DuckDB (analytics) and SQLite (transactional)\nServer-based: postgres, mysql, SQL Server\n\nMix of transactional and analytical\nDistributed SQL (database replicants across regions or hybrid (on-prem + cloud)\n\nmysql, postgres available for both in AWS Aurora (See below)\npostgres available using yugabytedb\nSQL Server on Azure SQL Database\nCloud Spanner on GCP\n\n\n\nApache Avro\n\nRow storage file format unlike parquet\nA single Avro file contains a JSON-like schema for data types and the data itself in binary format\n4x slower reading than csv but 1.5x faster writing than csv\n1.7x smaller file size than csv\n\nWrapper for db connections (e.g.¬†con_depA &lt;- connect_databaseA(username = ..., password = ...) )\n# ... other stuff including code for \"connect_odbc\" function\n\n# connection attempt loop\nwhile(try &lt; retries) {\n¬† ¬† con &lt;- connect_odbc(source_db = \"&lt;database name&gt;\"\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† username = username,\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† password = password)\n¬† ¬† if(class(con) == \"NetexxaSQL\") {\n¬† ¬† ¬† ¬† try &lt;- retries + 1\n¬† ¬† } else if (!\"NetezzaSQL\" %in% class(con) & try &lt; retries {\n¬† ¬† ¬† ¬† warning(\"&lt;database name&gt; connection failed. Retrying...\")\n¬† ¬† ¬† ¬† try &lt;- try + 1\n¬† ¬† ¬† ¬† Sys.sleep(retry_wait)\n¬† ¬† } else {\n¬† ¬† ¬† ¬† try &lt;- try + 1\n¬† ¬† ¬† ¬† warning(\"&lt;database name&gt; connection failed\")\n¬† ¬† }\n}\n\nGuessing ‚ÄúNetezzaSQL‚Äù is some kind of error code for a failed connection to the db\n\nBenchmarks\n\nExample\n\nData\n\n~54,000,000 rows and 6 columns\n10 .rds files with gz compression is 220MB total,\n\nIf they were .csv, 1.5 GB\n\nSQLite file is 3 GB\nDuckDB file is 2.5 GB\nArrow creates a structure of directories, 477 MB total\n\nOperation: read, filter, group_by, summarize\nResults\n##¬† format¬† ¬† ¬† ¬† ¬† median_time mem_alloc\n##¬† &lt;chr&gt;¬† ¬† ¬† ¬† ¬† ¬† ¬† &lt;bch:tm&gt; &lt;bch:byt&gt;\n## 1 R (RDS)¬† ¬† ¬† ¬† ¬† ¬† ¬† 1.34m¬† ¬† 4.08GB\n## 2 SQL (SQLite)¬† ¬† ¬† ¬† ¬† 5.48s¬† ¬† 6.17MB\n## 3 SQL (DuckDB)¬† ¬† ¬† ¬† ¬† 1.76s¬† 104.66KB\n## 4 Arrow (Parquet)¬† ¬† ¬† 1.36s¬† 453.89MB\n\nTradional relational db solutions balloon up the file size\n\nSQLite 2x, DuckDB 1.66x (using csv size)",
    "crumbs": [
      "Databases",
      "Relational"
    ]
  },
  {
    "objectID": "qmd/db-relational.html#sec-db-rel-brands",
    "href": "qmd/db-relational.html#sec-db-rel-brands",
    "title": "Relational",
    "section": "Brands",
    "text": "Brands\n\nSQLite vs MySQL as transactional dbs (article)\n\nSQLite:\n\nEmbedded, size ~600KB\nLimited data types\nBeing self-contained, other clients on a network would not have access to the database (no multi-users) unlike with MySQL\nNo built-in authentication that is supported\nMultiple processes are able to access the database at the same time, but making changes at the same time is not something supported\nUse Cases\n\nData being confined in the files of the device is not a problem\nNetwork access to the db is not needed\nApplications that will minimally access the database and not require heavy calculations\n\n\nMySQL:\n\nopposites of the sqlite stuff\nSize ~600MB\nsupports replication and scalability\nSecurity is a large; built-in features to keep unwanted people from easily accessing data\nUse cases\n\ntransactions are more frequent like on web or desktop applications\nif network capabilities are a must\nmulti-user access and therefore security and authentication\nlarge amounts of data\n\n\n\nMySQL\n\nInstallation docs\nBasic intro\nSee SQL notebook\n\nSQLite\n\n{RSQLite}\nsqlime: Online SQLite playground\n\nCloud SQL - Google service to provide hosting services for relational dbs (see Google, BigQuery &gt;&gt; Misc). Can use postgres, mysql, etc. on their machines.\n\nCloud SQL Insights - good query optimization tool\n\nAWS RDS for db instances (see Database, postgres &gt;&gt; AWS RDS)\n\nAvailable: Amazon Aurora, MySQL, MariaDB, postgres, Oracle, Microsoft SQL Server\nRDS (Relational Database Service)\n\nBenefits over hosting db on EC2: AWS handles scaling, availability, backups, and software and operating system updates\n\n\nAWS Aurora - MySQL- and PostgreSQL-compatible enterprise-class database\n\nStarting at &lt;$1/day.\nSupports up to 64TB of auto-scaling storage capacity, 6-way replication across three availability zones, and 15 low-latency read replicas.\nCreate MySQL and Postgres instances using AWS Cloudformation",
    "crumbs": [
      "Databases",
      "Relational"
    ]
  },
  {
    "objectID": "qmd/docker-fundamentals.html",
    "href": "qmd/docker-fundamentals.html",
    "title": "14¬† Docker, Fundamentals",
    "section": "",
    "text": "TOC\n\n‚ÄúA Cloud Guru: Docker Fundamentals‚Äù\nMisc\nCreating a Dockerfile\nImages\nDocker Hub\nRunning Containers\nNetworking Containers\nData Volumes\nOptimizing container file size\nRunning scripts when a container starts\nDocker utility functions\nDocker Compose\nManaging a web app with Docker-Compose\n\nMisc\n\n\n\nMisc\n\nNotes from the course, ‚ÄúA Cloud Guru: Docker Fundamentals‚Äù\nResources\n\nThe Ultimate Docker Cheatsheet\nCourse: Container Essentials: Build, Deploy, Scale\n\nMore advanced topics: swarm config, multi-container, app deployment, security, alt docker runtime\n\n\nTo start a Docker project, run docker init\n\nIntro\nExecute the command in the target project folder and it will generate docker files according to the programming language. Currently, the only DS language supported is Python.\nSets up the essential scaffolding for your project.\n\nA .dockerignore¬†to keep unwanted files out,\nA Dockerfile tailored to your project‚Äôs needs,\nA compose.yaml¬†for managing multi-container setups\nA README.Docker.md¬†for documentation.¬†\n\n\nDocker file linting tools\n\nhadolint\nSynk\nTrivy\nClaire\nAnchore\n\nDocker help commands (every command preceded by ‚Äúdocker  ‚Äù)\n\nEvery management command has its own subset of commands associated with it\nmanagement\n\ncontainer\nimage\nnetwork\nnode\nplugin\nsecret\nservice\nstack\nswarm\nsystem\nvolume\nExample docker image --help\n\nshows sub-commands for management command image and short descriptions\nFor even more information on a sub-command, add ‚Äìhelp after the sub-command\n\nExample: docker image build --help\n\n\n\n\nLinux OS stuff\n\nDebian is a very stable Linux distribution\n\nDebian-Jessie is the latest version of Debian\n\n(As of July 6 2019, it‚Äôs called buster)\nSlim is a light-weight Debian-Jessie\n\nDebian-Wheezy is the version before Jessie\n\nAlpine is a very small Linux distribution (much smaller than even Slim)\n\nAn instance of an image or the result of running an image is called container\n\nAny changes made while running the container is lost once that container has been stopped.\n\nIf you create or add a file while in the container, stop the container, rerun the container, then it will no longer be there.\n\n\nAn image is the setup of the virtual computer. A combination of a file system and parameters. A package that rolls up everything you need to run an application.\n\nComposed of stacked layers where the layers are self-contained files\nYou can download, build, and run image, but they cannot be changed (immutable)\n\nYou can have many running containers of the same image.\nDocker Hub is a registry for docker repositories. It‚Äôs like a github for images. Each repo has 1 image but the repo can store many tagged versions of that image (version control)\nPull and run a docker image from Docker Hub from local cli\n\ndocker run url/repo/image\n\nexample: docker run docker.io/library/hello-world\n\ndocker.io = docker hub\nlibrary = repo name for all ‚Äúofficial‚Äù images\nhello-world = name of image\n\n\n\nTwo ways to create a image\n\nWhile inside a container, make changes. Then use commit command to create the new image with the changes\n\nNever used. Creating a dockerfile is superior.\n\nUsing a dockerfile\n\nimages on docker hub\n\nmdancho/h2o-verse\n\nh20, tidyverse, tensorflow, keras¬†\n~2 GB\n\n\n\nCreating a dockerfile (example of a toy python flask app)\n\nArchitecture¬†\n\nstart with FROM base:tag\n\nEssentially copy-pastes the base image\nusually good to start with a base image\n\nFROM python:2.7-alpine\n\npython is the base image and 2.7-alpine is the tag\n\n\n\nRUN executes commands or scripts as if you were in the container‚Äôs OS\n\nRUN mkdir /app\n\nmakes a directory called ‚Äúapp‚Äù\n\n\nWORKDIR sets the working directory for everything that happens after this command\n\nWORKDIR /app\n\nCOPY¬†has source path+file (local) and a destination path+file (container) as args\n\nCOPY requirements.txt requirements.txt\n\nthe first requirements.txt is in the same dir as the docker file so no ‚Äú/other-dir/‚Äù required\nthe second ‚Äúrequirements.txt‚Äù says the file is to be placed into the /app dir\n\nequivalent to ‚Äú/app/requirements.txt‚Äù because /app is our working directory\n\n\nCannot use .. to move above the dockerfile directory. Every path must be below it.\n\nInstall packages\n\nRUN pip -install¬† -r requirements.txt\n\ninside requirements.txt says Flask==0.12\n\n\nCopy entire local directory to the working directory\n\nCOPY . .\n\nfirst period says copy everything is the current local directory\nsecond period says put everything in the working dir\n\n\nLABELs have key-value pairs. can be useful in production settings. The can be retrieved with a command later on.¬†\n\nSome uses:\n\nfilter a container‚Äôs output based on a label ¬†\ncan include scripts for automatic load balancing (also see aws load balancer section below)\n\nLABEL maintainer=‚ÄúEric Book ericbook@email.com‚Äù\\\n\n\n\n\n¬†¬† ¬†¬†¬† ¬†¬†¬† ¬†¬†¬† ¬†¬†¬† ¬†¬†¬† ¬†¬†version=‚Äú1.0‚Äù¬†¬† ¬†\n\nOften changed or added to, so they should be close to the end of the dockerfile (but not the last line)\nCMD gives the default instruction when the image gets ran which is different from RUN commands, which are executed when the image is built.\n\nCMD flask run ‚Äìhost=0.0.0.0 ‚Äìport=5000\nUnder-the-hood: CMD instructions are arguments to an entrypoint script and get run through a default docker entrypoint (see waaaaay below for info about entrypoints)\n\nA \\\\ is Linux operator that chains together instructions so they can be in separate lines of code for easier reading. Think a space also does the same thing, but you can‚Äôt see it in the code.\nThe ordering of the inputs in the dockerfile will affect its size and efficiency\n\nThe source code is much more likely to change in the future than the package dependencies. Therefore even though there would be fewer COPY commands, it‚Äôs best to install the dependencies before copying the source code.\nWhenever changes to the source code occur Docker has a caching mechanism, so that it doesn‚Äôt rebuild everything above the layer where the changes occur. COPY . . executes in millisecs so the rebuild happens almost instantly while installing dependencies could take minutes.\n\nAttaching packages and libraries\n\nDownload and install R packages\n\nRUN R -e ‚Äúinstall.packages(c(‚Äòshinydashboard‚Äô, ‚Äòreticulate‚Äô, ‚Äòshiny‚Äô))‚Äù\n\nDownload, install Python libraries (ubuntu image)\n\nRUN ln -s /usr/bin/python3 /usr/bin/python && \\\n\n\n\n¬†¬† ¬†¬†¬† ¬†¬†¬† ¬†¬†¬† ¬†¬†¬† ¬†¬†¬† ¬†¬†¬† ¬†¬†¬† ¬†¬†¬† ¬†¬†¬† ¬†¬†¬† ¬†¬† ¬†¬†¬† ¬†¬†¬†¬†¬† ¬†¬†¬† ¬†¬†¬†ln -s /usr/bin/pip3 /usr/bin/pip¬†¬† ¬†\n\nRUN¬†apt-get update\nRUN apt-get install -y libpython-dev\nRUN apt-get install -y libpython3-dev\nImages\n\nBuild image from dockerfile (** don‚Äôt forget the dot at the end **)\n\ndocker image build -t web1 .\n\nimage is the management command\nbuild is the sub-command of image\n-t is the flag for ‚Äútag‚Äù\nweb1 is the tag. This allows us to to refer to this image as ‚Äúweb1‚Äù instead of a hash\n. says build the image and place in current directory\n\nEnd of build shows hash id ‚Äúsuccessfully built ‚Äù\ndocker image build -t web1:1.0\n\nversioning the image\nfor just ‚Äúweb1‚Äù the version will be web1:latest\n\n\nInspect the image\n\ndocker image inspect web1\ninfo in json format\nAt the top complete hash id\nshows the versions of the image under ‚Äúrepotag‚Äù\nvarious info about how the layers were created\nthe number of layers created is shown at the bottom by the lines preceded by a ‚Äúsha‚Äù and a hash\n\nList of images in local docker host\n\ndocker image ls\n\nbase images loaded from dockerfiles listed alongside the images we create\n images are ‚Äúdangling images.‚Äù They are failed image builds or images that were built on top of already existing images\n\nsafe to delete; frees up disk space\n\n\n\nDelete¬†local image\n\nUsing name and version tag\n\ndocker image rm web1:1.0\n\nUsing id\n\ndocker image rm -f 633f\n\n-f is the ‚Äúforce‚Äù flag\n\nnecessary when image has been tagged/copied (like when pushing to docker hub) and you want to remove both images.\n\n633f - only need the first four characters of the hash id\n\nids in docker image ls\n\n\n\n\n\nDocker Hub\n\nLogin to Docker Hub\n\nHave to do it once then a config file is created so you don‚Äôt have to do it again\nInput your username and password for your Docker Hub acct\ndocker login\n\nPush image to Docker Hub\n\nTag image with docker hub username\n\ndocker image tag web1 ercbk/web1:latest\n\nweb1 is the image we want to push\nercbk/web1:latest\n\nercbk is the username\nweb1 is the repo\nlatest is the tag\n\n\ndocker image ls will show the newly tagged image\n\nPush image to docker hub\n\ndocker image push ercbk/web1:latest\n\n\n\nRunning Containers\n\n** Unless you include -d in the run command, you will need to open a separate terminal (i.e.¬†different from the one you ran the run command in) in order run more docker cli commands while an app is running **\n** container names and ports need to be unique for each running container **\ndocker container ls\n\nlists all running containers, container_id, image, creation time, runtime, name\n-a\n\nshows all stopped containers\n\n\ndocker container rm &lt;name&gt;\n\ndeletes a stopped container\ncan also use 1st four characters of &lt;container_id&gt;\n\nshown in ls (see above)\n\n\ndocker container run\n\nthe basic run command\nhardly ever want to use just the basic commmand\n\nsee flags and examples below\n\n\ndocker container stop &lt;name&gt;\n\nstops a container\nadd more s to stop more than one container\n\nwith spaces between the names\n\nalso, ctrl + c\n\nuse it in the same terminal you used the run command in.\nonly works if you included the -it flag when you started it.\n\nalso see ‚Äìrm below\ndocker container stop $( docker container ls -q)\n\nStops all running containers\n-q flag says list them quietly, so it doesn‚Äôt print them out\nif you a ‚Äústop requires at least one argument‚Äù error, then there aren‚Äôt any containers running\n\n\ndocker container logs &lt;name&gt;\n\ncan also use 1st four characters of &lt;container_id&gt;\nfor active or inactive containers\nshows times container was accessed\n-f\n\nruns log in the foreground (i.e.¬†you can view accesses in real-time)\nctrl + c to kill it\n\n\ndocker container stats\n\nreal-time monitoring of active containers\nshows cpu and memory usage for each container\nnetwork input/output\n\nflags\n\n** Don‚Äôt think order matters except for that the name of the image needs to be last **\n-d\n\nruns the container in the background\nallows you to run commands in the same terminal window that you used the container run command in\n\n-e\n\nallows you to pass an environment variable into the container\nNot always necessary\n\nFlask requires one (e.g.¬†-e FLASK_APP=app.py)\n\napp.py is the name of the app script\n\n\nmultiple -e flags are allowed\n\n-e FLASK_DEBUG=1\n\nturns debug mode on when you run the container\nuse along with -v to make real-time changes to the app while the container is running\n\n\n\n-it\n\nallows for unix commands such as ctrl c to kill a process (such as a running container)\nmakes the docker container interactive. Which allows you to go into the container, navigate the file system, and make changes.\n\n‚Äìname\n\ndocker automatically provide a name for a container but this flag allows you to provide your own\n‚Äìname moose\n\n-p¬†\n\nports\n\nmap ports from local machine to ports within docker\n\nexpected to supply two ports separated by a colon\n-p 5000:5000\n\nThe first 5000 is the host port. The port you use to interact with the app in your browser (e.g.¬†localhost:5000)\n\nAll containers running on your docker host need to have unique ports to run on. (e.g.¬†-p 5001:8000)\nSo, I think this can be any port you want as long something else isn‚Äôt already using it.\n\nThe second 5000 is the container port that was specified in the dockerfile\n\nTried 5000:5000 and the app didn‚Äôt run in the browser even when the dockerfile had 5000 specified, but after specifying 800 in the dockerfile, -p 5000:8000 did work\n\nThis was run on the default docker network\n\nRunning -p 5000:5000 DID work on a custom network.\n\nAlso these are 2 different apps. The 1st one was stand-alone (03-lecture cloud guru docker fundamentals), and the 2nd ran in conjunction with a redis server (see custom network section below)(09-lecture in cloud guru docker fundamentals). Not sure if that makes a difference\n\n\n\nWhen running the container, if it‚Äôs an app, it can be viewed in a browser at localhost:5000 (i.e.¬†the first port specified)\n-p 5000\n\ndocker will attach a random port number\n\nYou can specify more than one port mapping e.g.¬†-p 5000:5000 -p 3838:3838\nports (second port specified) for common images\n\nredis (storage) - 6379\nnginx (web server) - 80\n\nopen source, handles a lot of connections efficiently, used to host a lot of websites\n\nRStudio: -p 3838:3838\nShiny apps launched from within RStudio: -p 8787:8787\n\n\n‚Äìrestart\n\nwith value, on-failure\n\nsays restart the container if there‚Äôs some catastrophic failure (e.g.¬†docker daemon dies and restarts)\nuseful in production\n\ncannot be included if ‚Äìrm is also used\n\n‚Äìrm\n\ndeletes the container once it has been stopped\ncannot be included if ‚Äìrestart is also used\n\n-v\n\nvolumes\nUse cases\n\nwhile developing. Makes changes inside the container in real-time.\nstore data in a db\nMapping to a folder on your local machine allows you to work on projects stored within the container\n\n(1st half) requires address (local machine) to the directory with the script (e.g.¬†app script not dockerfile) where you‚Äôre making changes  (2nd half) address inside the container where this directory should be ‚Äúmounted‚Äù (i.e.¬†where the script you‚Äôre making changes to is located inside the container)\n\n1st half\n\nFor Linux, you can used the shortcut, ‚Äú$PWD‚Äù which is a linux environment variable that stores the path to the working directory. It has the same value as the command ‚Äúpwd -L‚Äù which Prints Working Directory.\n\nfyi the -L has something to do with ‚Äúsymlinks‚Äù which are thinks that can be created that point to another directory. If you‚Äôre in in the symlink directory -L will print that directory and -P will print the directory that the symlink points to.\nsame thing for Mac, except the quotes might need to be included\n\nFor windows, ‚Äú/c/users/&lt;user_name&gt;/path/to/directory‚Äù\n\n2nd half\n\n‚Äú/app‚Äù which is where the app script is (also the working directory specified in the dockerfile for the example)\n\nExample in Linux, docker container run -it -p 5000:5000 -e FLASK\\_APP=app.py --rm -name web1 -e FLASK\\_DEBUG=1 -v $PWD:/app web1\nExample in Windows,¬†docker container run --rm -itd -p 5000:5000 -e FLASK\\_APP=1 -e FLASK\\_DEBUG=1 --name web2 -v /C/Users/tbats/Documents/R/Projects/Docker/Courses/cloud-guru-docker-fund/09-linking-containers-with-docker-networks:/app web2\n\nWhile running, if you open a new terminal and docker container inspect &lt;name&gt;, then there should be a ‚Äúmount‚Äù section with type = ‚Äúbind‚Äù\nTroubleshooting if changes don‚Äôt show up in the container in real-time\n\nit‚Äôs probably the path specifications for the value of the -v tag. Docker is picky, esp w/WIndows.\nMake sure code script and docker are on same drive\nNext try replacing Alpine Linux with Slim Linux in the dockerfile ‚Äì&gt; rebuild image ‚Äì&gt; run container with volume flag + the addresses like stated above\n\nSomething about inotify in Alpine not doing something\n\nsee exec command below\n\n\n\nExamples:\n\ndocker container run -it -p 5000:8000 -e FLASK\\_APP=app.py web1\n\nweb1 is the name of the image.\nThis is probably least number of flags necessary to run an app in a container\n\ndocker container run -it --rm --name web2 -p 5000:8000 -e FLASK\\_APP=app.py -d --restart on-failure web2\n\nexec\n\nexecutes an interactive bash session in the container\nContainer must be running, so open a new terminal window and type the code line:\n\ndocker container exec -it web1 bash\n\nwhere web1 in the name of the container\nbash is for the Slim distribution of Linux; use sh for Alpine\n\nOr run a bash command detached in the background docker exec -t -d web1 bash \"ls -al\"\n\nyou‚Äôll be logged in under ‚Äúroot‚Äù and dropped into wherever you designated the working directory in the dockerfile\nType ls -la to view the files\nExample of a debug\n\nPython app isn‚Äôt showing changes after running it with a volume flag (-v)\nyou exec a bash session inside the container\ndelete some .pyc files (might be corrupted somehow) that are created when python runs flask\n\nrm *.pyc\nls -la to confirm\n\ngo to local script and make changes, save\ngo to terminal window where container is running and see changes to scripts detected in the terminal\ngo to browser where app is running and refresh\nwait a few secs and changes show up. Yay.\ngo back to bash terminal window, ctrl + d to kill it\n\n\n\nNetworking Containers\n\nInternal networks (LANs), external networks (WAN)\n\nWAN is a wide area network. Can be public or private. Stretches across city or industrial park, etc\n\nAccess addresses\n\nservers bound to 0.0.0.0: give access to any computers on your LAN or WAN\nif localhost:, then only laptop running the server can connect to it\nif : then any computer on your LAN, WAN, or on the internet can connect\n\ne.g.¬†192.168.1.4:5000\n\n\nList of Docker networks\n\ndocker network ls\nipconfig (windows) ifconfig (Linux, Mac)\n\nshows info about networks\n\nbridge network is docker0 which is the docker supplied network\n\nfyi docker0 didn‚Äôt show up on Windows for me\n\n\nPing from one container to another\n\nNote: ping and ifconfig removed from Alpine and Slim image\n\nto reinstall, add this to 2nd line of dockerfile\n\nAlpine\n\nRUN apk update && apk add iputils\n\nSlim\n\nRUN apt-get update && apt-get install -y net-tools iputils-ping\n\n\n\ndocker exec web2 ping 172.17.0.2\n\nwhere 172.17.0.2 is the other container‚Äôs eth0 inet address\n\nfound by docker exec &lt;container name&gt; ifconfig\n\n\nctrl + c to stop the pinging\n\nView etc file\n\ndocker exec &lt;container name&gt; cat /etc/hosts\nshows eth0 inet address is mapped to container id\n\nCreate custom network\n\nallows us to connect containers by name which means if the addresses change, the apps won‚Äôt break.\ndocker network create --driver bridge &lt;name&gt;\n\nthe bridge driver is used for networking containers on the same docker host\nfor networking across multiple docker hosts, the overlay driver is used. (would need to research this further)\n\nInspect network\n\ndocker network inspect &lt;name&gt;\n\nadd container to custom network\n\nadd ‚Äìnet  to the run-container instruction\n\nexample:¬†docker container run -it --rm name web2 -p 5000:5000 -e FLASK\\_APP=app.py -d --net firstnetwork web2\nexample:¬†docker container run -it --rm name redis -p 6379:6379 -d --net firstnetwork redis:3.2-alpine\n\nThis containers only linked up with the app running on the browser when they were run on the custom network and not the default docker bridge network\nin debug mode:¬†docker container run -it --name web2 -p 5000:5000 -e FLASK\\_APP=app.py -e FLASK\\_DEBUG=1 -v /C/Users/tbats/Documents/R/Projects/Docker/Courses/cloud-guru-docker-fund/09-linking-containers-with-docker-networks:/app -d --rm --net firstnetwork web2\n\ncontainers will show up when you inspect firstnetwork\ncan now ping using container names (assuming both containers have been added to the network)\n\ndocker exec web2 ping redis\n\nweb2 is the container doing the pinging\nredis being pinged¬†\n\n\n\n\n\n\nData Volumes\n\nallows data to persist on docker host after the container is stopped\n\nShould save on the host for apps because they should be portable\nFor databases, not so bad\n\nA volume is nothing more than a folder on your computer that is linked to a folder inside the Docker container.\nDefault volume path on host, ‚Äú/var/lib/docker/volume/‚Äù\ndocker volume create web2\\_redis\n\nweb2_redis is the name of the volume\n\ngood idea to pick a name that‚Äôs relevant to job\n\n\ndocker volume ls\n\nlist of volumes\n\ndocker volume inspect web2\\_redis\n\nshows info about volume\n‚ÄúMountpoint‚Äù shows where the volume will be stored on the host machine\n\ndocker volume rm &lt;volume1 name&gt; &lt;volume2 name&gt;\n\nremove specific volumes by name\n\ndocker volume prune\n\nremoves all volumes\n\ndocker rm -v &lt;container name&gt;\n\nremoves container and anonymous volume\n\nWill not remove a named volume\n\n-v required else a ‚Äúdangling‚Äù is created\n\ndocker volume ls -qf dangling=true docker volume rm $(docker volume ls -qf dangling=true)\n\nremoves dangling volumes\n\nAdd volume flag, data volume name, and destination to container\n\ndocker container run -it --rm name redis -p 6379:6379 -d --net firstnetwork -v web2\\_redis:/data¬† redis:3.2-alpine\n\nweb2_redis is the name we gave to the data volume\n/data is designated by the redis people\n\nThis technique works for mysql, postgres, elasticsearch, etc. You just have to figure out the WHERE they decided that they want you to store your data (i.e.¬†the /data part)\n\ngo to their docker hub image page ‚Äì&gt; view readme ‚Äì&gt; look for section on persistent storage\nExample I went to postgres page and did an ctrl+f ‚Äúpersistent‚Äù and found a section describing when to use /data or /pgdata\n\n\n\n\nSaving the data\n\nredis does it automatically every 30 sec\nManual save if you need to save right away:\n\ndocker exec redis redis-cli SAVE\n\n\nExample: Named Volume\n\n\n¬† ¬† ¬† ¬† ¬† ¬† version: '3.8'\n¬† ¬† ¬† ¬† ¬† ¬† services:\n¬† ¬† ¬† ¬† ¬† ¬† ¬† db:\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† image: mysql\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† restart: always\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† environment:\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† MYSQL_ROOT_PASSWORD: root\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† MYSQL_DATABASE: test_db\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ports:\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† - \"3306:3306\"\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† volumes:\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† - db_data:/var/lib/mysql\n¬† ¬† ¬† ¬† ¬† ¬† volumes:\n¬† ¬† ¬† ¬† ¬† ¬† ¬† db_data:\n\n‚Äúdb_data‚Äù is the name\n‚Äú/var/lib/mysql‚Äù is the path inside the container\nAdvantages\n\nData persists after we restart or remove a container\nAccessible by other containers\n\nExample: Unnamed (aka Anonymous) Volume\n\n¬† ¬† ¬† ¬† ¬† ¬† version: '3.8'\n¬† ¬† ¬† ¬† ¬† ¬† services:\n¬† ¬† ¬† ¬† ¬† ¬† ¬† db:\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† image: mysql\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† restart: always\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† environment:\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† MYSQL_ROOT_PASSWORD: root\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† MYSQL_DATABASE: test_db\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ports:\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† - \"3306:3306\"\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† volumes:\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† - /var/lib/mysql\n\nNo volume name here but still has the path inside the container\nData will persist on restart of the container, but not after the container is stopped and removed\nNot accessible by other containers\nActually‚Ä¶ even without the ‚Äúvolumes‚Äù instruction, the ‚Äúmysql‚Äù image/dockerfile has a ‚ÄúVOLUME‚Äù instruction, so an anonymous volume would still be created.\nExample (Bind Mounts)\n\n¬† ¬† ¬† ¬† ¬† ¬† version: '3.8'\n¬† ¬† ¬† ¬† ¬† ¬† services:\n¬† ¬† ¬† ¬† ¬† ¬† ¬† db:\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† image: mysql\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† restart: always\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† environment:\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† MYSQL_ROOT_PASSWORD: root\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† MYSQL_DATABASE: test_db\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ports:\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† - \"3306:3306\"\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† volumes:\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† - $PWD/data:/var/lib/mysql\n\nInstead of using the default host directory for the volume, you can specify a location yourself\nfirst half (before colon) : where on the host machine to mount (i.e.¬†create) the volume\n\nIn the example, it‚Äôs located in the working directory + /data\nFor Linux, you can used the shortcut, ‚Äú$PWD‚Äù which is a linux environment variable that stores the path to the working directory. It has the same value as the command ‚Äúpwd -L‚Äù which Prints Working Directory.\n\nfyi the -L has something to do with ‚Äúsymlinks‚Äù which are thinks that can be created that point to another directory. If you‚Äôre in in the symlink directory -L will print that directory and -P will print the directory that the symlink points to.\nsame thing for Mac, except the quotes might need to be included\n\nFor windows, ‚Äú/c/users/&lt;user_name&gt;/path/to/directory‚Äù\n\nsecond half (after colon): specify the path inside the container you want to use for the volume. Usually specified by the db software.\n\nIn the example, it‚Äôs ‚Äú/var/lib/mysql‚Äù which has been specified by the mysql image\n\nSharing data between containers\n\nContainers sharing and receiving need to be on the same docker host\nAdd to app dockerfile\n\nVOLUME [‚Äú/app/public‚Äù]\n\nLine location in the dockerfile wasn‚Äôt specified but he put his right before the CMD line\nlocal directory with app has a folder named ‚Äúpublic‚Äù that will be shared with other containers.\n\n\nSteps\n\nbuild app image\nrun app image\nrun redis image with flags, ‚Äìvolume -from ¬†\n\ndocker container run -it --rm name redis -p 6379:6379 -d --net firstnetwork -v web2\\_redis:/data --volume -from web2¬† redis:3.2-alpine\n\n\nFiles in app‚Äôs public folder will be in redis‚Äôs container\n\nSteps\n\ngo into redis container\n\ndocker container exec -it redis sh\n\nchange directory to the public folder\n\ncd /app/public\n\ncontents of app‚Äôs public folder are in this folder too\n\nexamine contents by printing to the terminal\n\ncat main.css\n\n\n\n\nWhile containers are running, you can exec into volume container (e.g.¬†web2) add files, make changes to files, etc., and the files in the other containers will be updated in real time.\nAlternate method (*not recommended for production*)\n\nDon‚Äôt put the VOLUME instruction in the dockerfile\nAdd -v /app/public to the app run command\nAdd the ‚Äìvolume -from flags to the redis container run command like before\n\n\nOptimizing container file size\n\n.dockerignore\n\nContains file paths to files in the local project directory that you don‚Äôt want on the image (e.g.¬†files with private data, .git files can be huge)\nDuring the image build, when docker runs the COPY/ADD instructions it will bypass the files in the .dockerignore file\nFile paths in the .dockerignore file begin wherever you‚Äôve designated the working directory in WORKDIR in the dockerfile\nExamples:\n\n.git/\n\nadding a trailing ‚Äú/‚Äù isn‚Äôt necessary but lets people know it‚Äôs a directory and not a file\n\nmay want to include the .dockerignore itself\nfolder/*\n\nThe folder itself will be added to the image but all the content will be ignored\n\n**/*.txt\n\nsays ignore all files with the .txt extension\n\n!name.txt\n\nthis grants an exception to the name.txt file. It will be added to the image even with the **/*.txt\n\nYou can negate the .dockerignore file\n\nJust a * on line 1\nbegin each file path with a ‚Äú!‚Äù to specify which files to include\n\n\n\nRemoving the build dependency files of the system dependency files (** only for Alpine Linux **)\n\nIn other words the files used to build the system dependency files\nHe uses the example of a postgres dependency in the video, but no dependencies are actually listed in the dockerfile in the files folder.\n\n2 hypotheses on what he‚Äôs talking about:\n\nThe package you use in python to interact with the sql db has dependencies and files are needed to build those dependencies (build dependencies). After the dependencies are built, the build dependencies get deleted.\nThis image with include the sql db and that db has build dependencies. They get deleted\n\nI think it‚Äôs 1., but I dunno. In the Installing packages section above, there are examples of python-dev files being installed, but I‚Äôm not sure which hypothesis that favors.\n\n\n\nIt‚Äôs a bear of a bash script, so see dockerfile for details in cloud guru docker fund course, 012 lecture files folder on optimizing\n\nscript stays above the COPY . . instruction\nIf you use this script, thoroughly check everything and make sure the packages are working as intended.\n\nSteps\n\nfirst line (indent of the lines is the same in the file)\n\n\n\n\nRUN apk add --no-cache --virtual .build-deps \\\n\nsecond line, add dependencies\n\n¬† ¬† postgressql-dev dependency2 dependency3 \\\n\nThen add the rest of the bash gunk\n\n¬† ¬† && pip install -r requirements.txt \\\n¬† ¬† && find /usr/local \\\n¬† ¬† ¬† ¬† \\( -type d -a -name test -o -name tests \\) \\\n¬† ¬† ¬† ¬† -o \\( -type f -a -name '*.pyc' -o -name '*.pyo' \\) \\\n¬† ¬† ¬† ¬† -exec rm -rf '{}' + \\\n¬† ¬† && runDeps=\"$( \\\n¬† ¬† ¬† ¬† scanelf --needed --nobanner --recursive /usr/local \\\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† | awk '{ gsub(/,/, \"\\nso:\", $2); print \"so:\" $2 }' \\\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† | sort -u \\\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† | xargs -r apk info --installed \\\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† | sort -u \\\n¬† ¬† )\" \\\n¬† ¬† && apk add --virtual .rundeps $runDeps \\\n¬† ¬† && apk del .build-deps\n\nRunning scripts when a container starts\n\nInstead of making multiple similar dockerfiles/images, you can use entry points¬†into one dockerfile/image\n\nExamples:\n\nFor a postgressql image, you pass a run instruction inside the dockerfile that sets up your user authorization and password as an environment variable. It also has an ENTRYPOINT, so that if you have other projects that use a postgres sql db, they can gain access to that information through the entry point.\nRunning a db migration after a container starts\n\nrun_db_migration1 as environment variable but with a default value set to off. You can control that action through entrypoint and a script\n\ncontrol stuff in a nginx config to set an external ip after the container is running\n\n\nThe docker_entrypoint scripts aren‚Äôt in your dockerfile so they don‚Äôt add layers to an image\nSteps\n\nAdd lines to dockerfile\n\n\n\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† COPY docker-entrypoint.sh /\n\ndocker-entrypoint.sh should be a file in the root project directory\n\nNot in app directory, because it‚Äôs best practice to keep entrypoint files separate.\nFor details see section below\n\n\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† RUN chmod +x /docker-entrypoint.sh\n\ntells Linux to give the entrypoint script permission to run\n\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ENTRYPOINT [\"/docker-entrypoint.sh\"]\n\nentry point instruction that points to where the script is located.\nRun redis container as before\nBuild app image as before (gave it the name, ‚Äúwebentrypoint‚Äù)\nRun app container\n\ndocker container run -it --name webentrypoint -p 5000:5000 -e FLASK_APP=app.py -e FLASK_DEBUG=1 --rm --net firstnetwork webentrypoint\n\nname and image values changed to webentrypoint\nremoved -d flag because we want it to run in the foreground\nIf you go to localhost:5000 it has some message printed from the docker-entrypoint.sh script\nStop the app container\nRe-run the app container\n\ndocker container run -it --name webentrypoint -p 5000:5000 -e FLASK_APP=app.py -e FLASK_DEBUG=1 -e WEB2_COUNTER_MSG=\"Docker fans will have visited this page\" --rm --net firstnetwork webentrypoint\n\nWEB2_COUNTER_MSG is an environment variable that was given a default value inside the docker-entrypoint.sh script.\nSo we were able to change the environment variable without having to run the container, exec into the container, then change it.\nDetails on the dockerentrypoint.sh file\n\nWhen you run the container, this script gets run before the dockerfile\nExports an environment variable that gets accessed by the app.py script\n\n\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† #!/bin/sh\n\nComment that tells us that we‚Äôre running a shell script\n\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† set -e\n\nsays abort if there‚Äôs an error in the script\n\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† echo \"The Dockerfile ENTRYPOINT has been executed!\"\n\nLine that prints in the terminal when we run the container\n\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† export WEB2_COUNTER_MSG=\"${WEB2_COUNTER_MSG:-carbon based life forms have sensed this website}\"\n\nWhere the custom scripting takes place\nIn this case, WEB2_COUNTER_MSG environment variable is created and a default value set\n\nI think the syntax after the = has something to do with making the value a default value\n\n\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† exec \"$@\"\n\nSays that after running everything in this script, then run the CMD stuff in the dockerfile\nDocker utility functions (can be run in any directory)\n\ndocker system df\n\nshows resources being used by docker\nadding -v tag at the end, produces a verbose output all the info is broken down further\n\ndocker system info\n\ninfo about your docker installation\n\nuseful when reporting bugs, creating issues\nverify docker installation\n\n\ndocker system prune\n\ndeletes all the crud\n\nstopped containers that you forgot to include ‚Äìrm\nvolumes not used by at least one container\nnetworks not used by at least one container\nall dangling images\n\nadd -f flag to execute the prune without the confirmation message\n\nuseful for automating through cron jobs, etc.\n\nadd -a flag to remove ALL unused images\n\n**only run if want every image not being used by a running container to be deleted**\n\n\n\nDocker Compose (note dash between docker and compose in commands)\n\ndocker-compose.yml properties\n\nyaml files do not need to be in the same directory as your dockerfiles. You just have to give the path in the build property (see below)\n\n\n\nversion: '3'\n\n\n# pound sign is for comments\nservices:\n¬† redis:\n¬† ¬† image: 'redis:3.2-alpine'\n¬† ¬† ports:\n¬† ¬† ¬† - '6379:6379'\n¬† ¬† volumes:\n¬† ¬† ¬† - 'redis:/data'\n\n¬† web:\n¬† ¬† build: '.'\n¬† ¬† depends_on:\n¬† ¬† ¬† - 'redis'\n¬† ¬† env_file:\n¬† ¬† ¬† - '.env'\n¬† ¬† ports:\n¬† ¬† ¬† - '5000:5000'\n¬† ¬† volumes:\n¬† ¬† ¬† - '.:/app'\n\n¬† worker:\n¬† ¬† build: '.'\n¬† ¬† command: celery &lt;command&gt;\n¬† ¬† depends_on:\n¬† ¬† ¬† - 'redis'\n¬† ¬† env_file:\n¬† ¬† ¬† - '.env'\n¬† ¬† volumes:\n¬† ¬† ¬† - '.:/app'\n\nvolumes:\n¬† redis: {}\n\nversion is the version of the compose api\nservices are the containers we‚Äôre building\n\nservice names (e.g.¬†redis, web) will end up being the container and image names (see below for the exception)\nimage property\n\nuses the base:tag format like the value for the FROM instruction in the dockerfile\n\nbuild property\n\n‚Äò.‚Äô says build an image from the current directory\nIf your app has it‚Äôs own folder then you need to specify the path\n\nexample ‚Äò./web‚Äô¬†\n\n\nimage and build property in the same service\n\ndocker will build 2 of the same image\n\none with project name (build) and the other with the image value as the name for the image\n\nexample build: ‚Äò.‚Äô and image: ‚Äôercbk/web:1.0\n\nsays build image using dockerfile in current directory and name it ercbk/web:1.0\nuseful if the image is going to be pushed to the docker hub\n\n\nports\n\nports to be used for the container (see -p flag in running containers section for more details)\n\nthe ‚Äúbind‚Äù port (right side) supplied here needs to match the dockerfile\n\na  indicates a list in yamls.\n\nexample: forward 2 sets of ports\n\n\n\n\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† - '6379:6379'\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† - '5348:5348'\n\nnetwork\n\ntakes list inputs\nused when multiple networks are created\n\nexample: databases/workers communicate on one network while apps communicate on both networks\n\n\n\n¬† result:\n¬† ¬† build: ./result\n¬† ¬† command: nodemon --debug server.js\n¬† ¬† volumes:\n¬† ¬† ¬† - ./result:/app\n¬† ¬† ports:\n¬† ¬† ¬† - \"5001:80\"\n¬† ¬† networks:\n¬† ¬† ¬† - front-tier\n¬† ¬† ¬† - back-tier\n\n¬† worker:\n¬† ¬† build: ./worker\n¬† ¬† networks:\n¬† ¬† ¬† - back-tier\n\n¬† db:\n¬† ¬† image: postgres:9.4\n¬† ¬† volumes:\n¬† ¬† ¬† - \"db-data:/var/lib/postgresql/data\"\n¬† ¬† networks:\n¬† ¬† ¬† - back-tier\n\nvolumes\n\nsee data volumes section for more details\n means, just like for ports, values for this property are in list format\nIn the app service,\n\nspecify which directory to share\n\n‚Äô.:/app‚Äù says share the current local directory which will be the app directory in the container\n\n\nin the redis service,¬†\n\nweb2_redis is the name we give to the data volume\n/data is designated by the redis people\n\n\ndepends_on\n\nnecessary if one service depends on another. Indicates if one container needs to start before another.\n\nexample: web (app) depends on redis (db)\n\ntakes a value in list format\n\nenvironment\n\nmethod 1 for setting environment variables\nname: value pair\n\nexample:\n\n\n\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† environment:\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† FLASK_DEBUG: 'true'\n\nenv_file\n\nmethod 2 takes a list of environment files to load\nloads from top to bottom\n\nif any variables in a lower listed file match those in a file listed higher up, then the values in the earlier file get overwritten to the values in the later file\nuseful if you have a stock env file, decide to put the containers into production, then you can just add a production env file to the directory and the yaml\n\nexample: .env is the name of the file in the current directory ( is in the actual name)\nfile example\n\n\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† COMPOSE_PROJECT_NAME=web2\n\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† PYTHONBUFFERED=true¬†\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† FLASK_APP=app.py\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† FLASK_DEBUG=1\n\nThe flask variables are the ones we passed in the run containers section\nCan configure Docker Compose options here\n\nexample: supply a project name. Otherwise Docker would just use the current directory for the name of the project. Also gets added as a prefix for networks, etc. that Compose will create.\n\nThe Python buffer variable is necessary if you want to see the output from the terminal through Compose\nworker (service)\n\nUse the same dockerfile for both services\n\neg worker uses the same dockerfile as web in the example docker-compose.yml\n\nuseful for background services for your app\n\ncelery is a python library used this task\n\ndifferences\n\nno need for port to be exposed (therefore overriding the CMD instruction in the dockerfile)\nuses command property which replaces the CMD instruction in the dockerfile\n\n\nvolumes\n\nthe name given here needs to match the name of the volume given in the services property\n\nexample: for this yaml the name of the volume created is ‚Äúredis‚Äù\n\n{} - curly brackets are for adding options to the volume, such as being read-only (see docker docs for more options)\n\nin this example no options are supplied so it‚Äôs empty (still necessary though)\n\n\nManaging a web app with Docker-Compose¬†(note dash between docker and compose in commands)\n\nCurrent directory need to have the docker-compose.yml file\n\nunless you use the -f flag\n\ndocker-compose --help\n\ninfo on commands\n\ndocker-compose build\n\nbuilds an image of any service in the yaml file with a build property\ndocker image ls will show the built images with the project name as the prefix\n\nproject name set in the env file (see above)\n\n\ndocker-compose pull\n\npulls any other images specified with the image property in the yaml file\n\n¬†not just local but also pulls from docker hub I think\n\n\ndocker-compose up\n\nruns the project (everything created with project name prefix)\n\ncreates network\n\neg web2_defaults\n\ncreates volume\n\neg web2_redis\n\nstarts containers\n\neg web2_redis_1, web2_web_1\n\nthe ‚Äú_1‚Äù suffix is in case the project calls for multiple instances of the same container\n\nreminder: multiple apps require different ports (binding?) (see -p in running-a-container sections). Would also require setting up a load balancer. (see aws load balancer section below)\n\n\n\n\ndocker-compose up &lt;service&gt;\n\nstarts a specific service\nif you start a service with a dependency (‚Äúdepends_on‚Äù specified in yaml), it will also start that dependency service.\ndocker compose web\n\nstarts web but also redis, because web has a specified redis dependency\n\n\n\ndocker-compose stop\n\nstops all containers\n\ncan probably specify a container\n\neg docker-compose stop web\n\n\ncan also use ctrl+c, but (as of 2017) there‚Äôs a bug that throws an error and aborts instead\n\ndocker-compose up --build -d\n\n¬†runs both build and up commands at once\n-d says run in the background\n\ndocker-compose ps\n\nsimilar info as docker container ls but presented slightly differently\n\ndocker-compose logs -f\n\nsince containers running in the background -f needed to logs of the container activity\nIt‚Äôs a realtime log of all containers running in the project, so will require ctrl+c to exit\n\ndocker-compose restart\n\nrestarts all containers\n¬†can also just specify one container\n\neg docker-compose restart redis\n\n\ndocker-compose exec web\n\nfor a running container, you can execute commands inside the container from the outside\n\ndocker-compose exec web ls -la\n\nshows file contents inside container\n\n\nopening a shell inside the container\n\ndocker-compose exec web sh\n\nno -it flag necessary like for docker container exec (see running-a-container section)\n\n\nexit\n\nexits the shell\n\n\ndocker-compose run &lt;service&gt; &lt;command&gt;\n\nallows you to instantly run the container, execute a command inside the container, and exit the shell\nequivalent to the command sequence: up ‚Äì&gt; exec ‚Äì&gt; exit\nexample: docker-compose run redis redis-server ‚Äìversion\n\ndocker-compose rm\n\ndeletes all containers (only stopped ones I assume)\nThese will also be removed using the docker system prune from the docker-utility-functions section",
    "crumbs": [
      "Docker",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Docker, Fundamentals</span>"
    ]
  },
  {
    "objectID": "qmd/docker-misc.html",
    "href": "qmd/docker-misc.html",
    "title": "13¬† Misc",
    "section": "",
    "text": "13.1 BuildKit",
    "crumbs": [
      "Docker",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Misc</span>"
    ]
  },
  {
    "objectID": "qmd/docker-misc.html#buildkit",
    "href": "qmd/docker-misc.html#buildkit",
    "title": "13¬† Misc",
    "section": "",
    "text": "Allows you to use external caching sources and build mounts to speed up image builds through caching (requires Docker version ‚â•18.09)\n\nAble to supply a previously built image in your registry where Docker will check the manifest of the image, and pull any layers that can be used as local cache.\n\nNotes from Fast Docker Builds With Caching\nMust have the environment variable, DOCKER_BUILDKIT=1\nExternal Cache\n\nDocs\nExample (single stage build)\n\n\nDOCKER_BUILDKIT=1 docker build \\\n¬† --cache-from my-repo.com/my-image \\\n¬† --build-arg BUILDKIT_INLINE_CACHE=1 \\\n\nUse --build-arg BUILDKIT_INLINE_CACHE=1 and --cache-from arguments when building the image\n\n‚Äúmy-repo.com/my-image‚Äù is the url of the image you want Docker to pull dependencies (aka layers) that can be used as a local cache\n\nExample: Multi-Stage\nexport DOCKER_BUILDKIT=1\nIMAGE=my-repo.com/my-image\n\n# Build image for the build stage\ndocker build \\\n¬† --target build-stage \\\n¬† --cache-from \"$[{IMAGE}]{style='color: #990000'}:build-stage\" \\\n¬† --tag \"$[{IMAGE}]{style='color: #990000'}:build-stage\" \\\n¬† --build-arg BUILDKIT_INLINE_CACHE=1 \\\n¬† .\n\n# Build the final image\ndocker build \\\n¬† --cache-from \"${IMAGE_NAME}:build-stage\" \\\n¬† --cache-from \"${IMAGE_NAME}:latest\" \\\n¬† --tag \"${IMAGE_NAME}:latest\" \\\n¬† --build-arg BUILDKIT_INLINE_CACHE=1 \\\n¬† .\n\n# Push the build-stage image too so that it can be reused for cache\ndocker push \"${IMAGE_NAME}:build-stage\"\ndocker push \"${IMAGE_NAME}:latest\"\nThis shell script that gets referenced in the docker file (another example in this note; search for ‚Äúshell script‚Äù)\n‚Äúexport‚Äù creates the environment variable; IMAGE is a variable storing the URL of the externally cached image\n--target in the first build command to stop at the build-stage stage, and that\nThe second build command referenced both the build-stage and latest images as cache sources\nBuild Mounts\n\nThis type of caching is only available:\n\nlocally and cannot be reused across machines\nduring a single RUN instruction, so you need to either:\n\ncopy the files to a different location in the image before the RUN instruction finishes (e.g., with cp) or\nCOPY the cache directory from another image, e.g., a previously built build-stage image.\n\n\nSee the article for an example\n\nCredentials\n\nSteps\n\nPrepare an auth.toml file with your credentials\n\nExample (poetry LIB credentials for installing deps from a private repo)\n[http-basic]\n[http-basic.my_repo]\nusername = \"my_username\"\npassword = \"my_ephemeral_password\"\n\n\n\nPlace it outside of your Docker context or exclude it in .dockerignore (the cache would still get invalidated otherwise).\nUpdate your Dockerfile to include ‚Äú# syntax=docker/dockerfile:1.3‚Äù as the very first line, and\nAdjust install commands that require the credentials (e.g.¬†poetry install command becomes:)\n\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† RUN --mount=type=secret,id=auth,target=/root/.config/pypoetry/auth.toml \\\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† poetry install --no-dev --no-interaction --remove-untracked\n\nbuild the image with docker build --secret id=auth,src=auth.toml ...",
    "crumbs": [
      "Docker",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Misc</span>"
    ]
  },
  {
    "objectID": "qmd/docker-misc.html#multi-stage-dockerfile",
    "href": "qmd/docker-misc.html#multi-stage-dockerfile",
    "title": "13¬† Misc",
    "section": "13.2 Multi-Stage Dockerfile",
    "text": "13.2 Multi-Stage Dockerfile\n\nMisc\n\nWith multi-stage builds, you compile and build everything in an initial stage, and then, in a separate stage, you copy over just the necessary artifacts. This results in a much leaner, more efficient final image. It‚Äôs not only good practice for keeping image sizes down, but it also means quicker deployments and reduced storage costs.\n\nThe bigger the image size, the longer the run time and the higher the runtime cost\nGoing to multi-stage can drastically reduce image size (e.g.¬†65%)\n\nYou should consider moving your build to a multi-stage build when the build-required dependencies are no longer needed after the build is completed\nNotes from\n\nUsing multi-stage builds to make your docker image 10x smaller\nIntroduction to Multi-Stage Image Build for Python\n\n\nProcess\n\n\nFirst Build - Installing the required decencies and building the binaries. This image is also called the builder image\nSecond Build - Starting from a new base image and copying from the builder image binaries applications\n\nAttributes or settings from the first build are not inherited unless using specific commands or arguments such as the -- from=builder argument.\n\n\nExample: 2 Stages\nFROM ubuntu:20.04 AS final\nFROM ubuntu:20.04 as build\n# BUNDLE LAYERS\nRUN apt-get update -y && apt install -y --no-install-recommends \\\n  curl \\\n  osmium-tool \\\n&& rm -rf /var/lib/apt/lists/*\nRUN mkdir /osmfiles \\\n&& mkdir /merged \\\n&& curl http://download.geofabrik.de/europe/monaco-latest.osm.pbf -o /osmfiles/monaco.osm.pbf \\\n&& curl http://download.geofabrik.de/europe/andorra-latest.osm.pbf -o /osmfiles/andorra.osm.pbf \\\n&& osmium merge /osmfiles/monaco.osm.pbf /osmfiles/andorra.osm.pbf -o /merged/merged.osm.pbf\n\nFROM final\nRUN mkdir /merged\nCOPY --from=build /merged /merged\n\nStage 1: build\n\nStarts at FROM ubuntu:20.04 as build\nDownloads a couple geospatial files, then merges them and stores them in the ‚Äúmerged‚Äù folder\n\nStage 2: final\n\nStarts at FROM final\nCreates a ‚Äúmerged‚Äù dir and copies merged file from stage 1 (build) to the ‚Äúmerged‚Äù dir\nThe curl and osmium-tool dependencies that are installed in Stage 1 are not included in Stage 2 which reduces the size of the final image.\n\nI‚Äôm not sure if FROM ubuntu:20.04 AS final being the first line (instead of replacing the FROM final line) is necessary or not. It looks kind of redundant.\nIf a slimmer ubuntu image is used in the last stage, the size of the image can reduced further\n\nExample: Python Development Environment\n\nDockerfile\n# Stage I\nFROM python:3.10-slim AS builder\n\nARG VENV_NAME=\"my_project\"\nENV VENV_NAME=$VENV_NAME\n\nRUN mkdir requirements\nCOPY install_requirements.sh requirements/\n\nCOPY requirements.txt requirements/\nRUN bash ./requirements/install_requirements.sh $VENV_NAME\n\n# Stage II\nFROM python:3.10-slim\n\nARG VENV_NAME=\"my_project\"\nENV VENV_NAME=$VENV_NAME\n\nCOPY --from=builder /opt/$VENV_NAME /opt/$VENV_NAME\nRUN echo \"source /opt/$VENV_NAME/bin/activate\" &gt;&gt; ~/.bashrc\n\nSee link to article in Note From section for more details\nStage I:\n\nDefine the first stage as the builder by adding AS builder argument to the FROM command.\nImport the python:3.10 image as the base image\nUse an argument variable to set the virtual environment name\nCreate a local folder and copy the helper files ‚Äî install_requirements.shand requirements.txt\n\nrequirements.txt - File defines the Python environment required libraries.\ninstall_requirements.sh - Bash script sets the virtual environment and installs the libraries from the requirements.txt file.\n\nSet the virtual environment and install the required libraries\n\nStage II:\n\npython:3.10-slim is the base image\nCopy the Python virtual environment from the builder image using the COPY command with the ‚Äî from=builder argument.\n\nNeed to update the .bashrc file (See CLI, Linux &gt;&gt; Misc) and set it up again as the default virtual environment. Alternatively, you can copy the .bashrc file from the builder as the install_requirements.sh file did it on the builder image in Stage I.\n\n\n\nBuild Image\nbuild_docker.sh Dockerfile.multi-stage rkrispin/python-multi-stage:3.10\n\nbuild_docker.sh - Bash script that runs docker build command. Also has parameters for you to set the dockerfile name and the image name.\nBuilds the Dockerfile.multi-stage Dockerfile and names it as rkrispin/python-multi-stage:3.10",
    "crumbs": [
      "Docker",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Misc</span>"
    ]
  },
  {
    "objectID": "qmd/docker-misc.html#optimizations",
    "href": "qmd/docker-misc.html#optimizations",
    "title": "13¬† Misc",
    "section": "13.3 Optimizations",
    "text": "13.3 Optimizations\n\nResources\n\nVideo: Dockerfile: From Start to Optimized (DockerCon 2023) - Best practices for setting up Dockerfile and optimizing the build.\n\nOptimize the Dockerfile\nMulti-stage build\nSetting testing\nBuilding for multiple platforms",
    "crumbs": [
      "Docker",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Misc</span>"
    ]
  },
  {
    "objectID": "qmd/python-classes.html",
    "href": "qmd/python-classes.html",
    "title": "Classes",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Python",
      "Classes"
    ]
  },
  {
    "objectID": "qmd/python-classes.html#sec-py-class-misc",
    "href": "qmd/python-classes.html#sec-py-class-misc",
    "title": "Classes",
    "section": "",
    "text": "When to use classes:\n\nWhen you have a group of functions and they have many of the same arguments, this indicates a class might be helpful. Also, if one or more of the functions is used in the other functions, this is also an indication that creating a class would be better.\n\nSee Python, Snippets &gt;&gt; Refactor a group of functions into a class\n\nWhen you have code and data that go together and need to keep track of the current state\n\ne.g.¬†managing a bunch of students and grades\n\nWhen you see hierarchies, using classes leads to better code organization, less duplication, and reusable code.\n\nYou can make a single change in the base class (parent) and all child classes pick up the change\nExample: Report class\n\nYou can have a base class with shared attributes like report name, location and rows. But when you go into specifics like formats (xml, json, html), you could override a generate_report method in the subclass.\n\n\nEncapsulation\n\nWhen you want to separate external and internal interfaces in order to (ostensibly) hide internal code from the user.\nKeeps excess complexity from the user\n\n\nBest Practices\n\nUse camel case for class names\nUse snake case for methods and attributes\nAlways use self as the first argument of a method\nWrite docstrings for your classes so that your code is more understandable to potential collaborators and future you.\n\nCreate a Class that allows method chaining\n\n\nreturn self is what allows the chaining to happen",
    "crumbs": [
      "Python",
      "Classes"
    ]
  },
  {
    "objectID": "qmd/python-classes.html#sec-py-class-terms",
    "href": "qmd/python-classes.html#sec-py-class-terms",
    "title": "Classes",
    "section": "Terms",
    "text": "Terms\n\nClass inheritance - mechanism by which one class takes on the attributes and methods of another\nclass Employee:\n¬† ¬† def __init__(self, name, salary=0):\n¬† ¬† ¬† ¬† self.name = name\n¬† ¬† ¬† ¬† self.salary = salary\n__init__() is a constructor method. It assigns (or initializes) attributes that every object (aka instance) for this class must have.\nself is the 1st argument in any method definition. It refers to a particular instance.\nself.salary¬† = salary creates an attribute called¬†salary¬†and assigns to it the value of the¬†salary¬†parameter (default set to 0)\nClass attributes are attributes that have the same value for all class instances.\n\nAccessing a class attribute\n# access first employee's name attribute\ne1.name\n# access second employee's salary attribute\ne2.salary\n\nInstance Attributes - Values for these attribute depend on the instance (e.g.¬†they vary depending on each employee)\nInstantiate - Creating a new object from a class\ne1 = Employee(\"yyy\", 5000)¬† # name, salary\ne2 = Employee(\"zzz\", 8000)",
    "crumbs": [
      "Python",
      "Classes"
    ]
  },
  {
    "objectID": "qmd/python-classes.html#sec-py-class-meth",
    "href": "qmd/python-classes.html#sec-py-class-meth",
    "title": "Classes",
    "section": "Methods",
    "text": "Methods\n\nMisc\n\nIn most classes, best practice to at least include __init__ and __repr__ methods\n\n\n\nInstance Method\n\nFunctions that can only be called by an object from this class\n\nSimilar to regular functions with the difference of having ‚Äúself‚Äù as the first parameter\n\nclass Employee:\n¬† ¬† def __init__(self, name, salary=0):\n¬† ¬† ¬† ¬† self.name = name\n¬† ¬† ¬† ¬† self.salary = salary\n\n#Instance method\n¬† ¬† def give_raise(self, amount):\n¬† ¬† ¬† ¬† self.salary += amount\n¬† ¬† ¬† ¬† return f\"{self.name} has been given a {amount} raise\"\n\n# calling an instance method\n# instantiate object first\nobject = MyClass()¬†\nobject.method()\n\n\n\nDunder Methods\n\naka Magic or Special Methods\nResources\n\nEvery dunder method in Python\n\nAren‚Äôt meant to be called, usually invoked by an operation\n\nExamples\n\n__add__ invoked by myclass() + myclass()\n__str__ invoked by str(myclass())\n\nExample\nclass Address:\n¬† ¬† def __init__(self, street, city, state, zipcode, street2=''):\n¬† ¬† ¬† ¬† self.street = street\n¬† ¬† ¬† ¬† self.street2 = street2\n¬† ¬† ¬† ¬† self.city = city\n¬† ¬† ¬† ¬† self.state = state\n¬† ¬† ¬† ¬† self.zipcode = zipcode\n¬† ¬† def __str__(self):\n¬† ¬† ¬† ¬† lines = [self.street]\n¬† ¬† ¬† ¬† if self.street2:\n¬† ¬† ¬† ¬† ¬† ¬† lines.append(self.street2)\n¬† ¬† ¬† ¬† lines.append(f'{self.city}, {self.state} {self.zipcode}')\n¬† ¬† ¬† ¬† return '\\n'.join(lines)\n\n&gt;&gt;&gt; address = Address('55 Main St.', 'Concord', 'NH', '03301')\n&gt;&gt;&gt; print(address)\n55 Main St.\nConcord, NH 03301\n\nCan be an instance or class type of method\nfrom datetime import datetime, timedelta\nfrom typing import Iterable\nfrom math import ceil\nclass DateTimeRange:\n¬† ¬† def __init__(self, start: datetime, end_:datetime, step:timedelta = timedelta(seconds=1)):\n¬† ¬† ¬† ¬† self._start = start\n¬† ¬† ¬† ¬† self._end = end_\n¬† ¬† ¬† ¬† self._step = step\n\n¬† ¬† def __iter__(self) -&gt; Iterable[datetime]:\n¬† ¬† ¬† ¬† point = self._start\n¬† ¬† ¬† ¬† while point &lt; self._end:\n¬† ¬† ¬† ¬† ¬† ¬† yield point\n¬† ¬† ¬† ¬† ¬† ¬† point += self._step\n\n¬† ¬† def __len__(self) -&gt; int:\n¬† ¬† ¬† ¬† ¬† ¬† return ceil((self._end - self._start) / self._step)\n\n¬† ¬† def __contains__(self, item: datetime) -&gt; bool:\n¬† ¬† ¬† ¬† ¬† ¬† mod = divmod(item - self._start, self._step)\n¬† ¬† ¬† ¬† ¬† ¬† return item &gt;= self._start and item &lt; self._end and mod[1] == timedelta(0)\n\n¬† ¬† def __getitem__(self, item: int) -&gt; datetime:\n¬† ¬† ¬† ¬† n_steps = item if item &gt;= 0 else len(self) + item\n¬† ¬† ¬† ¬† return_value = self._start + n_steps * self._step\n¬† ¬† ¬† ¬† if return_value not in self:\n¬† ¬† ¬† ¬† ¬† ¬† raise IndexError()\n¬† ¬† ¬† ¬† return return_value¬†\n\n¬† ¬† def __str__(self):\n¬† ¬† ¬† ¬† return f\"Datetime Range [{self._start}, {self._end}) with step {self._step}\"\n\nClass DateTimeRange has methods that allows you to treat a date-range object like a list\n\nJust for illustration. Think methods in pandas can do this stuff\n\n__iter__ method - generator function that creates one element at a time, yields it to the caller, and allows the caller to process it\n\nExample creates datetime ranges instead of numeric ranges\n\n__len__ - find out the number of elements that are part of your range\n__getitem__- uses indexing syntax to retrieve entries from your objects\n__contains__- checks if an element is part of your range. T/F\n\ndivmod returns quotient and remainder.\n\nUsing these magic methods\nmy_range = DateTimeRange(datetime(2021,1,1), datetime(2021,12,1), timedelta(days=12)) #instantiate\nprint(my_range)¬† ¬† ¬† ¬† ¬† ¬† ¬† # __init__ or maybe __str__\nfor r in my_range:¬† ¬† ¬† ¬† ¬† # __iter__\n¬† ¬† do_something(r)\nlen(my_range)¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† # __len__\nmy_range[-2] in my_range¬† ¬† # __getitem__ (neg indexing), __contains__\n\nOthers\n\n__repr__ - Creates a string representation of the class object\n\nExample\nclass Person:\n    def __init__(self, name, age):\n        self.name = name\n        self.age = age\n\n    def __repr__(self):\n        return f\"Person(name='{self.name}', age={self.age})\"\n\nperson = Person(\"John\", 25)\nprint(person)\n#&gt; Person(name='John', age=25)\n\n__eq__ - Provides a method for comparing two class objects by their values.\n\nExample\nclass Person:\n  def __init__(self, age):\n    self.age = age\n\n  def __eq__(self, other):\n    return self.age == other.age\n\nalice = Person(18)\nbob = Person(19)\ncarl = Person(18)\n\nprint(alice == bob)\n#&gt; False\n\nprint(alice == carl)\n#&gt; True",
    "crumbs": [
      "Python",
      "Classes"
    ]
  },
  {
    "objectID": "qmd/python-classes.html#sec-py-class-inher",
    "href": "qmd/python-classes.html#sec-py-class-inher",
    "title": "Classes",
    "section": "Inheritance",
    "text": "Inheritance\n\n\nSome notes from this much more detailed example (article)\n\nShows how to combine different scripts¬† (aka modules) and diagram the hierarchies\nSome good debugging too\n\nRunning &lt;class&gt;.__mro__will show you the order of inheritance.\n\n\nInheritance models what is called an ‚Äúis a‚Äù relationship. This means that when you have a Derived (aka subclass, child) class that inherits from a Base (aka super, parent) class, you created a relationship where Derived is a specialized version of Base.\nChild classes inherit all of their parent‚Äôs attributes and methods, but they can also define their own attributes and methods.\nCan override or extend parent class attributes and methods\nclass Manager(Employee):\n¬† ¬† pass\n\nm1 = Manager(\"aaa\", 13000)\n\nManager is the child class and Employee is the parent class (see top)\nChild classes don‚Äôt require a constructor method for an object to be created\n\nExtending the instance attributes of the parent class\nclass Manager(Employee):\n¬† ¬† def __init__(self, name, salary=0, department):\n¬† ¬† ¬† ¬† Employee.__init__(self, name, salary=0)\n¬† ¬† ¬† ¬† self.department = department\n\nContructor method for the child class with the new attribute, ‚Äúdepartment,‚Äù in the arguments.\nParent class (Employee) constructor method is called and a new attribute, department, is defined.\n\n\n\nsuper()\n\nAlternative way of extending instance attributes through inheritance\nExample\nclass Rectangle:\n¬† ¬† def __init__(self, length, width):\n¬† ¬† ¬† ¬† self.length = length\n¬† ¬† ¬† ¬† self.width = width\n¬† ¬† def area(self):\n¬† ¬† ¬† ¬† return self.length * self.width\n¬† ¬† def perimeter(self):\n¬† ¬† ¬† ¬† return 2 * self.length + 2 * self.width\nclass Square(Rectangle):\n¬† ¬† def __init__(self, length):\n¬† ¬† ¬† ¬† super().__init__(length, length)\nclass Cube(Square):\n¬† ¬† def surface_area(self):\n¬† ¬† ¬† ¬† face_area = super().area()\n¬† ¬† ¬† ¬† return face_area * 6\n¬† ¬† def volume(self):\n¬† ¬† ¬† ¬† face_area = super().area()\n¬† ¬† ¬† ¬† return face_area * self.length\n\n&gt;&gt;&gt; cube = Cube(3)\n&gt;&gt;&gt; cube.surface_area()\n54\n&gt;&gt;&gt; cube.volume()\n27\n\nClass Cube inherits from Square and extends the functionality of .area() (inherited from the Rectangle class through Square) to calculate the surface area and volume of a Cube instance Also notice that the Cube class definition does not have an .__init__(). Because Cube inherits from Square and .__init__() doesn‚Äôt really do anything differently for Cube than it already does for Square, you can skip defining it, and the .__init__() of the other child class (Square) will be called automatically.\nsuper(Square, self).__init__(length, length) is equivalent to calling super without parameters (see above example)\nUsing super(Square, self).area() in class Cube. Setting the 1st parameter to Square instead of Cube causes super() to start searching for a matching method (in this case, .area()) at one level above Square in the instance hierarchy, in this case Rectangle.\n\nIf Square had an .area method, but you wanted to use Rectangle‚Äôs instead, this would be a way to do that.\n\nNote another difference between using super() and using the class name (e.g.¬†the first example) ‚Äî ‚Äúself‚Äù is NOT one of the args in super()\nExample: Child class of two separate hierarchies\n# Super class\nclass Rectangle:\n¬† ¬† def __init__(self, length, width, **kwargs):\n¬† ¬† ¬† ¬† self.length = length\n¬† ¬† ¬† ¬† self.width = width\n¬† ¬† ¬† ¬† super().__init__(**kwargs)\n¬† ¬† def area(self):\n¬† ¬† ¬† ¬† return self.length * self.width\n¬† ¬† def perimeter(self):\n¬† ¬† ¬† ¬† return 2 * self.length + 2 * self.width\n# Child class: Square class inherits from the Rectangle class\nclass Square(Rectangle):\n¬† ¬† def __init__(self, length, **kwargs):\n¬† ¬† ¬† ¬† super().__init__(length=length, width=length, **kwargs)\n# Child class: Cube class inherits from Square and also from Rectangle classes\nclass Cube(Square):\n¬† ¬† def surface_area(self):\n¬† ¬† ¬† ¬† face_area = super().area()\n¬† ¬† ¬† ¬† return face_area * 6\n¬† ¬† def volume(self):\n¬† ¬† ¬† ¬† face_area = super().area()\n¬† ¬† ¬† ¬† return face_area * self.length\n\n# Class (separate)\nclass Triangle:¬†\n¬† ¬† def __init__(self, base, height, **kwargs):¬†\n¬† ¬† ¬† ¬† self.base = base¬†\n¬† ¬† ¬† ¬† self.height = height¬†\n¬† ¬† ¬† ¬† super().__init__(**kwargs)¬†\n¬† ¬† def tri_area(self):¬†\n¬† ¬† ¬† ¬† return 0.5 * self.base * self.height\n\n# Inherits from a child class (and super class) and a class\nclass RightPyramid(Square, Triangle):\n¬† ¬† def __init__(self, base, slant_height, **kwargs):\n¬† ¬† ¬† ¬† self.base = base\n¬† ¬† ¬† ¬† self.slant_height = slant_height\n¬† ¬† ¬† ¬† kwargs[\"height\"] = slant_height\n¬† ¬† ¬† ¬† kwargs[\"length\"] = base\n¬† ¬† ¬† ¬† super().__init__(base=base, **kwargs)\n¬† ¬† def area(self):\n¬† ¬† ¬† ¬† base_area = super().area()\n¬† ¬† ¬† ¬† perimeter = super().perimeter()\n¬† ¬† ¬† ¬† return 0.5 * perimeter * self.slant_height + base_area\n¬† ¬† def area_2(self):\n¬† ¬† ¬† ¬† base_area = super().area()\n¬† ¬† ¬† ¬† triangle_area = super().tri_area()\n¬† ¬† ¬† ¬† return triangle_area * 4 + base_area\n\n&gt;&gt;&gt; pyramid = RightPyramid(base=2, slant_height=4)\n&gt;&gt;&gt; pyramid.area()\n20.0\n&gt;&gt;&gt; pyramid.area_2()\n20.0\n\nRightPyramid inherits from a child class (Square) and a class Triangle\n\nSince Square inherits from Rectangle, so does RightPyramid\nTriangle is a separate class not part of any hierarchy\n\nIf there‚Äôs an .area method in either of the classes, then super will search hierarchy of the class listed first (Square), then the hierarchy of the class listed second (Triangle)\n\nBest practice to make sure each class has different method names.\n\nEach class with an __init__ constructor gets a super().__init__() and **kwargs added to its args\n\nWhich is every class sans Cube. If Cube was inherited by a class, I think it would require an __init__ constructor and the super().__init__(**kwargs) expression.\nWithout doing this, calling .area_2() in RightPyramid will give us an AttributeError since .base and .height don‚Äôt have any values. (don‚Äôt completely understand this explanation)\n\nkwarg flow through super().__init__()\n\nIn RightPyramid __init__,¬† slant_height and base values are assigned to height and length keys in the kwargs dict\nsuper() passes base and the kwargs dict up to Square and Triangle\n\nTriangle uses height from kwargs and base\nSquare uses length from kwargs\n\nSquare passes length to Rectangle as values for both width and length\n\n\nAll classes now have the argument values necessary for their functions to work.\n\nNow RightPyramid can call those other classes‚Äô methods (e.g.¬†.area and .perimeter from Rectangle and tri_area from Triangle)\n\nI believe since every class has **kwargs arg in their super().__init__, each has every value in the kwarg dict even if they don‚Äôt need it.\n\nSo probably possible to add functions to those classes that would use those values\n\n\n\nExample: Using super with method other than __init__\nclass SalaryPolicy:¬†\n¬† ¬† def __init__(self, weekly_salary):¬†\n¬† ¬† ¬† ¬† self.weekly_salary = weekly_salary¬†\n¬† ¬† def calculate_payroll(self):¬†\n¬† ¬† ¬† ¬† return self.weekly_salary\n\nclass CommissionPolicy(SalaryPolicy):¬†\n¬† ¬† def __init__(self, weekly_salary, commission):¬†\n¬† ¬† ¬† ¬† super().__init__(weekly_salary)¬†\n¬† ¬† ¬† ¬† self.commission = commission¬†\n¬† ¬† def calculate_payroll(self):¬†\n¬† ¬† ¬† ¬† fixed = super().calculate_payroll()¬†\n¬† ¬† ¬† ¬† return fixed + self.commission\n\nIn CommissionPolicy‚Äôs calculate_payroll, super() accesses SalaryPolicy‚Äôs calculate_payroll method to get the weekly_salary value\n\n\n\n\n\nDiamond Problem\n\n\nAppears when you‚Äôre using multiple inheritance and deriving from two classes that have a common base class.\n\nThis can cause the wrong version of a method to be called.\ne.g.¬†TemporarySecretary uses multiple inheritance to derive from two classes that ultimately also derive from Employee. This causes two paths to reach the Employee base class, which is something you want to avoid in your designs.\n\n\n\n\nMixin Class\n\nOperates the same as Inheritance, but since it only provides simple behavior(s), it is easy to reuse with other classes without causing problems\nExample: Take certain class attributes and create a dict\n# In representations.py\nclass AsDictionaryMixin:\n¬† ¬† def to_dict(self):\n¬† ¬† ¬† ¬† return {\n¬† ¬† ¬† ¬† ¬† ¬† prop: self._represent(value)\n¬† ¬† ¬† ¬† ¬† ¬† for prop, value in self.__dict__.items()\n¬† ¬† ¬† ¬† ¬† ¬† if not self._is_internal(prop)\n¬† ¬† ¬† ¬† }\n¬† ¬† def _represent(self, value):\n¬† ¬† ¬† ¬† if isinstance(value, object):\n¬† ¬† ¬† ¬† ¬† ¬† if hasattr(value, 'to_dict'):\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† return value.to_dict()\n¬† ¬† ¬† ¬† ¬† ¬† else:\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† return str(value)\n¬† ¬† ¬† ¬† else:\n¬† ¬† ¬† ¬† ¬† ¬† return value\n¬† ¬† def _is_internal(self, prop):\n¬† ¬† ¬† ¬† return prop.startswith('_')\n\nto_dict is a dictionary comprehension\n\nmydict = {key:val for key, val in dict}\nReturns a dict with key:value (e.g.¬†property (aka attribute):value) pairs from a class‚Äôs __dict__ if the property (prop) doesn‚Äôt have an underscore\n\n_represent makes sure the ‚Äúvalue‚Äù is a value and not object\n_is_interna1 checks whether the attribute has an underscore in the name\n\nApply the Mixin class to any class the same way as using Inheritance\nclass Employee(AsDictionaryMixin):\n¬† ¬† def __init__(self, id, name, address, role, payroll):\n¬† ¬† ¬† ¬† self.id = id\n¬† ¬† ¬† ¬† self.name = name\n¬† ¬† ¬† ¬† self.address = address\n¬† ¬† ¬† ¬† self._role = role\n¬† ¬† ¬† ¬† self._payroll = payroll\n\nAsDicitionaryMixin is used as an arg to the class\n‚Äúself._role‚Äù and ‚Äúself._payroll‚Äù have underscores which tells to_dict not to include them in the resulting dictionary\nNot important to using a mixin class but note that ‚Äúaddress‚Äù is from the Address class via composition (see below). Therefore the Address class would also need to inherit AsDictionaryMixin for this to work\n\nUtilize\nimport json\n\ndef print_dict(d):\n¬† ¬† print(json.dumps(d, indent=2))\n\nfor employee in EmployeeDatabase().employees:\n¬† ¬† print_dict(employee.to_dict())\n\nprint_dict takes the dict output of employee._to_dict and converts it to a json format",
    "crumbs": [
      "Python",
      "Classes"
    ]
  },
  {
    "objectID": "qmd/python-classes.html#sec-py-class-comp",
    "href": "qmd/python-classes.html#sec-py-class-comp",
    "title": "Classes",
    "section": "Composition",
    "text": "Composition\n\n\nNotes from Inheritance and Composition: A Python OOP Guide\nComposition models a ‚Äúhas a‚Äù relationship. In composition, a class known as composite contains an object of another class known to as component.\nComposition design is typically more flexible than inheritance and is preferable to Inheritance\n\nPrevents ‚Äúclass explosion‚Äù\n\nFor complex projects, too many classes can lead to conflicts and errors because of the inevitable complex network of classes that are connected to each other.\nYou change your program‚Äôs behavior by providing new components that implement those behaviors instead of adding new classes to your hierarchy.\n\nOnly loose class connections in composition\n\nChanges to the component class rarely affect the composite class, and changes to the composite class never affect the component class\n\n\ntl;dr\n\nClasses are written in different py scripts and imported as ‚Äúmodules‚Äù in another script.\nAttribute(s) from a component class (e.g.¬†Address) are used in the composite class (e.g.¬†Employee)\nThen a composite class attribute object is assigned to the instantiated composite class‚Äôs empty attribute\n\nThis is the magic. One class‚Äôs attribute can be used as input into another class‚Äôs attribute without being tightly coupled to that other class (aka inheritance).\n\nThat input isn‚Äôt a value. It‚Äôs class type object.\nSee Utilize code block below\n\n\n\n\n\nComposition Through __init__ Attributes\n\nExample:\n# In contacts.py\n# Component class\nclass Address:\n¬† ¬† def __init__(self, street, city, state, zipcode, street2=''):\n¬† ¬† ¬† ¬† self.street = street\n¬† ¬† ¬† ¬† self.street2 = street2\n¬† ¬† ¬† ¬† self.city = city\n¬† ¬† ¬† ¬† self.state = state\n¬† ¬† ¬† ¬† self.zipcode = zipcode\n\n# In employees.py\n# Composite class\nclass Employee:\n¬† ¬† def __init__(self, id, name):\n¬† ¬† ¬† ¬† self.id = id\n¬† ¬† ¬† ¬† self.name = name\n¬† ¬† ¬† ¬† self.address = None\n\n# ManagerRole and SalaryPolicy are classes from different modules\nclass Manager(Employee, ManagerRole, SalaryPolicy):\n¬† ¬† def __init__(self, id, name, weekly_salary):\n¬† ¬† ¬† ¬† SalaryPolicy.__init__(self, weekly_salary)\n¬† ¬† ¬† ¬† super().__init__(id, name)\n\nYou would import these two modules into a third script and do stuff (see next code block)\nYou initialize the Address.address attribute to ‚ÄúNone‚Äù for now to make it optional, but by doing that, you can now assign an Address to an Employee.\n\ni.e.¬†the attributes of an Address instance from its __init__ are now available to be assigned to a Employee instance.\n\nManager is a child class of multiple other classes (inheritance) including Employee and therefore gets an .address attribute\n\nAside: ManagerRole doesn‚Äôt have an __init__ (i.e.¬†no attributes), so I‚Äôm not sure why super() is used here\n\nwhy not just use Employee.__init__?\nI would‚Äôve thought that ManagerRole would‚Äôve required the same inputs as Employee, so super() is used here to cover both at the same time.\n\nBut that‚Äôs not the case, MangerRole is there just for it‚Äôs method and not it‚Äôs attributes\n\nDoes being able to use ManageRole methods require super() (i.e.¬†necessary for Inheritance)?\n\n\nUtilize\nmanager = employees.Manager(1, 'Mary Poppins', 3000)\nmanager.address = contacts.Address(\n¬† ¬† '121 Admin Rd',¬†\n¬† ¬† 'Concord',¬†\n¬† ¬† 'NH',¬†\n¬† ¬† '03301'\n)\n# ... create other intances of different jobs in the company\n\n# guess this would be like a json\nemployees = [\n¬† ¬† manager,\n¬† ¬† secretary,\n¬† ¬† sales_guy,\n¬† ¬† factory_worker,\n¬† ¬† temporary_secretary,\n]\n\n# do work with the list class objs\nproductivity_system = productivity.ProductivitySystem()\nproductivity_system.track(employees, 40)\n\nThe Address class instance is assigned to the .address attribute of the Manager instance (which gets its .address attribute from Employee)\n\n\n\n\n\nComposition Through a Function Argument\n\nExample:\n# In hr.py\nclass PayrollSystem:\n¬† ¬† def calculate_payroll(self, employees):\n¬† ¬† ¬† ¬† print('Calculating Payroll')\n¬† ¬† ¬† ¬† print('===================')\n¬† ¬† ¬† ¬† for employee in employees:\n¬† ¬† ¬† ¬† ¬† ¬† print(f'Payroll for: {employee.id} - {employee.name}')\n¬† ¬† ¬† ¬† ¬† ¬† print(f'- Check amount: {employee.calculate_payroll()}')\n¬† ¬† ¬† ¬† ¬† ¬† if employee.address:\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† print('- Sent to:')\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† print(employee.address)\n¬† ¬† ¬† ¬† ¬† ¬† print('')\n\npayroll_system = hr.PayrollSystem()\npayroll_system.calculate_payroll(employees)\n\nThe input for calculate_payroll, employees, is a list of instantiated Employee class objects in the previous code chunk.\nName of the class is iterated and represents each instance\n\nHas attributes and methods available\n\n\n\n\n\nOther Module Classes Used as Attributes\n\nExample:\n# In employees.py\nfrom productivity import ProductivitySystem\nfrom hr import PayrollSystem\nfrom contacts import AddressBook\nclass EmployeeDatabase:\n¬† ¬† def __init__(self):\n¬† ¬† ¬† ¬† self._employees = [\n¬† ¬† ¬† ¬† ¬† ¬† {\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† 'id': 1,\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† 'name': 'Mary Poppins',\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† 'role': 'manager'\n¬† ¬† ¬† ¬† ¬† ¬† },\n¬† ¬† ¬† ¬† ¬† ¬† {\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† 'id': 2,\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† 'name': 'John Smith',\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† 'role': 'secretary'\n¬† ¬† ¬† ¬† ¬† ¬† }\n¬† ¬† ¬† ¬† ]\n¬† ¬† ¬† ¬† self.productivity = ProductivitySystem()\n¬† ¬† ¬† ¬† self.payroll = PayrollSystem()\n¬† ¬† ¬† ¬† self.employee_addresses = AddressBook()\n¬† ¬† @property\n¬† ¬† def employees(self):\n¬† ¬† ¬† ¬† return [self._create_employee(**data) for data in self._employees]\n¬† ¬† def _create_employee(self, id, name, role):\n¬† ¬† ¬† ¬† address = self.employee_addresses.get_employee_address(id)\n¬† ¬† ¬† ¬† employee_role = self.productivity.get_role(role)\n¬† ¬† ¬† ¬† payroll_policy = self.payroll.get_policy(id)\n¬† ¬† ¬† ¬† return Employee(id, name, address, employee_role, payroll_policy)\n\nProductivitySystem, PayrollSystem, AddressBook are classes imported from various modules\nAs attributes, these classes‚Äô methods are used in the _create_employee function\n\nReturn invokes the Employee class with values obtained by the various class methods\nEmployee class (not shown in this block) is already present in this module so it doesn‚Äôt have to be imported.",
    "crumbs": [
      "Python",
      "Classes"
    ]
  },
  {
    "objectID": "qmd/python-classes.html#sec-py-class-dec",
    "href": "qmd/python-classes.html#sec-py-class-dec",
    "title": "Classes",
    "section": "Decorators",
    "text": "Decorators\n\nThey can add additional features to a function\n\nUseful because you don‚Äôt have to refactor downstream code\n\nFunctions that take a function as input\n\nSee use cases throughout note and check bkmks\n\n@ is placed above a ‚Äúdecorated‚Äù function\n\nExample\n@decorator_1\ndef temperature():\nreturn temp\n\nCalling temperature() is actually calling decorator_1(temperature())\n\n\nExample: add additional features to a function\n\n\nAdds a timer to a function\n\nExample: Multiple decorators for a function\n@log_execution\n@timing\ndef my_function(x, y):\n¬† ¬† time.sleep(1)\n¬† ¬† return x + y\nSee Custom Examples for the ‚Äúlog_execution‚Äù decorator\n\n\nProperty\n\nArguments\nproperty(fget=None, fset=None, fdel=None, doc=None)\nBuilt-in decorator\nConstitutes a family of decorators\n\n@property: Declares the method as a property.\n\nfget - function to get value of the attribute\n\n@.setter: Specifies the setter method for a property that sets the value to a property.\n\nfset - function to set value of the attribute\nmust have the value argument that can be used to assign to the underlying private attribute\n\n@.deleter: Specifies the delete method as a property that deletes a property.\n\nfdel - function to delete the attribute\nmust have the value argument that can be used to assign to the underlying private attribute\n\n\nExample\n# Using @property decorator\nclass Celsius:\ndef __init__(self, temperature=0):\n¬† ¬† self._temperature = temperature\ndef to_fahrenheit(self):\n¬† ¬† return (self._temperature * 1.8) + 32\n\n\n# decorators\n# attribute getter\n@property\ndef temperature(self):\n¬† ¬† print(\"Getting value...\")\n¬† ¬† return self._temperature\n\n# also adds constraint to the temperature input\n@temperature.setter\ndef temperature(self, value):\n¬† ¬† print(\"Setting value...\")\n¬† ¬† if value &lt; -273.15:\n¬† ¬† ¬† ¬† raise ValueError(\"Temperature below -273 is not possible\")\n¬† ¬† self._temperature = value\n\n@temperature.deleter\ndef temperature(self, value):\n¬† ¬† print(\"Deleting value...\")\n¬† ¬† del self._temperature\n\n&gt;&gt; human = Celsius(37)\nSetting value...\n&gt;&gt; print(human.temperature)\nGetting value...\n37\n&gt;&gt; print(human.to_fahrenheit())\nGetting value...\n98.60000000000001\n&gt;&gt; del human.temperature\nDeleting value...\n&gt;&gt; coldest_thing = Celsius(-300)\nSetting value...\nTraceback (most recent call last):\nFile \"\", line 29, in\nFile \"\", line 4, in __init__\nFile \"\", line 18, in temperature\nValueError: Temperature below -273 is not possible\n\n.deleter didn‚Äôt work for me and neither did the conditional. Don‚Äôt my python version or what\n\n\n\n\nClass Method\n\nMethod that is bound to the class and not the object (aka instance) of the class.\nInstance attributes cannot be referred to with this method\nCan modify the class state that applies across all instances of the class\nUse Cases\n\nTo create new instances of the class without going trhough its normal __init__\nTo create a class instance that requires some async calls when instantiated, since __init__ cannot be async.\n\nStarts with a ‚Äúclassmethod‚Äù decorator\nclass MyClass:¬† ¬†\n@classmethod\ndef classmethod(cls):\n¬† ¬† return 'class method called', cls\nCalling a class method vs an instance method\n# calling a class method\n# no instantiation\nMyClass.classmethod()\n\n# calling an instance method\n# instantiates object first\nobject = MyClass()\nobject.method()\n\n\n\nStatic Method\n\nMethod bound to the class instance, not the class itself.\nDoes not take the class as a parameter.\nIt cannot access or modify the class at all.\nStarts with a ‚Äústaticmethod‚Äù decorator\nDoesn‚Äôt have any access to what the class is‚Äîit‚Äôs basically just a function, called syntactically like a method, but without access to the object and its internals (fields and other methods), which classmethod does have.\nSee SO thread for discussion on the differences between the static and class decorators and their uses\nclass Person:\n¬† def __init__(self, name, age):\n¬† ¬† self.name = name\n¬† ¬† self.age = age\n\n¬† @staticmethod\n¬† def isAdult(age):\n¬† ¬† return age &gt; 18\n\nisAdult(age) function doesn‚Äôt require the usual self argument, so it couldn‚Äôt reference the class even if it wanted to.\nMost often used as utility functions that are completely independent of a class‚Äôs state\nSee classmethod decorator for details on calling this method\n\n\n\n\nCustom Examples\n\nAlso see Code, Optimization &gt;&gt; Python &gt;&gt; Profile decorator\nUsing functools and decorators\nfrom functools import singledispatch\n\n@singledispatch\ndef process_data(data):\nraise NotImplementedError(f\"Type {type(data)} is unsupported\")\n\n@process_data.register\ndef process_dict(data: dict):\nprint(\"Dict is processed\")\n\n@process_data.register\ndef process_list(data: list):\nprint(\"List is processed\")\nMultiprocessing Function Execution Time Limiter\nimport multiprocessing\nfrom functools import wraps\n\nclass TimeExceededException(Exception):\n¬† ¬† pass\n## PART 1\n¬† ¬† def function_runner(*args, **kwargs):\n¬† ¬† ¬† ¬† \"\"\"Used as a wrapper function to handle\n¬† ¬† ¬† ¬† returning results on the multiprocessing side\"\"\"\n\n¬† ¬† ¬† ¬† send_end = kwargs.pop(\"__send_end\")\n¬† ¬† ¬† ¬† function = kwargs.pop(\"__function\")\n¬† ¬† ¬† ¬† try:\n¬† ¬† ¬† ¬† ¬† ¬† result = function(*args, **kwargs)\n¬† ¬† ¬† ¬† except Exception as e:\n¬† ¬† ¬† ¬† ¬† ¬† send_end.send(e)\n¬† ¬† ¬† ¬† ¬† ¬† return\n¬† ¬† ¬† ¬† send_end.send(result)\n\n¬† ¬† @parametrized\n¬† ¬† def run_with_timer(func, max_execution_time):\n¬† ¬† ¬† ¬† @wraps(func)\n¬† ¬† ¬† ¬† def wrapper(*args, **kwargs):\n¬† ¬† ¬† ¬† ¬† ¬† recv_end, send_end = multiprocessing.Pipe(False)\n¬† ¬† ¬† ¬† ¬† ¬† kwargs[\"__send_end\"] = send_end\n¬† ¬† ¬† ¬† ¬† ¬† kwargs[\"__function\"] = func\n\n¬† ¬† ¬† ¬† ¬† ¬† ## PART 2\n¬† ¬† ¬† ¬† ¬† ¬† p = multiprocessing.Process(target=function_runner, args=args, kwargs=kwargs)\n¬† ¬† ¬† ¬† ¬† ¬† p.start()\n¬† ¬† ¬† ¬† ¬† ¬† p.join(max_execution_time)\n¬† ¬† ¬† ¬† ¬† ¬† if p.is_alive():\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† p.terminate()\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† p.join()\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† raise TimeExceededException(\"Exceeded Execution Time\")\n¬† ¬† ¬† ¬† ¬† ¬† result = recv_end.recv()\n\n¬† ¬† ¬† ¬† ¬† ¬† if isinstance(result, Exception):\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† raise result\n\n¬† ¬† ¬† ¬† ¬† ¬† return result\n\n¬† ¬† ¬† ¬† return wrapper\n\nFrom Limiting Python Function Execution Time with a Parameterized Decorator via Multiprocessing\n\nRetry (e.g.¬†for an API)\nimport time\nfrom functools import wraps\n\ndef retry(max_tries=3, delay_seconds=1):\n¬† ¬† def decorator_retry(func):\n¬† ¬† ¬† ¬† @wraps(func)\n¬† ¬† ¬† ¬† def wrapper_retry(*args, **kwargs):\n¬† ¬† ¬† ¬† ¬† ¬† tries = 0\n¬† ¬† ¬† ¬† ¬† ¬† while tries &lt; max_tries:\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† try:\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† return func(*args, **kwargs)\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† except Exception as e:\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† tries += 1\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† if tries == max_tries:\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† raise e\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† time.sleep(delay_seconds)\n¬† ¬† ¬† ¬† return wrapper_retry\n¬† ¬† return decorator_retry\n\n@retry(max_tries=5, delay_seconds=2)\ndef call_dummy_api():\n¬† ¬† response = requests.get(\"https://jsonplaceholder.typicode.com/todos/1\")\n¬† ¬† return response\n\nTries to get an API response. If it fails, we retry the same task 5 times. Between each retry, we wait for 2 seconds.\n\nCache Function Results\ndef memoize(func):\n¬† ¬† cache = {}\n¬† ¬† def wrapper(*args):\n¬† ¬† ¬† ¬† if args in cache:\n¬† ¬† ¬† ¬† ¬† ¬† return cache[args]\n¬† ¬† ¬† ¬† else:\n¬† ¬† ¬† ¬† ¬† ¬† result = func(*args)\n¬† ¬† ¬† ¬† ¬† ¬† cache[args] = result\n¬† ¬† ¬† ¬† ¬† ¬† return result\n¬† ¬† return wrapper\n\n@memoize\ndef fibonacci(n):\n¬† ¬† if n &lt;= 1:\n¬† ¬† ¬† ¬† return n\n¬† ¬† else:\n¬† ¬† ¬† ¬† return fibonacci(n-1) + fibonacci(n-2)\n\nUses a dictionary, stores the function args, and returns values. When we execute this function, the decorated will check the dictionary for prior results. The actual function is called only when there‚Äôs no stored value before.\nUsing a dictionary to hold previous execution data is a straightforward approach. However, there is a more sophisticated way to store caching data. You can use an in-memory database, such as Redis.\n\nLogging (e.g.¬†ETL pipeline)\nimport logging\nimport functools\n\nlogging.basicConfig(level=logging.INFO)\n\ndef log_execution(func):\n¬† ¬† @functools.wraps(func)\n¬† ¬† def wrapper(*args, **kwargs):\n¬† ¬† ¬† ¬† logging.info(f\"Executing {func.__name__}\")\n¬† ¬† ¬† ¬† result = func(*args, **kwargs)\n¬† ¬† ¬† ¬† logging.info(f\"Finished executing {func.__name__}\")\n¬† ¬† ¬† ¬† return result\n¬† ¬† return wrapper\n\n@log_execution\ndef extract_data(source):\n¬† ¬† # extract data from source\n¬† ¬† data = ...\n¬† ¬† return data\n@log_execution\ndef transform_data(data):\n¬† ¬† # transform data\n¬† ¬† transformed_data = ...\n¬† ¬† return transformed_data\n@log_execution\ndef load_data(data, target):\n¬† ¬† # load data into target\n¬† ¬† ...\n\ndef main():\n¬† ¬† # extract data\n¬† ¬† data = extract_data(source)\n¬† ¬† # transform data\n¬† ¬† transformed_data = transform_data(data)\n¬† ¬† # load data\n¬† ¬† load_data(transformed_data, target)\n\noutput\nINFO:root:Executing extract_data\nINFO:root:Finished executing extract_data\nINFO:root:Executing transform_data\nINFO:root:Finished executing transform_data\nINFO:root:Executing load_data\nINFO:root:Finished executing load_data\n\nEmail Notification\nimport smtplib\nimport traceback\nfrom email.mime.text import MIMEText\ndef email_on_failure(sender_email, password, recipient_email):\n¬† ¬† def decorator(func):\n¬† ¬† ¬† ¬† def wrapper(*args, **kwargs):\n¬† ¬† ¬† ¬† ¬† ¬† try:\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† return func(*args, **kwargs)\n¬† ¬† ¬† ¬† ¬† ¬† except Exception as e:\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† # format the error message and traceback\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† err_msg = f\"Error: {str(e)}\\n\\nTraceback:\\n{traceback.format_exc()}\"\n\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† # create the email message\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† message = MIMEText(err_msg)\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† message['Subject'] = f\"{func.__name__} failed\"\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† message['From'] = sender_email\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† message['To'] = recipient_email\n\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† # send the email\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† with smtplib.SMTP_SSL('smtp.gmail.com', 465) as smtp:\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† smtp.login(sender_email, password)\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† smtp.sendmail(sender_email, recipient_email, message.as_string())\n\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† # re-raise the exception\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† raise\n\n¬† ¬† ¬† ¬† return wrapper\n\n¬† ¬† return decorator\n\n@email_on_failure(sender_email='your_email@gmail.com', password='your_password', recipient_email='recipient_email@gmail.com')\ndef my_function():\n¬† ¬† # code that might fail",
    "crumbs": [
      "Python",
      "Classes"
    ]
  },
  {
    "objectID": "qmd/python-classes.html#sec-py-class-opt",
    "href": "qmd/python-classes.html#sec-py-class-opt",
    "title": "Classes",
    "section": "Optimizations",
    "text": "Optimizations\n\nMisc\n\nNotes from How to Write Memory-Efficient Classes in Python\n\nUse __slots__ when creating large numbers of instances\n\n\nBy default, Python classes store their instance attributes in a private dictionary (__dict__), which dictionary allows you to add, modify, or delete the class attributes at runtime, but it creates a large memory burden when substantial numbers of instances are created.\nSlots reserves only a fixed amount of space for the specified attributes directly in each instance, instead of using the default dictionary.\nAny attempt to assign an attribute that is not listed in¬†__slots__¬†will raise an¬†AttributeError.\n\nThis can help prevent creating accidental attributes due to typos, but it can also be restrictive if you need to add additional attributes later in development.\n\nExample:\nclass Ant:\n  __slots__ = ['worker_id', 'role', 'colony']\n\n  def __init__(self, worker_id, role, colony):\n      self.worker_id = worker_id\n      self.role = role\n      self.colony = colony\n\nLazy Initialization for memory intensive operations\n\nScenario: Data Loading in an app\n\nUser wants to look at map or examine features before analyzing data.\nWithout Lazy Loading, the entire dataset is loaded upfront, leading to slower startup and potentially exceeding memory limits.\n\nCaching is also a good idea if you‚Äôre performing the same intensive computation more than once.\nExample: Lazy Loader\nfrom functools import cached_property\n\nclass DataLoader:\n\n    def __init__(self, path):\n        self.path = path\n\n    @cached_property\n    def dataset(self):\n        # load the dataset here\n        # this will only be executed once when the dataset property is first accessed\n        return self._load_dataset()\n\n    def _load_dataset(self):\n        print(\"Loading the dataset...\")\n\n        # load a big dataset here\n        df = pd.read_csv(self.path)\n        return df\n\nclass DataProcessor:\n    def __init__(self, path):\n          self.path = path\n        self.data_loader = DataLoader(self.path)\n\n    def process_data(self):\n        dataset = self.data_loader.dataset\n        print(\"Processing the dataset...\")\n        # Perform complex data processing steps on the loaded dataset\n        ...\n\n# instantiate the DataLoader class\npath = \"/[path_to_dataset]/mnist.csv\"\n\n# instantiate the DataProcessor class with the data file path\n# üëâ no data will be loaded at this stage! ‚úÖ\nprocessor = DataProcessor(path)\n\n# trigger the processing\nprocessor.process_data()  # The dataset will be loaded and processed when needed\n\nUse generators to reduce memory usage of loops\n\nSee Python, General &gt;&gt; Loops &gt;&gt; Generators\n\nAlso, in Loops &gt;&gt; Misc, there‚Äôs a chart that shows the memory benefits to using a generator vs list comprehension.",
    "crumbs": [
      "Python",
      "Classes"
    ]
  },
  {
    "objectID": "qmd/python-classes.html#sec-py-class-examp",
    "href": "qmd/python-classes.html#sec-py-class-examp",
    "title": "Classes",
    "section": "Examples",
    "text": "Examples\n\nRead file chunks, process, and write to parquet\nimport pandas as pd\n\nclass PandasChunkProcessor:\n    def __init__(self, filepath, chunk_size, verbose=True):\n        self.filepath = filepath\n        self.chunk_size = chunk_size\n        self.verbose = verbose\n\n    def process_data(self):\n        for chunk_id, chunk in enumerate(pd.read_csv(self.filepath, chunksize=self.chunk_size)):\n            processed_chunk = self.process_chunk(chunk)\n            self.save_chunk(processed_chunk, chunk_id)\n\n    def process_chunk(self, chunk):\n        # process each chunk of data\n        processed_chunk = processing_function(chunk)\n        return processed_chunk\n\n    def save_chunk(self, chunk, chunk_id):\n        # save each processed chunk to a parquet file\n        chunk_filepath = f\"./output_chunk_{chunk_id}.parquet\"\n        chunk.to_parquet(chunk_filepath)\n        if self.verbose:\n            print(f\"saved {chunk_filepath}\")\nTransforms variables by logging, can add 1 if necessary, back-transform\nfrom sklearn.base import BaseEstimator, TransformerMixin¬†\nfrom sklearn.preprocessing import PowerTransformer¬†\n\nclass CustomLogTransformer(BaseEstimator, TransformerMixin):¬†\n¬† ¬† def __init__(self):¬†\n¬† ¬† ¬† ¬† self._estimator = PowerTransformer()¬† # init a transformer¬†\n¬† ¬† def fit(self, X, y=None):¬†\n¬† ¬† ¬† ¬† X_copy = np.copy(X) + 1¬† # add one in case of zeroes¬†\n¬† ¬† ¬† ¬† self._estimator.fit(X_copy)¬†\n¬† ¬† ¬† ¬† return self¬†\n¬† ¬† def transform(self, X):¬†\n¬† ¬† ¬† ¬† X_copy = np.copy(X) + 1¬†\n¬† ¬† ¬† ¬† return self._estimator.transform(X_copy)¬† # perform scaling¬†\n¬† ¬† def inverse_transform(self, X):¬†\n¬† ¬† ¬† ¬† X_reversed = self._estimator.inverse_transform(np.copy(X))¬†\n¬† ¬† ¬† ¬† return X_reversed - 1¬† # return subtracting 1 after inverse transform\nPredictions for a Huggingface classifer\nimport sys\nfrom transformers import pipeline\nfrom typing import List\nimport numpy as np\nfrom time import perf_counter\nimport logging\n\n# Set up logger\nlogging.basicConfig(stream=sys.stdout, level=logging.INFO)\nlog = logging.getLogger(__name__)\n\nclass ZeroShotTextClassifier:\n¬† ¬† \"\"\"Class with only class methods\"\"\"\n¬† ¬† # Class variable for the model pipeline\n¬† ¬† classifier = None\n¬† ¬† @classmethod\n¬† ¬† def load(cls):\n¬† ¬† ¬† ¬† # Only load one instance of the model\n¬† ¬† ¬† ¬† if cls.classifier is None:\n¬† ¬† ¬† ¬† ¬† ¬† # Load the model pipeline.\n¬† ¬† ¬† ¬† ¬† ¬† # Note: Usually, this would also download the model.\n¬† ¬† ¬† ¬† ¬† ¬† # But, we download the model into the container in the Dockerfile\n¬† ¬† ¬† ¬† ¬† ¬† # so that it's built into the container and there's no download at\n¬† ¬† ¬† ¬† ¬† ¬† # run time (otherwise, each time we'll download a 1.5GB model).\n¬† ¬† ¬† ¬† ¬† ¬† # Loading still takes time, though. So, we do that here.\n¬† ¬† ¬† ¬† ¬† ¬† # Note: You can use a GPU here if needed.\n¬† ¬† ¬† ¬† ¬† ¬† t0 = perf_counter()\n¬† ¬† ¬† ¬† ¬† ¬† cls.classifier = pipeline(\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† \"zero-shot-classification\", model=\"facebook/bart-large-mnli\"\n¬† ¬† ¬† ¬† ¬† ¬† )\n¬† ¬† ¬† ¬† ¬† ¬† elapsed = 1000 * (perf_counter() - t0)\n¬† ¬† ¬† ¬† ¬† ¬† log.info(\"Model warm-up time: %d ms.\", elapsed)\n¬† ¬† @classmethod\n¬† ¬† def predict(cls, text: str, candidate_labels: List[str]):\n¬† ¬† ¬† ¬† assert len(candidate_labels) &gt; 0\n¬† ¬† ¬† ¬† # Make sure the model is loaded\n¬† ¬† ¬† ¬† cls.load()\n¬† ¬† ¬† ¬† # For the tutorial, let's create\n¬† ¬† ¬† ¬† # a custom object from the huggingface prediction.\n¬† ¬† ¬† ¬† # Our prediction object will include the label and score\n¬† ¬† ¬† ¬† t0 = perf_counter()\n¬† ¬† ¬† ¬† # pylint: disable-next=not-callable\n¬† ¬† ¬† ¬† huggingface_predictions = cls.classifier(text, candidate_labels)\n¬† ¬† ¬† ¬† elapsed = 1000 * (perf_counter() - t0)\n¬† ¬† ¬† ¬† log.info(\"Model prediction time: %d ms.\", elapsed)\n¬† ¬† ¬† ¬† # Create the custom prediction object.\n¬† ¬† ¬† ¬† max_index = np.argmax(huggingface_predictions[\"scores\"])\n¬† ¬† ¬† ¬† label = huggingface_predictions[\"labels\"][max_index]\n¬† ¬† ¬† ¬† score = huggingface_predictions[\"scores\"][max_index]\n¬† ¬† ¬† ¬† return {\"label\": label, \"score\": score}\nPayroll System\n\nEmployees\n# In employees.py\nfrom hr import (\n¬† ¬† SalaryPolicy,\n¬† ¬† CommissionPolicy,\n¬† ¬† HourlyPolicy\n)\nfrom productivity import (\n¬† ¬† ManagerRole,\n¬† ¬† SecretaryRole,\n¬† ¬† SalesRole,\n¬† ¬† FactoryRole\n)\nclass Employee:\n¬† ¬† def __init__(self, id, name):\n¬† ¬† ¬† ¬† self.id = id\n¬† ¬† ¬† ¬† self.name = name\nclass Manager(Employee, ManagerRole, SalaryPolicy):\n¬† ¬† def __init__(self, id, name, weekly_salary):\n¬† ¬† ¬† ¬† SalaryPolicy.__init__(self, weekly_salary)\n¬† ¬† ¬† ¬† super().__init__(id, name)\nclass Secretary(Employee, SecretaryRole, SalaryPolicy):\n¬† ¬† def __init__(self, id, name, weekly_salary):\n¬† ¬† ¬† ¬† SalaryPolicy.__init__(self, weekly_salary)\n¬† ¬† ¬† ¬† super().__init__(id, name)\nclass SalesPerson(Employee, SalesRole, CommissionPolicy):\n¬† ¬† def __init__(self, id, name, weekly_salary, commission):\n¬† ¬† ¬† ¬† CommissionPolicy.__init__(self, weekly_salary, commission)\n¬† ¬† ¬† ¬† super().__init__(id, name)\nclass FactoryWorker(Employee, FactoryRole, HourlyPolicy):\n¬† ¬† def __init__(self, id, name, hours_worked, hour_rate):\n¬† ¬† ¬† ¬† HourlyPolicy.__init__(self, hours_worked, hour_rate)\n¬† ¬† ¬† ¬† super().__init__(id, name)\nclass TemporarySecretary(Employee, SecretaryRole, HourlyPolicy):\n¬† ¬† def __init__(self, id, name, hours_worked, hour_rate):\n¬† ¬† ¬† ¬† HourlyPolicy.__init__(self, hours_worked, hour_rate)\n¬† ¬† ¬† ¬† super().__init__(id, name)\nProductivity\n# In productivity.py\nclass ProductivitySystem:\n¬† ¬† def track(self, employees, hours):\n¬† ¬† ¬† ¬† print('Tracking Employee Productivity')\n¬† ¬† ¬† ¬† print('==============================')\n¬† ¬† ¬† ¬† for employee in employees:\n¬† ¬† ¬† ¬† ¬† ¬† result = employee.work(hours)\n¬† ¬† ¬† ¬† ¬† ¬† print(f'{employee.name}: [{result}]')\n¬† ¬† ¬† ¬† print('')\nclass ManagerRole:\n¬† ¬† def work(self, hours):\n¬† ¬† ¬† ¬† return f'screams and yells for [{hours}] hours.'\nclass SecretaryRole:\n¬† ¬† def work(self, hours):\n¬† ¬† ¬† ¬† return f'expends [{hours}] hours doing office paperwork.'\nclass SalesRole:\n¬† ¬† def work(self, hours):\n¬† ¬† ¬† ¬† return f'expends [{hours}] hours on the phone.'\nclass FactoryRole:\n¬† ¬† def work(self, hours):\n¬† ¬† ¬† ¬† return f'manufactures gadgets for [{hours}] hours.'\nHR\n# In hr.py\nclass PayrollSystem:\n¬† ¬† def calculate_payroll(self, employees):\n¬† ¬† ¬† ¬† print('Calculating Payroll')\n¬† ¬† ¬† ¬† print('===================')\n¬† ¬† ¬† ¬† for employee in employees:\n¬† ¬† ¬† ¬† ¬† ¬† print(f'Payroll for: {employee.id} - {employee.name}')\n¬† ¬† ¬† ¬† ¬† ¬† print(f'- Check amount: {employee.calculate_payroll()}')\n¬† ¬† ¬† ¬† ¬† ¬† print('')\nclass SalaryPolicy:\n¬† ¬† def __init__(self, weekly_salary):\n¬† ¬† ¬† ¬† self.weekly_salary = weekly_salary\n¬† ¬† def calculate_payroll(self):\n¬† ¬† ¬† ¬† return self.weekly_salary\nclass HourlyPolicy:\n¬† ¬† def __init__(self, hours_worked, hour_rate):\n¬† ¬† ¬† ¬† self.hours_worked = hours_worked\n¬† ¬† ¬† ¬† self.hour_rate = hour_rate\n¬† ¬† def calculate_payroll(self):\n¬† ¬† ¬† ¬† return self.hours_worked * self.hour_rate\nclass CommissionPolicy(SalaryPolicy):\n¬† ¬† def __init__(self, weekly_salary, commission):\n¬† ¬† ¬† ¬† super().__init__(weekly_salary)\n¬† ¬† ¬† ¬† self.commission = commission\n¬† ¬† def calculate_payroll(self):\n¬† ¬† ¬† ¬† fixed = super().calculate_payroll()\n¬† ¬† ¬† ¬† return fixed + self.commission",
    "crumbs": [
      "Python",
      "Classes"
    ]
  },
  {
    "objectID": "qmd/regression-regularized.html",
    "href": "qmd/regression-regularized.html",
    "title": "Regularized",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Regression",
      "Regularized"
    ]
  },
  {
    "objectID": "qmd/regression-regularized.html#sec-reg-reg-misc",
    "href": "qmd/regression-regularized.html#sec-reg-reg-misc",
    "title": "Regularized",
    "section": "",
    "text": "Regularized Logistic Regression is most necessary when the number of candidate predictors is large in relationship to the effective sample size 3np(1‚àíp) where p is the proportion of Y=1 Harrell\nIf using sparse matrix, then you don‚Äôt need to normalize predictors\nPreprocessing\n\nStandardize numerics\nDummy or factor categoricals\nRemove NAs, na.omit\n\nPackages\n\n{glmnet} - handles families: Gaussian, binomial, Poisson, probit, quasi-poisson, and negative binomial GLMs, along with a few other special cases: the Cox model, multinomial regression, and multi-response Gaussian.\n{robustHD}: Robust methods for high-dimensional data, in particular linear model selection techniques based on least angle regression and sparse regression\nIn {{sklearn}} (see Model building, sklearn &gt;&gt; Algorithms &gt;&gt; Stochaistic Gradient Descent (SGD)), the hyperparameters are different than in R\n\nlambda (R) is alpha (py)\nalpha (R) is 1 - L1_ratio (py)\n\n{SLOPE} - Lasso regression that handles correlated predictors by clustering them\n{{Multi-Layer-Kernel-Machine}} - Multi-Layer Kernel Machine (MLKM) is a Python package for multi-scale nonparametric regression and confidence bands. The method integrates random feature projections with a multi-layer structur\n\nA fast implementation of Kernel Ridge Regression (KRR) (sklearn) which is used for non-parametric regularized regression.\nPaper: Multi-Layer Kernel Machines: Fast and Optimal Nonparametric Regression with Uncertainty Quantification\n\n\nVariable Selection\n\nFor Inference, only Adaptive LASSO is capable of handling block and time series dependence structures in data\n\nSee A Critical Review of LASSO and Its Derivatives for Variable Selection Under Dependence Among Covariates\n\n‚ÄúWe found that one version of the adaptive LASSO of Zou (2006) (AdapL.1se) and the distance correlation algorithm of Febrero-Bande et al.¬†(2019) (DC.VS) are the only ones quite competent in all these scenarios, regarding to different types of dependence.‚Äù\nThere‚Äôs a deeper description of the model in the supplemental materials of the paper. I think the ‚Äú.1se‚Äù means it‚Äôs using the lambda.1se from cv.\n\nlambda.1se : largest value of Œª such that error is within 1 standard error of the cross-validated errors for lambda.min.\n\nsee lambda.min, lambda.1se and Cross Validation in Lasso : Binomial Response for code to access this value.\n\n\n\nRe the distance correlation algorithm (it‚Äôs a feature selection alg used in this paper as benchmark vs LASSO variants)\n\n‚Äúthe distance correlation algorithm for variable selection (DC.VS) of Febrero-Bande et al.¬†(2019). This makes use of the correlation distance (Sz√©kely et al., 2007; Szekely & Rizzo, 2017) to implement an iterative procedure (forward) deciding in each step which covariate enters the regression model.‚Äù\nStarting from the null model, the distance correlation function, dcor.xy, in {fda.usc} is used to choose the next covariate\n\nguessing you want large distances and not sure what the stopping criteria is\n\nalgorithm discussed in this paper, Variable selection in Functional Additive Regression Models\n\nHarrell is skeptical. ‚ÄúI‚Äôd be surprised if the probability that adaptive lasso selects the‚Äùright‚Äù variables is more than 0.1 for N &lt; 500,000.‚Äù",
    "crumbs": [
      "Regression",
      "Regularized"
    ]
  },
  {
    "objectID": "qmd/regression-regularized.html#sec-reg-reg-conc",
    "href": "qmd/regression-regularized.html#sec-reg-reg-conc",
    "title": "Regularized",
    "section": "Concepts",
    "text": "Concepts\n\nShrinking effect estimates turns out to always be best\n\nOLS is the Best Linear Unbiased Estimator (BLUE), but being unbiased means the variance of the estimated effects is large from sample to sample and therefore outcome variable predictions using OLS don‚Äôt generalize well.\nIf you predicted y using the sample mean times some coefficient, it‚Äôs always(?) the case that you‚Äôll have a better generalization error with a coefficient less than 1 (shrinkage).\n\nRegularized Regression vs OLS\n\nAs N ‚Üë, standard errors ‚Üì\n\nregularized regression and OLS regression produce similar predictions and coefficient estimates.\n\nAs the number of covariates ‚Üë (relative to the sample size), variance of estimates ‚Üë\n\nregularized regression and OLS regression produce much different predictions and coefficient estimates\nTherefore OLS predictions are usually fine in a low dimension world (not usually the case)",
    "crumbs": [
      "Regression",
      "Regularized"
    ]
  },
  {
    "objectID": "qmd/regression-regularized.html#sec-reg-reg-ridge",
    "href": "qmd/regression-regularized.html#sec-reg-reg-ridge",
    "title": "Regularized",
    "section": "Ridge",
    "text": "Ridge\n\nThe regularization reduces the influence of correlated variables on the model because the weight is shared between the two predictive variables, so neither alone would have strong weights. This is unlike Lasso which just drops one of the variables (which one gets dropped isn‚Äôt consistent).\nLinear transformations in the design matrix will affect the predictions made by ridge regression.",
    "crumbs": [
      "Regression",
      "Regularized"
    ]
  },
  {
    "objectID": "qmd/regression-regularized.html#sec-reg-reg-lasso",
    "href": "qmd/regression-regularized.html#sec-reg-reg-lasso",
    "title": "Regularized",
    "section": "Lasso",
    "text": "Lasso\n\nWhen lasso drops a variable, it doesn‚Äôt mean that the variable wasn‚Äôt important.\n\nThe variable, \\(x_1\\), could‚Äôve been correlated with another variable, \\(x_2\\), and lasso happens to drop \\(x_1\\) because in this sample, \\(x_2\\), predicted the outcome just a tad better.\n\n\n\nAdaptive LASSO\n\n\nPurple dot indicates that it‚Äôs a weighted (\\(w_j\\)) version of LASSO\nGreen checkmark indicates it‚Äôs optimization is a convex problem\nBetter Selection, Bias Reduction are attributes that it has that are better than standard LASSO\nWeighted versions of the LASSO attach the particular importance of each covariate for a suitable selection of the weights. Joint with iteration, this modification allows for a reduction of the bias.\n\nZhou (2006) say that you should choose your weights so the adaptive Lasso estimates have the Oracle Property:\n\nYou will always identify the set of nonzero coefficients‚Ä¶when the sample size is infinite\nThe estimates are unbiased, normally distributed, and the correct variance (Zhou (2006) has the technical definition)‚Ä¶when the sample size is infinite.\n\nTo have these properties, \\(w_j = \\frac{1}{|\\hat\\beta_j|^q}\\), where \\(q &gt; 0\\) and \\(\\hat\\beta_j\\) is an unbiased estimate of the true parameter, \\(\\beta\\)\n\nGenerally, people choose the Ordinary Least Squares (OLS) estimate of \\(\\beta\\) because it will be unbiased. Ridge regression produces coefficient estimates that are biased, so you cannot guarantee the Oracle Property holds.\n\nIn practice, this probably doesn‚Äôt matter. The Oracle Property is an asymptotic guarantee (when \\(n \\rightarrow \\infty\\)), so it doesn‚Äôt necessary apply to your data with a finite number of observations. There may be scenarios where using Ridge estimates for weights performs really well. Zhou (2006) recommends using Ridge regression over OLS when your variables are highly correlated.\n\n\n\nSee article, Adaptive LASSO, for examples with a continuous, binary, and multinomial outcome",
    "crumbs": [
      "Regression",
      "Regularized"
    ]
  },
  {
    "objectID": "qmd/regression-regularized.html#sec-reg-reg-firth",
    "href": "qmd/regression-regularized.html#sec-reg-reg-firth",
    "title": "Regularized",
    "section": "Firth‚Äôs Estimator",
    "text": "Firth‚Äôs Estimator\n\nPenalized Logistic Regression estimator\nFor sample sizes less than around n = 1000 or sparse data, using Firth Estimator is recommended\nMisc\n\nNotes from\n\nThread\n\nPackages\n\n{brglm2} -\n{logistf} - Includes FLIC and FLAC extensions; uses profile penalized likelihood confidence intervals which outperform Wald intervals; includes a function that performs a penalized likelihood ratio test on some (or all) selected factors\n\nemmeans::emmeans is supported\n\n\nInvariant to linear transformations of the design matrix (i.e.¬†predictor variables) unlike Ridge Regression\nWhile the standard Firth correction leads to shrinkage in all parameters, including the intercept, and hence produces predictions which are biased towards 0.5, FLIC and FLAC are able to exclude the intercept from shrinkage while maintaining the desirable properties of the Firth correction and ensure that the sum of the predicted probabilities equals the number of events.\n\nPenalized Likelihood\n\\[\nL^*(\\beta\\;|\\;y) = L(\\beta\\;|\\;y)\\;|I(\\beta)|^{\\frac{1}{2}}\n\\]\n\nEquivalent to penalization of the log-likelihood by the Jeffreys prior\n\\(I(\\beta)\\) is the Fisher information matrix, i. e. minus the second derivative of the log likelihood\n\nMaximum Likelihood vs Firth‚Äôs Correction\n\nBias\n\nVariance\n\nCoefficient and CI bar comparison on a small dataset (n = 35, k = 7)\n\n\nLimitations\n\nRelies on maximum likelihood estimation, which can be sensitive to datasets with large random sampling variation. In such cases, Ridge Regression may be a better choice as it provides some shrinkage and can stabilize the estimates by pulling them towards the observed event rate.\nLess effective than ridge regression in datasets with highly correlated covariates\nFor the Firth Estimator, the Wald Test can perform poorly in data sets with extremely rare events.",
    "crumbs": [
      "Regression",
      "Regularized"
    ]
  },
  {
    "objectID": "qmd/surveys-sampling-methods.html",
    "href": "qmd/surveys-sampling-methods.html",
    "title": "Sampling Methods",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Surveys",
      "Sampling Methods"
    ]
  },
  {
    "objectID": "qmd/surveys-sampling-methods.html#sec-surv-sampmeth-misc",
    "href": "qmd/surveys-sampling-methods.html#sec-surv-sampmeth-misc",
    "title": "Sampling Methods",
    "section": "",
    "text": "Notes from:\n\nSurvey data in the field of economy and finance (ebook)",
    "crumbs": [
      "Surveys",
      "Sampling Methods"
    ]
  },
  {
    "objectID": "qmd/surveys-sampling-methods.html#sec-surv-sampmeth-terms",
    "href": "qmd/surveys-sampling-methods.html#sec-surv-sampmeth-terms",
    "title": "Sampling Methods",
    "section": "Terms",
    "text": "Terms\n\nIn a survey setting,\n\nU denotes a finite population (i.e.¬†[target population) of N units\nA sample s of n units (n‚â§N) is taken from U\n\nDesign Weights - The average number of units in the population that each sampled unit represents. This weight is determined by the sampling method and is an important part of the estimation process.\nEmpirical Design - When the inclusion probabilities (see below) are unknown\n\nSee Non-Probabilistic Sampling Methods\nExamples\n\nQuota Sampling - Units are selected so to reflect known structures for the population\nExpert Sampling - Units are selected according to expert advice\nNetwork Sampling - Existing sample units recruit future units from among their ‚Äònetwork‚Äô.\n\n\nEstimator of the parameter, Œ∏, is a function of sample observations\n\nExample: Sample Mean\n\\[\n\\hat{\\theta} = \\frac {\\sum_{i \\in S}y_i}{S} = \\bar{y}_S\n\\]\n\nPopulation mean of the study variable can be estimated by the mean value over the sample observations\n\n\nInclusion Probability - The probability for a unit to appear in the sample\nProbabilistic Design - When every element in the population has a fixed, known-in-advance inclusion probabilities\nSampling Bias - The probability distribution in the collected dataset deviates from its true natural distribution one would actually observe in the wilderness.\nSampling Frame - An exhaustive list of all the individuals which comprise the target population (Also see Surveys, Design &gt;&gt; Sources of Error &gt;&gt; Coverage or Frame Error)\n\nStudy Parameter (Œ∏) - Linear parameter of the study variable, such as a mean, a total or a proportion, or a more complex one such as a ratio between two population means, a correlation or a regression coefficient, a quantile (e.g.¬†median, quartile, quintile or decile) or an inequality measure such as the Gini or the Theil coefficient. (also see estimator)\nStudy Variable (y)\n\nQuantitative - Numerical information (e.g.¬†the total disposable income or the total food consumption)\nQualitative - Categorical information (e.g.¬†gender, citizenship, country of birth, marital status, occupation or activity status)",
    "crumbs": [
      "Surveys",
      "Sampling Methods"
    ]
  },
  {
    "objectID": "qmd/surveys-sampling-methods.html#sec-surv-sampmeth-probsamp",
    "href": "qmd/surveys-sampling-methods.html#sec-surv-sampmeth-probsamp",
    "title": "Sampling Methods",
    "section": "Probabilistic Sampling Methods",
    "text": "Probabilistic Sampling Methods\n\nSimple Random Sampling (SRS)\n\nA method of selecting n units out of N such that every sample s of size n has the same probability of selection\nSimple Inclusion Probability - the probability for a unit to appear in the sample\n\\[\n\\pi_i = \\mbox{Pr}(i \\in S) = \\sum \\limits_{i \\in s \\in S} \\mbox{Pr}(S = s) = \\frac {\\binom{N-1}{n-1}}{\\binom{N}{n}} = \\frac {n}{N}\n\\]\n\n\\(n\\) is the size of sample, \\(s\\), and \\(N\\) is the target population size\n\nDouble Inclusion Probability - the probability for 2 units to appear in the sample\n\\[\n\\pi_{ij} = \\mbox{Pr}(i,j \\in S) = \\sum \\limits_{i,j \\in s \\in S} \\mbox{Pr}(S = s) = \\frac {\\binom{N-2}{n-2}}{\\binom{N}{n}} = \\frac {n}{N} \\frac {n-1}{N-1}\n\\]\n\nWhere \\(i \\neq j\\)\n\\(n\\) is the size of sample, \\(s\\), and \\(N\\) is the target population size\n\nWithout Replacement (most common)\n\nAt the first extraction, each one of the population units will have an equal probability of selection, \\(1/N\\).\nAt the second extraction, the remaining N-1 units will have a selection probability equal to \\(1/(N-1)\\). Etc.\n\nWith Replacement - all the units of the population will have all the same probability of being selected \\(1/N\\) Advantages:\n\nIt‚Äôs simple and doesn‚Äôt use auxiliary information on the population\nThe selection is random and, then, any unit is favoured\nThe sample is representative Disadvantages:\nThe choice of the element is completely random\nA complete list of the population units is necessary\nIt‚Äôs time and cost consuming\n\nEstimated Total of the study variable, \\(\\hat{Y}\\)\n\\[\n\\hat{Y}_{SRS} = N \\bar{y}\n\\]\n\nWhere \\(N\\) is the target population size\n\nEstimated Mean of the study variable, \\(\\bar{Y}\\)\n\\[\n\\hat{\\bar{Y}}_{SRS} = \\bar{y}\n\\]\n\nWhere \\(\\bar{y}\\) is the sample mean\n\nVariance for Estimated Total\n\\[\nV(\\hat{Y}_{SRS}) = N^2 (1-f) \\frac {S^2_y}{n}\n\\]\n\n\\(S^2_y\\) is the dispersion of the study variable, \\(y\\), over the population \\(U\\)\n\\[\nS^2_y = \\frac {1}{N-1} \\sum_{i \\in U} (y_i - \\bar{Y})^2\n\\]\nSampling Rate or Sampling Fraction: \\(\\mbox{f} = n/N\\)\nFinite Population Correction Factor: \\(1-\\mbox{f}\\)\n\nVariance for Estimated Mean\n\\[\n\\hat{V}(\\bar{y}) = (1-\\mbox{f}) \\frac {s^2_y}{n}\n\\]\n\nSample Dispersion\n\\[\ns^2_y = \\frac{1}{n-1}\\sum_{i \\in s} (y_i - \\bar{y})^2\n\\]\n\nEstimated size of subpopulation, \\(A\\)\n\\[\n\\hat{N}_A = Np_A\n\\]\n\n\\(p_A\\) is the sample proportion of units from target subpopulation, \\(U_A\\)\n\ni.e.¬†(I think) \\(n_A / N_A\\)\n\nExamples: Subpopulations\n\nTotal number of males or females in the population\nTotal number of elderly people aged more than 65 in the population\nTotal number of establishments having more than 50 employees in a certain geographical region or in a sector of activity.\n\n\nVariance of sample proportion of subpopulation, \\(A\\)\n\\[\n\\hat{V}(p_A) = \\frac{p_A(1-p_A)}{n}\n\\]\nDomain Parameter Estimation\n\nRefers to estimating population parameters for sub-populations of interest, called domains. For instance, one may wish to estimate the mean household disposable income broken down by personal characteristics such as age, gender or citizenship\n\nI think this is different from ‚ÄúEstimated size of subpopulation, A‚Äù (above) because we‚Äôre estimating a study variable of subpopulation vs the size of the subpopulation\n\nEstimated Total of the study variable\n\\[\n\\hat{Y}_D = \\frac{N \\cdot n_D}{n} \\; \\bar{y}_D\n\\]\n\n\\(\\bar{y}_D\\) - The sample mean of study variable, \\(y\\), within the domain, \\(D\\)\n\\(n_D\\) - The total number of sample units from the sample \\(s\\) which fall into domain, \\(D\\)\n\nSample size \\(n_D\\) is a random variable of mean \\(\\bar{n}D = nP_D\\) where \\(P_D = N_D / N\\)\n\nI guess this is a random variable because this is strictly SRS, so you aren‚Äôt stratifying by \\(D\\) when you sample the target population. Therefore, the number of samples from \\(D\\) you happen to get will be random and have a distribution.\n\n\nAlternative: When the size of the domain, \\(N_D\\), of \\(U_D\\) is known\n\n\\(\\hat{Y}_{D,\\mbox{alt}} = N_D \\cdot \\bar{y}D\\)\nThis formula has a provably (see ebook in Misc) lower variance than the original formula\n\n\nVariance for Estimated Total\n\\[\nV(\\hat{Y}_D) \\approx N^2_D \\left(\\frac{1}{\\bar{n}_D} - \\frac{1}{N_D}\\right)S^2_D \\left(1 + \\frac{1-P_D}{CV^2_D} \\right)\n\\]\n\nWhere\n\\[\n\\begin{align}\n&S^2_D = \\sum_{k \\in U_D} \\frac{(y_k - \\bar{Y}_D)^2}{N_D - 1}\\\\\n&CV_D = \\frac{S_D}{\\bar{Y}_D}\n\\end{align}\n\\]\nAssumes the population sizes, \\(N\\) and \\(N_D\\), are ‚Äúlarge enough.‚Äù\nFor the Alternative Estimated Total formula (see above)\n\\[\nV(\\hat{Y}_{D,alt}) \\approx N^2_D \\left(\\frac{1}{\\bar{n}_D} - \\frac{1}{N_D} \\right)S^2_D\n\\]\n\nAssumes the sample size, \\(n_D\\), is ‚Äúlarge enough.‚Äù\nA provably lower variance (see ebook in Misc)\n\n\n\n\n\n\nUnequal Probability Sampling\n\nDifferent units in the population will have different probabilities of being included in a sample.\n\nUnlike SRS, where each unit has an equal probability of being included in the sample\n\nUnequal probability sampling can result in estimators having higher precision than when simple random sampling or other equal probability designs are used.\n\nEmphasizes the importance of utilizing so-called ‚Äúauxiliary‚Äù information as a way to boost sampling precision. (see œÄk below)\n\nHorvitz-Thompson estimator (without replacement selection)\n\nEstimated Total, \\(\\hat{Y}\\), for the study variable\n\\[\n\\hat{Y}_{HT}= \\sum_{k \\in S} \\frac{y_k}{\\pi_k} = \\sum_{k \\in s} d_ky_k\n\\]\n\\(d_k = 1/\\pi_k\\) is the design weight of unit, \\(k\\), of sample, \\(s\\)\n\\(\\pi_k\\) is the inclusion probability for unit, \\(k\\), of sample, \\(s\\)\n\nIn practice, as the study variable \\(y\\) is unknown, the inclusion probabilities should be taken proportional to an auxiliary variable \\(x\\) assumed to have a linear relationship with \\(y: œÄ \\propto x\\) (probability proportional to size sampling)\nAn inclusion probability that is optimal with respect to one study variable may be far from optimal with other study variables. In case of multi-purpose surveys, this is a major problem which generally prevents from using unequal probability sampling.\n\nAlternatively, survey statisticians use stratification as we know it always make accuracy better no matter the study variable.\n\n\n\nHansen-Hurwitz estimator has been proposed in case of sampling with replacement.\n\n\n\nCluster Sampling\n\nAssumes population has natural clusters (e.g.¬†family unit). Different from Stratified Sampling in that the clustering characteristic(s) is the same for all clusters (between cluster variation = 0) and the within cluster variation is heterogeneous (i.e.¬†within cluster variation != 0).\n\nExample: Family units in NYC are chosen randomly chosen. The variation between family members is whats studied.\n\nAdvantages:\n\nit‚Äôs efficient when the clusters constitute naturally formed subgroups, for which we don‚Äôt possess the list of the population\nStudying clusters can be less expensive than simple random sampling.\n\nDisadvantages:\n\nThe conditions of the clusters aren‚Äôt always respected. The clusters may contain similar elements.\n\n\n\n\nStratified Sampling\n\n\nMisc\n\nNotes from Chapter 3 Stratification\nThe population is classified into subpopulations, called strata, based on some categorical characteristics, such as age, gender, education\nStratified sampling buckets the population into k strata (e.g., countries), and then the experiment random samples individuals from each stratum independently.\nAssumes between group variation is not 0 (i.e.¬†heterogeneous) and within-group variation is 0 (i.e.¬†homogeneous)\nReasons for stratification\n\nBaseline for group A different from group B\nReason to believe the effect for group A will be different from group B\n\nAdvantages\n\nIt can be more efficient than simple random sampling\nThere is less risk of obtaining non-representative samples\n\nDisadvantages\n\nIt needs the availability of auxiliary information on the population.\nThere are strict conditions for the strata\n\n\nEstimated Total, \\(\\hat{Y}\\), for the study variable and the\nEstimated Mean of the study variable, \\(\\bar{Y}\\) (respectively)\n\\[\n\\begin{align}\n&\\hat{Y}_\\mbox{STSRS}=\\sum_{h=1}^H N_h \\bar{y}_h \\\\\n&\\hat{\\bar{Y}}_\\mbox{STSRS} = \\sum_{h=1}^H W_h \\bar{y}_h\n\\end{align}\n\\]\n\nAssumes SRS within each strata\n\\(N\\) is the population size\n\\(N_h\\) is the population strata size for strata, \\(h\\)\n\\(W_h\\) is the frequency weight where \\(W_h = N_h / N\\)\n\\(\\bar{y}_h\\) is the sample mean of strata, \\(h\\)\n\nVariance for Estimated Total (assuming SRS within strata)\n\\[\nV(\\hat{Y}_\\mbox{STSRS}) = \\sum_{h=1}^H N_h^2 (1-\\mbox{f}_h)\\frac{S_h^2}{n_h}\n\\]\n\n\\(n_h\\) is the sample size for stratum, \\(h\\)\nStratum Sampling Fraction: \\(\\mbox{f}_h = n_h / N_h = n/N\\) (which is just \\(\\mbox{f}\\))\n\nAssumes \\(\\mbox{f}_h\\) is the same for each strata\n\nStratum Dispersion: \\(S_h^2\\) should be similar to the sample dispersion for SRS below, except the domain of the variables is within stratum, \\(h\\) (e.g.¬†\\(n ‚Üí n_h,\\) \\(»≥ ‚Üí »≥_h\\), etc.)\n\nSample Dispersion for SRS\n\\[\ns_y^2 = \\frac{1}{n-1}\\sum_{i \\in s} (y_i - \\bar{y})^2\n\\]\n\n\nVariance for Estimated Mean (assuming SRS within strata)\n\\[\nV(\\bar{Y}_\\mbox{STSRS}) = (1 - \\mbox{f}) \\frac{s^2_h}{n_h}\n\\]\n\n\\(n_h\\) will be the same for all \\(h\\), so it‚Äôs constant in this case\nSampling Fraction: \\(\\mbox{f} = N / n\\)\n\n\\(n\\) is the overall sample size\n\nWithin-Stratum Dispersion\n\\[\nS_w^2 = \\sum_{h=1}^H W_hS_h^2\n\\]\n\n\\(S^2_h\\): See above\n\\(N\\) is the population size and \\(N_h\\) is the population strata size for strata, \\(h\\)\n\\(W_h\\) is the frequency weight where \\(W_h = N_h / N\\)\n\n\nDesign Weights: \\(d_i = N_h / n_h \\;\\;\\forall \\in s_h\\)\n\nFor SRS, design weights are equal within each stratum\n\\(s_h\\) is the set of samples within stratum, \\(h\\)\n\nStratum sample size allocation methods\n\nLet assume the overall sample size, \\(n\\), has been fixed (generally out of budgetary considerations). We seek to determine which sample size, \\(n_h\\), is to be drawn out of each stratum in order to achieve statistical optimality under cost considerations.\nEqual Allocation\n\n\\(n^\\mbox{eq}_h = n / H\\)\n\\(H\\) is the number of strata\nPerforms poorly when the dispersions, \\(S^2_h\\), are different from one stratum to another\n\nProportional Allocation\n\nConsists of selecting samples in each stratum in proportion to the size, \\(N_h\\), of the stratum population\n\\(n^\\mbox{prop}_h = (n \\cdot N_h) / N = n \\cdot W_h\\)\nVariance\n\\[\n\\begin{align}\nV(\\hat{\\bar{Y}}_\\mbox{prop}) &= (1-\\mbox{f})\\frac{S^2_w}{n} \\\\\n&=\\frac{\\sum_{h=1}^h W_h S^2_h}{n} - \\frac{\\sum_{h=1}^h W_h S^2_h}{N}\n\\end{align}\n\\]\n\nOptimal or Neyman Allocation\n\nSeeks to minimize the variance under the cost constraint\n\\[\n\\sum_{h=1}^H c_h n_h = C_0\n\\]\n\n\\(C_0\\) is the overall budget available and \\(c_h\\) the average survey cost for an individual in stratum \\(h\\).\n\nStrata Sample Size with Cost Constraint\n\\[\n\\forall h \\;\\; n_h^\\mbox{opt} = \\frac{N_hS_h}{\\sqrt{c_h}} \\frac{C_0}{\\sum_{h=1}^H N_h S_h \\sqrt{c_h}}\n\\]\nStrata Sample Size without Cost Constraint\n\\[\nn_h^\\mbox{opt} = n \\frac{N_hS_h}{\\sum_{h=1}^H N_hS_h}\n\\]\nVariance\n\\[\nV(\\hat{\\bar{Y}}_\\mbox{SRS}) = \\frac{1}{n}\\sum_h W_h(\\bar{Y}_h - \\bar{Y})^2 - \\frac{1}{n}\\sum_h W_h(S_h - \\bar{S})^2\n\\]\n\n\\(\\bar{S}\\) must be the mean sqrt dispersion across all stratum\n\nContrary to proportional allocation, the Neyman allocation is variable-specific: optimality is defined with respect to one study variable, and what is optimal with respect to one variable may be far from optimal with respect to another.\nThe gain in accuracy as compared to proportional allocation is pretty small. That‚Äôs why in practice proportional allocation is often preferred to optimal allocation.\n\nBalanced Allocation\n\\[\n\\forall h \\;\\; n_h^\\mbox{bal} = \\frac{\\tilde{n}}{H} + (n - \\tilde{n})W_h\n\\]\n\n\\(\\tilde{n}\\) is a subsample of \\(n\\) that is equally allocated (see above) among the strata which insures minimal precision within the strata (i.e.¬†locally)\nThe rest of the sample (\\(n-\\tilde{n}\\)) can be allocated using either proportional or optimal allocations (see above) in order to optimize accuracy for the overall sample (i.e.¬†globally)\nBoth proportional and Neyman allocations increase sample accuracy at global level, but may happen to perform very poorly when it comes to strata (e.g.¬†regional) level estimates.\n\n\n\n\n\nMulti-Stage Sampling\n\nMisc\n\nUseful when no sampling frame is available\nStages\n\nAt first-stage sampling, a sample of Primary Sampling Units (PSU) is selected using a probabilistic design (e.g.¬†simple random sampling or other, with or without stratification)\nAt second-stage sampling, a sub-sample of Secondary Sampling Units (SSU) is selected within each PSU selected at first-stage. The selection of SSU is supposed to be independent from one PSU to another.\nAt third-stage sampling a sample of Tertiary Sampling Units can be selected with each of the SSU selected at second stage.\netc.\n\nExample: (given an absence of any frame of individuals)\n\nSelect a sample of municipalities (first-stage sampling),\nSelect a sample of neighbourhoods (second-stage sampling) within each selected municipality,\nSelect a sample of households (third-stage sampling) within each of the neighbourhoods selected a second stage\nSelect a sample of individuals (fourth-stage sampling) within each household.\n\nAdvantages:\n\nCan be more efficient than using only 1 of the sampling strategies\nCan decrease sample size if there are numerous units within strata or clusters\n\nDisadvantages:\n\nIf sampling assumptions aren‚Äôt valid, multi-stage sampling results to be less efficient than simple random sampling.\n\n\nExample: 2-stage cluster sampling\n\nAdds a second stage to cluster sampling. After clusters are chosen, units within those clusters are randomly sampled.\n\nExample: 2-Stage Stratified Sampling\n\nNotes from Two Stage Stratified Random Sampling ‚Äî Clearly Explained\nUseful for when you have hierarchical strata (e.g.¬†towns/blocks and households)\nExample: An education study of students where:\n\nSchools (first stage sampling units) may be selected with probabilities proportional to school size\nStudents (second stage units) within selected schools may be selected by stratified random sampling\n\nStage 1\n\n\n(Random?) Sample from group of First Stage Units (FSU)\n\nEach FSU usually has a population within a range\ne.g.¬†census geographies (census block, metropolitan statistical area, etc.)\n\n\nStage 2\n\n\nAll Second Stage Units (SSU) within each FSU are pooled together to create a population\n\nSSUs are the base geography unit you want to measure\ne.g.¬†households\n\nThen each SSU is binned into Second Stage Strata (SSS) according to a characteristic or set of characteristics\n\ne.g.¬†race, age, income level, education, etc.\nThe SSS are stratified sampled\n\n\n\n\n\n\nSystematic Sampling\n\n\nSteps\n\nAfter choosing a sample size, n, calculate the sampling interval k = N/n, where N is the population size\n\nIn the example, we have 9 smiles and we want to obtain a sample of 3 units, then N = 9, n = 3 and k = 9/3 =3.\n\nSelect a random starting point, r, which is a random integer between 1 and k: 1‚â§r‚â§k.\n\nIn the example, r = 2, where 1‚â§r‚â§3.\n\nOnce the first unit is selected, we take every following kth item to build the sample: r, r+k, r+2k , ‚Ä¶ , r+(n-1)k.\n\nAdvantages:\n\nThe random selection is applied only on the first item, while the rest of the items selected depend on the position of the first item and a fixed interval at which items are picked.\n\nDisadvantages:\n\nIf the list of the population elements presents a determined order, there is the risk of obtaining a non-representative sample",
    "crumbs": [
      "Surveys",
      "Sampling Methods"
    ]
  },
  {
    "objectID": "qmd/surveys-sampling-methods.html#sec-surv-sampmeth-probsamp-nonprob",
    "href": "qmd/surveys-sampling-methods.html#sec-surv-sampmeth-probsamp-nonprob",
    "title": "Sampling Methods",
    "section": "Non-Probabilistic Sampling Methods",
    "text": "Non-Probabilistic Sampling Methods\n\nMisc\n\nMostly used when probabilistic methods aren‚Äôt possible due to rarity or difficulty in obtaining a representative sample of the population being studied or cost constraints of the experiment\nPackages\n\n{nonprobsvy} - Inference Based on Non-Probability Samples and {jointCalib} - A Joint Calibration of Totals and Quantiles\n\nPaper: Inference for non-probability samples using the calibration approach for quantiles\nUtilizes a method of joint calibration for totals and quantiles to extend existing inference methods for non-probability samples, such as inverse probability weighting, mass imputation and doubly robust estimators which produces results that are more robust against model mis-specification and helps to reduce bias and improve estimation efficiency.\n\n\n\nQuota Sampling\n\nSimilar to Stratified Sampling (see Probabilistic Sampling Methods) except:\n\nEach stratum‚Äôs sample size is called its quota\nEach stratum‚Äôs sample size takes into account its distribution in the whole population.\n\nExample: If 80% of the population are males, then 80% of the sample should be males.\n\nWithin each stratum‚Äôs quota, the interviewer is free to choose the participants to interview.\n\nThis seems to be the main difference\n\n\nAdvantages:\n\nIt‚Äôs time and cost-effective, in particular with respect to the stratified sampling.\n\nDisadvantages:\n\nThe results can be distorted due to the discretion of the interviewers or the non-response bias\nThe quota sample can produce a selection bias\n\n\nJudgemental Sampling (aka Purposive Sampling)\n\nThe researcher selects the participants because he believes they are representative of the population\n\nUseful when there is only a limited number of people with specific traits\n\nAdvantages:\n\nIt‚Äôs time and cost-effective\nIt‚Äôs suitable to study a certain cultural domain, where the knowledge of an expert is needed\n\nDisadvantages:\n\nIt can lead to a high selection bias the bigger is the gap between the researcher‚Äôs knowledge and the actual situation of the population\n\n\nConvenience Sampling\n\nThe researcher chooses anyone that is ‚Äúconvenient‚Äù to him, i.e.¬†people that are immediately available to answer the questions, without any specific criteria\n\nUsually volunteers\n\nAdvantages:\n\nIt‚Äôs very cheap and fast\n\nDisadvantages:\n\nIt leads to a non-representative sample\n\n\nSnowball Sampling\n\nThe researcher asks already recruited people to identify other potential participants, and so on\n\nUseful for rare populations, for which it‚Äôs not possible to have the list of the population or it‚Äôs difficult to locate the population.\n\ne.g.¬†illegal immigrants\n\n\nAdvantages:\n\nIt‚Äôs useful for market studies or researches about delicate topics.\n\nDisadvantages:\n\nThe sample may be non-representative since it‚Äôs not random, but depends on the people contacted directly or indirectly by the researcher\nIt‚Äôs time-consuming",
    "crumbs": [
      "Surveys",
      "Sampling Methods"
    ]
  },
  {
    "objectID": "qmd/outliers.html#sec-outliers-stat",
    "href": "qmd/outliers.html#sec-outliers-stat",
    "title": "Outliers",
    "section": "Statistics",
    "text": "Statistics\n\nFor a skewed distribution, a Winsorized Mean (percentage of points replaced) often has less bias than a Trimmed Mean\nFor a symmetric distribution, a Trimmed Mean (percentage of points removed) often has less variance than a Winsorized Mean.\nHodges‚ÄìLehmann Estimator\n\nPackages: {DescTools::HodgesLehmann}\nA robust and nonparametric estimator of a population‚Äôs location parameter.\nFor populations that are symmetric about one median, such as the Gaussian or normal distribution or the Student t-distribution, the Hodges‚ÄìLehmann estimator is a consistent and median-unbiased estimate of the population median.\n\nHas a Breakdown Point of 0.29, which means that the statistic remains bounded even if nearly 30 percent of the data have been contaminated.\n\nSample Median is more robust with breakdown point of 0.50 for symmetric distributions, but is less efficient (i.e.¬†needs more data).\n\n\nFor non-symmetric populations, the Hodges‚ÄìLehmann estimator estimates the ‚Äúpseudo‚Äìmedian‚Äù, which is closely related to the population median (relatively small difference).\n\nThe psuedo-median is defined for heavy-tailed distributions that lack a finite mean.\n\nFor two-samples, it‚Äôs the median of the difference between a sample from x and a sample from y.\nOne-Variable Procedure\n\nFind all possible two-element subsets of the vector.\nCalculate the mean of each two-element subset.\nCalculate the median of all the subset means.\n\nTwo-Variable Procedure\n\nFind all possible two-element subsets between the two vectors (i.e.¬†cartesian product)\nCalculate difference between subsets\nCalculate median of differences",
    "crumbs": [
      "Outliers"
    ]
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-misc",
    "href": "qmd/distributions.html#sec-distr-misc",
    "title": "Distributions",
    "section": "",
    "text": "For a skewed distribution, a Winsorized Mean (percentage of points replaced) often has less bias than a Trimmed Mean\nFor a symmetric distribution, a Trimmed Mean (percentage of points removed) often has less variance than a Winsorized Mean.",
    "crumbs": [
      "Distributions"
    ]
  },
  {
    "objectID": "qmd/distributions.html#sec-distr-cauchy",
    "href": "qmd/distributions.html#sec-distr-cauchy",
    "title": "Distributions",
    "section": "Cauchy",
    "text": "Cauchy\n\nIt‚Äôs a Student t-distribution with one degree of freedom\nThe Hodges-Lehmann estimate is an efficient estimator of the population median (See Outliers &gt;&gt; Statistics &gt;&gt; Hodges-Lehmann Estimator)",
    "crumbs": [
      "Distributions"
    ]
  },
  {
    "objectID": "qmd/outliers.html#sec-outliers-preproc",
    "href": "qmd/outliers.html#sec-outliers-preproc",
    "title": "Outliers",
    "section": "Preprocessing",
    "text": "Preprocessing\n\nRemoval\n\nAn option if there‚Äôs sound reasoning (e.g.¬†data entry error, etc.)\n\nWinsorization\n\nA typical strategy is to set all outliers (values beyond a certain threshold) to a specified percentile of the data\nExample: A 90% winsorization would see all data below the 5th percentile set to the 5th percentile, and data above the 95th percentile set to the 95th percentile.\nPackages\n\n({DescTools::Winsorize})\n({datawizard::winsorize})",
    "crumbs": [
      "Outliers"
    ]
  },
  {
    "objectID": "qmd/project-analyses.html",
    "href": "qmd/project-analyses.html",
    "title": "Analyses",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Projects",
      "Analyses"
    ]
  },
  {
    "objectID": "qmd/project-analyses.html#sec-proj-anal-misc",
    "href": "qmd/project-analyses.html#sec-proj-anal-misc",
    "title": "Analyses",
    "section": "",
    "text": "Also see\n\nLogistics, Demand Planning &gt;&gt; Stakeholder Questions\n\nThese are questions for the stakeholder(s) when preparing to create a forecasting model, but many apply to other types of projects including Analysis.\n\nProject, Development &gt;&gt; CRISP-DM\nProject, Development &gt;&gt; Agile &gt;&gt; Data Science Lifecycle\n\nSee Thread on an analysis workflow using {targets}",
    "crumbs": [
      "Projects",
      "Analyses"
    ]
  },
  {
    "objectID": "qmd/project-analyses.html#sec-proj-analy-gen",
    "href": "qmd/project-analyses.html#sec-proj-analy-gen",
    "title": "Analyses",
    "section": "General",
    "text": "General\n\nGeneral Questions\n\n‚ÄúWhat variables are relevant to the problem I‚Äôm trying to solve?‚Äù\n‚ÄúWhat are the key components of this data set?‚Äù\n‚ÄúCan this data be categorized?‚Äù\n‚ÄúIs this analysis result out of the ordinary?‚Äù\n‚ÄúWhat are the key relationships?‚Äù\n‚ÄúIs this the best way this company could be carrying out this task?‚Äù\n‚ÄúWhat will happen under new conditions?‚Äù\n‚ÄúWhat factors are best used to determine or predict this eventuality?‚Äù\n\nBreak down the problem into parts and focus on those during EDA\n\nAlso see Decison Intelligence &gt;&gt; Mental Models for details on methods to break down components\nExample: Why are sales down?\n\nHow are sales calculated?\n\ne.g.¬†Total Sales = # of Orders * Average Order Value\n\nBreakdown # of orders and average order value\n\nnumber of orders = number of walk-ins * % conversion\n\nHas walk-ins or conversion declined?\n\nAverage Order Value\n\nBin avg order value by quantiles, plot and facet or group by binned groups. Is one group more responsible for the decline than others?\n\n\nIs there regional or store or brand variability? (grouping variables)\n\n\nDrill down into each component until the data doesn‚Äôt allow you to go any farther.\nSegment data by groups\n\nColor or facet by cat vars\nPay attention to counts of each category (may need to collapse categories)\nCommon segments in product analytics\n\nFree vs Paid users\nDevice Type (desktop web vs mobile web vs native app)\nTraffic Source (people coming from search engines, paid marketing, people directly typing in your company‚Äôs URL into their browser, etc.)\nDay of the Week.",
    "crumbs": [
      "Projects",
      "Analyses"
    ]
  },
  {
    "objectID": "qmd/project-analyses.html#sec-proj-anal-tropf",
    "href": "qmd/project-analyses.html#sec-proj-anal-tropf",
    "title": "Analyses",
    "section": "TROPICS framework",
    "text": "TROPICS framework\n\nMisc\n\nFor analyzing changes in key performance metrics\nFrom https://towardsdatascience.com/answering-the-data-science-metric-change-interview-question-the-ultimate-guide-5e18d62d0dc6\nComponents: Time, Region, Other Internal Products, Platform, Industry and Competitors, Cannibalization, Segmentation\n\nTime\n\nWhat to explore\n\nHow has our performance been trending over the last few weeks (or months)?\n\nExample: If we saw a 10% increase in the last week, was the percentage change in the weeks before also 10%? In which case the 10% may actually be pretty normal? Or was the change lower? Higher?\n\nIs this change seasonal? Do we see the same spike around this time each year?\n\nExample: Does WhatsApp see a spike in messages sent during the holiday season?\n\nWas the change sudden or gradual? Did we see a sudden spike or drop overnight? Or has the metric gradually been moving in this direction over time?\n\nExample: If product usage jumps by 50% overnight could there be a bug in our logging systems?\n\nAre there specific times during the day or week where this change is more pronounced?\n\nSolution examples\n\nIf the change is seasonal then there may not necessarily be anything you need to ‚Äòsolve‚Äô for. But, you can leverage this to your advantage.\n\nExample: Amazon sales may jump up on Black Friday so they would want to make sure they have the proper infrastructure in place so the site doesn‚Äôt crash. They may also see if there are certain types of products that are popular purchases and increase their inventory accordingly.\n\nIf there is a sudden decline, there may be a bug in the logging or a new feature or update recently launched that‚Äôs creating problems that you may need to roll back.\nIf there‚Äôs a gradual decline, it may indicate a change in user behavior.\n\nExample: If the time spent listening to music is declining because people prefer to listen to podcasts then Spotify may want to focus more of their content inventory on podcasts.\n\n\n\nRegion\n\nWhat to explore\n\nIs this change concentrated in a specific region or do we see a similar change across the board?\n\nSolution examples\n\nThere may be newly enforced regulations in countries that are affecting your product metrics. You would need to do further research to assess the impacts of these regulations and potential workarounds.\n\nExample: Uber was temporarily banned in London in 2019 for repeated safety failures which resulted in a series of lawsuits and court cases.\n\nPopular local events may also be potential explanations. While these may not be areas to ‚Äòsolve‚Äô for they can be opportunities to take advantage of.\n\nExample: Coachella season means a jump in the number of Airbnb bookings in Southern California that are capitalized on by surge pricing.\n\n\n\nOther Internal Products\n\nWhat to explore\n\nIs this change specific to one product or is it company-wide? How does this metric vary across our other product offerings?\n\nExample: If the Fundraising feature on Facebook is seeing increased usage, is the swipe up to donate feature on Instagram (which Facebook owns) also seeing a similar uptick?\n\nAre there other metrics that have also changed in addition to the one in question?\n\nExample: If the time spent on Uber is going down, is the number of cancellations by drivers also declining (implying people are spending less time on the app because they‚Äôre having a more reliable experience)?\n\n\nSolution examples\n\nIf there is a metric change across our other features and products, it‚Äôs likely a larger problem we should address with multiple teams and may need a Public Relations consultant.\n\nExample: Elon + Twitter.\n\n\n\nPlatform\n\nWhat to explore\n\nMobile vs Desktop?\nMac vs Windows?\nAndroid vs iOS?\n\nSolution examples\n\nIf there was a positive change in our metric on a specific platform (e.g.¬†iOS) and coincides with an (iOS) update we released, we would want to do a retrospective to determine what about that update was favorable so we can double down on it. Alternatively, if the metric change was negative, we may want to reconsider and even roll back the update.\nIf the change was due to a change in the platform experience (e.g.¬†app store placement, ratings) we may want to seek advice from our marketing team since this is a top of the funnel problem\nIf users are showing astrong preference for a specific platform, we want to make sure that the experience of the preferred platform is up to par. We also need to make sure our platform-specific monetization strategies are switching to follow the trend.\n\nExample: Facebook‚Äôs ad model was initially tied to the desktop app only and had to be expanded as mobile became the platform of preference.\n\n\n\nIndustry & Competitors\n\nWhat to explore\n\nWhen our decline began, was there a new competitor or category that emerged?\n\nExample: Did the number of users listening to Apple podcasts go down when Clubhouse came on to the scene?\n\nHave competitors changed their offering lately?\nIs the category as a whole declining?\n\nSolution examples\n\nIf the category is shifting as a whole, we should begin looking at larger-scale changes to the app.\n\nExample: What Kodak should have done.\n\nIf there‚Äôs a new competitor taking our market share, we can begin with reactivation campaigns on churned users. We may also want to conduct user research to understand the gap between our offering and those of our competitors\n\n\nCannibalization\n\nWhat to explore\n\nAre other products or features in our offering experiencing growth in the face of our decline or vice versa?\nHave we released a new feature that is drawing users away from our old features? If so, can we fully attribute the release of the new feature with the decline in the metric of our feature in question?\n\nExample: When Facebook released reactions, did the number of comments on a post go down because people found it easier to press a react button instead of writing a comment?\n\n\nSolution examples\n\nCannibalization may not necessarily be a bad thing. We need to determine whether this shift in user interest across our features is favorable by determining whether the new features align better with the goals of the business.\nCannibalization may also be an indication of but it is indicative of a change in user behavior. In which case we may want to consider if perhaps our core metrics need to change as user behaviors change.\n\nExample: If users care more about watching Instagram stories than engaging with the Instagram feed we may want to optimize for retention (because the ephemeral nature of stories is more likely to motivate users to keep coming back to the platform) instead of time spent on the app.\n\nWe can also look at ways to bridge the two features together to create a more unified platform.\n\n\nSegmentation\n\nWhat to explore\n\nHow does this metric vary by¬†user type:\n\nAge, sex, education\nPower users versus casual users * New users versus existing users\n\nHow does this metric vary by different attributes of the product:\n\nExample: If the time spent watching YouTube videos is going down, is it across longer videos or shorter clips? Is it only for DIY videos or interview tutorial content? Is the same number of people that started watching a video the same but a large chunk of them stop watching it halfway through?\n\n\nSolution examples\n\nIf the metric varies between new and existing users then maybe there is a overcrowding effect.\n\nExample: Reddit forums could hit a critical mass where new users feel lost and less likely to engage than existing users resulting in a drop in engagements per user\n\nIf users are dropping off at certain parts of the funnel then maybe the experience at that funnel step is broken.\n\nExample: While the same number of people are starting carts on Amazon there may be a drop in purchases if the payment verification system isn‚Äôt working.",
    "crumbs": [
      "Projects",
      "Analyses"
    ]
  },
  {
    "objectID": "qmd/project-analyses.html#sec-proj-anal-actanl",
    "href": "qmd/project-analyses.html#sec-proj-anal-actanl",
    "title": "Analyses",
    "section": "Actionable Analyses",
    "text": "Actionable Analyses\n\nNotes from: Driving Product Impact With Actionable Analyses\nActionable insights do not only provide a specific data point that might be interesting, but lay out a clear narrative how this insight is connected to the problem at hand, what the ramifications are, as well as possible options and next steps to take with the associated benefits/risks of (not) acting upon these.\nNot Actionable: Users under the age of 25 hardly use audiobooks.\n\nIs this good, bad? Should they be listening to audiobooks and is there anything we should do about it?\n\nActionable: Users under the age of 25 hardly use audiobooks because they never explore the feature in the app. However users who listen to audiobooks have a 20% higher retention rate.\n\nThis information tells us that audiobooks represent a potential opportunity to increase retention amongst younger users, however there seems to be more work to be done to encourage users exploring this feature.\n\nSteps\n\nProblem Statement: High-level business problem to solve (e.g.¬†Increasing Retention, Conversion Rate, Average Order Value)\n\nCan also be in regards to a metric that‚Äôs believed to be highly associated with a North Star metric like a Primary metric (See KPIs)\n\nOpportunity Areas: Areas or problems with a strong connection to the problem at hand\n\nInvestigate behaviors of users with the behavior that you‚Äôre interested in (i.e.¬†high or low values of the desired metric).\nDiscovering the characteristics of these users can help to figure out ways to encourage other users to act similarily or gain insight into the type of users you want to attract.\n\nLevers: Different ways to work on the opportunity areas\n\nA lever should be data-based and able to be validated on whether working to increase or decrease the lever will lead to a positive solution to the problem statement.\nThere are typically multiple levers for a given opportunity area\n\nThese should be ordered in terms of priority, and priority should be given to the lever that is believed to result in the greatest impact on the opportunity area that will result in the greatest impact on the solution to the problem statement.\n\n\nExperiments [Optional]: Concrete implementation of a specific lever that can help prove/disprove our hypotheses.\n\nOptional but always helpful to convey recommendations and suggestions with concrete ideas for what the team could or should be building.\n\n\nExample\n\n\nProblem Statement: How can we increase daily listening time for premium users in the Spotify app?\n\nHypothesis: Daily Listening Time is strongly connected to retention for premium users and hensce to monthly revenue.\n\nOpportunity Areas:\n\nUsers who use auto-generated playlists have a x% higher daily listening time\nUsers who subscribed to at least 3 podcasts have a x% higher listening time per day than those who did not subscribe to any.\nUsers who listen to audiobooks have a x% higher daily listening time.\n\nLevers:\n\nOpportunity Area: Increase the percentage of users under 25 using audiobooks from x% to y%.\nQuestions:\n\nDo users not see the feature?\nDo users see the feature but don‚Äôt engage with the feature?\nDo users engage with the feature but drop off after a short amount of time?\n\nFinding: Users under 25 engage less with the Home Screen, the only screen where Audiobooks are promoted, and hence don‚Äôt see this feature in the App. This is likely leading low usage and engagement.\nLever: Increase prominence of Audiobooks within the app\nPrioritzation Table for Report\n\n\nExperiments:\n\n\n‚ÄúWe predict that adding a banner promoting Audiobooks when the App opens [Experiment Change] will increase younger users‚Äô daily listening time [Problem] because more younger users will see and listen to Audiobooks [Lever]. We will know this is true when we see an increase in young users using Audiobooks [Lever], followed by an increase in the daily listening time for younger users [Validation Metrics].‚Äù\nIf there is no significant increase in audiobook usage, then there many other ways to increase the visibility of a feature which can be the hypotheses of further experiments.\nIf , however, there is a significant increase in users using Audiobooks (lever) but no effect on daily listening time (main problem), then the lever is invalidated and we can move on to the next one.",
    "crumbs": [
      "Projects",
      "Analyses"
    ]
  },
  {
    "objectID": "qmd/project-analyses.html#sec-proj-anal-edap",
    "href": "qmd/project-analyses.html#sec-proj-anal-edap",
    "title": "Analyses",
    "section": "Exploratory Data Analysis Research Plan",
    "text": "Exploratory Data Analysis Research Plan\n\nNotes from Pluralsight Designing an Exploratory Data Analysis Research Plan\n\nSee code &gt;&gt; rmarkdown &gt;&gt; reports &gt;&gt; edarp-demo.Rmd\n\nDuring the development of the EDARP, all stakeholders can align their expectiations. Buy-in from the aligned stakeholders can help sell the project to the organization.\nEach section should have an introduction with a description about whats in it\nMock Schedule\n\nWeek 1: Data request by a department\nWeek 2: Data Scientist and department meet to formalize the research questions\n\nWorking backwards from the desired output can help frame the right questions to ask during this period\n\nWeek 3: Clear metrics are established. The use case of the product is defined (i.e.¬†who‚Äôs using it and what decisions are to be made). Sponsorship is set. Budgets are allocated.\nWeek 4: EDARP is finalized with everyone understanding the objectves, budget, product design, and product usage\nWeek 6: Data Scientist delivers the product to the department.\n\nSections of the Report\n\nAbstract\n\nHighlights the research questions\nWho the stakeholders are\nMetrics of success\nExample:\n\n‚ÄúThe foundational task was to develop sales insights across stores. Through the identification and inclusion of various business groups, data were gathered and questions were formed. The business groups included are Marketing, IT, Sales and Data Science. From this process we defined the primary goal of this research. This research adds understanding to how sales are driven across stores and develops a predictive model of sales across stores. These outcomes fit within budget and offer an expected ROI of 10%.‚Äù\n\n\nFigures and Tables\n\nOptional depending on audience\nSection where all viz is at\n\nIntroduction\n\nDetailed description of metrics of success\n\nExample\n\nROI 8%\nR2 75%\nInterpretability\n\n\n\nStakeholders\n\nMarketing\n\nList of people\n\nIT\nSales\nData Science\n\nBudget and Financial Impact\n\nNot always known, but this section is valuable if you‚Äôre able to include it.\nPotential vendor costs\nInfrastructure costs\nApplication developement\nFinancial impact, completed by finance team, result in an expected ROI of blah%\n\nMethods\n\nData description\nData wrangling\n\nWhat were the variables of interest and why (‚Äúdata wrangling involved looking at trends in sales across stores, store types, and states‚Äù)\n\nAutocorrelation\n\n‚ÄúTesting for autocorrelation was completed leading to insights in seasonality across the stores. We examined by the ACF an PACF metrics in the assessment of autocorrelation‚Äù\n\nClustering\nOutliers\n\nDescription of algorithm comparison and model selection\n\nWords not code or results\nExample\n\nInvolved training and testing regression, random forest,‚Ä¶\nRegression model served as a benchmark comparison across 5 models\nA discussion of interpretability and expected ROI guided the choice of the final model\n\n\n\n\nResults and Discussion\n\n‚ÄúThis section highlights the thought process that went into wrangling the data and building the models. A few of the insights gained in observation of the data are shared. Also, the assessment of the model is discussed at the end of the section.‚Äù\nVisualizing the Data (i.e.¬†EDA viz - descriptive, outliers, clusters)\n\nFigures\nInsights\nRepeat as needed\n\nVariable Importance\nFinal Model\n\nModel Assessment\n\nAlgorithm comparison metrics\nDynamic visual of model output\n\nSimple shiny graph with a user input and a graph\n\ne.g.¬†Choose store number - graph of sales forecast\n\n\n\n\nConclusion\n\nExample:\n\n‚ÄúThe research explored the possibility of building a predictive model to aid in forecasting sales across stores. We found that, given the established metrics of ROI greater than 8%, R-square of greater than .75 and interpretability in the models, this reasearch has resulted in a viable model for the business. Additionally, it was discovered the presence of some outlier phenomena in the data which has been identified by the stakeholders as acceptable noise. Further we discovered that there is a latent grouping to the stores across sales, store type and assortment. This insight will be used to guide marketings action in the future.‚Äù\n\n\nAppendix\n\nSchedule of Maintenance\nFuture Research",
    "crumbs": [
      "Projects",
      "Analyses"
    ]
  },
  {
    "objectID": "qmd/project-analyses.html#sec-proj-anal-datmet",
    "href": "qmd/project-analyses.html#sec-proj-anal-datmet",
    "title": "Analyses",
    "section": "Data Meta-Metrics",
    "text": "Data Meta-Metrics\n\nNotes from Data Meta Metrics\nMetrics for categorizing the quality of data being used in your analysis\nYou can be very confident about the methodologies you‚Äôre using to analyze data, but if there are issues with the underlying dataset, you might not be so confident in the results of an analysis or your ability to repeat the analysis.\n\nIdeally, we should be passing this information ‚Äî our confidences and our doubts ‚Äî on to stakeholders alongside any results or reports we share.\n\nUse Cases\n\nConvey the quality of the data and its collection process to technical and non-technical audience\nHelpful for diagnosing the strengths and weaknesses of data storage and collection across multiple departments.\nDevelop a data improvement process with an understanding of what data you do and don‚Äôt have and what you can and can‚Äôt collect.\n\nGood data: You know how and when it‚Äôs collected, it lives in a familiar database, and represents exactly what you expect it to represent.\nLess-Than-Stellar data: Data that comes with an ‚Äúoral history‚Äù and lots of caveats and exceptions when it comes to using it in practice.\n\ne.g.¬†When you ask a department for data and their responses are ‚ÄúAnna needs to download a report with very specific filters from a proprietary system and give you the data‚Äù or ‚ÄúCall Matt and see if he remembers‚Äù\n\nPotential Metrics - The type of metrics you use can depend on the analysis your doing\n\n\nRelevance: Ability to answer the question we were asking of it\nTrustworthiness: Will the data be accurate based on how it was collected, stored, and managed?\nRepeatability: How accessible is this data? Can the ETL process be faithfully reproduced?\n\nSlide Report Examples\n\nGood Data\n\nBad Data\n\n\nWith bad data, notes on why the data is bad are included in the slide.",
    "crumbs": [
      "Projects",
      "Analyses"
    ]
  },
  {
    "objectID": "qmd/post-hoc-analysis-anova.html",
    "href": "qmd/post-hoc-analysis-anova.html",
    "title": "ANOVA",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Post-Hoc Analysis",
      "ANOVA"
    ]
  },
  {
    "objectID": "qmd/post-hoc-analysis-anova.html#sec-phoc-anova-misc",
    "href": "qmd/post-hoc-analysis-anova.html#sec-phoc-anova-misc",
    "title": "ANOVA",
    "section": "",
    "text": "Packages\n\n{car} - Anova function that computes all 3 types of ANOVA table\n\nCan also be applied to glm models to produce Analysis of Deviance tables (e.g.¬†logistic, poisson, etc.)\nThink the other packages wrap this function, so they can be used instead in order to advantage of their plotting, testing conveniences.\n\n{grafify} - ANOVA wrappers, plotting, wrappers for {emmeans}\n{afex} - Analysis of Factorial EXperiments\n\nANOVA helper functions that fit the lm, center, apply contrasts, etc. in one line of code\n\nExample: afex::aov_car(Y ~ group * condition + Error(id), data = d)\nType III used, Factor variables created, Sum-to-Zero contrast is applied\n\nEffect plotting functions\n\n\nNotes from\n\nEverything You Always Wanted to Know About ANOVA\n\nANOVA vs.¬†Regression (GPT-3.5)\n\nDifferent Research Questions:\n\nANOVA is typically used when you want to compare the means of three or more groups to determine if there are statistically significant differences among them. It‚Äôs suited for situations where you‚Äôre interested in group-level comparisons (e.g., comparing the average test scores of students from different schools).\nRegression, on the other hand, is used to model the relationship between one or more independent variables and a dependent variable. It‚Äôs suitable for predicting or explaining a continuous outcome variable.\n\nData Type:\n\nANOVA is traditionally used with categorical independent variables and a continuous dependent variable. It helps assess whether the categorical variable has a significant impact on the continuous variable.\n\nThere are other variants such as ANCOVA (categorical and continuous IVs) and Analysis of Deviance (discrete outcome)\n\nRegression can be used with both categorical and continuous independent variables to predict a continuous dependent variable or to examine the relationship between variables.\n\nMultiple Factors:\n\nANOVA is designed to handle situations with multiple categorical independent variables (factors) and their interactions. It is useful when you are interested in understanding the combined effects of several factors.\nRegression can accommodate multiple independent variables as well, but it focuses on predicting the value of the dependent variable rather than comparing groups.\n\nHypothesis Testing:\n\nANOVA tests for differences in means among groups and provides p-values to determine whether those differences are statistically significant.\nRegression can be used for hypothesis testing, but it‚Äôs more often used for estimating the effect size and making predictions.\n\nAssumptions:\n\nANOVA assumes that the groups are independent and that the residuals (the differences between observed values and group means) are normally distributed and have equal variances.\nRegression makes similar assumptions about residuals but also assumes a linear relationship between independent and dependent variables.",
    "crumbs": [
      "Post-Hoc Analysis",
      "ANOVA"
    ]
  },
  {
    "objectID": "qmd/post-hoc-analysis-anova.html#sec-phoc-anova-gen",
    "href": "qmd/post-hoc-analysis-anova.html#sec-phoc-anova-gen",
    "title": "ANOVA",
    "section": "General",
    "text": "General\n\nFamily of procedures which summarizes the relationship between the underlying model and the outcome by partitioning the variation in the outcome into components which can be uniquely attributable to different sources according to the law of total variance.\nEssentially, each of the model‚Äôs terms is represented in a line in the ANOVA table which answers the question how much of the variation in Y can be attributed to the variation in X?\n\nWhere applicable, each source of variance has an accompanying test statistic (oftenF), sometimes called the omnibus test, which indicates the significance of the variance attributable to that term, often accompanied by some measure of effect size.\n\nOne-Way ANOVA - 1 categorical, independent variable\n\nDetermines whether there is a statistically significant difference in the means of the dependent variable across the different levels of the independent variable.\nExample: A researcher wants to compare the average plant height grown using three different types of fertilizer. They would use a one-way ANOVA to test if there is a significant difference in height between the groups fertilized with each type.\n\nTwo-Way ANOVA - 2 categorical, independent variables\n\nExample: 3 treatments are given to subjects and the researcher thinks that females and males will have different responses in general.\n\nTest whether there are treatment differences after accounting for sex effects\nTest whether there are sex differences after accounting for treatment effects\nTest whether the treatment effect is different for females and males if you allow the treatment \\(\\times\\) sex interaction to be in the model\n\n\nTypes\n\nTL;DR;\n\nI don‚Äôt see a reason not to run type III every time.\nType I: Sequential Attribution of Variation\nType II: Simultaneous Attribution of Variation\n\nFor interactions: Sequential-Simultaneous Attribution of Variation\n\nType III: Simultaneous Attribution of Variation for Main Effects and Interactions\nIf the categorical explanatory variables in the analysis are balanced, then all 3 types will give the same results. The results for each variable will be it‚Äôs unique contribution.\n\nExample:\n# balanced\ntable(d$Rx, d$condition)\n#&gt;           Ca Cb\n#&gt;   Placebo  5  5\n#&gt;   Dose100  5  5\n#&gt;   Dose250  5  5\n\n# imbalanced\ntable(d$group, d$condition)\n#&gt;      Ca Cb\n#&gt;   Gb  6  6\n#&gt;   Ga  5  6\n#&gt;   Gc  4  3\n\n\nType I: Sequential Sum of Squares\n\nVariance attribution is calculated sequentially so the order of variables in the model matters. Each term is attributed with a portion of the variation (represented by its SS) that has not yet been attributed to any of the previous terms.\nRarely used in practice because the order in which variation is attributed isn‚Äôt usually important\nExample: Order of terms matters\nanova(lm(Y ~ group + X, data = d))\n#&gt; Analysis of Variance Table\n#&gt; \n#&gt; Response: Y\n#&gt;           Df  Sum Sq Mean Sq F value   Pr(&gt;F)   \n#&gt; group      2    8783    4391  0.0918 0.912617   \n#&gt; X          1  380471  380471  7.9503 0.009077 **\n#&gt; Residuals 26 1244265   47856                    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(lm(Y ~ X + group, data = d))\n#&gt; Analysis of Variance Table\n#&gt; \n#&gt; Response: Y\n#&gt;           Df  Sum Sq Mean Sq F value  Pr(&gt;F)  \n#&gt; X          1  325745  325745  6.8067 0.01486 *\n#&gt; group      2   63509   31754  0.6635 0.52353  \n#&gt; Residuals 26 1244265   47856                  \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nSum of Squares values change based on the order of the terms in the model\nIn the first model,\n\nThe effect of group does not represent its unique contribution to Y‚Äôs variance, but instead its total contribution.\n\nThis reminds me of a dual path DAG where group is influenced by X. Here X‚Äôs variance contribution is included in group‚Äôs contribution since X is not conditioned upon. (See Causal Inference &gt;&gt; Dual Path DAGs)\n\nThe effect of X represents only what X explains after removing the contribution of group ‚Äî the variance attributed to X is strictly the variance that can be uniquely attributed to X, controlling for group\n\n\n\nType II: Simultaneous Sum of Squares\n\nThe variance attributed to each variable is its unique contribution ‚Äî variance after controlling for the other variables. Order of terms does not matter.\nExample\ncar::Anova(m, type = 2)\n#&gt; Anova Table (Type II tests)\n#&gt; \n#&gt; Response: Y\n#&gt;            Sum Sq Df F value   Pr(&gt;F)   \n#&gt; group       63509  2  0.6635 0.523533   \n#&gt; X          380471  1  7.9503 0.009077 **\n#&gt; Residuals 1244265 26                    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nSum of Squares values are equal to values of the Type 1 results when each variable is last.\nNote that factor variables, e.g.¬†group, are treated as 1 term and not broken down into dummy variables for each level.\n\nWith interactions, the method of calculation could be called, Sequential-Simultaneous.\n\nTerms are evaluated simultaneously in groups based on type of term, e.g.¬†main effects, 2-way interactions, 3-way interactions, etc., but sequentially according to the order of that term where the order of main effects &lt; 2-way interactions &lt; 3-way interactions, etc.\nAll main effects (1st order) are tested simultaneously (accounting for one another), then all 2-way interactions (2nd order) are tested simultaneously (accounting for the main effects and one another), and finally the 3-way interaction is tested (accounting for all main effects and 2-way interactions).\nSo, if you use this way to test a model with interactions, only the highest order term‚Äôs Sum of Squares represents a unique variance contribution.\n\n\nType III: Simultaneous-Simultaneous Sum of Squares\n\nThe Sum-of-Squares for each main effect and interaction is calculated as its unique contribution (i.e.¬†takes into account all other terms of the model).\nUnlike Type II, it allows you compare variance contributions for every term in your model.\nWithout centering continuous variables and applying sum-to-zero contrasts to categorical variables, tests results can change depending on the categorical level of the moderator. (Also see Regression, Linear &gt;&gt; Contrasts &gt;&gt; Sum-to-Zero)\n\nExample\n\nNo Centering, No Sum-to-Zero Contrasts\nm_int &lt;- lm(Y ~ group * X, data = d)\n\nd$group &lt;- relevel(d$group, ref = \"Gb\")\nm_int2 &lt;- lm(Y ~ group * X, data = d)\n\ncar::Anova(m_int, type = 3)\n#&gt;             Sum Sq Df F value    Pr(&gt;F)    \n#&gt; (Intercept) 538630  1 22.9922 6.994e-05 ***\n#&gt; group       738108  2 15.7536 4.269e-05 ***\n#&gt; X           101495  1  4.3325   0.04823 *  \n#&gt; group:X     682026  2 14.5566 7.246e-05 ***\n#&gt; Residuals   562240 24     \n\ncar::Anova(m_int2, type = 3)\n#&gt;             Sum Sq Df F value    Pr(&gt;F)    \n#&gt; (Intercept) 219106  1  9.3528  0.005402 ** \n#&gt; group       738108  2 15.7536 4.269e-05 ***\n#&gt; X           910646  1 38.8722 1.918e-06 ***\n#&gt; group:X     682026  2 14.5566 7.246e-05 ***\n#&gt; Residuals   562240 24  \n\nThe sum of squares and p-value change for X when the categorical variable‚Äôs reference level changed which shouldn‚Äôt matter given this is an omnibus test (i.e.¬†the categorical variable is treated as 1 entity and not set of dummy variables).\n\nCentered, Sum-to-Zero Contrasts Applied\n# center, contr.sum\nd_contr_sum &lt;- d |&gt; \n  mutate(X_c = scale(X, scale = FALSE))\ncontrasts(d_contr_sum$group) &lt;- contr.sum\nm_int_cont_sum &lt;- lm(Y ~ group * X_c, data = d_contr_sum)\ncar::Anova(m_int_cont_sum, type = 3)\n#&gt;              Sum Sq Df  F value    Pr(&gt;F)    \n#&gt; (Intercept) 4743668  1 202.4902 3.401e-13 ***\n#&gt; group         19640  2   0.4192   0.66231    \n#&gt; X_c          143772  1   6.1371   0.02067 *  \n#&gt; group:X_c    682026  2  14.5566 7.246e-05 ***\n#&gt; Residuals    562240 24\n\n# change reference level\nd_rl &lt;- d_contr_sum |&gt; \n  mutate(group_rl = relevel(group, ref = \"Gb\"))\ncontrasts(d_rl$group_rl) &lt;- contr.sum\ncar::Anova(lm(Y ~ group_rl * X_c, data = d_rl),\n           type = 3)\n#&gt;               Sum Sq Df  F value    Pr(&gt;F)    \n#&gt; (Intercept)  4743668  1 202.4902 3.401e-13 ***\n#&gt; group_rl       19640  2   0.4192   0.66231    \n#&gt; X_c           143772  1   6.1371   0.02067 *  \n#&gt; group_rl:X_c  682026  2  14.5566 7.246e-05 ***\n#&gt; Residuals     562240 24   \n\nNow when the reference level is changed, the sum-of-squares and p-value for X remain the same.",
    "crumbs": [
      "Post-Hoc Analysis",
      "ANOVA"
    ]
  },
  {
    "objectID": "qmd/post-hoc-analysis-anova.html#sec-phoc-anova-ass",
    "href": "qmd/post-hoc-analysis-anova.html#sec-phoc-anova-ass",
    "title": "ANOVA",
    "section": "Assumptions",
    "text": "Assumptions\n\nEach group category has a normal distribution.\nEach group category is independent of each other and identically distributed (iid)\nGroup categories have of similar variance (i.e.¬†homoskedastic variance)\n\nIf this is violated\n\nIf the ratio of the largest variance to the smallest variance is less than 4, then proceed with one-way ANOVA (robust to small differences)\nIf the ratio of the largest variance to the smallest variance is greater than 4, perform a Kruskal-Wallis test. This is considered the non-parametric equivalent to the one-way ANOVA. (example)\n\nEDA\ndata %&gt;%\n¬† group_by(program) %&gt;%\n¬† summarize(var=var(weight_loss))\n#&gt; A tibble: 3 x 2\n#&gt; ¬† program¬† var¬† ¬†\n#&gt; 1 A¬† ¬† ¬† 0.819\n#&gt; 2 B¬† ¬† ¬† 1.53¬†\n#&gt; 3 C¬† ¬† ¬† 2.46\nPerform a statisical test to see if these variables are statistically significant (See Post-Hoc Analysis, Difference-in-Means &gt;&gt; EDA &gt;&gt; Tests for Equal Variances)",
    "crumbs": [
      "Post-Hoc Analysis",
      "ANOVA"
    ]
  },
  {
    "objectID": "qmd/post-hoc-analysis-anova.html#sec-phoc-anova-math",
    "href": "qmd/post-hoc-analysis-anova.html#sec-phoc-anova-math",
    "title": "ANOVA",
    "section": "Mathematics",
    "text": "Mathematics\n\nAsides:\n\nThis lookd like the variance formula except for not dividing by the sample size to get the ‚Äúaverage‚Äù squared distance\nSSA formula - the second summation just translates to multiplying by ni, the group category sample size, since there is no j in that formula\n\nCalculate SSA and SSE\n\\[\n\\begin{align}\n\\text{SST} &= \\text{SSA} + \\text{SSE} \\\\\n&= \\sum_{i = 1}^a \\sum_{j=i}^{n_i} (x_{i,j} - \\mu)^2 \\\\\n&= \\sum_{i = 1}^a \\sum_{j=i}^{n_i} (\\bar x_i - \\mu)^2 + \\sum_{i = 1}^a \\sum_{j=i}^{n_i} (x_{i,j} - \\bar x_i)^2\n\\end{align}\n\\]\n\n\\(\\text{SST}\\): Sum of Squares Total\n\\(\\text{SSA}\\): Sum of Squares between categories, treatments, or factors\n\n‚ÄúA‚Äù stands for attributes (i.e.¬†categories)\n\n\\(\\text{SSE}\\): Sum of Squares of Errors; randomness within categories, treatments, or factors\n\\(x_{ij}\\): The jth observation of the ith category\n\\(\\bar x_i\\): The sample mean of category i\n\\(\\mu\\): The overall sample mean\n\\(n_i\\): The group category sample size\n\\(a\\): The number of group categories\n\nCalculate MSA and MSE\n\\[\n\\begin{align}\n\\text{MSE} &= \\frac{\\text{SSE}}{N-a} \\\\\n\\text{MSA} &= \\frac{\\text{SSA}}{a-1}\n\\end{align}\n\\]\n\nWhere N is the total sample size\n\nCalculate the F statistic and P-Value\n\\[\nF = \\frac{\\text{MSA}}{\\text{MSE}}\n\\]\n\nFind the p-value (need a table to look it up)\nIf our F statistic is less than the critical value F statistic for a \\(\\alpha = 0.05\\) than we cannot reject the null hypothesis (no statistical difference between categories)\n\nDiscussion\n\nIf there is a group category that has more variance than the others‚Äô attribute error (SSA), we should then pick that up when we compare it to the random error (SSE)\n\nIf a group is further away from the overall mean, then it will increase SSA and thus influence the overall variance but might not always increase random error",
    "crumbs": [
      "Post-Hoc Analysis",
      "ANOVA"
    ]
  },
  {
    "objectID": "qmd/post-hoc-analysis-anova.html#sec-phoc-anova-diag",
    "href": "qmd/post-hoc-analysis-anova.html#sec-phoc-anova-diag",
    "title": "ANOVA",
    "section": "Diagnostics",
    "text": "Diagnostics\n\nEta Squared\n\nMetric to describe the effect size of a variable\nRange: [0, 1]; values closer to 1 indicating that a specific variable in the model can explain a greater fraction of the variation\nlsr::etaSquared(anova_model) (use first column of output)\nGuidelines\n\n0.01: Effect size is small.\n0.06: Effect size is medium.\nLarge effect size if the number is 0.14 or above\n\n\nPost-ANOVA Tests\n\nAssume approximately Normal distributions\nFor links to more details about each test, https://www.statisticshowto.com/probability-and-statistics/statistics-definitions/post-hoc/\nDuncan‚Äôs new multiple range test (MRT)\n\nWhen you run Analysis of Variance (ANOVA), the results will tell you if there is a difference in means. However, it won‚Äôt pinpoint the pairs of means that are different. Duncan‚Äôs Multiple Range Test will identify the pairs of means (from at least three) that differ. The MRT is similar to the LSD, but instead of a t-value, a Q Value is used.\n\nFisher‚Äôs Least Significant Difference (LSD)\n\nA tool to identify which pairs of means are statistically different. Essentially the same as Duncan‚Äôs MRT, but with t-values instead of Q values.\n\nNewman-Keuls\n\nLike Tukey‚Äôs, this post-hoc test identifies sample means that are different from each other. Newman-Keuls uses different critical values for comparing pairs of means. Therefore, it is more likely to find significant differences.\n\nRodger‚Äôs Method\n\nConsidered by some to be the most powerful post-hoc test for detecting differences among groups. This test protects against loss of statistical power as the degrees of freedom increase.\n\nScheff√©‚Äôs Method\n\nUsed when you want to look at post-hoc comparisons in general (as opposed to just pairwise comparisons). Scheffe‚Äôs controls for the overall confidence level. It is customarily used with unequal sample sizes.\n\nTukey‚Äôs Test\n\nThe purpose of Tukey‚Äôs test is to figure out which groups in your sample differ. It uses the ‚ÄúHonest Significant Difference,‚Äù a number that represents the distance between groups, to compare every mean with every other mean.\n\nDunnett‚Äôs Test\n\nLike Tukey‚Äôs this post-hoc test is used to compare means. Unlike Tukey‚Äôs, it compares every mean to a control mean.\n{DescTools::DunnettTest}\n\nBenjamin-Hochberg (BH) Procedure\n\nIf you perform a very large amount of tests, one or more of the tests will have a significant result purely by chance alone. This post-hoc test accounts for that false discovery rate.",
    "crumbs": [
      "Post-Hoc Analysis",
      "ANOVA"
    ]
  },
  {
    "objectID": "qmd/post-hoc-analysis-anova.html#sec-phoc-anova-oneway",
    "href": "qmd/post-hoc-analysis-anova.html#sec-phoc-anova-oneway",
    "title": "ANOVA",
    "section": "One-Way",
    "text": "One-Way\n\nMeasures if there‚Äôs a difference in means between any group category\nExample: 1 control, 2 Test groups\n\nData\ndata &lt;- data.frame(Group = rep(c(\"control\", \"Test1\", \"Test2\"), each = 10),\nvalue = c(rnorm(10), rnorm(10),rnorm(10)))\ndata$Group&lt;-as.factor(data$Group)\nhead(data)\n#&gt; ¬† Group¬† ¬† ¬† value\n#&gt; 1 control¬† 0.1932123\n#&gt; 2 control -0.4346821\n#&gt; 3 control¬† 0.9132671\n#&gt; 4 control¬† 1.7933881\n#&gt; 5 control¬† 0.9966051\n#&gt; 6 control¬† 1.1074905\nFit model\nmodel &lt;- aov(value ~ Group, data = data)\nsummary(model)\n#&gt; ¬† ¬† ¬† ¬† ¬† ¬† Df¬† ¬† Sum Sq¬†  Mean Sq¬† F value¬† Pr(&gt;F)¬†\n#&gt; Group¬† ¬† ¬† ¬† 2¬† ¬†  4.407¬† ¬† 2.2036¬† ¬†  3.71¬† 0.0377 *\n#&gt; Residuals¬†  27¬† ¬† 16.035¬† ¬† 0.5939\n\n# or\nlm_mod &lt;- lm(value ~ Group, data = data)\nanova(lm_mod)\n\nP-Value &lt; 0.05 says at least 1 group category has a statistically significant different mean from another category\n\nDunnett‚Äôs Test\nDescTools::DunnettTest(x=data$value, g=data$Group)\n\n#&gt; Dunnett's test for comparing several treatments with a control :¬†\n#&gt; ¬† ¬† 95% family-wise confidence level\n#&gt; $control\n#&gt; ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† diff¬† ¬† lwr.ci¬† ¬† ¬† upr.ci¬† pval¬† ¬†\n#&gt; Test1-control -0.8742469 -1.678514 -0.06998022 0.0320 *¬†\n#&gt; Test2-control -0.7335283 -1.537795¬† 0.07073836 0.0768 .\n\nMeasures if there is any difference between treatments and the control\nThe mean score of the test1 group was significantly higher than the control group. The mean score of the test2 group was not significantly higher than the control group.\n\nTukey‚Äôs HSD\nstats::TukeyHSD(model, conf.level=.95)\n\nMeasures difference in means between all categories and each other",
    "crumbs": [
      "Post-Hoc Analysis",
      "ANOVA"
    ]
  },
  {
    "objectID": "qmd/post-hoc-analysis-anova.html#sec-phoc-anova-ancova",
    "href": "qmd/post-hoc-analysis-anova.html#sec-phoc-anova-ancova",
    "title": "ANOVA",
    "section": "ANCOVA",
    "text": "ANCOVA\n\nAnalysis of Covariance is used to measure the main effect and interaction effects of categorical variables on a continuous dependent variable while controlling the effects of selected other continuous variables which co-vary with the dependent variable.\nMisc\n\nAnalysis of covariance is classical terminology for linear models but we often use the term for nonlinear models (Harrell)\nSee also\n\nHarrell - Biostatistics for Biomedical Research Ch. 13\n\n\nAssumptions\n\nIndependent observations (i.e.¬†random assignment, avoid is having known relationships among participants in the study)\nLinearity: the relation between the covariate(s) and the dependent variable must be linear.\nNormality: the dependent variable must be normally distributed within each subpopulation. (only needed for small samples of n &lt; 20 or so)\nHomogeneity of regression slopes: the beta-coefficient(s) for the covariate(s) must be equal among all subpopulations. (regression lines for these individual groups are assumed to be parallel)\n\nFailure to meet this assumption implies that there is an interaction between the covariate and the treatment.\nThis assumption can be checked with an F test on the interaction of the independent variable(s) with the covariate(s).\n\nIf the F test is significant (i.e., significant interaction) then this assumption has been violated and the covariate should not be used as is.\nA possible solution is converting the continuous scale of the covariate to a categorical (discrete) variable and making it a subsequent independent variable, and then use a factorial ANOVA to analyze the data.\n\n\nThe covariate (adjustment variable) and the treatment are independent\nmodel &lt;- aov(grade ~ technique, data = data)\nsummary(model)\n\n#&gt; ¬† ¬† ¬† ¬† ¬† ¬† Df Sum Sq Mean Sq F value Pr(&gt;F)\n#&gt; technique¬† ¬† 2¬† ¬† 9.8¬† ¬† 4.92¬† ¬† 0.14¬† 0.869\n#&gt; Residuals¬† 87 3047.7¬† 35.03\n\nH0: variables are independent\n\n\nHomogeneity of variance: variance of the dependent variable must be equal over all subpopulations (only needed for sharply unequal sample sizes)\n# response ~ treatment\nleveneTest(exam ~ technique, data = data)\n\n#&gt; ¬† ¬† ¬† Df F value¬† ¬† Pr(&gt;F)¬† ¬†\n#&gt; group¬† 2¬† 13.752 6.464e-06 ***\n#&gt; ¬† ¬† ¬† 87\n\n# alt test\nfligner.test(size ~ location, my.dataframe)\n\nH0: Homogeneous variance\nThis one fails\n\nFit\nancova_model &lt;- aov(exam ~ technique + grade, data = data)\ncar::Anova(ancova_model, type=\"III\")\n\n#&gt; ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† Sum Sq Df F value¬† ¬† Pr(&gt;F)¬† ¬†\n#&gt; ¬† ¬† (Intercept) 3492.4¬† 1 57.1325 4.096e-11 ***\n#&gt; ¬† ¬† technique¬† 1085.8¬† 2¬† 8.8814 0.0003116 ***\n#&gt; ¬† ¬† grade¬† ¬† ¬† ¬† ¬† 4.0¬† 1¬† 0.0657 0.7982685¬† ¬†\n#&gt; ¬† ¬† Residuals¬† 5257.0 86\n\nWhen adjusting for current grade (covariate), study technique (treatment) has a significant effect on the final exam score (response).\n\nDoes the effect differ by treatment\npostHocs &lt;- multicomp::glht(ancova_model, linfct = mcp(technique = \"Tukey\"))\nsummary(postHocs)\n\n#&gt; ¬† ¬† ¬† ¬† ¬† ¬† Estimate Std. Error t value Pr(&gt;|t|)¬† ¬†\n#&gt; B - A == 0¬†  -5.279¬† ¬† ¬† 2.021¬† -2.613¬† 0.0284 *¬†\n#&gt; C - A == 0¬† ¬† 3.138¬† ¬† ¬† 2.022¬†  1.552¬† 0.2719¬† ¬†\n#&gt; C - B == 0¬† ¬† 8.418¬† ¬† ¬† 2.019¬†  4.170¬† &lt;0.001 ***\n\nAlso see Post-Hoc Analysis, Multilevel &gt;&gt; Tukey‚Äôs Test\n\\(A\\), \\(B\\), and \\(C\\) are the study techniques (treatment)\nSignificant differences between \\(B\\) and \\(A\\) and a pretty large difference between \\(B\\) and \\(C\\).\n\nExample: RCT\n\\[\n\\begin{align}\n\\text{post}_i &\\sim \\mathcal{N}(\\mu_i, \\sigma_\\epsilon)\\\\\n\\mu_i &= \\beta_0 + \\beta_1 \\text{tx}_i + \\beta_2 \\text{pre}_i\n\\end{align}\n\\]\nw2 &lt;- glm(\n¬† data = dw,\n¬† family = gaussian,\n¬† post ~ 1 + tx + pre)\n\nSpecification\n\npost, pre: The post-treatment and pre-treatment measurement of the outcome variable\ntx: The treatment indicator variable\n\\(\\beta_0\\): Population mean for the outcome variable in the control group\n\\(\\beta_1\\): Parameter is the population level difference in pre/post change in the treatment group, compared to the control group.\n\nAlso a causal estimate for the average treatment effect (ATE) in the population, œÑ\n\nBecause pre is added as a covariate, both \\(\\beta_0\\) and \\(\\beta_1\\) are conditional on the outcome variable, as collected at baseline before random assignment.",
    "crumbs": [
      "Post-Hoc Analysis",
      "ANOVA"
    ]
  },
  {
    "objectID": "qmd/project-development.html",
    "href": "qmd/project-development.html",
    "title": "Development",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Projects",
      "Development"
    ]
  },
  {
    "objectID": "qmd/project-development.html#sec-proj-dev-misc",
    "href": "qmd/project-development.html#sec-proj-dev-misc",
    "title": "Development",
    "section": "",
    "text": "Notes from\n\nAdapting Project Management Methodologies to Data Science\n\nAlong with overview of various methodologies, provides list of Agile foundational values and key principles\n\n\nLoose implementation of CRISP-DM with agile-based practices is recommended\nWaterfall or the newer variation with feedback loops between adjoining stages should not be used\n\nDesigned for manufacturing and construction where the progressive movement of a project is sequential. DS typically requires a lot of experimentation and a modification of requirements.\nAlthough can be useful for certain stages of the data science project such as planning, resource management, scope, and validation\n\nPrior to full deployment, run a pilot deployment\n\nOnly a few groups are given permission to use the product\nReceive feedback (e.g.¬†weekly meetings), fix bugs, and make changes\n\nAfter full deployment\n\nHave an education and training session for users\n\nNote problem areas. These may be potential next steps to improving the product\n\nCheck-in periodically with users to get feedback\n\nProtyping and Testing\n\nSee Lean Data Science\n\nThe idea is to build things that deliver value quickly\n\nIterative Building Steps\n\nBuild ‚Äògood enough‚Äô versions of the tool or project (MVPs)\nGive these to stakeholders to use and get feedback\nIncorporate feedback\nReturn to stakeholders to use and get more feedback\nIterate until project the stakeholder and you feel it has reached production-level\n\nBreak each project down into a set of smaller projects\n\nExample:\n\nMVP to test if the idea is feasible\nMore functional version of the MVP\nProductionized version of the product.\n\nTrack the impact of each of the sub-projects that comprise the larger projects\nAt each of these milestones, decide on whether to progress further on a project by using taking the impact score of the subproject into account\n\nExample: rule-based chatbot manages to\n\nChatbot: successfully helps 10,000 customers a month\n\n10,000 customers ‚®Ø 3 min average call = 30,000 mins = 500 hours.\n\nCall Center Agent\n\nCall-center agent‚Äôs time costs $200/hr in terms of salary and infrastructure,\n\nConclusion: MVP chatbot saves $100K a month and you could likely save even more with a more sophisticated chatbot.",
    "crumbs": [
      "Projects",
      "Development"
    ]
  },
  {
    "objectID": "qmd/project-development.html#sec-proj-dev-comms",
    "href": "qmd/project-development.html#sec-proj-dev-comms",
    "title": "Development",
    "section": "Communication",
    "text": "Communication\n\nRemind stakeholders of what it is we agreed in last meeting you‚Äôd do, what you did and how to interpret the results\nState what it is you need from the stakeholder.\nState whether the project progress in the middle, at it‚Äôs end, you‚Äôre wrapping up or what‚Äôs going on?\nA summary slide or results peppered with comments leading me through what it is I am looking at\nThe Minto Pyramid\n\nOrganize the message so that it starts with a conclusion which leads to the arguments that support it and ends in detailed information.\nProcess\n\nWrite conclusion (2-3 sentences max)\nSupporting arguments: Try to make them concise bullet points\nLink to a more detailed explanation at the bottom if need be\n\n\nMight be useful to time the arrival when the stakeholder is most likely able to read it.\n\ne.g.¬†If a stakeholder has a meeting at 9:30 every morning, it may be better to time the sending of the report to before or after that meeting.",
    "crumbs": [
      "Projects",
      "Development"
    ]
  },
  {
    "objectID": "qmd/project-development.html#sec-proj-dev-kanban",
    "href": "qmd/project-development.html#sec-proj-dev-kanban",
    "title": "Development",
    "section": "Kanban",
    "text": "Kanban\n\n\nPhysical or digital board where tasks are then outlined as story cards.\nEvery card will be extracted from left to right until it is completed.\nflexibility to execute tasks without getting constant deadlines\nMisc\n\nSeems like this could be used within a sprint (columns would have to be defined according to the sprint plan)\n\nAdvantages\n\nbottlenecks, overworked steps, etc. easily identified\neffective at communicating the work in progress for stakeholders and team members\noriented towards individual tasks instead of batches like in scrums\n\nDisadvantages\n\nlack of deadlines can lead to longer project times\nchallenging to define the columns for a data science Kanban board\nCustomer interaction is undefined. As such, customers may not feel dedicated to the process without the structured cadence of sprint reviews",
    "crumbs": [
      "Projects",
      "Development"
    ]
  },
  {
    "objectID": "qmd/project-development.html#sec-proj-dev-crisp",
    "href": "qmd/project-development.html#sec-proj-dev-crisp",
    "title": "Development",
    "section": "CRISP-DM",
    "text": "CRISP-DM\n\n\nCross-Industry Standard Process for Data Mining\nDefacto standard for data mining\nSupports replication, generalizable to any DS project\nPhases (not all are sequential, some phases are iterative):\n\n\n\nBusiness-Science Version\n\n\nProcess\n\nView Business as a Machine\n\nIsolating business units\n\nInternal: Sales, Manufacturing, Accounting, etc\nExternal: customers, suppliers\nVisualizing the connections\n\nDefining objectives\nCollecting outcomes\n\nUnderstand The Drivers\n\nInvestigate if objectives are being met\nSynthesize outcomes\nHypothesize drivers\n\nAt this stage, it‚Äôs critical to meet with subject-matter experts (SMEs). These are people in the organization that are close to process and customers. We need to understand what are the potential drivers of lead time. Form a general equation that they help create.\n\n\nMeasure Drivers\n\nCollect Data\n\nCollect data related to the high level drivers. This data could be stored in databases or it may need to be collected. We could collect competitor data, supplier data, sales data (Enterprise Resource Planning or ERP data), personnel data, and more.\nMay require effort to set up processes to collect it, but developing strategic data sources becomes a competitive advantage over time.\n\nDevelop KPIs\n\nRequires knowledge of customers and industry. Realize that a wealth of data is available outside of your organization. Learn where this data resides, and it becomes a tremendous asset.\n\n\nUncover Problems And Opportunities\n\nEvaluate performance vs KPIs\nHighlight potential problem areas\nReview the our project for what could have been missed\n\nTalk with SME‚Äôs to make sure they agree with your findings so far.\n\n\nEncode Decision Making Algorithms\n\nDevelop algorithms to predict and explain the problem\nOptimize decisions to maximize profit\n\ne.g.¬†For classification, threshold optimization using a custom cost function to optimize resources, costs, precision, and recall (See Diagnostics, Classification &gt;&gt; Scores &gt;&gt; Custom Cost Functions\n\nUse systematic decision-making algorithms to improve decision making\n\nExample: Employee Churn\n\n\nApp uses LIME to get prediction-level feature importance\n\n\nMeasure The Results\n\nCapture outcomes\nSynthesize results\nVisualize outcomes over time\n\nWe are looking for progress. If we have experienced good outcomes, then we need to recognize what contributed to those good outcomes.\nQuestions\n\nWere the decision makers using the tools?\nDid they follow the systematic recommendation?\nDid the model accurately predict risk?\nWere the results poor? Same questions apply.\n\n\n\nReport Financial Impact\n\nMeasure actual results\nTie to financial benefits\nReport financial benefit to key stakeholders\n\nIt‚Äôs insufficient to say that we saved 75 employees or 75 customers. Rather, we need to say that the average cost of a lost employee or lost customer is $100,000 per year, so we just saved the organization $7.5M/year. Always report as a financial value.\n\n\n\nExample: Customer Churn\n\nView business as a machine\n\nIsolating business units: The interaction occurs between Sales and the Customer\nDefining objectives: Make customers happy\nCollecting outcomes: We are slowly losing customers. It‚Äôs lowering revenue for the organization $500K per year.\n\nUnderstand The Drivers\n\nInvestigate if objectives are being met\n\nCustomer Satisfaction: Loss of customers generally indicates low satisfaction. This could be related to availability of products, poor customer service, or competition offering lower prices and/or better service or quality.\n\nSynthesize outcomes\n\nCustomers are leaving for a competitor. In speaking with Sales, several customers have stated ‚ÄúCompetition has faster delivery‚Äù. This is an indicator that lead time, or the ability to quickly service customers, is not competitive.\n\nHypothesize Drivers\n\nLead time is related to supplier delivery, inventory availability, personnel, and the scheduling process.\n\n\nMeasure Drivers\n\nAverage Lead Time: The level is 2-weeks, which is based on customer feedback on competitors.\nSupplier Average Lead Time: The level is 3 weeks, which is based on feedback related to our competitor‚Äôs suppliers.\nInventory Availability Percentage: The level of 90% is related based on where customers are experiencing unmet demand. This data comes from the ERP data comparing sale requests to product availability.\nPersonnel Turnover: The level of 15% is based on the industry averages.\n\nUncover Problems and Opportunities\n\nOur average lead time is 6 weeks compared to the competitor average lead time of 2 weeks, which is the first order cause for the customer churn\nOur supplier average lead time is on par with our competitor‚Äôs, which does not necessitate a concern.\nOur inventory percentage availability is 80%, which is too low to maintain a high customer satisfaction level. This could be a reason that churn is increasing.\nOur personnel turnover in key areas is zero over the past 12 months, so no cause for concern.",
    "crumbs": [
      "Projects",
      "Development"
    ]
  },
  {
    "objectID": "qmd/project-development.html#sec-proj-dev-agile",
    "href": "qmd/project-development.html#sec-proj-dev-agile",
    "title": "Development",
    "section": "Agile",
    "text": "Agile\n\nMisc\n\nResources\n\nhttps://www.atlassian.com/agile/project-management/overview\n\nFeatures adaptability, continuous delivery, iteration, and short time frames\n\n\n\nTerms\n\nEpic - Collection of high level tasks that may represent several user stories\n\nHelps to map the model outcome and define the correct stakeholders for the project\nA helpful way to organize your work and to create a hierarchy.\nThe idea is to break work down into shippable pieces so that large projects can actually get done and you can continue to ship value to your customers on a regular basis\nDelivered over a set of sprints\n\nInitiatives - Collections of epics that drive toward a common goal\nProduct Roadmap - Plan of action for how a product or solution will evolve over time\n\nExpressed and visualized as a set of initiatives plotted along a timeline\n\nScrum - A framework that‚Äôs objective is to fulfill customer needs through transparent communication, continuous progress, and collective responsibility\n\nData-Driven Scrum (DDS) - Scrums, as defined, have fixed lengths which can be an issue with DS projects\n\nSprints - Short periodic blocks that make up a scrum\n\nEach usually ranges from 2-4 weeks\nEach sprint is an entity that delivers the full result.\nComposed of a starting point and requirements that complete the project plan\nTheme - an organization goal that drive the creation of epics and initiatives\n\nUser Story - Smallest unit of work or a task; an informal, general explanation of a software feature written from the perspective of the end user. Its purpose is to articulate how a software feature will provide value to the customer.\n\nAfter reading a user story, the team knows why they are building, what they‚Äôre building, and what value it creates.\n\n\n\n\nValues for Data Analysis\n\nDecisions over dashboards: By focusing on what people want to do with data, we can move past the first set of questions they ask, focus on the valuable iteration and follow-up questions, build trust, cultivate curiosity and drive action.\nFunctional analysis over perfect outputs: To enable quick iterations, we‚Äôre going to have to spend less time crafting perfect outputs and focus on moving from one question to the next as quickly as possible.\nSharing data over gatekeeping data: We‚Äôre going to have to share responsibility for our data and data ‚Äúproducts‚Äù with our business partners. This will help build trust, and keep us all accountable for cultivating great data products and data-driven cultures.\nIndividuals and interactions over processes and tools: When in doubt, we need to rely on the relationships we‚Äôve built with the business over the tools we‚Äôve put in to help guide those relationships.\n\n\n\nData Science Lifecycle\n\n\nNotes from TDSP: When Agile Meets Data Science\nIf at any point we are not satisfied with our results or faced with changing requirements we can return to a previous step since this methodology is focused on iterative development\nSteps\n\nBusiness Understanding\n\nDefine objectives: Work with customers/stakeholders to identify the business problem we are trying to solve.\nIdentify data sources: Identify the data sources that we will need to solve it.\n\nData Acquisition and Understanding\n\nIngest the data: Bring the data into our environment that we are using for analytics.\nExplore the data: Exploratory data analysis (EDA) and determinine if it is adequate for model development.\nSet up a data pipeline: Build a process to ingest new data. A data pipeline can either be batch-based, real-time or a hybrid of the previous options.\nNote: While the data scientists on the team are working on EDA, the data engineers may be working on setting up a data pipeline, which allows us to complete this stage quicker\n\nModeling\n\nFeature engineering: Creat data features from raw data for model training.\n\nEnhanced by having a good understanding of the data.\n\nModel training: Split the data into training, validation, and testing sets. Train models\nModel evaluation: Evaluate those models by answering the following questions:\n\nWhat are the metrics that the model achieves on the validation/testing set?\nDoes the model solve the business problem and fit the constraints of the problem?\nIs this model suitable for production?\n\nNote: Could train one model and find that the results are not satisfactory and return to the feature engineering and model training stages to craft better features and try different modeling approaches.\n\nDeployment (Options)\n\nExposing the model through an API that can be consumed by other applications.\nCreating a microservice or containerized application that runs the model.\nIntegrating the model into a web application with a dashboard that displays the results of the predictions.\nCreating a batch process that runs the model and writes the predictions to a data source that can be consumed.\n\nStakeholder/customer acceptance\n\nSystem validation: Confirm that the data pipeline, model, and deployment satisfy the business use case and meet the needs of the customers.\nProject hand-off: Transfer the project to the group that will manage it in production. l\n\n\n\n\n\nProduct Roadmap Examples\n\nExample\n\nInitiative: build a forecast system to predict sales for an ice cream company\nEpics:\n\n‚ÄúAs a Sales Manager, I need to understand which regions I need to focus my outbound effort based on the sales forecast‚Äù\n‚ÄúAs a Logistics Manager, I need to estimate demand so that I can prepare our production accordingly‚Äù\n\nUser Story:\n\n‚ÄúAs a Logistics Manager, I need to see the forecast on my Production Dashboard‚Äù;\n‚ÄúAs a Logistics Manager, I need to have simulations around how weather predictions can change the forecast‚Äù;\n\n\n\n\n\nSprint Workflow\n\n\nBad flow chart, should be a circle where review loops back to planning\nSprint Review - The scrum team and stakeholders review what was accomplished in the sprint, what has changed in their environment, and collaborate on what to do next\n\nThese are necessary to avoiding issues that might destroy a project. (see below)\n\nData scientist participation will help with their communication skills and increase transparency in what they‚Äôre doing\nStakeholders might think a feature or ML result is feasible with the current data and tech stack. These are important to opportunities to explain why they aren‚Äôt feasible.\nRoles often bleed together. The planning portion is a good way to converge on a strategy of what to do next.\n\n\nSprint Planning (~15 minutes every two weeks)\n\nDevelop the next sprint‚Äôs goals\n\nDo the next sprint‚Äôs goals align with our goals in 3 months\nDo the next sprint‚Äôs goals align with our annual team goals/strategic vision\nRevise the next sprint‚Äôs goals to align with these goals if necessary\n\nBreak the sprint goals into tasks and sub tasks\nAssign the tasks/subtasks to members and estimate time to completion of these tasks\nExtended sprint planning (Every 3 months to roughly plan the next 3 months)\nStrategic meetings (6 months)\n\nSome technical details to starting a project\n\nNotes from The Technical Blockers to Turning Data Science Teams Agile\nStart a repo\n\nin the organization acct not under a personal acct\nuse readme as onboarding document\n\nlast person to join is in-charge of it\n\nThe last person will be best suited to edit/add details that clear up any confusion that they had when they were onboarded\nInclude ‚ÄúThis document is maintained by the most recent person to join the team. That person is currently: ____‚Äù\n\nExplicitly state that anyone can review code in your README. If someone isn‚Äôt familiar with a part of the code, they become so by reviewing it.\n\nEdit the settings of your repo. Make the main branch protected, don‚Äôt allow anyone to push directly to the main branch, and only allow PRs that have passed unit-tests (more about this later) and have undergone a code review.\n\nUpdate the team‚Äôs skills related to Agile\n\nIn the beginning, may not have a lot of tasks to assign as there may be design/requirements discussions\nMake sure everyone knows git and how to write unit tests\n\nCheck team members‚Äô personal accts to see how many commits they have, ‚Äúhttps://github.com/search?q=author:‚Äù\nCheck team members‚Äô projects for unit tests\nIf it doesn‚Äôt look like they don‚Äôt have much experience, assign a udemy, etc. course on the subject and require a certificate in order to be assigned tasks\n\n\nAssign tasks through Agile tools like ZenHub, Jira, or Trello\nSet-up a CI tool\n\nexamples: Github Actions, TravisCI, CircleCI, or Jenkins\nadd learning this tool as part of your ZenHub task boards and don‚Äôt allow people to move on until they‚Äôve learned it.\nrun your unit tests every time someone makes a PR\n\nDaily Stand-ups\n\nUsed to discuss what your daily work will be, and it should be short\nProject strategy meeting should be immediately after the stand-up\nEach team member answers only 3 questions:\n\nWhat will you do today?\nWhat did you do yesterday?\n\nRather than a simple verbal status update. It can be better the show what you did.\n\ne.g.¬†show your coding screen and walk everyone through you code\n\nBenefits\n\nSomeone else on the team will have an idea for a better, faster, or simpler way to solve the problem\nEasier to catch a flaw after a few lines of code than after 1000 during a code review\nIf you find out that someone on my team is doing something very similar, and you can save time by reusing code.\nCool to see incremental progress every day instead of just the final product\n\n\nWhat are you blocked by?\nScreen-share these three questions written out on a PowerPoint slide.¬†\n\nCongratulate people on finishing the courses\nAssign a weekly changing role of scrum master\n\nThe scrum master makes sure the 3 questions above are answered by everyone.\nThis person doesn‚Äôt have to be the boss or most senior person.",
    "crumbs": [
      "Projects",
      "Development"
    ]
  },
  {
    "objectID": "qmd/project-planning.html",
    "href": "qmd/project-planning.html",
    "title": "37¬† Planning",
    "section": "",
    "text": "37.1 Misc",
    "crumbs": [
      "Projects",
      "<span class='chapter-number'>37</span>¬† <span class='chapter-title'>Planning</span>"
    ]
  },
  {
    "objectID": "qmd/project-planning.html#sec-proj-plan-misc",
    "href": "qmd/project-planning.html#sec-proj-plan-misc",
    "title": "37¬† Planning",
    "section": "",
    "text": "See Case Study: Applying a Data Science Process Model to a Real-World Scenario A VERY detailed article on the execution of a data science project within a manufacturing company, but can be generalized to other industries.\n  Goes through a scenario of step-by-step planning and execution of changing a manual stock replenishment process to an automated one\n\n  Notes on this article are in [Logistics](Logistics) &gt;&gt; Demand Forecasting &gt;&gt; Misc\nDL model cost calculator (github) (article)\nA clearly defined business problem and targeted success metrics that‚Äôs agreed upon by data scientists and stakeholders are essential before starting a project.\n\nIt should be measurable, clear, and actionable.\nUse the ‚ÄúChallenge Framework‚Äù to solve difficult problems\n\nEvery situation is a function of:\n\nIncentives\nPersonalities\nPerspectives\nConstraints\nResources\n\nIn most ‚Äútough‚Äù situations, 2+ are misaligned. Figure out which and hone in on them.\n\n\nBum, Buy, then Build\n\nBum free solutions while also relaxing quality thresholds.\nLook at buyable options, especially from large, mature organizations that offer low-cost, stable products (with potential discounts if the project is for a non-profit).\nResort to building only if:\n\nIt is far too inefficient to adapt workflows to existing solutions and/or\nThere is an opportunity for reuse by other nonprofits.\n\n\nAdd a buffer\n\nif the business goal is a precision of 95%, you can try tuning your model to an operating point of 96‚Äì97% on the offline evaluation set in order to account for the expected model degradation from data drift.\n\nContracts\n\nOnly promise what is in your power to deliver\n\nExample: A contract with the business stakeholders was to guarantee X% recall on known (historic) data.\n\nIt doesn‚Äôt try to make guarantees about something that the ML team doesn‚Äôt have complete control over: the actual recall in production depends on the amount of data drift, and is not predictable.\n\n\n\nDeliver a Minimally Viable Product (MVP) first.\n\nShould be a product with only the primary features required to get the job done\nThis process with help decide:\n\nhow to implement a more fully fledged product\nwhich additional features might be infeasible or not worth the time and effort to get working\n\n\nFor details on Project/DS Team ROI, see Organizational and Team Development &gt;&gt; Determining a Data Team‚Äôs ROI\nSources of data\n\nInternal resources: Existing historical datasets could be repurposed for new insights.\n\nConsiderations for collecting data\n\nWhether you want to collect qualitative or quantitative data\nThe method for collecting (e.g., surveys, using other reports)\nThe timeframe for the data\nSample size\nOwners of the data\nData sensitivity\nData storage and reporting method (e.g., Salesforce)\nPotential pitfalls or biases in the data (e.g., sample bias, confirmation bias)\n\n\nExternal resources: Governmental organizations, nonprofits, and research institutions have free, accessible datasources that span all different sectors (e.g., agriculture, healthcare, education).\n\nExamples Data.gov\n  _[10 Great Nonprofit Research Resources](https://topnonprofits.com/10-great-nonprofit-research-resources/)_\n\n  _[Forbes ‚Äî 35 Brilliant and Free Datasources](https://www.forbes.com/sites/bernardmarr/2016/02/12/big-data-35-brilliant-and-free-data-sources-for-2016/#5807bf00b54d)_\n\n  _[Springboard ‚Äî Free Public Datasets](https://www.springboard.com/blog/free-public-data-sets-data-science-project/)_\n\n\nOutputs vs Outcomes\n\nLogic model\n\nIt focuses on how inputs and activities translate to outputs and eventually\n\nExample",
    "crumbs": [
      "Projects",
      "<span class='chapter-number'>37</span>¬† <span class='chapter-title'>Planning</span>"
    ]
  },
  {
    "objectID": "qmd/project-planning.html#sec-proj-plan-gensteps",
    "href": "qmd/project-planning.html#sec-proj-plan-gensteps",
    "title": "37¬† Planning",
    "section": "37.2 General Steps for Starting a Project",
    "text": "37.2 General Steps for Starting a Project\n\nFraming a data question\n\nGuidelines\n\nPrecision: Be as detailed as possible in how you frame your questions. Avoid generic words like ‚Äòimprove‚Äô or ‚Äòsuccess.‚Äô If you want to improve something, specify by how much. If you want to achieve something, specify by when.\n\nDecide before starting what the minimum project performance is to productionize (i.e.¬†build a fully functional project) and launch (i.e.¬†deliver to all your customers)\n\nSetting these thresholds at the beginning will help to prevent you from bargaining with yourself or stakeholders to deliver the project that might harm your business\n\nAfter working hard on a project and pouring resources into it, it can be difficult to end it.\n\n\n\nData-Centric: Consider the role of data in your organization. Can data help you answer this question? Is it clear what data you will need to collect to answer this question? Can progress on the task be codified into a metric that can be measured? If the answer to any of these questions is ambiguous or negative, investing in additional data resources may be an inefficient allocation of resources.\n\nExample\n\nFigure Out What Data You Need\n\nGuidelines\n\nNecessity: Are the data you are collecting necessary? Avoid data bloat, which is over-collecting data points for a ‚Äújust in case‚Äù scenario. This makes sustaining long-term data collection of those fields far more burdensome.\nAvailability: Are there external, publicly available data sources like government data that you can leverage? If the data needs to be collected, how easy is it to collect? If it is hard to collect, do you have a plan and resources in place as to how you can ensure it is collected at regular intervals over time? One-off data collection is rarely helpful as there is no reference point to measure the impact of interventions over time.\nMaintainability: Can you maintain and easily update this data over time? Is the cost of doing so sustainable? This is critical because longitudinal data collection with standard fields is one of the most valuable resources for a nonprofit. Avoid constantly changing field names, a moving target of data collection objectives, and costly data collection procedures (like purchasing third-party data) that are not sustainable given your overall budget.\nReliability: If you are using a third party data source, do you trust the quality of the data? What are the ways this data may be biased, incomplete, or inaccurate?\n\nExample\n\nA nonprofit with a mission to find long-term housing for the unhoused. This organization may want to answer the question: ‚Äôwhat percentage of the unhoused have we been able to successfully rehabilitate in the area we serve?‚Äô\n\n\nOrganizational Buy-in\n\nProject manager tries to examine whether the project can fundamentally be classified as feasible and whether the requirements can be carried out with the available resources.\n\nExpert Interviews: Is the problem in general is very well suited for the deployment of data science and are there corresponding projects that have already been undertaken externally and also published?\nData science team: Are there a sufficient number of potentially suitable methods for this project and are the required data sources are available?\nIT department: check the available infrastructure and the expertise of the involved employees.\n\nMake sure everyone on the team agrees on what data that you want to collect and measure and who owns the data collection process.\nHaving someone of authority or that‚Äôs respected in each department involved in the development of the product will go a long way to building trust with users when it‚Äôs fully deployed\n\nExample: Demand Forecasting Automation\n\nTeam should consist of Supply Chain department and close collaboration with Sales and IT\n\n\nExample\n\nSuppose teachers at a school are interested in fielding quantitative surveys to track student outcomes, but there exists little incentive for teachers to collect this data on top of regular work. As a result, only one teacher in the school volunteers to design and administer the survey to their class. However, the survey results will now be limited to the students‚Äô experiences and outcomes for just the one class. The measured outcomes will be biased because they will not capture any variance across students from different classes in the school.\n\n\nCalculate How Much Data You Need to Collect\nMake sure you‚Äôve answered these questions\n\nIs the problem definition clear and specific? Are there measurable criteria that define success or failure?\nIs it technically feasible to address the defined problem within the designated timeframe? Is the data required for the envisioned solution approach available?\nDo all relevant stakeholders agree with the problem definition, performance indicators, and selection criteria?\nDoes the intended technical solution resolve the initial business problem?",
    "crumbs": [
      "Projects",
      "<span class='chapter-number'>37</span>¬† <span class='chapter-title'>Planning</span>"
    ]
  },
  {
    "objectID": "qmd/project-planning.html#sec-proj-plan-wkshp",
    "href": "qmd/project-planning.html#sec-proj-plan-wkshp",
    "title": "37¬† Planning",
    "section": "37.3 Workshopping",
    "text": "37.3 Workshopping\n\nHelps data scientists understand where their energy is most needed\nMisc\n\nNotes from: Successfully Combining Design Thinking and Data Science\nUsually last 1 hr\n\nSupplies\n\nDifferent coloured sticky notes ‚Äî enough so that everyone has at least 15‚Äì20\nWhiteboard markers for the sticky notes. Permanent markers will likely cause some unintended permanent damage and thin, ballpoint pens are difficult to read unless you‚Äôre right up close to them\n1 white board per group, or alternatively, 2 x large A2 pieces of paper\n\nParticipants\n\nKey business stakeholders involved in the area you‚Äôre working on ‚Äî you need management buy-in if anything is going to happen\nTwo or three people who will actually use the tools or insights you‚Äôll be delivering\nA facilitator (probably you) and a few members of your data science or analytics team\n\nChallenge Identification Workshop\n\nSuited for a situation when the business stakeholders know what their challenges are, but don‚Äôt know what you can do for them.\nGoals:\n\nDS: Try to understand the customer pain points as well as you can.\n\nDo not try to develop solutions in this workshop\n\nStakeholders: get as many ideas out as possible in the time allotted.\n\nAdditional Supplies\n\nPoster or slide with user journey, or something similar, depending on the business\n\nHelps to guide participants to home-in on customer pain points\nNeeds to be done before the workshop\nExample\n\n\nProcess\n\nIntroduction\n\nBrief introduction of your team\nBrief 5-minute ‚ÄòBest In Class‚Äô slide or two where I look at companies who are currently doing amazing things, preferably in the data science domain, in the area that our stakeholders work\nAddress goal of the workshop (see above for stakeholders)\n\nSplit into two or three groups\n\nIdeally between four to five people per group\nPut senior managers or executives in separate groups\n1 DS or analyst in each group\n\nWrite as many customer pain points as possible (aka Ideation)\n\nDuration: 25‚Äì30 minutes\nEverybody writes ideas on sticky note and puts on their board\n\nNo need for whole group to approve of the idea.\n\n‚ÄúBad‚Äù ideas are weeded out later\n\n\nIf you see themes popping up, go ahead and group similar sticky-notes together\nDS or analyst needs to pay attention to each proposed idea, so as to be able to write a fuller description of the idea later on\n\nIf possible, note the author of each idea so as to be able to ask them questions later if needed.\n\n(You) Walk around to each group\n\nRemind them of the rules (get down as many ideas as possible)\nPrompt them with questions to get more ideation happening within the groups\n\n\nOne member from each of the groups presents their group‚Äôs key ideas back to the rest of the room\n\nDuration: 3-5 min per group\n\nPlace similar stickies into groups or themes\nVote on the best ideas\n\nDuration: 3 min\nEveryone gets three dot stickers and is allowed to vote for any ideas\nThey can put all their stickies on one idea or divide them up however they like\nNo discussions\nIf it‚Äôs the case that one sticky within a theme of stickies gets all the votes, or even if the votes are all shared equally, consider all of those as votes towards the theme.\n\n\n\nPredefined Problem Sculpting Workshop\n\nThe difference between this approach and the first one is that here the business stakeholders already have an idea about what they need.\n\ne.g a metric of some sort, some kind of customer segmentation or some kind of tool\n\nExample\n\nDevelop ‚Äòaffluency score‚Äô for each banking customer.\n\nGoal: Answer 2 questions\n\nWhat is it that we‚Äôre trying to do?\n\nDefining what it is you‚Äôre trying to do will help to define what is meant by the metric/segment/measure you‚Äôre developing.\nFrom example (see Goal above), it‚Äôs vital that everyone in the room understands what is meant by ‚Äúaffluence.‚Äù\n\nDoes it mean:\n\nhow much money someone currently has?\nIt is a measure of their potential spend?\nDoes it refer to their lifestyle and how much debt they may take on and can realistically pay?\n\n\n\nWhy do we want to do this?\n\nThe answer to why has design implications\nExamples\n\nIs it something we‚Äôll use to set strategic goals?\nDo we want to use it to identify cross or upsell opportunities?\n\n\n\nAdditional Supplies:\n\nPoster or slide with these 2 questions\n\nProcess\n\nIntroduction\n\nBrief introduction of your team\nAddress goals of the workshop\n\nSplit into two or three groups\n\nIdeally between four to five people per group\nPut senior managers or executives in separate groups\n1 DS or analyst in each group\n\nHave groups answer the ‚ÄúWhat‚Äù question\n\nDuration: 15 min\n\nFeedback session with whole workshop\n\nDuration: 3 min\nGroup answers are presented and discussed\n\nHave groups answer the ‚ÄúWhy‚Äù question\n\nDuration: 15 min\n\nFeedback session with whole workshop\n\nDuration: 3 min\nGroup answers are presented and discussed\n\n\nCompare and address gaps\n\nDuration: 4 min\nCompare all group answers against the current solution\n\ne.g.¬†does the current metric represent these answers to ‚Äúwhat‚Äù and ‚Äúwhy‚Äù?\n\nIf there are gaps between the group answers and the current solution, try to figure out how best the fill those gaps.\n\n\nPost-Workshop Debrief\n\nShould occur the same day as the workshop\nDocument sticky notes and key ideas\n\nThe data scientists/analysts embedded in the groups should be able to expand on the ideas on the sticky notes\nIf the author of the idea was recorded, that should be included as well\n\nAs you‚Äôre starting to think of solutions to the pain points you discussed, reach out to authors/stakeholders to get their opinions on your thoughts and to understand where you might be able to get the data from.\n\n\nHave data team ideate on solutions to these painpoints, etc.",
    "crumbs": [
      "Projects",
      "<span class='chapter-number'>37</span>¬† <span class='chapter-title'>Planning</span>"
    ]
  },
  {
    "objectID": "qmd/project-planning.html#sec-proj-plan-thgscndr",
    "href": "qmd/project-planning.html#sec-proj-plan-thgscndr",
    "title": "37¬† Planning",
    "section": "37.4 Things to consider when choosing a project",
    "text": "37.4 Things to consider when choosing a project\n\nMisc\n\nAlso see Optimization, general &gt;&gt; Budget Allocation\n\nConsiderations\n\nBalancing the goals of your department with the desires of stakeholders\n\nBefore handling projects that are possible with the available data, address projects of immediate need to the business according to stakeholders.\n\nNeed to feel they‚Äôre getting what they think is value from the data department.\n\nAlso, have to ask, ‚Äúhow are we going to make money from this output?‚Äù and metrics ‚ÄúWhat organizational KPIs are tied to these metrics and how?‚Äù.\n\nIt is reasonable to (tactfully) say that the ‚Äúwins‚Äù column of your self-evaluation needs some big victories this year, and the ‚Äútop 10 products shipped this hour‚Äù dashboard isn‚Äôt going to get us where we want to be as an organization. Some fluff is acceptable to keep the peace.\nNo one will rush to the defense of the data team come budget season, regardless of how many low-value asks you successfully delivered, as requested, in the prior year (this is called executive amnesia).\n\n\nBeing useful\n\nSituations\n\nImproving upon a metric score (business or model)\n\n‚Äúanalyze actual failure cases and have a hard and honest think about what will fix the largest number of them. (My prediction: improving the data will by and large trump a more complex method.) The point is, you have to have a real think about what will actually improve the number, and that might involve work that‚Äôs stupid and boring. Don‚Äôt just reach for a smart new shiny thing and hope for the best.‚Äù\n\na lot of product people running around saying ‚Äúwe want to do this thing asap, but we don‚Äôt know if it‚Äôs possible\n\nJust find a product person who seems sane, buddy up with them, and work on what they want. They should be experts in their product, have an understanding of what the potential market for it is, and so really know what will be useful and what won‚Äôt.\n\n\n\n\nProcess\n\nScore Potential Projects\n\nCreate a score for ‚Äúimpact‚Äù\n\nPossible impact score features\n\nBack of the envelope monetary value\nCustomer experience score / Net Promoter Score (NPS)\n\n\nCreate a score for ‚Äúdifficulty‚Äù\n\nPossible difficulty score features\n\nhow long you believe the project will take to build\nresource constraints into consideration\nhow difficult the data may be to acquire and clean\n\n\nCategorize each project as Low Hanging Fruit, Quick Wins, Long Term Projects, and Distractions based the Impact and Difficulty Scores\n\nSee chart above\n\nPrioritize Low Hanging Fruit\nConsult with stakeholders and decide which Quick Wins and Long Term Projects to pursue",
    "crumbs": [
      "Projects",
      "<span class='chapter-number'>37</span>¬† <span class='chapter-title'>Planning</span>"
    ]
  },
  {
    "objectID": "qmd/project-planning.html#sec-proj-plan-elevptch",
    "href": "qmd/project-planning.html#sec-proj-plan-elevptch",
    "title": "37¬† Planning",
    "section": "37.5 Elevator Pitch",
    "text": "37.5 Elevator Pitch\n\nComponents\n\nValue Proposition:\n\nDescribe the problem\n\nWho is dissatisfied with the current environment\nThe targeted segment that your project is addressing\n\nDescribe the product/service that you are developing and what specific problem that it is solving\n\nYour Differentiation:\n\nDescribe the alternative (perhaps your competition or currently available product/service)\nDescribe a specific (not all) functionality or feature that is different from the currently available product/service.\n\n\nDelivery Formula\n\nFor [targeted segment of customers/users],\nWho are dissatisfied with [currently available product/service]\nOur product/service is a [new product category]\nThat provides a [key problem-solving capability]\nUnlike [product/service alternative],\nWe have assembled [Key product/service features for your specific application]\n\nExample For political election campaign managers,\n  **Who are dissatisfied with** the traditional polling products,\n\n  **Our application is a** new type of polling product\n\n  **That provides** the ability to design, implement, and get results within 24 hours.\n\n  **Unlike** the other traditional polling products that take over 5 days to complete,\n\n  **We have assembled** a quick and more economic yet comparably accurate polling product.\nExample For front line criminal investigators,\n  **Who are dissatisfied with** generic dashboards that display too much unnecessary information,\n\n  **Our application is a** new type of Intelligence product\n\n  **That provides** a highly customized and machine learning-enabled risk assessment tool that allows the investigator to uncover a hidden network of potentially criminals.\n\n  **Unlike** the current dashboard that provides information that is often not very useful,\n\n  **We have assembled** an intelligence product that allows them to make associations between known and unknown actors of interest.",
    "crumbs": [
      "Projects",
      "<span class='chapter-number'>37</span>¬† <span class='chapter-title'>Planning</span>"
    ]
  },
  {
    "objectID": "qmd/project-planning.html#sec-proj-plan-edarp",
    "href": "qmd/project-planning.html#sec-proj-plan-edarp",
    "title": "37¬† Planning",
    "section": "37.6 Exploratory Data Analysis Research Plan (EDARP)",
    "text": "37.6 Exploratory Data Analysis Research Plan (EDARP)\n\nImperative that this plan be formulated before work on the project begins\n\n\nDevelop a research question and objectives to be met\n\nA concise research question and objective allows the stakeholders to be informed on how their departments will be affected.\nSoft skills are important in being able to make a stakeholder comfortable with answering questions.\nIt helps if each stakeholder feels like their input is necessary to the project. More likely to get invested in its success.¬†May require salesmanship to convince them that the knowledge of self has future benefits\nComing up with a concise question often involves drilling down into the mechanics of an organization.\n\nThis process can lead to discovering the importance of the research. Also, can lead to other research questions.\n\nIf stakeholder is ambiguous about what they want. Useful to use a backwards path by starting with what the stakeholder envisions as the final output\n\n‚ÄúCan you tell me about the decisions you are going to make based on the output of this request? Understanding those decisions will help us ensure that the (report/data/model) we build for you will fulfill your needs.‚Äù\n\nIf a concise question cannot be obtained or the objective is open-ended, you should gather all the information obtained and use it determine what you think the objective of the project should be. Then, develop a prototype\n\nAfter a portion of the project is completed, it will be necessary to share the progress with the stakeholders and revise aspects of the project as needed. More of an iterative process.\nThe iteration can increase buy-in from stakeholders and develop a more concise question\n\nAfter the question is decided upon, what does success look like? What are the metrics of evaluation?¬†¬†\n\nDefine methods to address the question\n\ntype of method depends on the type of question\n\ndescriptive\n\nUsually answered with historical data, e.g historical trends. Often presented in dashboards\nmean, min, max, etc.\nplots for understanding relationships\noutlier detection\n\npredictive\n\npredictive models and forecasting\nregression, classification, clustering\n\nprescriptive\n\npredictive models + proposing actions\n\n\n\nDetermine which data is on-hand and which need to be acquired\n\nAlso see pipeline section below\nNote data sources\n\nStructured\nUnstructured\nInternal\nExternal\n\nWrangle data\n\nDetermine the budget and adjust prior steps to conform within this budget\n\nDetermined required tools\n\ncompute capacity\nend user requirements\nbusiness interests\n\nMake effort to minimize cost as much as possible. Makes dialogue with stakeholders easier\n\nStakeholders\n\nA departments that will make use of the output or be effected by should be involved\n\nConvincing them to be involved is important if any issues with budgets, etc. pop-up. Having more people on your side will help.\n\nUsing a proof-of-concept or information of past successes of such projects within or outside the organization can help.\nNeed to proof on how this research will increase their ROI. Prototypes with datascience outputs can go a long way convincing or increasing buy-in from skeptical stakeholders.\n\n\nTheir concerns should be noted and addressed.\nOften times they are the end-user, so having them involved can lead to a better output. A greater probability of the project being involved in decision making.\n\nHow will the end-users make use of the output?\n\nShould the output be a report or app?\n\nDevelop a plan of action for upkeep or maintenance of the project\n\nMonitoring model drift\nChanges in the business\nResolve any lingering user concerns\nDetermine schedule for updating project¬†\nSchedule stakeholder meeting to discuss the project and determine if further steps needed",
    "crumbs": [
      "Projects",
      "<span class='chapter-number'>37</span>¬† <span class='chapter-title'>Planning</span>"
    ]
  },
  {
    "objectID": "qmd/project-planning.html#sec-proj-plan-decmod",
    "href": "qmd/project-planning.html#sec-proj-plan-decmod",
    "title": "37¬† Planning",
    "section": "37.7 Decision Models",
    "text": "37.7 Decision Models\n\n\nNotes from https://towardsdatascience.com/learn-how-to-financially-evaluate-your-analytics-projects-with-a-robust-decision-model-6655a5229872\nAlso see\n\nAlgorithms, Business &gt;&gt; Cost-Benefit Analysis\nOptimization, general &gt;&gt; Budget Allocation\nVideo Bizsci Learning Lab 68\n\n{tidyquant} has excel functions for Net Present Value (NPV), Future Value of Cashflow (FV), and Present Value of Future Cashflow (PV)\n** A lot of this stuff wasn‚Äôt used in the examples**\n\nFocus on fixed and variable costs, investment, depreciation, cost of capital, tax rate, revenue, qualitative factors\n\nNet Present Value (NPV) is used as the final quantitative metric (Value in flow chart)\n\nShort running projects can just use the orange boxes to calculate value\nLong running projects would use orange, blue, and gray boxes to calculate value\n\nThe values for each variable need to calculated for each year (or whatever time unit) of the project.\nDifferent scenarios (pessimistic, optimistic) can be created by modifying the parameters of the model like the life span, the initial investment, or the costs, to see the potential financial impact on Value.\nMaking a decision\n\nCompare your NPV and the Internal Rate of Return (IRR) with alternative analytics projects that the organization is considering to see which one is more attractive.\nBalance the quantitative and qualitative factors while considering the potential risks of the project.\n\nExample\n\n‚ÄúIn quantitative factors, we used the cash flow model where we calculated a net present value of USD $7,187,955. Related to qualitative factors and potential risks, we can conclude that if the project is back up by an influential group of leaders, then the positive aspects exceed the negative ones. On the balance, we would advise a GO decision for this project.‚Äù\n\n\n\nExecutive Presentation\n\nThe conclusion should be short, precise, and convincing format\nCrucial to persuade key stakeholders to invest in your project\nUse a good action title, writing down the most relevant requirements of your project, use a chart to visualize your results, and have some key takeaways on the bottom with a recommendation.\n\nProposal\n\nSections 1-3 should be put into a concise executive summary\nTell a story\n\nEvery item should be consistent with every other part\n\ne.g.¬†a deliverable shouldn‚Äôt be mentioned if it‚Äôs not part of solving the issue in the problem statement\n\nShould flow from more abstract ideas to more and more detail\n\nFormat\n\nProblem Statement\n\nPaint a bleak picture and explain how the problem could‚Äôve been overcome if your project were in production when the problem occurred\nList examples where risk and costs occurred, opportunities were missed, etc. and how your project would‚Äôve prevented or taken advantage of these things if it had been in production\n\nskill upgrades or efficiency gains are NOT good answers\n\n\nVision\n\nTies your project to the companies long range strategy\n\nBenefits\n\ndetails¬† about the specific new capabilities provided or costs avoided\nneed to be quantifiable\n\nDeliverables\n\ndetails about other non-primary benefits\n\nSuccess Criteria\n\nCharacteristics\n\nspecific, measurable, bounded by a time period, realistic\n\n\nThe plan\n\noverview of the steps the project will take\n\nincludes deadlines, waterfall or agile approach, etc.\n\n\nCost/Budget\n\n\nTerms\n\nCost/Investment\n\nFixed\n\nPersonnel salaries\n\nIf you‚Äôre using outside data consultants, Glassdoor has average salaries, https://www.glassdoor.com/Salaries/data-engineer-salary-SRCH_KO0,13.htm\n\nexample: ~$100K each\n\n\nAdditional investments could be access to external data sources, research, and software licenses\n\nTableau can cost around $48K per year\n\nTraining for any additional or current personnel on tools or subject matter\n\nworkshops, online learning\ntax-reclamation example: ~$12K total\n\n\nVariable\n\nCloud storage\n\ntax-reclamation example: AWS (pay-as-you-go) $75K-$95K per year\n\n\n\nTax rate\n\ntax-reclamation example: 28%\n\nRevenue\n\nForecasts of revenue\n\nCorrection Factors\n\nExplains the difference between profit after tax and cashflow\ne.g.¬†depreciation of investment, changing working capital, investment, change in financing or borrowing\n\ncost of sales = (revenue*some%) - 1\n\nrefers to what the seller has to pay in order to create the product and get it into the hands of a paying customer.\n\nMaybe this is a catch-all for anything else not covered in the Cost/Investment section\n\naka cost of revenue (sales/service) or cost of goods (manufacturing)\n30% used in example\nBegins after year 1\n\ndepreciation per year = (investment - salvage_value)/(num_years - 1)\n\nFor an datascience project, the salvage_value is 0 most of the time.\ndepreciation value is listed for each year after the first year of the project\ngets subtracted from revenue in calculation of profit_before_tax calculation\ngets added to profit_after_tax in cashflow equation\n\nChange in working capital = (expected_yearly_revenue/365) * customer_credit_period\n\ncustomer_credit_period (days): number of days that a¬†customer¬†is allowed to wait before paying an invoice. The concept is important because it indicates the amount of working capital that a business is willing to invest in its accounts receivable in order to generate sales.\n\n60 days used in example\n\nOnly a cost in the 1st yr and then it is recouped in the last year (somehow)\n\nsounds like this whole thing is just an accounting procedure\n\nDunno how this relates to a datascience project\n\n\n\nCost of Capital\n\nUsed to characterize risk here. Think 5-10% is used for back-of-the-envelope calculations.\n\nIn the tax-reclamation example, 8.3% was used\n\nAlso see\n\nAlgorithms, Product &gt;&gt; Cost Benefit Analysis (CBA)\nMorgan Stanley‚Äôs Guide to Cost of Capital (Also in R &gt;&gt; Documents &gt;&gt; Business)\n\nThread about the guide\n\n\nCost of Capital is typically calculated as Weighted Average Cost of Capital (WACC)\n\nSee Finance, Glossary &gt;&gt; Weighted Average Cost of Capital (WACC)\n\nAs of January 2019, transportation in railroads has the highest cost of capital at 11.17%. The lowest cost of capital can be claimed by non-bank and insurance financial services companies at 2.79%.\nHigh CoCs - Biotech and pharmaceutical drug companies, steel manufacturers, Internet (software) companies, and integrated oil and gas companies. Those industries tend to require significant capital investment in research, development, equipment, and factories.\nLow CoCs - Money center banks, power companies, real estate investment trusts (REITs), retail grocery and food companies, and utilities (both general and water). Such companies may require less equipment or benefit from very steady cash flows.\n\nNet Present Value (NPV)\n\nUses cashflow (excludes year 1) and cost of capital as inputs\n\nExcel has some sort of NPV wizard that does the calculation (see tidyquant for npv function)\nExample in article provided link to his googlesheet so the formula is there or maybe googlesheets has a wizard too.\n\n\nInternal Rate of Return (IRR)\n\nIt‚Äôs the value that the cost of capital (aka discount rate) would have to be for NPV and cost to zero out each other.\nThe interest rate at which a company borrows against itself as a proxy for opportunity cost. Typically, large and/or public organizations have a budgetary IRR of 10% to 15% depending on the industry and financial situation.\nHigher is better\nOnly valid in very limited circumstances. MIRR is much better\n\nSee Finance, Glossary &gt;&gt; IRR, MIRR\n\n\nQualitative Factors\n\nOther things that have value but are difficult to quantify. Might change a quantitatively negative value project into a positive value project\n\nUses columns negative, positive, impact\n\nnegative and positive are indicators\nimpact has values low, medium, and high\n\n\nExamples\n\nProject has flexibility (+) (calculated using real options analysis(?))\nStrategic Fit (+) - supports strategy of the company (?)\n\nincreases trust with public\nmarketing boosted because they can use buzzwords like ‚ÄúAI‚Äù or ‚Äúdata-driven‚Äù in ads\n\nIncreases competency (+) - may help the company down the road\n\nmore agile, data-driven, familiarity with newer technologies\n\nred tape or bureaucracy or politics causing delays (-)\nImplementation (-)\n\ncan be a negative, if the leadership that‚Äôs needed is tied up with other projects\nChallenges",
    "crumbs": [
      "Projects",
      "<span class='chapter-number'>37</span>¬† <span class='chapter-title'>Planning</span>"
    ]
  },
  {
    "objectID": "qmd/project-planning.html#sec-proj-plan-capf",
    "href": "qmd/project-planning.html#sec-proj-plan-capf",
    "title": "37¬† Planning",
    "section": "37.8 Choosing a Pipeline Framework",
    "text": "37.8 Choosing a Pipeline Framework\n - Kind of like working backwards in the data analysis workflow (design driven development) - Quantify the problems and develop KPIs that can inform the direction of the business - 10-15 organizational KPIs is common - Examples: Number of daily sales, the number of new customers, decreasing cost of operations/logistics - Build data pipelines around these critical KPIs - What data can we use that we already have access to? - What kind of internal data do we need to capture? - What kind of third party/external data could be useful? - What is the least amount of data available that can be used to approximate each KPI metric? - After the simplest version of the metric is built, you can brainstorm on how it can be improved. - Personalize a general metric to your specific business. - Split a general metric into smaller metrics can increase accuracy (e.g.¬†product line, geography, etc.) - What kind of storage will we need? - Adding a pipeline for a new data source - Capture a sample of the data to examine its potential. (e.g.¬†cookies on your website) - If metric measured from new data is potentially valuable, build a simple, less robust (without sacrificing too much data integrity) pipeline to your lake. - Meaning fewer built-in quality checks - Doing it this way does add technical debt - If metric feeds into a company-wide metric, then additional checks will need to be added - E.g. no duplicate datasets, data corruption, or data losses. - After 6 months or so, if metric remains useful, make pipeline more robust. - Adding third party data pipeline (e.g.¬†Equifax) - E.g. demographic, income data - If a product has a certain demographics associated with it, this data can be used to weight customers relative to the demographic specifics of that product. - Get a sample first to determine if it‚Äôs worth it - 3rd party data is expensive to buy and usually a reoccurring cost. - Can be time-consuming and resource intensive to add the pipeline - Get transparency on how the data was collected and validity checks used - Periodically reevaluate whether the metrics fundamentally still make sense from a business perspective and how they can be improved. - Expand pipelines to capture additional data in order to continue to refine metrics - Drop and add metrics as business and trends change. - Redefine metrics as needed -",
    "crumbs": [
      "Projects",
      "<span class='chapter-number'>37</span>¬† <span class='chapter-title'>Planning</span>"
    ]
  },
  {
    "objectID": "qmd/python-reticulate.html",
    "href": "qmd/python-reticulate.html",
    "title": "reticulate",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Python",
      "reticulate"
    ]
  },
  {
    "objectID": "qmd/python-reticulate.html#sec-py-retic-misc",
    "href": "qmd/python-reticulate.html#sec-py-retic-misc",
    "title": "reticulate",
    "section": "",
    "text": "Before interactively running python in RStudio, start REPL\nreticulate::repl_python()",
    "crumbs": [
      "Python",
      "reticulate"
    ]
  },
  {
    "objectID": "qmd/python-reticulate.html#sec-py-retic-pynr",
    "href": "qmd/python-reticulate.html#sec-py-retic-pynr",
    "title": "reticulate",
    "section": "Python in R Scripts",
    "text": "Python in R Scripts\n\nVia source_python\n\nExample\nlibrary(tidyverse)\nlibrary(reticulate)\nsource(\"funs.R\")\nuse_virtualenv(\"../../\")\nsource_python(\"funs.py\")\n\n# stuff\n\nfor (r in 1:nrow(res)) {\n  cat(r, \"\\n\")\n\n  tmp_wikitext &lt;- get_wikitext(res$film[r], res$year[r]) # r fun\n\n  # skip if get_wikitext fails\n  if (is.na(tmp_wikitext)) next\n  if (length(tmp_wikitext) == 0) next\n\n  # give the text to openai\n  tmp_chat &lt;- tryCatch(\n    get_results(client, tmp_wikitext), # py fun\n    error = \\(x) NA\n  )\n\n  # if openai returned a dict of 2\n  if (length(tmp_chat) == 2) {\n    res$writer[r] &lt;- tmp_chat$writer\n    res$producer[r] &lt;- tmp_chat$producer\n  }\n}\nget_results is a python function defined in funs.py",
    "crumbs": [
      "Python",
      "reticulate"
    ]
  },
  {
    "objectID": "qmd/python-reticulate.html#sec-py-retic-rmark",
    "href": "qmd/python-reticulate.html#sec-py-retic-rmark",
    "title": "reticulate",
    "section": "RMarkdown",
    "text": "RMarkdown\n\nAlso see Quarto &gt;&gt; R and Python\nBasic set-up\n---\ntitle: \"R Notebook\"\noutput: html_notebook\n---\n\n\n```{r}\nknitr::opts_chunk$set(\n¬† echo = TRUE,\n¬† message = FALSE,\n¬† warning = FALSE\n)\n```\n```{r}\nlibrary(reticulate) \n```\n```{python}\nimport pandas as pd \nimport numpy as np\n```",
    "crumbs": [
      "Python",
      "reticulate"
    ]
  },
  {
    "objectID": "qmd/post-hoc-analysis-multilevel.html",
    "href": "qmd/post-hoc-analysis-multilevel.html",
    "title": "Multilevel",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Post-Hoc Analysis",
      "Multilevel"
    ]
  },
  {
    "objectID": "qmd/post-hoc-analysis-multilevel.html#sec-phoc-mixeff-misc",
    "href": "qmd/post-hoc-analysis-multilevel.html#sec-phoc-mixeff-misc",
    "title": "Multilevel",
    "section": "",
    "text": "Also see Post-Hoc Analysis, general\nPackages\n\n{effectsize} - Has many of the metrics discussed here and others ‚Äî¬†with confidence intervals",
    "crumbs": [
      "Post-Hoc Analysis",
      "Multilevel"
    ]
  },
  {
    "objectID": "qmd/post-hoc-analysis-multilevel.html#sec-phoc-mixeff-tukey",
    "href": "qmd/post-hoc-analysis-multilevel.html#sec-phoc-mixeff-tukey",
    "title": "Multilevel",
    "section": "Tukey Test",
    "text": "Tukey Test\n\nDifference in effects\nExample: Is there a statistically significant difference between the estimated effects of the categories of the fixed effect, ‚ÄúSeason‚Äù Data from Multilevel Modeling and Effects Statistics for Sports Scientists in R\n\n{multcomp}{emmeans}\n\n\nlibrary(multcomp)\n# pairwise comparisons\nfit_tukey &lt;- glht(fit, linfct=mcp(Season=\"Tukey\"))\nsummary(fit_tukey)\n##¬†\n## Simultaneous Tests for General Linear Hypotheses\n##¬†\n## Multiple Comparisons of Means: Tukey Contrasts\n##¬†\n##¬†\n## Fit: lmer(formula = Distance ~ Season + (1 | Athlete), data = data)\n##¬†\n## Linear Hypotheses:\n##¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† Estimate Std.  Error z value Pr(&gt;|z|)¬† ¬†\n## Postseason - Inseason == 0¬† ¬†     36.71¬† ¬†90.08¬†  0.408¬† ¬† 0.911¬† ¬†\n## Preseason - Inseason == 0¬† ¬†    1166.00¬† ¬†90.08¬† 12.944¬†  &lt;1e-05 ***\n## Preseason - Postseason == 0¬†    1129.29¬† 110.32¬† 10.236¬†  &lt;1e-05 ***\n## ---\n## Signif. codes:¬† 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## (Adjusted p values reported -- single-step method)\n\n\nemmeans(fit, specs = pairwise ~ Season)\n## $emmeans\n##¬† Season¬† ¬† emmean¬† SE¬†  df lower.CL upper.CL\n##¬† Inseason¬† ¬† 5104 137 20.8¬† ¬†  4818¬† ¬† 5389\n##¬† Postseason¬† 5140 151 30.6¬† ¬†  4831¬† ¬† 5449\n##¬† Preseason¬† ¬†6270 151 30.6¬† ¬†  5961¬† ¬† 6579\n## Degrees-of-freedom method: kenward-roger¬†\n## Confidence level used: 0.95¬†\n## $contrasts\n##¬† contrast¬† ¬† ¬† ¬† ¬† ¬† ¬†  estimate¬† ¬† SE¬† df t.ratio p.value\n##¬† Inseason - Postseason¬† ¬†  -36.7¬† 90.1 978¬† -0.408 0.9125¬†\n##¬† Inseason - Preseason¬† ¬† -1166.0¬† 90.1 978 -12.944 &lt;.0001¬†\n##¬† Postseason - Preseason¬† -1129.3 110.3 978 -10.236 &lt;.0001¬†\nDegrees-of-freedom method: kenward-roger¬†\nP value adjustment: tukey method for comparing a family of 3 estimates\n\n\n\n\nInterpretation\n\nThere is NOT a difference between the effect that Postseason has on Distance and the effect that Inseason has on Distance.\nThere is a difference with between the other two pairs of categores\nEstimated mean distance given season type\n\nI‚Äôm not sure these estimates are appropriate in this situation since the Season variable is inherently unbalanced.\nAlso see emmeans Post-Hoc Analysis, emmeans",
    "crumbs": [
      "Post-Hoc Analysis",
      "Multilevel"
    ]
  },
  {
    "objectID": "qmd/post-hoc-analysis-multilevel.html#sec-phoc-mixeff-cohensd",
    "href": "qmd/post-hoc-analysis-multilevel.html#sec-phoc-mixeff-cohensd",
    "title": "Multilevel",
    "section": "Cohen‚Äôs D",
    "text": "Cohen‚Äôs D\n\nStandardized difference in means given a grouping variable\nGenerally recommended to use \\(g_{\\text{rm}}\\) or \\(g_{\\text{av}}\\)\n\nStandard practice is use whichever one of those two values is closer to \\(d_s\\) , because it helps make the result comparable with between-subject studies.\nCorrection for bias can be important when dof &lt; 50\nAppropriate Version Per Use Case\n\n\n\n\n\n\n\nUse\nVersion\n\n\n\n\nIndependent groups, power analyses where \\(\\sigma_\\text{pop}\\) is known or \\(\\sigma\\) is calculated with \\(n\\)\n\\(d_{\\text{pop}}\\)\n\n\nIndependent groups, power analyses where \\(\\sigma_\\text{pop}\\) is unknown or \\(\\sigma\\) is calculated with \\(n-1\\)\n\\(d_s\\)\n\n\nIndependent groups, corrects for small sample bias; report for use in meta-analyses\n\\(g\\)\n\n\nIndependent groups, when treatment might affect SD\n\\(\\Delta\\)\n\n\nCorrelated groups; generally recommended over \\(g_{\\text{rm}}\\)\n\\(g_{\\text{av}}\\)\n\n\nCorrelated groups; more conservative than \\(g_{\\text{av}}\\)\n\\(g_{\\text{rm}}\\)\n\n\nCorrelated groups; power analyses\n\\(d_z\\)\n\n\n\n\nNotes from: Calculating and reporting effect sizes to facilitate cumulative science: a practical primer for t-tests and ANOVAs (Lakens)\nCan be used to compare effects across studies, even when the dependent variables are measured in different ways\n\nExamples\n\nWhen one study uses 7-point scales to measure dependent variables, while the other study uses 9-point scales\nWhen completely different measures are used, such as when one study uses self-report measures, and another study used physiological measurements.\n\n\nThe bias-corrected version is known as Hedges‚Äô g, and in the r family of effect sizes, the correction for eta squared (Œ∑2) is known as omega squared (œâ2)\nGuidelines\n\nRange: 0 to \\(\\infty\\)\nCohen (1992)\n\n|d| &lt; 0.2 ‚Äúnegligible‚Äù\n|d| &lt; 0.5 ‚Äúsmall‚Äù\n|d| &lt; 0.8 ‚Äúmedium‚Äù\notherwise ‚Äúlarge‚Äù\n\nOthers: Automated Interpretation of Indices of Effect Size\nValues should not be interpreted rigidly\n\ne.g.¬†Small effect sizes can have large consequences, such as an intervention that leads to a reliable reduction in suicide rates with an effect size of d = 0.1.\n\nThe only reason to use these benchmarks is when the findings are extremely novel, and cannot be compared to related findings in the literature.\n\nTwo groups of Independent Observations (Between-Subjects)\n\\[\n\\begin{align}\nd_s &= \\frac{\\bar X_1 - \\bar X_2}{\\sqrt{\\frac{(n_1-1)SD^2_1 + (n_2-1)SD^2_2}{n_1 + n_2 - 2}}}\\\\\n&= t\\;\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}\\\\\n& \\approx \\frac{2t}{\\sqrt{N}}\n\\end{align}\n\\]\n\nWhere the denominator is the pooled standard deviation\n\\(t\\) is the t-value of two-sample t-test\nTypically used in an a priori power analysis for between-subjects designs\nHedges‚Äô g (bias-corrected)\n\\[\ng_s = d_s \\times \\left(1-\\frac{3}{4(n_1 + n_2) - 9}\\right)\n\\]\n\nThe same correction is used for all types of Cohen‚Äôs d\nThe difference between Hedges‚Äôs gs and Cohen‚Äôs ds is very small, especially in sample sizes above 20\n\nInterpretation: A percentage of the standard deviation. Best to relate it to other effect sizes in the literature and it‚Äôs practical consequences if possible.\n\ne.g.¬†\\(d_s = 0.5\\) says the difference in means is half a standard deviation.\n\nWhenever standard deviations differ substantially between groups, Glass‚Äôs \\(\\Delta\\) should also be reported\n\nOne Sample or Correlated Samples (Within-Subjects)\n\\[\n\\begin{aligned}\n&d_z = \\frac{M_{\\text{diff}}}{S_{\\text{diff}}} = \\frac{t}{\\sqrt{n}} \\\\\n&\\begin{aligned}\n\\text{where} \\quad S_{\\text{diff}}^{(1)} &= \\sqrt{\\frac{\\sum(X_{\\text{diff}} - M_{\\text{diff}})^2}{N-1}} \\\\\nS_{\\text{diff}}^{(2)} &= \\sqrt{\\text{SD}_1^2 + \\text{SD}_2^2 - (2\\cdot r\\cdot \\text{SD}_1 \\cdot \\text{SD}_2)}\n\\end{aligned}\n\\end{aligned}\n\\]\n\n\\(M_{\\text{diff}}\\) is the difference between the mean (M) of the difference scores and the comparison value, \\(\\mu\\) (typically 0)\n\nFor paired data, the mean of the difference scores is equal to the difference in means of the two groups, so you may see it described or calculated either way.\n\n\\(X_{\\text{diff}}\\) are the difference scores (i.e.¬†the difference between the repeated measurements)\n\\(S_\\text{diff}\\) is the SD of the difference scores.\n\nIt can be calculated two different ways, but I doubt both are equal to each other.\nThe second way seems to be the preferred way since it incorporates a correlation measure.\n\n\\(t\\) is the t-value of a paired samples t-test\n\\(r\\) is the correlation between measurements\n\nRepeated Measures (Within-Subjects)\n\\[\nd_{\\text{rm}} = d_z \\cdot \\sqrt{2(1-r)}\n\\]\n\nAlternative\n\\[\nd_{\\text{av}} = \\frac{M_{\\text{diff}}}{\\frac{\\text{SD}_1 + \\text{SD}_2}{2}}\n\\]\n\nIgnores the correlation between measures\n\nIf it is believe that the intervention/treatment affected the SD after the intervention, then it is advised to only use either (pre-treatment) \\(\\text{SD}_1\\) (recommended) or (post-treatment) \\(\\text{SD}_2\\) and report which one is used. The calculated effect is then known as Glass‚Äôs \\(\\boldsymbol{\\Delta}\\)\n\nExample: Distance (outcome), Season (Grouping variable)\n\nComparing Distance means given Season (3 levels) type\nData from Multilevel Modeling and Effects Statistics for Sports Scientists in R\n\n\n{effsize}{rstatix}{esci}\n\n\nAnother package, {effectsize}, is similar in that its formula arg only allows for grouping variables with only 2 levels\n\nMay have other features though, since it‚Äôs part of the easystats suite.\n\nlibrary(effsize)\neffsize::cohen.d(preseason_data$Distance, inseason_data$Distance)\n##¬†\n## Cohen's d\n##¬†\n## d estimate: 0.9157833 (large)\n## 95 percent confidence interval:\n##¬† ¬† lower¬† ¬† upper¬†\n## 0.7493283 1.0822383\n\nSeason is a categorical fixed effect with 3 levels\nOther Available Arguments: hedges.correction, pooled, paired, within, noncentral\n\n\n\nlibrary(rstatix)\ndata %&gt;%¬†\n¬† rstatix::cohens_d(Distance ~ Season, ci = TRUE)\n\n#&gt;     .y.¬† ¬† ¬†   group1¬† ¬†  group2¬† ¬† effsize¬† ¬†  n1¬† ¬†  n2   conf.low conf.high magnitude¬†\n#&gt; *  &lt;chr&gt;¬† ¬†    &lt;chr&gt;¬† ¬† ¬† &lt;chr&gt;¬† ¬† ¬† ¬†&lt;dbl&gt;   &lt;int&gt;  &lt;int&gt;¬† ¬† &lt;dbl&gt;¬† ¬† &lt;dbl&gt; &lt;ord&gt;¬† ¬† ¬†\n#&gt; 1 Distance   Inseason¬† Postseason   -0.0317¬†   600¬†   200¬† ¬†  -0.18¬† ¬† ¬†0.13  negligible\n#&gt; 2 Distance   Inseason¬†  Preseason¬†   -0.877¬† ¬† 600¬†   200¬† ¬†  -1.06¬† ¬† -0.71       large¬† ¬† ¬†\n#&gt; 3 Distance Postseason   Preseason¬†   -0.884¬† ¬† 200¬†   200¬† ¬†  -1.09¬† ¬† -0.68       large\n\nSame types of arguments as {effsize} are available and also bootstrap CIs\nMagnitude (interpretation) by Cohen‚Äôs (1992) guidelines\n\n\n\nestimate &lt;- esci::estimate_mdiff_two(\n  data = mydata,\n  outcome_variable = Prediction,\n  grouping_variable = Exposure,\n  conf_level = 0.95,\n  assume_equal_variance = TRUE\n)\nestimate$es_smd |&gt; \n  tidyr::gather(key = \"type\", \n                value = \"value\")\n#&gt;                      type             value\n#&gt; 1   outcome_variable_name        Prediction\n#&gt; 2  grouping_variable_name          Exposure\n#&gt; 3                  effect            20 ‚Äí 1\n#&gt; 4             effect_size 0.571611929854665\n#&gt; 5                      LL 0.327273973938463\n#&gt; 6                      UL  0.81492376943417\n#&gt; 7               numerator  11.3842850063322\n#&gt; 8             denominator  19.8603120279963\n#&gt; 9                      SE 0.124402744976289\n#&gt; 10                     df               268\n#&gt; 11               d_biased 0.573217832141019\n\nestimate$es_smd_properties$message\n#&gt; This standardized mean difference is called d_s because the standardizer used was s_p. d_s has been corrected for bias. Correction for bias can be important when df &lt; 50.  See the rightmost column for the biased value.\n\nFairly large effect: d = 0.57 95% CI [0.33, 0.81] and the confidence interval is fairly narrow\nMakes available the type of cohen‚Äôs d and the denominator used",
    "crumbs": [
      "Post-Hoc Analysis",
      "Multilevel"
    ]
  },
  {
    "objectID": "qmd/post-hoc-analysis-multilevel.html#common-language-effect-size",
    "href": "qmd/post-hoc-analysis-multilevel.html#common-language-effect-size",
    "title": "Multilevel",
    "section": "Common Language Effect Size",
    "text": "Common Language Effect Size\n\nAKA Probability of Superiority\nConverts the effect size into a percentage which is supposed to more understandable for laymen\nMisc\n\nNotes from The Common Language Effect Size Statistic\nPackages\n\n{ebtools::cles}\n\n\nInterpretation\n\nBetween-Subjects: The probability that a randomly sampled person from the first group will have a higher observed measurement than a randomly sampled person from the second group\nWithin-Subjects: The probability that an individual has a higher value on one measurement than the other.\n\nFormula\n\nAssumes variables are normally distributed and \\(\\sigma_1 = \\sigma_2\\)\n\nOriginal paper gives some evidence that these formulas are pretty robust to violations though.\nRecommended only for continuous variables\n\nBetween-Subjects\n\\[\n\\begin{align}\n\\tilde d &= \\frac{|M_1 - M_2|}{\\sqrt{p_1\\text{SD}_1^2 + p_2\\text{SD}_2^2}} \\\\\nZ &= \\frac{\\tilde d}{\\sqrt{2}}\n\\end{align}\n\\]\n\n\\(M_i\\): The mean of the ith group variable\n\\(p_i\\): The proportion of the sample size of the ith group variable\n\nWithin Subjects\n\\[\nZ = \\frac{|M_1 - M_2|}{\\sqrt{\\operatorname{SD}_1^2 + \\operatorname{SD}_2^2 - 2 \\times r \\times \\operatorname{SD}_1 \\times \\operatorname{SD}_2}}\n\\]\n\n\\(r\\) is the Pearson correlation between the group variables\n\n\nAlternative Generalization\n\n\\(A_{1,2} = P(X_1 &gt; X_2) + 0.5 \\times P(X_1 = X_2)\\)\nApplies for any, not necessarily continuous, distribution that is at least ordinally scaled\n\nEqual to CL in the continuous case\nInterpreted as an estimate of the value of CL that would be obtained if the distribution of X were continuous.",
    "crumbs": [
      "Post-Hoc Analysis",
      "Multilevel"
    ]
  },
  {
    "objectID": "qmd/post-hoc-analysis-multilevel.html#eta-squared",
    "href": "qmd/post-hoc-analysis-multilevel.html#eta-squared",
    "title": "Multilevel",
    "section": "Eta Squared",
    "text": "Eta Squared\n\nNotes from: Calculating and reporting effect sizes to facilitate cumulative science: a practical primer for t-tests and ANOVAs (Lakens)\nEffect Size for ANOVA\nMeasures the proportion of the variation in Y that is associated with membership of the different groups defined by X, or the sum of squares of the effect divided by the total sum of squares\neta squared is an uncorrected effect size estimate that estimates the amount of variance explained based on the sample, and not based on the entire population.\npartial eta squared (Œ∑2p) to improve the comparability of effect sizes between studies, which expresses the sum of squares of the effect in relation to the sum of squares of the effect and the sum of squares of the error associated with the effect.\nAlthough Œ∑2p is more useful when the goal is to compare effect sizes across studies, it is not perfect, because Œ∑2p differs when the same two means are compared in a within-subjects design or a between-subjects design.\nAn \\(\\eta^2\\) of 0.13 means that 13% of the total variance can be accounted for by group membership.\nCIs should be at 90%, because if you use 95%, it‚Äôs possible that even with a significant F-test, the CI will contain 0. For 90%, this doesn‚Äôt happen.\nEta Squared\n\\[\n\\eta^2 = \\frac{\\text{SS}_{\\text{effect}}}{\\text{SS}_{\\text{total}}}\n\\]\n\n\\(\\text{SS}_{\\text{effect}}\\) and \\(\\text{SS}_{\\text{total}}\\) are obtained from the ANOVA results\nThe correction for eta squared (\\(\\eta^2\\)) is known as omega squared (\\(\\omega^2\\)). Still biased but less biased. The difference is typically small, and the bias decreases as the sample size increases.\n\\[\n\\begin{align}\n\\omega^2 &= \\frac{\\operatorname{df}_{\\text{effect}}(\\operatorname{MS_{\\text{effect}}}-\\operatorname{MS_{\\text{error}}})}{\\operatorname{SS_{\\text{total}}} + \\operatorname{MS_{\\text{error}}}} \\quad \\text{(between-subjects)} \\\\\n\\omega^2 &= \\frac{\\operatorname{df}_{\\text{effect}}(\\operatorname{MS_{\\text{effect}}}-\\operatorname{MS_{\\text{error}}})}{\\operatorname{SS_{\\text{total}}} + \\operatorname{MS_{\\text{subjects}}}} \\quad \\text{(within-subjects)} \\\\\n\\end{align}\n\\]\n\nPartial Eta Squared\n\\[\n\\begin{align}\n\\eta_p^2 &= \\frac{\\operatorname{SS_{\\text{effect}}}}{\\operatorname{SS_{\\text{effect}}} + \\operatorname{SS_{\\text{error}}}} \\quad \\text{(fixed and measured variables)}\\\\\n\\eta_p^2 &= \\frac{F \\times \\operatorname{df}_{\\text{effect}}}{F \\times \\operatorname{df}_{\\text{effect}} + \\operatorname{df}_{\\text{error}}} \\quad \\text{(fixed variables)}\n\\end{align}\n\\]\n\nfixed (e.g., manipulated), not random (e.g., measured)\nBias-Lessened\n\\[\n\\omega_p^2 = \\frac{\\operatorname{df}_{\\text{effect}}(\\operatorname{MS_{\\text{effect}}}-\\operatorname{MS_{\\text{error}}})}{\\operatorname{df}_{\\text{effect}} \\times  \\operatorname{MS_{\\text{effect}}} + (N - \\operatorname{df}_{\\text{effect}}) \\times \\operatorname{MS_{\\text{error}}}}\n\\]\n\nSame equation whether it‚Äôs for between-subject designs and within-subject designs\n\n\nRecommend researchers report Œ∑2G and/or Œ∑2p, at least until generalized omega-squared is automatically provided by statistical software packages\n\nFor designs where all factors are manipulated between participants, Œ∑2p and Œ∑2G are identical, so either effect size can be reported. For within-subjects designs and mixed designs where all factors are manipulated, Œ∑2p can always be calculated from the F-value and the degrees of freedom using formula 13, but Œ∑2G cannot be calculated from the reported results,and therefore I recommend reporting Œ∑2G for these designs\nsupplementary spreadsheet provides a relatively easy way to calculate Œ∑2G for commonly used designs. For designs with measured factors or covariates, neither Œ∑2p nor Œ∑2G can be calculated from the\n\nAppropriate Version Per Use Cases\n\n\n\n\n\n\n\n\nUse Case\nVersion\nLess Biased Version\n\n\n\n\nComparisons within a single study\n\\(\\eta^2\\)\n\\(\\omega^2\\)\n\n\nPower analyses, and for comparisons of effect sizes across studies with the same experimental design\n\\(\\eta_p^2\\)\n\\(\\omega_p^2\\)\n\n\nMeta-Analyses to compare across various experimental designs\n\\(\\eta_G^2\\)\n\\(\\omega_G^2\\)\n\n\n\nGuidelines\n\nCohen‚Äôs benchmarks were developed for comparisons between unrestricted populations (e.g., men vs.¬†women), and using these benchmarks when interpreting the Œ∑2p effect size in designs that include covariates or repeated measures is not consistent with the onsiderations upon which the benchmarks were based.\n\nAlthough \\(\\eta_G^2\\) can be compared using these guidelines, it is preferable to compare effect sizes with those in the literature.",
    "crumbs": [
      "Post-Hoc Analysis",
      "Multilevel"
    ]
  },
  {
    "objectID": "qmd/feature-reduction.html",
    "href": "qmd/feature-reduction.html",
    "title": "Feature Reduction",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Feature Reduction"
    ]
  },
  {
    "objectID": "qmd/feature-reduction.html#sec-feat-red-misc",
    "href": "qmd/feature-reduction.html#sec-feat-red-misc",
    "title": "Feature Reduction",
    "section": "",
    "text": "Curse of Dimensionality\n\nIts when there are more variables than observations.\nCauses the least squares coefficient estimates to lose uniqueness.\nCauses overfitting in ML algorithms\n\nPackages\n\n{intRinsic} - Likelihood-Based Intrinsic Dimension Estimators; implements the ‚ÄòTWO-NN‚Äô and ‚ÄòGride‚Äô estimators and the ‚ÄòHidalgo‚Äô Bayesian mixture model\n\nProvides a clustering function for the Hidalgo model\nGraphical outputs built using ggplot2 so they are customizable\nSee section 5 (Summary and discussion) of the vignette for the recommended workflow and examples\n\n{Rdimtools} - feature selection, manifold learning, and intrinsic dimension estimation (IDE) methods\n\nCurrent version delivers 145 Dimension Reduction (DR) algorithms and 17 Intrinsic Dimension Estimator (IDE) methods.\n\n{RDRToolbox} - nonlinear dimension reduction with Isomap and LLE\n\nFor Time Series, see\n\nForecasting, Multivariate &gt;&gt; Dynamic Factor Models\n(Below) Dynamic Mode Decomposition\n{freqdom.fda} (Paper) - Dynamic Functional Principal Components Analysis (FPCA)",
    "crumbs": [
      "Feature Reduction"
    ]
  },
  {
    "objectID": "qmd/feature-reduction.html#sec-feat-red-terms",
    "href": "qmd/feature-reduction.html#sec-feat-red-terms",
    "title": "Feature Reduction",
    "section": "Terms",
    "text": "Terms\n\nIntrinsic Dimension (ID) - the minimal number of parameters needed to represent all the information contained in the data without significant information loss. A necessary piece of information to have before attempting to perform any dimensionality reduction, manifold learning, or visualization tasks. An indicator of the complexity of the features of a dataset.\nIsomap (IM) - nonlinear dimension reduction technique presented by Tenenbaum, Silva and Langford in 2000 [3, 4]. In contrast to LLE, it preserves global properties of the data. That means, that geodesic distances between all samples are captured best in the low dimensional embedding\nLocally Linear Embedding (LLE) - introduced in 2000 by Roweis, Saul and Lawrence. It preserves local properties of the data by representing each sample in the data by a linear combination of its k nearest neighbors with each neighbor weighted independently. LLE finally chooses the low dimensional representation that best preserves the weights in the target space.\nProjection Methods - maps the original data to a lower-dimensional space. The projection function can be linear, as in the case of PCA or or nonlinear, as in the case of locally linear embedding, Isomap, and tSNE.\nGeometric Methods - rely on the topology of a dataset, exploiting the properties of the distances between data points",
    "crumbs": [
      "Feature Reduction"
    ]
  },
  {
    "objectID": "qmd/feature-reduction.html#sec-feat-red-pca",
    "href": "qmd/feature-reduction.html#sec-feat-red-pca",
    "title": "Feature Reduction",
    "section": "PCA",
    "text": "PCA\n\nDescription\n\nCreates a subset of variables that maximises the covariance with the initial variable set, in order to store as much information as possible in a lower dimension.\nCompute an orthogonal basis of the space created by the original set of variables. The vectors creating this basis are the eigenvectors of the variance-covariance matrix. Reducing the dimension is then easily done by selecting the eigenvectors that are most representative of the initial data: those that contain most of the covariance. The amount of covariance stored by the vectors is quantified by the eigenvalues: the larger the eigenvalue, the more interesting its associated vectors.\nProjects variables orthogonally which removes correlation between predictor variables. The projection is in the direction of maximum variation such that the variation is distributed unequally among the transformed vectors. This¬†allows the user to reduce the feature space while still being able to capture most of variance in the data.\nThe principal components are equal to linear combinations of the correlated variables and these components are orthogonal to each other.\nWhy? When multiple variables are highly correlated to each other it causes the math used calculate regression models to break down. High dimension datasets also require large amount computational resources. Too many columns compared to the number of rows.\n\n\n\nMisc\n\nAs a multicollinearity detector?\n\n‚Äúuse principal component analysis, and examine the screeplot, or proportion of variation explained by a subset of principal components. If all (or almost all) of the variation is explained with a small subset of all the variables, it means you have a multicollinearity problem. You will need to drop some variables or do some other dimension reduction to fix it before choosing your final model.‚Äù\nI mean what if you have 20 variables and 3 are collinear, would this be detectable with PCA? I don‚Äôt think so. Seem more likely that it would take a large portion of your variables being collinear for it to be detectable in this fashion.\n\n\n\n\nPreprocessing\n\nNotes from thread\nCenter variables\n\ncenter = T is default in prcomp( )\nIf variables are NOT on similar scales, then the data need to be scaled, also.\n\nProbably safer to always scale.\n\n\nSqrt any count variables\nLog any variable with a heavy tail\nIf you have too many features, use a sparse matrix to speed the process.\n\n\n\nDiagnostics\n\nTest for localization (repo with R code/docs)\n\nBad: if you make a histogram of a component (or loading) vector and it has really big outliers (aka localization)\n\nMeans this vector is mostly noise\n\nSolution: Regularized spectral clustering (links to resources)\nD_r = Diagonal(1/ sqrt(rs + mean(rs))\nD_c = Diagonal(1/ sqrt(cs + mean(cs))\n# Do SVD on\nD_r %*% A %*% D_c\n\nA is your matrix\nrs is a vector containing the row sums of the matrix\ncs is a vector containing the column sums of the matrix\n\n\n\n\n\nSteps\n\nCenter data in design matrix, A (n x p)\n\nIf data are centered and scaled then the computation in step 2 will result in the correlation matrix instead of the covariance matrix.\n\nCompute \\(n \\times n\\) Covariance Matrix,\n\\[\nC_x = \\frac{1}{n-1}AA^T\n\\]\n\nAlso seen \\(A^T A\\) but I don‚Äôt think it matters. The upper triangle and the lower triangle of this product are just reverse covariances of each other and thus equal and I suspect the order just switches flips the triangles. The eigenvectors/eigenvalues get reordered later on anyways.\nThe diagonal of this matrix is the variable variances.\n\nCalculate eigenvectors and eigenvalues: \\(C_x V = D_\\lambda V\\) shows the covariance matrix as a transformation matrix. \\(D\\) is a diagonal matrix (\\(p\\times p\\)) with eigenvalues along the diagonal. \\(V\\) is a matrix (\\(p \\times p\\)) of eigenvectors\n\\[\nD_\\lambda = VC_x V^{-1}\n\\]\nOrder eigenvalues from largest to smallest\nOrder the eigenvectors according to the order of their corresponding eigenvalues\nEquation for the ith value of the PC1 vector: \\(\\text{PC1}_i = V_{(,1)} \\cdot A_{(i,)}\\)\n\n\\(\\text{PC2}\\) is similar except \\(V_{(,2)}\\) is used\nWhere all the variables in \\(A\\) have been standardized and \\(V\\) contains the loadings (see below)\n\n\n\n\nNotes\n\n\\(AA^T\\) is positive definite\n\nWhich means it‚Äôs symmetric\nWhich means it has real eigenvalues¬†and orthogonal eigenvectors\nWhich means the eigenvectors have covariances = 0\nWhich means the eigenvectors aren‚Äôt correlated.\n\nThe eigenvalues are eigenvector‚Äôs standard deviations which determines how much variance is explained by that PC.\nThe variance of a variable is the dot-product of itself and it‚Äôs transpose, \\(x_i \\cdot x^t_i\\)\nThe covariance between two variables, \\(x_i \\cdot x^t_j\\)\nIn step 3 equation, eigenvalues give the magnitude (length of vector) and eigenvectors the direction after being transformed by the covariance matrix.\nElements in a PC vector are called scores and elements in the V eigenvector are called loadings.\n\nThe loadings are the coefficients in the linear combination of variables that equals the PC vector\nLoadings range from -1 to 1\nVariables with high loadings (usually defined as .4 in absolute value or higher because this suggests at least 16% of the measured variable variance overlaps with the variance of the component) are most representative of the component\nThe sign of a loading (+ or -) indicates whether a variable and a principal component are positively or negatively correlated.\n\nScaling your design matrix variables just means your using a correlation matrix instead of a covariance matrix.\nPCA is sensitive to outliers. Variance explained will be inflated in the direction of the outlier\n\nGuessing this means components strongly influenced by variables with outlier values will have their variance-explained value inflated\n\nRow order of data matters as to which interpretation (latent) of component is valid from Principle Components and Penguins\n\nUsed data from palmerpenguins to create a ‚Äúpenguin size‚Äù variable from performing PCA on the data.\nIn one row order, high values of pc1 were associated with high body mass, but after scrambling the rows, high values of pc1 were associated with low body mass.\nHave to be careful when adding new data to the PCA-created feature. It might arbitrarily change the sign of the component and change the meaning of the feature.\n\nPCA doesn‚Äôt take the response variable into account (unsupervised). Therefore, the directions (eigenvectors) obtained may be well-suited for the predictor variables, but not necessarily optimal for predicting the response. It does often produce pretty good results though.\n\nAn alternative would be Partial Least Squares (PLS) which does take the response into account (supervised).\n\nIn practice, PLS reduces bias while potentially increasing the variance so the benefit vs PCA regression is usually a wash.\nCapable of handling multivariate regression\nPopular in chemometrics for analyzing spectra.\n\n\n\n\n\nPlots\n\nMisc\n\nNotes from: https://support.minitab.com/en-us/minitab/18/help-and-how-to/modeling-statistics/multivariate/how-to/principal-components/interpret-the-results/all-statistics-and-graphs/\n(See bkmk) use broom::augment to add original data to pca output. Coloring the points by categorical variables can help with interpreting the components\nAlso see pkgs in notebook for visualization options\n\nScore\n\n\nClusters\n\nIf data follow a multivariate normal distribution then scores should be randomly distributed around zero\nIf there are clusters, then there may be multiple distributions present\n\nExtreme points (e.g.¬†point in bottom right) might be outliers and it might be worthwhile to investigate them further\n\nLoadings\n\n\nNeed to imagine an axis at (0,0). Don‚Äôt know why they don‚Äôt plot them with those axes.\nFinding the largest variable influences on a PC can used to interpret it‚Äôs meaning (think latent variable)\nArrows\n\nA (near) horizontal arrow (along the x-axis) describes that the feature contributes strongly toward PC1.\nA (near) vertical arrow (along the y-axis) describes that a feature contributes strongly towards PC2.\n\nValues\n\nLoadings range from -1 to 1\nThe termination coordinate of the line gives the loading values for that variable for both PCs\nLoadings (absolute magnitude) close to 0 indicate that the variable has little influence on that PC\nLarger the absolute value of the loading the greater the influence on that PC\nNegative values have negative influence on the latent variable that the PC represents (vice versa for positive values)\n\nAngle\n\nacute angles represent a positive correlation between those variables\nobtuse angles represent a negative correlation between those variables\n90 degree angles represent independence between those variables\n\nExample\n\nAge, Residence, and Employ have large influences on PC1 (interpretation: financial stability)\nCredit cards, Debt, and Education have large influences on PC2 (interpretation: credit history)\nSays, ‚ÄúAs the number credit cards increases, credit history (PC2 interpretation) becomes more negative.‚Äù\n\n\nBi-Plot\n\n\nCombination plot of the score and loading plot\nCan augment pca output (see top of section) with original data and color the scores by different categorical variables\n\nIf a categorical variable level is clustered around Education, you could say as Education rises, the more likely that that person is &lt;categorical level&gt;.\nIn turn, that categorical level would be either positively or negatively (depending on the loading sign) associated with that PC.\n\n\nInterpretation\n\nExample: Bluejays\n\nLoadings\n\n\nPC2 represents the difference between bill size and skull size\n\nLoadings together with components plot\n\n\nMale birds larger than female birds\n\nIf you look at the loadings plot, negative pc1 corresponds to larger size and the components plot shows males with negative PC1 values\n\nBoth sexes have large and short bills relative to their overall size\n\nMales and females both show values above and below 0 in PC2\nLarger bills but smaller bodies (+PC2) and larger bodies but smaller bills (-PC2)\n\n\nVariance Explained\n\n\nOverall bird size explains &gt; 50% of the variation in measurements\n\n\n\nExample: How much variation in a principal component can be explained by a categorical variable\n# Penguins dataset\n# pca_values is a prcomp() object\npca_points &lt;-¬†\n¬† # first convert the pca results to a tibble\n¬† as_tibble(pca_values$x) %&gt;%¬†\n¬† # now we'll add the penguins data\n¬† bind_cols(penguins)\n## # A tibble: 6 x 12\n##¬† ¬† PC1¬† ¬† PC2¬† ¬† PC3¬† ¬† PC4 species island bill_length_mm bill_depth_mm\n##¬† &lt;dbl&gt;¬† &lt;dbl&gt;¬† &lt;dbl&gt;¬† &lt;dbl&gt; &lt;fct&gt;¬† &lt;fct&gt;¬† ¬† ¬† ¬† ¬† &lt;dbl&gt;¬† ¬† ¬† ¬† &lt;dbl&gt;\n## 1 -1.85 -0.0320¬† 0.235¬† 0.528 Adelie¬† Torge‚Ä¶¬† ¬† ¬† ¬† ¬† 39.1¬† ¬† ¬† ¬† ¬† 18.7\n## 2 -1.31¬† 0.443¬† 0.0274¬† 0.401 Adelie¬† Torge‚Ä¶¬† ¬† ¬† ¬† ¬† 39.5¬† ¬† ¬† ¬† ¬† 17.4\n## 3 -1.37¬† 0.161¬† -0.189¬† -0.528 Adelie¬† Torge‚Ä¶¬† ¬† ¬† ¬† ¬† 40.3¬† ¬† ¬† ¬† ¬† 18¬†\n## 4 -1.88¬† 0.0123¬† 0.628¬† -0.472 Adelie¬† Torge‚Ä¶¬† ¬† ¬† ¬† ¬† 36.7¬† ¬† ¬† ¬† ¬† 19.3\n## 5 -1.92 -0.816¬† 0.700¬† -0.196 Adelie¬† Torge‚Ä¶¬† ¬† ¬† ¬† ¬† 39.3¬† ¬† ¬† ¬† ¬† 20.6\n## 6 -1.77¬† 0.366¬† -0.0284¬† 0.505 Adelie¬† Torge‚Ä¶¬† ¬† ¬† ¬† ¬† 38.9¬† ¬† ¬† ¬† ¬† 17.8\n## # ‚Ä¶ with 4 more variables: flipper_length_mm &lt;int&gt;, body_mass_g &lt;int&gt;,\n## #¬† sex &lt;fct&gt;, year &lt;int&gt;\n\npc1_mod &lt;-¬†\n¬† lm(PC1 ~ species, pca_points)\nsummary(pc1_mod)\n## Call:\n## lm(formula = PC1 ~ species, data = pca_points)\n##¬†\n## Residuals:\n##¬† ¬† Min¬† ¬† ¬† 1Q¬† Median¬† ¬† ¬† 3Q¬† ¬† Max¬†\n## -1.3011 -0.4011 -0.1096¬† 0.4624¬† 1.7714¬†\n##¬†\n## Coefficients:\n##¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† Estimate Std. Error t value Pr(&gt;|t|)¬† ¬†\n## (Intercept)¬† ¬† ¬† -1.45753¬† ¬† 0.04785¬† -30.46¬† &lt;2e-16 ***\n## speciesChinstrap¬† 1.06951¬† ¬† 0.08488¬† 12.60¬† &lt;2e-16 ***\n## speciesGentoo¬† ¬† 3.46748¬† ¬† 0.07140¬† 48.56¬† &lt;2e-16 ***\n## ---\n## Signif. codes:¬† 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n##¬†\n## Residual standard error: 0.5782 on 330 degrees of freedom\n## Multiple R-squared:¬† 0.879,¬† Adjusted R-squared:¬† 0.8782¬†\n## F-statistic:¬† 1198 on 2 and 330 DF,¬† p-value: &lt; 2.2e-16\n\nFrom https://bayesbaes.github.io/2021/01/28/PCA-tutorial.html\nAdjusted R-squared: 0.8782\n\n\nCan be seen visually in this chart by looking at the points in relation to the x-axis where species is segregated pretty nicely.\n\n\n\n\n\nOutliers\n\nMahalanobis Distance (MD)\n\nThis method might be problematic. Supposedly outliers affect the covariance matrix which affects the PCA, which affects the scores, which affects theMahalanobis distance (MD). So the MD might be biased and not be accurate in determining outliers\n\nRobust forms of PCA (see section below) would be recommended if you suspect outliers in your data.\n\nDisplays the Mahalanobis distance (MD) for each observation and a reference line to identify outliers. The Mahalanobis distance is the distance between each data point and the centroid of multivariate space (the overall mean).\n\nOutliers determined by whether the Mahalanobis Distance is greater than the square root of the Chi-Square statistic where m is the number of variables and Œ± = 0.05\n\nNo outliers in the chart above as all MDs lower than the threshold at 4.4\n\n\nLeverage Points and Orthogonal Outliers\n\nNotes from https://towardsdatascience.com/multivariate-outlier-detection-in-high-dimensional-spectral-data-45878fd0ccb8\nTypes\n\nLeverage Points\n\ncharacterized by a high score distance\ngood leverage points also have short orthogonal distance and bad leverage points have long orthogonal distances\ngood leverage points have a positive effect\n\nOrthogonal Outliers\n\ncharacterized by a high orthogonal distance\n\n\nType determined by (see article for the math)\n\nScore DIstance (SD) - the distance an observation is from center of K-dimensional PCA subspace\nOrthogonal Distance (OD) -¬†the deviation ‚Äî i.e.¬†lack of fit ‚Äî of an observation from the k-dimensional PCA subspace\nOutliers are determined by Chi-Square test very similar to the¬†Mahalanobis Distance method (see above).\n\n\n\nIn the example shown, the red dots are data known to be measurement errors. Most of the red dots are captured in the orthogonal and bad sections but quite a few normal observation (blue) points too. So this method needs to be used as a guide and followed up upon when it flags points.\n\n\n\nHotelling‚Äôs T2 and SPE/DmodX (Complementary Tests)\n\n{{pca}}\nHotelling‚Äôs T2 works by computing the chi-square tests across the top n_components for which the p-values are returned that describe the likeliness of an outlier. This allows for ranking the outliers from strongest to weak.\nSPE/DmodX (distance to model) based on the mean and covariance of the first 2 PCs\n\n\n\n\nExtensions\n\nRobust PCA\n\nData with outliers and high dimensional data (p &gt;&gt; n) are not suitable for regular PCA where p is the number of variables.\nLow Dim methods (only valid when n &gt; 2p) that find robust (against outliers) estimates of the covariance matrix\n\nS-estimator, MM-estimator, (Fast)MCD-estimator, re-weighted MCD- (RMCD) estimator\n\nHigh Dim Methods\n\nRobust PCA by projection-pursuit (PP-PCA)\n\nfinds directions for eigenvectors that maximize a ‚Äúprojection index‚Äù instead of directions that maximize variance\n\nMAD or Qn-estimator is used a projection index\n\n\nSpherical PCA (SPCA)\n\nhandles outliers by projecting points onto a sphere instead of a line or plane\n\nalso uses MAD or Qn-estimator\n\n\nRobust PCA (ROBPCA)\n\ncombines projection index approach with low dim robust covariance estimation methods somehow\n\nRobust Sparse PCA (ROSPCA)\n\nsame but uses sparse pca\napplicable to both symmetrically distributed data and skewed data\n\n\n\nKernel PCA\n\nPackages: {kernlab}\nNonlinear data (notebook)\nPCA in a hypothetical (kernel trick), higher dimensional space\nWith more dimensions, data points become more separable.\nResults depend on type of kernel\n\nGaussian Kernel\n\nTuning parameter: sigma\n\n\n\n\n\n\nTidymodels\n\nRecipe step\n# if only using dummy vars, no sure if normalization is necessary\n# step_normalize(&lt;pca variables&gt;)\nstep_pca(starts_with(\"tf_\"), num_comp = tune())\n# don't forget to include num_comp in your tuning grid\nTaking a tidymodel‚Äôs recipe object and performing PCA\ntf_mat &lt;- recipe_obj %&gt;%\n¬† ¬† # normalizing tokenized indicators (?)\n¬† ¬† # since these are all dummy vars, not sure if a normalization step is necessary)\n¬† ¬† step_normalize(starts_with(\"tf_\")) %&gt;%\n¬† ¬† prep() %&gt;%\n¬† ¬† bake() %&gt;%\n¬† ¬† # only want to pca text features\n¬† ¬† select(starts_with(\"tf_\") %&gt;%\n¬† ¬† as.matrix()\n\ns &lt;- svd(tf_mat)\n# scree plot\ntidy(s, matrix = \"d\") %&gt;%\n¬† ¬† filter(PC &lt;= 50) %&gt;%\n¬† ¬† ggplot(aes(x = PC, y = percent)) +\n¬† ¬† geom_point()\n\nmatrix (tidy arg):\n\n‚Äúu‚Äù, ‚Äúsamples‚Äù, ‚Äúscores‚Äù, or ‚Äúx‚Äù: Returns info about the map from the original space to the pc space\n‚Äúv‚Äù, ‚Äúrotation‚Äù, ‚Äúloadings‚Äù, or ‚Äúvariables‚Äù: Returns information about the map from the pc space to the original space\n‚Äúd‚Äù, ‚Äúeigenvalues‚Äù, or ‚Äúpcs‚Äù: Returns information about the eigenvalues\n\n\nExample\nlibrary(tidymodels)¬†\nlibrary(workflowsets)¬†\nlibrary(tidyposterior)¬†\ndata(meats, package= \"modeldata\")¬†\n# Keep only the water outcome¬†\nmeats &lt;- select(meats, -fat, -protein)¬†\nset.seed(1)¬†\nmeat_split &lt;- initial_split(meats)¬†\nmeat_train &lt;- training(meat_split)¬†\nmeat_test &lt;- testing(meat_split)¬†\nset.seed(2)¬†\nmeat_folds &lt;- vfold_cv(meat_train, repeats = 3)\nbase_recipe &lt;-¬†\n¬† recipe(water ~ ., data = meat_train) %&gt;%¬†\n¬† step_zv(all_predictors()) %&gt;%¬†\n¬† step_YeoJohnson(all_predictors()) %&gt;%¬†\n¬† step_normalize(all_predictors())¬†\npca_recipe &lt;-¬†\n¬† base_recipe %&gt;%¬†\n¬† step_pca(all_predictors(), num_comp = tune())¬†\npca_kernel_recipe &lt;-¬†\n¬† base_recipe %&gt;%¬†\n¬† step_kpca_rbf(all_predictors(), num_comp = tune(), sigma = tune())",
    "crumbs": [
      "Feature Reduction"
    ]
  },
  {
    "objectID": "qmd/feature-reduction.html#sec-feat-red-efa",
    "href": "qmd/feature-reduction.html#sec-feat-red-efa",
    "title": "Feature Reduction",
    "section": "Exploratory Factor Analysis (EFA)",
    "text": "Exploratory Factor Analysis (EFA)\n\nIdentifies a number of latent factors that explain correlations between observed variables\n\nFrequently employed in social sciences where the main interest lies in measuring and relating unobserved constructs such as emotions, attitudes, beliefs and behaviour.\nLatent variables, referred to also as factors, account for the dependencies among the observed variables, referred to also as items or indicators, in the sense that if the factors are held fixed, the observed variables would be independent.\nIn exploratory factor analysis the goal is the following: for a given set of observed variables x1, . . . , xp one wants to find a set of latent factors Œæ1, . . . , Œæk, fewer in number than the observed variables (k &lt; p), that contain essentially the same information.\nIn confirmatory factor analysis, the objective is to verify a social theory. Hence, a factor model is specifed in advance and its fit to the empirical data is tested.\n\nMisc\n\nPackages\n\n{psych} - factor analysis, item response theory, reliability analysis\n{factominer} - Multiple Factor Analysis (MFA}\n{fspe} - Model selection method for choosing number of factors\n\nUses the connection between model-implied correlation matrices and standardized regression coefficients to do model selection based on out-of-sample prediction errors\n\n\nTwo main approaches for analysing ordinal variables with factor models:\n\nUnderlying Response Variable (URV)\n\nThe ordinal variables are generated by underlying continuous variables partially observed through their ordinal counterparts. (also see Regression, Ordinal &gt;&gt; Cumulative Link Models (CLM))\n\nItem Response Theory (IRT)\n\nOrdinal indicators are treated as they are.\n\n\n\nMethods for selecting the right number of factors\n\nMisc\n\nIssue: more factors always improve the fit of the model\n\nParallel Analysis: analyze the patterns of eigenvalues of the correlation matrix\nModel Selection: likelihood ratio tests or information criteria\n\nComparison with PCA\n\nPCA is a technique for reducing the dimensionality of one‚Äôs data, whereas EFA is a technique for identifying and measuring variables that cannot be measured directly (i.e.¬†latent factor)\nWhen variables don‚Äôt have anything in common, EFA won‚Äôt find a well-defined underlying factor, but PCA will find a well-defined principal component that explains the maximal amount of variance in the data.\nDifferences in the results between PCA and EFA don‚Äôt tend to be obvious in practice. As the number of variables (&gt;40 variables) involved in the analysis grows, results from PCA and EFA become more and more similar.\nSimilarly calculated method to PCA, but FA is an analysis on a reduced correlation matrix, for which the ones in the diagonal have been replaced by squared multiple correlations (SMC)\n\nA SMC is the estimate of the variance that the underlying factor(s) explains in a given variable (aka communality).\n\nThe variability in measured variables in PCA causes the variance in the principal component. This is in contrast to EFA, where the latent factor is seen as causing the variability and pattern of correlations among measured variables\nAn eigenvalue decomposition of the full correlation matrix is done in PCA, yet for EFA, the eigenvalue decomposition is done on the reduced correlation matrix\nFactor Analysis is a latent variable measurement model\n\nThe causal relationship is flipped in FA as compared to PCA.\n\n\nF is the latent variable (instead of component in PCA), b is a weight (like loadings in PCA), Y is a predictor variable, and u is an error\n\nHere, b estimates how much F contributes to Y",
    "crumbs": [
      "Feature Reduction"
    ]
  },
  {
    "objectID": "qmd/feature-reduction.html#sec-feat-red-autoenc",
    "href": "qmd/feature-reduction.html#sec-feat-red-autoenc",
    "title": "Feature Reduction",
    "section": "Autoencoders",
    "text": "Autoencoders\n\nUnsupervised neural networks that learn efficient coding from the input unlabelled data. They try to reconstruct the input data by minimizing the reconstruction loss\nMisc\n\nUndercomplete Autoencoder (AE) ‚Äî the most basic and widely used type, frequently referred to as an Autoencoder\nSparse Autoencoder (SAE) ‚Äî uses sparsity to create an information bottleneck\nDenoising Autoencoder (DAE) ‚Äî designed to remove noise from data or images\nVariational Autoencoder (VAE) ‚Äî encodes information onto a distribution, enabling us to use it for new data generation\n\nLayers\n\nEncoder: Mapping from Input space to lower dimension space\nDecoder: Reconstructing from lower dimension space to Output space\n\nProcess\n\n\nEncodes the input data (X) into another dimension (Z), and then reconstructs the output data (X‚Äô) using a decoder network\nThe encoded embedding (Z) is preferably lower in dimension compared to the input layer and contains all the efficient coding of the input layer\nOnce the reconstruction loss is minimized, the learned weights or embeddings, in the Encoder layer can be used as features in ML models and the Encoder layer can be used to generate embeddings on future data.\n\nSparse Autoencoder (SE)\n\n\nUses regularization\nDimension reduction in the center is achieved through deactivating neurons\nExample\n\nThe model consists of 5 layers: one input, three hidden and one output.\nInput and output layers contain 784 neurons each (the shape of our data, i.e number of columns), with the size of hidden layers reduced to 16 neurons each.\nWe will train the model over 50 epochs and plot a loss chart (see below).\nWe will separate the encoder part of the model and save it to our project directory. Note, if you are not planning to reuse the same model afterwards, you don‚Äôt need to keep a copy of it.",
    "crumbs": [
      "Feature Reduction"
    ]
  },
  {
    "objectID": "qmd/feature-reduction.html#sec-feat-red-dmd",
    "href": "qmd/feature-reduction.html#sec-feat-red-dmd",
    "title": "Feature Reduction",
    "section": "Dynamic Mode Decomposition (DMD)",
    "text": "Dynamic Mode Decomposition (DMD)\n\nCombines PCA and fourier transform\nSupposed to handle time series better than PCA\n{{pydmd}}",
    "crumbs": [
      "Feature Reduction"
    ]
  },
  {
    "objectID": "qmd/forecasting-statistical.html",
    "href": "qmd/forecasting-statistical.html",
    "title": "Statistical",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Forecasting",
      "Statistical"
    ]
  },
  {
    "objectID": "qmd/forecasting-statistical.html#sec-fcast-stat-misc",
    "href": "qmd/forecasting-statistical.html#sec-fcast-stat-misc",
    "title": "Statistical",
    "section": "",
    "text": "Packages\n\nCRAN Task View\n\nFor intermittent data, see Logistics &gt;&gt; Demand Planning &gt;&gt; Intermittent Demand\nLet the context of the decision making process determine the units of the forecast\n\ni.e.¬†Don‚Äôt forecast on a hourly scale just because you can.\n\nWhat can be forecast depends on the predictability of the event:\n\nHow well we understand the factors that contribute to it;\n\nWe have a good idea of the contributing factors: electricity demand is driven largely by temperatures, with smaller effects for calendar variation such as holidays, and economic conditions.\n\nHow much data is available;\n\nThere is usually several years of data on electricity demand available, and many decades of data on weather conditions.\n\nHow similar the future is to the past;\n\nFor short-term forecasting (up to a few weeks), it is safe to assume that demand behaviour will be similar to what has been seen in the past.\n\nWhether the forecasts can affect the thing we are trying to forecast.\n\nFor most residential users, the price of electricity is not dependent on demand, and so the demand forecasts have little or no effect on consumer behaviour.\n\n\nStarting a project\n\nUnderstand the dgp through eda and talking to domain experts\n\nHow are sales generated? (e.g.¬†online, brick and mortar,‚Ä¶)\n\nWhat is the client currently using to forecast?\n\nModel that you need to beat\nWhere does it fail?\n\nBiased? underfitting or overfitting somewhere\nMissing seasonality?\n\n\nWhat is the loss function?\n\nCarrying this many items in inventory results in this cost\nIf we‚Äôre out of stock and lose this many sales, how much does this cost\n\nWhat does the client really want?\n\nHow is success measured\n\n\nFable models produce different results with NAs in the time series\n\nIn rolling cfr project, steinmetz‚Äôs manually calc‚Äôd rolling 7-day means and his lagged vars had NAs, models using data with and without NAs had different score\n\nIt is helpful to keep track of and understand what our forecast bias has historically been.¬† Even where we are fortunate enough to show a history of bias in both directions.\nForecasting shocks is difficult for an algorithm\n\nIt can better to smooth out (expected) shocks (Christmas) in the training data and then add an adjustment to the predictions during the dates of the shocks.\nThe smoothed out data will help the algorithm produce more accurate predictions for days when there isn‚Äôt an expected shock.\nExamples of shocks that may need training data to have manual adjustments and not be smoothed by an algorithm\n\nOne-Time spikes due to abnormal weather conditions\nOne-Off promotions\nA sustained marketing campaign that is indistinguishable from organic growth.\n\n\nIntermittent(or sporadic) time series (lotsa zeros).\n\n{thief} has the latest methods while {tsintermittent} has older methods\n\nBenchmark models\n\nNaive\n28-day moving average (i.e.¬†4 week MA)",
    "crumbs": [
      "Forecasting",
      "Statistical"
    ]
  },
  {
    "objectID": "qmd/forecasting-statistical.html#sec-fcast-stat-terms",
    "href": "qmd/forecasting-statistical.html#sec-fcast-stat-terms",
    "title": "Statistical",
    "section": "Terms",
    "text": "Terms\n\nWeak stationarity (commonly referred to as just stationarity)(aka covariance stationary) - Implies that the mean and the variance of the time series are finite and do not change with time.\nCointegration - \\(x_t\\) and \\(y_t\\) are cointegrated if \\(x_t\\) and \\(y_t\\) are \\(I(1)\\) series and there exists a \\(\\beta\\) such that \\(z_t = x_t - \\beta y_t\\) is an \\(I(0)\\) series (i.e.¬†stationary).\n\nImportant for understanding stochastic or deterministic trends.\nThe differences in the means of the set of cointegrated series remain constant over time, without offering an indication of directionality\nMight have low correlation, and highly correlated series might not be cointegrated at all.\nCan use Error Correction Model (ECM) with differenced data and inserting a error correction term (residuals from a OLS regression)\n\nStochastic - No value of a variable is known with certainty. Some values may be more likely than others (probabilistic). Variable gets mapped onto a distribution.",
    "crumbs": [
      "Forecasting",
      "Statistical"
    ]
  },
  {
    "objectID": "qmd/forecasting-statistical.html#sec-fcast-stat-preproc",
    "href": "qmd/forecasting-statistical.html#sec-fcast-stat-preproc",
    "title": "Statistical",
    "section": "Preprocessing",
    "text": "Preprocessing\n\nFilling in gaps\n\nBi-directional forecast method from AutoML for time series: advanced approaches with FEDOT framework\n\nSteps\n\nSmooth series prior to the gap\n\nThey used a ‚ÄúGaussian filter w/sigma = 2‚Äù (not sure what that is)\n\nCreate lagged features of the smoothed series\nForecast using ridge regression where h = length of gap\nRepeat in the opposite direction using the series after the gap\nUse the average of the forecasts to fill the gap in the series.\n\n\n\nLog before differencing (SO post)\nDetrend or Difference\n\n(The goal is to get a stationary series, so if one doesn‚Äôt work try the other.)\nDifferencing (for unit root processes)(stochastic trend)\n\nif the process requires differencing to be made stationary, then it is called difference stationary and possesses one or more unit roots.\n\nSometimes see charts of roots and a unit circle. I read this in an article about VAR models ‚Äúprocess is stationary if all the roots \\(z_1, \\ldots , z_n\\) of the determinant \\(\\det(\\psi(z))\\), or \\(\\det(I ‚àí Bz) = 0\\), lie outside of the unit circle.‚Äù\n\nOne advantage of differencing over detrending to remove trend is that no parameters are estimated in the differencing operation.\nOne disadvantage, however, is that differencing does not yield an estimate of the stationary process\nIf the goal is to coerce the data to stationarity, then differencing may be more appropriate.\nDifferencing is also a viable tool if the trend is fixed\n\nRandom Walking looking series should be differenced and not detrended.\n\nBackshift operator notation:\n\nIn general: \\(\\nabla^d = (1 ‚àí B)^d\\)\n\nWhere \\(d\\) is the order of differencing\nFractional differencing is when \\(0 \\lt d \\lt 1\\)\n\nWhen \\(0 \\lt d \\lt 0.5\\), the series is classified as a long term memory series (often used for environmental time series arising in hydrology)\n\nIf \\(d\\) is negative, then its called forward-shift differencing\n\nExamples:\n\nIdentities:\n\\[\nB y_t = y_{t-1} \\\\\nB^2 y_t = y_{t-2}\n\\]\nSeasonal Difference:\n\\[\n(1 - B)(1 - B^m) y_t = (1 - B - B^m + B^{m + 1})y_t = y_t - y_{t-1} - y_{t-m} + y_{t-m-1}\n\\]\nARIMA : AR(p)I(d) = MA(q)\n\\[\n(1-\\phi_1 B - \\cdots - \\phi_p B^p)(1-B)^d y_t = c+(1+\\theta_1 B + \\cdots + \\theta_q B^q)\\epsilon_t\n\\]\nARIMA(1,1,1)(1,1,1)4 for quarterly data (m = 4)\n\\[\n(1-\\phi_1 B)(1-\\Phi B^4)(1-B)(1-B^4)y_t = (1+\\theta_1 B)(1+\\Theta B^4)\\epsilon_t\n\\]\n\n\n\nDetrending (for trend-stationary processes)(deterministic trend)\n\nIt is possible for a time series to be non-stationary, yet have no unit root and be trend-stationary\n\na trend-stationary process is a stochastic process from which an underlying trend (function solely of time) can be removed (detrended), leaving a stationary process.\n\nIf an estimate of the stationary process is essential, then detrending may be more appropriate.\nHow is this back-transformed after forecasting?\n\nmaybe look at ‚Äúforecasting with STL‚Äù section in fpp2\n\n\nIn both unit root and trend-stationary processes, the mean can be growing or decreasing over time; however, in the presence of a shock, trend-stationary processes are mean-reverting (i.e.¬†transitory, the time series will converge again towards the growing mean, which was not affected by the shock) while unit-root processes have a permanent impact on the mean (i.e.¬†no convergence over time).\nTesting\n\nKPSS test: H0 = Trend-Stationary, Ha = Unit Root.\n\nurca::ur_kpss the H0 is stationarity\ntseries::kpss.test(res, null = \"Trend\") where H0 is ‚Äútrend-stationarity‚Äù\n\nDickey-Fuller tests: H0 = Unit Root, Ha = Stationary or Trend-Stationary depending on version\nKPSS-type tests are intended to complement unit root tests, such as the Dickey‚ÄìFuller tests. By testing both the unit root hypothesis and the stationarity hypothesis, one can distinguish series that appear to be stationary, series that appear to have a unit root, and series for which the data (or the tests) are not sufficiently informative to be sure whether they are stationary.\n\nSteps:\n\nADF:\n\nIf H0 rejected. The trend (if any) can be represented by a deterministic linear trend.\nIf H0 is not rejected then we apply the KPSS test.\n\nKPSS :\n\nIf H0 rejected then we conclude that there is a unit root and work with the first differences of the data.\n\nUpon the first differences of the series we can test the significance of other regressors or choose an ARMA model.\n\nIf H0 is not rejected then data doesn‚Äôt contain enough information. In this case it may be safer to work with the first differences of the series.\n\n\nSteps when using an ARIMA:\n\nSuppose the series is not trending\n\nIf the ADF test (without trend) rejects, then apply model directly\nIf the ADF test (without trend) does not reject, then model after taking difference (maybe several times)\n\nSuppose the series is trending\n\nIf the ADF test (with trend) rejects, then apply model after detrending the series\nIf the ADF test (with trend) does not reject, then apply model after taking difference (maybe several times)",
    "crumbs": [
      "Forecasting",
      "Statistical"
    ]
  },
  {
    "objectID": "qmd/forecasting-statistical.html#sec-fcast-stat-diag",
    "href": "qmd/forecasting-statistical.html#sec-fcast-stat-diag",
    "title": "Statistical",
    "section": "Diagnostics",
    "text": "Diagnostics\n\nTesting for significant difference between model forecasts\n\nNemenyi test\n\nsutils::nemenyi\n\nMCB\n\ngreybox::rmcb",
    "crumbs": [
      "Forecasting",
      "Statistical"
    ]
  },
  {
    "objectID": "qmd/forecasting-statistical.html#sec-fcast-stat-alg",
    "href": "qmd/forecasting-statistical.html#sec-fcast-stat-alg",
    "title": "Statistical",
    "section": "Algorithms",
    "text": "Algorithms\n\nMisc\n\nAuto Arima, ETS and Theta are general-purpose methods particularly well-suited for monthly, quarterly and annual data\nTBATS and STL will also handle multiple seasonalities such as arise in daily and weekly data.\nWhen to try nonlinear models (see Forecasting, Nonlinear)\n\nLinear prediction methods (e.g.¬†ARIMA) don‚Äôt produce adequate predictions\nChaotic nature of the time series is obvious (e.g.¬†frequent, unexplainable shocks that can‚Äôt be explained by noise)\n\n\n\n\nRegression (including ARIMA)\n\nMisc\n\nDouble check auto_arima, for the parameters, (p, d, q), one should pick q to be at least p (link)\nSometimes the error terms are called random shocks.\nIf using lm and there are NAs, make sure to use na.action = NULL else they get removed and therefore dates between variables won‚Äôt match-up. See lm doc for further details on best practices.\nARIMA models make h-step out predictions by iterating 1-step forward predictions and feeding the intermediate predictions in as if they were actual observations (0 are used for the errors)\nPolynomial Autoregression (AR) models exponentiate the lags. So the design matrix includes the lags and the exponentiated series.\n\nIf the polynomial is order 3, then order 2 is also included. So, now, the design matrix would be the lags, the square of each lag, and the cube of each lag\nExample:\nlibrary(dplyr); library(timetk)\n# tbl w/polynomial design matrix of order 3\n# value is the ts values\npoly_tbl &lt;- group_tbl %&gt;%\n¬† tk_augment_lags(.value = value, .lags = 1:4) %&gt;%\n¬† mutate(across(contains(\"lag\"), \n¬†        .fns = list(~.x^2, ~.x^3), \n¬†        .names = \"{.col}_{ifelse(.fn==1, 'quad','cube')}\"))\n\n.fn is the item number in the .fns list.\nSquared lag 2 will have the name ‚Äúvalue_lag2_quad‚Äù\n\n\n\nTypes\n\nAR: single variable with autoregressive dependent variable terms\nARMA: same as AR but errors models as a moving average\nARIMA: same as ARMA but with differencing the timeseries\nSARIMA: same as ARIMA but also with seasonal P, D, Q terms\nARMAX: same as ARMA but with additional exogenous predictors\nDynamic Regression: OLS regression with modeled (usually arima) errors\n\nOLS vs ARIMA\n\nJohn Mount\n\nThe fear in using standard regression for time series problems is that the error terms are likely correlated.\n\nSo one can no longer appeal to the Gauss Markov Theorem (i.e.¬†OLS is BLUE) to be assured of good out of sample performance (link)\n\n\nRyer and Chan regarding Dynamic Regression vs OLS\n\n‚ÄúRegression (with arima errors) coefficient estimate on Price is similar to that from the OLS regression fit earlier, but the standard error of the estimate is about 10% lower than that from the simple OLS regression. This illustrates the general result that the simple OLS estimator is consistent but the associated standard error is generally not trustworthy‚Äù\n\nHyndman\n\n‚ÄúThe forecasts from a regression model with autocorrelated errors are still unbiased, and so are not ‚Äúwrong,‚Äù but they will usually have larger prediction intervals than they need to. Therefore we should always look at an ACF plot of the residuals.‚Äù\nThe estimated coefficients are no longer the best estimates, as some information has been ignored in the calculation;\n\nMeaning modeling the errors to take into account the autocorrelation\n\nAny statistical tests associated with the model (e.g., t-tests on the coefficients) will be incorrect.\n\nAffected by the bloated std errors\n\nThe AICc values of the fitted models are no longer a good guide as to which is the best model for forecasting.\nIn most cases, the p-values associated with the coefficients will be too small, and so some predictor variables will appear to be important when they are not. This is known as ‚Äúspurious regression.‚Äù\n\n\nTrend (\\(\\beta\\) \\(t\\)) is modeled by setting the variable \\(t\\) to just an index variable (i.e.¬†\\(t = 1, \\ldots, T\\)). Modeling quadratic trend would be adding in \\(t^2\\) to the model formula.\n\nHyndman suggests that using splines is a better approach than using t2\nFrom Steinmitz‚Äôs CFR article\n\nInstead of trend (like in {forecast}), he‚Äôs using poly(date, 2) to include a quadratic trend\n\n\nR2 and Adjusted-R2\n\nAppropriate for time series (i.e.¬†estimate of the population R2), as long as the data are stationary and weakly dependent\n\ni.e.¬†The variances of both the errors and the dependent variable do not change over time.\ni.e.¬†If \\(y_t\\) has a unit root (Integrated of order 1, I(1)) (needs differenced)\n\n\nInterpretation of coefficients\n\\[\ny_t = \\alpha + \\beta_0 x_t + \\beta_1 x_{t-1} + \\cdots + \\beta_s x_{t-s} + \\cdots + \\beta_q x_{t-q}\n\\]\n\nIf \\(x\\) increases by one unit today, the change in \\(y\\) will be \\(\\beta_0+\\beta_1+...+\\beta_s\\) after \\(s\\) periods; This quantity is called the \\(s\\)-period interim multiplier. The total multiplier is equal to the sum of all \\(\\beta\\) s in the model.\n\nResiduals\n\nTypes\n\n‚ÄúRegression‚Äù is for the main model\n\nOriginal data minus the effect of the regression variables\n\n‚ÄúInnovation‚Äù is for the error model\n\nDefault arg\nHyndman uses these for dynamic regression residual tests\n\n\nAutocorrelation tests\n\nFailing the test does not necessarily mean that (a) the model produces poor forecasts; or (b) that the prediction intervals are inaccurate. It suggests that there is a little more information in the data than is captured in the model. But it might not matter much.\nBreusch-Godfrey test designed for pure regression or straight AR model\n\nDoes handle models with lagged dependent vars as predictors\nLM (lagrange multiplier) test\nforecast::checkresiduals can calculate it and display it, but you don‚Äôt have access to the values programmatically\n\nDefaults for lag is \\(\\min(10,n/5)\\) for nonseasonal and \\(\\min(2m, n/5)\\) for seasonal where the frequency is seasonality, m\nlag &lt;- ifelse(freq &gt; 1, 2 * freq, 10)\nlag &lt;- min(lag, round(length(residuals)/5))\nlag &lt;- max(df+3, lag)\n\n{lmtest} and {DescTools} (active) packages have the function that forecast uses but only takes lm objects\n\nDurbin-Watson designed for pure regression\n\nError term can‚Äôt be correlated with predictor to use this test\n\nSo no lagged dependent variables can be used as predictors\nThere is an durbin alternate test mentioned in stata literature that can do lagged variables but I haven‚Äôt seen a R version that specifies that‚Äôs the version it is.\n\n{lmtest} and {DescTools} takes a lm object and has a small sample size correction available\n{car::durbinWatsonTest} takes a lm object or residual vector.\n\nOnly lm returns p-value. Residual vector returns DW statistic\n\np-values \\(\\lt 0.05\\) \\(\\rightarrow\\) Autocorrelation present\nDW statistic guide (\\(0 \\lt \\text{DW} \\lt 4\\))\n\nAround 2 \\(\\rightarrow\\) No Autocorrelation\nSignifcantly \\(\\lt 2\\) \\(\\rightarrow\\) Positive Correlation\n\nSaw values \\(\\lt 1\\) have p-values = 0\n\nSignificantly \\(\\gt 2\\) \\(\\rightarrow\\) Negative Correlation\n\n\nLjung-Box\n\nFor dynamic regression, arima, ets, etc.\n\nThere‚Äôs a SO post that shows this shouldn‚Äôt be used for straight regression\n\nFor straight AR models, the comments show it should be fine as long as lags \\(\\gt\\) model [df]{arg-text} (see below)\n\n\nTest is whether a group of lagged residuals has significant autocorrelation, so an acf of the residuals might show individual spikes but the group as a whole may not have significant autocorrelation\n\nIf you see a spike in the residuals, may be interesting to include that lag number in the group of lags and see if significance of the group changes\n\n{feasts::ljung_box}\n\nRequires numeric residuals vector, model degrees of freedom, number of lags to check\n\nThe model df is number of variables used in the regression + intercept + p + q (of ARIMA error model)\n\ne.g.¬†Model with predictors: trend + cases and an error model: arima (2,1,1) had df = 2 (predictors: trend, cases) + 1 (intercept) + 2 (p) + 1 (q) = 6 d.f.\ndof &lt;- length(fit$coef)\n\nSee Breusch-Godfrey section for number of lags to use\n\n\np-values \\(\\lt 0.05\\) \\(\\rightarrow\\) autocorrelation present\n\n\n\nSpectral analysis takes the approach of specifying a time series as a function of trigonometric components (i.e.¬†Regression with fourier terms)\n\nA smoothed version of the periodogram, called a spectral density, can also be constructed and is generally preferred to the periodogram.\n\n\n\n\nRandom Walk\n\nA process integrated to order 1, (an I(1) process) is one where its rate of change is stationary. Brownian motion is a canonical I(1) process because its rate of change is Gaussian white noise, which is stationary. But the random walk itself is not stationary. So the \\(t+1\\) value of a random walk is just the value at \\(t\\) plus a number sampled from some bell curve.\nCharacteristics\n\nLong periods of apparent trends up or down\nSudden and unpredictable changes in direction\n\nA special case of an autoregressive model\n\\[\ny_t = c + \\phi_1 y_{t-1} + \\cdots + \\phi_p y_{t-p} + \\epsilon_t\n\\]\n\nWhere \\(c=0\\), \\(p=1\\), \\(\\phi = 1\\), and \\(\\epsilon \\sim \\mathcal {N}(0, s)\\)\n\nDrift\n\n\n\n\n\n\n\n\nFeature\nRandom Walk without Drift\nRandom Walk with Drift\n\n\n\n\nSteps\nPurely random, equal probability left/right\nBiased, one direction slightly more likely\n\n\nChange in value\nAverage change is zero\nAverage change includes a constant drift\n\n\nPath\nZig-zag around starting point\nZig-zag with upward/downward trend\n\n\nMean\nStays roughly the same\nIncreases/decreases over time depending on drift\n\n\nVariance\nIncreases with time\nIncreases with time\n\n\nStationarity\nNon-stationary\nNon-stationary\n\n\n\nExamples with and without drift\n\n\n\n\n\n\nProphet\n\nThe basic methodology is an iterative curve-matching routine, where Prophet will then train your data on a bigger period, then predict again and this will repeat until the end point is reached.\nThe development team of Prophet claim that its strengths are:\n\nWorking with high-frequency data (hourly, daily, or weekly) with multi-seasonality, such as hour of day, day of week and time of year;\nSpecial events and bank holidays that are not fixed in the year;\nAllowing for the presence of a reasonable number of missing values or large outliers;\nAccounting for changes in the historical trends and non-linear growth curves in a dataset.\n\nFurther advantages include the ability to train from a moderate sized dataset, without the need for specialist commercial software, and fast start up times for development.\nDisadvantages\n\nNo autoregressive (i.e.¬†lags of target series) features since it‚Äôs a curve-fitting algorithm\n\nTime series decomposition by prophet:\n\n\\(g(t)\\): Logistic or linear growth trend with optional linear splines (linear in the exponent for the logistic growth). The library calls the knots ‚Äúchange points.‚Äù\n\\(s(t)\\): Sine and cosine (i.e.¬†Fourier series) for seasonal terms.\n\\(h(t)\\): Gaussian functions (bell curves) for holiday effects (instead of dummies, to make the effect smoother).\n\n\n\n\nKalman Filter\n\nMisc\n\nNotes from How a Kalman filter works, in pictures\nIf a dynamic system is linear and with Gaussian noise (inaccurate measurements, etc.), the optimal estimator of the hidden states is the Kalman Filter\n\nFor nonlinear systems, we use the extended Kalman filter, which works by simply linearizing the predictions and measurements about their mean. (I may do a second write-up on the EKF in the future)\nGood for predictions where the measurements of the outcome variable over time can be noisy\n\nAssumptions\n\nGaussian noise\nMarkov property\n\nIf you know \\(x_{t‚àí1}\\), then knowledge of \\(x_{t‚àí2},\\ldots , x_0\\) doesn‚Äôt give any more information about xt (i.e.¬†not much autocorrelation if at all)\n\n\ntl;dr\n\nA predicted value from a physically-determined autoregression-type equation with 1 lag that gets adjusted for measurement error\n\nAdvantages\n\nLight on memory (they don‚Äôt need to keep any history other than the previous state)\nVery fast, making them well suited for real time problems and embedded systems\n\nUse cases\n\nEngineering: common for reducing noise from sensor signals (i.e.¬†smoothing out measurements)\nDetection-based object tracking (computer vision)\n\n\n\n\nFirst set of equations\n\n\nNotes\n\nThis set of equations deals physical part of the system. It‚Äôs kinda how we typically forecast.\n\nThe \\(\\hat x_k\\) equation is pretty much like a typical auto-regression plus explanatory variables except for the F matrix which may require knowledge of system dynamics\n\nWiki shows a term, \\(w_k\\), added to the end of the \\(\\hat x_k\\) equation. \\(w_k\\) is the process noise and is assumed to be drawn from a zero mean multivariate normal distribution,\n\nThe new best estimate is a prediction made from previous best estimate, plus a correction for known external influences.\n\n\\(\\hat x_k\\): The step-ahead predicted ‚Äústate‚Äù; \\(\\hat x_{k-1}\\) is the current ‚Äústate‚Äù\n\\(u_k\\) (‚Äúcontrol‚Äù vector): An explanatory variable(s)\n\\(F_k\\) (‚Äúprediction‚Äù matrix) and \\(B_k\\) (‚Äúcontrol‚Äù matrix) are transformation matrices\n\n\\(F_k\\) was based on one of Galileo‚Äôs equations of motion in the example so this might be very context specific\nMight need to based on substantial knowledge of the system to create a system of linear equations (i.e.¬†\\(F_k\\) matrix) that can be used to model the it.\n\n\nAnd the new uncertainty is predicted from the old uncertainty, with some additional uncertainty from the environment.\n\n\\(P_k\\) and \\(P_{k-1}\\) are variance/covariance matrices for the step-ahead predicted state and current state respectively\n\\(Q_k\\) is the uncertainty term for the variance/covariance matrix of the predicted state distribution\n\n\n\n\nSecond Set of Equations\n¬†\n\nNotes\n\nThese equations refine the prediction of the first set of equations by taking into account various sources of measurement error in the observed outcome variable\nThe equations do this by finding the intersection, which is itself a distribution, of the transformed prediction distribution, \\(Œº_0\\), and the measurement distribution, \\(Œº_1\\).\n\n\nThis mean, \\(\\mu'\\), of this intersection distribution is the predicted value that most likely to be the true value\n\n\n\\(H_k\\) is a transformation matrix that maps the predicted state (result of the first set of equations), \\(\\hat x_k\\) , to the measurement space\n\nWhere \\(H_k \\cdot \\hat x_k\\) is the expected measurement (pink area) (i.e.¬†Mean of the distribution of transformed prediction)\n\n\\(\\vec z_k\\) is the mean of the measurement distribution (green area)\n\\(\\hat x_k'\\) is the intersection of the transformed prediction distribution and the measurement distribution (i.e.¬†the predicted state thats most likely to true)\n\\(R_k\\) is the uncertainty term for variance/covariance matrix for the measurement distribution\n\\(K'\\) is called the Kalman Gain\n\nDidn‚Äôt read anything interpretative about the value. Just seems to a mathematical construct that‚Äôs part of the derivation.\nIn the derivation, it starts out as the ratio of the measurement covariance matrix to the sum of the measurement variance covariance matrix and the transformed prediction variance covariance matrix\n\n\n\n\n\nProcess\n\n\n\nHyperparameters\n\n\\(Q\\) is the process noise covariance\n\nControls how sensitive the model will be to process noise.\n\n\\(R\\) is the measurement noise variance\n\nControls how quickly the model adapts to changes in the hidden state.\n\n\nGuessing ‚Äústd‚Äù is the default value?\n\n\n\n\n\n\nExponential Smoothing\n\nThe general idea is that future values are a weighted average of past values, with the weights decaying exponentially as we go back in time\nMethods\n\nSimple Exponential Smoothing\ndouble Exponential Smoothing or Holt‚Äôs Method (for time series with a trend)\nTriple Exponential Smoothing or Holt-Winter‚Äôs method (for time series with a trend and sesaonality)\n\n\n\n\nTBATS\n\nTrigonometric seasonality, Box-Cox transformation, ARMA errors, Trend, and Seasonal components\nCan treat non-linear data, solve the autocorrelation problem in residuals since it uses an ARMA model, and it can take into account multiple seasonal periods\nRepresents each seasonal period as a trigonometric representation based on Fourier series. This allows the model to fit large seasonal periods and non-integer seasonal periods",
    "crumbs": [
      "Forecasting",
      "Statistical"
    ]
  },
  {
    "objectID": "qmd/forecasting-statistical.html#sec-fcast-stat-intv",
    "href": "qmd/forecasting-statistical.html#sec-fcast-stat-intv",
    "title": "Statistical",
    "section": "Interval Forecasting",
    "text": "Interval Forecasting\n\nNotes from Video: ISF 2021 Keynote\nInterval data is commonly analyzed by modeling the range (difference between interval points)\n\nRange data doesn‚Äôt provide information about the variation of the mean (aka level) over time.\nRange only provides information about the boundaries, where interval analysis provides information about the boundary and the interior of the interval.\n\nProvides more information than point forecasts.\nData examples:\n\nDaily Temperature, Stock Prices: Each day a high and low values are recorded\nStock Price Volatility, Bid-Ask spread use hi-lo value differences\nIntra-House Inequality: difference between wife and husband earnings\nUrban-Rural income gap\nInterval-Valued Output Growth Rate: China reports it‚Äôs targeted growth rate as a range now.\nDiastolic and Systolic blood pressure\n\nOthers: Blood Lipid, White Blood Cell Count, Hemoglobin\n\n\nExamples where (generalized) intervals can be modeled instead of differences:\n\nStock Volatility\n\nGARCH models often used to model volitility but Conditional Autoregressive Range (CARR) gives better forecasts\n\nBecause GARCH model is only based on the closing price but the CARR model uses the range (difference).\n\nDynamic Interval Modeling\n\nUse Autoregressive Interval (ARI) model to estimate the parameters using an interval time series (not the range)\nThen take the forecasted left and right values of the interval to forecast the volatility range in a CARR model\nThe extra information of the interval data over time (instead of a daily range) yields a more efficient estimation of the parameters\n\n\nCapital Asset Pricing Model (CAPM)\n\nAlso see Finance, Valuation &gt;&gt; Cost of Capital &gt;&gt; WACC &gt;&gt; Cost of Equity\nStandard Equation\n\\[\nR_t - R_{ft} = \\alpha + Œ≤(R_{mt} - R_{ft}) + \\epsilon_t\n\\]\n\n\\(R_t\\): Return of Certain Portfolio\n\\(R_{ft}\\): Risk-Free Interest Fate\n\\(R_{mt}\\): Return of Market Portfolio\n\\(R_t - R_{ft}\\): Asset Risk Premium\n\nInterval-based version\n\\[\nY_t = (\\alpha_0 + \\beta_0I_0) + \\beta X_t + u_t\n\\]\n\n\\(I_0 = [-0.5, 0.5]\\)\n\\(Y_t = [R_{ft}, R_t]\\)\n\\(X_t = [R_{ft}, R_{mt}]\\)\nThe \\(R_t - R_{ft}\\) can then be calculated by taking the difference of the interval bounds of the interval-based predictions\n\n\n\nModel the center of the interval and the range in a bi-variate VAR model (doesn‚Äôt use all points in the interval data)\n\nBi-variate Nonlinear Autoregressive Model for center and range\n\nHas an indicator variable that captures nonlinearity of interval data\n\nSpace-time autoregressive model\n\nAutoregressive Conditional Interval model\n\nThe interval version of an ARMA model\n\ndepends on lags and lagged residuals\n\nACI(p,q):\n\n\\[\nY_t = (\\alpha_0 + \\beta_0 I_0) + \\sum_{j=`}^p \\beta_jY_{t-j} + \\sum_{j=1}^p \\gamma_ju_{t-j} + u_t\n\\]\n\n\\(\\alpha_0\\), \\(\\beta_0\\), \\(\\beta_j\\), \\(\\gamma_j\\) are unknown scalar parameters\n\\(I_0 = [-\\frac{1}{2},\\; \\frac{1}{2}]\\) is a unit interval\n\\(\\alpha_0 + \\beta_0I_0 = [\\frac{\\alpha_0 - \\beta_0}{2},\\: \\frac{\\alpha_0 + \\beta_0}{2}]\\) is a constant interval intercept\n\\(u_t\\) is the interval residuals that satisfies \\(\\mathbb{E}(u_t\\;|\\;I_{t-1}) = [0,0]\\)\n\\(Y_t\\) is a random interval variable\n\nObjective function that gets minimized is called \\(D_k\\) distance\n\n\\(D^2_K [u_t(\\theta),0]\\)\n\n\\(u_t(\\theta)\\) is the interval residuals\n\\(K\\) refers to some kind of kernel function\n\nIt‚Äôs a wacky quadratic with constants a,b,c\nMeasures the distance between all pairs of points\n\n\nThe minimization is a two-stage process\n\nFinds the optimal kernel, \\(K\\), then uses it to minimize the residuals to estimate the parameters\n\n\n\nThreshold Autoregressive Interval (TARI)\n\nNonlinear ACI model and interval version of TAR(p) model (¬Ø\\_(„ÉÑ)_/¬Ø)\n2-Procedure Model\n\nBasically 2 autoregressive equations with an \\(i_1u_t\\) or \\(i_2u_t\\) added on to the end.\nThe interval series, \\(Y_t\\) ,follows one of the equations based on threshold variable \\(q_t\\) is less than or equal to a threshold parameter, \\(\\gamma\\) or greater than.\n\nEstimation is similar to ACI model\nFor more details, need to research what a TAR model (Terasvirta, Tjostheim, and Granger 2010) is",
    "crumbs": [
      "Forecasting",
      "Statistical"
    ]
  },
  {
    "objectID": "qmd/networks-knowledge-graphs.html",
    "href": "qmd/networks-knowledge-graphs.html",
    "title": "Knowledge Graphs",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Networks",
      "Knowledge Graphs"
    ]
  },
  {
    "objectID": "qmd/networks-knowledge-graphs.html#sec-net-kg-misc",
    "href": "qmd/networks-knowledge-graphs.html#sec-net-kg-misc",
    "title": "Knowledge Graphs",
    "section": "",
    "text": "Notes from:\n\nWhat is a knowledge graph?\nHow to Convert Any Text Into a Graph of Concepts\n\nTools\n\nkgl - Knowledge Graph Query Language\n\nInterface and sytax to query knowledge graph data\n\n\nUse Cases\n\nCalculate the centralities for any node, to understand how important a concept (node) is to the body of work\nAnalyze connected and disconnected sets of concepts, or calculate communities of concepts for a deep understanding of the subject matter.\nUsed to implement Graph Retrieval Augmented Generation (GRAG or GAG). Can give much better results than RAG when querying an LLM about documents.\n\nRetrieving the context that is the most relevant for the query with a simple semantic similarity search is not always effective. Especially, when the query does not provide enough context about its true intent, or when the context is fragments across a large corpus of text.\n\nRetail: Knowledge graphs have been for up-sell and cross-sell strategies, recommending products based on individual purchase behavior and popular purchase trends across demographic groups.\nEntertainment: Knowledge graphs are also leveraged for artificial intelligence (AI) based recommendation engines for content platforms, like Netflix, SEO, or social media. Based on click and other online engagement behaviors, these providers recommend new content for users to read or watch.\nFinance: This technology has also been used for know-your-customer (KYC) and anti-money laundering initiatives within the finance industry. They assist in financial crime prevention and investigation, allowing banking institutions to understand the flow of money across their clientele and identify noncompliant customers.\nHealthcare: Knowledge graphs are also benefiting the healthcare industry by organizing and categorizing relationships within medical research. This information assists providers by validating diagnoses and identifying treatment plans based on individual needs.",
    "crumbs": [
      "Networks",
      "Knowledge Graphs"
    ]
  },
  {
    "objectID": "qmd/networks-knowledge-graphs.html#sec-net-kg-terms",
    "href": "qmd/networks-knowledge-graphs.html#sec-net-kg-terms",
    "title": "Knowledge Graphs",
    "section": "Terms",
    "text": "Terms\nKnowledge Graph - Also known as a semantic network, represents a network of real-world entities ‚Äî i.e.¬†objects, events, situations, or concepts ‚Äî and illustrates the relationship between them. Each node represents a concept and each edge is a relationship between a pair of such concepts. This information is usually stored in a graph database and visualized as a graph structure, prompting the term knowledge ‚Äúgraph.‚Äù",
    "crumbs": [
      "Networks",
      "Knowledge Graphs"
    ]
  },
  {
    "objectID": "qmd/networks-knowledge-graphs.html#sec-net-kg-proc",
    "href": "qmd/networks-knowledge-graphs.html#sec-net-kg-proc",
    "title": "Knowledge Graphs",
    "section": "Process",
    "text": "Process\n\n\nCorpus Example:\nMary had a little lamb,\nYou‚Äôve heard this tale before;\nBut did you know she passed her plate,\nAnd had a little more!\nSteps\n\nSplit the corpus of text into chunks. Assign a chunk_id to each of these chunks.\nFor every text chunk, extract concepts and their semantic relationships using a LLM. This relation is assigned a weight of W1. There can be multiple relationships between the same pair of concepts. Every such relation is an edge between a pair of concepts.\nConsider that the concepts that occur in the same text chunk are also related by their contextual proximity. This relation is assigned a weight of W2. Note that the same pair of concepts may occur in multiple chunks.\nGroup similar pairs, sum their weights, and concatenate their relationships. So now we have only one edge between any distinct pair of concepts. The edge has a certain weight and a list of relations as its name.\nPopulate nodes (concepts) and edges (relations) in a graph data structure or a graph database.\nVisualize",
    "crumbs": [
      "Networks",
      "Knowledge Graphs"
    ]
  },
  {
    "objectID": "qmd/networks-knowledge-graphs.html#examples",
    "href": "qmd/networks-knowledge-graphs.html#examples",
    "title": "Knowledge Graphs",
    "section": "Examples",
    "text": "Examples",
    "crumbs": [
      "Networks",
      "Knowledge Graphs"
    ]
  },
  {
    "objectID": "qmd/association-copulas.html",
    "href": "qmd/association-copulas.html",
    "title": "3¬† Copulas",
    "section": "",
    "text": "3.1 Misc",
    "crumbs": [
      "Association",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Copulas</span>"
    ]
  },
  {
    "objectID": "qmd/association-copulas.html#sec-assoc-cop-misc",
    "href": "qmd/association-copulas.html#sec-assoc-cop-misc",
    "title": "3¬† Copulas",
    "section": "",
    "text": "Notes from: https://hudsonthames.org/copula-for-pairs-trading-introduction/\nAlso see\n\nForecasting, Nonlinear &gt;&gt; Misc &gt;&gt; packages, copulas\nFinance &gt;&gt; Mean Reversion Strategy or Pairs Trading\n\nPackages\n\n{{latentcor}}: semi-parametric latent Gaussian copula models\n{LocalCop} (Intro): Local Likelihood Inference for Conditional Copula Models\n\nImplements a local likelihood estimator for the dependence parameter in bivariate conditional copula models. Copula family and local likelihood bandwidth parameters are selected by leave-one-out cross-validation.\n\n\nUsed in finance for non-linear and tail risk qualities but currently doesn‚Äôt take autocorrelation into account\nConditional copulas models allow the dependence structure between multiple response variables to be modelled as a function of covariates.\nUsed to create joint distributions that can be used to describe associated ‚Äúentities‚Äù that may not be from the same distribution\n\nIf each entity has a different behavior, we cannot assume they follow the same distribution.\nAnd most importantly, each entity is likely to influence the others ‚Äî we cannot assume they are independent. Take product cannibalization, for example: In retail, a successful product pulls demand away from similar items in its category.\nHence, each entity may have a different distribution. Plus, we should find a way to model their correlation, since independence is seldom feasible in most practical scenarios.\n\nNotes\n\nA copula is a multivariate distribution that can be formed from a variety of underlying distributions (e.g.¬†gamma, normal, beta) with a specified correlation structure (depending on the type of copula you choose). You create one of these copulas from your data, and sample from it. These samples are used to run simulations on.\n\nExample:\n\nTake return prices from a few correlated stocks and create a copula.\nTrain a model with economic predictors and your sampled copula data as the response.\nFeed values for you economic predictors that indicate an economic state (e.g.¬†recession) to your model and forecast the response to see how that group of stocks reacts.\n\n\nThe specified correlation stucture is called the ‚Äúdependence structure.‚Äù\n\ne.g.¬†asymetrical correlation or tail correlation\n\nThe Gaussian copula is typically described as \\(\\Phi_R (\\Phi^{-1}(u_1), \\ldots, \\Phi^{-1}(u_d))\\), but each \\(u\\) is NOT a variable in your data. Each \\(u\\) is the result of feeding a variable of your data through its ECDF. That result is always a uniform random variable with 0,1 parameters, \\(\\mathcal{U}(0,1)\\), hence the ‚Äúu.‚Äù So that copula definition is equivalent to \\(\\Phi_R (\\Phi^{-1}(F_1(X_i), \\ldots, \\Phi^{-1}(F_d(X_d))\\) where \\(X_i\\) is one of your data variables and \\(F_i\\) is its ECDF.\nEach ECDF of your data is a marginal distribution is often simply referred to as a ‚Äúmarginal.‚Äù\n{vinecopula} and {copula} have a function called pobs which feeds your data through an ecdf and scales it by (n+1). Didn‚Äôt know it had to be scaled, so maybe an ecdf doesn‚Äôt always output values between 0 and 1 like a cdf does.",
    "crumbs": [
      "Association",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Copulas</span>"
    ]
  },
  {
    "objectID": "qmd/association-copulas.html#sec-assoc-cop-its",
    "href": "qmd/association-copulas.html#sec-assoc-cop-its",
    "title": "3¬† Copulas",
    "section": "3.2 Inverse Transform Sampling",
    "text": "3.2 Inverse Transform Sampling\n\nNotes from video\nUnderstanding IVS will help with understanding the copula mathematics\nRelationship between PDF and CDF (e.g.¬†exponential distribution)\n\n\nProbability x ‚â§ 2 is\n\nthe shaded area of the PDF\nIs the output of CDF(x) where x = 2\n\nEquivalence between the PDF and CDF shown in the top integral\n\nMore general form shown in the bottom integral\n\n\nThe inverse of the CDF give you the value of x for any probability\n\n\ne.g.¬†CDF-1(0.7) = 2 and CDF-1(0.5) = 0.7\nWhichever distribution‚Äôs CDF-1 is used, the output of that function will be from that distribution (e.g.¬†2, 0.7)\n\nMathematically:\n\\[\nu_i \\sim \\mathcal{U}(0, 1)\\\\\nx_i = \\mbox{CDF}^-1(u_i)\n\\]\n\n\nExpression says to take a sample from a Uniform distribution, plug that into the inverse CDF, and get a sample from the that CDF‚Äôs distribution\n\n\n\nExample: Gamma Distribution\ngamma1 &lt;- rgamma(1e6, shape=1)\n\nhist(gamma1, main='gamma distribution', cex.main=1.3, cex.lab=1.3, cex.axis=1.3, prob='true')\n\n# pgamma is the cdf of gamma\nu &lt;- pgamma(gamma1, shape=1)\nhist(u, main='Histogram of uniform samples from gamma CDF', cex.main=1.3, cex.lab=1.3, cex.axis=1.3, prob='true')\n\n# qgamma is the inverted cdf of gamma\ngamma_transformed &lt;- qgamma(u, shape=1)\nhist(gamma_transformed, main='Histogram of transformed gamma', cex.main=1.3, cex.lab=1.3, cex.axis=1.3,prob='true')",
    "crumbs": [
      "Association",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Copulas</span>"
    ]
  },
  {
    "objectID": "qmd/association-copulas.html#sec-assoc-cop-sklar",
    "href": "qmd/association-copulas.html#sec-assoc-cop-sklar",
    "title": "3¬† Copulas",
    "section": "3.3 Sklar‚Äôs Theorem",
    "text": "3.3 Sklar‚Äôs Theorem\n\nGuarantees the existence and uniqueness of a copula for two continuous random variables\nFor two random variables \\(S_1\\), \\(S_2\\) in \\([-\\infty, \\infty]\\). \\(S_1\\) and \\(S_2\\) have their own fixed, continuous CDFs, \\(F_1\\), \\(F_2\\).\n\nConsider their (cumulative) joint distribution\n\\[\nH(s_1, s_2) := P(S_1 \\leq s_1, S_2 \\leq s_2)\n\\]\nNow take the uniformly distributed quantile random variable, \\(U_1(S_1)\\), \\(U_2(S_2)\\). For every pair, \\((u_1, u_2)\\), drawn from the pair‚Äôs quantile, we define the *bivariate copula, \\(C: [0,1] \\times [0,1] \\rightarrow [0,1]\\) as:\n\\[\n\\begin {align}\nC(u_1, u_2) &= P(U_1 \\leq u_1, U_2 \\leq u_2) \\\\\n            &= P(S_1 \\leq F_1^{-1}(u_1), S_1 = F_2^{-1}(u2)) \\\\\n            &= H(F_1^{-1}(u_1), F_2^{-1}(u_2))\n\\end {align}\n\\]\n\nWhere \\(F_1^{-1}\\) and \\(F_2^{-1}\\) are inverses (i.e.¬†solved for S) of the marginal CDFs, \\(F_1\\) and \\(F_2\\).\nA copula is just the joint cumulative density for quantiles of a pair of random variables.\n\\(H\\) is ‚Äúsome‚Äù function. It varies with the type of copula (see types section).\nSee Probability notebook, ‚ÄúSimulation of a random variable values of a distribution using the distribution‚Äôs cdf‚Äù section and bookmarks similarly named for some intuition behind what‚Äôs happening in Sklar‚Äôs Theorem\n\nIf the 1st quantile is from 0 to 0.25 then randomly select some numbers in that range using U(0,1)\n\nThis is unclear to me, he might be calling every number drawn from U(0,1) a ‚Äúquantile‚Äù\nBut I think because you‚Äôre using quantiles it‚Äôs non-linear which kind a makes sense (thinking about why quantile regression is used sometimes), so maybe he is talking about deciles, quartiles, etc.\nMaybe (see third line of this section) each quantile is treated as a separate dateset where numbers are drawn from U(0,1) and copula calculated.\n\nFind the inverse CDF of the distribution\nPlug those numbers from the 1st quantile into the inverse CDF to get the simulated values\nRepeat for other random variable\n\n\n\n\n\nThe scatter plot crosshair says of the value of the inverse CDF of variable U using input 0.1 corresponds to the value of the inverse CDF of variable V using input 0.3. The closer the points are to the y = x line the greater the association between them (like a Q-Q plot)\nMathematically the cumulative conditional probabilities shown in the scatter plot are given by taking partial derivatives of the joint inverse CDF:\n\n\nAside: taking the derivative of a (not inverse) marginal (not joint) CDF is the pdf\n\nThe copula density is defined as:\n\n\nWhich is a probability density. Larger the copula density, the denser the clump of points in the scatter plot\n\n\nCoefficients of Lower and Upper Tail Dependence: quantifies the strength of association during joint tail events for each random variable‚Äôs distribution\n\nNot discernible from plots, so needs to be calculated\nUpper tail dependencies refers to the how closely two variables increase together during an extreme ‚Äúpositive‚Äù event\n\ne.g.¬†How strongly 2 stocks move together during an huge gain\nLower tail dependencies are similar except the event is in extreme ‚Äúnegative‚Äù direction\n\nFor stocks at least, lower tail dependencies tend to be much stronger than upper tail tendencies\n\nTypes\n\nNot including the actual bivariate copula formulas because I‚Äôm not sure how the ‚ÄúH‚Äù (see bivariate copula def above) works in practice (and I don‚Äôt want to frustrate future me). I am including descriptions and important characteristics which should have practical applicability. See article for copula formulas.\nArchimedean\n\nParametric and uniquely determined by generator functions, œÜ, that use a parameter, Œ∏\n\nŒ∏ measures how ‚Äúclosely‚Äù the two random variables are ‚Äúrelated‚Äù, and its exact range and interpretation are different across different Archimedean copulas\nGenerators seem to act like the inverse CDFs in the bivariate copula formula\nGenerators:\n\n\nSymmetric and scalable to multiple variables, although a closed-form solution may not be available in higher dimensions\n\nElliptical\n\nSymmetric and easily extended to multiple variables\nAssumes symmetry on both upward co-moves and downward moves (i.e.¬†lacks flexibility)\nGaussian - uses Gaussian inverse CDF and a correlation matrix\nStudent-t - similar as Gaussian but with degrees of freedom\n\nMixed\n\nWeighted ensemble of the copulas above\nHelps with overfitting and more finely calibrating upper and lower tail dependencies\nWeights should sum to 1\n\nOther Notes\n\nThe wording below is a bit confusing\n\n‚ÄúDon‚Äôt have‚Äù I think means doesn‚Äôt have the capability to detect or isn‚Äôt sensitive to\n‚ÄúStronger center dependence‚Äù might mean a greater ability to detect or maybe a center dependence bias\n\nI‚Äôm not even sure what a ‚Äúcenter‚Äù dependency means\n\n\nFrank and Gaussian copulas don‚Äôt have tail dependencies\n\nGaussian contributed to 2008 financial crisis\n\nFrank copula has a stronger center dependence than a Gaussian (?)\nCopulas with upper tail dependence: Gumbel, Joe, N13, N14, Student-t.\nCopulas with lower tail dependence: Clayton, N14 (weaker than upper tail), Student-t.\nStudent t copula emphasizes extreme results: it is usually good for modelling phenomena where there is high correlation in the extreme values (the tails of the distribution).\n\nNote also that the correlation is symmetrical, so the strength of correlation is the same for both tails. This might be an issue for some applications.\n\n\n\nNotes\n\nDefinition\n\n\nCopulas are joint cumulative distribution functions (c.d.f.) for unit-uniform random variables\n\nProbability integral transform\n\n\nStates that we can transform any continuous random variable to a uniform one by plugging it into its own c.d.f.\nTransform a uniform random variable to any continuous random variable\n\n\nSo plugging a Uniform random variable into the quantile function which is the inverse of the cdf and outputs a continuous random variable\n\n\nGaussian Copula\n\n\nExample\n\nDefining a (generic) Copula (aka joint cdf) for two random variables\n\n\nWhere FX(x), FY(y) are cdfs of Gamma, Beta distributions respectively\n\nDefining a Gaussian Copula for these 2 random variables\n\n\nFormat: Copula = joint cdf(quantile(cdf(gamma_random_variable), cdf(gamma_random_variable))\n\nConstruct a Copula\n\nTransform the Gamma and Beta marginals into Uniform marginals via the respective c.d.f.s\nTransform the Uniform marginals into standard Normal marginals via the quantile functions\nDefine the joint distribution via the multivariate Gaussian c.d.f. with zero mean, unit variance and non-zero covariance (covariance matrix R)\n\nSample from a Copula\n\nThe bi-variate random variable has the above properties. (standard Gamma/Beta marginals with Gaussian Copula dependencies)\nSteps (reverse of the copula process)\n\nDraw a sample from a bi-variate Gaussian with mean zero, unit variance and non-zero covariance (covariance matrix R).\n\nYou now have two correlated standard Gaussian variables.\n\nTransform both variables with the standard Gaussian c.d.f. (i.e.¬†plugging each into a gaussian cdf)\n\nYou now have two correlated Uniform variables. (via probability integral transform)\n\nTransform one variable with the standard Beta quantile function and the other variable with the Gamma quantile function\n\nCode (Julia)\nusing Measures\nRandom.seed!(123)\n\n# Step 1: Sample bi-variate Gaussian data with zero mean and unit variance\nmu = zeros(2)\nR = [1 0.5; 0.5 1]\nsample = rand(MvNormal(mu,R),10000)\n\n# Step 2: Transform the data via the standard Gaussian c.d.f.\nsample_uniform = cdf.(Normal(), sample)\n\n# Step 3: Transform the uniform marginals via the standard Gamma/Beta quantile functions\nsample_transformed = sample_uniform\nsample_transformed[1,:] = quantile.(Gamma(),sample_transformed[1,:])\nsample_transformed[2,:] = quantile.(Beta(),sample_transformed[2,:])\n\n\nVisuals\n\nNote: We could drop the zero-mean, unit-variance assumption on the multivariate Gaussian.\n\nIn that case we would have to adjust the Gaussian c.d.f. to the corresponding marginals in order to keep the integral probability transform valid.\nSince we are only interested in the dependency structure (i.e.¬†covariances), standard Gaussian marginals are sufficient and easier to deal with\n\nExample: Gaussian Copula derived from beta and gamma vectors (i.e.¬†‚Äúmarginals‚Äù) (article)\n\nIn this example, marginal 1 and marginal 2 are sampled from the beta and gamma distributions, respectively\n# draw our data samples from 2 distributions, a beta and a gamma -¬†\nbeta1 = stats.distributions.beta(a=10, b=3).rvs(1000)\ngamma1 = stats.distributions.gamma(a=1, loc=0).rvs(1000)\n\n# - we use the emprical cdf instead of beta's or gamma's cdf\n# - we do this to show that copulas can be computed regardless of the\n#¬† underlying distributions\necdf1 = ECDF(beta1)¬† ¬† ¬† # F(beta1) = u1\necdf2 = ECDF(gamma1)¬† ¬† ¬† # F(gamma1) = u2\n# small correction to remove infinities\necdf1.y[0]=0.0001\necdf2.y[0]=0.0001\n\nx1=stats.norm.ppf(ecdf1.y) # Œ¶^-1(u1)\nx2=stats.norm.ppf(ecdf2.y) # Œ¶^-1(u1)\n\n# Parameters of Œ¶2\nmu_x = 0\nvariance_x = 1\nmu_y = 0\nvariance_y = 1\ncov=0.8\n\n# I think this is just some preprocessing to get the vectors into the correct shape for the mvn function\nX, Y = np.meshgrid(x1,x2)\npos = np.empty(X.shape + (2,))\npos[:, :, 0] = X; pos[:, :, 1] = Y\n\n#remember phi2 is just a multivariate normal CDF\nrv = stats.multivariate_normal([mu_x, mu_y], [[variance_x, cov], [cov, variance_y]])\nphi2=rv.cdf(pos)\n\nNote how Empirical CDFs are used which what you‚Äôd use if you didn‚Äôt know the underlying distribution of your two vectors\n\nSee Distributions &gt;&gt; Terms &gt;&gt; Empirical CDFs\n\nSteps\n\nCompute ECDFs of vectors with ‚Äúunknown‚Äù distributions\nApply Gaussian Copula formula\n\nCompute gaussian inverse CDFs for each vector\nDecide on parameter values of multivariate gaussian distribution\n\nMean, Variance, and Covariance\n\nCreate multivariate gaussian distribution\nApply multivariate gaussian CDF to the inverse CDFs of the two vectors",
    "crumbs": [
      "Association",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Copulas</span>"
    ]
  },
  {
    "objectID": "qmd/db-nosql.html",
    "href": "qmd/db-nosql.html",
    "title": "NoSQL",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Databases",
      "NoSQL"
    ]
  },
  {
    "objectID": "qmd/db-nosql.html#sec-db-nosql-misc",
    "href": "qmd/db-nosql.html#sec-db-nosql-misc",
    "title": "NoSQL",
    "section": "",
    "text": "High-Performance data ingestion and retrieval for specific applications, especially with structured or semi-structured data.\n\nData lakes can also store semi-structured data, but they don‚Äôt have the responsiveness (i.e.¬†low latency, reading, writing) for apps, etc.\nNoSQL dbs can be specialized for particular tasks like social media, email, gaming, etc.\n\nApplications (Google Gemini)\n\nData Analytics: For real-time log monitoring, frequent analysis, smaller datasets, and short term to medium term storage. For long-term storage, a data lake would be a better choice.\nCaching: Due to their high speed and low latency, they are perfect for caching frequently accessed data, significantly improving application performance. Popular use cases include caching user sessions, search results, and product information in e-commerce platforms.\nSession Management: In web applications, they efficiently store user session data, shopping carts, and temporary preferences. This enables seamless user experience and personalization across sessions.\nIoT and Real-time Data: Sensors and devices in IoT ecosystems generate large volumes of real-time data. These types of dbs excel at capturing and storing sensor readings, timestamps, and device states, offering real-time insights and analytics.\nGaming and Leaderboards: Prominent use in online gaming for storing player profiles, high scores, and game state information. Their fast retrieval and update capabilities ensure smooth gameplay and accurate leaderboards.\nSocial Media and Messaging: Storing user profiles, connections, messages, and notifications benefits greatly from the scalability and efficient retrieval. This enables handling millions of users and delivering real-time interactions.\nAuthentication and Authorization: Securel storage of user credentials, session tokens, and access control information. This enables efficient user authentication and authorization, securing access to sensitive data and functionalities.\nConfiguration Management: Storing application configuration settings, API keys, and environment variables in a key-value store simplifies management and deployment. This allows for dynamic configuration changes and simplifies scaling processes.\n\nBrands: ScyllaDB, Cassandra, MongoDB, DynamoDB\nAfter MongoDB‚Äôs license change to SSPL, many cloud providers opted to offer older versions of MongoDB. As a result, availability of features won‚Äôt be consistent across cloud service providers.",
    "crumbs": [
      "Databases",
      "NoSQL"
    ]
  },
  {
    "objectID": "qmd/misc.html",
    "href": "qmd/misc.html",
    "title": "Misc",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Misc"
    ]
  },
  {
    "objectID": "qmd/misc.html#sec-misc-misc",
    "href": "qmd/misc.html#sec-misc-misc",
    "title": "Misc",
    "section": "",
    "text": "Search for R packages\n\nList of CRAN Task Views\nJournal of Statistical Software Search\nR-Universe Search\n{packagefinder} - For searching for packages on CRAN\n\nWindows\n\n\n\nShortcut\nDescription\n\n\n\n\nCtrl¬†+¬†Tab\nChange application\n\n\nCtrl¬†+¬†~\nChange window within an application\n\n\n\nBrowser\n\n\n\n\n\n\n\nAction\nShortcut\n\n\n\n\nTo Address Bar\nCtrl + L\n\n\nOpen a new window\nCtrl + n\n\n\nOpen a new window in Incognito mode\nCtrl + Shift + n\n\n\nOpen a new tab, and jump to it\nCtrl + t\n\n\nReopen previously closed tabs in the order they were closed\nCtrl + Shift + t\n\n\nJump to the next open tab\nCtrl + Tab or Ctrl + PgDn\n\n\nJump to the previous open tab\nCtrl + Shift + Tab or Ctrl + PgUp\n\n\nJump to a specific tab\nCtrl + 1 through Ctrl + 8\n\n\nJump to the rightmost tab\nCtrl + 9\n\n\nOpen your home page in the current tab\nAlt + Home\n\n\nOpen the previous page from your browsing history in the current tab\nAlt + Left arrow\n\n\nOpen the next page from your browsing history in the current tab\nAlt + Right arrow\n\n\nClose the current tab\nCtrl + w or Ctrl + F4\n\n\nClose the current window\nCtrl + Shift + w or¬†Alt + F4\n\n\nMinimize the current window\nAlt + Space then n\n\n\nMaximize the current window\nAlt + Space then¬†x\n\n\nQuit Google Chrome\nAlt + f then x\n\n\nMove tabs right or left\nCtrl + Shift + PgUp or Ctrl + Shift + PgDn\n\n\n\nR-devel (&gt;= 4.4.0) gained a command-line option to adjust the connections limit (previous limit was 128 parallel workers)\n$ R\n&gt; parallelly::availableConnections()\n[1] 128\n\n$ R --max-connections=512\n&gt; parallelly::availableConnections()\n[1] 512",
    "crumbs": [
      "Misc"
    ]
  },
  {
    "objectID": "qmd/misc.html#sec-misc-rstud",
    "href": "qmd/misc.html#sec-misc-rstud",
    "title": "Misc",
    "section": "RStudio",
    "text": "RStudio\n\nJob: Run script in the background\nlibrary(rstudioapi)\njobRunScript(\"wfsets_desperation_tune.R\", name = \"tune\", exportEnv = \"R_GlobalEnv\")\n\nNeed to look up args\nI think exportEnv takes the variables in your current environment and runs the script with them as inputs\n\nShortcuts\n\n\n\n\n\n\n\nShortcut\nDescription\n\n\n\n\nAlt + Shift + k\nKeyboard Shortcuts\n\n\nCtrl + Shift + p\nCommand Palette\n\n\nCtrl + Shift + f\nFind in Files\n\n\nCtrl + Alt + up/down\nMultiple Cursors\n\n\nCtrl¬†+¬†Shift¬†+¬†z\nReverse Undo\n\n\nCtrl¬†+¬†Shift¬†+¬†a\nFormat highlighted code (style/linter the code)\n\n\nCtrl¬†+¬†d\nDelete current line\n\n\nAlt + up/down\nYank line up or down\n\n\nCtrl¬†+¬†Alt + up/down\nCopy the above line (or selected lines) down or up\n\n\nCtrl¬†+¬†.\nGo to file/function name\n\n\nAlt + Shift + m\nFocus on Terminal\n\n\n\n\nCustomizing Shortcuts in RStudio\n{shrtcts} - Make anything a shortcut in RStudio",
    "crumbs": [
      "Misc"
    ]
  },
  {
    "objectID": "qmd/misc.html#sec-misc-hack",
    "href": "qmd/misc.html#sec-misc-hack",
    "title": "Misc",
    "section": "Hackathon Criteria",
    "text": "Hackathon Criteria",
    "crumbs": [
      "Misc"
    ]
  },
  {
    "objectID": "qmd/misc.html#sec-misc-update",
    "href": "qmd/misc.html#sec-misc-update",
    "title": "Misc",
    "section": "Update R",
    "text": "Update R\n\nMisc\n\n{rig} - r version management system\nupdate.packages(checkBuilt = TRUE, ask = FALSE) is supposed to search for packages in other R versions and update them in the new R version, but I haven‚Äôt tried it, yet.\nInstall the newest stable version to check it out\nrig add next\nR-next\n\nR-&lt;ver&gt; runs a R version without it being the default\n\nErrors when compiling from source may require installing libraries and they‚Äôll supply code to install via ‚Äúpacman‚Äù\n\nOpen Start &gt;&gt; scroll down to RTools40 &gt;&gt; RTools Bash\nPaste pacman code and hit enter to install\n\nProblem packages in the past\n\n{brms} dependency, {igraph}, didn‚Äôt have a binary on CRAN and wouldn‚Äôt compile from source even with correct libraries installed.\n\nSol‚Äôn: install.packages(\"igraph\", repos = 'https://igraph.r-universe.dev')\n\ninstalls dev version from r-universe\n\n\nSome {easystats} packages had gave {pak} some problems. No difficulties using install.packages with default repo or if they had a r-universe repo though.\n\n\nSteps\n\nCopy user installed packages in current R version\n\nIn R:\nsquirrel &lt;- names(installed.packages(priority = \"NA\")[, 1]) # user installed packages\nreadr::write_rds(squirrel, \"packages.rds\")\n\nThen, close RStudio\n\n\nRTools: Check to see if you have the latest because you‚Äôll need it to compile some of newest versions of packages.\n\nYour rtools folder has the version in it‚Äôs folder name.\nrtools website has the latest version and an .exe to download\nrig add rtools43 will install the RTools for R 4.3 or you can specify any version\nrig add rtools will install all RTools versions for all R versions that are installed.\n\nCheck/Update rig version\n\nIn powershell: rig --version\nCheck current rig release: link\nDownload and install if your version isn‚Äôt current\n\nInstall new version of R\n\nClose R if not already closed\nrig add release installs the latest version of R.\nrig default &lt;new_r_version&gt; sets that version as the default\n\nAdd R and RTools to path\n\nRight-click Windows &gt;&gt; System &gt;&gt; (right panel) Advanced System Settings &gt;&gt; Environment Variables &gt;&gt; Under User Variables, highlight Path, click Edit &gt;&gt; Click Add\n\nR: Add path to directory with all the RScript, R exe, etc. e.g.¬†‚ÄúC:\\Program Files\\R\\R-4.2.3\\bin\\x64‚Äù\nRTools: e.g.¬†‚ÄúC:\\rtools43\\usr\\bin‚Äù\n\n\nOpen R and confirm new version\n\nIf RStudio\n\nThe setting of the new version to the ‚Äúdefault‚Äù version of R in rig should result in RStudio loading the new version.\nIf not, Tools &gt;&gt; Global Options &gt;&gt; General\n\nUnder ‚ÄúR version‚Äù, click ‚Äúchange‚Äù button; choose new R version\nQuit session and restart RStudio\n\n\n\nInstall ‚Äúhigh maintenance‚Äù packages\n\nI‚Äôve had issues with {pak} installing packages that need to be compiled. Maybe be worth trying {pak} first to see if they‚Äôve fixed it.\n{cmdstanr} doesn‚Äôt live on CRAN, so you have to use: install.packages(\"cmdstanr\", repos = c(\"https://mc-stan.org/r-packages/\", getOption(\"repos\")))\n\nCheck for latest cmdstan version\n\nAfter loading the package, library(cmdstanr) , it should run a check on your cmdstan version and tell you if there‚Äôs a newer version.\nTo update, first check toolchain: check_cmdstan_toolchain()\n\nMight tell you to update RTools or that you need some C++ library added\n\nFix C++ toolchain with check_cmdstan_toolchain(fix = TRUE)\nUpdate cmdstan: install_cmdstan()\nMay need to install {rstudioapi} and run rstudioapi::restartSession() (programmatically) or just ctrl + shift + f10 so that this package can be used as a dependency for other packages that need to be installed.\n\n\n{rstanarm}: install.packages(\"rstanarm\")\n\nInstall other packages\nmoose &lt;- readRDS(\"packages.rds\")\nmoose &lt;- moose[!moose %in% c(\"cmdstanr\", \"rstanarm\", \"ebtools\", \"translations\", \"&lt;RStudio add-ins&gt;\")]\n\n# Next time, add a try/catch? or maybe purrr::safely, so that it continues through errors. Also, need to log pkgs that do error.\nfor (i in seq_len(length(moose))) {\n  print(moose[i])\n  pak::pkg_install(moose[i])\n}\n\nfs::file_delete(\"packages.rds\")\n\n{ebtools} is my personal helper package.\n{translations} is a system package that shouldn‚Äôt have been included when I saved the packages from previous version, but was when I recently updated. Might not be necessary to include it in the excuded packages in the future.\n\nCheck for updates of RStudio (link)\n\nCurrent version under Help &gt;&gt; About Rstudio\nPossible to check for updates under Help &gt;&gt; Check for Updates, but that‚Äôs failed me before.",
    "crumbs": [
      "Misc"
    ]
  },
  {
    "objectID": "qmd/post-hoc-analysis-general.html",
    "href": "qmd/post-hoc-analysis-general.html",
    "title": "General",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Post-Hoc Analysis",
      "General"
    ]
  },
  {
    "objectID": "qmd/post-hoc-analysis-general.html#sec-phoc-gen-misc",
    "href": "qmd/post-hoc-analysis-general.html#sec-phoc-gen-misc",
    "title": "General",
    "section": "",
    "text": "Also see Mathematics, Statistics &gt;&gt; Descriptive Statistics&gt;&gt; Understanding CI, SD, and SEM Bars\nNotes from\n\nhttps://www.andrewheiss.com/blog/2019/01/29/diff-means-half-dozen-ways/\n\nPackages\n\n{rstatix}\n{dabestr} for visualization\n\nVisualization for differences (Thread)\n\n{esci} (From article)\n\nestimate &lt;- esci::estimate_mdiff_two(\n  data = mydata,\n  outcome_variable = Prediction,\n  grouping_variable = Exposure,\n  conf_level = 0.95,\n  assume_equal_variance = TRUE\n)\nesci::plot_mdiff(\n  estimate,\n  effect_size = \"mean\"\n)\n\nThe left y-axis is the outcome value given each group variable level\n\nShows each group level‚Äôs mean with error bars and a mini-density figure\n\nThe right y-axis is the mean difference value\n\nThe axis is centered on the mean of the 1 group level which shows that its the reference level\nThe mean of the 20 group level is centered on the mean difference value\nThe mean difference is shows with error bar and mini-density figure.\n\neffect = ‚Äúmedian‚Äù is also available\nerror_layout = c(‚Äúhalfeye‚Äù, ‚Äúeye‚Äù, ‚Äúgradient‚Äù, ‚Äúnone‚Äù) are error bar options\nrope = c(-5, 5) - Decided prior to analysis, says you‚Äôve determined an effect larger than 5 units is the ROPE (region of practical equivalence) (i.e.¬†subjective determination of a substantial effect size)\n{ggplot} object so customizable",
    "crumbs": [
      "Post-Hoc Analysis",
      "General"
    ]
  },
  {
    "objectID": "qmd/post-hoc-analysis-general.html#sec-phoc-gen-eda",
    "href": "qmd/post-hoc-analysis-general.html#sec-phoc-gen-eda",
    "title": "General",
    "section": "EDA",
    "text": "EDA\n\nMisc\n\nIn code examples, movies_clean is the data; rating is the numeric; and genre (action vs comedy) is the group variable\n\n\n\nCharts\n\nBox, Histogram, Density for two groups (factor(genre))\npacman::p_load(ggplot2movies,ggplot2, ggridges, patchwork)\n\n# Make a custom theme\n# I'm using Asap Condensed; download from¬†\n# https://fonts.google.com/specimen/Asap+Condensed\ntheme_fancy &lt;- function() {\n¬† theme_minimal(base_family = \"Asap Condensed\") +\n¬† ¬† theme(panel.grid.minor = element_blank())\n}\neda_boxplot &lt;- ggplot(movies_clean, aes(x = genre, y = rating, fill = genre)) +\n¬† geom_boxplot() +\n¬† scale_fill_manual(values = c(\"#0288b7\", \"#a90010\"), guide = FALSE) +¬†\n¬† scale_y_continuous(breaks = seq(1, 10, 1)) +\n¬† labs(x = NULL, y = \"Rating\") +\n¬† theme_fancy()\neda_histogram &lt;- ggplot(movies_clean, aes(x = rating, fill = genre)) +\n¬† geom_histogram(binwidth = 1, color = \"white\") +\n¬† scale_fill_manual(values = c(\"#0288b7\", \"#a90010\"), guide = FALSE) +¬†\n¬† scale_x_continuous(breaks = seq(1, 10, 1)) +\n¬† labs(y = \"Count\", x = \"Rating\") +\n¬† facet_wrap(~ genre, nrow = 2) +\n¬† theme_fancy() +\n¬† theme(panel.grid.major.x = element_blank())\neda_ridges &lt;- ggplot(movies_clean, aes(x = rating, y = fct_rev(genre), fill = genre)) +\n¬† stat_density_ridges(quantile_lines = TRUE, quantiles = 2, scale = 3, color = \"white\") +¬†\n¬† scale_fill_manual(values = c(\"#0288b7\", \"#a90010\"), guide = FALSE) +¬†\n¬† scale_x_continuous(breaks = seq(0, 10, 2)) +\n¬† labs(x = \"Rating\", y = NULL,\n¬† ¬† ¬† subtitle = \"White line shows median rating\") +\n¬† theme_fancy()\n(eda_boxplot | eda_histogram) /¬†\n¬† ¬† eda_ridges +¬†\n¬† plot_annotation(title = \"Do comedies get higher ratings than action movies?\",\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† subtitle = \"Sample of 400 movies from IMDB\",\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† theme = theme(text = element_text(family = \"Asap Condensed\"),\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† plot.title = element_text(face = \"bold\",\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† size = rel(1.5))))\n\n\n\nTest for Equal Variances\n\nMisc\n\n‚ÄúGlass and Hopkins (1996 p.¬†436) state that the Levene and B-F tests are‚Äùfatally flawed‚Äù; It isn‚Äôt clear how robust they are when there is significant differences in variances and unequal sample sizes. ‚Äù\n\nBartlett test: Check homogeneity of variances based on the mean\nbartlett.test(rating ~ genre, data = movies_clean)\nLevene test: Check homogeneity of variances based on the median, so it‚Äôs more robust to outliers\ncar::leveneTest(rating ~ genre, data = movies_clean)\n\nAlso {DescTools}\nOther tests are better\n\nFligner-Killeen test: Check homogeneity of variances based on the median, so it‚Äôs more robust to outliers\nfligner.test(rating ~ genre, data = movies_clean)\nBrown-Forsythe (B-F) Test (link)\n\nAttempts to correct for the skewness of the Levene Test transformation by using deviations from group medians.\n\nLess likely than the Levene test to incorrectly declare that the assumption of equal variances has been violated.\n\nThought to perform as well as or better than other available tests for equal variances\n\nonewaytests::bf.test(weight_loss ~ program, data = data)\n\np-value &lt; 0.05 means the difference in variances is statistically significant",
    "crumbs": [
      "Post-Hoc Analysis",
      "General"
    ]
  },
  {
    "objectID": "qmd/post-hoc-analysis-general.html#sec-phoc-gen-fdim",
    "href": "qmd/post-hoc-analysis-general.html#sec-phoc-gen-fdim",
    "title": "General",
    "section": "Frequentist Difference-in-Means",
    "text": "Frequentist Difference-in-Means\n\nT-test\n\nTest whether the difference between means is statistically different from 0\nDefault is for non-equal variances\nt_test_eq &lt;- t.test(rating ~ genre, data = movies_clean, var.equal = TRUE)\nt_test_eq_tidy &lt;- tidy(t_test_eq) %&gt;%\n¬† # Calculate difference in means, since t.test() doesn't actually do that\n¬† mutate(estimate_difference = estimate1 - estimate2) %&gt;%\n¬† # Rearrange columns\n¬† select(starts_with(\"estimate\"), everything())\nFor unequal variances, Welch‚Äôs T-Test:\n\nvar.equal = FALSE\nRecommended for large datasets\n\n\n\n\nHotelling‚Äôs T2\n\nMultivariate generalization of Welch‚Äôs T-Test\n{DescTools::HotellingsT2Test}\n{ICSNP}\n\n\n\nNon-Parametric Difference-in-Means Tests\n\nWilcoxon Rank Sum and Signed Rank: wilcox.test\n\n1 or 2 variables/‚Äúsamples‚Äù\n2 variable aka ‚ÄúMann-Whitney‚Äù\ncoin::wilcox_test\n\nfor exact, asymptotic and Monte Carlo conditional p-values, including in the presence of ties\n\n\nKruskal-Wallis: kruskal.test(rating ~ genre, data = movies_clean)\n\nMore than 2 variables/‚Äúsamples‚Äù\n\n\n\n\nKolomogorov-Smirnov\n\nks.test\nCalculates the difference in cdf of each sample\n2 variables/‚Äúsamples‚Äù\nFor mixed or discrete, see {KSgeneral}\n\n\n\nBootstrap\n\nAlso see Statistical Concepts &gt;&gt; Bootstrapping\nSteps\n\nCalculate a sample statistic, or¬†Œ¥:¬†This is the main measure you care about: the difference in means, the average, the median, the proportion, the difference in proportions, the chi-squared value, etc.\nUse simulation to invent a world where¬†Œ¥¬†is null:¬†Simulate what the world would look like if there was no difference between two groups, or if there was no difference in proportions, or where the average value is a specific number.\nLook at¬†Œ¥¬†in the null world: Put the sample statistic in the null world and see if it fits well.\nCalculate the probability that¬†Œ¥¬†could exist in null world: This is the p-value, or the probability that you‚Äôd see a¬†Œ¥¬†at least that high in a world where there‚Äôs no difference.\nDecide if¬†Œ¥¬†is statistically significant: Choose some evidentiary standard or threshold (like 0.05) for deciding if there‚Äôs sufficient proof for rejecting the null world.\n\nStandard Method\nlibrary(infer)\n\n# Calculate the difference in means\ndiff_means &lt;- movies_clean %&gt;%¬†\n¬† specify(rating ~ genre) %&gt;%\n¬† # Order here means we subtract comedy from action (Action - Comedy)\n¬† calculate(\"diff in means\", order = c(\"Action\", \"Comedy\"))\n\nboot_means &lt;- movies_clean %&gt;%¬†\n¬† specify(rating ~ genre) %&gt;%¬†\n¬† generate(reps = 1000, type = \"bootstrap\") %&gt;%¬†\n¬† calculate(\"diff in means\", order = c(\"Action\", \"Comedy\"))\n\nboostrapped_confint &lt;- boot_means %&gt;% get_confidence_interval()\n\nboot_means %&gt;%¬†\n¬† visualize() +¬†\n¬† shade_confidence_interval(boostrapped_confint,\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† color = \"#8bc5ed\", fill = \"#85d9d2\") +\n¬† geom_vline(xintercept = diff_means$stat, size = 1, color = \"#77002c\") +\n¬† labs(title = \"Bootstrapped distribution of differences in means\",\n¬† ¬† ¬† x = \"Action ‚àí Comedy\", y = \"Count\",\n¬† ¬† ¬† subtitle = \"Red line shows observed difference; shaded area shows 95% confidence interval\") +\n¬† theme_fancy()\nDowney‚Äôs Process: Generate a world where there‚Äôs no difference by shuffling all the action/comedy labels through permutation\n# Step 1: Œ¥ = diff_means (see above)\n\n# Step 2: Invent a world where Œ¥ is null\ngenre_diffs_null &lt;- movies_clean %&gt;%¬†\n¬† specify(rating ~ genre) %&gt;%¬†\n¬† hypothesize(null = \"independence\") %&gt;%¬†\n¬† generate(reps = 5000, type = \"permute\") %&gt;%¬†\n¬† calculate(\"diff in means\", order = c(\"Action\", \"Comedy\"))\n\n# Step 3: Put actual observed Œ¥ in the null world and see if it fits\ngenre_diffs_null %&gt;%¬†\n¬† visualize() +¬†\n¬† geom_vline(xintercept = diff_means$stat, size = 1, color = \"#77002c\") +\n¬† scale_y_continuous(labels = comma) +\n¬† labs(x = \"Simulated difference in average ratings (Action ‚àí Comedy)\", y = \"Count\",\n¬† ¬† ¬† title = \"Simulation-based null distribution of differences in means\",\n¬† ¬† ¬† subtitle = \"Red line shows observed difference\") +\n¬† theme_fancy()\n\nIf line is outside null distribution, then the difference value doesn‚Äôt fit in a world where the null hypothesis is the truth\n\nGenerate a p-value\n# Step 4: Calculate probability that observed Œ¥ could exist in null world\ngenre_diffs_null %&gt;%¬†\n¬† get_p_value(obs_stat = diff_means, direction = \"both\") %&gt;%¬†\n¬† mutate(p_value_clean = pvalue(p_value))",
    "crumbs": [
      "Post-Hoc Analysis",
      "General"
    ]
  },
  {
    "objectID": "qmd/post-hoc-analysis-general.html#sec-phoc-gen-bdim",
    "href": "qmd/post-hoc-analysis-general.html#sec-phoc-gen-bdim",
    "title": "General",
    "section": "Bayesian Difference-in-Means",
    "text": "Bayesian Difference-in-Means\n\nMisc\n\nNotes from\n\nHalf a dozen frequentist and Bayesian ways to measure the difference in means in two groups | Andrew Heiss\n\nFrequentist null hypothesis significance testing (NHST) determines the probability of the data given a null hypothesis (i.e.¬†\\(P(data|H)\\), yielding results that are often unwieldy, phrased as the probability of rejecting the null if it is true (hence all that talk of ‚Äúnull worlds‚Äù). In contrast, Bayesian analysis determines the probability of a hypothesis given the data (i.e.P(H|data)), resulting in probabilities that are directly interpretable.\n\n\n\nRegression (Equal Variances)\n\nWith {brms}\nbrms_eq &lt;- brm(\n¬† # bf() is an alias for brmsformula() and lets you specify model formulas\n¬† bf(rating ~ genre),¬†\n¬† # Reverse the levels of genre so that comedy is the base case\n¬† data = mutate(movies_clean, genre = fct_rev(genre)),\n¬† prior = c(set_prior(\"normal(0, 5)\", class = \"Intercept\"),\n¬† ¬† ¬† ¬† ¬† ¬† set_prior(\"normal(0, 1)\", class = \"b\")),\n¬† chains = 4, iter = 2000, warmup = 1000, seed = 1234,\n¬† file = \"cache/brms_eq\"\n)\n# median of posterior and CIs\nbrms_eq_tidy &lt;-¬†\n¬† broom::tidyMCMC(brms_eq, conf.int = TRUE, conf.level = 0.95,¬†\n¬† ¬† ¬† ¬† ¬† estimate.method = \"median\", conf.method = \"HPDinterval\")\n\nfamily = gaussian (default)\nb_intercept: mean comedy score while the\nb_genreAction: difference from mean comedy score (i.e.¬†difference in means)\n\nsSays ‚ÄúWe‚Äôre 95% confident that the true population-level difference in rating is between -0.968 and -0.374, with a median of -0.666.‚Äù\n\n\n\n\n\nRegression (unequal variances)\n\nWith {brms}\nbrms_uneq &lt;- brm(\n¬† bf(rating ~ genre, sigma ~ genre),¬†\n¬† data = mutate(movies_clean, genre = fct_rev(genre)),\n¬† prior = c(set_prior(\"normal(0, 5)\", class = \"Intercept\"),\n¬† ¬† ¬† ¬† ¬† ¬† set_prior(\"normal(0, 1)\", class = \"b\"),\n¬† ¬† ¬† ¬† ¬† ¬† # models the variance for each group (e.g. comedy and action)\n¬† ¬† ¬† ¬† ¬† ¬† set_prior(\"cauchy(0, 1)\", class = \"b\", dpar = \"sigma\")),\n¬† chains = CHAINS, iter = ITER, warmup = WARMUP, seed = BAYES_SEED,\n¬† file = \"cache/brms_uneq\"\n)\n\n# median of posterior and CIs\nbrms_uneq_tidy &lt;-¬†\n¬† tidyMCMC(brms_uneq, conf.int = TRUE, conf.level = 0.95,¬†\n¬† ¬† ¬† ¬† ¬† estimate.method = \"median\", conf.method = \"HPDinterval\") %&gt;%¬†\n¬† # sigma terms on log-scale so exponentiate them to get them back to original scale\n¬† mutate_at(vars(estimate, std.error, conf.low, conf.high),\n¬† ¬† ¬† ¬† ¬† ¬† funs(ifelse(str_detect(term, \"sigma\"), exp(.), .)))\n\nInterpretation for intercept and main effect estimates same as before\nb_sigma_intercept and b_sigma_genreAction are the std.devs for those posteriors\n\n\n\n\nBayesian Estimation Supersedes the T-test (BEST)\n\nUnequal Variances, student-t distribution\nSame as before but with a coefficient for ŒΩ, the degrees of freedom, for the student-t distribution.\nModels each group distribution (by removing intercept w/ 0 + formula notation), then calculates difference in means by hand\nbrms_uneq_robust_groups &lt;- brm(\n¬† bf(rating ~ 0 + genre,\n¬† ¬† sigma ~ 0 + genre),¬†\n¬† family = student,\n¬† data = mutate(movies_clean, genre = fct_rev(genre)),\n¬† prior = c(\n¬† ¬† # Set group mean prior\n¬† ¬† set_prior(\"normal(6, 2)\", class = \"b\", lb = 1, ub = 10),\n¬† ¬† # Ser group variance priors. We keep the less informative cauchy(0, 1).\n¬† ¬† set_prior(\"cauchy(0, 1)\", class = \"b\", dpar = \"sigma\"),\n¬† ¬† set_prior(\"exponential(1.0/29)\", class = \"nu\")),\n¬† chains = CHAINS, iter = ITER, warmup = WARMUP, seed = BAYES_SEED,\n¬† file = \"cache/brms_uneq_robust_groups\"\n)\n\nbrms_uneq_robust_groups_tidy &lt;-¬†\n¬† tidyMCMC(brms_uneq_robust_groups, conf.int = TRUE, conf.level = 0.95,¬†\n¬† ¬† ¬† ¬† ¬† estimate.method = \"median\", conf.method = \"HPDinterval\") %&gt;%¬†\n¬† # Rescale sigmas\n¬† mutate_at(vars(estimate, std.error, conf.low, conf.high),\n¬† ¬† ¬† ¬† ¬† ¬† funs(ifelse(str_detect(term, \"sigma\"), exp(.), .))\n\nbrms_uneq_robust_groups_post &lt;- posterior_samples(brms_uneq_robust_groups) %&gt;%¬†\n¬† # We can exponentiate here!\n¬† mutate_at(vars(contains(\"sigma\")), funs(exp)) %&gt;%¬†\n¬† # For whatever reason, we need to log nu?\n¬† mutate(nu = log10(nu)) %&gt;%¬†\n¬† mutate(diff_means = b_genreAction - b_genreComedy,\n¬† ¬† ¬† ¬† diff_sigma = b_sigma_genreAction - b_sigma_genreComedy) %&gt;%¬†\n¬† # Calculate effect sizes, just for fun\n¬† mutate(cohen_d = diff_means / sqrt((b_sigma_genreAction + b_sigma_genreComedy)/2),\n¬† ¬† ¬† ¬† cles = dnorm(diff_means / sqrt((b_sigma_genreAction + b_sigma_genreComedy)), 0, 1))\n\nbrms_uneq_robust_groups_tidy_fixed &lt;-¬†\n¬† tidyMCMC(brms_uneq_robust_groups_post, conf.int = TRUE, conf.level = 0.95,¬†\n¬† ¬† ¬† ¬† ¬† estimate.method = \"median\", conf.method = \"HPDinterval\")\n## # A tibble: 9 x 5\n##¬† term¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† estimate std.error conf.low conf.high\n##¬† &lt;chr&gt;¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† &lt;dbl&gt;¬† ¬† &lt;dbl&gt;¬† ¬† &lt;dbl&gt;¬† ¬† &lt;dbl&gt;\n## 1 b_genreComedy¬† ¬† ¬† ¬† 5.99¬† ¬† ¬† 0.109¬† ¬† 5.77¬† ¬† ¬† 6.19¬†\n## 2 b_genreAction¬† ¬† ¬† ¬† 5.30¬† ¬† ¬† 0.107¬† ¬† 5.09¬† ¬† ¬† 5.50¬†\n## 3 b_sigma_genreComedy¬† 1.47¬† ¬† ¬† 0.0882¬† ¬† 1.30¬† ¬† ¬† 1.64¬†\n## 4 b_sigma_genreAction¬† 1.47¬† ¬† ¬† 0.0826¬† ¬† 1.31¬† ¬† ¬† 1.62¬†\n## 5 nu¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† 1.48¬† ¬† ¬† 0.287¬† ¬† 0.963¬† ¬† 2.04¬†\n## 6 diff_means¬† ¬† ¬† ¬† ¬† -0.690¬† ¬† ¬† 0.151¬† ¬† -1.01¬† ¬† -0.415\n## 7 diff_sigma¬† ¬† ¬† ¬† ¬† 0.00100¬† ¬† 0.111¬† ¬† -0.212¬† ¬† 0.217\n## 8 cohen_d¬† ¬† ¬† ¬† ¬† ¬† -0.571¬† ¬† ¬† 0.126¬† ¬† -0.818¬† ¬† -0.327\n## 9 cles¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† 0.368¬† ¬† ¬† 0.0132¬† ¬† 0.341¬† ¬† 0.391\nCohen‚Äôs d:¬† standardized difference in means (Also see Post-Hoc Analysis, Multilevel &gt;&gt; Cohen‚Äôs D)\n\n\nMedium effect size in the example above\nThe denominator in this calculation is the square root of the average std.dev, but it doesn‚Äôt look like any of the ones used in the wiki article\n\nLooks closer to the Strictly Standardized Mean Difference (SSMD) (wiki)\n\n\nCommon language effect size (CLES): Probability that a rating sampled at random from one group will be greater than a rating sampled from the other group.\n\n36.8% chance that we could randomly select an action rating from the comedy distribution",
    "crumbs": [
      "Post-Hoc Analysis",
      "General"
    ]
  },
  {
    "objectID": "qmd/post-hoc-analysis-general.html#sec-phoc-gen-dd",
    "href": "qmd/post-hoc-analysis-general.html#sec-phoc-gen-dd",
    "title": "General",
    "section": "Dichotomous Data",
    "text": "Dichotomous Data\n\nMean (probability-of-event) + CI Estimation\n\nLarge Population\n\\[\n\\hat p \\pm z_{\\alpha/2} \\sqrt {\\frac{\\hat p(1-\\hat p)}{n}}\n\\]\nbinom::binom.asymp(x=x, n=n, conf.level=0.95)\n##¬† ¬† ¬† method¬† x¬† n¬† ¬† ¬† mean¬† ¬† ¬† lower¬† ¬† upper\n## 1 asymptotic 52 420 0.1238095 0.09231031 0.1553087\nSmall/Finite Population\n\\[\n\\hat p \\pm z_{\\alpha/2} \\sqrt {\\frac{\\hat p(1-\\hat p)}{n} \\cdot \\frac{N-n}{N-1}}\n\\]\n\nSee ?binom::binom.confint for many methods\n‚ÄúWhen the intracluster correlation coefficient is high and the prevalence, p, is less than 0.10 or greater than 0.90, the Agresti-Coull and Clopper-Pearson intervals perform best. In other settings, the Clopper-Pearson interval is unnecessarily wide. In general, the Logit, Wilson, Jeffreys, and Agresti-Coull intervals perform well, although the Logit interval may be intractable when the standard error is equal to zero.‚Äù (paper)\n\n1-Sample Proportion Test\n\nExample: Do 50% of infants start walking by 12 months of age?\n&gt; table(walkby12)\n\n#&gt; walkby12\n#&gt; 0 1\n#&gt; 14 36\n\nprop.test(36,50,p=0.5,correct=FALSE)\n\n#&gt; 1-sample proportions test without continuity correction\n#&gt; data: 36 out of 50, null probability 0.5\n#&gt; X-squared = 9.68, df = 1, p-value = 0.001863\n#&gt; alternative hypothesis: true p is not equal to 0.5\n#&gt; 95 percent confidence interval:\n#&gt; 0.5833488 0.8252583\n#&gt; sample estimates:\n#&gt; p\n#&gt; 0.72\n\np-value &lt; 0.05 therefore the null hypothesis of 50% of infants walking is rejected\ncorrect = FALSE says this is a large sample (See assumptions in difference of proportions &gt;&gt; Z-Test)\nCI for the population proportion estimate is given.\n\n\nBayesian\n# Mean proportion estimated with prior that mean lies between 0.05 and 0.15\n\n##Function to determine beta parameters s.t. the 2.5% and 97.5% quantile match the specified values\ntarget &lt;- function(theta, prior_interval, alpha=0.05) {\n¬† sum( (qbeta(c(alpha/2, 1-alpha/2), theta[1], theta[2]) - prior_interval)^2)\n}\n## Find the prior parameters\nprior_params &lt;- optim(c(10,10),target, prior_interval=c(0.05, 0.15))$par\n## [1]¬† 12.04737 116.06022\n# not really sure how this works. Guessing theta1,2 is c(10,10) but then there doesn't seem to be an unknown to optimize for.\n\n## Compute credibile interval from a beta-binomial conjugate prior-posterior approach\nbinom::binom.bayes(x=x, n=n, type=\"central\", prior.shape1=prior_params[1], prior.shape2=prior_params[2])\n##¬† method¬† x¬† n¬† shape1¬† shape2¬† ¬† ¬† mean¬† ¬† ¬† lower¬† ¬† upper¬† sig\n## 1¬† bayes 52 420 64.04737 484.0602 0.1168518 0.09134069 0.1450096 0.05\n\n##Plot of the beta-posterior\np &lt;- binom::binom.bayes.densityplot(ci_bayes)\n##Add plot of the beta-prior\ndf &lt;- data.frame(x=seq(0,1,length=1000)) %&gt;% mutate(pdf=dbeta(x, prior_params[1], prior_params[2]))\np + geom_line(data=df, aes(x=x, y=pdf), col=\"darkgray\",lty=2) +\n¬† coord_cartesian(xlim=c(0,0.25)) + scale_x_continuous(labels=scales::percent)\n\n# Estimated with a flat prior (essentially equivalent to the frequentist approach)\nbinom::binom.bayes(x=x, n=n, type=\"central\", prior.shape1=1, prior.shape2=1))\n##¬† method¬† x¬† n shape1 shape2¬† ¬† ¬† mean¬† ¬† ¬† lower¬† ¬† upper¬† sig\n## 1¬† bayes 52 420¬† ¬† 53¬† ¬† 369 0.1255924 0.09574062 0.158803 0.05\n\nFrom https://staff.math.su.se/hoehle/blog/2017/06/22/interpretcis.html\nInterpretation\n\nTechnical: ‚Äú95% equi-tailed credible interval resulting from a beta-binomial conjugate Bayesian approach obtained when using a prior beta with parameters such that the similar 95% equi-tailed¬†prior credible interval¬†has limits 0.05 and 0.15. Given these assumptions the interval 9.1%- 14.5% contains 95% of your subjective posterior density for the parameter.‚Äù\nNontechnical: the true value is in that interval with 95% probability or just this 95% Bayesian confidence interval is 9.1%- 14.5%.\n\n\n\n\n\nDifference in Proportions\n\nCochran-Mantel-Haenszel Test: This test is appropriate when you have data from multiple 2x2 tables (strata) and want to test the association between two categorical variables while controlling for the effects of a third variable (confounding variable).\n\nSee Discrete Analysis Notebook\n\nDo NOT use Fisher‚Äôs Exact Test.\n\nSeveral different p-values can be associated with a single table, making scientific inference inconsistent\nDespite the fact that Fisher‚Äôs test gives exact p-values, some authors have argued that it is conservative, i.e.¬†that its actual rejection rate is below the nominal significance level. The issue has to do with Fisher‚Äôs test conditioning on the margin totals.\n\nLikelihood ratios, posterior probabilities and mid-p-values - lead to more consistent inferences. Recommendations from this paper:\n\nA Bayesian interval for the log odds ratio with Jeffreys‚Äô reference prior\nConditional Likelihood Ratio Test\n\n{ProfileLikelihood}: LR.pvalue(y1, y2, n1, n2, interval=0.01)\n\n\nZ-Test\n\\[\n\\begin{align}\n&Z = \\frac{(\\hat p_1 - \\hat p_2)}{\\sqrt{\\hat p (1 - \\hat p) \\left(\\frac{1}{n_1}-\\frac{1}{n_2}\\right)}} \\\\\n&\\text{where} \\;\\; \\hat p = \\frac{Y_1 + Y_2}{n_1 + n_2}\n\\end{align}\n\\]\n\nThe z-test comparing two proportions is equivalent to the chi-square test of independence\nTerms\n\n\\(\\hat p_i\\) is the sample proportion\n\\(\\hat p\\) is the overall proportion\n\\(Y_i\\) is the sample count\n\\(n_i\\) is the sample size\n2-tail hypothesis test; If Z &gt; 1.96 or Z &lt; -1.96 (i.e.¬†p-value &lt; 0.05), then the sample proportions are NOT equal.\n\nCheck Assumptions\n\nIndependent observations and sufficient sample sizes.\n\nFor each sample, i:\n\\[\n\\begin{align}\n&n_i \\cdot \\hat p_i &gt; 5 \\\\\n&n_i \\cdot (1-\\hat p_i) &gt; 5\n\\end{align}\n\\]\nThere is a ‚Äúcontinuity correction‚Äù arg (correct = TRUE)that when set to TRUE can correctly compute the CIs for when proportions for each/one event are less than 5\n\n\nExample\nres &lt;- prop.test(x = c(490, 400), n = c(500, 500), correct = FALSE)\n\n#&gt; 2-sample test for equality of proportions with continuity correction\n\n#&gt; data:  c(490, 400) out of c(500, 500)\n#&gt; X-squared = 82.737, df = 1, p-value &lt; 2.2e-16\n#&gt; alternative hypothesis: two.sided\n#&gt; 95 percent confidence interval:\n#&gt;  0.1428536 0.2171464\n#&gt; sample estimates:\n#&gt; prop 1 prop 2 \n#&gt;   0.98   0.80 \n\np-value &lt; 0.05, so proportions are statistically different\nThe confidence interval given for the true proportion if there is one group, or for the difference in proportions if there are 2 groups and p argument isn‚Äôt provided\n2-sided test is default\nchisq.test() is exactly equivalent to prop.test() but it works with data in matrix form.\n\n\nMcNemar‚Äôs test\n\nFor comparing paired data of 2 groups\nMainly useful when the measurements are on the nominal or ordinal scale\nTests for significant difference in frequencies of paired samples when it has binary responses\n\nH0: There is no significant change in individuals after the treatment\nH1: There is a significant change in individuals after the treatment\n\n\n# data is unaggregated (i.e. paired measurements from individuals)\ntest &lt;- mcnemar.test(table(data$pretreatment, data$posttreatment))\n#&gt; McNemar's Chi-squared test with continuity correction\n#&gt; data: table(data$before, data$after)\n#&gt; McNemar's chi-squared = 0.5625, df = 1, p-value = 0.4533\n\ncorrect = TRUE (default) - Continuity correction (increases ‚Äúusefulness and accuracy of the test‚Äù so probably better to leave it as TRUE)\nx & y are factor vectors\nx can be a matrix of aggregated counts\n\nIf the 1st row, 2nd cell or 2nd row, 1st cell have counts &lt; 50, then use the Exact Tests to get accurate p-values (For details see above for page in notebook)\n\nInterpretation: p-value is 0.45, above the 5% significance level and therefore the null hypothesis cannot be rejected",
    "crumbs": [
      "Post-Hoc Analysis",
      "General"
    ]
  },
  {
    "objectID": "qmd/post-hoc-analysis-general.html#sec-phoc-gen-catdat",
    "href": "qmd/post-hoc-analysis-general.html#sec-phoc-gen-catdat",
    "title": "General",
    "section": "Categorical Data",
    "text": "Categorical Data\n\nExample: Bayesian; 3 level Categorical Variable\n\nData and Model\n\nCalculate Differences\n\nVisualize (Code in previous chunk)",
    "crumbs": [
      "Post-Hoc Analysis",
      "General"
    ]
  },
  {
    "objectID": "qmd/scraping.html",
    "href": "qmd/scraping.html",
    "title": "Scraping",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Scraping"
    ]
  },
  {
    "objectID": "qmd/scraping.html#sec-scrap-misc",
    "href": "qmd/scraping.html#sec-scrap-misc",
    "title": "Scraping",
    "section": "",
    "text": "Packages\n\n{rvest}\n{rselenium}\n{selenium}\n{selenider} - Wrapper functions around {chromote} and {selenium} functions that utilize lazy element finding and automatic waiting to make scraping code more reliable\n{shadowr} - For shadow DOMs\n\nResources\n\nWeb Scraping with R\n\nIn loops, use Sys.sleep (probably) after EVERY selenium function. Sys.sleep(1) might be all that‚Äôs required. ({selenider} fixes this problem)\n\nSee Projects &gt; foe &gt; gb-level-1_9-thread &gt; scrape-gb-levels.R\nMight not always be needed, but absolutely need if you‚Äôre filling out a form and submitting it.\nMight even need one at the top of the loop\nIf a Selenium function stops working, adding Sys.sleeps are worth a try.\n\nSometimes clickElement( ) stops working for no apparent reason.¬†When this happens used sendKeysToElement(list(\"laptops\",key=\"enter\"))\nIn batch scripts (.bat), sometimes after a major windows update, the Java that selenium uses will trigger Windows Defender (WD) and cause the scraping script to fail (if you have it scheduled). If you run the .bat script manually and then when the WD box rears its ugly head, just click ignore. WD should remember after that and not to mess with it.\nRSelenium findElement(using = \"\") options ‚Äúclass name‚Äù : Returns an element whose class name contains the search value; compound class names are not permitted.\n\n‚Äúcss selector‚Äù : Returns an element matching a CSS selector.\n‚Äúid‚Äù : Returns an element whose ID attribute matches the search value.\n‚Äúname‚Äù : Returns an element whose NAME attribute matches the search value.\n‚Äúlink text‚Äù : Returns an anchor element whose visible text matches the search value.\n‚Äúpartial link text‚Äù : Returns an anchor element whose visible text partially matches the search value.\n‚Äútag name‚Äù : Returns an element whose tag name matches the search value.\n‚Äúxpath‚Äù : Returns an element matching an XPath expression.",
    "crumbs": [
      "Scraping"
    ]
  },
  {
    "objectID": "qmd/scraping.html#sec-scrap-terms",
    "href": "qmd/scraping.html#sec-scrap-terms",
    "title": "Scraping",
    "section": "Terms",
    "text": "Terms\n\nStatic Web Page: A web page (HTML page) that contains the same information for all users. Although it may be periodically updated, it does not change with each user retrieval.\nDynamic Web Page: A web page that provides custom content for the user based on the results of a search or some other request. Also known as ‚Äúdynamic HTML‚Äù or ‚Äúdynamic content‚Äù, the ‚Äúdynamic‚Äù term is used when referring to interactive Web pages created for each user.",
    "crumbs": [
      "Scraping"
    ]
  },
  {
    "objectID": "qmd/scraping.html#sec-scrap-rvest",
    "href": "qmd/scraping.html#sec-scrap-rvest",
    "title": "Scraping",
    "section": "rvest",
    "text": "rvest\n\nMisc\n\nNotes from: Pluralsight.Advanced.Web.Scraping.Tactics.R.Playbook\n\nUses css selectors or xpath to find html nodes\nlibrary(rvest)\npage &lt;- read_html(\"&lt;url&gt;\")\nnode &lt;- html_element(page, xpath = \"&lt;xpath&gt;\"\n\nFind css selectors\n\nselector gadget\n\nclick selector gadget app icon in Chrome in upper right assuming you‚Äôve installed it already\nclick item on webpage you want to scrape\n\nit will highlight other items as well\n\nclick each item you DON‚ÄôT want to deselect it\ncopy the selector name in box at the bottom of webpage\nUse html_text to pull text or html_attr to pull a link or something\n\ninspect\n\nright-click item on webpage\nclick inspect\nhtml element should be highlighted in elements tab of right side pan\nright-click element ‚Äì&gt; copy ‚Äì&gt; copy selector or copy xpath\n\n\n\nExample: Access data that needs authentication¬†(also see RSelenium version)\n\nnavigate to login page\nsession &lt;- session(\"&lt;login page url&gt;\")\nFind ‚Äúforms‚Äù for username and password\nform &lt;- html_form(session)[[1]]\nform\n\nEvidently there are multiple forms on a webpage. He didn‚Äôt give a good explanation for why he chose the first one\n‚Äúsession_key‚Äù and ‚Äúsession_password‚Äù are the ones needed\n\nFill out the necessary parts of the form and send it\nfilled_form &lt;- html_form_set(form, session_key = \"&lt;username&gt;\", session_password = \"&lt;password&gt;\")\nfilled_form # shows values that inputed next the form sections\nlog_in &lt;- session_submit(session, filled_form)\nConfirm that your logged in\nlog_in # prints url status = 200, type = text/html, size = 757813 (number of lines of html on page?)\nbrowseURL(log_in$url) # think this maybe opens browser\n\nExample: Filter a football stats table by selecting values from a dropdown menu on a webpage (also see RSelenium version)\n\nAfter set-up and navigating to url, get the forms from the webpage\nforms &lt;- html_form(session)\nforms # prints all the forms\n\nThe fourth has all the filtering menu categories (team, week, position, year), so that one is chosen\n\nFill out the form to enter the values you want to use to filter the table and submit that form to filter the table\nfilled_form &lt;- html_form_set(forms[[4]], \"team\" = \"DAL\", \"week\" = \"all\", \"position\" = \"QB\", \"year\" = \"2017\")\nsubmitted_session &lt;- session_submit(session = session, form = filled_form)\nLook for the newly filtered table\ntables &lt;- html_elements(submitted_session, \"table\")\ntables\n\nUsing inspect, you can see the 2nd one has &lt;table class = ‚Äúsortable stats-table‚Ä¶etc\n\nSelect the second table and convert it to a dataframe\nfootball_df &lt;- html_table(tables[[2]], header = TRUE)",
    "crumbs": [
      "Scraping"
    ]
  },
  {
    "objectID": "qmd/scraping.html#sec-scrap-rsel",
    "href": "qmd/scraping.html#sec-scrap-rsel",
    "title": "Scraping",
    "section": "RSelenium",
    "text": "RSelenium\n\nAlong with installing package you have to know the version of the browser driver of the browser you‚Äôre going to use\n\nhttps://chromedriver.chromium.org/downloads\nFind Chrome browser version\n\nThrough console\nsystem2(command = \"wmic\",\n¬† ¬† ¬† ¬† args = 'datafile where name=\"C:\\\\\\\\Program Files         (x86)\\\\\\\\Google\\\\\\\\Chrome\\\\\\\\Application\\\\\\\\chrome.exe\" get Version /value')\n\nList available Chrome drivers\nbinman::list_versions(appname = \"chromedriver\")\n\nIf no exact driver version matches your browser version,\n\nEach version of the Chrome driver supports Chrome with matching major, minor, and build version numbers.\n\nExample: Chrome driver 73.0.3683.20¬† supports all Chrome versions that start with 73.0.3683\n\n\n\n\nStart server and create remote driver\n\na browser will pop up and say ‚ÄúChrome is being controlled by automated test software‚Äù\n\nlibrary(RSelenium)\ndriver &lt;- rsDriver(browser = c(\"chrome\"), chromever = \"&lt;driver version&gt;\", port = 4571L) # assume the port number is specified by chrome driver ppl.\nremDr &lt;- driver[['client']] # can also use $client\nNavigate to a webpage\nremDr$navigate(\"&lt;url&gt;\")\nremDR$maxWindowSize(): Set the size of the browser window to maximum.\n\nBy default, the browser window size is small, and some elements of the website you navigate to might not be available right away\n\nGrab the url of the webpage you‚Äôre on\nremDr$getCurrentUrl()\nGo back and forth between urls\nremDr$goBack()\nremDr$goForward()\nFind html element (name, id, class name, etc.)\nwebpage_element &lt;- remDr$findElement(using = \"name\", value = \"q\") \n\nSee Misc section for selector options\nWhere ‚Äúname‚Äù is the element class and ‚Äúq‚Äù is the value e.g.¬†name=‚Äúq‚Äù if you used the inspect method in chrome\nAlso see Other Stuff &gt;&gt; Shadow DOM elements &gt;&gt; Use {shadowr} for alternate syntax to search for web elements\n\nHighlight element in pop-up browser to make sure you have the right thing\nwebpage_element$highlightElement()\nExample: you picked a search bar for your html element and now you want to use the search bar from inside R\n\nEnter text into search bar\nwebpage_element$sendKeysToElement(list(\"Scraping the web with R\"))\nHit enter to execute search\nwebpage_element$sendKeysToElement(list(key = \"enter\"))\n\nYou are now on the page with the results of the google search\n\nScrape all the links and titles on that page\nwebelm_linkTitles &lt;- remDr$findElement(using = \"css selector\", \".r\") \n\nInspect showed ‚Äù\n\n\n. Notice he used ‚Äú.r‚Äù. Says it will pick-up all elements with ‚Äúr‚Äù as the class.\n\nGet titles\n# first title\nwebelm_linkTitles[[1]]$getElementText()\n\n# put them all into a list\ntitles &lt;- purrr::map_chr(webelm_linkTitles, ~.x$getElementText())\ntitles &lt;- unlist(lapply(\n    webelm_linkTitles, \n    function(x) {x$getElementText()}\n\nExample: Access data that needs user authentication¬†(also see rvest version)\n\nAfter set-up and navigating to webpage, find elements where you type in your username and password\nwebelm_username &lt;- remDr$findElement(using = \"id\", \"Username\")\nwebelm_pass &lt;- remDr$findElement(using = \"id, \"Password\")\nEnter username and password\nwebpage_username$sendKeysToElement(list(\"&lt;username&gt;\"))\nwebpage_pass$sendKeysToElement(list(\"&lt;password&gt;\"))\nClick sign-in button and click it\nwebelm_sbutt &lt;- remDr$findElement(using = \"class\", \"psds-button\")\nwebelm_sbutt$clickElement()\n\nExample: Filter a football stats table by selecting values from a dropdown menu on a webpage (also see rvest version)\n\nThis is tedious ‚Äî use rvest to scrape this if possible (have to use rvest at the end anyways). html forms are the stuff.\nAfter set-up and navigated to url, find drop down ‚Äúteam‚Äù menu element locator using inspect in the browser and use findElement\nwebelem_team &lt;- remDr$findElement(using = \"name\", value = \"team\") # conveniently has name=\"team\" in the html\n\nAlso see Other Stuff &gt;&gt; Shadow DOM elements &gt;&gt; Use {shadowr} for alternate syntax to search for web elements\n\nclick team dropdown\nwebelem_team$clickElement()\nGo back to inspect in the browser, you should be able to expand the team menu element. Left click value that you want to filter team by to highlight it. Then right click the element and select ‚Äúcopy‚Äù ‚Äì&gt; ‚Äúcopy selector‚Äù. Paste selector into value arg\nwebelem_DAL &lt;- remDr$findElement(using = \"css\", value = \"edit-filters-0-team &gt; option:nth-child(22)\")\nwebelem_DAL$clickElement()\n\nAlso see Other Stuff &gt;&gt; Shadow DOM elements &gt;&gt; Use {shadowr} for alternate syntax to search for web elements\nRepeat process for week, position, and year drop down menu filters\n\nAfter you‚Äôve selected all the values in the dropdown, click the submit button to filter the table\nwebelem_submit &lt;- remDr$findElement(using = \"css\", value =     \"edit-filters-0-actions-submit\") \nwebelem_submit$clickElement()\n\nFinds element by using inspect on the submit button and copying the selector\n\nGet the url of the html code of the page with the filtered table. Read html code into R with rvest.\nurl &lt;- remDr$getPageSource()[[1]]\nhtml_page &lt;- rvest::read_html(url)\n\nIf you want the header, getPageSource(header = TRUE)\n\nUse rvest to scrape the table. Find the table with the stats\nall_tables &lt;- rvest::html_elements(html_page, \"table\")\nall_tables\n\nUsed the ‚Äúhtml_elements‚Äù version instead of ‚Äúelement‚Äù\nThird one has ‚Äú&lt;table class =‚Äùsortable stats-table full-width blah blah‚Äù\n\nSave to table to dataframe\nfootball_df &lt;- rvest::html_table(all_tables[[3]], header = TRUE)",
    "crumbs": [
      "Scraping"
    ]
  },
  {
    "objectID": "qmd/scraping.html#sec-scrap-ostuff",
    "href": "qmd/scraping.html#sec-scrap-ostuff",
    "title": "Scraping",
    "section": "Other Stuff",
    "text": "Other Stuff\n\nClicking a semi-infinite scroll button (e.g.¬†‚ÄúSee more‚Äù)\n\nExample: For-Loop\n# Find Page Element for Body\nwebElem &lt;- remDr$findElement(\"css\", \"body\")\n\n# Page to the End\nfor (i in 1:50) {\n¬† message(paste(\"Iteration\",i))\n¬† webElem$sendKeysToElement(list(key = \"end\"))\n\n¬† # Check for the Show More Button\n¬† element&lt;- try(unlist(\n¬†     remDr$findElement(\n¬†       \"class name\",\n¬†       \"RveJvd\")$getElementAttribute('class')), silent = TRUE)\n\n¬† #If Button Is There Then Click It\n¬† Sys.sleep(2)\n¬† if(str_detect(element, \"RveJvd\") == TRUE){\n¬† ¬† buttonElem &lt;- remDr$findElement(\"class name\", \"RveJvd\")\n¬† ¬† buttonElem$clickElement()\n¬† }\n\n¬† # Sleep to Let Things Load\n¬† Sys.sleep(3)\n}\n\narticle\nAfter scrolling to the ‚Äúend‚Äù of the page, there‚Äôs a ‚Äúshow me more button‚Äù that loads more data on the page\n\nExample: Recursive\nload_more &lt;- function(rd) {\n  # scroll to end of page\n  rd$executeScript(\"window.scrollTo(0, document.body.scrollHeight);\", args = list())\n\n  # Find the \"Load more\" button by its CSS selector and ...\n  load_more_button &lt;- rd$findElement(using = \"css selector\", \"button.btn-load.more\")\n\n  # ... click it\n  load_more_button$clickElement()\n\n  # give the website a moment to respond\n  Sys.sleep(5)\n}\n\nload_page_completely &lt;- function(rd) {\n  # load more content even if it throws an error\n  tryCatch({\n    # call load_more()\n    load_more(rd)\n    # if no error is thrown, call the load_page_completely() function again\n    Recall(rd)\n  }, error = function(e) {\n    # if an error is thrown return nothing / NULL\n  })\n}\n\nload_page_completely(remote_driver)\n\narticle\nRecall is a base R function that calls the same function it‚Äôs in.\n\n\nShadow DOM elements\n\n#shadow-root and shadow dom button elements\nMisc\n\nTwo options: {shadowr} or JS script\n\nExample: Use {shadowr}\n\nMy stackoverflow post\nSet-up\npacman::p_load(RSelenium, shadowr)\ndriver &lt;- rsDriver(browser = c(\"chrome\"), chromever = chrome_driver_version)\n# chrome browser\nchrome &lt;- driver$client\nshadow_rd &lt;- shadow(chrome)\nFind web element\n\nSearch for element using html tag\n\n\nwisc_dl_panel_button4 &lt;- shadowr::find_elements(shadow_rd, 'calcite-button')\nwisc_dl_panel_button4[[1]]$clickElement()\n\nShows web element located in #shadow-root\nSince there might be more than one element with the ‚Äúcalcite-button‚Äù html tag, we use the plural, find_elements, instead of find_element\nThere‚Äôs only 1 element returned, so we use [[1]] index to subset the list before clicking it\n\nSearch for web element by html tag and attribute\nwisc_dl_panel_button3 &lt;- find_elements(shadow_rd, 'button[aria-describedby*=\"tooltip\"]')\nwisc_dl_panel_button3[[3]]$clickElement()\n\n‚Äúbutton‚Äù is the html tag which is subsetted by the brackets, and ‚Äúaria-describedby‚Äù is the attribute\nOnly part of the attribute‚Äôs value is used, ‚Äútooltip,‚Äù so I think that‚Äôs why ‚Äú*=‚Äù instead of just ‚Äú=‚Äù is used. I believe the ‚Äú*‚Äù may indicate partial-matching.\nSince there might be more than one element with this¬† html tag + attribute combo, we use the plural, find_elements, instead of find_element\nThere are 3 elements returned, so we use [[3]] index to subset the list to element we want before clicking it\n\n\nExample: Use a JS script and some webelement hacks to get a clickable element\n\nMisc\n\n‚Äú.class_name‚Äù\n\nfill in spaces with periods\n\n‚Äú.btn btn-default hidden-xs‚Äù becomes ‚Äú.btn.btn-default.hidden-xs‚Äù\n\n\n\nYou can find the element path to use in your JS script by going step by step with JS commands in the Chrome console (bottom window)\n\nSteps\n\nWrite JS script to get clickable element‚Äôs elementId\n\nStart with element right above first shadow-root element and use querySelector\nMove to the next element inside the next shadow-root element using shadowRoot.querySelector\nContinue to desired clickable element\n\nIf there‚Äôs isn‚Äôt another shadow-root that you have to open, then the next element can be selected usingquerySelector\nIf you do have to click on another shadow-root element to open another branch, then used shadowRoot.querySelector\nExample\n\n\n‚Äúhub-download-card‚Äù is just above shadow-root so it needs querySelector\n‚Äúcalcite-card‚Äù is an element that‚Äôs one-step removed from shadow-root, so it needs shadowRoot.querySelector\n‚Äúcalcite-dropdown‚Äù (type = ‚Äúclick‚Äù) is not directly (see div) next to shadow-root , so it can selected using querySelector\n\n\nWrite and execute JS script\nwisc_dlopts_elt_id &lt;- chrome$executeScript(\"return document.querySelector('hub-download-card').shadowRoot.querySelector('calcite-card').querySelector('calcite-dropdown');\")\n\nMake a clickable element or just click the damn thing\n\nclickable element (sometimes this doesn‚Äôt work; needs to be a button or type=click)\n\nUse findElement to find a generic element class object that you can manipulate\nUse ‚Äú@‚Äù ninja-magic to force elementId into the generic webElement to coerce it into your button element\nUse clickElement to click the button\n\n# think this is a generic element that can always be used\nmoose &lt;- chrome$findElement(\"css\", \"html\")\nmoose@.xData$elementId &lt;- as.character(wisc_dlopts_elt_id)\nmoose$clickElement()\n\nClick the button\nchrome$executeScript(\"document.querySelector('hub-download-card').shadowRoot.querySelector('calcite-card').querySelector('calcite-dropdown').querySelector('calcite-dropdown-group').querySelector('calcite-dropdown-item:nth-child(2)').click()\")\n\n\n\nGet data from a hidden input\n\narticle\nHTML Element\n&lt;input type=\"hidden\" id=\"overview-about-text\" value=\"%3Cp%3E100%25%20Plant-Derived%20Squalane%20hydrates%20your%20skin%20while%20supporting%20its%20natural%20moisture%20barrier.%20Squalane%20is%20an%20exceptional%20hydrator%20found%20naturally%20in%20the%20skin,%20and%20this%20formula%20uses%20100%25%20plant-derived%20squalane%20derived%20from%20sugar%20cane%20for%20a%20non-comedogenic%20solution%20that%20enhances%20surface-level%20hydration.%3Cbr%3E%3Cbr%3EOur%20100%25%20Plant-Derived%20Squalane%20formula%20can%20also%20be%20used%20in%20hair%20to%20increase%20heat%20protection,%20add%20shine,%20and%20reduce%20breakage.%3C/p%3E\"&gt;\nExtract value and decode the text\noverview_text &lt;- webpage |&gt;\n  html_element(\"#overview-about-text\") |&gt;\n  html_attr(\"value\") |&gt;\n  URLdecode() |&gt;\n  read_html() |&gt;\n  html_text()\n\noverview_text\n#&gt; [1] \"100% Plant-Derived Squalane hydrates your skin while supporting its natural moisture barrier.",
    "crumbs": [
      "Scraping"
    ]
  },
  {
    "objectID": "qmd/sql.html",
    "href": "qmd/sql.html",
    "title": "SQL",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "qmd/sql.html#sec-sql-misc",
    "href": "qmd/sql.html#sec-sql-misc",
    "title": "SQL",
    "section": "",
    "text": "Resources\n\nPublicly Available SQL Databases: Need to email administrator to gain access\nSQL for Data Scientists in 100 Queries\n\nSQLite, administrative commands, query commands, JSON ops, python\n\n\ndplyr::show_query can convert a dplyr expression to SQL for db object (e.g.¬†dbplyr,¬† duckdb, arrow)\nQueries in examples\n\nWindow Functions\n\nAverage Salary by Job Title\nAverage Unit Price for each CustomerId\nRank customers by amount spent\nCreate a new column that ranks Unit Price in descending order for each CustomerId\nCreate a new column that provides the previous order date‚Äôs Quantity for each ProductId\nCreate a new column that provides the very first Quantity ever ordered for each ProductId\nCalculate a cumulative moving average UnitPrice for each CustomerId\nRank customers for each department by amount spent\nFind the model and year of the car that been on the lot the longest\nCreate a subset (CTE)\n\nCalculate a running monthly total (aka cumsum)\n\nAlso running average\n\nCalculate a running monthly total for each account id\nCalculate a 3 months rolling running total using a window that includes the current month.\nCalculate a 7 months rolling running total using a window where the current month is always the middle month\nCalculate the number of consecutive days spent in each country\n\n\nCTE\n\nAverage monthly cost per campaign for the company‚Äôs marketing efforts\nCount the number of interactions of new users\nThe average top Math test score for students in California\n\nBusiness Queries\n\n7-day Simple Moving Average (SMA)\nRank product categories by shipping cost for each shipping address\nDaily counts of open jobs (where ‚Äúopen‚Äù is an untracked daily status)\nGet the latest order from each customer\nOverall median price\nMedian price for each product\nOverall median price and quantity\n\nProcessing Expressions\n\nProvide subtotals for a hierarchical group of fields (e.g.¬†family, category, subcategory)\n\nSee NULLs &gt;&gt; COALESCE\n\n\n\nOrder of Operations\n\n\nHigher ranked functions can be inserted inside lower ranked functions\n\ne.g a window function can be inside a SELECT function but not inside a WHERE clause\nThere are exceptions and hacks around this in some cases\n\n\nTypes of Commands\n\nData Query Language (DQL) - used to find and view data without making any permanent changes to the database.\nData Manipulation Language (DML) - used to make permanent changes to the data, such as updating values or deleting them.\nData Definition Language (DDL) - used to make permanent changes to the table, such as creating or deleting a table.\nData Control Language (DCL) - used for administrative commands, such as adding or removing users of different tables and databases.\nTransact Control Language (TCL) - advanced SQL that deals with transaction level statements.\n\nMicrosoft SQL Server format for referencing a table:\n\n[database].[schema].[tablename]\nAlternative\nUSE my_data_base\nGO\n\nCheck if a table is updatable\nSELECT table_name, is_updatable\nFROM information_schema.views\n\nUseful if some of the tables you are working with are missing values that you need to add\nIf not updatable, then you‚Äôll need to contact the database administrator to request permission to update that specific table\nShow all tables\nSHOW FULL TABLES -- mysql",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "qmd/sql.html#sec-sql-setup",
    "href": "qmd/sql.html#sec-sql-setup",
    "title": "SQL",
    "section": "Set-up",
    "text": "Set-up\n\npostgres\n\nDownload postgres\npgAdmin is an IDE commonly used with postgres\n\nOpen pgAdmin and click on ‚ÄúAdd new server.‚Äù\n\nSets up connection to existing server so make sure postgres is installed beforehand\n\nCreate Tables\n\nhome &gt;&gt; Databases (1) &gt;&gt; postgres &gt;&gt; Query Tool\n\nIf needed, give permission to pgAdmin to access data from a folder\n\nMight be necessary to upload csv files\n\nImport csv file\n\nright-click the table name &gt;&gt; Import/Export\nOptions tab\n\nSelect import, add file path to File Name, choose csv for format, select Yes for Header, add , for Delimiter\n\nColumns tab\n\nuncheck columns not in the csv (probably the primary key)\n\nWonder if NULLs will be automatically inserted for columns in the table that aren‚Äôt in the file.",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "qmd/sql.html#sec-sql-terms",
    "href": "qmd/sql.html#sec-sql-terms",
    "title": "SQL",
    "section": "Terms",
    "text": "Terms\n\nBatch - a set of sql statements e.g.¬†statements between BEGIN and END\nCompiled object - you can create a function written in C/C++ and load into the database (at least in postgres) to achieve high performance.\nCorrelated Columns - tells how good the match between logical and physical ordering is.\nCorrelated/Uncorrelated Subqueries\n\ncorrelated- a type of query, where inner query depends upon the outcome of the outer query in order to perform its execution\n\nA correlated subquery can be thought of as a filter on the table that it refers to, as if the subquery were evaluated on each row of the table in the outer query\n\n\nuncorrelated - a type of sub-query where inner query doesn‚Äôt depend upon the outer query for its execution.\n\nIt is an independent query, the results of which are returned to and used by the outer query once (not per row).\n\n-- Uncorrelated subquery:\n-- inner query, c1, only depends on table2\nselect c1, c2\n¬† from table1 where c1 = (select max(x) from table2);\n\n-- Correlated subquery:\n-- inner query, c1, depends on table1 and table2\nselect c1, c2\n¬† from table1 where c1 = (select x from table2 where y = table1.c2);\n\nFunctions execute at a different level of priority and are handled differently than Views. You will likely see better performance.\nIndex - a quick lookup table (e.g.¬†field or set of fields) for finding records users need to search frequently. An index is small, fast, and optimized for quick lookups. It is very useful for connecting the relational tables and searching large tables. (also see DB, Engineering &gt;&gt; Cost Optimizations)\nMigrations (schema) - version control system for your database schema. Management of incremental, reversible changes and version control to relational database schemas. A schema migration is performed on a database whenever it is necessary to update or revert that database‚Äôs schema to some newer or older version.\nPhysical Ordering - A PostgreSQL table consists of one or more files of 8KB blocks (or ‚Äúpages‚Äù). The order in which the rows are stored in the file is the physical ordering.\nPredicate - defines a logical condition being applied to rows in a table. (e.g.¬†IN, EXISTS, BETWEEEN, LIKE, ALL, ANY)\nScalar/Non-Scalar Subqueries\n\nA scalar subquery returns a single value (one column of one row). If no rows qualify to be returned, the subquery returns NULL.\nA non-scalar subquery returns 0, 1, or multiple rows, each of which may contain 1 or multiple columns. For each column, if there is no value to return, the subquery returns NULL. If no rows qualify to be returned, the subquery returns 0 rows (not NULLs).\n\nSelectivity - the fraction of rows in a table or partition that is chosen by the predicate\n\nRefers to the quality of a filter in its ability to reduce the number of rows that will need to be examined and ultimately returned\n\nWith a high selectivity, using the primary key or indexes to get right to the rows of interest\nWith a low selectivity, a full table scan would likely be needed to get the rows of interest.\n\nHigher selectivity means: more unique data; fewer duplicates; fewer number of rows for each key value\nUsed to estimate the cost of a particular access method; it is also used to determine the optimal join order. A poor choice of join order by the optimizer could result in a very expensive execution plan.\n\nSoft-deleted - An operation in which a flag is used to mark data as unusable, without erasing the data itself from the database\nSurrogate key - very similar to a primary key in that it is a unique value for an object in a table. However, rather than being derived from actual data present in the table, it is a field generated by the object itself. It has no business value like a primary key does, but is rather only used for data analysis purposes. Can be generated using different columns that already exist in your table or more often from two or more tables. dbt function definition\n\nExamples: PostgreSQL serial column, Oracle sequence column, or MySQL auto_increment column, Snowflake _file + _line columns\nExample: Each employee id is concatenated with a department id (e.g.¬†marketing or finance)\n\n\nTransaction - a set of queries tied together such that if one query fails, the entire set of queries are rolled back to a pre-query state if the situation dictates.\n\nA database transaction, by definition, must be atomic, consistent, isolated and durable. These are popularly known as ACID properties.¬† These properties can ensure the concurrent execution of multiple transactions without conflict.\n\nViews - database objects that represent saved SELECT queries in ‚Äúvirtual‚Äù tables.\n\nContains a query plan. ¬†For each query executed, the planner has to evaluate what‚Äôs being asked and calculate an optimal path. Views already have this plan calculated so it allows subsequent queries to be returned with almost no friction in processing aside from data retrieval.\nSome views are updateable but under certain conditions (1-1 mapping of rows in view to underlying table, no group_by, etc.)",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "qmd/sql.html#sec-sql-basics",
    "href": "qmd/sql.html#sec-sql-basics",
    "title": "SQL",
    "section": "Basics",
    "text": "Basics\n\nMisc\n\nDROP TABLE &lt;table name&gt; - Deletes table from database. Used to remove obsolete or redundant tables from the database.\n\n\n\nCreate Tables\n\nIf you don‚Äôt include the schema as part of the table name (e.g.¬†schema_name.table_name), pgadmin automatically places it into the ‚Äúpublic‚Äù schema directory\nField Syntax: name, data type, constraints\nExample: Create table as select (CTAS)\nCREATE TABLE new_table AS¬†\nSELECT *¬†\nFROM old_table¬†\nWHERE condition;\nExample: Table 1 (w/primary key)\nDROP TABLE IF EXISTS classrooms CASCADE;\nCREATE TABLE classrooms (\n¬† ¬† id INT PRIMARY KEY GENERATED ALWAYS AS IDENTITY,\n¬† ¬† teacher VARCHAR(100)\n¬† ¬† );\n -- OR\nCREATE TABLE classrooms (¬†\n¬† ¬† id INT GENERATED ALWAYS AS IDENTITY,¬†\n¬† ¬† teacher VARCHAR(100)\n¬† ¬† PRIMARY KEY(id)¬†\n¬† ¬† );¬† ¬†\n\n‚Äúclassrooms‚Äù is the name of the table; ‚Äúid‚Äù and ‚Äúteacher‚Äù are the fields\nCASCADE - postgres won‚Äôt delete the table if other tables point to it, so cascade will override measure.\nGENERATED ALWAYS AS IDENTITY - makes it so you don‚Äôt have to keep track of which ‚Äúid‚Äù values have been used when adding rows. You can ommit the value for ‚Äúid‚Äù and just add the values for the other fields\nSee tutorial for options, usage, removing, adding, etc. this constraint\nINSERT INTO classrooms\n¬† ¬† (teacher)\nVALUES\n¬† ¬† ('Mary'),\n¬† ¬† ('Jonah');\n\nAlso see Add Data &gt;&gt; Example: chatGPT\n\n\nExample: Table 2 (w/foreign key)\nDROP TABLE IF EXISTS students CASCADE;\nCREATE TABLE students (\n¬† ¬† id INT PRIMARY KEY GENERATED ALWAYS AS IDENTITY,\n¬† ¬† name VARCHAR(100),\n¬† ¬† classroom_id INT,\n¬† ¬† CONSTRAINT fk_classrooms\n    ¬† ¬† FOREIGN KEY(classroom_id)\n¬† ¬† ¬† ¬† REFERENCES classrooms(id)\n);\n\n‚Äústudents‚Äù is the name of the table; ‚Äúid‚Äù, ‚Äúname‚Äù, and ‚Äúclassroom_id‚Äù are the fields\nCreate a foreign key that points to the ‚Äúclassrooms‚Äù table\n\nExpression\n\nfk_classrooms is the name of the CONSTRAINT\n‚Äúclassroom_id‚Äù is the field that will be the FOREIGN KEY\nREFERENCES points the foreign key to the classrooms table‚Äôs primary key, ‚Äúid‚Äù\n\nforeign keys can point to any table\n\n\nPostgres won‚Äôt allow you to insert a row into students with a ‚Äúclassroom_id‚Äù that doesn‚Äôt exist in the ‚Äúid‚Äù field of classrooms but will allow you to use a NULL placeholder\n-- Explicitly specify NULL\nINSERT INTO students\n¬† ¬† (name, classroom_id)\nVALUES\n¬† ¬† ('Dina', NULL);¬†\n\n-- Implicitly specify NULL\nINSERT INTO students\n¬† ¬† (name)\nVALUES\n¬† ¬† ('Evan');\n\nAlso see Add Data &gt;&gt; Example: chatGPT\n\n\n\nExample\nCREATE TABLE members (\n¬† ¬† id serial primary key,\n¬† ¬† second_name character varying(200) NOT NULL,\n¬† ¬† date_joined date NOT NULL DEFAULT current_date,\n¬† ¬† member_id integer references members(id),\n¬† ¬† booking_start_time timestamp without timezone NOT NULL\n\nThe ‚Äúserial‚Äù data type does the same thing as GENERATED ALWAYS AS IDENTITY (see first example), but is NOT compliant with the SQL standard. Use GENERATED ALWAYS AS IDENTITY\n‚Äúreferences‚Äù seems to be another old way to create foreign keys (see 2nd example for proper way)\n‚Äúcharacter varying‚Äù - variable-length with limit (e.g limit of 200 characters)\n\ncharacter(n), char(n) are for fixed character lengths; text is for unlimited character lengths\n\n‚Äúcurrent_date‚Äù is a function that will insert the current date as a value\n‚Äútimestamp without timezone‚Äù is literally that\n\nalso available: time with/without timezone, date, interval (see Docs for details)\n\n\nExample: MySQL\nCREATE DATABASE products;\n\nCREATE TABLE `products`.`prices` (\n  `pid` int(11) NOT NULL AUTO_INCREMENT,\n  `category` varchar(100) NOT NULL,\n  `price` float NOT NULL,\n  PRIMARY KEY (`pid`)\n);\n\nINSERT INTO products.prices\n    (pid, category, price)\nVALUES\n    (1, 'A', 2),\n    (2, 'A', 1),\n    (3, 'A', 5),\n    (4, 'A', 4),\n    (5, 'A', 3),\n    (6, 'B', 6),\n    (7, 'B', 4),\n    (8, 'B', 3),\n    (9, 'B', 5),\n    (10, 'B', 2),\n    (11, 'B', 1)\n;\n\n\n\nAdd Data\n\nExample: Copy/Paste table values into chatGPT to get the query\n\nExample: Add data via .csv\nCOPY assignments(category, name, due_date, weight)\nFROM 'C:/Users/mgsosna/Desktop/db_data/assignments.csv'\nDELIMITER ','\nCSV HEADER;\n\n‚Äúassignments‚Äù is the table; ‚Äúcategory‚Äù, ‚Äúname‚Äù, ‚Äúdue_date‚Äù, ‚Äúweight‚Äù are fields that you want to import from the csv file\n** The order of the columns must be the same as the ones in the CSV file **\nHEADER keyword to indicate that the CSV file contains a header\nMight need to have superuser access in order to execute the COPY statement successfully\n\n\n\n\nUpdate Table\n\nUpdate target table by transaction id (BQ)(link)\ninsert target_table (transaction_id)\n  select transaction_id \n  from source_table \n  where transaction_id &gt; (select max(transaction_id) from target_table)\n;\n\nMight not be possible with denormalized star-schema datasets in modern data warehouses.\n\nUpdate Target Table by transaction date (BQ) (link)\nmerge last_online t\nusing (\n  select\n      user_id,\n      last_online\n  from\n    (\n        select\n            user_id,\n            max(timestamp) as last_online\n\n        from \n            connection_data\n        where\n            date(_partitiontime) &gt;= date_sub(current_date(), \n                                             interval 1 day)\n        group by\n            user_id\n\n    ) y\n\n) s\non t.user_id = s.user_id\nwhen matched then\n  update set last_online = s.last_online, \n             user_id = s.user_id\nwhen not matched then\n  insert (last_online, user_id) \n    values (last_online, user_id)\n;\nselect * from last_online\n;\n\nMERGE performs UPDATE, DELETE, and INSERT\n\nUPDATE or DELETE clause can be used when two or more data match.\nINSERT clause can be used when two or more data are different and do not match.\nThe UPDATE or DELETE clause can also be used when the given data does not match the source.\n\n_partitiontime is a field BQ creates to record the row‚Äôs ingestion time (See Google, Big Query &gt;&gt; Optimization &gt;&gt; Partitions\n\nUpdate Target Table with Source Data (link)\nMERGE INTO target_table tgt\nUSING source_table src \n ON tgt.customer_id = src.customer_id\nWHEN MATCHED THEN\n UPDATE SET\n   tgt.is_active = src.is_active,\n   tgt.updated_date = '2024-04-01'::DATE\nWHEN NOT MATCHED THEN\n INSERT\n   (customer_id, is_active, updated_date)\n VALUES\n (src.customer_id, src.is_active, '2024-04-01'::DATE)\n; \n\nThe statement uses the MERGE keyword to conditionally update or insert rows into a target table based on a source table.\nIt matches rows between the tables using the ON clause and the customer_id column.\nThe WHEN MATCHED THEN clause specifies the update actions for matching rows.\nThe WHEN NOT MATCHED THEN clause specifies the insert actions for rows that don‚Äôt have a match in the target table.\nThe ::DATE cast ensures that the updated_date value is treated as a date.\n\n\n\n\nDelete Rows\n\nDELETE CASCADE\n\nDeletes related rows in child tables when a parent row is deleted from the parent table\nApply cascade to foreign key in the child table\nCREATE TABLE parent_table(\n    id SERIAL PRIMARY KEY,\n    ...\n);\n\nCREATE TABLE child_table(\n    id SERIAL PRIMARY KEY,\n    parent_id INT,\n    FOREIGN_KEY(parent_id) \n       REFERENCES parent_table(id)\n       ON DELETE CASCADE\n);\n\nIn the child table, the parent_id is a foreign key that references the id column of the parent_table.\nThe ON DELETE CASCADE is the action on the foreign key that will automatically delete the rows from the child_table whenever corresponding rows from the parent_table are deleted.\nExample\nDELETE FROM parent_table \nWHERE id = 1;\n\nid = 1 in parent_table corresponds to parent_id = 1 in child_table. Therefore, all rows matching those conditions in both tables will be deleted.\n\n\n\n\n\n\nSubqueries\n\n**Using CTEs instead of subqueries make code more readable**\n\nSubqueries make it difficult to understand their context in the larger query\nThe only way to debug a subquery is by turning it into a CTE or pulling it out of the query entirely.\nCTEs and subqueries have a similar runtime, but subqueries make your code more complex for no reason.\n\nNotes from How to Use SubQueries in SQL\n\nAlso shows the alt method of creating a temporary table to compute the queries\n\nUse cases\n\nFiltering rows from a table with the context of another.\nPerforming double-layer aggregations such as average of averages or an average of sums.\nAccessing aggregations with a subquery.\n\nTables used in examples\n\nStore A (store_a)\n\n\nStore B is similar\n\n\nExample: Filtering rows\nselect * \nfrom sandbox.store_b\nwhere product_id IN (\n¬† ¬† select product_id\n¬† ¬† from sandbox.store_b\n¬† ¬† group by product_id¬†\n¬† ¬† having count(product_id) &gt;= 3\n);\n\nfilters the rows with products that have been bought at least three times in store_b\n\nExample: Multi-Layer Aggregation\nselect avg(average_price.total_value) as average_transaction from (\n¬† select transaction_id, sum(price_paid) as total_value\n¬† from sandbox.store_a\n¬† group by transaction_id\n¬† ) as average_price\n;\n\ncomputes the average of all transactions\ncan‚Äôt apply an average directly, as our table is oriented to product_ids and not to transaction_ids\n\nExample: Filtering the table based on an Aggregation\nselect @avg_transaction:= avg(agg_table.total_value)\nfrom (\n  select transaction_id, sum(price_paid) as total_value\n  from sandbox.store_a\n  group by transaction_id\n) as agg_table;\n\nselect *¬†\nfrom sandbox.store_a\nwhere transaction_id in (\n  select transaction_id\n  from sandbox.store_a\n  group by transaction_id\n¬† having sum(price_paid) &gt; @avg_transaction\n)\n\nfilters transactions that have a value higher than the average (where the output must retain the original product-oriented row)\n\n\n\n\nJoins\n\n\n\nCross Join -¬† acts like an expand_grid; where each value in the join key column gets all combinations of rows in both tables (also see above pic)\n\n\nEfficient join\nWhen you add the where clause, the cross join acts similarly to an inner join, except you aren‚Äôt joining it on any specified column\nExample:\nSELECT\n¬† schedule.event,\n¬† calendar.number_of_days\nFROM schedule\nCROSS JOIN calendar\nWHERE schedule.total_number &lt; calendar.number_of_days\n\nOnly join the row in the ‚Äúschedule‚Äù table with the rows in the ‚Äúcalendar‚Äù table that meet the specified condition\n\n\nNatural Join - don‚Äôt need to specify join columns; need to have two columns in each table with the same name\n\nUse cases\n\nThere are a lot of common columns with the same name across multiple tables\n\nThey will all be used as joining keys.\n\nYou don‚Äôt want to type out all of the common columns in select just to avoid outputting the same columns multiple times.\n\n\nselect *\nfrom table_a\nnatural join table_b\n;\n\n-- natural + outer\nselect *\nfrom table_a\nnatural outer join table_b\n;",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "qmd/sql.html#sec-sql-r",
    "href": "qmd/sql.html#sec-sql-r",
    "title": "SQL",
    "section": "R",
    "text": "R\n\n{dbplyr}\n\nDocs\nQueries will not be run and data will not be retrieved unless you ask for it: they all return a new tbl_dbi object.\nUse compute() to run the query and save the results in a temporary in the database\nUse collect() to retrieve the results to R.\nUse show_query to see the query.\nUse explain to describe the query plan\nSyntax for accessing tables\ncon |&gt; \n  tbl(I(\"catalog_name.schema_name.table_name\"))\ncon |&gt; \n  tbl(I(\"schema_name.table_name\"))\nGet query from dplyr code\ntbl_to_sql &lt;- function(tbl) {\n¬† dplyr::show_query(tbl) |&gt;¬†\n¬† ¬† capture.output() |&gt;¬†\n¬† ¬† purrr::discard_at(1) |&gt;¬†\n¬† ¬† paste(collapse = \" \")\n}\n\nshow_query outputs a separate line for each expression. This code transforms query output into a one long string.\nAlso see Generating SQL with {dbplyr} and sqlfluff\n\n\n\n\n{DBI}\n\nConnect to or Create a SQLite database\ncon &lt;- DBI::dbConnect(drv = RSQLite::SQLite(),\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† here::here(\"db_name.db\"),\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† timeout = 10)\nConnect to Microsoft SQL Server\ncon &lt;- DBI::dbConnect(odbc::odbc(),¬†\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† Driver = \"SQL Server\",¬†\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† Server = \"SERVER\",¬†\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† Database = \"DB_NAME\",¬†\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† Trusted_Connection = \"True\",¬†\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† Port = 1433)\nClose connection: dbDisconnect(con)\nCancel a running query (postgres)\n# Store PID\npid &lt;- DBI::dbGetInfo(conn)$pid\n\n# Cancel query and get control of IDE back\n# SQL command\nSELECT pg_cancel_backend(&lt;PID&gt;)\n\nUseful if query is running too long and you want control of your IDE back\n\nCreate single tables from a list of tibbles to a database\npurrr::map2(table_names, list_of_tbls, ~ dbWriteTable(con, .x, .y))\nLoad all tables from a database into a list\ntables &lt;- dbListTables(con)\nall_data &lt;- map(tables, dbReadTable, conn = con)\nCan use map_dfr if all the tables have the same columns\nDynamic queries with {glue}\n\nExample: MS SQL Server\nvars &lt;- c(\"columns\", \"you\", \"want\", \"to\", \"select\")\ndate_var &lt;- 'date_col'\nstart_date &lt;- as.Date('2022-01-01')\ntoday &lt;- Sys.Date()\ntablename &lt;- \"yourtablename\"\nschema_name &lt;- \"yourschema\"\nquery &lt;- glue_sql(.con = con, \"SELECT TOP(10) {`vars`*} FROM {`schema_name`}.{`tablename`} \")\nDBI::dbGetQuery(con, query)\n\nvars format collapses the vars vector, separated by commas, so that it resembles a SELECT statement\n\n\n\n\n\nPRQL\n\nDocs\n{prqlr}\nA dplyr + SQL hybrid language\nUsing with DuckDB\nlibrary(prqlr); library(duckdb)\ncon &lt;- dbConnect(duckdb(), dbdir = \":memory\")\ndbWriteTable(con, \"mtcars\", mtcars)\n\"from mtcars | filter cyl &gt; 6 | select {cyl, mpg}\" |&gt; \n  prql_compile() |&gt; \n  dbGetQuery(conn = con)",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "qmd/sql.html#sec-sql-bestp",
    "href": "qmd/sql.html#sec-sql-bestp",
    "title": "SQL",
    "section": "Best Practices",
    "text": "Best Practices\n\nMisc\n\nResources\n\nSQL Style Guide\n\nUse aliases only when table names are long enough so that using them improves readability (but choose meaningful aliases)\nDo not use¬†SELECT *. Explicitly list columns instead\nUse comments to document business logic\nA comment at the top should provide a high-level description\nUse an auto-formatter\nGeneral Optimizations\n\nRemoving duplicates or filtering out null values at the beginning of your model will speed up queries\nReplace complex code with window functions\n\nExample: Replace GROUP_BY + TOP with a partition + FIRST_VALUE()\nFIRST_VALUE(test_score) OVER(PARTITION BY student_name ORDER BY test_score DESC)\nExample: AVG(test_score) OVER(PARTITION BY student_name)\n\n\n\nCTEs\n\nBreak down logic in CTEs using¬†WITH ‚Ä¶ AS\nThe¬†SELECT¬†statement inside each CTE must do a single thing (join, filter or aggregate)\nThe CTE name should provide a high-level explanation\nThe last statement should be a¬†SELECT¬†statement querying the last CTE\n\nJoins\n\nUse¬†WHERE¬†for filtering, not for joining\nFavor¬†LEFT JOIN¬†over¬†INNER JOIN; in most cases, it‚Äôs essential to know the distribution of¬†NULLs\nAvoid using‚ÄùSelf-Joins.‚Äù Use window functions instead (see Google, BigQuery &gt;&gt; Optimization for details on self-joins)\nWhen doing equijoins (i.e., joins where all conditions have the¬†something=another¬†form), use the¬†USING¬†keyword\nBreak-up joins using OR into UNION because SQL uses nested operations for JOIN + OR queries which slow things.\n\nBad\n\nGood\n\nUNION simply joins the outputs of two separate SELECT statements and retains only one occurrence of duplicated rows if there are any.\n\n\nStyle Guide",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "qmd/sql.html#sec-sql-index",
    "href": "qmd/sql.html#sec-sql-index",
    "title": "SQL",
    "section": "Indexes",
    "text": "Indexes\n\nMisc\n\nAn index may consist of up to 16 columns\nThe first column of the index must always be present in the query‚Äôs filter, order , join or group operations to be used\n\nCreate Index on an existing table (postgres)\nCREATE INDEX\n¬† ¬† score_index ON grades(score, student);\n\n‚Äúscore_index‚Äù is the name of the index\n‚Äúgrades‚Äù is the name of the table\n‚Äúscore‚Äù and ‚Äústudent‚Äù are fields to be used as the indexes\n\nCreate Index that only uses a specific character length\n/* mysql */\nCREATE TABLE test (blob_col BLOB, INDEX(blob_col(10)));\n\nindex only uses the first 10 characters of the column value of a BLOB column type\n\nCreate index with multiple columns\n/* mysql */\nCREATE TABLE test (\n¬† ¬† id¬† ¬† ¬† ¬† INT NOT NULL,\n¬† ¬† last_name¬† CHAR(30) NOT NULL,\n¬† ¬† first_name CHAR(30) NOT NULL,\n¬† ¬† PRIMARY KEY (id),\n¬† ¬† INDEX name (last_name,first_name)\n)\nUsage of multiple column index (** order of columns is important **)\nSELECT * FROM test WHERE last_name='Jones';\nSELECT * FROM test\n¬† WHERE last_name='Jones' AND first_name='John';\nSELECT * FROM test\n¬† WHERE last_name='Jones'\n¬† AND (first_name='John' OR first_name='Jon');\nSELECT * FROM test\n¬† WHERE last_name='Jones'\n¬† AND first_name &gt;='M' AND first_name &lt; 'N';\n\nIndex is used when both columns are used as part of filtering criteria or when only the left-most column is used\nif you have a three-column index on (col1, col2, col3), you have indexed search capabilities on (col1), (col1, col2), and (col1, col2, col3).\n\nInvalid usage of multiple column index\nSELECT * FROM test WHERE first_name='John';\nSELECT * FROM test\n¬† WHERE last_name='Jones' OR first_name='John';\n\nThe ‚Äúname‚Äù index won‚Äôt be used in these queries since\n\nfirst_name is NOT the left-most column specified in the index\nOR is used instead of AND\n\n\nCreate index with DESC, ASC\n/* mysql */\nCREATE TABLE t (\n¬† c1 INT, c2 INT,\n¬† INDEX idx1 (c1 ASC, c2 ASC),\n¬† INDEX idx2 (c1 ASC, c2 DESC),\n¬† INDEX idx3 (c1 DESC, c2 ASC),\n¬† INDEX idx4 (c1 DESC, c2 DESC)\n);\n\nUsed by ORDER BY\n\nSee Docs to see what operations and index types support Descending Indexes\n\nNote: idx_a on column_p, column_q desc is not the same as an * Index idx_a on column_q desc, column p or, * Index idx_b on column_p desc, column q\n\nUsage of Descending Indexes\nORDER BY c1 ASC, c2 ASC¬† ¬† -- optimizer can use idx1\nORDER BY c1 DESC, c2 DESC¬† -- optimizer can use idx4\nORDER BY c1 ASC, c2 DESC¬† -- optimizer can use idx2\nORDER BY c1 DESC, c2 ASC¬† -- optimizer can use idx3\n\nSee previous example for definition of idx* names\nSee Docs to see what operations and index types support Descending Indexes",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "qmd/sql.html#sec-sql-part",
    "href": "qmd/sql.html#sec-sql-part",
    "title": "SQL",
    "section": "Partitioning",
    "text": "Partitioning\n\nMisc\n\nAlso see\n\nMySQL Docs\nGoogle, BigQuery &gt;&gt; Optimization &gt;&gt; Partitioning and Clustering\nDB, Engineering &gt;&gt; Cost Optimization &gt;&gt; Partitioning\n\nall of your queries to the partitioned table must contain the partition_key in the WHERE clause",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "qmd/sql.html#sec-sql-views",
    "href": "qmd/sql.html#sec-sql-views",
    "title": "SQL",
    "section": "Views",
    "text": "Views\n\nA smaller data object that contains the subset of data resulting from a specific query\nWhereas a query happens after data is loaded, a materialized view is a precomputation\nThe computation is done once, and changes to the data are incorporated as they occur, making subsequent updates to the view much cheaper and more efficient than querying the entire database from scratch.\nCreate a View\n\nExample: Create view as select (CVAS)\nCREATE VIEW high_earner AS¬†\nSELECT p.id AS person_id, j.salary\nFROM People p\nJOIN Job j¬†\nON p.job = j.title\nWHERE j.salary &gt;= 200000;\n\nQuery a view (same as a table): SELECT * FROM high_earner\nUpdate view\nCREATE OR REPLACE VIEW high_earner AS¬†\nSELECT p.id AS person_id, j.salary\nFROM People p\nJOIN Job j¬†\nON p.job = j.title\nWHERE j.salary &gt;= 150000;\n\nExpects the query output to retain the same number of columns, column names, and column data types. Thus, any modification that results in a change in the data structure will raise an error.\n\nList views\n\nSELECT *¬†\nFROM information_schema.views\nWHERE table_schema NOT IN ('pg_catalog', 'information_schema');\n\n‚Äútable_name‚Äù has the names of the views\n‚Äúview_definition‚Äù shows the query stored in the view\nWHERE command is included to omit built-in views from PostgreSQL.\n\nDelete view: DROP VIEW high_earner;",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "qmd/sql.html#sec-sql-vars",
    "href": "qmd/sql.html#sec-sql-vars",
    "title": "SQL",
    "section": "Variables",
    "text": "Variables\n\nMisc\n\nAlso see Business Queries &gt;&gt; Medians\n\nUser-defined\n\nDECLARE and SET\n-- Declare your variables\nDECLARE @start date\nDECLARE @stop date\n--‚ÄäSET the relevant values for each variable\nSET @start = '2021-06-01'\nSET @stop = GETDATE()\n\nDECLARE sets the variable type (e.g.¬†date)\nSET assigns a value\n\nOr just use DECLARE\nDECLARE @Iteration Integer = 0;\nExamples\n\nExample: Exclude 3 months of data from the query\nSELECT t1.[DATETIME], COUNT(*) AS vol\nFROM Medium.dbo.Earthquakes t1\nWHERE t1.[DATETIME] BETWEEN @start AND DATEADD(MONTH, -3, @stop)\nGROUP BY t1.[DATETIME]\nORDER BY t1.[DATETIME] DESC;\n\nSee above for the definitions of @start and @stop\n\nExample: Apply a counter\n-- Declare the variable (a SQL Command, the var name, the datatype)\nDECLARE @counter INT;\n--‚ÄäSet the counter to 20\nSET @counter = 20;\n-- Print the initial value\nSELECT @counter AS _COUNT;\n--‚ÄäSelect and increment the counter by one\nSELECT @counter = @counter + 1;\n-- Print variable\nSELECT @counter AS _COUNT;\n--‚ÄäSelect and increment the counter by one\nSELECT @counter += 1;\n--‚ÄäPrint the variable\nSELECT @counter AS _COUNT;\n\n\nSystem\n\nROWCOUNT - returns the number of rows affected by the last previous statement\n\nExample\nBEGIN\n¬† ¬† SELECT\n¬† ¬† ¬† ¬† product_id,\n¬† ¬† ¬† ¬† product_name\n¬† ¬† FROM\n¬† ¬† ¬† ¬† production.products\n¬† ¬† WHERE\n¬† ¬† ¬† ¬† list_price &gt; 100000;\n¬† ¬† IF @@ROWCOUNT = 0\n¬† ¬† ¬† ¬† PRINT 'No product with price greater than 100000 found';\nEND",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "qmd/sql.html#sec-sql-funs",
    "href": "qmd/sql.html#sec-sql-funs",
    "title": "SQL",
    "section": "Functions",
    "text": "Functions\n\n‚Äú||‚Äù Concantenate strings. e.g ‚ÄòPost‚Äô || ‚ÄògreSQL‚Äô ‚Äì&gt; PostgreSQL\nBEGIN‚Ä¶END - defines a compound statement or statement block. A compound statement consists of a set of SQL statements that execute together. A statement block is also known as a batch\n\nA compound statement can have a local declaration for a variable, a cursor, a temporary table, or an exception\n\nLocal declarations can be referenced by any statement in that compound statement, or in any compound statement nested within it.\nLocal declarations are invisible to other procedures that are called from within a compound statement\n\n\nCOMMIT - a transaction control language that is used to permanently save the changes done in the transaction in tables/databases. The database cannot regain its previous state after its execution of commit.\nDATEADD - adds units of time to a variable or value\n\ne.g.¬†DATEADD(month, -3, '2021-06-01')\n\nsubtracts 3 months from 2021-06-01\n\n\nDATE_TRUNC - pulls a component of a date object.\n\ne.g.¬†date_trunc('month', date_var) as month\n\nDENSE_RANK- similar to the RANK , but it does not skip any numbers even if there is a tie between the rows.\n\nValues are ranked by the column specified in ORDER BY expression of the window function\n\nEXPLAIN - a means of running your query as a what-if to see what the planner thinks about it. It will show the process the system goes through to get to the data and return it.\nEXPLAIN\nSELECT\n¬† ¬† s.id AS student_id,\n¬† ¬† g.score\nFROM\n¬† ¬† students AS s\nLEFT JOIN\n¬† ¬† grades AS g\n¬† ¬† ON s.id = g.student_id\nWHERE\n¬† ¬† g.score &gt; 90\nORDER BY\n¬† ¬† g.score DESC;\n/*\nQUERY PLAN\n----------\nSort (cost=80.34..81.88 rows=617 width=8)\n[...] Sort Key: g.score DESC\n[...] -&gt; Hash Join (cost=16.98..51.74 rows=617 width=8)\n[...] Hash Cond: (g.student_id = s.id)\n[...] -&gt; Seq Scan on grades g (cost=0.00..33.13 rows=617 width=8)\n[...] Filter: (score &gt; 90)\n[...] -&gt; Hash (cost=13.10..13.10 rows=310 width=4)\n[...] -&gt; Seq Scan on students s (cost=0.00..13.20 rows=320 width=4)\n*/\n\nSequentially scanning (‚ÄúSeq Scan‚Äù) the grades and students tables because the tables aren‚Äôt indexed\n\nAny Seq Scan, parallel or not, is sub-optimal\n\nEXPLAIN (BUFFERS) also shows how may data pages the database had to fetch using slow disk read operations (‚Äúread‚Äù), and how many of them were cached in memory (‚Äúshared hit‚Äù)\nAlso see Databases, Engineering &gt;&gt; Cost Optimizations\n\nEXPLAIN ANALYZE - tells the planner to not only hypothesize on what it would do, but actually run the query and show the results.\n\nShows where indexes are being hit ‚Äî or not hit as it may be. You can step through and re-optimize your basic and complex queries.\nAlso see Databases, Engineering &gt;&gt; Cost Optimizations\n\nGETDATE() - Gets the current date\nGO - Not a sql function. Used by some interpreters as a reset.\n\ni.e.¬†any variables set before the GO statement will now not be recognized by the interpreter.\nHelps to separate code into different sections\n\nISDATE - boolean - checks that a variable is a date type\nQUALIFY - clause filters the results of window functions.\n\nuseful when answering questions like fetching the most XXX value of each category\nQUALIFY does with window functions as what HAVING does with GROUP BY. As a result, in the order of execution, QUALIFY is evaluated after window functions.\nSee Business Queries &gt;&gt; Get the latest order from each customer\n\nUNNEST - BigQuery - takes an ARRAY and returns a table with a row for each element in the ARRAY (docs)\n\nGoogle Analytics, Analysis &gt;&gt; Example 17",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "qmd/sql.html#sec-sql-udfs",
    "href": "qmd/sql.html#sec-sql-udfs",
    "title": "SQL",
    "section": "User Defined Functions (UDF)",
    "text": "User Defined Functions (UDF)\n\nMisc\n\nAvailable in SQL Server (Docs1, Docs2), Postgres (Docs), BigQuery (docs), etc.\nKeep a dictionary with the UDFs you‚Äôve created and make sure to share it with any collaborators.\nCan be persistent or temporary\n\nPersistent UDFs can be used across multiple queries, while temporary UDFs only exist in the scope of a single query\n\n\nCreate\n\nExample: temporary udf (BQ)\nCREATE TEMP FUNCTION AddFourAndDivide(x INT64, y INT64)\nRETURNS FLOAT64\nAS (\n¬† (x + 4) / y\n);\nSELECT\n¬† val, AddFourAndDivide(val, 2)\nFROM\n¬† UNNEST([2,3,5,8]) AS val;\nExample: persistent udf (BQ)\nCREATE FUNCTION mydataset.AddFourAndDivide(x INT64, y INT64)\nRETURNS FLOAT64\nAS (\n¬† (x + 4) / y\n);\n\nSELECT\n¬† val, mydataset.AddFourAndDivide(val, 2)\nFROM\n¬† UNNEST([2,3,5,8,12]) AS val;\n\nDelete persistent udf: DROP FUNCTION &lt;udf_name&gt;\nWith Scalar subquery (BQ)\nCREATE TEMP FUNCTION countUserByAge(userAge INT64)\nAS (\n¬† (SELECT COUNT(1) FROM users WHERE age = userAge)\n);\nSELECT\n¬† countUserByAge(10) AS count_user_age_10,\n¬† countUserByAge(20) AS count_user_age_20,\n¬† countUserByAge(30) AS count_user_age_30;",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "qmd/sql.html#sec-sql-loops",
    "href": "qmd/sql.html#sec-sql-loops",
    "title": "SQL",
    "section": "Loops",
    "text": "Loops\n\nMisc\n\nposgres docs for loops\n\nWHILE\n\nExample: Incrementally add to a counter variable\n-- Declare the initial value\nDECLARE @counter INT;\nSET @counter = 20;\n-- Print initial value\nSELECT @counter AS _COUNT;\n--‚ÄäCreate a loop\nBEGIN;\n--‚ÄäLoop code starting point\nWHILE @counter &lt; 30\nSELECT @counter = @counter + 1;\n--‚ÄäLoop finish\nEND;\n--‚ÄäCheck the value of the variable\nSELECT @counter AS _COUNT;\n\nCursors (Docs)\n\nRather than executing a whole query at once, it is possible to set up a cursor that encapsulates the query, and then read the query result a few rows at a time.\n\nOne reason for doing this is to avoid memory overrun when the result contains a large number of rows. (However, PL/pgSQL users do not normally need to worry about that, since FOR loops automatically use a cursor internally to avoid memory problems.)\nA more interesting usage is to return a reference to a cursor that a function has created, allowing the caller to read the rows. This provides an efficient way to return large row sets from functions.\n\nExample (article (do not pay attention dynamic sql. it‚Äôs for embedding sql in C programs))\nDECLARE\n¬† ¬† cur_orders CURSOR FOR¬†\n¬† ¬† ¬† ¬† SELECT order_id, product_id, quantity\n¬† ¬† ¬† ¬† FROM order_details\n¬† ¬† ¬† ¬† WHERE product_id = 456;\n¬† ¬† product_inventory INTEGER;\nBEGIN\n¬† ¬† OPEN cur_orders;\n¬† ¬† LOOP\n¬† ¬† ¬† ¬† FETCH cur_orders INTO order_id, product_id, quantity;\n¬† ¬† ¬† ¬† EXIT WHEN NOT FOUND;\n¬† ¬† ¬† ¬† SELECT inventory INTO product_inventory FROM products WHERE product_id = 456;\n¬† ¬† ¬† ¬† product_inventory := product_inventory - quantity;\n¬† ¬† ¬† ¬† UPDATE products SET inventory = product_inventory WHERE product_id = 456;\n¬† ¬† END LOOP;\n¬† ¬† CLOSE cur_orders;\n¬† ¬† -- do something after updating the inventory, such as logging the changes\nEND;\n\nA table called ‚Äúproducts‚Äù that contains information about all products, including the product ID, product name, and current inventory. You can use a cursor to iterate through all orders that contain a specific product and update its inventory.\nA cursor called ‚Äúcur_orders‚Äù that selects all order details that contain a specific product ID. We then define a variable called ‚Äúproduct_inventory‚Äù to store the current inventory of the product.\nInside the loop, we fetch each order ID, product ID, and quantity from the cursor, subtract the quantity from the current inventory and update the products table with the new inventory value.\nFinally, we close the cursor and do something after updating the inventory, such as logging the changes.",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "qmd/sql.html#sec-sql-winfun",
    "href": "qmd/sql.html#sec-sql-winfun",
    "title": "SQL",
    "section": "Window Functions",
    "text": "Window Functions\n\nUnlike GROUP BY, keeps original columns after an aggregation\n\n\nAllows you to work with both aggregate and non-aggregate values all at once\n\nBetter performance than using GROUP BY + JOIN to get the same result\n\n\nDespite the order of operations, if you really need to have a window function inside a WHERE clause or GROUP BY clause, you may get around this limitation by using a subquery or a WITH query\n\nExample: Remove duplicate rows\nWITH temporary_employees as\n(SELECT¬†\n¬† employee_id,\n¬† employee_name,\n¬† department,\n¬† ROW_NUMBER() OVER(PARTITION BY employee_name,\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† department,\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† employee_id) as row_count\nFROM Dummy_employees)\n\nSELECT *\nFROM temporary_employees\nWHERE row_count = 1\n\n3 Types of Window Functions\n\n\nLEAD() will give you the row AFTER the row you are finding a value for.\nLAG() will give you the row BEFORE the row you are finding a value for.\nFIRST_VALUE() returns the first value in an ordered, partitioned data output.\n\nGeneral Syntax\n\n\nwindow_function is the name of the window function we want to use (e.g.¬†see above)\n\nexpression is the name of the column that we want the window function operated on.\n\nMay not be necessary depending on what window_function is used\n\nOVER is just to signify that this is a window function\n\nPARTITION BY divides the rows into partitions so we can specify which rows to use to compute the window function\n\npartition_list is the name of the column(s) we want to partition by (i.e.¬†group_by)\n\nORDER BY is used so that we can order the rows within each partition. This is optional and does not have to be specified\n\norder_list is the name of the column(s) we want to order by\n\nROWS (optional; typically not used) used to subset the rows within each partition.\n\nframe_clause defines how much to offset from our current row\nSyntax: ROWS BETWEEN &lt;starting_row&gt; AND &lt;ending_row&gt;\n\nOptions for starting and ending row\n\nUNBOUNDED PRECEDING ‚Äî all rows before the current row in the partition, i.e.¬†the first row of the partition\n[some #] PRECEDING ‚Äî # of rows before the current row\nCURRENT ROW ‚Äî the current row\n[some #] FOLLOWING ‚Äî # of rows after the current row\nUNBOUNDED FOLLOWING ‚Äî all rows after the current row in the partition, i.e.¬†the last row of the partition\n\nExamples\n\nROWS BETWEEN 3 PRECEDING AND CURRENT ROW ‚Äî this means look back the previous 3 rows up to the current row.\nROWS BETWEEN UNBOUNDED PRECEDING AND 1 FOLLOWING ‚Äî this means look from the first row of the partition to 1 row after the current row\nROWS BETWEEN 5 PRECEDING AND 1 PRECEDING ‚Äî this means look back the previous 5 rows up to 1 row before the current row\nROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING ‚Äî this means look from the first row of the partition to the last row of the partition\n\n\n\n\nExample: Average Salary by Job Title\n\n\nTables for Examples\n\nExample: Average Unit Price for each CustomerId\n\nSELECT CustomerId,¬†\n¬† ¬† ¬† UnitPrice,¬†\n¬† ¬† ¬† AVG(UnitPrice) OVER (PARTITION BY CustomerId) AS ‚ÄúAvgUnitPrice‚Äù\nFROM [Order]¬†\nINNER JOIN OrderDetail ON [Order].Id = OrderDetail.OrderId\nExample: Average Unit Price for each group of CustomerId AND EmployeeId\n\nSELECT CustomerId,¬†\n¬† ¬† ¬† EmployeeId,¬†\n¬† ¬† ¬† AVG(UnitPrice) OVER (PARTITION BY CustomerId, EmployeeId) AS ‚ÄúAvgUnitPrice‚Äù\nFROM [Order]¬†\nINNER JOIN OrderDetail ON [Order].Id = OrderDetail.OrderId\nExample: Create a new column that ranks Unit Price in descending order for each CustomerId\n\nSELECT CustomerId,¬†\n¬† ¬† ¬† OrderDate,¬†\n¬† ¬† ¬† UnitPrice,¬†\n¬† ¬† ¬† ROW_NUMBER() OVER (PARTITION BY CustomerId ORDER BY UnitPrice DESC) AS ‚ÄúUnitRank‚Äù\nFROM [Order]¬†\nINNER JOIN OrderDetail¬†\nON [Order].Id = OrderDetail.OrderId\n\nSubstituting RANK in place of ROW_NUMBER should produce the same results\nNote that ranks are skipped (e.g.¬†rank 3 for ALFKI) when there are rows with the same rank\n\nIf you don‚Äôt want ranks skipped, use DENSE_RANK for the window function\n\n\nExample: Create a new column that provides the previous order date‚Äôs Quantity for each ProductId\n\nSELECT ProductId,¬†\n¬† ¬† ¬† OrderDate,¬†\n¬† ¬† ¬† Quantity,¬†\n¬† ¬† ¬† LAG(Quantity) OVER (PARTITION BY ProductId ORDER BY OrderDate) AS \"LAG\"\nFROM [Order]¬†\nINNER JOIN OrderDetail ON [Order].Id = OrderDetail.OrderId\n\nUse LEAD for the following quantity\n\nExample: Create a new column that provides the very first Quantity ever ordered for each ProductId\n\nSELECT ProductId,¬†\n¬† ¬† ¬† OrderDate,¬†\n¬† ¬† ¬† Quantity,¬†\n¬† ¬† ¬† FIRST_VALUE(Quantity) OVER (PARTITION BY ProductId ORDER BY OrderDate) AS \"FirstValue\"\nFROM [Order]¬†\nINNER JOIN OrderDetail ON [Order].Id = OrderDetail.OrderId\nExample: Calculate a cumulative moving average UnitPrice for each CustomerId\n\nSELECT CustomerId,¬†\n¬† ¬† ¬† UnitPrice,¬†\n¬† ¬† ¬† AVG(UnitPrice) OVER (PARTITION BY CustomerId¬†\n¬† ¬† ¬† ORDER BY CustomerId¬†\n¬† ¬† ¬† ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS ‚ÄúCumAvg‚Äù\nFROM [Order]\nINNER JOIN OrderDetail ON [Order].Id = OrderDetail.OrderId\nExample: Rank customers for each department by amount spent\nSELECT\n¬† ¬† customer_name,\n¬† ¬† customer_id,\n¬† ¬† amount_spent,\n¬† ¬† department_id,\n¬† ¬† RANK(amount_spent) OVER(ORDER BY amount_spent DESC PARTITION BY department_id) AS spend_rank\nFROM employees\nExample: Find the model and year of car that been on lot the longest\nSELECT¬†\nFIRST_VALUE(name) OVER(PARTITION BY model, year ORDER BY date_at_lot ASC) AS oldest_car_name\nmodel,\nyear\nFROM cars\nRunning Totals/Averages (Cumulative Sums)\n\nUses SUM as the window function\n\nJust replace SUM with AVG to get running averages\n\nExample\n\n\nGenerate a new dataset grouped by month, instead of timestamp. (CTE)\n\nOnly include three fields: account_id, occurred_month and total_amount_usd\nOnly computed for the following accounts: 1041 , 1051, 1061, 10141.\n\nCompute a running total ordered by occurred_month, without collapsing the rows in the result set.\n\nDisplay 2 columns: occurred_month and cum_amnt_usd_by_month\n\n\nBecause no partition was specified, the running total is applied on the full dataset and ordered by (ascending) occurred_month\n\nExample running total by grouping variable\n\nUsing previous CTE\n\nCompute a running total by account_id, ordered by occurred_month, and account_id (i.e.¬†a separate running total for each account_id.)\n\nDisplay 3 columns: account_id, occurred_month, and cum_mon_amnt_usd_by_account\n\n\n\nSame as previous example except a partition column (account_id) is added\n\nExample Running total over various window lengths\n\nUsing previous CTE\n\nCompute a 3 months rolling running total using a window that includes the current month.\nCompute a 7 months rolling running total using a window where the current month is always the middle month.\n\n\nFirst case uses 2 PRECEDING rows and the CURRENT_ROW\nSecond case uses 3 PRECEDING rows and 3 FOLLOWING rows and the CURRENT_ROW\n\nExample: Calculate the number consecutive days spent in each country (sqlite)\nwith ordered as (\n  select \n    created,\n    country,\n    lag(country) over (order by created desc)\n      as previous_country\n  from \n    raw\n),\ngrouped as (\n  select \n    country, \n    created, \n    count(*) filter (\n      where previous_country is null\n      or previous_country != country\n    ) over (\n      order by created desc\n      rows between unbounded preceding\n      and current row\n    ) as grp\n  from \n    ordered\n)\nselect\n  country,\n  date(min(created)) as start,\n  date(max(created)) as end,\n  cast(\n    julianday(date(max(created))) -\n    julianday(date(min(created))) as integer\n  ) as days\nfrom \n  grouped\ngroup by\n  country, grp\norder by\n  start desc;\n\nPost\n\nGoes over the code and thought process step-by-step with shows original data and results during intermediate steps\n\nThread\n\nEvidently only sqlite and postgres support filter. Someone in the thread suggest an alternate method.\n\nOutput:\ncountry         start         end           days\nUnited Kingdom  2023-06-08  2023-06-08  0\nUnited States   2019-09-02  2023-05-11  1347\nFrance          2019-08-25  2019-08-31  6\nMadagascar      2019-07-31  2019-08-07  7\nFrance          2019-07-25  2019-07-25  0\nUnited States   2019-05-04  2019-06-30  57\nUnited Kingdom  2018-08-29  2018-09-10  12\nUnited States   2018-08-05  2018-08-10  5",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "qmd/sql.html#sec-sql-ctes",
    "href": "qmd/sql.html#sec-sql-ctes",
    "title": "SQL",
    "section": "Common Table Expressions (CTE)",
    "text": "Common Table Expressions (CTE)\n\nThe result set of a query which exists temporarily and for use only within the context of a larger query. Much like a derived table, the result of a CTE is not stored and exists only for the duration of the query.\nAlso see\n\nWindow Functions &gt;&gt; Running Totals &gt;&gt; Examples\nGoogle, Google Analytics, Analysis &gt;&gt; Examples 12-15, 18, 19\n\nUse Cases\n\nNeeding to reference a derived table multiple times in a single query\nAn alternative to creating a view in the database\nPerforming the same calculation multiple times over across multiple query components\n\nImproves readability and usually no performance difference\n\nPrior to PostgreSQL 12, https://hakibenita.com/be-careful-with-cte-in-postgre-sql , something with the caching mechanism created a bottleneck. Currently, version 13 is the latest, so hopefully not a common problem anymore.\n\nSteps\n\nInitiate a CTE using ‚ÄúWITH‚Äù\nProvide a name for the result soon-to-be defined query\nAfter assigning a name, follow with ‚ÄúAS‚Äù\nSpecify column names (optional step)\nDefine the query to produce the desired result set\nIf multiple CTEs are required, initiate each subsequent expression with a comma and repeat steps 2-4.\nReference the above-defined CTE(s) in a subsequent query\n\nSyntax\nWITH\nexpression_name_1 AS\n(CTE query definition 1)\n[, expression_name_X AS\n¬† (CTE query definition X)\n, etc ]\nSELECT expression_A, expression_B, ...\nFROM expression_name_1\nExample\n\nComparison with a ‚Äúderived‚Äù query\n‚ÄúWhat is the average monthly cost per campaign for the company‚Äôs marketing efforts?‚Äù\nUsing CTE workflow\n-- define CTE:\nWITH Cost_by_Month AS\n(SELECT campaign_id AS campaign,\n¬† ¬† ¬† TO_CHAR(created_date, 'YYYY-MM') AS month,\n¬† ¬† ¬† SUM(cost) AS monthly_cost\nFROM marketing\nWHERE created_date BETWEEN NOW() - INTERVAL '3 MONTH' AND NOW()\nGROUP BY 1, 2\nORDER BY 1, 2)\n\n-- use CTE in subsequent query:\nSELECT campaign, avg(monthly_cost) as \"Avg Monthly Cost\"\nFROM Cost_by_Month\nGROUP BY campaign\nORDER BY campaign\nUsed derived query\n-- Derived\nSELECT campaign, avg(monthly_cost) as \"Avg Monthly Cost\"\nFROM\n¬† ¬† -- this is where the derived query is used\n¬† ¬† (SELECT campaign_id AS campaign,\n¬† ¬† ¬† TO_CHAR(created_date, 'YYYY-MM') AS month,\n¬† ¬† ¬† SUM(cost) AS monthly_cost\n¬† ¬† FROM marketing\n¬† ¬† WHERE created_date BETWEEN NOW() - INTERVAL '3 MONTH' AND NOW()\n¬† ¬† GROUP BY 1, 2\n¬† ¬† ORDER BY 1, 2) as Cost_By_Month\nGROUP BY campaign\nORDER BY campaign\n\nExample\n\nCount the number of interactions of new users\nSteps\n\nGet new users\nCount interactions\nGet interactions of new users\n\n\nWITH new_users AS (\n¬† ¬† SELECT id\n¬† ¬† FROM users\n¬† ¬† WHERE created &gt;= '2021-01-01'\n),\ncount_interactions AS (\n¬† ¬† SELECT id,\n¬† ¬† ¬† ¬† COUNT(*) n_interactions\n¬† ¬† FROM interactions\n¬† ¬† GROUP BY id\n),\ninteractions_by_new_users AS (\n¬† ¬† SELECT id,\n¬† ¬† ¬† ¬† n_interactions\n¬† ¬† FROM new_users\n¬† ¬† ¬† ¬† LEFT JOIN count_interactions USING (id)\n)\n\nSELECT *\nFROM interactions_by_new_users\nExample\n\nFind the average top Math test score for students in California\nSteps\n\nGet a subset of students (California)\nGet a subset of test scores (Math)\nJoin them together to get all Math test scores from California students\nGet the top score per student\nTake the overall average\n\nDerived Query (i.e.¬†w/o CTE)\nSELECT AVG(score)\nFROM¬†\n¬† (SELECT students.id, MAX(test_results.score) as score\n¬† FROM students¬†\n¬† JOIN schools ON (\n¬† ¬† students.school_id = schools.id AND schools.state = 'CA'\n¬† )\n¬† JOIN test_results ON (\n¬† ¬† students.id = test_results.student_id\n¬† ¬† AND test_results.subject = 'math'\n¬† )\n¬† GROUP BY students.id) as tmp\nUsing CTE\nWITH\n¬† student_subset as (\n¬† ¬† SELECT students.id¬†\n¬† ¬† FROM students¬†\n¬† ¬† JOIN schools ON (\n¬† ¬† ¬† students.school_id = schools.id AND schools.state = 'CA'\n¬† ¬† )\n¬† ),\n¬† score_subset as (\n¬† ¬† SELECT student_id, score¬†\n¬† ¬† FROM test_results¬†\n¬† ¬† WHERE subject = 'math'\n¬† ),\n¬† student_scores as (\n¬† ¬† SELECT student_subset.id, score_subset.score\n¬† ¬† FROM student_subset¬†\n¬† ¬† JOIN score_subset ON (\n¬† ¬† ¬† ¬† student_subset.id = score_subset.student_id\n¬† ¬† )\n¬† ),\n¬† top_score_per_student as (\n¬† ¬† SELECT id, MAX(score) as score¬†\n¬† ¬† FROM student_scores¬†\n¬† ¬† GROUP BY id\n¬† )\n\nSELECT AVG(score)¬†\nFROM top_score_per_student",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "qmd/sql.html#sec-sql-str",
    "href": "qmd/sql.html#sec-sql-str",
    "title": "SQL",
    "section": "Strings",
    "text": "Strings\n\nConcatenate\n\nAlso see Processing Expressions &gt;&gt; NULLs\n‚Äú||‚Äù\nSELECT 'PostgreSQL' || ' ' || 'Databases' AS result;\n\n¬† ¬† result\n--------------\nPostgreSQL Databases\nCONCAT\nSELECT CONCAT('PostgreSQL', ' ', 'Databases') AS result;\n\n¬† ¬† result\n--------------\nPostgreSQL Databases\nWith NULL values\nSELECT CONCAT('Harry', NULL, 'Peter');\n\n--------------\nHarryPeter\n\n‚Äú||‚Äù won‚Äôt work with NULLs\n\nColumns\nSELECT first_name, last_name,¬†\nCONCAT(first_name,' ' , last_name) \"Full Name\"¬†\nFROM candidates;\n\nNew column, ‚ÄúFull Name‚Äù, is created with concatenated columns\n\n\nSplitting (BQ)\nSELECT\n*,\nCASE WHEN ARRAY_LENGTH(SPLIT(page_location, '/')) &gt;= 5¬†\n¬† ¬† ¬† ¬† ¬† AND\n¬† ¬† ¬† ¬† ¬† CONTAINS_SUBSTR(ARRAY_REVERSE(SPLIT(page_location, '/'))[SAFE_OFFSET(0)], '+')\n¬† ¬† ¬† ¬† ¬† AND (LOWER(SPLIT(page_location, '/')[SAFE_OFFSET(4)]) IN¬†\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ('accessories','apparel','brands','campus+collection','drinkware',\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† 'electronics','google+redesign',\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† 'lifestyle','nest','new+2015+logo','notebooks+journals',\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† 'office','shop+by+brand','small+goods','stationery','wearables'\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† )\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† OR\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† LOWER(SPLIT(page_location, '/')[SAFE_OFFSET(3)]) IN¬†\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ('accessories','apparel','brands','campus+collection','drinkware',\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† 'electronics','google+redesign',\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† 'lifestyle','nest','new+2015+logo','notebooks+journals',\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† 'office','shop+by+brand','small+goods','stationery','wearables'\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† )\n¬† ¬† ¬† ¬† ¬† )\n¬† ¬† ¬† ¬† ¬† THEN 'PDP'\n¬† ¬† ¬† ¬† ¬† WHEN NOT(CONTAINS_SUBSTR(ARRAY_REVERSE(SPLIT(page_location, '/'))[SAFE_OFFSET(0)], '+'))\n¬† ¬† ¬† ¬† ¬† AND (LOWER(SPLIT(page_location, '/')[SAFE_OFFSET(4)]) IN¬†\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬†   ('accessories','apparel','brands','campus+collection','drinkware',\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† 'electronics','google+redesign',\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† 'lifestyle','nest','new+2015+logo','notebooks+journals',\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† 'office','shop+by+brand','small+goods','stationery','wearables'\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† )\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† OR¬†\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† LOWER(SPLIT(page_location, '/')[SAFE_OFFSET(3)]) IN¬†\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ('accessories','apparel','brands','campus+collection','drinkware',\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† 'electronics','google+redesign',\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† 'lifestyle','nest','new+2015+logo','notebooks+journals',\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† 'office','shop+by+brand','small+goods','stationery','wearables'\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† )\n¬† ¬† ¬† ¬† ¬† )\n¬† ¬† ¬† ¬† ¬† THEN 'PLP'\n¬† ¬† ¬† ELSE page_title\n¬† ¬† ¬† END AS page_title_adjusted¬†\nFROM¬†\n¬† unnested_events\n\nFrom article, gist\nQuery is creating a new categorical column, ‚Äúpage_title_adjusted,‚Äù that is ‚ÄúPDP‚Äù when a substring in ‚Äúpage_location‚Äù is one of a set of words, and ‚ÄúPLP‚Äù when it‚Äôs not, and the value of page_title otherwise.\nSPLIT splits the string by separator, ‚Äò/‚Äô\nCONTAINS_SUBSTR is looking for substring with a ‚Äú+‚Äù\n[SAFE_OFFSET(3)] pulls the 4th substring (think this indexes by 0?)\nAfter it‚Äôs been reversed via ARRAY_REVERSE (?)\nELSE says use the value for page_title when length of the substrings after splitting page_location is 5 or less\n‚Äúunnested_events‚Äù is a CTE",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "qmd/sql.html#sec-sql-arr",
    "href": "qmd/sql.html#sec-sql-arr",
    "title": "SQL",
    "section": "Arrays",
    "text": "Arrays\n\nMisc\n\nPostGres\n\nIndexing Arrays starts at 1, not at 0\n\n\nCreate Array (BQ)\nSELECT ARRAY\n¬† (SELECT 1 UNION ALL\n¬† SELECT 2 UNION ALL\n¬† SELECT 3) AS new_array;\n+-----------+\n| new_array |\n+-----------+\n| [1, 2, 3] |\n+-----------+\n\nSELECT\n¬† ARRAY\n¬† ¬† (SELECT AS STRUCT 1, 2, 3\n¬† ¬† UNION ALL SELECT AS STRUCT 4, 5, 6) AS new_array;\n+------------------------+\n| new_array¬† ¬† ¬† ¬† ¬† ¬† ¬† |\n+------------------------+\n| [{1, 2, 3}, {4, 5, 6}] |\n+------------------------+\n\nSELECT ARRAY\n¬† (SELECT AS STRUCT [1, 2, 3] UNION ALL\n¬† SELECT AS STRUCT [4, 5, 6]) AS new_array;\n+----------------------------+\n| new_array¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† |\n+----------------------------+\n| [{[1, 2, 3]}, {[4, 5, 6]}] |\n+----------------------------+\nCreate a table with Arrays (Postgres)\n\nCREATE TEMP TABLE shopping_cart (\n¬† cart_id serial PRIMARY KEY,\n¬† products text ARRAY\n¬† );\nINSERT INTO\n¬† shopping_cart(products)\nVALUES\n¬† (ARRAY['product_a', 'product_b']),\n¬† (ARRAY['product_c', 'product_d']),\n¬† (ARRAY['product_a', 'product_b', 'product_c']),\n¬† (ARRAY['product_a', 'product_b', 'product_d']),\n¬† (ARRAY['product_b', 'product_d']);\n\n-- alt syntax w/o ARRAY\nINSERT INTO\n¬† shopping_cart(products)\nVALUES\n¬† ('{\"product_a\", \"product_d\"}');\n\nAlso see Basics &gt;&gt; Add Data &gt;&gt; Example: chatGPT\n\nSubset an array (postgres)\n\nSELECT\n¬† cart_id,\n¬† products[1] AS first_product -- indexing starts at 1\nFROM\n¬† shopping_cart;\nSlice an array (postgres)\n\nSELECT\n¬† cart_id,\n¬† products [1:2] AS first_two_products\nFROM\n¬† shopping_cart\nWHERE\n¬† CARDINALITY(products) &gt; 2;\nUnnest an array (postgres)\n\nSELECT\n¬† cart_id,\n¬† UNNEST(products) AS products\nFROM\n¬† shopping_cart\nWHERE\n¬† cart_id IN (3, 4);\n\nUseful if you want to perform a join\n\nFilter according to items in arrays (postgres)\nSELECT\n¬† cart_id,\n¬† products\nFROM\n¬† shopping_cart\nWHERE\n¬† 'product_c' = ANY (products);\n\nOnly rows with arrays that have ‚Äúproduct_c‚Äù will be returned\n\nChange array values using UPDATE, SET\n-- update arrays¬†\nUPDATE\n¬† shopping_cart\nSET\n¬† products = ARRAY['product_a','product_b','product_e']\nWHERE\n¬† cart_id = 1;\n\nUPDATE¬†\n¬† shopping_cart\nSET\n¬† products[1] = 'product_f'\nWHERE\n¬† cart_id = 2;\nSELECT\n¬† *\nFROM\n¬† shopping_cart\nORDER BY cart_id;\n\nFirst update: all arrays where cart_id == 1 are set to [‚Äòproduct_a‚Äô,‚Äòproduct_b‚Äô,‚Äòproduct_e‚Äô]\nSecond update: all array first values where cart_id == 2 are set to ‚Äòproduct_f‚Äô\n\nInsert array values\n\nARRAY_APPEND - puts value at the end of the array\nUPDATE\n¬† shopping_cart\nSET\n¬† products = ARRAY_APPEND(products, 'product_x')\nWHERE\n¬† cart_id = 1;\n\narrays in product column where cart_id == 1 get ‚Äúproduct_x‚Äù appended to the end of their arrays\n\nARRAY_PREPEND - puts value at the beginning of the array\nUPDATE¬†\n¬† shopping_cart\nSET\n¬† products = ARRAY_PREPEND('product_x', products)\nWHERE\n¬† cart_id = 2;\n\narrays in product column where cart_id == 2 get ‚Äúproduct_x‚Äù prepended to the beginning of their arrays\n\n\nARRAY_REMOVE - remove array item\nUPDATE\n¬† shopping_cart\nSET\n¬† products = array_remove(products, 'product_e')\nWHERE cart_id = 1;\n\narrays in product column where cart_id == 1 get ‚Äúproduct_e‚Äù removed from their arrays\n\nARRAY_CONCAT(BQ), ARRAY_CAT(postgres) - Concantenate\nSELECT ARRAY_CONCAT([1, 2], [3, 4], [5, 6]) as count_to_six;\n+--------------------------------------------------+\n| count_to_six¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† |\n+--------------------------------------------------+\n| [1, 2, 3, 4, 5, 6]¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† |\n+--------------------------------------------------+\n\n-- postgres\nSELECT\n¬† cart_id,\n¬† ARRAY_CAT(products, ARRAY['promo_product_1', 'promo_product_2'])\nFROM shopping_cart\nORDER BY cart_id;\nARRAY_TO_STRING - Coerce to string (BQ)\nWITH items AS\n¬† (SELECT ['coffee', 'tea', 'milk' ] as list\n¬† UNION ALL\n¬† SELECT ['cake', 'pie', NULL] as list)\nSELECT ARRAY_TO_STRING(list, '--') AS text\nFROM items;\n+--------------------------------+\n| text¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† |\n+--------------------------------+\n| coffee--tea--milk¬† ¬† ¬† ¬† ¬† ¬† ¬† |\n| cake--pie¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† |\n+--------------------------------+\n\nWITH items AS\n¬† (SELECT ['coffee', 'tea', 'milk' ] as list\n¬† UNION ALL\n¬† SELECT ['cake', 'pie', NULL] as list)\nSELECT ARRAY_TO_STRING(list, '--', 'MISSING') AS text\nFROM items;\n+--------------------------------+\n| text¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† |\n+--------------------------------+\n| coffee--tea--milk¬† ¬† ¬† ¬† ¬† ¬† ¬† |\n| cake--pie--MISSING¬† ¬† ¬† ¬† ¬† ¬† |\n+--------------------------------+\nARRAY_AGG - gather values of a group by variable into an array (doc)\n\nMakes the output more readable\nExample: Get categories for each brand\n-- without array_agg\nselect\n¬† ¬† brand,\n¬† ¬† category\nfrom order_item\ngroup by brand, category\norder by brand, category\n;\nResults:\n| brand¬† | category¬† |¬†\n| ------ | ---------- |¬†\n| Arket¬† | jacket¬† ¬† |\n| COS¬† ¬† | shirts¬† ¬† |\n| COS¬† ¬† | trousers¬† |¬†\n| COS¬† ¬† | vest¬† ¬† ¬† |\n| Levi's | jacket¬† ¬† |\n| Levi's | jeans¬† ¬† ¬† |\n\n-- with array_agg\nselect\n¬† brand,\n¬† array_agg(distinct category) as all_categories\nfrom order_item\ngroup by brand\norder by brand\n;\nResults:\n| brand¬† | all_categories¬† ¬† ¬† ¬† ¬† ¬† ¬† |¬†\n| ------ | ---------------------------- |¬†\n| Arket¬† | ['jacket']¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† |\n| COS¬† ¬† | ['shirts','trousers','vest'] |\n| Levi's | ['jacket','jeans']¬† ¬† ¬† ¬† ¬† |\n| Uniqlo | ['shirts','t-shirts','vest'] |\n\nARRAY_SIZE - function takes an array or a variant as input and returns the number of items within the array/variant (doc)\n\nExample: How many categories does each brand have?\nselect\n¬† brand,\n¬† array_agg(distinct category) as all_categories,\n¬† array_size(all_categories) as no_of_cat\nfrom order_item\ngroup by brand\norder by brand\n;\nResults:\n| brand¬† | all_categories¬† ¬† ¬† ¬† ¬† ¬† ¬† | no_of_cat |\n| ------ | ---------------------------¬† | --------- |\n| Arket¬† | ['jacket']¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† | 1¬† ¬† ¬† ¬† |\n| COS¬† ¬† | ['shirts','trousers','vest'] | 3¬† ¬† ¬† ¬† |\n| Levi's | ['jacket','jeans']¬† ¬† ¬† ¬† ¬† | 2¬† ¬† ¬† ¬† |\n| Uniqlo | ['shirts','t-shirts','vest'] | 3¬† ¬† ¬† ¬† |\n\n-- postgres using CARDINALITY to get array_size\nSELECT\n¬† cart_id,\n¬† CARDINALITY(products) AS num_products\nFROM\n¬† shopping_cart;\n\nARRAY_CONTAINS checks if a variant is included in an array and returns a boolean value. (doc)\n\nVariant is just a specific category\nNeed to cast the item you‚Äôd like to check as a variant first\nSyntax: ARRAY_CONTAINS(variant, array)\nExample: What brands have jackets?\nselect\n¬† brand,\n¬† array_agg(distinct category) as all_categories,\n¬† array_size(all_categories) as no_of_cat,\n¬† array_contains('jacket'::variant,all_categories) as has_jacket\nfrom order_item\ngroup by brand\norder by brand\n;\nResults:\n| brand¬† | all_categories¬† ¬† ¬† ¬† ¬† ¬† ¬† | no_of_cat | has_jacket |\n| ------ | ---------------------------¬† | --------- | ---------- |\n| Arket¬† | ['jacket']¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† | 1¬† ¬† ¬† ¬† | true¬† ¬† ¬† |\n| COS¬† ¬† | ['shirts','trousers','vest'] | 3¬† ¬† ¬† ¬† | false¬† ¬† ¬† |\n| Levi's | ['jacket','jeans']¬† ¬† ¬† ¬† ¬† | 2¬† ¬† ¬† ¬† | true¬† ¬† ¬† |\n| Uniqlo | ['shirts','t-shirts','vest'] | 3¬† ¬† ¬† ¬† | false¬† ¬† ¬† |\n\n-- postgres contains_operator, @&gt;\nSELECT\n¬† cart_id,\n¬† products\nFROM\n¬† shopping_cart\nWHERE\n¬† products¬† @&gt; ARRAY['product_a', 'product_b'];\n\n‚Äú@&gt;‚Äù example returns all rows with arrays containing product_a and product_b",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "qmd/sql.html#sec-sql-bizq",
    "href": "qmd/sql.html#sec-sql-bizq",
    "title": "SQL",
    "section": "Business Queries",
    "text": "Business Queries\n\nSimple Moving Average (SMA)\n\nExample: 7-day SMA including today\nSELECT\n¬† Date, Conversions,\n¬† AVG(Conversions) OVER (ORDER BY Date ROWS BETWEEN 6 PRECEDING AND\n¬† CURRENT ROW) as SMA\nFROM daily_sales\n\nExample: 3-day SMA not including today\nselect\n¬† date,\n¬† sales,\n¬† avg(sales) over (order by date\n¬† ¬† ¬† ¬† rows between 3 preceding and current row - 1) as moving_avg\nfrom table_daily_sales\nExample: Rank product categories by shipping cost for each shipping address\n\nSELECT Product_Category,\n¬† Shipping_Address,\n¬† Shipping_Cost,\n¬† ROW_NUMBER() OVER\n¬† ¬† ¬† ¬† ¬† ¬† ¬† (PARTITION BY Product_Category,\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† Shipping_Address\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ORDER BY Shipping_Cost DESC) as RowNumber,\n¬† RANK() OVER¬†\n¬† ¬† ¬† ¬† (PARTITION BY Product_Category,\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† Shipping_Address\n¬† ¬† ¬† ¬† ORDER BY Shipping_Cost DESC) as RankValues,\n¬† DENSE_RANK() OVER¬†\n¬† ¬† ¬† ¬† ¬† ¬† ¬† (PARTITION BY Product_Category,\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† Shipping_Address¬†\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ORDER BY Shipping_Cost DESC) as DenseRankValues\nFROM Dummy_Sales_Data_v1\nWHERE Product_Category IS NOT NULL\nAND Shipping_Address IN ('Germany','India')\nAND Status IN ('Delivered')\n\nRANK() retrieves ranked rows based on the condition of ORDER BY clause. As you can see there is a tie between 1st two rows i.e.¬†first two rows have same value in Shipping_Cost column (which is mentioned in ORDER BY clause).\nDENSE_RANK is similar to the RANK , but it does not skip any numbers even if there is a tie between the rows. This you can see in Blue box in the above picture.\nRank resets to 1 when ‚ÄúShipping_Address‚Äù changes location\n\nExample: Total order quantity for each month\nSELECT strftime('%m', OrderDate) as Month,\n¬† ¬† ¬† SUM(Quantity) as Total_Quantity\nfrom Dummy_Sales_Data_v1\nGROUP BY strftime('%m', OrderDate)\n\nstrftime extracts the month (%m) from the datetime column, ‚ÄúOrderDate‚Äù\n\nExample: Daily counts of open jobs\n\nThe issue is that there aren‚Äôt rows for transactions that remain in a type of holding status\n\ne.g.¬†Job Postings website has date columns for the date the job posting was created, the date the job posting went live on the website, and the date the job posting was taken down (action based timestamps), but no dates for the status between ‚Äúwent live‚Äù and ‚Äútaken down‚Äù.\n\n\n-- create a calendar column\nSELECT parse_datetime('2020‚Äì01‚Äì01 08:00:00', 'yyyy-MM-dd H:m:s') + (interval '1' day * d) as cal_date from¬†\nFROM ( SELECT\nROW_NUMBER() OVER () -1 as d\nFROM\n(SELECT 0 as n UNION SELECT 1) p0,\n(SELECT 0 as n UNION SELECT 1) p1,\n(SELECT 0 as n UNION SELECT 1) p2,\n(SELECT 0 as n UNION SELECT 1) p3,\n(SELECT 0 as n UNION SELECT 1) p4,\n(SELECT 0 as n UNION SELECT 1) p5,\n(SELECT 0 as n UNION SELECT 1) p6,\n(SELECT 0 as n UNION SELECT 1) p7,\n(SELECT 0 as n UNION SELECT 1) p8,\n(SELECT 0 as n UNION SELECT 1) p9,\n(SELECT 0 as n UNION SELECT 1) p10\n)\n\n-- left-join your table to the calendar column\nSelect\n¬† ¬† c.cal_date,\n¬† ¬† count(distinct opp_id) as \"historical_prospects\"\nFrom calendar c\nLeft Join\n¬† ¬† opportunities o\n¬† ¬† on\n¬† ¬† ¬† ¬† o.stage_entered ‚â§ c.cal_date¬†\n¬† ¬† ¬† ¬† and (o.stage_exited is null or o.stage_exited &gt; c.cal_date)\n\nCalendar column should probably be a CTE\nNotes from Using SQL to calculate trends based on historical status\nSome flavours of SQL have a generate_series function, which will create this calendar column for you\nFor one particular month, then create an indicator column with ‚Äúif posting_publish_date ‚â§ 2022‚Äì01‚Äì01 and (posting_closed_date is null or posting_closed_date &gt; 2022‚Äì01‚Äì31) then True‚Äù and then filter for True and count.\n\nExample: Get the latest order from each customer\n-- Using QUALIFY\nselect\n¬† ¬† date,\n¬† ¬† customer_id,\n¬† ¬† order_id,\n¬† ¬† price\nfrom customer_order_table\nqualify row_number() over (partition by customer_id order by date desc) = 1\n;\n\n-- CTE w/window function\nwith order_order as\n(\nselect\n¬† ¬† date,\n¬† ¬† customer_id,\n¬† ¬† order_id,\n¬† ¬† price,\n¬† ¬† row_number() over (partition by customer_id order by date desc)¬† ¬†\n¬† ¬† as order_of_orders\nfrom customer_order_table¬†\n)\n\nselect\n¬† ¬† *\nfrom order_order\nwhere order_of_orders = 1\n;\nResults:\n| date¬† ¬† ¬† | customer_id | order_id | price |\n|------------|-------------|----------|-------|\n| 2022-01-03 | 002¬† ¬† ¬† ¬† | 212¬† ¬† ¬† | 350¬† |\n| 2022-01-06 | 005¬† ¬† ¬† ¬† | 982¬† ¬† ¬† | 300¬† |\n| 2022-01-07 | 001¬† ¬† ¬† ¬† | 109¬† ¬† ¬† | 120¬† |\nMedians\n\nNotes from How to Calculate Medians with Grouping in MySQL\n\nVariables:\n\npid: unique id variable\ncategory: A or B\nprice: random value between 1 and 6\n\n\nExample: Overall median price\nSELECT AVG(sub.price) AS median\nFROM ( \n    SELECT @row_index := @row_index + 1 AS row_index, p.price\n    FROM products.prices p, (SELECT @row_index := -1) r\n    WHERE p.category = 'A'\n    ORDER BY p.price \n) AS sub\nWHERE sub.row_index IN (FLOOR(@row_index / 2), CEIL(@row_index / 2))\n;\n\nmedian|\n------+\n   3.0|\n\n@row_index is a SQL variable that is initiated in the FROM statement and updated for each row in the SELECT statement.\nThe column whose median will be calculated (the price column in this example) should be sorted. It doesn‚Äôt matter if it‚Äôs sorted in ascending or descending order.\nAccording to the definition of median, the median is the value of the middle element (total count is odd) or the average value of the two middle elements (total count is even). In this example, category A has 5 rows and thus the median is the value of the third row after sorting. The values of both FLOOR(@row_index / 2) and CEIL(@row_index / 2) are 2 which is the third row. On the other hand, for category B which has 6 rows, the median is the average value of the third and fourth rows.\n\nExample: Median price for each product\nSELECT\n    sub2.category,\n    CASE WHEN MOD(sub2.total, 2) = 1 THEN sub2.mid_prices\n         WHEN MOD(sub2.total, 2) = 0 THEN (SUBSTRING_INDEX(sub2.mid_prices, ',', 1) + SUBSTRING_INDEX(sub2.mid_prices, ',', -1)) / 2\n    END AS median    \nFROM \n    (\n        SELECT \n            sub1.category,\n            sub1.total,\n            CASE WHEN MOD(sub1.total, 2) = 1 THEN SUBSTRING_INDEX(SUBSTRING_INDEX(sub1.prices, ',', CEIL(sub1.total/2)), ',', '-1')\n                 WHEN MOD(sub1.total, 2) = 0 THEN SUBSTRING_INDEX(SUBSTRING_INDEX(sub1.prices, ',', sub1.total/2 + 1), ',', '-2')\n            END AS mid_prices\n        FROM \n            (\n                SELECT\n                    p.category,\n                    GROUP_CONCAT(p.price ORDER BY p.price) AS prices,\n                    COUNT(*) AS total\n                FROM products.prices p\n                GROUP BY p.category\n            ) sub1\n    ) sub2\n;\n\ncategory|median|\n--------+------+\nA       |3     |\nB       |3.5   |\n\nBreaking down the subqueries\n\nSort prices per category\nSELECT\n    category,\n    GROUP_CONCAT(price ORDER BY p.price) AS prices,\n    COUNT(*) AS total\nFROM products.prices p\nGROUP BY p.category\n;\n\ncategory|prices     |total|\n--------+-----------+-----+\nA       |1,2,3,4,5  |    5|\nB       |1,2,3,4,5,6|    6|\n\nIf your table has a lot of data, GROUP_CONCAT would not contain all the data. In this case, you increase the limit for GROUP_CONCAT by: SET GROUP_CONCAT_MAX_LEN = 100000;\n\nGet middle prices according to whether the total count is an odd or even number\nSELECT \n    sub1.category,\n    sub1.total,\n    CASE WHEN MOD(sub1.total, 2) = 1 THEN SUBSTRING_INDEX(SUBSTRING_INDEX(sub1.prices, ',', CEIL(sub1.total/2)), ',', '-1')\n         WHEN MOD(sub1.total, 2) = 0 THEN SUBSTRING_INDEX(SUBSTRING_INDEX(sub1.prices, ',', sub1.total/2 + 1), ',', '-2')\n    END AS mid_prices\nFROM \n    (\n        SELECT\n            p.category,\n            GROUP_CONCAT(p.price ORDER BY p.price) AS prices,\n            COUNT(*) AS total\n        FROM products.prices p\n        GROUP BY p.category\n    ) sub1\n;\n\ncategory|total|mid_prices|\n--------+-----+----------+\nA       |    5|3         |\nB       |    6|3,4       |\n\nWe use the¬†MOD¬†function (modulo) to check if the total count is an odd or even number.\nThe¬†SUBSTRING_INDEX¬†function is used twice to extract the middle elements.\n\n\n\nExample: Overall median of price and quantity\nSELECT\n    CASE WHEN MOD(sub2.total, 2) = 1 THEN sub2.mid_prices\n         WHEN MOD(sub2.total, 2) = 0 THEN (SUBSTRING_INDEX(sub2.mid_prices, ',', 1) + SUBSTRING_INDEX(sub2.mid_prices, ',', -1)) / 2\n    END AS median_of_price,\n    CASE WHEN MOD(sub2.total, 2) = 1 THEN sub2.mid_quantities\n         WHEN MOD(sub2.total, 2) = 0 THEN (SUBSTRING_INDEX(sub2.mid_quantities, ',', 1) + SUBSTRING_INDEX(sub2.mid_prices, ',', -1)) / 2\n    END AS median_of_quantity\nFROM \n    (\n        SELECT \n            sub1.total,\n            CASE WHEN MOD(sub1.total, 2) = 1 THEN SUBSTRING_INDEX(SUBSTRING_INDEX(sub1.prices, ',', CEIL(sub1.total/2)), ',', '-1')\n                 WHEN MOD(sub1.total, 2) = 0 THEN SUBSTRING_INDEX(SUBSTRING_INDEX(sub1.prices, ',', sub1.total/2 + 1), ',', '-2')\n            END AS mid_prices,\n            CASE WHEN MOD(sub1.total, 2) = 1 THEN SUBSTRING_INDEX(SUBSTRING_INDEX(sub1.quantities, ',', CEIL(sub1.total/2)), ',', '-1')\n                 WHEN MOD(sub1.total, 2) = 0 THEN SUBSTRING_INDEX(SUBSTRING_INDEX(sub1.quantities, ',', sub1.total/2 + 1), ',', '-2')                 \n            END AS mid_quantities\n        FROM \n            (\n                SELECT\n                    COUNT(*) AS total,\n                    GROUP_CONCAT(o.price ORDER BY o.price) AS prices,\n                    GROUP_CONCAT(o.quantity ORDER BY o.quantity) AS quantities\n                FROM products.orders o\n            ) sub1\n    ) sub2\n;\n\n\nmedian_of_price|median_of_quantity|\n---------------+------------------+\n3              |30                |\n\nSimilar to previous example\nVariables: order_id, price, quantity",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "qmd/sql.html#sec-sql-trans",
    "href": "qmd/sql.html#sec-sql-trans",
    "title": "SQL",
    "section": "Transactions",
    "text": "Transactions\n\nMisc\n\nAlso see\n\nTerms &gt;&gt; Transaction\nDatabase, Warehouses &gt;&gt; Database Triggers - Shows how to efficiently transfer data from a transactional database to a warehouse/relational database by setting up event triggers and staging tables.\n\nWhen the transaction is successful, COMMIT is applied. When the transaction is aborted, incorrect execution, system failure ROLLBACK occurs.\n\nOnly used with INSERT, UPDATE and DELETE\nBEGIN TRANSACTION: It indicates the start point of an explicit or local transaction.\n\nRepresents a point ast which the data referenced by a connection is logically and physically consistent.\nIf errors are encountered, all data modifications made after the BEGIN TRANSACTION can be rolled back to return the data to this known state of consistency\nSyntax: BEGIN TRANSACTION transaction_name ;\n\nSET TRANSACTION: Places a name on a transaction.\n\nSyntax: SET TRANSACTION [ READ WRITE | READ ONLY ];\n\nCOMMIT: used to permanently save the changes done in the transaction in tables/databases. The database cannot regain its previous state after its execution of commit.\n\nIf everything is in order with all statements within a single transaction, all changes are recorded together in the database is called committed. The COMMIT command saves all the transactions to the database since the last COMMIT or ROLLBACK command\nExample: Delete records\nDELETE FROM Student WHERE AGE = 20;\nCOMMIT;\n\nDeletes those records from the table which have age = 20 and then commits the changes in the database.\n\n\nROLLBACK: used to undo the transactions that have not been saved in the database. The command is only been used to undo changes since the last commit\n\nIf any error occurs with any of the SQL grouped statements, all changes need to be aborted. The process of reversing changes is called rollback. This command can only be used to undo transactions since the last COMMIT or ROLLBACK command was issued.\nSyntax: ROLLBACK;\n\nSAVEPOINT: creates points within the groups of transactions in which to ROLLBACK.\n\nSyntax: SAVEPOINT &lt;savepoint_name&gt;;\nA savepoint is a point in a transaction in which you can roll the transaction back to a certain point without rolling back the entire transaction.\nRemove a savepoint: RELEASE SAVEPOINT &lt;savepoint_name&gt;\nExample: Rollback a deletion\nSAVEPOINT SP1;\n//Savepoint created.\nDELETE FROM Student WHERE AGE = 20;\n//deleted\nSAVEPOINT SP2;\n//Savepoint created.\nROLLBACK TO SP1;\n//Rollback completed.",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "qmd/sql.html#sec-sql-procexp",
    "href": "qmd/sql.html#sec-sql-procexp",
    "title": "SQL",
    "section": "Processing Expressions",
    "text": "Processing Expressions\n\nUse multiple conditions in a WHERE expression\nselect\n¬† ¬† *\nfrom XXX_table\nwhere 1=1\n¬† ¬† (if condition A) and clause 1¬†\n¬† ¬† (if condition B) and clause 2¬†\n¬† ¬† (if condition C) and clause 3\n;\n\nThe ‚Äú1=1‚Äù prevents errors that would occur when the first condition doesn‚Äôt apply to any rows.\n\nCan also use ‚Äútrue‚Äù\n\n\nSelect unique rows without using DISTINCT\n\nUsing UNION\nSELECT employee_id,\n¬† ¬† ¬† employee_name,\n¬† ¬† ¬† department\nFROM Dummy_employees\nUNION\nSELECT employee_id,\n¬† ¬† ¬† employee_name,\n¬† ¬† ¬† department\nFROM Dummy_employees\n\nthere must be same number and order of columns in both the SELECT statements\n\nUsing INTERSECT\nSELECT employee_id,\n¬† ¬† ¬† employee_name,\n¬† ¬† ¬† department\nFROM Dummy_employees\nINTERSECT\nSELECT employee_id,\n¬† ¬† ¬† employee_name,\n¬† ¬† ¬† department\nFROM Dummy_employees\n\nThere must be same number and order of columns in both the SELECT statements\n\nUsing ROW_NUMBER\nWITH temporary_employees as (\n  SELECT\n  ¬† employee_id,\n  ¬† employee_name,\n  ¬† department,\n  ¬† ROW_NUMBER() OVER(PARTITION BY employee_name,\n  ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† department,\n  ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† employee_id) as row_count\n  FROM Dummy_employees\n)\n\nSELECT *\nFROM temporary_employees\nWHERE row_count = 1\nUsing GROUP BY\nSELECT employee_id,\n¬† ¬† ¬† employee_name,\n¬† ¬† ¬† department\nFROM Dummy_employees\nGROUP BY employee_id,\n¬† ¬† ¬† ¬† employee_name,\n¬† ¬† ¬† ¬† department\n\nJust need to group by all the columns. Useful to use in conjunction with aggregate functions.\n\n\nCASE WHEN\nSELECT OrderID,\n¬† ¬† ¬† OrderDate,\n¬† ¬† ¬† Sales_Manager,\n¬† ¬† ¬† Quantity,\n¬† ¬† ¬† CASE WHEN Quantity &gt; 51 THEN 'High'\n¬† ¬† ¬† ¬† ¬† ¬† WHEN Quantity &lt; 51 THEN 'Low'\n¬† ¬† ¬† ¬† ¬† ¬† ELSE 'Medium'¬†\n¬† ¬† ¬† END AS OrderVolume\nFROM Dummy_Sales_Data_v1\n\nEND AS specifies the name of the new column, ‚ÄúOrderVolume‚Äù\nELSE specifies the value when none of the conditions are met\n\nIf you did not mention ELSE clause and no condition is satisfied, the query will return NULL for that specific record\n\n\nPivot Wider\n\nSELECT Sales_Manager,\n¬† ¬† ¬† COUNT(CASE WHEN Shipping_Address = 'Singapore' THEN OrderID\n¬† ¬† ¬† ¬† ¬† ¬† END) AS Singapore_Orders,\n\n¬† ¬† ¬† COUNT(CASE WHEN Shipping_Address = 'UK' THEN OrderID\n¬† ¬† ¬† ¬† ¬† ¬† END) AS UK_Orders,\n\n¬† ¬† ¬† COUNT(CASE WHEN Shipping_Address = 'Kenya' THEN OrderID\n¬† ¬† ¬† ¬† ¬† ¬† END) AS Kenya_Orders,\n\n¬† ¬† ¬† COUNT(CASE WHEN Shipping_Address = 'India' THEN OrderID\n¬† ¬† ¬† ¬† ¬† ¬† END) AS India_Orders\nFROM Dummy_Sales_Data_v1\nGROUP BY Sales_Manager\n\nDepending on your use-case you can also use different aggregation such as SUM, AVG, MAX, MIN with CASE statement.\n\n\n\nNULLs\n\nDivision and NULLS\n\nAny division with NULL values with have a result of NULL.\nisNull allows to get a different resulting value\nSELECT IsNull(&lt;column&gt;, 0) / 45\n\nAll NULL values in the column will replaced with 0s during the division operation.\n\n\nCOALESCE\n\nSubstitute a default value in place of NULLs\nSELECT COALESCE(column_name, 'Default Value') AS processed_column\nFROM table_name;\n\nSELECT COALESCE(order_date, current_date) AS processed_date\nFROM orders;\n\nSELECT\n  product ||' - '||\n  COALESCE(subcategory, category, family, 'no product description ')\n    AS product_and_subcategory\nFROM stock\n\n3rd Expression: If there is a NULL in subcategory, then it looks in category, then into family, and finally if all those fields have NULLs, it uses ‚Äúno product description‚Äù as the value.\n\nConcantenating Strings where NULLs are present\nSELECT COALESCE(first_name, '') || ' ' || COALESCE(last_name, '') AS full_name\nFROM employees;\n\nNULLs are replaced with an empty string so transformation doesn‚Äôt break\n\nPerforming calculations involving numeric columns where there are NULLs\nSELECT COALESCE(quantity, 0) * COALESCE(unit_price, 0) AS total_cost\nFROM products;\n\nSELECT product,\n  quantity_available,\n  minimum_to_have,\n  COALESCE(minimum_to_have, quantity_available * 0.5) AS threshold\nFROM stock\n\nNULLs are substituted with 0s so the calcuation doesn‚Äôt break\n\nAs part of a join in case keys have missing values\nSELECT *\nFROM employees e\nLEFT JOIN departments d ON COALESCE(e.department_id, 0) = COALESCE(d.id, 0);\nWith Aggregate Functions\nSELECT department_id, COALESCE(SUM(salary), 0) AS total_salary\nFROM employees\nGROUP BY department_id;\nMake hierarchical subtotals output more readable\n\nSELECT COALESCE(family,'All Families') AS family,\n COALESCE(category,'All Categories') AS category,\n COALESCE(subcategory,'All Subcategories') AS subcategory,\n SUM(quantity_available) as quantity_in_stock\nFROM stock\nGROUP BY ROLLUP(family, category, subcategory)\nORDER BY family, category, subcategory\n\nROLLUP¬†clause assumes a hierarchy among the columns¬†family,¬†category, and¬†subcategory. Thus, it generates all the grouping sets that make sense considering the hierarchy:¬†GROUP BY family,¬†GROUP BY family, category¬†and¬†GROUP BY family, category, subcategory.\n\nThis is the reason why¬†ROLLUP¬†is often used to generate subtotals and grand totals for reports.\n\nWithout COALESCE , the text in the unused columns for the subtotals would be NULLs.\n\n\n\n\n\nDuplicated Rows\n\nUses QUALIFY as a window function to filter out duplicates\n/* removes duplicate rows at the order_id level */\nSELECT * FROM orders\nQUALIFY row_number() over (partition by order_id order by created_at) = 1\n\nMore verbose example of what‚Äôs happening\nWITH temporary_employees as¬†\n(SELECT¬†\n¬† employee_id,¬†\n¬† employee_name,¬†\n¬† department,¬†\n¬† ROW_NUMBER() OVER(PARTITION BY employee_name,¬†\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† department,¬†\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† employee_id) as row_count¬†\nFROM Dummy_employees)\n\nSELECT *¬†\nFROM temporary_employees¬†\nWHERE row_count = 1\n\nUse a hash column as id column, then test for duplicates, remove them or investigate them (BigQuery)\n\nWITH\n¬† ¬† inbound_zoo_elephants AS (\n¬† ¬† ¬† ¬† SELECT *\n¬† ¬† ¬† ¬† FROM flowfunctions.examples.zoo_elephants\n¬† ¬† ),\n¬† ¬† add_row_hash AS (\n¬† ¬† ¬† ¬† SELECT\n¬† ¬† ¬† ¬† ¬† ¬† *,\n¬† ¬† ¬† ¬† ¬† ¬† TO_HEX(MD5(TO_JSON_STRING(inbound_zoo_elephants))) AS hex_row_hash\n¬† ¬† ¬† ¬† FROM inbound_zoo_elephants\n¬† ¬† )\n\nSELECT\n¬† ¬† COUNT(*) AS records,\n¬† ¬† COUNT(DISTINCT hex_row_hash) AS unique_records\nFROM add_row_hash\n\nNo duplicate records found, since ‚Äúrecords‚Äù = 9 and ‚Äúunique_records‚Äù = 9\n\nif records &gt; unique_records, duplicates exist\n\nCan select distinct hex_row_hash if you want to remove duplicates\nCan count hex_row_hash then filter where hex_row_hash &gt; 1 to find which rows are duplicates\nNotes from link\nDescription\n\nflowfunctions is the project name\nexamples is a directory (?)\nzoo_elephants is the dataset\n\nSteps\n\nTO_JSON_STRING - creates column with json string for each row\nMD5 hashes that string\nTO_HEX makes it alpha-numeric and gets rid of the symbols in the hash\n\nEasier to deal with in BigQuery\nAssume this is still unique (?)\n\n\nNote: By adding ‚Äútrue‚Äù value, TO_JSON_STRING(inbound_zoo_elephants, true) , TO_JSON_STRING adds line breaks to the json string for easier readability.\nHashing function options\n\nMD5 -¬† shortest one (16 characters), fine for this use case\n\ncryptographically broken, returns 16 characters and suffices for our use-case. Other options are\n\nFARM_FINGERPRINT - returns a signed integer of variable length\nSHA1, SHA256 and SHA512, which return 20, 32 and 64 bytes respectively and are more secure for cryptographic use cases.\n\n\n\n\n\nNested Data\n\nRecursive CTE\n\nRecursive CTEs are used primarily when you want to query hierarchical data or graphs. This could be a company‚Äôs organizational structure, a family tree, a restaurant menu, or various routes between cities\nAlso see\n\nWhat Is a Recursive CTE in SQL?\n\nTutorial, 3 examples, and links to other articles\n\n\nSyntax\nWITH RECURSIVE cte_name AS (\n¬† ¬† cte_query_definition (the anchor member)\n¬† ¬† UNION ALL\n¬† ¬† cte_query_definition (the recursive member)\n)\n\nSELECT *\nFROM¬† cte_name;\nExample: : postgres\nWITH RECURSIVE category_tree(id, name, parent_id, depth, path) AS (\n¬† SELECT id, name, parent_id, 1, ARRAY[id]\n¬† FROM categories\n¬† WHERE parent_id IS NULL\n¬† UNION ALL\n¬† SELECT categories.id, categories.name, categories.parent_id, category_tree.depth + 1, path || categories.id\n¬† FROM categories\n¬† JOIN category_tree ON categories.parent_id = category_tree.id\n)\n\nSELECT id, name, parent_id, depth, path\nFROM category_tree;\n\nCTE (WITH) + RECURSIVE says it‚Äôs a recursive query.\nUNION ALLcombines the results of both statements.\n\nExample is defined by 2 Select statements\n\nAnchor Member: First SELECT statement selects the root nodes of the category tree (nodes with no parent)\n\nRoot node is indicated by ‚Äúparent_id‚Äù = NULL\n\nRecursive member: Second SELECT statement selects the child nodes recursively\n\nAlso see Arrays for further examples of the use of UNION ALL\n\nThe ‚Äúdepth‚Äù column is used to keep track of the depth of each category node in the tree.\n\n‚Äú1‚Äù in the first statement\n‚Äúcategory_tree.depth + 1‚Äù in the second statement\n\nWith every recursion, the CTE will add 1 to the previous depth level, and it will do that until it reaches the end of the hierarchy\n\n\nThe ‚Äúpath‚Äù column is an array that stores the path from the root to the current node.\n\n‚ÄúARRAY[id]‚Äù in the first statement\n‚Äúpath || categories.id‚Äù in the second statement\n\n‚Äú||‚Äù concatenates ‚Äúpath‚Äù and ‚Äúid‚Äù columns (See Strings)\n\n\n\n\n\n\n\nBinning\n\nCASE WHEN\n\nSELECT\n Name, \n Grade,\n CASE\n  WHEN Grade &lt; 10 THEN '0-9'\n  WHEN Grade BETWEEN 10 and 19 THEN '10-19'\n  WHEN Grade BETWEEN 20 and 29 THEN '20-29'\n  WHEN Grade BETWEEN 30 and 39 THEN '30-39'\n  WHEN Grade BETWEEN 40 and 49 THEN '40-49'\n  WHEN Grade BETWEEN 50 and 59 THEN '50-59'\n  WHEN Grade BETWEEN 60 and 69 THEN '60-69'\n  WHEN Grade BETWEEN 70 and 79 THEN '70-79'\n  WHEN Grade BETWEEN 80 and 89 THEN '80-89'\n  WHEN Grade BETWEEN 90 and 99 THEN '90-99'\n  END AS Grade_Bucket\n FROM students\n\nBETWEEN is inclusive of the end points\nFlexible for any size of bin you need\n\nFLOOR\nSELECT\n Name,\n Grade,\n FLOOR(Grade / 10) * 10 AS Grade_Bucket\nFROM students\n\nCan easily scale up the number of bins without having to increase the lines of code\nOnly useful for evenly spaced bins\n\nLEFT JOIN on preformatted table\nCREATE OR REPLACE TABLE bins (\n    Lower_Bound INT64,\n    Upper_Bound INT64,\n    Grade_Bucket STRING\n);\n\nINSERT bins (Lower_Bound, Upper_Bound, Grade_Bucket)\nVALUES\n (0, 9, '0-9')\n (10, 19, '10-19')\n (20, 29, '20-29')\n (30, 39, '30-39')\n (40, 49, '40-49')\n (50, 59, '50-59')\n (60, 69, '60-69')\n (70, 79, '70-79')\n (80, 89, '80-89')\n (90, 99, '90-99');\n\nSELECT\n A.Name, \n A.Grade,\n B.Grade_Bucket\nFROM students AS A\nLEFT JOIN bins AS B\nON A.Grade BETWEEN B.Lower_Bound AND B.Upper_Bound\n\n‚Äúbins‚Äù table acts a template that funnels the values from your table into the correct bins\n\n\n\n\nTime Series\n\nExtract components from date-time columns\n/* MySQL */\nEXTRACT(part_of_date FROM date_time_column_name)\nYEAR(date_time_column_name)\nMONTH(date_time_column_name)\nMONTHNAME(date_time_column_name)\nDATE_FORMAT(date_time_column_name)\n\n/* SQLte */\nSELECT strftime('%m', OrderDate) as Month\n\nstrftime codes\n\n\nPreprocess Time Series with 4 Lags (article)\nWITH top_customers as (\n¬† ¬† --- select the customter ids you want to track\n),\ntransactions as (\n¬† ¬† SELECT¬†\n¬† ¬† ¬† cust_id,¬†\n¬† ¬† ¬† dt,¬†\n¬† ¬† ¬† date_trunc('hour', cast(event_time as timestamp)) as event_hour,¬†\n¬† ¬† ¬† count(*) as transactions\n¬† ¬† FROM ourTable\n¬† ¬† WHERE\n¬† ¬† ¬† ¬† dt between cast(date_add('day', -7, current_date) as varchar)¬†\n¬† ¬† ¬† ¬† and cast(current_date as varchar)\n¬† ¬† GROUP BY 1,2,3 Order By event_hour asc\n)\n\nSELECT transactions.cust_id,\n¬† ¬† ¬† transactions.event_hour,\n¬† ¬† ¬† day_of_week(transactions.event_hour) day_of_week,\n¬† ¬† ¬† ¬† hour(transactions.event_hour) hour_of_day,\n¬† ¬† ¬† ¬† transactions.transactions as transactions,\n¬† ¬† ¬† ¬† LAG(transactions,1) OVER¬†\n¬† ¬† ¬† ¬† ¬† (PARTITION BY transactions.cust_id ORDER BY event_hour) AS lag1,\n¬† ¬† ¬† ¬† LAG(transactions,2) OVER¬†\n¬† ¬† ¬† ¬† ¬† (PARTITION BY transactions.cust_id ORDER BY event_hour) AS lag2,\n¬† ¬† ¬† ¬† LAG(transactions,3) OVER¬†\n¬† ¬† ¬† ¬† ¬† (PARTITION BY transactions.cust_id ORDER BY event_hour) AS lag3,\n¬† ¬† ¬† ¬† LAG(transactions,4) OVER¬†\n¬† ¬† ¬† ¬† ¬† (PARTITION BY transactions.cust_id ORDER BY event_hour) AS lag4\nFROM transactions¬†\n¬† ¬† join top_customers¬†\n¬† ¬† ¬† on transactions.cust_id = top_customers.cust_id\n\n/* output */\n\"cust_id\", \"event_hour\", \"day_of_week\", \"hour_of_day\", \"transactions\", \"lag1\", \"lag2\", \"lag3\", \"lag4\"\n\"Customer-123\",\"2023-01-14 00:00:00.000\",\"6\",\"0\",\"4093\",,,,,,\n\"Customer-123\",\"2023-01-14 01:00:00.000\",\"6\",\"1\",\"4628\",\"4093\",,,,,\n\"Customer-123\",\"2023-01-14 02:00:00.000\",\"6\",\"2\",\"5138\",\"4628\",\"4093\",,,,\n\"Customer-123\",\"2023-01-14 03:00:00.000\",\"6\",\"3\",\"5412\",\"5138\",\"4628\",\"4093\",,,\n\"Customer-123\",\"2023-01-14 04:00:00.000\",\"6\",\"4\",\"5645\",\"5412\",\"5138\",\"4628\",\"4093\",\n\"Customer-123\",\"2023-01-14 05:00:00.000\",\"6\",\"5\",\"5676\",\"5645\",\"5412\",\"5138\",\"4628\",\n\"Customer-123\",\"2023-01-14 06:00:00.000\",\"6\",\"6\",\"6045\",\"5676\",\"5645\",\"5412\",\"5138\",\n\"Customer-123\",\"2023-01-14 07:00:00.000\",\"6\",\"7\",\"6558\",\"6045\",\"5676\",\"5645\",\"5412\",\n\nDataset contains number of transactions made per customer per hour.\n2 WITH clauses: the first just extracts a list of customers we are interested in. Here you can add any condition that is supposed to filter in or out specific customers (perhaps you want to filter new customers or only include customers with sufficient traffic). The second WITH clause simply creates the first data set ‚Äî Dataset A, which pulls a week of data for these customers and selects the customer id, date, hour, and number of transactions.\nFinally, the last and most important SELECT clause generates Dataset B, by using SQL lag() function on each row in order to capture the number of transactions in each of the hours that preceded the hour in the row.",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "qmd/sql.html#sec-sql-tools",
    "href": "qmd/sql.html#sec-sql-tools",
    "title": "SQL",
    "section": "Tools",
    "text": "Tools\n\nChatSQL: Convert plain text to MySQL query by ChatGPT\n{{sqlglot}} - no dependency Python SQL parser, transpiler, optimizer, and engine\n\nFormat SQL or translate between nearly twenty different SQL dialects.\n\nIt doesn‚Äôt just transpile active SQL code, too. Moves comments from one dialect to another.\n\nThe parser itself can be customized\nCan also help you analyze queries, traverse parsed expression trees, and incrementally (and, programmatically) build SQL queries.\nsupport for optimizing SQL queries, and performing semantic diffs.\nCan be used to unit test queries through mocks based on Python dictionaries.\nExample: : translate duckdb to hive\nimport sqlglot\nsqlglot.transpile(\n¬† \"SELECT EPOCH_MS(1618088028295)\",¬†\n¬† read = \"duckdb\",¬†\n¬† write = \"hive\"\n)[0]\n---\n'SELECT FROM_UNIXTIME(1618088028295 / 1000)'",
    "crumbs": [
      "SQL"
    ]
  },
  {
    "objectID": "qmd/visualization-diagrams.html",
    "href": "qmd/visualization-diagrams.html",
    "title": "Diagrams",
    "section": "",
    "text": "Mermaid",
    "crumbs": [
      "Visualization",
      "Diagrams"
    ]
  },
  {
    "objectID": "qmd/visualization-diagrams.html#sec-vis-dia-merm",
    "href": "qmd/visualization-diagrams.html#sec-vis-dia-merm",
    "title": "Diagrams",
    "section": "",
    "text": "Misc\n\nResources\n\nDocs, Theming, Cheatsheet, Quarto\n\nCurrently mermaid code chunks cannot be placed below bullets\n\n\n\nExamples\n\nFlowchart Top-Bottom (Source, Code)\n\n\n%% Top-Bottom direction\nflowchart TB\n    %% Classes with styling\n    classDef default color:#383838,fill:#FFF7F1,stroke-width:1px\n    classDef external color:#383838,fill:#E6EEF8,stroke-width:1px\n    classDef normal color:#081457,fill:#E3E6FC,stroke-width:1px\n    classDef local fill:#FFC700,stroke:#333,stroke-width:1px\n    classDef remote fill:#D2BDF2,stroke:#201434,stroke-width:1px\n    classDef notouch fill:#F99697,stroke:#A4050E,stroke-width:1px\n\n    %% Aliases, classes, and shapes applied to nodes\n    GH[(\"@main\")]:::remote\n    MDOUT[(\"@md-outputs\")]:::notouch\n    PAGES[(\"@gh-pages\")]:::notouch\n    DEPLOY([\"ci_deploy()\"]):::external\n    CIBUILDMD([\"ci_build_markdown()\"]):::external\n    CIBUILDSITE([\"ci_build_site()\"]):::external\n\n    %% Box around subset of nodes\n    %% Aliases, classes, and shapes applied\n    subgraph virtual machine\n    REPO[\"[repo]\"]:::local\n    BUILT[\"[repo]/site/built\"]:::local\n    SITE[\"[repo]/site/docs\"]:::local\n    VLESS(\"validate_lesson()\"):::normal\n    BUILDMD([\"build_markdown()\"]):::normal\n    BUILDSITE([\"build_site()\"]):::normal\n    end\n\n    %% Arrows and connection types\n    GH ---&gt; REPO\n    REPO -.- VLESS\n    DEPLOY ---&gt; VLESS\n    DEPLOY ---&gt; CIBUILDMD\n    DEPLOY ---&gt; CIBUILDSITE\n    VLESS -.- BUILDMD\n    CIBUILDMD ---&gt; MDOUT\n    MDOUT &lt;-.-&gt; BUILT\n    CIBUILDMD ---&gt; BUILDMD\n    CIBUILDSITE ---&gt; PAGES\n    PAGES &lt;-.-&gt; SITE\n    CIBUILDSITE ---&gt; BUILDSITE\n    BUILT -.- BUILDSITE\n    VLESS -.- BUILDSITE\n    BUILDMD --&gt; BUILT\n    BUILDSITE --&gt; SITE\n\n\n\n\n%% Top-Bottom direction\nflowchart TB\n    %% Classes with styling\n    classDef default color:#383838,fill:#FFF7F1,stroke-width:1px\n    classDef external color:#383838,fill:#E6EEF8,stroke-width:1px\n    classDef normal color:#081457,fill:#E3E6FC,stroke-width:1px\n    classDef local fill:#FFC700,stroke:#333,stroke-width:1px\n    classDef remote fill:#D2BDF2,stroke:#201434,stroke-width:1px\n    classDef notouch fill:#F99697,stroke:#A4050E,stroke-width:1px\n\n    %% Aliases, classes, and shapes applied to nodes\n    GH[(\"@main\")]:::remote\n    MDOUT[(\"@md-outputs\")]:::notouch\n    PAGES[(\"@gh-pages\")]:::notouch\n    DEPLOY([\"ci_deploy()\"]):::external\n    CIBUILDMD([\"ci_build_markdown()\"]):::external\n    CIBUILDSITE([\"ci_build_site()\"]):::external\n\n    %% Box around subset of nodes\n    %% Aliases, classes, and shapes applied\n    subgraph virtual machine\n    REPO[\"[repo]\"]:::local\n    BUILT[\"[repo]/site/built\"]:::local\n    SITE[\"[repo]/site/docs\"]:::local\n    VLESS(\"validate_lesson()\"):::normal\n    BUILDMD([\"build_markdown()\"]):::normal\n    BUILDSITE([\"build_site()\"]):::normal\n    end\n\n    %% Arrows and connection types\n    GH ---&gt; REPO\n    REPO -.- VLESS\n    DEPLOY ---&gt; VLESS\n    DEPLOY ---&gt; CIBUILDMD\n    DEPLOY ---&gt; CIBUILDSITE\n    VLESS -.- BUILDMD\n    CIBUILDMD ---&gt; MDOUT\n    MDOUT &lt;-.-&gt; BUILT\n    CIBUILDMD ---&gt; BUILDMD\n    CIBUILDSITE ---&gt; PAGES\n    PAGES &lt;-.-&gt; SITE\n    CIBUILDSITE ---&gt; BUILDSITE\n    BUILT -.- BUILDSITE\n    VLESS -.- BUILDSITE\n    BUILDMD --&gt; BUILT\n    BUILDSITE --&gt; SITE\n\n\n\n\n\n\n\nFlowchart Left-Right (Source, Code)\n\n\n%% Left-Right direction\nflowchart LR\n    %% Classes with styling\n    classDef default color:#383838,fill:#FFF7F1,stroke-width:1px\n    classDef external color:#383838,fill:#E6EEF8,stroke-width:1px\n    classDef normal color:#081457,fill:#E3E6FC,stroke-width:1px\n    classDef local fill:#FFC700,stroke:#333,stroke-width:1px\n    classDef remote fill:#D2BDF2,stroke:#201434,stroke-width:1px\n    classDef notouch fill:#F99697,stroke:#A4050E,stroke-width:1px\n\n    %% Classes and shapes applied to nodes\n    WEEK[\\\"CRON weekly\"\\]:::remote\n    MONTH[\\\"CRON monthly\"\\]:::remote\n\n    %% Boxes around subsets of nodes\n    %% Aliases, classes, and shapes applied\n    subgraph MAIN WORKFLOW\n    push[\\\"push to main\"\\]:::remote\n    md-outputs[(\"md-outputs\")]:::local\n    gh-pages[(\"gh-pages\")]:::local\n    sandpaper-main.yaml:::normal\n    end\n\n    subgraph \"UPDATES (requires SANDPAPER_WORKFLOW token)\"\n    update-cache.yaml:::normal\n    update-workflows.yaml:::normal\n    update-cache[(\"update/packages\")]:::notouch\n    update-workflows[(\"update/workflows\")]:::notouch\n    PR[/\"pull request\"/]:::remote\n    end\n\n    %% Arrows and connection types\n    push --&gt; sandpaper-main.yaml\n    WEEK --&gt; sandpaper-main.yaml\n    sandpaper-main.yaml -.-&gt;|\"pushes to\"| md-outputs\n    sandpaper-main.yaml -.-&gt;|\"pushes to\"| gh-pages\n    WEEK --&gt; update-cache.yaml\n    MONTH --&gt; update-workflows.yaml\n    update-cache.yaml -.-&gt;|\"pushes to\"| update-cache\n    update-workflows.yaml -.-&gt;|\"pushes to\"| update-workflows\n    update-cache.yaml -.-&gt;|\"creates\"| PR\n    update-workflows.yaml -.-&gt;|\"creates\"| PR\n\n\n\n\n%% Left-Right direction\nflowchart LR\n    %% Classes with styling\n    classDef default color:#383838,fill:#FFF7F1,stroke-width:1px\n    classDef external color:#383838,fill:#E6EEF8,stroke-width:1px\n    classDef normal color:#081457,fill:#E3E6FC,stroke-width:1px\n    classDef local fill:#FFC700,stroke:#333,stroke-width:1px\n    classDef remote fill:#D2BDF2,stroke:#201434,stroke-width:1px\n    classDef notouch fill:#F99697,stroke:#A4050E,stroke-width:1px\n\n    %% Classes and shapes applied to nodes\n    WEEK[\\\"CRON weekly\"\\]:::remote\n    MONTH[\\\"CRON monthly\"\\]:::remote\n\n    %% Boxes around subsets of nodes\n    %% Aliases, classes, and shapes applied\n    subgraph MAIN WORKFLOW\n    push[\\\"push to main\"\\]:::remote\n    md-outputs[(\"md-outputs\")]:::local\n    gh-pages[(\"gh-pages\")]:::local\n    sandpaper-main.yaml:::normal\n    end\n\n    subgraph \"UPDATES (requires SANDPAPER_WORKFLOW token)\"\n    update-cache.yaml:::normal\n    update-workflows.yaml:::normal\n    update-cache[(\"update/packages\")]:::notouch\n    update-workflows[(\"update/workflows\")]:::notouch\n    PR[/\"pull request\"/]:::remote\n    end\n\n    %% Arrows and connection types\n    push --&gt; sandpaper-main.yaml\n    WEEK --&gt; sandpaper-main.yaml\n    sandpaper-main.yaml -.-&gt;|\"pushes to\"| md-outputs\n    sandpaper-main.yaml -.-&gt;|\"pushes to\"| gh-pages\n    WEEK --&gt; update-cache.yaml\n    MONTH --&gt; update-workflows.yaml\n    update-cache.yaml -.-&gt;|\"pushes to\"| update-cache\n    update-workflows.yaml -.-&gt;|\"pushes to\"| update-workflows\n    update-cache.yaml -.-&gt;|\"creates\"| PR\n    update-workflows.yaml -.-&gt;|\"creates\"| PR\n\n\n\n\n\n\n\nGitgraph (Source, Code)\n\n\ngitGraph\n    commit id: \"abcd\"\n    commit id: \"efgh\" tag: \"0.14.0\"\n    branch feature1\n    branch feature2\n    checkout main\n    checkout feature1\n    commit id: \"ijkl\"\n    commit id: \"mnop\"\n    checkout main\n    merge feature1 tag: \"customTag\"\n    checkout feature2\n    commit id: \"qrst\"\n    commit id: \"uvwx\"\n\n\n\n\ngitGraph\n    commit id: \"abcd\"\n    commit id: \"efgh\" tag: \"0.14.0\"\n    branch feature1\n    branch feature2\n    checkout main\n    checkout feature1\n    commit id: \"ijkl\"\n    commit id: \"mnop\"\n    checkout main\n    merge feature1 tag: \"customTag\"\n    checkout feature2\n    commit id: \"qrst\"\n    commit id: \"uvwx\"\n\n\n\n\n\n\n\nThere‚Äôs an issue currently with adding id and type options to merge.",
    "crumbs": [
      "Visualization",
      "Diagrams"
    ]
  },
  {
    "objectID": "qmd/visualization-diagrams.html#d2",
    "href": "qmd/visualization-diagrams.html#d2",
    "title": "Diagrams",
    "section": "D2",
    "text": "D2\n\nResources\n\nQuarto extension\nDocs\nSVG Sofware Architecture Icons\n\nPackages\n\n{d2r} - Supports the creation, reading, writing, and rendering of D2 diagrams using R",
    "crumbs": [
      "Visualization",
      "Diagrams"
    ]
  },
  {
    "objectID": "qmd/feature-engineering-time-series.html",
    "href": "qmd/feature-engineering-time-series.html",
    "title": "Time Series",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Feature Engineering",
      "Time Series"
    ]
  },
  {
    "objectID": "qmd/feature-engineering-time-series.html#sec-feat-eng-ts-misc",
    "href": "qmd/feature-engineering-time-series.html#sec-feat-eng-ts-misc",
    "title": "Time Series",
    "section": "",
    "text": "Also see\n\nForecasting, ML &gt;&gt; Feature Engineering\n\nPackages\n\n{feasts}\n\nThe package works with tidy temporal data provided by the tsibble package to produce time series features, decompositions, statistical summaries and convenient visualisations. These features are useful in understanding the behaviour of time series data, and closely integrates with the tidy forecasting workflow used in the fable package\n\n{theft}\n\nProvides a single point of access to &gt; 1200 time-series features from a range of existing R and Python packages. The packages which theft ‚Äòsteals‚Äô features from currently are:\n\ncatch22 (R; see Rcatch22 for the native implementation on CRAN)\nfeasts (R)\ntsfeatures (R)\nKats (Python)\ntsfresh (Python)\nTSFEL (Python)\n\n\n{timetk}\n\nIncorporates tsfeatures package, timetk::tk_tsfeatures\nExample: Take weekly dataset and compute tsfeatures for each quarter\nnew_dat &lt;- dat %&gt;%\n¬† ¬† mutate(date_rounded = lubridate::round_date(date, \"quarter\")) %&gt;%\n¬† ¬† group_by(date_rounded) %&gt;%\n¬† ¬† timetk::tk_tsfeatures(\n¬† ¬† ¬† ¬† .date_var = date,\n¬† ¬† ¬† ¬† .value = price,\n¬† ¬† ¬† ¬† .features = c(\"median\", \"frequency\", \"stl_features\", \"entropy\", \"acf_features\"),\n¬† ¬† ¬† ¬† .prefix = \"tsfeat_\"\n¬† ¬† ) %&gt;%\n¬† ¬† ungroup()\n\n.features is for specifying the names of the features from tsfeatures that you want to include\n.prefix is the prefix for the newly created column names\ndate_rounded is a column that has the date for each quarter\n\n\n{fractaldim} (paper) - The fractal dimension of a series measures its roughness or smoothness.\n{tsfeatures}\n\nExample\nlibrary(tsfeatures)\nts_fts &lt;- \n  tsfeatures(\n    ts_data,\n    features = c(\n        \"acf_features\", \"outlierinclude_mdrmd\",\n        \"arch_stat\", \"max_level_shift\",\n        \"max_var_shift\", \"entropy\",\n        \"pacf_features\", \"firstmin_ac\",\n        \"std1st_der\", \"stability\",\n        \"firstzero_ac\", \"hurst\",\n        \"lumpiness\", \"motiftwo_entro3\"\n      )\n  )\n\n{{kats}}\n\nTime series analysis, including detection, forecasting, feature extraction/embedding, multivariate analysis, etc. Kats is released by Facebook‚Äôs Infrastructure Data Science team.\n\n{{pytimetk}} - Python version of {timetk}\n{{temporian}} - Similar to {{pytimetk}}\nhctsa - a Matlab software package for running highly comparative time-series analysis. It extracts thousands of time-series features from a collection of univariate time series and includes a range of tools for visualizing and analyzing the resulting time-series feature matrix. Can be ran through CLI. Calculates like 7000 features.\n\ncatch22 (paper) extracts only 22 canonical features (so much faster) used in hctsa and can be used in R {Rcatch22}, Python {{pycatch22}}, or Julia Catch22.jl.\n\n\nIssue: Features that aren‚Äôt available during the forecast horizon aren‚Äôt useful.\n\nSolutions:\n\nBut you can use a lagged value of predictor or an aggregated lagged value e.g.¬†averages, rolling averages, tiled/windowed averages\n\nExample: average daily customers for the previous week.\n\nSome features are one value per series, but the functions could be fed a lagged window (length of horizon?) of the whole series and generate a value for each window.\n\n\nDifference features that are really linear or have little variation. Change in value can be more informative\nForecasting shocks is difficult for an algorithm\n\nIt can better to smooth out (expected) shocks (Christmas) in the training data and then add an adjustment to the predictions during the dates of the shocks.\nThe smoothed out data will help the algorithm produce more accurate predictions for days when there isn‚Äôt an expected shock.\nExamples of shocks that may need training data to have manual adjustments and not be smoothed by an algorithm\n\none-time spikes due to abnormal weather conditions\none-off promotions\na sustained marketing campaign that is indistinguishable from organic growth.\n\n\nModels with large numbers (100s) of features increases the opportunity for feature drift\n\nPackage feature set comparison\n\nFrom paper, An Empirical Evaluation of Time Series Feature Sets\nMisc\n\nI think feasts ‚äÜ tsfeatures ‚äÜ catch22 ‚äÜ hctsa\nkats is a facebook python library\ntheft package integrates all these packages\n\nfeature redundancy\n\n\nSays catch22 features have fewer things in common with each other that the other packages\n\nComputation time\n\n\nNo surprise hctsa takes the most time. It‚Äôs like 7K features or something stupid\ntsfeatures,feasts are pretty disappointing\ncatch22 is excellent",
    "crumbs": [
      "Feature Engineering",
      "Time Series"
    ]
  },
  {
    "objectID": "qmd/feature-engineering-time-series.html#sec-feat-eng-ts-tran",
    "href": "qmd/feature-engineering-time-series.html#sec-feat-eng-ts-tran",
    "title": "Time Series",
    "section": "Tranformations",
    "text": "Tranformations\n\nLogging a feature can create more compact ranges, which then enables more efficient neural network training\nLog before differencing (SO post)\nstep_normalize(all_predictors)\n\nActually standardizes the variables\nIf you‚Äôre using predictors on different scales\n\nlm (and AR) are scale invariant, so not really necessary for those models\n\n\nSmoothing\n\nLOESS (LOcally WEighted Scatter-plot Smoother)\n\nWeights are applied to the neighborhood of each point which depend on the distance from the point\nA polynomial regression is fit at each data point with points in the neighborhood as explanatory variables\nSome robustness to outliers (by downweighting large residuals and refitting)\nspan: the distance from each data that considered the neighborhood is controlled by this argument\n\nDefault: 0.75\n&lt; 1: the value represents the proportion of the data that is considered to be neighbouring x, and the weighting that is used is proportional to 1-(distance/maximum distance)3)3, which is known as tricubic\nChoosing a value that‚Äôs too small will result in insufficient data near x for an accurate fit, resulting in a large variance\nChoosing a value that‚Äôs too large will result in over-smoothing and a loss of information, hence a large bias.\n\ndegree: degree of the polynomial regression used to fit the neighborhood data points\n\nDefault: 2 (quadratic)\nHigh degree: provides a better approximation of the population mean, so less bias, but there are more factors to consider in the model, resulting in greater variance.\n\nHigher than 2 typically doesn‚Äôt improve the fit very much.\n\nLower degree: (i.e.¬†1, linear) has more bias but pulls back variance at the boundaries.\n\nExample: {ggplot}\nggplot(data, aes(x = time, y = price)) +\n¬† ¬† geom_line(alpha = 0.55, color = \"black\") +¬†\n¬† ¬† geom_smooth(aes(color = \"loess\"), formula = y ~ x, method = \"loess\", se = FALSE, span = 0.70) +\n¬† ¬† scale_color_manual(name = \"smoothers\", values = c(\"ma\" = \"red\", \"loess\" = \"blue\"))\n\n\nExample: base r\nloess_mod &lt;- stats::loess(price ~ time, data = dat, span = 0.05, degree = 4)\ndat$loess_price &lt;- fitted(loess_mod)\nCubic Regression Splines\n\nExample: {mgcv}: mgcv::gam(price ~ s(time, bs = \"cs\"), data = data, method = \"REML\")$fitted.values",
    "crumbs": [
      "Feature Engineering",
      "Time Series"
    ]
  },
  {
    "objectID": "qmd/feature-engineering-time-series.html#sec-feat-eng-ts-eng",
    "href": "qmd/feature-engineering-time-series.html#sec-feat-eng-ts-eng",
    "title": "Time Series",
    "section": "Engineered",
    "text": "Engineered\n\nMisc\n\nTidymodels\n\ndate variable needs to be role = ID for ML models\n\n\n\n\nDecimal Date\n\nrecipe::step_date(year_month_var, features = c(\"decimal\"))\n\n\n\nCalendar Features\n\nCalandar Variables\n\nday of the month, day of the year, week of the month, week of the year, month, and year\nhour of the week (168 hours/week)\nminute, hour\nmorning/afternoon/ night\nExample: step_date, step_time\nexample_data &lt;- tibble(date = Sys.time() + 9 ^ (1:10))\n\nrecipe(~ ., data = example_data) |&gt;\n  step_date(all_datetime(), \n            features = c(\"year\", \"doy\", \"week\", \"decimal\", \"semester\", \n                         \"quarter\", \"dow\", \"month\")) |&gt;\n  step_time(all_datetime(),\n            features = c(\"am\", \"hour\", \"hour12\", \"minute\", \"second\", \n                         \"decimal_day\")) |&gt;\n  prep() |&gt;\n  bake(new_data = NULL) |&gt;\n  glimpse()\n\n#&gt; Rows: 10\n#&gt; Columns: 15\n#&gt; $ date             &lt;dttm&gt; 2023-12-07 11:46:02, 2023-12-07 11:47:14, 2023-12-07‚Ä¶\n#&gt; $ date_year        &lt;int&gt; 2023, 2023, 2023, 2023, 2023, 2023, 2024, 2025, 2036,‚Ä¶\n#&gt; $ date_doy         &lt;int&gt; 341, 341, 341, 341, 342, 347, 31, 108, 77  155\n#&gt; $ date_week        &lt;int&gt; 49, 49, 49, 49, 49, 50, 5, 16, 11, 23\n#&gt; $ date_decimal     &lt;dbl&gt; 2023.933, 2023.933, 2023.933, 2023.933, 2023 .935, 202‚Ä¶\n#&gt; $ date_semester    &lt;int&gt; 2, 2, 2, 2, 2, 2, 1, 1, 1, 1\n#&gt; $ date_quarter     &lt;int&gt; 4, 4, 4, 4, 4, 4, 1, 2, 1, 2\n#&gt; $ date_dow         &lt;fct&gt; Thu, Thu, Thu, Thu, Fri, Wed, Wed, Fri, Mon,  Fri\n#&gt; $ date_month       &lt;fct&gt; Dec, Dec, Dec, Dec, Dec, Dec, Jan, Apr, Mar,  Jun\n#&gt; $ date_am          &lt;lgl&gt; TRUE, TRUE, TRUE, FALSE, TRUE, FALSE, FALSE,  FALSE, F‚Ä¶\n#&gt; $ date_hour        &lt;int&gt; 11, 11, 11, 13, 4, 15, 20, 18, 13, 19\n#&gt; $ date_hour12      &lt;int&gt; 11, 11, 11, 1, 4, 3, 8, 6, 1, 7\n#&gt; $ date_minute      &lt;int&gt; 46, 47, 58, 35, 10, 23, 22, 11, 34, 59\n#&gt; $ date_second      &lt;dbl&gt; 2.890565, 14.890565, 2.890565, 14.890565, 2 .890565, 1‚Ä¶\n#&gt; $ date_decimal_day &lt;dbl&gt; 11.76747, 11.78747, 11.96747, 13.58747, 4 .16747, 15.3‚Ä¶\nmodeltime::step_timeseries_signature creates a similar set of calendar features\n\nDaylight Savings - At one point in the year, we have 23 hours in a day, and in another time, we have 25 hours in a day\n\nIf using a smooth::adam model, then it shifts seasonal indices, when the time change happens. All you need to do for this mechanism to work is to provide an object with timestamps to the function (for example, zoo).\n\nLeap Year\n\nBecomes less important when we model week of year seasonality instead of the day of year or hour of year\n\nHolidays\n\nstep_holiday_signature\n# Sample Data\ndates_in_2017_tbl &lt;- tibble::tibble(\n    index = tk_make_timeseries(\"2017-01-01\", \"2017-12-31\", by = \"day\")\n)\n\n# Add US holidays and Non-Working Days due to Holidays\n# - Physical Holidays are added with holiday pattern (individual) and locale_set\nrec_holiday &lt;- recipe(~ ., dates_in_2017_tbl) %&gt;%\n    step_holiday_signature(index,\n                           holiday_pattern = \"^US_\",\n                           locale_set      = \"US\",\n                           exchange_set    = \"NYSE\")\nbake(rec_holiday_prep, dates_in_2017_tbl)\n#&gt; # A tibble: 365 √ó 21\n#&gt;    index      index_exch_NYSE index_locale_US index_US_NewYearsDay\n#&gt;    &lt;date&gt;               &lt;dbl&gt;           &lt;dbl&gt;                &lt;dbl&gt;\n#&gt;  1 2017-01-01               0               1                    1\n#&gt;  2 2017-01-02               1               0                    0\n#&gt;  3 2017-01-03               0               0                    0\n#&gt;  4 2017-01-04               0               0                    0\n#&gt;  5 2017-01-05               0               0                    0\n#&gt;  6 2017-01-06               0               0                    0\n#&gt;  7 2017-01-07               0               0                    0\n#&gt;  8 2017-01-08               0               0                    0\n#&gt;  9 2017-01-09               0               0                    0\n#&gt; 10 2017-01-10               0               0                    0\n#&gt; # ‚Ñπ 355 more rows\n#&gt; # ‚Ñπ 17 more variables: index_US_MLKingsBirthday &lt;dbl&gt;,\n#&gt; #   index_US_InaugurationDay &lt;dbl&gt;, index_US_LincolnsBirthday &lt;dbl&gt;,\n#&gt; #   index_US_PresidentsDay &lt;dbl&gt;, index_US_WashingtonsBirthday &lt;dbl&gt;,\n#&gt; #   index_US_CPulaskisBirthday &lt;dbl&gt;, index_US_GoodFriday &lt;dbl&gt;,\n#&gt; #   index_US_MemorialDay &lt;dbl&gt;, index_US_DecorationMemorialDay &lt;dbl&gt;,\n#&gt; #   index_US_IndependenceDay &lt;dbl&gt;, index_US_LaborDay &lt;dbl&gt;, ‚Ä¶\n\nIndicators for holidays based on locales\nIndicators for when business is off based on stock exchanges\n\n\nAs splines\n\nExample: {tidymodels}\nstep_mutate(release_year = year(release_date),\n¬† ¬† ¬† ¬† ¬† ¬† release_week = week(release_date)) %&gt;%\nstep_ns(release_year, deg_free = tune(\"deg_free_year\")) %&gt;%\nstep_ns(releas_week, deg_free = tune(\"deg_free_week\"))\n\nMay need lubridate loaded for the mutate part\nCan also use a basis spline (step_bs)\n\nExample: {mgcv}\n\nctamm &lt;- \n  gamm(temp ~ s(day.of.year, bs = \"cc\", k=20) + s(time, bs = \"cr\"),\n¬† ¬† ¬†  data = cairo,\n¬† ¬† ¬† ¬†correlation = corAR1(form = ~1|year))\n\nFrom pg 371, ‚ÄúGeneralized Additive Models: An Introduction with R, 2nd Ed‚Äù (See R/Documents/Regression)\nHighly seasonal so uses a cyclic penalized cubic regression spline for ‚Äúday.of.year‚Äù\n10 peaks and 10 valleys probably explains ‚Äúk = 20‚Äù\n\nWith regression models, you have to be careful about encoding categoricals/discretes as ordinal (i.e.¬†integers). Linear regression does not model non-monotonic relationships between the input features and the target while tree models do.\n\nFor example, the raw numerical encoding (0-24) of the ‚Äúhour‚Äù feature prevents the linear model from recognizing that an increase of hour in the morning from 6 to 8 should have a strong positive impact on the number of bike rentals while a increase of similar magnitude in the evening from 18 to 20 should have a strong negative impact on the predicted number of bike rentals.\nOptions\n\nOne-hot encoding for small cardinality (e.g.¬†hours) and binning for large cardinality features (e.g.¬†minutes)\nSpline tranformation\n\nReduces number of features with comparable performance to one-hot\nPeriod isn‚Äôt used, just here (examples) for reference to the number of splines chosen\nnum_knots = num_splines + 1\nDegree 3 was used in the sklearn tutorial\n\nSKLearn also has an extrapolation=‚Äúperiodic‚Äù arg\n\n‚ÄúPeriodic splines with a periodicity equal to the distance between the first and last knot are used. Periodic splines enforce equal function values and derivatives at the first and last knot. For example, this makes it possible to avoid introducing an arbitrary jump between Dec 31st and Jan 1st in spline features derived from a naturally periodic‚Äùday-of-year‚Äù input feature. In this case it is recommended to manually set the knot values to control the period.‚Äù\nDon‚Äôt see this arg in step_bs or step_ns\n\n\nExamples\n\nHour feature (period = 24, num_splines = 12)\nWeekday (day of the week, numeric) feature (period=7, num_splines=3)\nMonth (period=12, num_splines=6)\n\n\nSpline transform + step_kpca_poly or step_kpca_rbf\n\nProduced best results in sklearn tutorial\nKernel function smooths out the spline\nAdd kpca_poly allows a regression model to capture non-linear (spline) interactions (kpca_poly)\n\nBoosted trees naturally capture these features\n\nExample in sklearn tutorial used n_components=300 in elasticnet regression which seems crazy, but their point was that if you were to create non-linear interaction features manually it‚Äôd be in the thousands\nUsing one-hot features instead of splines would require 3 or 4 times the number of components to reach the same performance which substantially increases training time.\n\n\n\n\n\n\n\nClustering\n\nSee Clustering, Time Series for details\n\n\n\nLags\n\nIf there is a gap period between the training and the validation (or test) set, all lags should be larger than this period\n\nSometimes predictions will have to be made with data that isn‚Äôt up-to-date. So your model training should mimic this.\n\n{recipe}\nrecipe::step_lag(var, lag: 4:7)\n\nCreates 4 lagged variables with lags 4:7\n\n{slider} - more sophisticated way without data leakage\nSCORE_recent &lt;- \n  slider::slide_index_dbl(SCORE,\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† date,\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† mean,\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† na.rm = TRUE,\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† .before = lubridate::days(365*3),\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† .after = -lubridate::days(1),\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† .complete = FALSE)\n\nafter is -lubridate::days(1) says don‚Äôt include current day\n\n‚ÄúPrevents data leakage by ensuring that this feature does not include information from the current day in its calculation‚Äù\n\nNot sure exactly why including the current day would cause data leakage\n\n\n\n{dplyr} + {purrr}\ncalculate_lags &lt;- function(df, var, lags){\n¬† map_lag &lt;- lags %&gt;% map(~partial(lag, n = .x))\n¬† return(df %&gt;% mutate(across(.cols = [{{var}}]{style='color: goldenrod'}, .fns = map_lag, .names = \"{.col}_lag[{lags}]{style='color: #990000'}\")))\n}\ntsla %&gt;%\n¬† calculate_lags(close, 1:3)\n\n\n\nRolling\n\n{feasts} functions\n\nTiling (non-overlappping) features\nSliding window (overlapping) features\n\nExponentially weighted moving average on lags (more recent values get more weight)\n\nHelps smooth out you lags if your data is noisy\nMight be easier to just smooth the outcome\nH2O‚Äôs weight parameter, alpha,¬† has a range between 0.9 and 0.99\n\nExample: smoothed over 2 days\n\nsales, 1 day ago = 3; 2 days ago = 4.5; and alpha = 0.95\nsmoothed sales = [3.0*(0.95^1) + 4.5*(0.95^2)] / [(0.95^1) + (0.95^2)] = 3.73\n\n\n\nMoving Averages\ndt_ma = data.table::frollmean(data[, 2], n = window, align = \"right\", fill = NA, algo = \"fast\")\nrcpp_ma = RcppRoll::roll_mean(data[, 2], n = window, align = \"right\", fill = NA\n\n{data.table} looks to be slightly faster than {RcppRoll} in a benchmark\n\nBoth are substantially faster than {TTR} and base R. {zoo}‚Äôs was slowest\n\n{TTR} produced different values (floating point precision) than the other packages\n\n\n\nDescriptive statistics\n\nExample: (weekly) units sold\n\nMetrics: mean, standard deviation, minimum, maximum, 25th percentile, 50th percentile, and 75th percentile\nRolling time windows: 1, 2, 3, 4, 12, 26, 52 weeks prior\n\nExample: rolling average sales from the previous year\n\ne.g.¬†2-week average sales of the previous year\n\n\n\n\n\nInteractions\n\nExamples\n\nInteractions between lags\nBetween workday (indicator) and hour\n\nDistinguishes between 8am on a monday and 8am on a sunday\nSpline transform hour and create interaction with workday\nMain effects(x + y) and interactions (xy) only are included, not variables of the form, x2\n\n\n\n\n\n‚ÄúDays Since‚Äù a Specific Date\n\nCheck scatter (e.g.¬†price vs last review date) and see if variance is heteroskedastic but should probably be log transformed\n# think this data only has year and month variables (numeric?)\nstep_mutate(days_since = lubridate::today() - lubridate::ymd(year, month, \"01\"))\n\n# last review is a date var\nstep_mutate(last_review = as.integer(Sys.Date() - last_review))\nExample: {lubridate}\nlength_of_stay &lt;- start_date %--% end_date / ddays(1) \nis_los_g90 &lt;- start_date %--% current_date &gt;= ddays(90)\n** consider using the inverse: 1 / number of days since **\n\nThis keeps the value between 0 and 1\nHave to watch out for 0s in the denominator and replace those values with 0 or replace the count with a really small fraction\n\nExamples\n\nLog days since the brand or product first appeared on the market\nThe number of weeks since the product was last sold\n\n\n\n\nInterval Groups\n\nCreating group indicators for different intervals in the series\nProbably involves some sort of clustering of the series or maybe this is what ‚Äúwavelets‚Äù are.\n\n\n\nFourier Transform (sine, cosine)\n\nDecision trees based algorithms (Random Forest, Gradient Boosted Trees, XGBoost) build their split rules according to one feature at a time. This means that they will fail to process these two features simultaneously whereas the cos/sin values are expected to be considered as one single coordinates system.\nHandled differently in different articles so ¬Ø\\_(„ÉÑ)_/¬Ø\nFrom https://towardsdatascience.com/how-to-handle-cyclical-data-in-machine-learning-3e0336f7f97c\n\\[\n\\begin{align}\n\\text{Hour}_{\\sin} &= \\sin \\left(\\frac{2\\pi\\cdot\\text{Hour}}{\\max (\\text{Hour})}\\right) \\\\\n\\text{Hour}_{\\cos} &= \\cos \\left(\\frac{2\\pi\\cdot\\text{Hour}}{\\max (\\text{Hour})}\\right)\n\\end{align}\n\\]\n\nExample: Tranforming an hour variable instead one-hot encoding\nThe argument is that since time features are cyclical, their transform should be a cyclical function. This way the difference between transformed 1pm and 2pm values are more closely related than if they were one-hot encoded where the difference between 1pm and 2pm is the same as 1pm and 8pm.\n\nFrom sklearn article\nperiod &lt;- 24 # hours variable\nvar &lt;- dat$hour¬† ¬† ¬† ¬† ¬† ¬†\nsin_encode &lt;- function (x, period) {sin(x / period * 2 * pi)}\ncos_encode &lt;- function (x, period) {cos(x / period * 2 * pi)}\nsin_encode(x = var, period = period)\nstep_harmonic\nexample_data &lt;- tibble(\n  year = 1700:1988,\n  n_sunspot = sunspot.year\n)\n\nsunspots_rec &lt;-¬†\n¬† recipe(n_sunspot ~ year, data = sun_train) |&gt; \n¬†   step_harmonic(year, \n¬†                 frequency = 1 / 11, \n¬†                 cycle_size = 1, # sunspots happen once every 11 yrs\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† role = \"predictor\",\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† keep_original_cols = FALSE) |&gt; \n    prep() |&gt;\n    bake(new_data = NULL)\n\n#&gt; # A tibble: 289 √ó 3\n#&gt;    n_sunspot year_sin_1 year_cos_1\n#&gt;        &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n#&gt;  1         5  -2.82e- 1     -0.959\n#&gt;  2        11  -7.56e- 1     -0.655\n#&gt;  3        16  -9.90e- 1     -0.142\n#&gt;  4        23  -9.10e- 1      0.415\n#&gt;  5        36  -5.41e- 1      0.841\n#&gt;  6        58   6.86e-14      1    \n#&gt;  7        29   5.41e- 1      0.841\n#&gt;  8        20   9.10e- 1      0.415\n#&gt;  9        10   9.90e- 1     -0.142\n#&gt; 10         8   7.56e- 1     -0.655\n#&gt; # ‚Ñπ 279 more rows\n\n\n\nChange Point Detection\n\nChange point analysis is concerned with detecting and locating structure breaks in the underlying model of a sequence of observations ordered by time, space or other variables.\nPackages\n\n{fastcpd} (Vignette): Fast Change Point Detection in R\n\nExtensible to all kinds of change point problems with a user specified cost function apart from the built-in cost functions.\n\n\nPersistent Homology Notes from Topological Change Point Detection\n\nTopological method\nClassifies the state of a set of features over a sliding window where the states are normal (incoherent), onset (partially coherent), synchronized (fully coherent)\nPython has a library that does this stuff, {geotto-tda}\n\nFor each window a 2-D pearson dissimilarity matrix is computed and a Vietoris-Rips persistence score is calculated ‚Äúup to homology 2.‚Äù Repeat for each window.\nThe amplitude, persistent entropy and number of diagram points per homology dimension are calculated for each resulting persistence diagram resulting in a feature vector (or maybe its a matrix)\nTuning parameters\n\nWindow Size\nStride (how many steps the window slides)\nCoherence threshold (classification threshold for ‚Äúsynchronized‚Äù)\n\nExample: 0.8\n\nOnset threshold (classification threshold for ‚Äúonset‚Äù)\n\nExample: 0.5\n\n\n\nUse the trained model to predict classification categories that can be used as a feature.\nThis state is supposed to be predictive of whether a change point is about to happen in the time series\n\nBayesian Change Point Indicator Regression\n\n\n\nDomain Specific\n\nNet calculation: recipe::step_mutate(monthly_net = monthly_export - monthly_import)\nAmount of the last sale\nTotal volume of units sold for each product up to that date\n\nIndicates of long-term historical sales performance\n\nCustomer Age - How long a customer has been with the company\ndat_prep_tbl &lt;- \n  dat_raw |&gt; \n    mutate(dt_customer = dmy(dt_customer),\n           dt_customer_age = -1 * (dt_customer - min(dt_customer)) / ddays(1)) |&gt;\n    select(-dt_customer)\n\nSubtracts the minimum of the customer-first-bought-from-the-company date variable from each customer‚Äôs first-bought date.\ndt_customer is a date the customer first bought from the company but in the raw dataset was a character type, so lubridate::dmy coerces it to a Date type\nWhy multiply by -1 instead of reversing the objects being substracted? Why divide by ddays(1)? I dunno. The resultant object is a dbl type, so maybe it‚Äôs a formatting thing.\n\n\n\n\nMissing Values\n\nWhen aggregating values (e.g.¬†daily to weekly), information about missing values is lost\nCreate a variable that is the sum of missing values over the aggregated period\nMissing rows in the original dataset (due to a lack of sales on those particular dates) can also be counted and added to the dataset",
    "crumbs": [
      "Feature Engineering",
      "Time Series"
    ]
  },
  {
    "objectID": "qmd/feature-engineering-time-series.html#sec-feat-eng-ts-discrete",
    "href": "qmd/feature-engineering-time-series.html#sec-feat-eng-ts-discrete",
    "title": "Time Series",
    "section": "Discretization",
    "text": "Discretization\n\nMisc\n\nUsed mostly for univariate time series modeling such as trend forecasting, anomaly detection, and classification\n\ne.g.¬†Anomaly Detection: After digitization, if the kth cluster contains very few elements relative to the other clusters, then this might be considered a trend anomaly.\n\nHelpful in compressing time series with large numbers of data points to make analysis (e.g.¬†trends, correlations, and other interesting patterns) easier.\n\nAdaptive Brownian Bridge-based Aggregation (ABBA)\n\nAdaptive Brownian bridge-based aggregation (ABBA) is a method for the symbolic representation of temporal data that can accurately capture important trends and shapes in time series data.\nPackages\n\n{{fABBA}} (JOSS)- Uses a fast alternative digitization method (i.e., greedy aggregation) instead of k-means clustering, providing significant speedup and improved tolerance-based digitization (without the need to specify the number, k, of symbols a priori).\n\nProcess:\n\nCompression via an adaptive piecewise linear approximation: ABBA approximates the time series by a sequence of linear segments, where each segment is represented by its change in x-value (length) and change in y-direction (increment).\n‚ÄúDigitization‚Äù via mean-based clustering: ABBA uses a clustering algorithm to assign a unique symbol to each cluster of the (length, increment) tuples, resulting in a symbolic representation of the time series.\n\nABBA has several advantages over other symbolic representations like SAX and 1d-SAX:\n\nIt can significantly outperform these methods in terms of reconstruction accuracy while also reducing the computational complexity.\nIt does not require the number of time series symbols to be specified in advance.\nIt can be used to compress other data types like images, not just time series.",
    "crumbs": [
      "Feature Engineering",
      "Time Series"
    ]
  },
  {
    "objectID": "qmd/anomaly-detection.html",
    "href": "qmd/anomaly-detection.html",
    "title": "Anomaly Detection",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Anomaly Detection"
    ]
  },
  {
    "objectID": "qmd/anomaly-detection.html#sec-anomdet-misc",
    "href": "qmd/anomaly-detection.html#sec-anomdet-misc",
    "title": "Anomaly Detection",
    "section": "",
    "text": "Also see\n\nOutliers\nFeature Engineering, Time Series &gt;&gt; Discretization\nBookmarks\n\nTime Series &gt;&gt; Cleaning/Processing &gt;&gt; Outliers\nBusiness Applications &gt;&gt; Fraud/Anomaly Detection\n\n\nPackages\n\nCRAN Task View\n{{pyod}}: A Comprehensive and Scalable Python Library for Outlier Detection (Anomaly Detection)\n\nResources\n\nDeep Learning for Anomaly Detection: A Review (2021 Pang et al)\nA Unifying Review of Deep and Shallow Anomaly Detection (2021 Ruff et al)\n\nProposes a formal mathematical definition of the notion of outlier\nReviews ‚Äúa broad area of the field - including deep neural networks - with a probabilistic perspective on the methods‚Äù\n\nThere and Back Again: Outlier Detection Between Statistical Reasoning and Data Mining Algorithms (2019 Zimek and Filzmoser)\n\nHigh-Level perspective on outlier detection\nStatistical view on the methods and a probabilistic interpretation of the anomaly scores\n\n\nA dataset that contains anomalies and has them labeled by a human expert is the ideal case since the problem then turns into a classification task.\n\nThe human can augment a sample with an annotation of which exact features are responsible for the anomaly (e.g., a segmentation mask in the context of image data).\n\nFrom Sci Kit Learn (link)\n\nCommon Questions\n\n‚ÄúI need to know why your model detected an anomaly. I need sound root cause analysis before I adjust my manufacturing process.‚Äù (see variable importance tracking)\n‚ÄúAnomaly detection is not enough: when a model detects an anomaly, it‚Äôs already too late. I need prediction to justify investing time and effort into such an approach.‚Äù (see event rates)\n‚ÄúI need prescription: tell me what I should do to prevent a failure from happening.‚Äù (see variable importance tracking)\n\nCommon Criticisms\n\n‚ÄúThere are some false positives, I don‚Äôt have time to investigate each event!‚Äù (see event rates)\n‚ÄúYour model only detects anomalies when they already happened, it‚Äôs useless!‚Äú (see event rates)\n‚ÄúI have hundreds of sensors: when an anomaly is detected by your model, I still have to investigate my whole operations, I‚Äôm not saving any time here!‚Äù (see variable importance tracking)\n\nAmazon Lookout for Equipment - managed service from AWS dedicated to anomaly detection\nML Algorithms (Descriptions from BRDAD paper)\n\nBagged Regularized k-distances for Anomaly Detection (BRDAD) is a distance algorithm that converts the unsupervised anomaly detection problem into a convex optimization problem. Is able to address the sensitivity challenge of the hyperparameter choice in distance-based algorithms. It has two hyperparameters, including the bagging rounds B and the subsampling size s. For the sake of convenience, s = [n/B] is fixed so the bagging rounds B is the only one hyper-parameter and is set to be B = 5 as default.\nElliptic Envelope is suitable for normally-distributed data with low dimensionality. As its name implies, it uses the multivariate normal distribution to create a distance measure to separate outliers from inliers. {{sklearn.covariance.EllipticEnvelope}}\nDistance-To-Measure (DTM) is a distance-based algorithm which employs a generalization of the k nearest neighbors named ‚Äúdistance-to-measure‚Äù. As suggested by the authors, the number of neighbors k is fixed to be k = 0.03 √ó sample size.\nk-Nearest Neighbors (k-NN) is a distance-based algorithm that uses the distance of a point from its k-th nearest neighbor to distinguish anomalies.\nLocal Outlier Factor (LOF) is a distance-based algorithm that measures the local deviation of the density of a given data point with respect to its neighbors. {{sklearn.neighbors.LocalOutlierFactor}}\nPartial Identification Forest (PIDForest) is a forest-based algorithm that computes the anomaly score of a point by determining the minimum density of data points across all subcubes partitioned by decision trees. Authors‚Äô implementation uses number of trees T = 50, the number of buckets B = 5, and the depth of trees p = 10 .\nIsolation Forest (iForest) is a forest-based algorithm that works by randomly partitioning features of the data into smaller subsets and distinguishing between normal and anomalous points based on the number of ‚Äúsplits‚Äù required to isolate them, with anomalies requiring fewer splits. {{sklearn.ensemble.IsolationForest}}\nOne-class SVM (OCSVM) is a kernel-based algorithm which tries to separate data from the origin in the transformed high-dimensional predictor space. An O(n) approximate solution to the One-Class SVM. Note that the O(n¬≤) One-Class SVM works well on our small example dataset. {{sklearn.linear_model.SGDOneClassSVM}}\n\nAny abnormal event visible in your time series will either be a:\n\nPrecursor Event\nDetectable Anomaly (forewarning about a future event)\nA Failure\nMaintenance Activity\nHealing Period (while your industrial process recovers after an issue)\n\nTypes\n\nShocks - abrupt changes, spikes\nLevel Shifts - can happen when a given time series shifts between range of values based on underlying conditions or operating modes.\n\nLevel ‚Äì The average value for a specific time period\nIf you want to consider all operating modes when detecting anomalies, you need to take care to include all of them in your training data\n\nTrending: a set of signals can change over time (not necessarily in the same direction).\n\nWhen you want to assess the condition of a process or of a piece of equipment, these trending anomalies will be great precursors events to search. They will help you build forewarning signals before actual failures may happen.\n\n\nUse average event rates to filter out false positives and predict an upcoming event\n\n\nTake action if the event rate (i.e.¬†rate of predicted events) starts to grow too large (allowing you to move from detecting to predicting)\n\ne.g.¬†You can decide to only notify an operator after the daily event rate reaches at least 200 per day. This would allow you to only react to 3 events out of the 41 detected (aka predicted) during this period\n\nUse historical event data to calculate an event rate threshold\nBy only reacting after a threshold predicted event rate has been reached, you filter out false positives (when scarce events are detected)\n\nTrack variable importance over time to narrow the field of potential causes of an event\n\n\n2 stacked column charts represent two anomalous events.\nEach color is a predictor variable (e.g.¬†sensor) that was important to the prediction of the event.\n\nOnly need to focus on a few (e.g.¬†top five predictor variables)\n\nUse to examine false positives and actual events\n\n\nFor the false positive (left)\n\nThe percentage of importance attributed the top 5 is much less than that for an actual event\nRed is less important and Yellow is more important than when there‚Äôs an actual event\n\n\n\nRolling Spectral Entropy\n\n\n\nSpectral Entropy is the normalized (power) spectral density )(PSD)\nMisc\n\nNotes from Anomaly Detection in Univariate Stochastic Time Series with Spectral Entropy\n\nGuidelines\n\nA series which has strong trend and seasonality (and so is easy to forecast) will have entropy close to 0.\n\nIn the case of noisy time series, this indicates an anomaly.\n\nA series that is very noisy (and so is difficult to forecast) will have entropy close to 1.\n\nExample\n\ndef spectral_entropy(x, freq, nfft=None):¬† ¬†\n¬† ¬† _, psd = periodogram(x, freq, nfft = nfft)¬† ¬†\n¬† ¬† # calculate shannon entropy of normalized psd\n¬† ¬† psd_norm = psd / np.sum(psd)\n¬† ¬† entropy = np.nansum(psd_norm * np.log2(psd_norm))\n¬† ¬† return -(entropy / np.log2(psd_norm.size))\n\nwindow = 200\nnfft = None\ndf = pd.DataFrame(data=x, columns=['x'])\ndf['x_roll_se'] = df['x'].rolling(window).apply(lambda x: spectral_entropy(x,freq=100,nfft=nfft))\n\nIf the FFT size is not specified, we will use the window size",
    "crumbs": [
      "Anomaly Detection"
    ]
  },
  {
    "objectID": "qmd/anomaly-detection.html#sec-anomdet-charts",
    "href": "qmd/anomaly-detection.html#sec-anomdet-charts",
    "title": "Anomaly Detection",
    "section": "Charts",
    "text": "Charts\n\nThe goal to breakdown an anomaly (e.g.¬†manufacturing process outage) into constituent parts (sensor readings).\n\n\nBy analyzing the sensor readings, you can potentially find the area causing the anomaly\nThere also might be leading indicators that are predictive of an anomaly.\n\nOften you‚Äôre looking at many time series (e.g.¬†a manufacturing process) when performing EDA for anomaly detection, and conventional time series charts are insufficient\nHorizon Chart\n\n\nHue (e.g.¬†blue or red) represents values above or below a certain value\nDarkness/lightness of the color represents the extremeness of the value\n\ni.e.¬†the darker the color the larger the magnitude of the y-axis value\n\nVertical layers in the normal chart become horizontal layers of the horizon chart\n\nlayer feature may provide more detail than the strip chart\n\n\nStrip Chart\n\nSimilar to horizon in that y-axis values get binned and represented by colors\n\nExample\n\n\nColors\n\nBlue - low, Gold - medium, Red - high\n\nColumns of red indicate shocks (e.g.¬†around 2022-11-15)\n\nExample\n\n\nColors\n\nBlue - low, Gold - medium, Red - high\n\nMajor color changes in color indicate trend/level shifts\n\ne.g.¬†the change from a lot of red to a lot of blue after December 2017",
    "crumbs": [
      "Anomaly Detection"
    ]
  },
  {
    "objectID": "qmd/anomaly-detection.html#sec-anomdet-isofor",
    "href": "qmd/anomaly-detection.html#sec-anomdet-isofor",
    "title": "Anomaly Detection",
    "section": "Isolation Forests",
    "text": "Isolation Forests\n\nAlso see Algorithms, ML &gt;&gt; Trees &gt;&gt; Isolation Forests\nA tree-based approach where outliers are more quickly isolated by random splits than inliers\nNotes from paper: https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/icdm08b.pdf\nThe tree algorithm chooses a predictor at random for the root node. Then randomly chooses either the minimum or the maximum of that variable as the splitting value. The algorithm recursively subsamples like normal trees (choosing variables and split points in the same manner) until each terminal node has one data point or replicates of the same data point or preset maximum tree height is reached. Across the trees of a forest, anomalies with have a shorter average path length from root to terminal node.\n\nThe algorithm is basically looking for observations with combinations of variables that have extreme values. The process of continually splitting subsamples of data will run out data points and be reduced to a single observation more quickly for an anomalous observation than a common observation.\nMakes sense. Picturing a tree structure, there shouldn‚Äôt be too many observations with more that a few minimums/maximums of variable values. The algorithm weeds out these observations as it moves down the tree structure.\n\nAny or all of these wouldn‚Äôt necessarily be global minimum/maximums since we‚Äôre dealing with subsamples of variable values as we move down the tree.\n\nPaper has some nice text boxes with pseudocode that goes through the steps of the algorithm.\n\nAnomaly scores range from 0 to 1. Observations with a shorter average path length will have a larger score.\n\nAnomaly score\n\\[\n\\begin{aligned}\n&s(x_i, n) = 2^{-\\frac{\\mathbb{E}(h(x_i))}{c(n)}}\\\\\n&\\begin{aligned}\n\\text{where}\\quad c(n) &= 2H(n-1) - \\frac{2(n-1)}{n} \\\\\nH(i) &= \\ln(i) + \\gamma\n\\end{aligned}\n\\end{aligned}\n\\]\n\n\\(\\mathbb{E}(h(x_i))\\) is the average path length across the isolation forest for that observation\n\\(H(i)\\) is the harmonic number and \\(\\gamma\\) is Euler‚Äôs constant\n\nGuidelines\n\nThe closer an observation‚Äôs score is to 1 the more likely that it is an anomaly\nThe closer to zero, the more likely the observation isn‚Äôt an anomaly.\nObservations with scores around 0.5 means that the algorithm can‚Äôt find a distinction.",
    "crumbs": [
      "Anomaly Detection"
    ]
  },
  {
    "objectID": "qmd/anomaly-detection.html#sec-anomdet-autoenc",
    "href": "qmd/anomaly-detection.html#sec-anomdet-autoenc",
    "title": "Anomaly Detection",
    "section": "Autoencoders",
    "text": "Autoencoders\n\nAn outlier is something that the autoencoder has not seen often during training, so it might have trouble finding a good encoding for them.\n\nAn autoencoder tries to learn good encodings for a given dataset. Since most data points in the dataset are not outliers, the autoencoder will be influenced most by the normal data points and should perform well on them.\n\nMisc\n\nAlso see Feature Reduction &gt;&gt; Autoencoders",
    "crumbs": [
      "Anomaly Detection"
    ]
  },
  {
    "objectID": "qmd/feature-reduction-general.html",
    "href": "qmd/feature-reduction-general.html",
    "title": "General",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Feature Reduction",
      "General"
    ]
  },
  {
    "objectID": "qmd/feature-reduction-general.html#sec-featred-gen-misc",
    "href": "qmd/feature-reduction-general.html#sec-featred-gen-misc",
    "title": "General",
    "section": "",
    "text": "Curse of Dimensionality\n\nIts when there are more variables than observations.\nCauses the least squares coefficient estimates to lose uniqueness.\nCauses overfitting in ML algorithms\n\nPackages\n\n{intRinsic} - Likelihood-Based Intrinsic Dimension Estimators; implements the ‚ÄòTWO-NN‚Äô and ‚ÄòGride‚Äô estimators and the ‚ÄòHidalgo‚Äô Bayesian mixture model\n\nProvides a clustering function for the Hidalgo model\nGraphical outputs built using ggplot2 so they are customizable\nSee section 5 (Summary and discussion) of the vignette for the recommended workflow and examples\n\n{Rdimtools} - feature selection, manifold learning, and intrinsic dimension estimation (IDE) methods\n\nCurrent version delivers 145 Dimension Reduction (DR) algorithms and 17 Intrinsic Dimension Estimator (IDE) methods.\n\n{RDRToolbox} - nonlinear dimension reduction with Isomap and LLE",
    "crumbs": [
      "Feature Reduction",
      "General"
    ]
  },
  {
    "objectID": "qmd/feature-reduction-general.html#sec-featred-gen-terms",
    "href": "qmd/feature-reduction-general.html#sec-featred-gen-terms",
    "title": "General",
    "section": "Terms",
    "text": "Terms\n\nIntrinsic Dimension (ID) - the minimal number of parameters needed to represent all the information contained in the data without significant information loss. A necessary piece of information to have before attempting to perform any dimensionality reduction, manifold learning, or visualization tasks. An indicator of the complexity of the features of a dataset.\nIsomap (IM) - nonlinear dimension reduction technique presented by Tenenbaum, Silva and Langford in 2000 [3, 4]. In contrast to LLE, it preserves global properties of the data. That means, that geodesic distances between all samples are captured best in the low dimensional embedding\nLocally Linear Embedding (LLE) - introduced in 2000 by Roweis, Saul and Lawrence. It preserves local properties of the data by representing each sample in the data by a linear combination of its k nearest neighbors with each neighbor weighted independently. LLE finally chooses the low dimensional representation that best preserves the weights in the target space.\nProjection Methods - maps the original data to a lower-dimensional space. The projection function can be linear, as in the case of PCA or or nonlinear, as in the case of locally linear embedding, Isomap, and tSNE.\nGeometric Methods - rely on the topology of a dataset, exploiting the properties of the distances between data points",
    "crumbs": [
      "Feature Reduction",
      "General"
    ]
  },
  {
    "objectID": "qmd/feature-reduction-general.html#sec-featred-gen-pca",
    "href": "qmd/feature-reduction-general.html#sec-featred-gen-pca",
    "title": "General",
    "section": "PCA",
    "text": "PCA\n\nDescription\n\nCreates a subset of variables that maximises the covariance with the initial variable set, in order to store as much information as possible in a lower dimension.\nCompute an orthogonal basis of the space created by the original set of variables. The vectors creating this basis are the eigenvectors of the variance-covariance matrix. Reducing the dimension is then easily done by selecting the eigenvectors that are most representative of the initial data: those that contain most of the covariance. The amount of covariance stored by the vectors is quantified by the eigenvalues: the larger the eigenvalue, the more interesting its associated vectors.\nProjects variables orthogonally which removes correlation between predictor variables. The projection is in the direction of maximum variation such that the variation is distributed unequally among the transformed vectors. This¬†allows the user to reduce the feature space while still being able to capture most of variance in the data.\nThe principal components are equal to linear combinations of the correlated variables and these components are orthogonal to each other.\nWhy? When multiple variables are highly correlated to each other it causes the math used calculate regression models to break down. High dimension datasets also require large amount computational resources. Too many columns compared to the number of rows.\n\n\n\nMisc\n\nAs a multicollinearity detector?\n\n‚Äúuse principal component analysis, and examine the screeplot, or proportion of variation explained by a subset of principal components. If all (or almost all) of the variation is explained with a small subset of all the variables, it means you have a multicollinearity problem. You will need to drop some variables or do some other dimension reduction to fix it before choosing your final model.‚Äù\nI mean what if you have 20 variables and 3 are collinear, would this be detectable with PCA? I don‚Äôt think so. Seem more likely that it would take a large portion of your variables being collinear for it to be detectable in this fashion.\n\n\n\n\nPreprocessing\n\nNotes from thread\nCenter variables\n\ncenter = T is default in prcomp( )\nIf variables are NOT on similar scales, then the data need to be scaled, also.\n\nProbably safer to always scale.\n\n\nSqrt any count variables\nLog any variable with a heavy tail\nIf you have too many features, use a sparse matrix to speed the process.\n\n\n\nDiagnostics\n\nTest for localization (repo with R code/docs)\n\nBad: if you make a histogram of a component (or loading) vector and it has really big outliers (aka localization)\n\nMeans this vector is mostly noise\n\nSolution: Regularized spectral clustering (links to resources)\nD_r = Diagonal(1/ sqrt(rs + mean(rs))\nD_c = Diagonal(1/ sqrt(cs + mean(cs))\n# Do SVD on\nD_r %*% A %*% D_c\n\nA is your matrix\nrs is a vector containing the row sums of the matrix\ncs is a vector containing the column sums of the matrix\n\n\n\n\n\nSteps\n\nCenter data in design matrix, A (n x p)\n\nIf data are centered and scaled then the computation in step 2 will result in the correlation matrix instead of the covariance matrix.\n\nCompute \\(n \\times n\\) Covariance Matrix,\n\\[\nC_x = \\frac{1}{n-1}AA^T\n\\]\n\nAlso seen \\(A^T A\\) but I don‚Äôt think it matters. The upper triangle and the lower triangle of this product are just reverse covariances of each other and thus equal and I suspect the order just switches flips the triangles. The eigenvectors/eigenvalues get reordered later on anyways.\nThe diagonal of this matrix is the variable variances.\n\nCalculate eigenvectors and eigenvalues: \\(C_x V = D_\\lambda V\\) shows the covariance matrix as a transformation matrix. \\(D\\) is a diagonal matrix (\\(p\\times p\\)) with eigenvalues along the diagonal. \\(V\\) is a matrix (\\(p \\times p\\)) of eigenvectors\n\\[\nD_\\lambda = VC_x V^{-1}\n\\]\nOrder eigenvalues from largest to smallest\nOrder the eigenvectors according to the order of their corresponding eigenvalues\nEquation for the ith value of the PC1 vector: \\(\\text{PC1}_i = V_{(,1)} \\cdot A_{(i,)}\\)\n\n\\(\\text{PC2}\\) is similar except \\(V_{(,2)}\\) is used\nWhere all the variables in \\(A\\) have been standardized and \\(V\\) contains the loadings (see below)\n\n\n\n\nNotes\n\n\\(AA^T\\) is positive definite\n\nWhich means it‚Äôs symmetric\nWhich means it has real eigenvalues¬†and orthogonal eigenvectors\nWhich means the eigenvectors have covariances = 0\nWhich means the eigenvectors aren‚Äôt correlated.\n\nThe eigenvalues are eigenvector‚Äôs standard deviations which determines how much variance is explained by that PC.\nThe variance of a variable is the dot-product of itself and it‚Äôs transpose, \\(x_i \\cdot x^t_i\\)\nThe covariance between two variables, \\(x_i \\cdot x^t_j\\)\nIn step 3 equation, eigenvalues give the magnitude (length of vector) and eigenvectors the direction after being transformed by the covariance matrix.\nElements in a PC vector are called scores and elements in the V eigenvector are called loadings.\n\nThe loadings are the coefficients in the linear combination of variables that equals the PC vector\nLoadings range from -1 to 1\nVariables with high loadings (usually defined as .4 in absolute value or higher because this suggests at least 16% of the measured variable variance overlaps with the variance of the component) are most representative of the component\nThe sign of a loading (+ or -) indicates whether a variable and a principal component are positively or negatively correlated.\n\nScaling your design matrix variables just means your using a correlation matrix instead of a covariance matrix.\nPCA is sensitive to outliers. Variance explained will be inflated in the direction of the outlier\n\nGuessing this means components strongly influenced by variables with outlier values will have their variance-explained value inflated\n\nRow order of data matters as to which interpretation (latent) of component is valid from Principle Components and Penguins\n\nUsed data from palmerpenguins to create a ‚Äúpenguin size‚Äù variable from performing PCA on the data.\nIn one row order, high values of pc1 were associated with high body mass, but after scrambling the rows, high values of pc1 were associated with low body mass.\nHave to be careful when adding new data to the PCA-created feature. It might arbitrarily change the sign of the component and change the meaning of the feature.\n\nPCA doesn‚Äôt take the response variable into account (unsupervised). Therefore, the directions (eigenvectors) obtained may be well-suited for the predictor variables, but not necessarily optimal for predicting the response. It does often produce pretty good results though.\n\nAn alternative would be Partial Least Squares (PLS) which does take the response into account (supervised).\n\nIn practice, PLS reduces bias while potentially increasing the variance so the benefit vs PCA regression is usually a wash.\nCapable of handling multivariate regression\nPopular in chemometrics for analyzing spectra.\n\n\n\n\n\nPlots\n\nMisc\n\nNotes from: https://support.minitab.com/en-us/minitab/18/help-and-how-to/modeling-statistics/multivariate/how-to/principal-components/interpret-the-results/all-statistics-and-graphs/\n(See bkmk) use broom::augment to add original data to pca output. Coloring the points by categorical variables can help with interpreting the components\nAlso see pkgs in notebook for visualization options\n\nScore\n\n\nClusters\n\nIf data follow a multivariate normal distribution then scores should be randomly distributed around zero\nIf there are clusters, then there may be multiple distributions present\n\nExtreme points (e.g.¬†point in bottom right) might be outliers and it might be worthwhile to investigate them further\n\nLoadings\n\n\nNeed to imagine an axis at (0,0). Don‚Äôt know why they don‚Äôt plot them with those axes.\nFinding the largest variable influences on a PC can used to interpret it‚Äôs meaning (think latent variable)\nArrows\n\nA (near) horizontal arrow (along the x-axis) describes that the feature contributes strongly toward PC1.\nA (near) vertical arrow (along the y-axis) describes that a feature contributes strongly towards PC2.\n\nValues\n\nLoadings range from -1 to 1\nThe termination coordinate of the line gives the loading values for that variable for both PCs\nLoadings (absolute magnitude) close to 0 indicate that the variable has little influence on that PC\nLarger the absolute value of the loading the greater the influence on that PC\nNegative values have negative influence on the latent variable that the PC represents (vice versa for positive values)\n\nAngle\n\nacute angles represent a positive correlation between those variables\nobtuse angles represent a negative correlation between those variables\n90 degree angles represent independence between those variables\n\nExample\n\nAge, Residence, and Employ have large influences on PC1 (interpretation: financial stability)\nCredit cards, Debt, and Education have large influences on PC2 (interpretation: credit history)\nSays, ‚ÄúAs the number credit cards increases, credit history (PC2 interpretation) becomes more negative.‚Äù\n\n\nBi-Plot\n\n\nCombination plot of the score and loading plot\nCan augment pca output (see top of section) with original data and color the scores by different categorical variables\n\nIf a categorical variable level is clustered around Education, you could say as Education rises, the more likely that that person is &lt;categorical level&gt;.\nIn turn, that categorical level would be either positively or negatively (depending on the loading sign) associated with that PC.\n\n\nInterpretation\n\nExample: Bluejays\n\nLoadings\n\n\nPC2 represents the difference between bill size and skull size\n\nLoadings together with components plot\n\n\nMale birds larger than female birds\n\nIf you look at the loadings plot, negative pc1 corresponds to larger size and the components plot shows males with negative PC1 values\n\nBoth sexes have large and short bills relative to their overall size\n\nMales and females both show values above and below 0 in PC2\nLarger bills but smaller bodies (+PC2) and larger bodies but smaller bills (-PC2)\n\n\nVariance Explained\n\n\nOverall bird size explains &gt; 50% of the variation in measurements\n\n\n\nExample: How much variation in a principal component can be explained by a categorical variable\n# Penguins dataset\n# pca_values is a prcomp() object\npca_points &lt;-¬†\n¬† # first convert the pca results to a tibble\n¬† as_tibble(pca_values$x) %&gt;%¬†\n¬† # now we'll add the penguins data\n¬† bind_cols(penguins)\n## # A tibble: 6 x 12\n##¬† ¬† PC1¬† ¬† PC2¬† ¬† PC3¬† ¬† PC4 species island bill_length_mm bill_depth_mm\n##¬† &lt;dbl&gt;¬† &lt;dbl&gt;¬† &lt;dbl&gt;¬† &lt;dbl&gt; &lt;fct&gt;¬† &lt;fct&gt;¬† ¬† ¬† ¬† ¬† &lt;dbl&gt;¬† ¬† ¬† ¬† &lt;dbl&gt;\n## 1 -1.85 -0.0320¬† 0.235¬† 0.528 Adelie¬† Torge‚Ä¶¬† ¬† ¬† ¬† ¬† 39.1¬† ¬† ¬† ¬† ¬† 18.7\n## 2 -1.31¬† 0.443¬† 0.0274¬† 0.401 Adelie¬† Torge‚Ä¶¬† ¬† ¬† ¬† ¬† 39.5¬† ¬† ¬† ¬† ¬† 17.4\n## 3 -1.37¬† 0.161¬† -0.189¬† -0.528 Adelie¬† Torge‚Ä¶¬† ¬† ¬† ¬† ¬† 40.3¬† ¬† ¬† ¬† ¬† 18¬†\n## 4 -1.88¬† 0.0123¬† 0.628¬† -0.472 Adelie¬† Torge‚Ä¶¬† ¬† ¬† ¬† ¬† 36.7¬† ¬† ¬† ¬† ¬† 19.3\n## 5 -1.92 -0.816¬† 0.700¬† -0.196 Adelie¬† Torge‚Ä¶¬† ¬† ¬† ¬† ¬† 39.3¬† ¬† ¬† ¬† ¬† 20.6\n## 6 -1.77¬† 0.366¬† -0.0284¬† 0.505 Adelie¬† Torge‚Ä¶¬† ¬† ¬† ¬† ¬† 38.9¬† ¬† ¬† ¬† ¬† 17.8\n## # ‚Ä¶ with 4 more variables: flipper_length_mm &lt;int&gt;, body_mass_g &lt;int&gt;,\n## #¬† sex &lt;fct&gt;, year &lt;int&gt;\n\npc1_mod &lt;-¬†\n¬† lm(PC1 ~ species, pca_points)\nsummary(pc1_mod)\n## Call:\n## lm(formula = PC1 ~ species, data = pca_points)\n##¬†\n## Residuals:\n##¬† ¬† Min¬† ¬† ¬† 1Q¬† Median¬† ¬† ¬† 3Q¬† ¬† Max¬†\n## -1.3011 -0.4011 -0.1096¬† 0.4624¬† 1.7714¬†\n##¬†\n## Coefficients:\n##¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† Estimate Std. Error t value Pr(&gt;|t|)¬† ¬†\n## (Intercept)¬† ¬† ¬† -1.45753¬† ¬† 0.04785¬† -30.46¬† &lt;2e-16 ***\n## speciesChinstrap¬† 1.06951¬† ¬† 0.08488¬† 12.60¬† &lt;2e-16 ***\n## speciesGentoo¬† ¬† 3.46748¬† ¬† 0.07140¬† 48.56¬† &lt;2e-16 ***\n## ---\n## Signif. codes:¬† 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n##¬†\n## Residual standard error: 0.5782 on 330 degrees of freedom\n## Multiple R-squared:¬† 0.879,¬† Adjusted R-squared:¬† 0.8782¬†\n## F-statistic:¬† 1198 on 2 and 330 DF,¬† p-value: &lt; 2.2e-16\n\nFrom https://bayesbaes.github.io/2021/01/28/PCA-tutorial.html\nAdjusted R-squared: 0.8782\n\n\nCan be seen visually in this chart by looking at the points in relation to the x-axis where species is segregated pretty nicely.\n\n\n\n\n\nOutliers\n\nMahalanobis Distance (MD)\n\nThis method might be problematic. Supposedly outliers affect the covariance matrix which affects the PCA, which affects the scores, which affects theMahalanobis distance (MD). So the MD might be biased and not be accurate in determining outliers\n\nRobust forms of PCA (see section below) would be recommended if you suspect outliers in your data.\n\nDisplays the Mahalanobis distance (MD) for each observation and a reference line to identify outliers. The Mahalanobis distance is the distance between each data point and the centroid of multivariate space (the overall mean).\n\nOutliers determined by whether the Mahalanobis Distance is greater than the square root of the Chi-Square statistic where m is the number of variables and Œ± = 0.05\n\nNo outliers in the chart above as all MDs lower than the threshold at 4.4\n\n\nLeverage Points and Orthogonal Outliers\n\nNotes from https://towardsdatascience.com/multivariate-outlier-detection-in-high-dimensional-spectral-data-45878fd0ccb8\nTypes\n\nLeverage Points\n\ncharacterized by a high score distance\ngood leverage points also have short orthogonal distance and bad leverage points have long orthogonal distances\ngood leverage points have a positive effect\n\nOrthogonal Outliers\n\ncharacterized by a high orthogonal distance\n\n\nType determined by (see article for the math)\n\nScore DIstance (SD) - the distance an observation is from center of K-dimensional PCA subspace\nOrthogonal Distance (OD) -¬†the deviation ‚Äî i.e.¬†lack of fit ‚Äî of an observation from the k-dimensional PCA subspace\nOutliers are determined by Chi-Square test very similar to the¬†Mahalanobis Distance method (see above).\n\n\n\nIn the example shown, the red dots are data known to be measurement errors. Most of the red dots are captured in the orthogonal and bad sections but quite a few normal observation (blue) points too. So this method needs to be used as a guide and followed up upon when it flags points.\n\n\n\nHotelling‚Äôs T2 and SPE/DmodX (Complementary Tests)\n\n{{pca}}\nHotelling‚Äôs T2 works by computing the chi-square tests across the top n_components for which the p-values are returned that describe the likeliness of an outlier. This allows for ranking the outliers from strongest to weak.\nSPE/DmodX (distance to model) based on the mean and covariance of the first 2 PCs\n\n\n\n\nExtensions\n\nRobust PCA\n\nData with outliers and high dimensional data (p &gt;&gt; n) are not suitable for regular PCA where p is the number of variables.\nLow Dim methods (only valid when n &gt; 2p) that find robust (against outliers) estimates of the covariance matrix\n\nS-estimator, MM-estimator, (Fast)MCD-estimator, re-weighted MCD- (RMCD) estimator\n\nHigh Dim Methods\n\nRobust PCA by projection-pursuit (PP-PCA)\n\nfinds directions for eigenvectors that maximize a ‚Äúprojection index‚Äù instead of directions that maximize variance\n\nMAD or Qn-estimator is used a projection index\n\n\nSpherical PCA (SPCA)\n\nhandles outliers by projecting points onto a sphere instead of a line or plane\n\nalso uses MAD or Qn-estimator\n\n\nRobust PCA (ROBPCA)\n\ncombines projection index approach with low dim robust covariance estimation methods somehow\n\nRobust Sparse PCA (ROSPCA)\n\nsame but uses sparse pca\napplicable to both symmetrically distributed data and skewed data\n\n\n\nKernel PCA\n\nPackages: {kernlab}\nNonlinear data (notebook)\nPCA in a hypothetical (kernel trick), higher dimensional space\nWith more dimensions, data points become more separable.\nResults depend on type of kernel\n\nGaussian Kernel\n\nTuning parameter: sigma\n\n\n\n\n\n\nTidymodels\n\nRecipe step\n# if only using dummy vars, no sure if normalization is necessary\n# step_normalize(&lt;pca variables&gt;)\nstep_pca(starts_with(\"tf_\"), num_comp = tune())\n# don't forget to include num_comp in your tuning grid\nTaking a tidymodel‚Äôs recipe object and performing PCA\ntf_mat &lt;- recipe_obj %&gt;%\n¬† ¬† # normalizing tokenized indicators (?)\n¬† ¬† # since these are all dummy vars, not sure if a normalization step is necessary)\n¬† ¬† step_normalize(starts_with(\"tf_\")) %&gt;%\n¬† ¬† prep() %&gt;%\n¬† ¬† bake() %&gt;%\n¬† ¬† # only want to pca text features\n¬† ¬† select(starts_with(\"tf_\") %&gt;%\n¬† ¬† as.matrix()\n\ns &lt;- svd(tf_mat)\n# scree plot\ntidy(s, matrix = \"d\") %&gt;%\n¬† ¬† filter(PC &lt;= 50) %&gt;%\n¬† ¬† ggplot(aes(x = PC, y = percent)) +\n¬† ¬† geom_point()\n\nmatrix (tidy arg):\n\n‚Äúu‚Äù, ‚Äúsamples‚Äù, ‚Äúscores‚Äù, or ‚Äúx‚Äù: Returns info about the map from the original space to the pc space\n‚Äúv‚Äù, ‚Äúrotation‚Äù, ‚Äúloadings‚Äù, or ‚Äúvariables‚Äù: Returns information about the map from the pc space to the original space\n‚Äúd‚Äù, ‚Äúeigenvalues‚Äù, or ‚Äúpcs‚Äù: Returns information about the eigenvalues\n\n\nExample\nlibrary(tidymodels)¬†\nlibrary(workflowsets)¬†\nlibrary(tidyposterior)¬†\ndata(meats, package= \"modeldata\")¬†\n# Keep only the water outcome¬†\nmeats &lt;- select(meats, -fat, -protein)¬†\nset.seed(1)¬†\nmeat_split &lt;- initial_split(meats)¬†\nmeat_train &lt;- training(meat_split)¬†\nmeat_test &lt;- testing(meat_split)¬†\nset.seed(2)¬†\nmeat_folds &lt;- vfold_cv(meat_train, repeats = 3)\nbase_recipe &lt;-¬†\n¬† recipe(water ~ ., data = meat_train) %&gt;%¬†\n¬† step_zv(all_predictors()) %&gt;%¬†\n¬† step_YeoJohnson(all_predictors()) %&gt;%¬†\n¬† step_normalize(all_predictors())¬†\npca_recipe &lt;-¬†\n¬† base_recipe %&gt;%¬†\n¬† step_pca(all_predictors(), num_comp = tune())¬†\npca_kernel_recipe &lt;-¬†\n¬† base_recipe %&gt;%¬†\n¬† step_kpca_rbf(all_predictors(), num_comp = tune(), sigma = tune())",
    "crumbs": [
      "Feature Reduction",
      "General"
    ]
  },
  {
    "objectID": "qmd/feature-reduction-general.html#sec-featred-gen-efa",
    "href": "qmd/feature-reduction-general.html#sec-featred-gen-efa",
    "title": "General",
    "section": "Exploratory Factor Analysis (EFA)",
    "text": "Exploratory Factor Analysis (EFA)\n\nIdentifies a number of latent factors that explain correlations between observed variables\n\nFrequently employed in social sciences where the main interest lies in measuring and relating unobserved constructs such as emotions, attitudes, beliefs and behaviour.\nLatent variables, referred to also as factors, account for the dependencies among the observed variables, referred to also as items or indicators, in the sense that if the factors are held fixed, the observed variables would be independent.\nIn exploratory factor analysis the goal is the following: for a given set of observed variables x1, . . . , xp one wants to find a set of latent factors Œæ1, . . . , Œæk, fewer in number than the observed variables (k &lt; p), that contain essentially the same information.\nIn confirmatory factor analysis, the objective is to verify a social theory. Hence, a factor model is specifed in advance and its fit to the empirical data is tested.\n\nMisc\n\nPackages\n\n{psych} - factor analysis, item response theory, reliability analysis\n{factominer} - Multiple Factor Analysis (MFA}\n{fspe} - Model selection method for choosing number of factors\n\nUses the connection between model-implied correlation matrices and standardized regression coefficients to do model selection based on out-of-sample prediction errors\n\n\nTwo main approaches for analysing ordinal variables with factor models:\n\nUnderlying Response Variable (URV)\n\nThe ordinal variables are generated by underlying continuous variables partially observed through their ordinal counterparts. (also see Regression, Ordinal &gt;&gt; Cumulative Link Models (CLM))\n\nItem Response Theory (IRT)\n\nOrdinal indicators are treated as they are.\n\n\n\nMethods for selecting the right number of factors\n\nMisc\n\nIssue: more factors always improve the fit of the model\n\nParallel Analysis: analyze the patterns of eigenvalues of the correlation matrix\nModel Selection: likelihood ratio tests or information criteria\n\nComparison with PCA\n\nPCA is a technique for reducing the dimensionality of one‚Äôs data, whereas EFA is a technique for identifying and measuring variables that cannot be measured directly (i.e.¬†latent factor)\nWhen variables don‚Äôt have anything in common, EFA won‚Äôt find a well-defined underlying factor, but PCA will find a well-defined principal component that explains the maximal amount of variance in the data.\nDifferences in the results between PCA and EFA don‚Äôt tend to be obvious in practice. As the number of variables (&gt;40 variables) involved in the analysis grows, results from PCA and EFA become more and more similar.\nSimilarly calculated method to PCA, but FA is an analysis on a reduced correlation matrix, for which the ones in the diagonal have been replaced by squared multiple correlations (SMC)\n\nA SMC is the estimate of the variance that the underlying factor(s) explains in a given variable (aka communality).\n\nThe variability in measured variables in PCA causes the variance in the principal component. This is in contrast to EFA, where the latent factor is seen as causing the variability and pattern of correlations among measured variables\nAn eigenvalue decomposition of the full correlation matrix is done in PCA, yet for EFA, the eigenvalue decomposition is done on the reduced correlation matrix\nFactor Analysis is a latent variable measurement model\n\nThe causal relationship is flipped in FA as compared to PCA.\n\n\nF is the latent variable (instead of component in PCA), b is a weight (like loadings in PCA), Y is a predictor variable, and u is an error\n\nHere, b estimates how much F contributes to Y",
    "crumbs": [
      "Feature Reduction",
      "General"
    ]
  },
  {
    "objectID": "qmd/feature-reduction-general.html#sec-featred-gen-autoenc",
    "href": "qmd/feature-reduction-general.html#sec-featred-gen-autoenc",
    "title": "General",
    "section": "Autoencoders",
    "text": "Autoencoders\n\nUnsupervised neural networks that learn efficient coding from the input unlabelled data. They try to reconstruct the input data by minimizing the reconstruction loss\nMisc\n\nUndercomplete Autoencoder (AE) ‚Äî the most basic and widely used type, frequently referred to as an Autoencoder\nSparse Autoencoder (SAE) ‚Äî uses sparsity to create an information bottleneck\nDenoising Autoencoder (DAE) ‚Äî designed to remove noise from data or images\nVariational Autoencoder (VAE) ‚Äî encodes information onto a distribution, enabling us to use it for new data generation\n\nLayers\n\nEncoder: Mapping from Input space to lower dimension space\nDecoder: Reconstructing from lower dimension space to Output space\n\nProcess\n\n\nEncodes the input data (X) into another dimension (Z), and then reconstructs the output data (X‚Äô) using a decoder network\nThe encoded embedding (Z) is preferably lower in dimension compared to the input layer and contains all the efficient coding of the input layer\nOnce the reconstruction loss is minimized, the learned weights or embeddings, in the Encoder layer can be used as features in ML models and the Encoder layer can be used to generate embeddings on future data.\n\nSparse Autoencoder (SE)\n\n\nUses regularization\nDimension reduction in the center is achieved through deactivating neurons\nExample\n\nThe model consists of 5 layers: one input, three hidden and one output.\nInput and output layers contain 784 neurons each (the shape of our data, i.e number of columns), with the size of hidden layers reduced to 16 neurons each.\nWe will train the model over 50 epochs and plot a loss chart (see below).\nWe will separate the encoder part of the model and save it to our project directory. Note, if you are not planning to reuse the same model afterwards, you don‚Äôt need to keep a copy of it.",
    "crumbs": [
      "Feature Reduction",
      "General"
    ]
  },
  {
    "objectID": "qmd/feature-reduction-time-series.html",
    "href": "qmd/feature-reduction-time-series.html",
    "title": "Time Series",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Feature Reduction",
      "Time Series"
    ]
  },
  {
    "objectID": "qmd/feature-reduction-time-series.html#sec-featred-ts-misc",
    "href": "qmd/feature-reduction-time-series.html#sec-featred-ts-misc",
    "title": "Time Series",
    "section": "",
    "text": "Also see\n\nForecasting, Multivariate &gt;&gt; Dynamic Factor Models\n\nPackages\n\n{freqdom.fda} (Paper) - Dynamic Functional Principal Components Analysis (FPCA)",
    "crumbs": [
      "Feature Reduction",
      "Time Series"
    ]
  },
  {
    "objectID": "qmd/feature-reduction-time-series.html#sec-featred-ts-dmd",
    "href": "qmd/feature-reduction-time-series.html#sec-featred-ts-dmd",
    "title": "Time Series",
    "section": "Dynamic Mode Decomposition (DMD)",
    "text": "Dynamic Mode Decomposition (DMD)\n\nCombines PCA and fourier transform\nSupposed to handle time series better than PCA\nPackages\n\n{{pydmd}}",
    "crumbs": [
      "Feature Reduction",
      "Time Series"
    ]
  },
  {
    "objectID": "qmd/surveys-analysis.html",
    "href": "qmd/surveys-analysis.html",
    "title": "39¬† Analysis",
    "section": "",
    "text": "39.1 Misc",
    "crumbs": [
      "Surveys",
      "<span class='chapter-number'>39</span>¬† <span class='chapter-title'>Analysis</span>"
    ]
  },
  {
    "objectID": "qmd/surveys-analysis.html#sec-surveys-anal-misc",
    "href": "qmd/surveys-analysis.html#sec-surveys-anal-misc",
    "title": "39¬† Analysis",
    "section": "",
    "text": "Packages\n\n{survey} - Lumley‚Äôs package\n{srvyr} - Brings parts of dplyr‚Äôs syntax to survey analysis, using the survey package.\n{svylme} (Paper) - Mixed models for complex surveys\n{svyvgam} - Inference based on the survey package for the wide range of parametric models in the ‚ÄòVGAM‚Äô package\n{jtools} - Support for the survey package‚Äôs svyglm objects as well as weighted regressions\n{dropout} (JOSS) - Classifies missing values based on the occurrence of single missing values, section dropouts and complete dropouts which allows it to produce summary statistics of different response patterns and relate them to the overall occurrence of missing values.\n{fastsurvey} (article): Making the {survey} hundreds of times faster using {Rcpp}\n\nResources\n\nExploring Complex Survey Data Analysis in R - Ebook from {srvyr} owner\nIntroduction to Regression Methods for Public Health Using R: Chapter 8 Analyzing Complex Survey Data\nSurvey Data Analysis with R - UCLA article that gives a nice overview of the {survey}\n\nQuestionnaire data can be modelled using ordinal regression (Liddell & Kruschke, 2018)\nPairwise Likelihood (original, mathy paper, factor analysis w/ordinal data paper, usage on customer survey data ($) paper)\n\na special case of composite likelihood methods that uses lower-order conditional or marginal log-likelihoods instead of the full log-likelihood\n\nWhen the number of items is greater than five (p &gt; 5), Full Information Likelihood (FIML) is only feasible when the Item Response Theory (IRT) framework is used. However, even in IRT, FIML becomes very computationally heavy as the number of factors increases. Using Pairwise likelihood is a suitable alternative\n\nIgnoring the survey design features (such as stratification, clustering and unequal selection probabilities) can lead to erroneous inferences on model parameters because of sample selection bias caused by informative sampling.\nIt is tempting to expand the models by including among the auxiliary variables all the design variables that define the selection process at the various levels and then ignore the design and apply standard methods to the expanded model. The main difficulties with this approach are the following:\n\nNot all design variables may be known or accessible to the analyst\nToo many design variables can lead to difficulties in making inference from the expanded model\nThe expanded model may no longer be of scientific interest to the analyst\n\ndesign-based approach can provide asymptotically valid repeated sampling inferences without changing the analyst‚Äôs model.\nresampling methods, such as the jackknife and the bootstrap for survey data, can provide valid variance estimators and associated inferences on the census parameters\n\nIn other cases, it is necessary to estimate the model variance of the census parameters from the sample. The estimator of the total variance is then given by the sum of this estimator and the re-sampling variance estimator.\n\nExample: in an education study of students, schools (first stage sampling units) may be selected with probabilities proportional to school size and students (second stage units) within selected schools by stratified random sampling.\n\nsee Surveys, Sampling Methods &gt;&gt; Probabilistic Sampling Methods &gt;&gt; Multi-Stage Sampling\nAgain, ignoring the survey design and using traditional methods for multi-level models can lead to erroneous inferences in the presence of sample selection bias\n\nIn the design-based approach, estimation of variance component parameters of the model is more difficult than that of regression parameters.\n\n\nasymptotically valid even when the sample sizes within sampled clusters (level 1 units) are small, unlike some of the existing methods, but knowledge of the joint inclusion probabilities within sampled clusters is required.\n\nLarge variations in cluster sizes may cause an issue, see Lumley\n\n\nExample of debiasing a dataset by other means than by weighting by population\n\nThe economist created a death-by-covid risk probability model. They had a bunch of medical records with patient comorbidities, age, gender, positive test, hospitalized, death/no death, etc. (people with other illnesses already) and were worried that the people who tested positive but just stayed at home (i.e.¬†no medical records like younger people). Not correcting for this bias of undetected cases would bias their risk probabilities.\n\nFailing to correct this bias would lead to underestimating the risks associated with comorbidities, and to overestimating the risks among those without listed conditions.\n\nThey used an estimated metric, national cfr per age group per gender per week (separate dataset from CDC which has stats on groups with and without medical records). When a week‚Äôs sample cfr didn‚Äôt match that week‚Äôs national cfr, they would randomly sample people in the dataset who didn‚Äôt meet the selection criteria (i.e.¬†positive covid test) and assign them a positive test. They continued to add these reclassified people to that week‚Äôs sample until the sample cfr matched the national cfr. Thus, debiasing they‚Äôre data set.\nThought this was an interesting case because it used a estimated metric to ‚Äúweight‚Äù subgroups within their sample to make it more representative of the ‚Äútrue‚Äù population.\n\nAlso see Projects &gt;&gt; Rolling COVID-19 CFR\n\nhttps://www.economist.com/graphic-detail/2021/03/11/how-we-built-our-covid-19-risk-estimator\n\nCeiling or floor effects occur when the tests or scales are relatively easy or difficult such that substantial proportions of individuals obtain either maximum or minimum scores and that the true extent of their abilities cannot be determined.\n\nSounds like censoring (See Regression, Other &gt;&gt; Censored and Truncated Data)\nCeiling or floor effects alone would induce, respectively, attenuation or inflation in mean estimates. And both ceiling and floor effects would result in attenuation in variance estimates.\n{DACF}\n\nRecovers mean and variance given data with ceiling/floor effects\nAllows for mean comparison tests such as t-test and ANOVA for data with ceiling/floor effects",
    "crumbs": [
      "Surveys",
      "<span class='chapter-number'>39</span>¬† <span class='chapter-title'>Analysis</span>"
    ]
  },
  {
    "objectID": "qmd/surveys-analysis.html#sec-surveys-anal-wts",
    "href": "qmd/surveys-analysis.html#sec-surveys-anal-wts",
    "title": "39¬† Analysis",
    "section": "39.2 Weights",
    "text": "39.2 Weights\n\nMisc\n\nSurveys responses are often biased due to coverage error, sampling error and non-response bias. Weighting is often an important step when analyzing survey data. For each unit in the sample (e.g.¬†respondent to a survey), we attach a weight that can be understood as the approximate number of people from the target population that this respondent represents. Weights adjust the sample distribution more towards the population distribution\n\nThe green bars show the sample with weights applied.\nThe weighted average will also be less biased to the extent the response is correlated with respondent‚Äôs age.\nThe weighted distribution is not fully corrected, mainly because of bias-variance considerations\n\nPackages\n\n{{balance}} - see section below\n{CBPS} - Covariate Balancing Propensity Scores (CBPS)\n\nAlso see\n\nTypes &gt;&gt; Covariate Balancing Propensity Scores (CBPS)\n{{balance}} &gt;&gt; Steps &gt;&gt; Calculate Weights &gt;&gt; Methods\n\n\n\n\nTypes\n\nFrequency Weights\n\nSteps\n\nRemove the duplicate observations\n\nDuplicates don‚Äôt add any additional information\n\nWeight each observation by the square root of number of times it appeared in the original dataset, \nSSE needs to be divided by n - k + 1\n\nwhere n is the number of observations in the original dataset and k is the number of predictors in the regression\n\n\n\nImportance Weights - focus on how much each row of the data set should influence model estimation. These can be based on data or arbitrarily set to achieve some goal.\nAnalytic Weights - If a data point has an associated precision, analytic weighting helps a model focus on the data points with less uncertainty (such as in meta-analysis).\n(Inverse) Probability Weights (wiki) - {{balance}} refers to this type as ‚Äúinverse propensity weights‚Äù\n\nAlso see below, {{balance}} &gt;&gt; Steps &gt;&gt; Adjust &gt;&gt; Options\nUsed to reduce bias when respondents have different probabilities of selection Adjusts a non-random sample to represent a population by weighting the sample units. It assumes two samples: A sample of respondents to a survey (or in a more general framework, a biased panel).\n  A sample of a target population, often referred to as \"reference sample\" or \"reference survey.\"\n      This sample includes a larger coverage of the population or a better sampling properties in a way that represents the population better.\n\n      It often includes only a limited number of covariates and doesn't include the outcome variables (the survey responses).\n\n      In different cases it can be the whole target apopulation (in case it is available), a census data (based on a survey) or an existing survey.\nPropensity Score - the probability to be included in the sample (the respondents group) conditioned on the characteristics of the unit Let pi = Pr{i ‚àà S | xi} with i = 1 ‚Ä¶ n.\n  i is the unit (aka respondent), n is the total number of respondents, S is the sample of respondents\n      X is a set of covariates that are available for the sample and the target population\n\n  pi‚Äã is the estimated probabilities of being in the sample using logistic regression\n      Data includes both sample and target population\n\n      outcome is a binary variable (1/0): 1 = Sample, 0 = Target\n\n      covariates are X\n\n  Also see [Econometrics, General](Econometrics, General) &gt;&gt; Propensity Score Matching\nCalculate Weights \n\ndi is ‚Ä¶?\n\n\nCovariate Balancing Propensity Scores (CBPS)\n\nWhen estimating propensity score, there is often a process of adjusting the model and choosing the covariates for better covariate balancing. The goal of CBPS is to allow the researcher to avoid this iterative process and suggest an estimator that is optimizing both the propensity score and the balance of the covariates together.\nMain advantage is in cases when the researcher wants better balance on the covariates than traditional propensity score methods - because one believes the assignment model might be misspecified and would like to avoid the need to fit followup models to improve the balance of the covariates.\nAlso see\n\nMisc &gt;&gt; packages &gt;&gt; {CBPS}\n{{balance}} &gt;&gt; Steps &gt;&gt; Adjust &gt;&gt; Options\n\n\n\n{{balance}}\n\nA Python package for adjusting biased data samples.\n\nProvides eda, weights calculation, comparison of variables before and after weighting\n\nSteps:\n\nEDA:\n\nUnderstanding the initial bias in the sample data relative to a target population we would like to infer\nSummary Statistics\n\nThe limitation of using the mean is that it is not easily comparable between different variables since they may have different variances.\nASMD (Absolute Standardized Mean Deviation) measures the difference between the sample and target for each covariate.\n\nIt uses weighted average and std.dev for the calculations (e.g.: to take design weights into account).\nThis measure is the same as taking the absolute value of Cohen‚Äôs d statistic (also related to SSMD), when using the (weighted) standard deviation of the population.\n\nNot sure why it says ‚Äú(weighted)‚Äù when it‚Äôs the std.dev of the population since weights are applied to sample data. Maybe the population estimate is itself a weighted calculation.\nGuidelines on effect size for Cohen‚Äôs D should apply here, too.\nFor categorical variables, the ASMD can be calculated as the average of the ASMD applied to each of the one-hot encoding of the categories of the variable\n\nAlso see\n\nPost-Hoc Analysis, general &gt;&gt; Bayesian &gt;&gt; Cohen‚Äôs D, SSMD\nPost-Hoc Analysis, Multilevel &gt;&gt; Cohen‚Äôs D\n\n\n\nVisualizations\n\nQ-Q plot (continuous)\n\nThe closer the line is to the 45-degree-line the better (i.e.: the less bias is observed in the sample as compared to the target population).\n\nBar Plots (categorical)\n\n\nCalculate Weights:\n\nAdjust the data to correct for the bias by producing weights for each unit in the sample based on propensity scores\nPreprocessing (‚Äúusing best practices in the field‚Äù):\n\nTransformations are done on both the sample dataframe and the target dataframe together\nMissing values¬† - adds a column ‚Äò_is_na‚Äô to any variable that contains missing values\n\nConsidered as a separate category for the adjustment\n\nFeature Engineering\n\nContinuous - bucketed into 10 quantiles buckets.\nCategorical - rare categories (with less than 5% prevalence) are grouped together so to avoid overfitting rare events\n\n\nMethods\n\nInverse Propensity Weighting (IPW)\n\nCoefficients, parameters of the fitted models are available\nSee above, Types &gt;&gt; (Inverse) Probability Weights Using LASSO logistic regression keeps the inflation of the variance as minimal as possible while still addressing the meaningful differences in the covariates between the sample and the target\nDesign Effect (max_de) for tuning penalty factor, Œª, and the trimming ratio parameter\n\nA measure of the expected impact of a sampling design on the variance of an estimator for some parameter\nmax_de=X - the regularization parameter and the trimming ratio parameter are chosen by a grid search over the 10 models with the max design effect value\n\nDefault is 1.5\nAssumption: larger design effect often implies better covariate balancing.\nWithin these 10 models, the model with the smallest ASMD is chosen.\n\nmax_de=None - optimization is performed by cross-validation of the logistic model\n\npenalty factor, Œª, is chosen when the MSE is at most 1 standard error from the minimal MSE\nthe trimming ratio parameter is set by the user, and default to 20\n\n\n\nCovariate Balancing Propensity Scores (CBPS)\n\nEstimates the propensity score in a way that optimizes prediction of the probability of sample inclusion as well as the covariates balance.\nAlso see\n\nTypes &gt;&gt; Covariate Balancing Propensity Scores (CBPS)\nMisc &gt;&gt; packages &gt;&gt; {CBPS}\n\nDesign Effect (max_de)\n\na measure of the expected impact of a sampling design on the variance of an estimator for some parameter\ndefault is 1.5; If ‚ÄúNone‚Äù, then optimization is unconstrained\n\n\nPost-Stratification\n\nPost-processing of the weights:\n\nTrims - trims the weights in order to avoid overfitting of the model and unnecessary variance inflation.\n\nOptions\n\nMean-Ratio - ratio from above according to which the weights are trimmed by mean(weights) * ratio. Default is 20.\nPercentile - winsorization is applied\n\n\nNormalizing to population size - weights can be described as approximating the number of units in the population this unit of the sample represents.\n\n\nCompare data with and without weights\n\nEvaluate the final bias and the variance inflation after applying the fitted weights.\nCompares ASMD score (See EDA), Design Effect, Model proportion deviance explained (if inverese propensity weighting method was used)\n\nASMD: since categorical variables are hot-encoded, a comparison (with/without weights) is made for each level\n\nComparison of means is available\nSimilar charts used in EDA are available that show a comparison between weighted/not weighted\nResponse Rates with/without weights\nEffects on outcome variable",
    "crumbs": [
      "Surveys",
      "<span class='chapter-number'>39</span>¬† <span class='chapter-title'>Analysis</span>"
    ]
  },
  {
    "objectID": "qmd/surveys-analysis.html#sec-surveys-anal-modeling",
    "href": "qmd/surveys-analysis.html#sec-surveys-anal-modeling",
    "title": "39¬† Analysis",
    "section": "39.3 Modeling",
    "text": "39.3 Modeling\n\nTidymodels\n\nMisc\n\nstep_dummy_multi_choice\nexample_data &lt;- tribble(\n  ~lang_1,    ~lang_2,   ~lang_3,\n  \"English\",  \"Italian\", NA,\n  \"Spanish\",  NA,        \"French\",\n  \"Armenian\", \"English\", \"French\",\n  NA,         NA,        NA\n)\n\nrecipe(~., data = example_data) |&gt;\n  step_dummy_multi_choice(starts_with(\"lang\")) |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\n#&gt; # A tibble: 4 √ó 5\n#&gt;   lang_1_Armenian lang_1_English lang_1_French lang_1_Italian lang_1_Spanish\n#&gt;             &lt;int&gt;          &lt;int&gt;         &lt;int&gt;          &lt;int&gt;          &lt;int&gt;\n#&gt; 1               0              1             0              1              0\n#&gt; 2               0              0             1              0              1\n#&gt; 3               1              1             1              0              0\n#&gt; 4               0              0             0              0              0\n\nMight be useful for preprocessing items with the same possible answers since creating one-hot encodes the typical way would result in duplicate columns.\n\n\nFrequency weights are used for all parts of the preprocessing, model fitting, and performance estimation operations.\n\nThis includes v-fold CV splits for now (see Using case weights with tidymodels for details)\n\nImportance weights only affect the model estimation and supervised recipes steps (i.e.¬†depend on the outcome variable).\n\nNot used with yardstick functions for calculating measures of model performance.\n\nExample: Importance weights\n\n\ntraining_sim &lt;-\n¬† training_sim %&gt;%¬†\n¬† mutate(\n¬† ¬† case_wts = ifelse(class == \"class_1\", 60, 1),\n¬† ¬† case_wts = parsnip::importance_weights(case_wts)\n¬† )\n\nset.seed(2)\nsim_folds &lt;- vfold_cv(training_sim, strata = class)\n\nsim_rec &lt;-¬†\n¬† recipe(class ~ ., data = training_sim) %&gt;%¬†\n¬† step_ns(starts_with(\"non_linear\"), deg_free = 10) %&gt;%¬†\n¬† step_normalize(all_numeric_predictors())\n\nlr_spec &lt;-\n¬† logistic_reg(penalty = tune(), mixture = 1) %&gt;%\n¬† set_engine(\"glmnet\")\n\nlr_wflow &lt;-\n¬† workflow() %&gt;%\n¬† add_model(lr_spec) %&gt;%\n¬† add_recipe(sim_rec) %&gt;%¬† ¬† ¬† ¬†\n¬† add_case_weights(case_wts)\n\ncls_metrics &lt;- metric_set(sensitivity, specificity)\ngrid &lt;- tibble(penalty = 10^seq(-3, 0, length.out = 20))\nset.seed(3)\nlr_res &lt;-\n¬† lr_wflow %&gt;%\n¬† tune_grid(resamples = sim_folds, grid = grid, metrics = cls_metrics)\nautoplot(lr_res) # calibration curves\n\nDescription\n\nBinary outcome; lasso\n‚Äúclass_1‚Äù (80 obs) is severely imbalanced with ‚Äúclass_2‚Äù (4920)\n\nclass_1 observations get a weight of 60 since 4920/80 = 61.5 which is ~ 60\n\n\nrecipe will automatically detect the weights (pretty sure it doesn‚Äôt matter whether on no ‚Äúcase_wts‚Äù is included in formula, e.g.¬†class ~ .)\n\nSince these are performance weights and step_ns and step_normalize don‚Äôt depend on the outcome variable (i.e.¬†supervised), case weights are not used in these transformations.\n\nSteps\n\nadd ‚Äúcase_wts‚Äù variable to df\nuse add_case_weights function in workflow code\n\nRemove the case weights from a workflow\n\nlr_unwt_wflow &lt;-\n¬† lr_wflow %&gt;%\n¬† remove_case_weights()\n\nUseful if you want to make a comparison between models",
    "crumbs": [
      "Surveys",
      "<span class='chapter-number'>39</span>¬† <span class='chapter-title'>Analysis</span>"
    ]
  },
  {
    "objectID": "qmd/eda-general.html",
    "href": "qmd/eda-general.html",
    "title": "General",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "EDA",
      "General"
    ]
  },
  {
    "objectID": "qmd/eda-general.html#sec-eda-gen-",
    "href": "qmd/eda-general.html#sec-eda-gen-",
    "title": "General",
    "section": "",
    "text": "First contact with an unfamiliar database\n\nselect * from limit 50\nLook for keys/fields to connect tables\nMake running list of Q‚Äôs, try to answer them by poking around first\nFind team/code responsible for DB and ask for time to review questions ‚Äì communication can be a superpower here!\n\nUse domain knowledge to assess peculier relationships\n\nExample: Is there a nonlinear relationship between Driver hours and Incentive Level\n\n\nCommon sense says if we raise payment bonuses, we should see more drivers want to work more hours.\nReason behind the relationship shown in this chart is omitted variables: weather and holiday.\n\nIncentives stop having an effect on drivers because they hate going out in shitty weather and want to stay home with their family on the holidays.",
    "crumbs": [
      "EDA",
      "General"
    ]
  },
  {
    "objectID": "qmd/eda-general.html#sec-eda-gen-bascln",
    "href": "qmd/eda-general.html#sec-eda-gen-bascln",
    "title": "General",
    "section": "Basic Cleaning",
    "text": "Basic Cleaning\n\nTidy column names\nShrink long column names to something reasonable enough for an axis label\nMake sure continuous variables aren‚Äôt initially coded as categoricals and vice versa\nMake note of columns with several values per cell and will¬†need to be separated into multiple columns (e.g.¬†addresses)\nFind duplicate rows\n\nSee\n\nCode, Snippets &gt;&gt; Cleaning\nSQL &gt;&gt; Processing Expressions &gt;&gt; Duplicates\nPython, Pandas &gt;&gt; Distinct\n\nThese can cause data leakage if the same row is in the test and train sets.\n\nMake a note to remove columns that the target is a function of\n\ne.g.¬†Don‚Äôt use monthly salary to predict yearly salary\n\nRemove columns that occur after the target event\n\ne.g.¬†Using info occurring in or after a trial to predict something pre-trial\n\nYou won‚Äôt have this info beforehand when you make your prediction\n\n\nOrdinal categorical\n\nReorder by a number in the text (parse_number)\nmutate(income_category = fct_reorder(income_category, parse_number(income_category)),\n¬† ¬† ¬† # manually fix category that is still out of order\n¬† ¬† ¬† # moves \"Less thatn $40K\" to first place in the levels\n¬† ¬† ¬† income_category = fct_relevel(income_category, \"Less than $40K\"))",
    "crumbs": [
      "EDA",
      "General"
    ]
  },
  {
    "objectID": "qmd/eda-general.html#sec-eda-gen-pkgs",
    "href": "qmd/eda-general.html#sec-eda-gen-pkgs",
    "title": "General",
    "section": "Packages",
    "text": "Packages\n\nBase R coplot can be used for quick plots of all combinations of categorical and continuous variables for up to 4 variables\n\nSee Continuous Predictor vs Outcome &gt;&gt; Continuous Outcome for examples\n\n{skimr::skim} - Overall summary, check completion percentage for vars with too many NAs\n{dataexplorer}\ncreate_report(airquality)\ncreate_report(diamonds, y = \"price\") # specify response variable\n\nRuns multiple functions to analyze dataset\n\n{dataxray} - Table with interactive distributions, summary stats, missingness, proportions. (dancho article/video)\n{trelliscope} - Quick, interactive, facetted pairwise plots, built with JS\n{explore} - Interactive data exploration or automated report\n\nexplore - If you want to explore a table, a variable or the relationship between a variable and a target (binary, categorical or numeric). The output of these functions is a plot (automatically checks if an attribute is categorical or numerical, chooses the best plot-type and handles outliers).\ndescribe - If you want to describe a dataset or a variable (number of na, unique values, ‚Ä¶) The output of these functions is a text.\nexplain - To create a simple model that explains a target. explain_tree() for a decision tree, explain_forest() for a random forest and explain_logreg() for a logistic regression.\nreport - To generate an automated report of all variables. A target can be defined (binary, categorical or numeric)\nabtest - To test if a difference is statistically significant\n\n{visdat} has decent visualization for group comparison, missingness, correlation, etc.\n{Hmisc::describe}\nsparkline::sparkline(0)\ndes &lt;- describe(d)\nplot(des) # maybe for displaying in Viewer pane\nprint(des, 'both') # maybe just a console df of the numbers\nmaketabs(print(des, 'both'), wide=TRUE) # for Quarto\n\n‚Äúboth‚Äù says display ‚Äúcontinuous‚Äù and ‚Äúcategorical‚Äù\n‚Äúcontinuous‚Äù\n\n‚Äúcategorical‚Äù\n\nColumns (from Hmisc Ref Manual)\n\n‚ÄúInfo‚Äù: Info which is a relative information measure using the relative efficiency of a proportional odds/Wilcoxon test on the variable relative to the same test on a variable that has no ties. Info is related to how continuous the variable is, and ties are less harmful the more untied values there are. The formula for Info is one minus the sum of the cubes of relative frequencies of values divided by one minus the square of the reciprocal of the sample size. The lowest information comes from a variable having only one distinct value following by a highly skewed binary variable. Info is reported to two decimal places.\n‚ÄúMean‚Äù and ‚ÄúSum‚Äù (Binary): , the sum (number of 1‚Äôs) and mean (proportion of 1‚Äôs)\n\n\nLux - Jupyter notebook widget that provides visual data profiling via existing pandas functions which makes this extremely easy to use if you are already a pandas user. It also provides recommendations to guide your analysis with the intent function. However, Lux does not give much indication as to the quality of the dataset such as providing a count of missing values for example.\n{{pandas_profiling}} - Produces a rich data profiling report with a single line of code and displays this in line in a Juypter notebook. The report provides most elements of data profiling including descriptive statistics and data quality metrics. Pandas-profiling also integrates with Lux.\n{{sweetviz}} - Provides a comprehensive and visually attractive dashboard covering the vast majority of data profiling analysis needed. This library also provides the ability to compare two versions of the same dataset which the other tools do not provide.\n{{ydata-profiling}} - Data profiling, automates, and standardizes the generation of detailed reports, complete with statistics and visualizations",
    "crumbs": [
      "EDA",
      "General"
    ]
  },
  {
    "objectID": "qmd/eda-general.html#sec-eda-gen-miss",
    "href": "qmd/eda-general.html#sec-eda-gen-miss",
    "title": "General",
    "section": "Missingness",
    "text": "Missingness\n\nAlso see\n\nMissingness\nModel Building, tidymodels &gt;&gt; Recipe &gt;&gt; Imputation\n\nPackages\n\n{naniar} - Tidy ways to summarize, visualize, and manipulate missing data with minimal deviations from the workflows in ggplot2 and tidy data\n{mde} - Functions for percentage stats and various preprocessing (group) actions (e.g.¬†dropping, recoding, etc.)\n{qreport} - Harrell package\n\nA few of the charts aren‚Äôt intuitive and don‚Äôt have good documentation in terms of explaining how to interpret them.\nFits an ordinal logistic regression model to describe which types of subjects (based on variables with no NAs) tend to have more variables missing.\nHierarchically clusters variables that have similar observations missing\nSee naclus docs, RMS Ch.19.1, R Workflow Ch.2.7 (interprets the clustering), Ch.6 (interpretes the ordinal regression) (possibly more use cases in that ebook)\n\n\nQuestions\n\nWhich features contain missing values?\nWhat proportion of records for each feature comprises missing data?\nIs the missing data missing at random (MAR) or missing not at random (MNAR) (i.e.¬†informative)?\nAre the features with missing values correlated with other features?\n\nCategoricals for binary classification\n\ntrain_raw %&gt;%\n¬† select(\n¬† ¬† damaged, precipitation, visibility, engine_type,\n¬† ¬† flight_impact, flight_phase, species_quantity\n¬† ) %&gt;%\n¬† pivot_longer(precipitation:species_quantity) %&gt;%\n¬† ggplot(aes(y = value, fill = damaged)) +\n¬† geom_bar(position = \"fill\") +\n¬† facet_wrap(vars(name), scales = \"free\", ncol = 2) +\n¬† labs(x = NULL, y = NULL, fill = NULL)\n\nThe NAs (top row in each facet) aren‚Äôt 50/50 between the two levels of the target. The target is imbalanced and the NAs seem to be predictive of ‚Äúno damage,‚Äù so they aren‚Äôt random.\nSince these NAs look predictive, you can turn them into a category by using step_unknown in the preprocessing recipe.",
    "crumbs": [
      "EDA",
      "General"
    ]
  },
  {
    "objectID": "qmd/eda-general.html#sec-eda-gen-out",
    "href": "qmd/eda-general.html#sec-eda-gen-out",
    "title": "General",
    "section": "Outliers",
    "text": "Outliers\n\nAlso see Outliers\nAbnormalities due to likely data entry errors\n\nExample: store == ‚Äúopen‚Äù and sales == 0 or store == ‚Äúclosed‚Äù and sales &gt; 0\n\nPotential sol‚Äôn: replace 0‚Äôs (open) with mean sales and sales &gt;0 (closed) with 0s\n\n\nExtreme counts in charts when grouping by a cat var\n\nWhy is one category‚Äôs count so low or so high?\n\nMay need subject matter expert\n\nWhat can be done to increase or decrease that category‚Äôs count?\n\nFor prediction, experiment with keeping or removing outliers while fitting baseline models",
    "crumbs": [
      "EDA",
      "General"
    ]
  },
  {
    "objectID": "qmd/eda-general.html#sec-eda-gen-grpcal",
    "href": "qmd/eda-general.html#sec-eda-gen-grpcal",
    "title": "General",
    "section": "Group Calculations",
    "text": "Group Calculations\n\nAlso see Feature Engineering, General &gt;&gt; Domain Specific\nVariance of Value by Group\n\nExample: how sales vary between store types over a year\nimportant to standardize the value by group\n\ngroup_by(group), mutate(sales = scale(sales))\n\nWhich vary wildly and which are more stable\n\nRates by Group\n\nExample: sales($) per customer\n\ngroup_by(group), mutate(sales_per_cust = sum(sales)/sum(customers)\n\n\nAvg by Group(s)\ndat %&gt;%\nselect(cat1, cat2, num) %&gt;%\ngroup_by(cat1, cat2) %&gt;%\nsummarize(freq = n(),\n¬† ¬† ¬† ¬† ¬† avg_cont = mean(num))",
    "crumbs": [
      "EDA",
      "General"
    ]
  },
  {
    "objectID": "qmd/eda-general.html#sec-eda-gen-cont",
    "href": "qmd/eda-general.html#sec-eda-gen-cont",
    "title": "General",
    "section": "Continuous Variables",
    "text": "Continuous Variables\n\nDoes the variable have a wide range. (i.e.¬†values across multiple magnitudes: 101 and 102 and ‚Ä¶ etc.)\n\nIf so, log the variable\n\nHistogram - Check shape of distribution\nggplot(aes(var)) +\n¬† ¬† geom_histogram()\n\nLooking at skew. Is it roughly¬†normal?\nDoes filter(another_var &gt; certain_value (see below) help it look more normal?\nIs it multi-modal\n\nSee Regression, Other &gt;&gt; Multi-Modal(visuals, tests, modelling, etc.)\n{{gghdr}} - Visualization of Highest Density Regions in ggplot2\nInteractions &gt;&gt; Outcome: Categorical &gt;&gt; Binary Outcome (pct_event) vs Discrete by Discrete (or binary in this case)\n\nIs the variable highly skewed\n\nIf so, try:\n\nChanging units (min to hr),\nfilter(some_var &gt; some_value)\nsome combination of the above make more normal?\n\nNormality among predictors isn‚Äôt necessary, but I think it improves fit or prediction somewhat\n\nlog transformation may help some if the skew isn‚Äôt too extreme\n\n\n\nQ-Q plot to check fit against various distributions\n\n{ggplot}\nggplot(data)+\n¬† ¬† stat_qq(aes(sample = log_profit_rug_business))+\n¬† ¬† stat_qq_line(aes(sample = log_profit_rug_business))+\n¬† ¬† labs(title = 'log(profit) Normal QQ')\n\nA plot of the sample (or observed) quantiles of the given data against the theoretical (or expected) quantiles.\nSee article for the math and manual code\nstat_qq, stat_qq_line default distributions are Normal\nggplot::stat_qq docs have some good examples on how to use q-q plots to test your data against different distributions using MASS::fitdistr to get the distributional parameter estimates. Available distributions: ‚Äúbeta‚Äù, ‚Äúcauchy‚Äù, ‚Äúchi-squared‚Äù, ‚Äúexponential‚Äù, ‚Äúgamma‚Äù, ‚Äúgeometric‚Äù, ‚Äúlog-normal‚Äù, ‚Äúlognormal‚Äù, ‚Äúlogistic‚Äù, ‚Äúnegative binomial‚Äù, ‚Äúnormal‚Äù, ‚ÄúPoisson‚Äù, ‚Äút‚Äù and ‚Äúweibull‚Äù\n\n{dataexplorer}\n## View quantile-quantile plot of all continuous variables\nplot_qq(diamonds)\n\n## View quantile-quantile plot of all continuous variables by feature `cut`\nplot_qq(diamonds, by = \"cut\") \nSkewed Variables\n\nx &lt;- list()\nn &lt;- 300\nx[[1]] &lt;- rnorm(n)\nx[[2]] &lt;- exp(rnorm(n))\nx[[3]] &lt;- -exp(rnorm(n))\n\npar(mfrow = c(2,3), bty = \"l\", family = \"Roboto\")\n\nqqnorm(x[[1]], main = \"Normal\")\nqqnorm(x[[2]], main = \"Right-skewed\")\nqqnorm(x[[3]], main = \"Left-skewed\")\nlapply(x, function(x){plot(density(x), main = \"\")})\nGood fits\n\nnormal distribution\n\nBad fits\n\nUniform data tested against a normal distibution\n\nUniform data tested against an exponential distribution\n\n\n\n\nIs the mean/median above or below any important threshold?\n\ne.g.¬†CDC considers a BMI &gt; 30 as obese. Health Insurance charges rise sharply at this threshold\n\nIs there an important threshold value?\n\n1 value ‚Äì&gt; split into a binary\nMultiple values ‚Äì&gt; Multinomial\n\nExamples\n\nBinary\n\nWhether a user spent more than $50 or didn‚Äôt (See Charts &gt;&gt; Categorical Predictors vs Outcome)\nIf user had activity on the weekend or not\n\nMultinomial\n\nTimestamp to morning/afternoon/ night,\nOrder values into buckets of $10‚Äì20, $20‚Äì30, $30+\n\n\n\nEmpirical Cumulative Density function (ecdf)\n\nggplot(aes(x = numeric_var, color = cat) +\n¬† ¬† stat_ecdf()\n\nShows the percentage of sample (y-axis) that are below a numeric_var value (x-axis)\n{sfsmisc::ecdf.ksCI} - plots the ecdf and 95% CIs (see Harrell for details of the CI calculation)\nCan view alongside a table of group means to see if the different percentiles differ from the story of just looking at the mean.\ndata %&gt;%\n¬† ¬† group_by(categorical_var) %&gt;%\n¬† ¬† summarize(mean(numeric_var))",
    "crumbs": [
      "EDA",
      "General"
    ]
  },
  {
    "objectID": "qmd/eda-general.html#sec-eda-gen-cat",
    "href": "qmd/eda-general.html#sec-eda-gen-cat",
    "title": "General",
    "section": "Categorical/Discrete Variables",
    "text": "Categorical/Discrete Variables\n\nCount number of rows per category level (or use skimr or DataExplorer)\ntbl %&gt;% count(cat_var, sort = True)\nLooking for how skewed data might be (only a few categories have most of the obs)\nIf levels are imbalanced, consider: initial_split(data, strata = imbalanced_var)\nFor cat vars with levels with too few counts, consider lumping together\n\nLevels with too few data will have large uncertainties about the effect and the bloated std.devs can cause some models to throw errors\n\nCount NAs (or use skimr or DataExplorer)\ntbl %&gt;%\n¬† map_df(~ sum(is.na(.))) %&gt;%\n¬† gather(key = \"feature\", value = \"missing_count\") %&gt;%\n¬† arrange(desc(missing_count))\nVars with too many NAs, may need to be dropped or imputed\n\nSome models don‚Äôt handle NAs\n\nIf the number of NAs is within tolerance and you decide to impute, you need to find out what kind of ‚Äúmissingness‚Äù you have before you choose the imputation method. Some cause issues with certain types of missingness. (e.g.¬†mean and missing-not-at-random (MNAR))\nYear variable\ndata |&gt;\n¬† ¬† count(year) |&gt;\n¬† ¬† arrange(desc(year)) |&gt;\n¬† ¬† ggplot(aes(year, n)) +\n¬† ¬† geom_line()\n\nLooking for skew.\nIs data older or¬†more recent?\n\nFree Text Sometimes these columns are just metadata (a url, product description, etc.), but other times they could have valuable information (e.g.¬†customer feedback). If a column seems like it contains valuable information for your prediction task, you generate features from it text length, appearance/frequency of certain keywords, etc.\n\nTokenize\n\nSee below code for ‚ÄúFacetted bar by variable with counts of the values‚Äù and the use of separate_rows to manually tokenize more useful when the columns don‚Äôt have stopwords\n\n\nVisualize value counts for multiple variables\n\nFacetted bar by variable with counts of the values\n\ncategorical_variables &lt;- board_games %&gt;%\n¬† ¬† ¬† # select all cat vars\n¬† ¬† ¬† select(game_id, name, family, category, artist, designer, mechanic) %&gt;%\n¬† ¬† ¬† # \"type\" receives all colnames; \"value\" receives their values\n¬† ¬† ¬† gather(type, value, -game_id, -name) %&gt;%\n¬† ¬† ¬† filter(!is.na(value)) %&gt;%\n¬† ¬† ¬† # Some values of vars are free text separated by commas; code makes each value into a separate row\n¬† ¬† ¬† separate_rows(value, sep = \",\") %&gt;%\n¬† ¬† ¬† arrange(game_id)\ncategorical_counts &lt;- categorical_variables %&gt;%\n¬† ¬† ¬† count(type, value, sort = TRUE)\n\ncategorical_counts %&gt;%\n¬† ¬† ¬† # type is gathered colnames of the variables\n¬† ¬† ¬† group_by(type) %&gt;%\n¬† ¬† ¬† # high cardinality variables, so only show top 10\n¬† ¬† ¬† top_n(10, n) %&gt;%\n¬† ¬† ¬† ungroup() %&gt;%\n¬† ¬† ¬† mutate(value = fct_reorder(value, n)) %&gt;%\n¬† ¬† ¬† ggplot(aes(value, n, fill = type)) +\n¬† ¬† ¬† geom_col(show.legend = FALSE) +\n¬† ¬† ¬† facet_wrap(~ type, scales = \"free_y\") +\n¬† ¬† ¬† coord_flip() +\n¬† ¬† ¬† labs(title = \"Most common categories\")\n\n‚Äútype‚Äù has the names of the variables, ‚Äúvalue‚Äù has the levels of the variable",
    "crumbs": [
      "EDA",
      "General"
    ]
  },
  {
    "objectID": "qmd/eda-general.html#sec-eda-gen-corr",
    "href": "qmd/eda-general.html#sec-eda-gen-corr",
    "title": "General",
    "section": "Correlation/Association",
    "text": "Correlation/Association\n\nMisc\n\nAlso see\n\nAssociation, General\nNotebook &gt;&gt; Statistical Inference &gt;&gt; Correlation\nInteractions &gt;&gt; Continuous Outcome &gt;&gt; Correlation Heatmaps\n\n{correlationfunnel} - Dancho‚Äôs package; bins numerics, then dummies all character and binned numerics, then runs a pearson correlation vs the outcome variable. Surprisingly it‚Äôs useful to use Pearson correlations for binary variables as long as you have a mix of 1s and 0s in each variable. (Cross-Validated post)\nchurn_df %&gt;%\n¬† ¬† binarize() %&gt;%\n¬† ¬† correlate(&lt;outcome_var&gt;) %&gt;%\n¬† ¬† plot_correlation_funnel()\n\ncorrelate returns a sorted? tibble in case you don‚Äôt want the plot\nThe funnel plot is a way of combining and ranking all the correlation plots into a less eye-taxing visual.\nUses stats::cor for calculation so you can pass args to it and but changing the method (e.g.¬†method = c(\"pearson\", \"kendall\", \"spearman\") ) won‚Äôt matter, since pearson and spearman (and probably kendall) will be identical for binary variables.\n\nFor binary vs.¬†binary, also see Association, General &gt;&gt; Discrete &gt;&gt; Binary Similarity Measures and Cramer‚Äôs V\n\nPairwise plots for patterns\n\nOutcome vs Predictor\nPredictor vs Predictor\n\nInteractions\nMulticollinearity\n\nCorrelation/Association scores for linear relationships\nHistograms for variations between categories\nExample: {{ggforce}}\n\nggplot(palmerpenguins::penguins, aes(x = .panel_x, y = .panel_y)) +\n  geom_point(aes(color = species), alpha = .5) +\n  geom_smooth(aes(color = species), method = \"lm\") +\n  ggforce::geom_autodensity(aes(color = species, fill = after_scale(color)), alpha = .7) +\n  scale_color_brewer(palette = \"Set2\", name = NULL) +\n  ggforce::facet_matrix(vars(names), layer.lower = 2, layer.diag = 3)\n\nLinear\n\n{greybox} for testing correlation between different types of variables\n\nMulticollinearity\n\nVIF (performance::check_collinearity(fit) or greybox::determ or vif(fit))\nUse PCA ‚Äî if only a few (depends on the number of variables) pc explain all or almost all of the variation, then you could have a multicollinearity problem\n\nNonlinear\n\nScatterplots for non-linear patterns,\nCorrelation metrics\nAlso see General Additive Models &gt;&gt; Diagnostics for a method of determining a nonlinear relationship for either continuous or categorical outcomes.\n\nCategorical\n\n2-level x 2-level: Cramer‚Äôs V\n2-level or multi-level x multi-level\n\nChi-square or exact tests\n\nLevels vs Levels correlation\n\nMultiple Correspondence Analysis (MCA) (see bkmks &gt;&gt; Features &gt;&gt; Reduction)\n\nBinary outcome vs Numeric predictors\n# numeric vars should be in a long tbl. Use pivot longer to make two columns (e.g. metric (var names) value (value))\n\nnumeric_gathered %&gt;%\n¬† group_by(metric) %&gt;%\n¬† # rain_tomorrow is the outcome; event_level says which factor level is the event your measuring\n¬† roc_auc(rain_tomorrow, value, event_level = \"second\") %&gt;%\n¬† arrange(desc(.estimate)) %&gt;%\n¬† mutate(metric = fct_reorder(metric, .estimate)) %&gt;%\n¬† ggplot(aes(.estimate, metric)) +\n¬† geom_point() +\n¬† geom_vline(xintercept = .5) +\n¬† labs(x = \"AUC in positive direction\",\n¬† ¬† ¬† title = \"How predictive is each linear predictor by itself?\",\n¬† ¬† ¬† subtitle = \".5 is not predictive at all; &lt;.5 means negatively associated with rain, &gt;.5 means positively associated\")\n\n.5 is not predictive at all; &lt;.5 means negatively associated with rain, &gt;.5 means positively associated\n\n\nOrdinal\n\nPolychoric",
    "crumbs": [
      "EDA",
      "General"
    ]
  },
  {
    "objectID": "qmd/eda-general.html#sec-eda-gen-contout",
    "href": "qmd/eda-general.html#sec-eda-gen-contout",
    "title": "General",
    "section": "Continuous Predictor vs Outcome",
    "text": "Continuous Predictor vs Outcome\n\nMisc\n\nIf the numeric-numeric relation isn‚Äôt linear, then the model will be misspecified: an influential variable may be overlooked or the assumption of linearity may produce a model that fails in important ways to represent the relationship.\nAlso see General Additive Models &gt;&gt; Diagnostics for a method of determining a nonlinear relationship for either continuous or categorical outcomes.\n\n\n\nContinuous Outcome\n\nContinuous vs Continuous by Continuous\n\ncoplot(lat ~ long | depth, data = quakes)\n\ncoplot is base R.\n\nExamples from Six not-so-basic base R functions\n\nThe six plots show the relationship of these two variables for different values of depth\nThe bar plot at the top indicates the range of depth values for each of the plots\nFrom lowest depth to highest depth, the default arrangement of the plots is from bottom row, left to right, and upwards\n\ne.g.¬†The 4th lowest depth is on the top row, farthest to the left.\n\nrows = 1 would arrange all plots in 1 row.\noverlap = 0 will remove overlap between bins\n\nContinuous vs Continuous by Continuous by Continuous\n\ncoplot(lat ~ long | depth * mag, data = quakes, number = c(3, 4))\n\nShows the relationship with depth from left to right and the relationship with magnitude from top to bottom.\nnumber = c(3, 4) says you want 3 bins for depth and 4 bins for mag\nFrom lowest depth, mag to highest depth, mag, the arrangement of the plots is from bottom row, left to right, and upwards\n\ne.g.¬†The 2nd lowest depth (columns) and 3rd lowest mag (rows) is in the 3rd from bottom row and 2nd column.\n\n\nContinuous vs Continuous by Categorical by Categorical\n\ncoplot(flowers ~ weight|nitrogen * treat, data = flowers,\n        panel = function(x, y, ...) {\n        points(x, y, ...)\n        abline(lm(y ~ x), col = \"blue\")})\n\nFrom An Introduction to R\nSame arrangement scheme as the plots above\n\ne.g.¬†nitrogen = ‚Äúmedium‚Äù and treat = ‚Äútip‚Äù is the cell at middle column, top row\n\n\nScagnostics (paper) - metrics to examine numeric vs numeric relationships\n\n{scagnostics}\nScagnostics describe various measures of interest for pairs of variables, based on their appearance on a scatterplot. They are useful tool for discovering interesting or unusual scatterplots from a scatterplot matrix, without having to look at every individual plot\nMetrics: Outlying, Skewed, Clumpy, Sparse, Striated, Convex, Skinny, Stringy, Monotonic\n\n‚ÄúStraight‚Äù (paper) seems to have been swapped for ‚ÄúSparse‚Äù (package)\n\nPotential use cases\n\nFinding linear/nonlinear relationships\nClumping or clustered patterns could indicate an interaction with a categorical variable\n\nScore Guide\n\n\nHigh value: Red\nLow value: Blue\nCouldn‚Äôt find the ranges of these metrics in the paper or the package docs\nShows how scatterplot patterns correspond to metric values\n\n\n\n\n\nCategorical Outcome\n\nFor binary outcome, look for variation between numeric variables and each outcome level\n\n# numeric vars should be in a long tbl.\n# Use pivot longer to make two columns (e.g. metric (var names) value (value)) with the binary outcome (e.g rain_tomorrow) as a separate column\nnumeric_gathered %&gt;%\n¬† ggplot(aes(value, fill = rain_tomorrow)) +\n¬† geom_density(alpha = 0.5) +\n¬† facet_wrap(~ metric, scales = \"free\")\n# + scale_x_log10()\n\nSeparation between the two densities would indicate predictive value.\nIf one of colored density is further to the right than the other then the interpretation would be:\n\nHigher values of metric result in a greater probability of &lt;outcome category of the right-most density&gt;\n\nNormalize the x-axis with rank_percentile(value)\n\nnumeric_gathered %&gt;%\n¬† ¬† mutate(rank = percent_rank(value)) %&gt;%\n¬† ¬† ggplot(aes(rank, fill = churned)) +¬†\n¬† ¬† ¬† geom_density(alpha = 0.5) +¬†\n¬† ¬† ¬† facet_wrap(~ metric, scales = \"free\")\n\nNot sure why you‚Äôd do this unless there was a reason to compare the separation of densities (i.e.¬†strength of association with outcome) between the predictors.\n\n\nEstimated AUC for binary outcome ~ numeric predictor\nnumeric_gathered &lt;- train %&gt;%\n¬† mutate(rainfall = log2(rainfall + 1)) %&gt;%\n¬† gather(metric, value, min_temp, max_temp, rainfall, contains(\"speed\"), contains(\"humidity\"), contains(\"pressure\"), contains(\"cloud\"),¬† ¬† ¬† ¬† contains(\"temp\"))\n\nnumeric_gathered %&gt;%\n¬† group_by(metric) %&gt;%\n¬† # \"rain_tomorrow\" is a binary factor var\n¬† # \"second\" says the event we want the probability for is the second level of the binary factor variable\n¬† yardstick::roc_auc(rain_tomorrow, value, event_level = \"second\") %&gt;%\n¬† arrange(desc(.estimate)) %&gt;%\n¬† mutate(metric = fct_reorder(metric, .estimate)) %&gt;%\n¬† ggplot(aes(.estimate, metric)) +\n¬† geom_point() +\n¬† geom_vline(xintercept = .5) +\n¬† labs(x = \"AUC in positive direction\",\n¬† ¬† ¬† title = \"How predictive is each linear predictor by itself?\",\n¬† ¬† ¬† subtitle = \".5 is not predictive at all; &lt;.5 means negatively associated with rain, &gt;.5 means positively associated\")",
    "crumbs": [
      "EDA",
      "General"
    ]
  },
  {
    "objectID": "qmd/eda-general.html#sec-eda-gen-catout",
    "href": "qmd/eda-general.html#sec-eda-gen-catout",
    "title": "General",
    "section": "Categorical Predictor vs Outcome",
    "text": "Categorical Predictor vs Outcome\n\nContinuous Outcome\n\nBoxplot by Categorical\n\nfct_reorder¬† says order cat_var by a num_var\n\nMake sure data is NOT grouped\n\ndata %&gt;%\n¬† ¬† mutate(cat_var = fct_reorder(cat_var, numeric_outcome)) %&gt;%\n¬† ¬† ggplot(aes(numeric_outcome, cat_var)) +\n¬† ¬† geom_boxplot()\n\nIf all the medians line up then no relationship. A slope or nonlinear shows relationship.\n\nfct_lump¬† can be used to create an ‚Äúother‚Äù group.\n\ndata %&gt;%\n¬† ¬† mutate(cat_var = fct_lump(cat_var, 8),\n¬† ¬† ¬† ¬† ¬† cat_var = fct_reorder(cat_var, numeric_outcome)) %&gt;%\n¬† ¬† ggplot(aes(numeric_outcome, cat_var)) +\n¬† ¬† geom_boxplot()\n\nUseful for cat_vars with too many levels which can muck-up a graph\nSays to keep the top 8 levels with the highest counts and put rest in ‚Äúother‚Äù.\n\nAlso takes proportions. Negative values says keep lowest.\n\n\n\nBoxplot by Categorical (Titanic5 dataset)\n\n\nY-Axis is the ‚ÄúClass‚Äù categorical with 3 levels\nFor ticket price, only class 1 shows any variation\nFor Age, there‚Äôs a clear trend but also considerable overlap between classes\n\nDensity Plot + One-Way ANOVA\n\n\nThe ANOVA confirms that all mpg means stratified by country are different (pval &lt; 0.05) and the density plot visualizes that a difference in means is very likely due to the US and there‚Äôs also potentially a difference between Japan and Europe.\n\n\n\n\nCategorical Outcome\n\nHistograms of cat_vars split by response_var¬†\ndf %&gt;%\n¬† ¬† select(cat_vars) %&gt;%\n¬† ¬† pivot_longer(key, value = cat_vars, response_var) %&gt;%\n¬† ¬† ggplot(aes(value)) +\n¬† ¬† geom_bar(fill = response_var) +\n¬† ¬† facet_wrap( ~key, scales = \"free\")\n\nJust looking for variation in the levels of the cat_var given response var. More variation = more likely to be a better predictor\nEach facet will be a level of the response variable\n\nError Bar Plot\n# outcome variable is a binary for whether or not it rained on that day\ngroup_binary_prop &lt;- function(tbl) {\n¬† ¬† ret &lt;- tbl %&gt;%\n¬† ¬† ¬† ¬† # count of events for each category (successes)\n¬† ¬† ¬† ¬† summarize(n_rain = sum(rain_tomorrow == \"Rained\"),\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† # count of rows for each category (trials)\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† n = n()) %&gt;%\n¬† ¬† ¬† ¬† arrange(desc(n)) %&gt;%\n¬† ¬† ¬† ¬† ungroup() %&gt;%\n¬† ¬† ¬† ¬† # probability of event for each category\n¬† ¬† ¬† ¬† mutate(pct_rain = n_rain / n,\n¬† ¬† ¬† ¬† ¬† ¬† ¬† # jeffreys interval\n¬† ¬† ¬† ¬† ¬† ¬† ¬† # bayesian CI for binomial proportions\n¬† ¬† ¬† ¬† ¬† ¬† ¬† low = qbeta(.025, n_rain + .5, n - n_rain + .5),\n¬† ¬† ¬† ¬† ¬† ¬† ¬† high = qbeta(.975, n_rain + .5, n - n_rain + .5)) %&gt;%\n¬† ¬† ¬† ¬† # proportion of all events for each category\n¬† ¬† ¬† ¬† mutate(pct = n_rain / sum(n_rain))\n¬† ¬† ¬† ¬† # this was the original but this would just be proportion of the total data for each caategory\n¬† ¬† ¬† ¬† # mutate(pct = n / sum(n))\n¬† ¬† ret\n}\n\n# error bar plot\n# cat vs probability of event w/CIs\ntrain %&gt;%\n¬† ¬† # cat predictor\n¬† ¬† group_by(location = fct_lump(location, 50)) %&gt;%\n¬† ¬† # apply custom function\n¬† ¬† group_binary_prop() %&gt;%\n¬† ¬† mutate(location = fct_reorder(location, pct_rain)) %&gt;%\n¬† ¬† ggplot(aes(pct_rain, location)) +\n¬† ¬† geom_point(aes(size = pct)) +\n¬† ¬† geom_errorbarh(aes(xmin = low, xmax = high), height = .3) +\n¬† ¬† scale_size_continuous(labels = percent, guide = \"none\", range = c(.5, 4)) +\n¬† ¬† scale_x_continuous(labels = percent) +\n¬† ¬† labs(x = \"Probability of raining tomorrow\",\n¬† ¬† ¬† y = \"\",\n¬† ¬† ¬† title = \"What locations get the most/least rain?\",\n¬† ¬† ¬† subtitle = \"Including 95% confidence intervals. Size of points is proportional to frequency\")\n\nBinary Outcome: Group by cat predictors and calculate proportion of event\nThis needs some tidyeval so it can generalize to other binary(?) outcome vars\n\nSimpler (uncommented) version\nsummarize_churn &lt;- function(tbl) {\n¬† ¬† tbl %&gt;%\n¬† ¬† ¬† ¬† summarize(n = n(),\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† n_churned = sum(churned == \"yes\"),\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† pct_churned = n_churned/n,\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† low = qbeta(.025, n_churned + .5, n - n_churned + .5),¬†\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† high = qbeta(.975, n_churned + .5, n - n_churned + .5)) %&gt;%\n¬† ¬† ¬† ¬† arrange(desc(n))\n}\n\nplot_categorical &lt;- function(tbl, categorical, ...) {\n¬† ¬† tbl %&gt;%¬† ¬† ¬† ¬†\n¬† ¬† ¬† ¬† ggplot(aes(pct_churned, cat_pred), ...) +\n¬† ¬† ¬† ¬† geom_col() +\n¬† ¬† ¬† ¬† geom_errorbar(aes(xmin = low, xmax = high), height = 0.2, color = red) +\n¬† ¬† ¬† ¬† scale_x_continuous(labels = percent) +\n¬† ¬† ¬† ¬† labs(x = \"% in category that churned\")\n}\n\ndata %&gt;%\n¬† ¬† group_by(cat_var) %&gt;%\n¬† ¬† summarize_churn() %&gt;%\n¬† ¬† plot_categorical(cat_var)\nBinary Outcome vs Two Binned Continuous\n\nsummarize_churn &lt;- function(tbl) {\n¬† ¬† tbl %&gt;%\n¬† ¬† ¬† ¬† summarize(n = n(),\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† n_churned = sum(churned == \"yes\"),\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† pct_churned = n_churned/n,\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† low = qbeta(.025, n_churned + .5, n - n_churned + .5),¬†\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† high = qbeta(.975, n_churned + .5, n - n_churned + .5)) %&gt;%\n¬† ¬† ¬† ¬† arrange(desc(n))\n}\n\ndata %&gt;%\n¬† ¬† mutate(avg_trans_amt = total_trans_amt / total_trans_ct,\n¬† ¬† ¬† ¬† ¬† total_transactions = ifelse(total_trans_ct &gt;= 50,\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† \"&gt; 50 Transactions\",\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† \"&lt; 50 Transactions\"),\n¬† ¬† ¬† ¬† ¬† avg_transaction = ifelse(avg_trans_amt &gt;= 50,\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† \"&gt; $50 Average\",\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† \"&lt; $50 Average\")\n¬† ¬† ) %&gt;%\n¬† ¬† group_by(total_transactions,avg_transaction) %&gt;%\n¬† ¬† summarize_churn() %&gt;%\n¬† ¬† ggplot(aes(total_transactions, avg_transaction)) +\n¬† ¬† geom_tile(aes(fill = pct_churned)) +\n¬† ¬† geom_text(aes(label = percent(pct_churned, 1))) +\n¬† ¬† scale_fill_gradient2(low = \"blue\", high = \"red\", midpoint = 0.3) +\n¬† ¬† labs(x = \"How many transactions did the customer do?\",\n¬† ¬† y = \"What was the average transaction size?\",\n¬† ¬† fill = \"% churned\",\n¬† ¬† title = \"Dividing customers into segments\")\n\nSegmentation chart\nEach customer‚Äôs spend is averaged and binned (&gt; or &lt; $50)\nEach customer‚Äôs transaction count is binned (&gt; or &lt; 50)\nThe df is grouped by both binned vars, so you get 4 subgroups\n\nProportions of each subgroup that falls into the event category of then binary variable (e.g.¬†churn) are calculated\nLow and high quantiles for churn counts are calculated (typical calc of CIs for the proportions of binary variables)\n\nUsed to add context of whether these are high proportions, low proportions, etc.",
    "crumbs": [
      "EDA",
      "General"
    ]
  },
  {
    "objectID": "qmd/eda-general.html#sec-eda-gen-inter",
    "href": "qmd/eda-general.html#sec-eda-gen-inter",
    "title": "General",
    "section": "Interactions",
    "text": "Interactions\n\nMisc\n\nY-Axis is the response, X-Axis is the explanatory variable of interest, and the Grouping Variable is the moderator\nInterpretation\n\nSignificant Interactions - The lines of the graph cross or sometimes if they converge (if there‚Äôs enough data/power)\n\nThis pattern is a visual indication that the effects of one IV change as the second IV is varied.\nIf either line has a non-linear pattern (e.g.¬†U-Shaped), yet still cross, it may indicate a non-linear interaction\n\nNon-Significant Interactions - Lines that are close to parallel.\n\nAlso see\n\nRegression, Interactions for details\nDiagnostics, Model Agnostic &gt;&gt; DALEX &gt;&gt; Instance Level &gt;&gt; Break-Down &gt;&gt; Example: Assume Interactions\n\nTypical Format: outcome_mean vs pred_var by pred_var\ndata %&gt;%¬†\n¬† group_by(pred1, pred2) %&gt;%¬†\n¬† summarize(out_mean = mean(outcome)) %&gt;%¬†\n¬† ggplot(aes(y = out_mean, x = pred1, color = pred2)+\n¬† ¬† geom_point() +\n¬† ¬† geom_line()\n\nMay also need a ‚Äúgroup = pred2‚Äù in the aes function\n\n\n\n\nContinuous Outcome\n\nContinuous vs Continuous, Scatter with Smoother by a Categorical\n\nggplot(w, aes(x=age, y=price, color=factor(class))) +\n¬† geom_point() +\n¬† geom_smooth() +\n¬† scale_y_continuous(trans='sqrt') +\n¬† guides(color=guide_legend(title='Class')) +\n¬† hlabs(age, price)\n\nContinuous outcome has been transformed so that the lower values can be more visible\n‚ÄúClass‚Äù == 1 ‚®Ø Age shows some variation but the other two classes do not seem to show much. Lookng at the scatter of red dots, I‚Äôm skeptical that variation being shown by the curve.\n\nAlthough the decent separation of the ‚ÄúClass‚Äù groups may be what indicates an informative interaction\n\n\nContinuous vs Binary by Binary\n\n\nSignificant interaction effect (crossing)\n\nVariable A had no significant effect on participants in Condition B1 but caused a decline from A1 to A2 for those in Condition B2\n\n\nContinuous vs Continuous by Categorical\n\nplot_manufacturer &lt;- function(group) {\n\n  ## check if input is valid\n  if (!group %in% mpg$manufacturer) stop(\"Manufacturer not listed in the data set.\")\n\n  ggplot(mapping = aes(x = hwy, y = displ)) +\n    ## filter for manufacturer of interest\n    geom_point(data = filter(mpg, manufacturer %in% group), \n               color = \"#007cb1\", alpha = .5, size = 4) +\n    ## add shaded points for other data\n    geom_point(data = filter(mpg, !manufacturer %in% group), \n               shape = 1, color = \"grey45\", size = 2) +\n    scale_x_continuous(breaks = 2:8*5) +\n    ## add title automatically based on subset choice\n    labs(x = \"Highway gallons\", y = \"Displacement\", \n         title = group, color = NULL)\n}\n\ngroups &lt;- unique(mpg$manufacturer)\nmap(groups, ~plot_manufacturer(group = .x))\n\nThe grouping variable is the facet variable but also highlights the dots with color\nHighlighting plus using all the data in each chart helps add context with the other groups when you want to compare groups but in a low data situation.\n\nContinuous vs Continuous by Ordinal\n\nplot_scatter_lm &lt;- function(data, var1, var2, pointsize = 2, transparency = .5, color = \"\") {\n\n  ## check if inputs are valid\n  if (!is.data.frame(data)) stop(\"data needs to be a data frame.\")\n  if (!is.numeric(pull(data[var1]))) stop(\"Column var1 needs to be of type numeric, passed as string.\")\n  if (!is.numeric(pull(data[var2]))) stop(\"Column var2 needs to be of type numeric, passed as string.\")\n  if (!is.numeric(pointsize)) stop(\"pointsize needs to be of type numeric.\")\n  if (!is.numeric(transparency)) stop(\"transparency needs to be of type numeric.\")\n  if (color != \"\") { if (!color %in% names(data)) stop(\"Column color needs to be a column of data, passed as string.\") }\n\n  g &lt;- \n    ggplot(data, aes(x = !!sym(var1), y = !!sym(var2))) +\n    geom_point(aes(color = !!sym(color)), size = pointsize, alpha = transparency) +\n    geom_smooth(aes(color = !!sym(color), color = after_scale(prismatic::clr_darken(color, .3))), \n                method = \"lm\", se = FALSE) +\n    theme_minimal(base_family = \"Roboto Condensed\", base_size = 15) +\n    theme(panel.grid.minor = element_blank(),\n          legend.position = \"top\")\n\n  if (color != \"\") { \n    if (is.numeric(pull(data[color]))) {\n      g &lt;- g + scale_color_viridis_c(direction = -1, end = .85) +\n        guides(color = guide_colorbar(\n          barwidth = unit(12, \"lines\"), barheight = unit(.6, \"lines\"), title.position = \"top\"\n        ))\n    } else {\n      g &lt;- g + scale_color_brewer(palette = \"Set2\")\n    }\n  }\n\n  return(g)\n}\n\nmap2(\n  c(\"displ\", \"displ\", \"hwy\"), \n  c(\"hwy\", \"cty\", \"cty\"),\n  ~plot_scatter_lm(\n    data = mpg, var1 = .x, var2 = .y, \n    color = \"cyl\", pointsize = 3.5\n  )\n)\n\nA continuous color scale is used for the ordinal variable\nTrend shows relationship follows the ordinal variable values for the most part which might indicate that this interaction would be predictive\n\nInteresting values might be at dots where the colors are swapped ‚Äî defying the order of the ordinal variable\n\n\nContinuous vs Continuous by Categorical by Categorical\n\nplot_manufacturer_marginal &lt;- function(group, save = FALSE) {\n\n  ## check if input is valid\n  if (!group %in% mpg$manufacturer) stop(\"Manufacturer not listed in the data set.\")\n  if (!is.logical(save)) stop(\"save should be either TRUE or FALSE.\")\n\n  ## filter data\n  data &lt;- filter(mpg, manufacturer %in% group)\n\n  ## set limits\n  lims_x &lt;- range(mpg$hwy) \n  lims_y &lt;- range(mpg$displ)\n\n  ## define colors\n  pal &lt;- RColorBrewer::brewer.pal(n = n_distinct(mpg$class), name = \"Dark2\")\n  names(pal) &lt;- unique(mpg$class)\n\n  ## scatter plot\n  main &lt;- ggplot(data, aes(x = hwy, y = displ, color = class)) +\n    geom_point(size = 3, alpha = .5) +\n    scale_x_continuous(limits = lims_x, breaks = 2:8*5) +\n    scale_y_continuous(limits = lims_y) +\n    scale_color_manual(values = pal, name = NULL) +\n    labs(x = \"Highway miles per gallon\", y = \"Displacement\") +\n    theme(legend.position = \"bottom\")\n\n  ## boxplots\n  right &lt;- ggplot(data, aes(x = manufacturer, y = displ)) +\n    geom_boxplot(linewidth = .7, color = \"grey45\") +\n    scale_y_continuous(limits = lims_y, guide = \"none\", name = NULL) +\n    scale_x_discrete(guide = \"none\", name = NULL) +\n    theme_void()\n\n  top &lt;- ggplot(data, aes(x = hwy, y = manufacturer)) +\n    geom_boxplot(linewidth = .7, color = \"grey45\") +\n    scale_x_continuous(limits = lims_x, guide = \"none\", name = NULL) +\n    scale_y_discrete(guide = \"none\", name = NULL) +\n    theme_void()\n\n  ## combine plots\n  p &lt;- top + plot_spacer() + main + right + \n    plot_annotation(title = group) + \n    plot_layout(widths = c(1, .05), heights = c(.1, 1))\n\n  ## save multi-panel plot\n  if (isTRUE(save)) {\n    ggsave(p, filename = paste0(group, \".pdf\"), \n           width = 6, height = 6, device = cairo_pdf)\n  }\n\n  return(p)\n}\n\nplot_manufacturer_marginal(\"Dodge\")\n\n{ggside} should be able to add these marginal plots with fewer lines of code.\nThis is one of a set of facetted charts by the categorical, ‚Äúmanufacturer‚Äù\nDots are grouped by categorical, ‚Äúclass‚Äù\nTop boxplot shows a minivan as an outlier in terms of hwy mpg.\nBox plots and the scatter plot are combined using {patchwork}\n\nCorrelation Heatmaps\n\nFilter data by different levels of a categorical, then note how correlations between numeric predictors and the numeric outcome change\nExample: PM 2.5 pollution (outcome) vs complete dataset and filtered for Wind Direction = NE\n\nComplete\n\nWind Direction = NE\n\nInterpretation\n\nTemperature‚Äôs correlation (potentially its predictive strength) would lessen if would be interacted with Wind Direction. So we do NOT want to interact wind direction and temperature\n\nArticle didn‚Äôt show whether it increases with other directions\n\nWind Strength‚Äôs (cws) correlation with the outcome would increase if interacted with Wind Direction. So we do want to interacted wind direction and wind strength\n\nFor ML, I think you‚Äôd dummy the wind direction, then multiply windspeed times each of the dummies.\n\n\n\n\nBoxplot by Discrete (Binned) Continuous\n\npmin can be similarily used as fct_lump (see below) but for discrete integer variables\n\nIf the distribution of the discrete numeric is skewed to the right, then pmin will bin all integers larger than some number\n\nMost of the distribution are small integers and the rest will be binned into a sort of ‚Äúother‚Äù category (e.g.¬†14)\n\nIf the distribution is skewed to the left, pmax can be used similarily.\n\ndata %&gt;%\n¬† ¬†mutate(integer_var = pmin(integer_var, 14) %&gt;%\n¬† ¬†ggplot(aes(int_var, numeric_outcome, group = int_var)) +\n¬† ¬†geom_boxplot()\n\nIf all the medians line up then no relationship. A slope or nonlinear pattern shows relationship.\n\n\n\n\n\n\nCategorical Outcome\n\nNumeric vs Numeric by Cat Outcome\n\nScatter with 45 degree line\nggplot(aes(num_predictor1, num_predictor2, color = cat_outcome_var)) +\n¬† ¬†geom_point() +\n¬† ¬†geom_abline(color = \"red\")\n\nLook for groupings or other patterns wrt to cat var.\nCat-var colored points above line skew more towards the higher y-var than x-var and vice versa for below the 45 degree line.\nLine also shows how linearly correlated the two num vars are.\nIf clustering present, could indicate a good interaction pair with the numeric : cat_var\n\nScatter with linear smooth (or loess)\n\nggplot(aes(num_predictor1, num_predictor2)) +\n¬† ¬†geom_point(alpha = 0.25) +\n¬† ¬†geom_smooth(aes(color = cat_outcome_var), method = \"lm\")\n\nProduces a lm line for each outcome var category\nLooking for differing trends for ranges of values on the x-axis. A pattern for one line that is substantially different from the other line\nExample: At around 28, the blue line trend rises while the red line continues to slope downwards, and they actually cross to where at some threshold of x, the relationship is the opposite. So an interaction is likely present\n\n\nBinary Outcome (pct_event) vs Discrete by Discrete (or binary in this case)\n\ndata %&gt;%\n¬† ¬† mutate(avg_trans_amt = total_trans_amt / total_trans_ct) %?%\n¬† ¬† group_by(total_trans_ct = cut(total_trans_ct, c(0,30, 40, 50, 60, 80, Inf)),\n¬† ¬† ¬† ¬† ¬† ¬† avg_trans_amt = ifelse(avg_trans_amt &gt;= 50, \"&gt; $50\", \"&lt; $50\") %&gt;%\n¬† ¬† ¬† ¬† ¬† ¬† # use to figure out best cut point(s) that keeps the ribbon width small-ish on all lines\n¬† ¬† ¬† ¬† ¬† ¬† # avg_trans_amt = cut(avg_trans_amt, c(0, 50, 100, 130, Inf)) %&gt;%¬† ¬†\n¬† ¬† summarize(n = n(),\n¬† ¬† ¬† ¬† ¬† ¬† ¬† n_churned = sum(churned == \"yes\"),\n¬† ¬† ¬† ¬† ¬† ¬† ¬† pct_churned = n_churned/n,\n¬† ¬† ¬† ¬† ¬† ¬† ¬† low = qbeta(.025, n_churned + .5, n - n_churned + .5),¬†\n¬† ¬† ¬† ¬† ¬† ¬† ¬† high = qbeta(.975, n_churned + .5, n - n_churned + .5)) %&gt;%\n¬† ¬† ¬† ¬† arrange(desc(n)) %&gt;%\n¬† ¬† ggplot(aes(total_trans_ct, pct_churned, color = avg_trans_amt) +\n¬† ¬† geom_point() +\n¬† ¬† geom_line() +\n¬† ¬† geom_ribbon(aes(ymin = low, ymax = high))¬† ¬† ¬† ¬† ¬† ¬†\n\nInterpretation:\n\nClear alternating trend from about 0 to 40 on the x-axis says there‚Äôs probably an interaction (at least with the binned versions of these variables) between total_trans_ct and avg_trans_amt.\n\ni.e.¬†The relationship between transaction count and churned (binary outcome) (pct_churned) depends on the average transaction amount\n\n\nExample: The cut points for avg_trans_amt were chosen from its distribution\n\nThe distribution was bi-modal and the 3 cutpoints were the 1st mode, point that splits both modal distributions, and the 2nd mode.\n{Upsetr} might be useful to examine bimodal structure and determine cutpoints based on categorical predictor values and not just outcome values\n{gghdr} - viz for multi-modal distribtutions\nAlso see Regression, Other &gt;&gt; Mult-Modal\n\nExample of likely no interaction\n\n\nBlue and red lines move in unison. Same trend directions.\n\nThere is separation, so the mean value of percent churn is different. Also, the slopes are different, so the rates of increase and decrease would be different. I‚Äôm not convinced. I‚Äôd like to see if an interaction term wouldn‚Äôt be significant\nkaggle sliced s01e07 dataset - percent churn (y-axis), revolving balance bucketed (x-axis), color = total_transactions dicotomized. DRob video for the code.\n\n\n\nBinary Outcome (pct_event) vs Categorical by Categorical\n\nSliding Window Continuous vs Binary Outcome (Proportion of Event) by Categorical\n\nggplot(z, aes(x=price, y=`Moving Proportion`, col=factor(class))) +\n¬† geom_line() + guides(color=guide_legend(title='Class')) +\n¬† xlab(hlab(price)) + ylab('Survival')\n\n‚ÄúMoving Proportion‚Äù is the mean of the binary outcome (probability of an event) over a sliding window of ‚ÄúTotal Price‚Äù\n‚ÄúTotal Price‚Äù should be sorted in ascending order and grouped by ‚ÄúClass‚Äù before the sliding window is applied\nHarrell uses a default window of 15 observations on either side of the target point, but says the results can be noisy. Recommends passing the results through a smoother\n\nSo, might want to add a geom_smooth to the code chunk\nI might like to see the data points to see how many points at the ends of lines there are. Smoothed lines can be misleading on the boundaries.\n\n\nSliding Window Continuous vs Binary Outcome (Proportion of Event) by 2 Categoricals\n\nggplot(d, aes(x=age, y=`Moving Proportion`, col=factor(class))) +\n¬† geom_smooth() +\n¬† facet_wrap(~ sex) +\n¬† ylim(0, 1) + xlab(hlab(age)) + ylab('Survival') +\n¬† guides(color=guide_legend(title='Class'))\n\nSimilar to above but grouped by 2 variables before the sliding window calculation.\n\nGrouped Bar\n\nsummarize_churn &lt;- function(tbl) {\n¬† ¬† tbl %&gt;%\n¬† ¬† ¬† ¬† summarize(n = n(),\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† n_churned = sum(churned == \"yes\"),\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† pct_churned = n_churned/n,\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† # Jeffrey's Interval (Bayesian CI)\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† low = qbeta(.025, n_churned + .5, n - n_churned + .5),¬†\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† high = qbeta(.975, n_churned + .5, n - n_churned + .5)) %&gt;%\n¬† ¬† ¬† ¬† arrange(desc(n))\n}\nplot_categorical &lt;- function(tbl, categorical, ...) {\n¬† ¬† tbl %&gt;%¬† ¬† ¬† ¬†\n¬† ¬† ¬† ¬† ggplot(aes(pct_churned, [{{categorical}}]{style='color: goldenrod'}), ...) +¬†\n¬† ¬† ¬† ¬† geom_col(position = position_dodge()) +¬†\n¬† ¬† ¬† ¬† geom_errorbar(aes(xmin = low, xmax = high),\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† height = 0.2, color = red,\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† position = position_dodge(width = 1) +\n¬† ¬† ¬† ¬† scale_x_continuous(labels = percent) +\n¬† ¬† ¬† ¬† labs(x = \"% in category that churned\")\n}\ndata %&gt;%\n¬† ¬† group_by(cat_var1, cat_var2) %&gt;%\n¬† ¬† summarize_churn() %&gt;%\n¬† ¬† plot_categorical(cat_var1, fill = cat_var2, group = cat_var2)\n\nInterpretation: Probably not an interaction variable. Pct Churned by education Level doesn‚Äôt vary (much) by¬† Gender especially if you take the error bars into account\n\nOnly for ‚Äúcollege‚Äù do you see a flip in the relationship where females churn more than men, but it‚Äôs still within the error bars.\n\n\n\nBinary Outcome vs Binary by Categorical\n\n\nNot certain but I‚Äôd think you‚Äôd want your outcome on the x-axis. Although, if you swapped the x-axis variable with the grouping variable, you‚Äôd probably come to the same conclusion. Therefore, it may not matter that much\nShows percent, and not counts",
    "crumbs": [
      "EDA",
      "General"
    ]
  },
  {
    "objectID": "qmd/db-dbt.html",
    "href": "qmd/db-dbt.html",
    "title": "dbt",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Databases",
      "dbt"
    ]
  },
  {
    "objectID": "qmd/db-dbt.html#sec-db-dbt-misc",
    "href": "qmd/db-dbt.html#sec-db-dbt-misc",
    "title": "dbt",
    "section": "",
    "text": "List of available DB Adaptors\n\nRuns on Python, so adaptors are installed via pip\n\nNotes from\n\nAnatomy of a dbt project\nWhat is dbt?\nDocs\n\nResources\n\nCreate a Local dbt Project\n\nUses docker containers to set up a local dbt project and a local postgres db to play around with\n\n\nArchitecture\n\nTypical Workflow\ndbt deps\ndbt seed\ndbt snapshot\ndbt run\ndbt run-operation {{ macro_name }}\ndbt test\nStyle Guide Components\n\nNaming conventions (the case to use and tense of the column names)\nSQL best practices (commenting code, CTEs, subqueries, etc.)\nDocumentation standards for your models\nData types of date, timestamp, and currency columns\nTimezone standards for all dates\n\nCastor - tool that takes your project and autofills much of the documentation\n\n\nHas a free tier\nVery helpful if you have the same column name in multiple datasets, you don‚Äôt have to keep defining it\nTribal Knowledge\n\nWhen a dataset is discussed in a team slack channel, Castor pulls the comments and adds them to the documentation of the dataset\n\n\nLightdash - BI tool for dbt projects - free tier for self hosting",
    "crumbs": [
      "Databases",
      "dbt"
    ]
  },
  {
    "objectID": "qmd/db-dbt.html#sec-db-dbt-setup",
    "href": "qmd/db-dbt.html#sec-db-dbt-setup",
    "title": "dbt",
    "section": "Set-Up",
    "text": "Set-Up\n\nBasic set-up: Article\n\nExample uses postgres adaptor\n\nWithin a python virtual environment\n\nCreate: python3 -m vevn dbt-venv\nActivate: source dbt-venv/bin/activate\n\nShould be able to see a (dbt-venv) prefix in every line on the terminal\n\nInstall dbt-core: pip install dbt-core\n\nSpecific version: pip install dbt-core==1.3.0\nConfirm installation by checking version: dbt --version\n\nInstall plugins\npip install dbt-bigquery\npip install dbt-spark\n# etc...",
    "crumbs": [
      "Databases",
      "dbt"
    ]
  },
  {
    "objectID": "qmd/db-dbt.html#sec-db-dbt-desc",
    "href": "qmd/db-dbt.html#sec-db-dbt-desc",
    "title": "dbt",
    "section": "Description",
    "text": "Description\n\nBuilt for data modeling\n\nModels are like sql queries\n\nCurrent understanding\n\nData is brought in from warehouses via base models and basic transformations are performed (models &gt;&gt; staging directory)\nThen the data is transformed to the desired state via intermediate models and calculations performed (models &gt;&gt; marts directory)\nThen the final product is stored in the data directory\n\nModularizes SQL code and makes it reusable across ‚Äúmodels‚Äù\n\n\nRunning the orders model also runs the base_orders model and base_payments model (i.e.¬†dependencies for orders)\n\nNot sure this exactly right. Seems like doing this would result in wasting time rerunning the same dependencies multiple times\n\nbase_orders and base_payments are independent in that they can also be used in other models\nCreates more dependable code because you‚Äôre using the same logic in all your models\nMakes runs faster since you aren‚Äôt wasting time and resources running the same blocks of code over and over again\n\nYou can schedule running sets of models by tagging them (e.g.¬†#daily, #weekly)\nVersion Control\n\nSnapshots provide a mechanism for versioning datasets\nWithin every yaml file is an option to include the version\n\nPackage add-ons that allow you to interact with spark, snowflake, duckdb, redshift, etc.\nDocumentation for every step of the way\n\n.yml files can be used to generate a website (localhost:8080) around all of your dbt documentation.\n\ndbt docs generate\ndbt docs serve",
    "crumbs": [
      "Databases",
      "dbt"
    ]
  },
  {
    "objectID": "qmd/db-dbt.html#sec-db-dbt-opt",
    "href": "qmd/db-dbt.html#sec-db-dbt-opt",
    "title": "dbt",
    "section": "Optimizations",
    "text": "Optimizations\n\nRuns parallelized\n\nModels that have dependencies aren‚Äôt run until their upstream models are completed but models that don‚Äôt depend on one another are run at the same time.\nThe thread parameter in your dbt_project.yml specifies how many models are permitted to run in parallel",
    "crumbs": [
      "Databases",
      "dbt"
    ]
  },
  {
    "objectID": "qmd/db-dbt.html#sec-db-dbt-projfil",
    "href": "qmd/db-dbt.html#sec-db-dbt-projfil",
    "title": "dbt",
    "section": "Project Files",
    "text": "Project Files\n\nProject Templates\n\nStyle Guide\n\nMore detailed: link\n\nExample Starter Project\n\n\nprofiles.yml\n\nNot included in project directory\nOnly have to worry about this file if you set up dbt locally.\ndoc\nCreated by dbt init in ~/.dbt/\nContents\n\ndatabase connection, database credentials that dbt will use to connect to the data warehouse\nIf you work on multiple projects locally, the different project names (configured in the dbt_project.yml file) will allow you to set up various profiles for other projects.\n‚Ä¶ something about ‚Äútargets‚Äù but not sure what this is or how it‚Äôs used\n\n\ndbt_project.yml\n\nDocs\nMain configuration file for your project\nFill in your project name and profile name\n\nAdd variables and models\n\npackages.yml\n\nList of external dbt packages you want to use in your project\nFormat\npackages:\n¬† ¬† - package: dbt-labs/dbt_utils\n¬† ¬† ¬† version: 0.7.3",
    "crumbs": [
      "Databases",
      "dbt"
    ]
  },
  {
    "objectID": "qmd/db-dbt.html#sec-db-dbt-comp",
    "href": "qmd/db-dbt.html#sec-db-dbt-comp",
    "title": "dbt",
    "section": "Components",
    "text": "Components\n\nVariables\n\nDefined in the project.yml and used in models\n\nExample: Assigning States to Regions\nvars:\n¬† state_lookup:\n¬† ¬† Northeast:\n¬† ¬† ¬† - CT\n¬† ¬† ¬† - ME\n¬† ¬† Midwest:\n¬† ¬† ¬† - IL\n¬† ¬† ¬† - IN\n\nUsing the variables in a model\n{# Option 1 #}\nSELECT state,\n¬† ¬† ¬† CASE {% for k, v in var(\"state_lookup\").items() %}\n¬† ¬† ¬† ¬† ¬† ¬† WHEN state in ({% for t in v %}'{{ t }}'{% if not loop.last %}, {% endif %}{% endfor %}) THEN {{ k }}{% endfor %}\n¬† ¬† ¬† ¬† ¬† ¬† ELSE NULL END AS region\n¬† FROM {{ ref('my_table') }}\n\n{# Option 2 #}\nSELECT state,\n¬† ¬† ¬† CASE {% for k, v in var(\"state_lookup\").items() %}\n¬† ¬† ¬† ¬† ¬† ¬† WHEN state in ({{ t|csl }}) THEN {{ k }}{% endfor %}\n¬† ¬† ¬† ¬† ¬† ¬† ELSE NULL END AS region\n¬† FROM {{ ref('my_table') }}\n\nVariables are accessed using var\nThis is a complicated example, see docs for something simpler\n{% ... %} are used to encapsulate for-loops and if-then conditions, see docs\n\n{# ... #} is for comments\n\nOption 2 uses a csl filter (comma-separated-list)\n\n\n\n\nModels\n\nMisc\n\nTagging\n\nAllows you to run groups of models\n\nExampledbt run --models tag:daily\n\n\n\n\n\nBest Practices\n\nModularity where possible\n\nSame as the functional mindset: ‚Äúif there‚Äôs any code that‚Äôs continually repeated, then it should be a function(i.e.¬†its own separate model in dbt).‚Äù\n\nReadability\n\nComment\nUse CTEs instead of subqueries\nUse descriptive names\n\nExample: if you are joining the tables ‚Äúusers‚Äù and ‚Äúaddresses‚Äù in a CTE, you would want to name it ‚Äúusers_joined_addresses‚Äù instead of ‚Äúuser_addresses‚Äù\n\n\nExample: Comments, CTE, Descriptive Naming\nWITH\nActive_users AS (\n¬† SELECT\n¬† ¬† Name AS user_name,\n¬† ¬† Email AS user_email,\n¬† ¬† Phone AS user_phone,\n¬† ¬† Subscription_id\n¬† FROM users\n¬† --- status of 1 means a subscription is active\n¬† WHERE subscription_status = 1\n),\nActive_users_joined_subscriptions AS (\n¬† SELECT\n¬† ¬† Active_users.user_name,\n¬† ¬† active_users.user_email,\n¬† ¬† Subscriptions.subscription_id,\n¬† ¬† subscriptions.start_date ,\n¬† ¬† subscriptions.subscription_length\n¬† FROM active_users\n¬† LEFT JOIN subscriptions\n¬† ¬† ON active_users.subscription_id = subscriptions.subscription_id\n)\nSELECT * FROM Active_users_joined_subscriptions\n\n\n\nCategories\n\nStaging\n\nContains all the individual components of your project that the other layers will use in order to craft more complex data models.\nEach model bears a one-to-one relationship with the source data table it represents (i.e.¬†1 staging model per source)\nTypical Transformations: recasting, column renaming, basic computations (such as KBs to MBs or GBs), categorization (e.g.¬†using CASE WHEN statements).\n\nAggregations and joins should also be avoided\n\nUsually materialized as views.\n\nAllows any intermediate or mart models referencing the staging layer to get access to fresh data and at the same time it saves us space and reduces costs.\n\nExample\n\nBoth the Stripe and Braintree payments are recast into a consistent shape, with consistent column names.\n\n\nMarts\n\nWhere everything comes together in a way that business-defined entities and processes are constructed and made readily available to end users via dashboards or applications.\nSince this layer contains models that are being accessed by end users it means that performance matters. Therefore, it makes sense to materialize them as tables.\n\nIf a table takes too much time to be created (or perhaps it costs too much), then you may also need to consider configuring it as an incremental model.\n\nA mart model should be relatively simple and therefore, too many joins should be avoided\nExample\n\nA monthly recurring revenue (MRR) model that classifies revenue per customer per month as new revenue, upgrades, downgrades, and churn, to understand how a business is performing over time.\n\nIt may be useful to note whether the revenue was collected via Stripe or Braintree, but they are not fundamentally separate models.\n\n\n\nBase/Intermediate\n\nBase\n\nBasic transformations (e.g.¬†cleaning up the names of the columns, casting to different data types)\nOther models use these models as data sources\n\nPrevents errors like accidentally casting your dates to two different types of timestamps, or giving the same column two different names.\n\nTwo different timestamp castings can cause all of the dates to be improperly joined downstream, turning the model into a huge disaster\n\n\nUsually occuring in staging\nRead directly from a source, which is typically a schema in your data warehouse\n\nSource object: { source('campaigns', 'channel') }\n\ncampaigns is the name of the source in the .yml file\nchannel is the name of a table from that source\n\n\n\nIntermediate\n\nBrings together the atomic building blocks that reside on staging layer such that more complex and meaningful models are constructed\nUsually occuring in marts\nAdditional transformations that particular marts-models require\n\nCreated to isolate complex operations\n\nTypically used for joins between multiple base models\n\n\nShould not be directly exposed to end users via dashboards or applications\nOther models should reference them as Common Table Expressions although there may be cases where it makes sense to materialize them as Views\n\nMacros called via run-operation cannot reference ephemeral objects such as CTEs\nRecommended to start with ephemeral objects unless this doesn‚Äôt work for the specific use case\nWhenever you decide to materialize them as Views, it may be easier to to do so in a custom schema, that is a schema outside of the main schema defined in your dbt profile.\n\nIf the same intermediate model is referenced by more than one model then it means your design has probably gone wrong.\n\nUsually indicates that you should consider turning your intermediate model into a macro. reference the base models rather than from a source\nReference object\n\n{ ref('base_campaign_types') }\nbase_campaign_types is a base model\n\n\n\n\n\n\n\n\nMacros\n\nSimilar to functions in excel\nDefine custom functions in the macros folder or override default macros and macros from a package\nSee bkmks for tutorials on writing custom macros with jinja\n{dbtplyr} macros\n\ndplyr tidy selectors, across, etc.\n\n\n\n\nSeeds\n\nSeeds are csv files that you add to your dbt project to be uploaded to your data warehouse.\n\nUploaded into your data warehouse using the dbt seed command\n\nBest suited to static data which changes infrequently.\n\nUse Cases:\n\nA list of unique codes or employee ids that you may need in your analysis but is not present in your current data.\nA list of mappings of country codes to country names\nA list of test emails to exclude from analysis\n\n\nReferenced in downstream models the same way as referencing models ‚Äî by using the ref function",
    "crumbs": [
      "Databases",
      "dbt"
    ]
  },
  {
    "objectID": "qmd/db-dbt.html#sec-db-dbt-dir",
    "href": "qmd/db-dbt.html#sec-db-dbt-dir",
    "title": "dbt",
    "section": "Directories",
    "text": "Directories\n\nModels\n\nSources (i.e.¬†data sources) are defined in src_&lt;source&gt;.yml files in your models directory\n\n.yml files contain definitions and tests\n.doc files contain source documentation\n\nModels (i.e.¬†sql queries) are defined stg_&lt;source&gt;.yml\n\n.yml files contain definitions and tests\n.doc files contain source documentation\nThe actual models are the .sql files\n\nExample\n\n\nStaging:\n\nDifferent data sources will have separate folders underneath staging (e.g.¬†stripe).\n\nMarts:\n\nUse cases or departments have different folders underneath marts (e.g.¬†core or marketing)\n\n\n\nData\n\nContains all manual data that will be loaded to the database by dbt.\nTo load the .csv files in this folder to the database, you will have to run the dbt seed command.\nFor github or other repos, do not put large files or files with sensitive information here\n\nAcceptable use cases: yearly budget, status mappings, category mappings, etc\n\n\nSnapshots\n\nCaptures of the state of a table at a particular time\nDocs\nbuild a slowly changing dimension (SCD) table for sources that do not support change data capture (CDC)\nExample\n\nEvery time the status of an order change, your system overrides it with the new information. In this case, there we cannot know what historical statuses that an order had.\nDaily snapshots of this table builds a history and allows you to track order statuses",
    "crumbs": [
      "Databases",
      "dbt"
    ]
  },
  {
    "objectID": "qmd/db-dbt.html#sec-db-dbt-pkgs",
    "href": "qmd/db-dbt.html#sec-db-dbt-pkgs",
    "title": "dbt",
    "section": "Packages",
    "text": "Packages\n\nAvailable Packages: link\nInstall Packages - dbt deps\nPackages\n\naudit-helper\n\nCompares columns, queries; useful if refactoring code or migrating db\n\ncodegen\n\nGenerate base model, barebones model and source .ymls\n\ndbt-athena - Adaptor for AWS Athena\ndbt-expectations\n\nData validation based on great expectations py lib\n\ndbt-utils\n\nTon of stuff for tests, queries, etc.\n\ndbtplyr - Macros\n\ndplyr tidy selectors, across, etc.\n\ndbt-duckdb - Adapter for duckdb\nexternal-tables\n\nCreate or replace or refresh external tables\nGuessing this means any data source (e.g.¬†s3, spark, google, another db like snowflake, etc.) that isn‚Äôt the primary db connected to the dbt project\n\nlogging\n\nProvides out-of-the-box functionality to log events for all dbt invocations, including run start, run end, model start, and model end.\nCan slow down runs substantially\n\nre_data\n\nDashboard for monitoring, macros, models\n\nprofiler\n\nImplements dbt macros for profiling database relations and creating doc blocks and table schemas (schema.yml) containing said profiles\n\nspark-utils\n\nEnables use of (most of) the {dbt-utils} macros on spark",
    "crumbs": [
      "Databases",
      "dbt"
    ]
  },
  {
    "objectID": "qmd/db-dbt.html#sec-db-dbt-dvaut",
    "href": "qmd/db-dbt.html#sec-db-dbt-dvaut",
    "title": "dbt",
    "section": "Data Validation and Unit Tests",
    "text": "Data Validation and Unit Tests\n\n\nMisc\n\nBuilt-in support for CI/CD pipelines to test your ‚Äúmodels‚Äù and stage them before committing to production\nRun test - dbt test\nSee also\n\nDocs\nPackages\nHow to do Unit Testing in dbt\n\nMost of the tests are defined in a models-type .yml file in the models directory\nCan be applied to a model or a column\n\nMacros: Pre-Made or Custom\n\nCustom (aka Singular) tests should be located in a tests folder.\n\ndbt will evaluate the SQL statement.\nThe test will pass if no row is returned and failed if at least one or more rows are returned.\nUseful for testing for some obscurity in the data\nExample: Check for duplicate rows when joining two tables\nselect\na.id\nfrom {{ ref(‚Äòtable_a‚Äô) }} a\nleft join {{ ref(‚Äòtable_b‚Äô) }} b\non a.b_id = b.id\ngroup by a.id\nhaving count(b.id)&gt;1\n\ni.e.¬†If I join table a with table b, there should only be one record for each unique id in table a\nProcess\n\nJoin the tables on their common field\nGroup them by the id that should be distinct\nCount the number of duplicates created from the join.\n\nThis tells me that something is wrong with the data.\nAdd a having clause to filter out the non-dups\n\n\n\n\n\nMock data\n\n\nData used for unit testing SQL code\nTo ensure completeness, it‚Äôs best if analysts or business stakeholders are the ones provide test cases or test data\nStore in the ‚Äúdata‚Äù folder (typically .csv files)\n\neach CSV file represents one source table\nshould be stored in a separate schema (e.g.¬†unit_testing) from production data\ndbt seed (see below, Other &gt;&gt; seeds) command is used to load mock data into the data warehouse\n\n\nTests\n\nFreshness (docs) - Used to define the acceptable amount of time between the most recent record, and now, for a table\n\nExample\nsources:\n¬† - name: users\n¬† ¬† freshness:\n¬† ¬† ¬† warn_after:\n¬† ¬† ¬† ¬† count: 3\n¬† ¬† ¬† ¬† period: day\n¬† ¬† ¬† error_after:\n¬† ¬† ¬† ¬† count: 5\n¬† ¬† ¬† ¬† period: day\n\n\nExample: project.yml\n\nExample: Unit Test\n\nAdd test to dbt_project.yml\nseeds:\n¬† unit_testing:\n¬† ¬† revenue:\n¬† ¬† ¬† schema: unit_testing\n¬† ¬† ¬† +tags:\n¬† ¬† ¬† ¬† - unit_testing\n\nEvery file in the unit_testing/revenue folder will be loaded into unit_testing\nExecuting dbt build -s +tag:unit_testing will run all the seeds/models/tests/snapshots with tag unit_testing and their upstreams\n\nCreate macro that switches the source data in the model being tested from production data (i.e.¬†using { source() } ) to mock data (i.e.¬†using ref ) when a unit test is being run\n{% macro select_table(source_table, test_table) %}\n¬† ¬† ¬† {% if var('unit_testing', false) == true %}\n\n¬† ¬† ¬† ¬† ¬† ¬† {{ return(test_table) }}\n¬† ¬† ¬† {% else %}\n¬† ¬† ¬† ¬† ¬† ¬† {{ return(source_table) }}\n¬† ¬† ¬† {% endif %}\n{% endmacro %}\n\nArticle calls this file ‚Äúselect_table.sql‚Äù\n2 inputs: ‚Äúsource_table‚Äù (production data) and ‚Äútest_table‚Äù (mock data)\nmacro returns the appropriate table based on the variable in the dbt command\n\nIf the command doesn‚Äôt provide unit_testing variable or the value is false , then it returns source_table , otherwise it returns test_table.\n\n\nAdd macro code chunk to model\n{{ config\n¬† ¬† (\n¬† ¬† ¬† ¬† materialized='table',\n¬† ¬† ¬† ¬† tags=['revenue']\n¬† ¬† )\n}}\n{% set import_transaction = select_table(source('user_xiaoxu','transaction'), ref('revenue_transaction')) %}\n{% set import_vat = select_table(source('user_xiaoxu','vat'), ref('revenue_vat')) %}\nSELECT\n¬† ¬† date\n¬† ¬† , city_name\n¬† ¬† , SUM(amount_net_booking) AS amount_net_booking\n¬† ¬† , SUM(amount_net_booking * (1 - 1/(1 + vat_rate)))¬† AS amount_vat\nFROM {{ import_transaction }}\nLEFT JOIN {{ import_vat }} USING (city_name)\nGROUP BY 1,2\n\nInside the {%...%} , the macro ‚Äúselect_table‚Äù is called to set the local variables, ‚Äúimport_transaction‚Äù and ‚Äúimport_vat‚Äù which are later used in the model query\nModel file is named ‚Äúrevenue2.sql‚Äù\n\nRun model and test using mock data: dbt build -s +tag:unit_testing --vars 'unit_testing: true'\n\nRun model with production data (aka source data): dbt build -s +tag:revenue --exclude tag:unit_testing\n\nCompare output\nversion: 2\nmodels:\n¬† - name: revenue\n¬† ¬† meta:\n¬† ¬† ¬† owner: \"@xiaoxu\"\n¬† ¬† tests:\n¬† ¬† ¬† - dbt_utils.equality:\n¬† ¬† ¬† ¬† ¬† compare_model: ref('revenue_expected')\n¬† ¬† ¬† ¬† ¬† tags: ['unit_testing']\n\nModel properties file that‚Äôs named revenue.yml in the models directory\nBy including tags: [‚Äòunit_testing‚Äô] we can insure that we don‚Äôt run this test in production (see build code above with --exclude tag:unit_testing\n\nMacro for comparing numeric output\n{% test advanced_equality(model, compare_model, round_columns=None) %}\n{% set compare_columns = adapter.get_columns_in_relation(model) | map(attribute='quoted') %}\n{% set compare_cols_csv = compare_columns | join(', ') %}\n{% if round_columns %}\n¬† ¬† {% set round_columns_enriched = [] %}\n¬† ¬† {% for col in round_columns %}\n¬† ¬† ¬† ¬† {% do round_columns_enriched.append('round('+col+')') %}\n¬† ¬† {% endfor %}\n¬† ¬† {% set selected_columns = '* except(' + round_columns|join(', ') + \"), \" + round_columns_enriched|join(', ') %}\n{% else %}\n¬† ¬† {% set round_columns_csv = None %}\n¬† ¬† {% set selected_columns = '*' %}\n{% endif %}\nwith a as (\n¬† ¬† select {{compare_cols_csv}} from {{ model }}\n),\nb as (\n¬† ¬† select {{compare_cols_csv}} from {{ compare_model }}\n),\na_minus_b as (\n¬† ¬† select {{ selected_columns }} from a\n¬† ¬† {{ dbt_utils.except() }}\n¬† ¬† select {{ selected_columns }} from b\n),\nb_minus_a as (\n¬† ¬† select {{ selected_columns }} from b\n¬† ¬† {{ dbt_utils.except() }}\n¬† ¬† select {{ selected_columns }} from a\n),\nunioned as (\n¬† ¬† select 'in_actual_not_in_expected' as which_diff, a_minus_b.* from a_minus_b\n¬† ¬† union all\n¬† ¬† select 'in_expected_not_in_actual' as which_diff, b_minus_a.* from b_minus_a\n)\nselect * from unioned\n{% endtest %}\n\nFile called ‚Äúadvanced_equality.sql‚Äù",
    "crumbs": [
      "Databases",
      "dbt"
    ]
  },
  {
    "objectID": "qmd/db-duckdb.html#duckplyr",
    "href": "qmd/db-duckdb.html#duckplyr",
    "title": "DuckDB",
    "section": "duckplyr",
    "text": "duckplyr\n\nMisc\n\nDocs\n\nShow query plan with explain\nlibrary(\"duckplyr\")\nas_duckplyr_df(data.frame(n=1:10)) |&gt; \n  mutate(m = n + 1) |&gt; \n  filter (m &gt; 5) |&gt; \n  count() |&gt; \n  explain()\n\n#&gt; ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n#&gt; ‚îÇ         PROJECTION        ‚îÇ\n#&gt; ‚îÇ   ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ   ‚îÇ\n#&gt; ‚îÇ             n             ‚îÇ\n#&gt; ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                             \n#&gt; ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n#&gt; ‚îÇ    UNGROUPED_AGGREGATE    ‚îÇ\n#&gt; ‚îÇ   ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ   ‚îÇ\n#&gt; ‚îÇ        count_star()       ‚îÇ\n#&gt; ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                            \n#&gt; ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n#&gt; ‚îÇ           FILTER          ‚îÇ\n#&gt; ‚îÇ   ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ   ‚îÇ\n#&gt; ‚îÇ(+(CAST(n AS DOUBLE), 1.0) ‚îÇ\n#&gt; ‚îÇ           &gt; 5.0)          ‚îÇ\n#&gt; ‚îÇ   ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ   ‚îÇ\n#&gt; ‚îÇ           EC: 10          ‚îÇ\n#&gt; ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                             \n#&gt; ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n#&gt; ‚îÇ     R_DATAFRAME_SCAN      ‚îÇ\n#&gt; ‚îÇ   ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ   ‚îÇ\n#&gt; ‚îÇ         data.frame        ‚îÇ\n#&gt; ‚îÇ   ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ   ‚îÇ\n#&gt; ‚îÇ             n             ‚îÇ\n#&gt; ‚îÇ   ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ   ‚îÇ\n#&gt; ‚îÇ           EC: 10          ‚îÇ\n#&gt; ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\nQuery plan consists of a scan, a filter, projections and an aggregate.\n\nExpressions that aren‚Äôt support won‚Äôt have ‚ÄúProjection‚Äù in the query plan\nas_duckplyr_df(data.frame(n=1:10)) |&gt; \n  mutate(m=(\\(x) x+1)(n)) |&gt; \n  explain()\n\n#&gt; ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n#&gt; ‚îÇ     R_DATAFRAME_SCAN      ‚îÇ\n#&gt; ‚îÇ   ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ   ‚îÇ\n#&gt; ‚îÇ         data.frame        ‚îÇ\n#&gt; ‚îÇ   ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ   ‚îÇ\n#&gt; ‚îÇ             n             ‚îÇ\n#&gt; ‚îÇ             m             ‚îÇ\n#&gt; ‚îÇ   ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ   ‚îÇ\n#&gt; ‚îÇ           EC: 10          ‚îÇ\n#&gt; ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\nThe translation of the anonymous function failed which caused it to fallback to the computation happening in the R engine.\nThere will usually be a performance hit from the fallback due to ‚Äì for example ‚Äì the lack of automatic parallelization",
    "crumbs": [
      "Databases",
      "DuckDB"
    ]
  },
  {
    "objectID": "qmd/google-bigquery.html",
    "href": "qmd/google-bigquery.html",
    "title": "BigQuery",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Google",
      "BigQuery"
    ]
  },
  {
    "objectID": "qmd/google-bigquery.html#sec-goog-bigq-misc",
    "href": "qmd/google-bigquery.html#sec-goog-bigq-misc",
    "title": "BigQuery",
    "section": "",
    "text": "Also see\n\nFinOps: Four Ways to Reduce Your BigQuery Storage Cost\n\n{bigrquery}\nI think query sizes under a 1TB are free\n\nif you go above that, then it‚Äôs cheaper to look at flex spots\n\nData Manipulation Language (DML) - Enables you to update, insert, and delete data from your BigQuery tables. (i.e.¬†transaction operations?) (docs)\nBigQuery vs Cloud SQL\n\nCloud SQL is a service where relational databases can be managed and maintained in Google Cloud Platform. It allows its users to take advantage of the computing power of the Google Cloud Platform instead of setting up their own infrastructure. Cloud SQL supports specific versions of MySQL, PostgreSQL, and SQL Server.\nBigQuery is a cloud data warehouse solution provided by Google. It also comes with a built-in query engine. Bigquery has tools for data analytics and creating dashboards and generating reports. Cloud SQL does not have strong monitoring and metrics logging like BigQuery.\nBigQuery comes with applications within itself, Cloud SQL doesn‚Äôt come with any applications.\nCloud SQL also has more database security options than BigQuery.\nThe storage space in Cloud SQL depends on the db engine being used, while that of Bigquery is equivalent to that of Google cloud storage.\nPricing\n\nBoth have free tiers\nCloudSQL has 2 types: Per Use and Packages\n\nIf usage over 450 hours monthly, then packages is a good option\n\n\n\nLeft pic is Per Use pricing; Right pic is Package pricing\n\n\nBigQuery based on Usage",
    "crumbs": [
      "Google",
      "BigQuery"
    ]
  },
  {
    "objectID": "qmd/google-bigquery.html#sec-goog-bigq-sqlfun",
    "href": "qmd/google-bigquery.html#sec-goog-bigq-sqlfun",
    "title": "BigQuery",
    "section": "SQL Functions",
    "text": "SQL Functions\n\nUNNEST - BigQuery - takes an ARRAY and returns a table with a row for each element in the ARRAY (docs)\n\nGoogle, Analytics, Analysis &gt;&gt; Example 17",
    "crumbs": [
      "Google",
      "BigQuery"
    ]
  },
  {
    "objectID": "qmd/google-bigquery.html#sec-goog-bigq-specexp",
    "href": "qmd/google-bigquery.html#sec-goog-bigq-specexp",
    "title": "BigQuery",
    "section": "BQ Specific Expressions",
    "text": "BQ Specific Expressions\n\nNotation Rules\n\nSquare brackets [ ] indicate optional clauses.\nParentheses ( ) indicate literal parentheses.\nThe vertical bar | indicates a logical OR.\nCurly braces { } enclose a set of options.\nA comma followed by an ellipsis within square brackets [, ‚Ä¶ ] indicates that the preceding item can repeat in a comma-separated list.\n\nUsing EXCEPT within SELECT\n\nPIVOT for pivot tables\n\n\n\nWith more than 1 aggregate\nselect * from (select No_of_Items, Item, City from sale)\n¬† ¬† pivot(sum(No_of_Items) Total_num, AVG(No_of_Items) Avg_num\n¬† ¬† for Item in ('Laptop', 'Mobile'))\n\nUNPIVOT\n\nselect * from sale\nunpivot(Sales_No for Items in (Laptop, TV, Mobile))\n\nIt‚Äôs a pivot_longer function that puts columns, Laptop, TV, and Mobile, into Items and their values into Sales_No\nCollapse columns into fewer categories\n\nselect * from sale\n¬† ¬† unpivot(\n¬† ¬† ¬† ¬† (Category1, Category2)\n¬† ¬† ¬† ¬† for Series\n¬† ¬† ¬† ¬† in ((Laptop, TV) as 'S1', (Tablet, Mobile) as 'S2')\n¬† ¬† )\n\nColumns have been collapsed into 2 categories, S1 and S2\n\n2 columns for each category\n\nValues for each category gets its own column\n\n\nGROUP BY + ROLLUP\n\n\ntotal sales (where quarter = null) and subtotals (by quarter) by year\n\nQUALIFY\n\n\nAllows you to apply it like a WHERE condition on a column created in your SELECT statement because it‚Äôs evaluated after the GROUP BY, HAVING, and WINDOW statements\n\ni.e.¬†a WHERE function that is executed towards the end of the order of operations instead of at the beginning\n\nUsing a WHERE instead of QUALIFY, the above query looks like this",
    "crumbs": [
      "Google",
      "BigQuery"
    ]
  },
  {
    "objectID": "qmd/google-bigquery.html#sec-goog-bigq-vars",
    "href": "qmd/google-bigquery.html#sec-goog-bigq-vars",
    "title": "BigQuery",
    "section": "Variables (aka Parameters)",
    "text": "Variables (aka Parameters)\n\nWays to create variables\n\nUsing a CTE\n\nBasically just using a CTE and calling it a variable or table of variables\n\nUsing BigQuery procedural language\n\n\n\nStatic values using CTE\n\n1 variable, 1 value\n¬† -- Input your own value\nWITH\n¬† variable AS (\n¬† SELECT 250 AS product_threshold)\n\n¬† -- Main Query\nSELECT\n¬† *\nFROM\n¬† `datastic.variables.base_table`,\n¬† variable\nWHERE\n¬† product_revenue &gt;= product_threshold\n\nCTE\n\n‚Äúvariable‚Äù is the name of the CTE that stores the variable\n‚Äúproduct_threshold‚Äù is¬† set to 250\n\nQuery\n\nThe CTE is called in the FROM statement, then the ‚Äúproduct_threshold‚Äù can be used in the WHERE expression\n\n\n1 variable, multiple values\n-- Multiple values\nWITH\n¬† variable AS (\n¬† SELECT\n¬† ¬† *\n¬† FROM\n¬† ¬† UNNEST([250,45,75]) AS product_threshold)\n\n-- Main Query\nSELECT\n¬† *,\n¬† 'base_table_2' AS table_name\nFROM\n¬† `datastic.variables.base_table_2`,\n¬† variable\nWHERE\n¬† product_revenue IN (product_threshold);\n\nCTE\n\n‚Äúvariable‚Äù is the name of the CTE that stores the variable\nUses SELECT, FROM syntax\nlist of values is unnested into the variable, ‚Äúproduct_threshold‚Äù\n\nSee SQL Functions for UNNEST def\nUNNEST essentially coerces the list into a 1 column vector\n\n\nQuery\n\nThe CTE is called in the FROM statement, then the ‚Äúproduct_threshold‚Äù can be used in the WHERE expression\n\nNot sure why parentheses are around the variable in this case\n\nTable is filtered by values in the variable\n\n\nMultiple variables with multiple values\n¬† -- Multiple variables as a table\nWITH\n¬† variable AS (\n¬† SELECT\n¬† ¬† product_threshold\n¬† FROM\n¬† ¬† UNNEST([\n¬† ¬† ¬† ¬† STRUCT(250 AS price,'Satin Black Ballpoint Pen' AS name),¬†\n¬† ¬† ¬† ¬† STRUCT(45 AS price,'Ballpoint Led Light Pen' AS name),¬†\n¬† ¬† ¬† ¬† STRUCT(75 AS price,'Ballpoint Led Light Pen' AS name)]\n¬† ¬† ¬† ¬† ) AS product_threshold)\n¬† -- Main Query\nSELECT\n¬† *\nFROM\n¬† `datastic.variables.base_table`,\n¬† variable\nWHERE\n¬† product_revenue = product_threshold.price\n¬† AND product_name = product_threshold.name\n\nAlso see (Dynamic values using CTE &gt;&gt; Multiple variables, 1 valuej) where ‚Äútable.variable‚Äù syntax isn‚Äôt used\nCTE\n\n‚Äúvariable‚Äù is the name of the CTE that stores the variable\nInstead of SELECT *, SELECT  is used\n\nnot sure if that‚Äôs necessary or not\n\nUNNEST + STRUCT coerces the array into 2 column table\n\nThe ‚Äúprice‚Äù and ‚Äúname‚Äù variables each have multiple values\nEach STRUCT expression is a row in the table\n\n\nQuery\n\nThe CTE is called in the FROM statement, then the ‚Äúproduct_threshold‚Äù can be used in the WHERE expression\nEach variable is accessed by ‚Äútable.variable‚Äù syntax\nSurprised IN isn‚Äôt used and that you can do this with ‚Äú=‚Äù operator\n\n\n\n\n\nDynamic values using CTE\n\nValue is likely to change when performing these queries with new data\n1 variable, 1 value\n\nExample: value is a statistic of a variable in a table\n¬† -- Calculate twice the average product revenue¬†\nWITH\n¬† variable AS (\n¬† SELECT\n¬† ¬† AVG(product_revenue)*3 AS product_average\n¬† FROM\n¬† ¬† `datastic.variables.base_table`)\n\n-- Main Query\nSELECT\n¬† *\nFROM\n¬† `datastic.variables.base_table`,\n¬† variable\nWHERE\n¬† product_revenue &gt;= product_average\n\nFor basic structure, see (Static values using CTE &gt;&gt; 1 variable, 1 value)\nValue is calculated in the SELECT statement and stored as ‚Äúproduct_average‚Äù\n\n\n1 variable, multiple values\n\nExample: current product names\nWITH\n¬† variable AS (\n¬† SELECT\n¬† ¬† product_name AS product_threshold\n¬† FROM\n¬† ¬† `datastic.variables.base_table`\n¬† WHERE\n¬† ¬† product_name LIKE '%Google%')\n\n-- Main Query\nSELECT\n¬† *\nFROM\n¬† `datastic.variables.base_table`,\n¬† variable\nWHERE\n¬† product_name IN (product_threshold)\n\nFor basic structure, see (Static values using CTE &gt;&gt; 1 variable, multiple values)\nCTE\n\nProduct names with ‚ÄúGoogle‚Äù are stored in ‚Äúproduct_threshold‚Äù\n\n\n\nMultiple variables, 1 value\nWITH\n¬† variable AS (\n¬† SELECT\n¬† ¬† MIN(order_date) AS first_order,\n¬† ¬† MAX(order_date) AS last_order\n¬† FROM\n¬† ¬† `datastic.variables.base_table_2`)\n\n¬† -- Main Query\nSELECT\n¬† a.*\nFROM\n¬† `datastic.variables.base_table` a,\n¬† variable\nWHERE\n¬† order_date BETWEEN first_order\n¬† AND last_order\n\nBasically the same as the 1 variable, 1 value example\nCTE\n\nvariable is the name of the CTE where ‚Äúfirst_order‚Äù and ‚Äúlast_order‚Äù are stored\n\nQuery\n\nNot idea why ‚Äúa.*‚Äù is used here\n\n\n\n\n\nProcedural Language\n\nMisc\n\nDocs\nNotes from\n\nBigQuery SQL Procedural Language to Simplify Data Engineering\n\n\nDeclare/Set\n\nDECLARE statement initializes variables\nSET statement will set the value for the variable\nExample: Basic\n\nExample: SET within IF/THEN conditional\n\n\nChecks if a table had the latest data before running the remaining SQL\nProcedure\n\nchecks the row count of the prod_data table where the daily_date field is equal to 2022‚Äì11‚Äì18 and sets that value to the rowcnt variable\nusing IF-THEN conditional statements\n\nIf rowcnt is equal to 1, meaning if there‚Äôs data found for 2022‚Äì11‚Äì18, then the string FOUND LATEST DATA will be shown.\nElse the latest_date is set to the value of the max date in the prod_data table and DATA DELAYED is displayed along with the value of latest_date.\n\n\nResult: data wasn‚Äôt found and the latest_date field shows 2022‚Äì11‚Äì15.\n\n\nLoop/Leave\n\nExample: Loops until a condition is met before running your SQL statements\n\n\nContinues from 2nd Declare/Set example\nProcedure\n\nA counter variable is added with default = -1\nSubtract days from 2022‚Äì11‚Äì18 using the date_sub function by the counter variable until the rowcnt variable equals 1.\nOnce rowcnt equals 1 the loop ends using the LEAVE statement\n\n\n\nTable Function\n\na user-defined function that returns a table\nDocs\nCan be used anywhere a table input is authorized\n\ne.g.¬†subqueries, joins, select/from, etc.\n\nExample: Creating\nCREATE OR REPLACE TABLE FUNCTION mydataset.names_by_year(y INT64)\nAS\n¬† SELECT year, name, SUM(number) AS total\n¬† FROM `bigquery-public-data.usa_names.usa_1910_current`\n¬† WHERE year = y\n¬† GROUP BY year, name\n\ny is the variable and its type is INT64\n\nExample: Usage\nSELECT * FROM mydataset.names_by_year(1950)\n¬† ORDER BY total DESC\n¬† LIMIT 5\nExample: Delete\nDROP TABLE FUNCTION mydataset.names_by_year",
    "crumbs": [
      "Google",
      "BigQuery"
    ]
  },
  {
    "objectID": "qmd/google-bigquery.html#sec-goog-bigq-remote",
    "href": "qmd/google-bigquery.html#sec-goog-bigq-remote",
    "title": "BigQuery",
    "section": "Remote Functions",
    "text": "Remote Functions\n\nUser defined functions (UDF)\nNotes from\n\nRemote Functions in BigQuery\nBigQuery UDFs Complete Guide\n\nDocs\nUseful in situations where you need to run code outside of BigQuery, and situations where you want to run code written in other languages\nDon‚Äôt want go overboard with remote functions because they have performance and cost disadvantages compared to native SQL UDFs\n\nyou‚Äôll be paying for both BigQuery and Cloud Functions.\n\nUse Cases\n\nInvoke a model on BigQuery data and create a new table with enriched data. This also works for pre-built Google models like Google Translate and Vertex Entity Extraction\n\nNon-ML enrichment use cases include geocoding and entity resolution.\nif your ML model is in TensorFlow, I recommend that you directly load it as a BigQuery ML model. That approach is more efficient than Remote Functions.\n\nLook up real-time information (e.g.¬†stock prices, currency rates) as part of your SQL workflows.\n\nExample: a dashboard or trading application simply calls a SQL query that filters a set of securities and then looks up the real-time price information for stocks that meet the selection criteria\nWITH stocks AS (\nSELECT\n¬† symbol\nWHERE\n¬† ...\n)\nSELECT symbol, realtime_price(symbol) AS price\nFROM stocks\n\nWhere realtime_price is a remote function\n\n\nReplace Scheduled ETL with Dynamic ELT\n\nELT as need can result in a significant reduction in storage and compute costs\n\nImplement hybrid cloud workflows.\n\nMake sure that the service you are invoking can handle the concurrency\n\nInvoking legacy code from SQL",
    "crumbs": [
      "Google",
      "BigQuery"
    ]
  },
  {
    "objectID": "qmd/google-bigquery.html#sec-goog-bigq-flex",
    "href": "qmd/google-bigquery.html#sec-goog-bigq-flex",
    "title": "BigQuery",
    "section": "Flex Slots",
    "text": "Flex Slots\n\nDocs\nFlex slots are like spot instances on aws but for running queries\n\nDocs\nA BigQuery slot is a virtual CPU used by BigQuery to execute SQL queries.\nBigQuery automatically calculates how many slots are required by each query, depending on query size and complexity\n\nUsers on Flat Rate commitments no longer pay for queries by bytes scanned and instead pay for reserved compute resources (‚Äúslots‚Äù and time)\n\nWith on-demand pricing, you pay for the cost of the query and bytes scanned\n\nUsing Flex Slots commitments, users can now cancel the reservation anytime after 60 seconds.\n\nAt $20/500 slot-hours, billed per second, Flex Slots can offer significant cost savings for On-Demand customers whose query sizes exceed 1TiB.\nview reservation assignments on the Capacity Management part of the BigQuery console\n\nAn hour‚Äôs worth of queries on a 500 slot reservation for the same price as a single 4TiB on-demand query (currently priced at $5/TiB)\nExperiment\n\n\n\nNot sure why there aren‚Äôt lower count on-demand slots. Maybe you have to use 2000 slots for on-demand.\nX-axis is the duration of the query\n\n\nYou‚Äôre charged by the minute (I think) with 1 minute being the minimum of Idle time.",
    "crumbs": [
      "Google",
      "BigQuery"
    ]
  },
  {
    "objectID": "qmd/google-bigquery.html#sec-goog-bigq-opt",
    "href": "qmd/google-bigquery.html#sec-goog-bigq-opt",
    "title": "BigQuery",
    "section": "Optimization",
    "text": "Optimization\n\nMisc\n\nAlso see\n\nDatabases, Engineering &gt;&gt; Cost Optimizations\nSQL &gt;&gt; Best Practices\n\nNotes from 14 Ways to Optimize BigQuery SQL Performance\nSet-up Query Monitoring:\n\nGoals\n\nspot expensive/heavy queries executed by anyone from the organization. The data warehouse can be shared among the entire organization including people who don‚Äôt necessarily understand SQL but still try to look for information. An alert is to warn them about the low-quality of the query and Data Engineers can help them with good SQL practices.\nspot expensive/heavy scheduled queries at the early stage. It‚Äôs going to be risky if a scheduled query is very expensive. Having the alerting in place can prevent a high bill at the end of the month.\nunderstand the resource utilization and do a better job on capacity planning.\n\nGuide\n\n‚ÄúBytes shuffled‚Äù affects query time; ‚ÄúBytes processed‚Äù affects query cost\n\nLIMIT speeds up performance, but doesn‚Äôt reduce costs\n\nFor data exploration, consider using BigQuery‚Äôs (free) table preview option instead.\nThe row restriction of LIMIT clause is applied after SQL databases scan the full range of data. Here‚Äôs the kicker ‚Äî most distributed database (including BigQuery) charges based on the data scans but not the outputs, which is why LIMIT doesn‚Äôt help save a dime.\nTable Preview\n\n\nallows you to navigate the table page by page, up to 200 rows at a time and it‚Äôs completely free\n\n\nAvoid using SELECT * . Choose only the relevant columns that you need to avoid unnecessary, costly full table scans\n\nWith row-based dbs, all columns get read anyway, but with columnar dbs, like BigQuery, you don‚Äôt have to read every column.\n\nUse EXISTS instead of COUNT when checking if a value is present\n\nIf you don‚Äôt need the exact count, use EXISTS() because it exits the processing cycle as soon as the first matching row is found\n\nSELECT EXISTS (\n¬† SELECT\n¬† ¬† number\n¬† FROM\n¬† ¬† `bigquery-public-data.crypto_ethereum.blocks`\n¬† WHERE\n¬† ¬† timestamp BETWEEN \"2018-12-01\" AND \"2019-12-31\"\n¬† ¬† AND number = 6857606\n)\nUse Approximate Aggregate Functions\n\nWhen you have a big dataset and you don‚Äôt need the exact count, use approximate aggregate functions instead\nUnlike the usual brute-force approach, approximate aggregate functions use statistics to produce an approximate result instead of an exact result.\n\nExpects the error rate to be ~ 1 to 2%.\n\nAPPROX_COUNT_DISTINCT()\nAPPROX_QUANTILES()\nAPPROX_TOP_COUNT()\nAPPROX_TOP_SUM()\nHYPERLOGLOG++\n\nSELECT\n¬† APPROX_COUNT_DISTINCT(miner)\nFROM\n¬† `bigquery-public-data.crypto_ethereum.blocks`\nWHERE\n¬† timestamp BETWEEN '2019-01-01' AND '2020-01-01'\nReplace Self-Join with Windows Function\n\nSelf-join are always inefficient and should only be used when absolutely necessary. In most cases, we can replace it with a window function.\nA self-join is when a table is joined with itself.\n\nThis is a common join operation when we need a table to reference its own data, usually in a parent-child relationship.\n\nExample\nWITH\n¬† cte_table AS (\n¬† SELECT\n¬† ¬† DATE(timestamp) AS date,\n¬† ¬† miner,\n¬† ¬† COUNT(DISTINCT number) AS block_count\n¬† FROM\n¬† ¬† `bigquery-public-data.crypto_ethereum.blocks`\n¬† WHERE\n¬† ¬† DATE(timestamp) BETWEEN \"2022-03-01\"\n¬† ¬† AND \"2022-03-31\"\n¬† GROUP BY\n¬† ¬† 1,2\n¬† )\n\n/* self-join */\nSELECT\n¬† a.miner,\n¬† a.date AS today,\n¬† a.block_count AS today_count,\n¬† b.date AS tmr,\n¬† b.block_count AS tmr_count,\n¬† b.block_count - a.block_count AS diff\nFROM\n¬† cte_table a\nLEFT JOIN\n¬† cte_table b\n¬† ON\n¬† ¬† DATE_ADD(a.date, INTERVAL 1 DAY) = b.date\n¬† ¬† AND a.miner = b.miner\nORDER BY\n¬† a.miner,\n¬† a.date\n\n/* optimized */\nSELECT\n¬† miner,\n¬† date AS today,\n¬† block_count AS today_count,\n¬† LEAD(date, 1) OVER (PARTITION BY miner ORDER BY date) AS tmr,\n¬† LEAD(block_count, 1) OVER (PARTITION BY miner ORDER BY date) AS tmr_count,\n¬† LEAD(block_count, 1) OVER (PARTITION BY miner ORDER BY date) - block_count AS diff\nFROM\n¬† cte_table a\n\nORDER BY or JOIN on INT64 columns if you can\n\nWhen your use case supports it, always prioritize comparing INT64 because it‚Äôs cheaper to evaluate INT64 data types than strings.\nIf the join keys belong to certain data types that are difficult to compare, then the query becomes slow and expensive.\ni.e.¬†join on an int instead of a string\n\nInstead of NOT IN , use NOT EXISTS operator to write anti-joins because it triggers a more resource-friendly query execution plan\n\nanti-join¬† - a JOIN operator with an exclusion clause WHERE NOT IN , WHERE NOT EXISTS, etc) that removes rows if it has a match in the second table.\nSee article for an example\n\nIn any complex query, filter the data as early in the query as possible\n\nApply filtering functions early and often in your query to reduce data shuffling and wasting compute resources on irrelevant data that doesn‚Äôt contribute to the final query result\ne.g.¬†SELECT DISTINCT , INNER JOIN , WHERE , GROUP BY\n\nExpressions in your WHERE clauses should be ordered with the most selective expression first\n\nDoesn‚Äôt matter except for edge cases (e.g.¬†the example below didn‚Äôt result in a faster query) such as:\n\nIf you have a large number of tables in your query (10 or more).\nIf you have several EXISTS, IN, NOT EXISTS, or NOT IN statements in your WHERE clause\nIf you are using nested CTE (common table expressions) or a large number of CTEs.\nIf you have a large number of sub-queries in your FROM clause.\n\nNot optimized\nWHERE\n¬† miner LIKE '%a%'\n¬† AND miner LIKE '%b%'\n¬† AND miner = '0xc3348b43d3881151224b490e4aa39e03d2b1cdea'\n\nThe LIKE expressions are string searches which are expensive so they should be towards the end\nThe expression with the ‚Äú=‚Äù operator is the ‚Äúmost selective‚Äù expression since it‚Äôs for a particular value of ‚Äúminer,‚Äù so it should be near the beginning\n\nOptimized\nWHERE\n¬† miner = '0xc3348b43d3881151224b490e4aa39e03d2b1cdea'\n¬† AND miner LIKE '%a%'\n¬† AND miner LIKE '%b%'\n\nUtilize PARTITIONS and/or CLUSTERS to significantly reduce amount of data that‚Äôs scanned\n\nMisc\n\nAlso see\n\nDB, Engineering &gt;&gt; Cost Optimization &gt;&gt; Partitions and Indexes for CLUSTER\nSQL &gt;&gt; Partitions\nPartitioning Docs\nClustering Docs\n\nNotes from\n\noriginal optimization article\nHow to Use Partitions and Clusters in BigQuery Using SQL\n\nWhen to use Clustering instead of Partitioning:\n\nYou need more granularity than partitioning allows.\nYour queries commonly use filters or aggregation against multiple columns.\nThe cardinality of the number of values in a column or group of columns is large.\nYou don‚Äôt need strict cost estimates before query execution.\nPartitioning results in a small amount of data per partition (approximately less than 10 GB). Creating many small partitions increases the table‚Äôs metadata, and can affect metadata access times when querying the table.\nPartitioning results in a large number of partitions, exceeding the limits on partitioned tables.\nYour DML operations (See Misc section) frequently modify (for example, every few minutes) most partitions in the table.\n\nUse BOTH partitions and clusters on tables that are bigger than 1 GB to segment and order the data.\n\nFor big tables, it‚Äôs beneficial to both partition and cluster.\n\nLimits\n\n4,000 partitions per table\n4 cluster columns per table\n\nInfo about partititoning and cluster located in Details tab of your table\n\n\nPartitioning\n\nMisc\n\nPartition columns should always be picked based on how you expect to use the data, and not depending on which column would evenly split the data based on size.\n\nExample: partition on county because your analysis or transformations will largely be done by county even though since some counties may be much larger than others and will cause the partitions to be substantially imbalanced.\n\n\nTypes of Partition Keys\n\nTime-Unit Column: Tables are partitioned based on a time value such as timestamps or dates.\n\nDATE,TIMESTAMP, or DATETIME types\n\nIngestion Time: Tables are partitioned based on the timestamp when BigQuery ingests the data.\n\nUses a pseudocolumn named _PARTITIONTIME or _PARTITIONDATE with the value of the ingestion time for each row, truncated to the partition boundary (such as hourly or daily) based on UTC time\n\nInteger Range: Tables are partitioned based on a number. (e.g.¬†customer_id)\n\nExample: Partition by categorical\nCREATE TABLE database.zoo_partitioned\nPARTITION BY zoo_name AS\n¬† (SELECT *\n¬† FROM database.zoo)\nExample: Partition by date; Truncate to month\nCREATE OR REPLACE TABLE\n  `datastic.stackoverflow.questions_partitioned`\nPARTITION BY\nDATE_TRUNC(creation_date,MONTH) AS (\n  SELECT\n  *\n  FROM\n  `datastic.stackoverflow.questions`)\n\ncreation_date is truncated to a month which reduces the number of partitions needed for this table\n\nDays would exceed the 4000 partition limit\n\n\nPartition Options\n\npartition_expiration_days: BigQuery deletes the data in a partition when it expires. This means that data in partitions older than the number of days specified here will be deleted.\nrequire_partition_filter: Users can‚Äôt query without filtering (WHERE clause) on your partition key.\nExample: Set options\nCREATE OR REPLACE TABLE\n¬† `datastic.stackoverflow.questions_partitioned`\nPARTITION BY\nDATE_TRUNC(creation_date,MONTH) OPTIONS(partition_expiration_days=180,\n¬† ¬† require_partition_filter=TRUE) AS (\n¬† SELECT\n¬† ¬† *\n¬† FROM\n¬† ¬† `datastic.stackoverflow.questions`)\n\nALTER TABLE\n¬† `datastic.stackoverflow.questions_partitioned`\nSET\n¬† OPTIONS(require_partition_filter=FALSE,partition_expiration_days=10)\n\nQuerying: Don‚Äôt add a function on top of a partition key\n\nExample: WHERE\n\nBad: WHERE CAST(date AS STRING) = '2023-12-02'\n\ndate, the partition column and filtering column, is transformed\nSeems like a bad practice in general\n\nGood: WHERE date = CAST('2023-01-01' AS DATE)\n\nThe filtering condition is transformed to match the column type\n\n\n\n\nClustering\n\nClustering divides the table into even smaller chunks than partition\nA Clustered Table sorts the data into blocks based on the column (or columns) that we choose and then keeps track of the data through a clustered index.\nDuring a query, the clustered index points to the blocks that contain the data, therefore allowing BigQuery to skip through irrelevant ones. The process of skipping irrelevant blocks on scanning is known as block pruning.\nBest with values that have high cardinality, which means columns with various possible values such as emails, user ids, names, categories of a product, etc‚Ä¶\nAble cluster on multiple columns and you can cluster different data types (STRING, DATE, NUMERIC, etc‚Ä¶)\nExample: Cluster by categorical\nCREATE TABLE database.zoo_clustered\nCLUSTER BY animal_name AS\n¬† (SELECT *\n¬† FROM database.zoo)\nExample: Cluster by tag\nCREATE OR REPLACE TABLE\n¬† `datastic.stackoverflow.questions_clustered`\nCLUSTER BY tags AS (¬†\n¬† SELECT\n¬† ¬† *\n¬† FROM\n¬† ¬† `datastic.stackoverflow.questions`)\n\nPartition and Cluster\n\nExample\nCREATE OR REPLACE TABLE\n¬† `datastic.stackoverflow.questions_partitioned_clustered`\nPARTITION BY\n¬† DATE_TRUNC(creation_date,MONTH)\nCLUSTER BY\n¬† tags AS (\n¬† SELECT\n¬† ¬† *\n¬† FROM\n¬† ¬† `datastic.stackoverflow.questions`)\n\n\nUse ORDER BY only in the outermost query or within window clauses (analytic functions)\n\nOrdering is a resource intensive operation that should be left until the end since tables tend to be larger at the beginning of the query.\nBigQuery‚Äôs SQL Optimizer isn‚Äôt affected by this because it‚Äôs smart enough to recognize and run the order by clauses at the end no matter where they‚Äôre written.\n\nStill a good practice though.\n\n\nPush complex operations, such as regular expressions and mathematical functions to the end of the query\n\ne.g.¬†REGEXP_SUBSTR()¬† and SUM()\n\nUse SEARCH() for nested data\n\nCan search for relevant keywords without having to understand the underlying data schema\n\nTokenizes text data, making it exceptionally easy to find data buried in unstructured text and semi-structured JSON data\n\nTraditionally when dealing with nested structures, we need to understand the table schema in advance, then appropriately flatten any nested data with UNNEST() before running a combination of WHERE and REGEXP clause to search for specific terms. These are all compute-intensive operators.\nExample\n-- old way\nSELECT\n¬† `hash`,\n¬† size,\n¬† outputs\nFROM\n¬† `bigquery-public-data.crypto_bitcoin.transactions`\nCROSS JOIN\n¬† UNNEST(outputs)\nCROSS JOIN\n¬† UNNEST(addresses) AS outputs_address\nWHERE\n¬† block_timestamp_month BETWEEN \"2009-01-01\" AND \"2010-12-31\"\n¬† AND REGEXP_CONTAINS(outputs_address, '1LzBzVqEeuQyjD2mRWHes3dgWrT9titxvq')\n\n-- with search()\nSELECT\n¬† `hash`,\n¬† size,\n¬† outputs\nFROM\n¬† `bigquery-public-data.crypto_bitcoin.transactions`\nWHERE\n¬† block_timestamp_month BETWEEN \"2009-01-01\" AND \"2010-12-31\"\n¬† AND SEARCH(outputs, ‚Äò`1LzBzVqEeuQyjD2mRWHes3dgWrT9titxvq`‚Äô)\nCreate a search index for the column to enable point-lookup text searches\n# To create the search index over existing BQ table\nCREATE SEARCH INDEX my_logs_index ON my_table (my_columns);\n\nCaching\n\nBigQuery has a cost-free, fully managed caching feature for our queries\nBigQuery automatically caches query results into a temporary table that lasts for up to 24 hours after the query has ran.\n\nCan toggle the feature through Query Settings on the Editor UI\n\nCan verify whether cached results are used by checking ‚ÄúJob Information‚Äù after running the query. The Bytes processed should display ‚Äú0 B (results cached)‚Äù.\nNot all queries will be cached. Exceptions include: A query is not cached when it uses non-deterministic functions, such as CURRENT_TIMESTAMP(), because it will return a different value depending on when the query is executed.\n\nWhen the table referenced by the query received streaming inserts because any changes to the table will invalidate the cached results. If you are querying multiple tables using a wildcard.",
    "crumbs": [
      "Google",
      "BigQuery"
    ]
  },
  {
    "objectID": "qmd/google-bigquery.html#sec-goog-bigq-mod",
    "href": "qmd/google-bigquery.html#sec-goog-bigq-mod",
    "title": "BigQuery",
    "section": "Modeling",
    "text": "Modeling\n\nMisc\n\nDocs\nmodel options\n\nTrain/Validation/Test split\n\nCreate or choose a unique column\n\nCreate\n\nUse a random number generator function such as RAND() or UUID()\nCreate a hash of a single already unique field or a hash of a combination of fields that creates a unique row identifier\n\nFARM_FINGERPRINT() is a common function\n\nAlways gives the same results for the same input\nReturns an INT64 value (essentially a number, rather than a combination of numbers and characters) that we can control with other mathematical functions such as MOD() to produce our split ratio.\nOthers don‚Äôt have these qualities, e.g.¬†MD5() or SHA()\n\n\n\n\n\nBigQueryML\n\nSyntax\nCREATE MODEL dataset.model_name\n¬† OPTIONS(model_type=‚Äôlinear_reg‚Äô, input_label_cols=[‚Äòinput_label‚Äô])\nAS SELECT * FROM input_table;\n\nMake predictions with ML.PREDICT\n\nExample: Logistic Regression\nCREATE MODEL `mydata.adults_log_reg`\nOPTIONS(model_type='logistic_reg') AS\nSELECT *,\n¬† ad.income AS label\nFROM\n¬† `mydata.adults_data` ad\n\nOutput\n\n\nModel appears in the sidebar alongside your data table. Click on your model to see an evaluation of the training performance.",
    "crumbs": [
      "Google",
      "BigQuery"
    ]
  },
  {
    "objectID": "qmd/python-pandas.html",
    "href": "qmd/python-pandas.html",
    "title": "Pandas",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Python",
      "Pandas"
    ]
  },
  {
    "objectID": "qmd/python-pandas.html#sec-py-pandas-misc",
    "href": "qmd/python-pandas.html#sec-py-pandas-misc",
    "title": "Pandas",
    "section": "",
    "text": "Examples of using numeric indexes to subset dfs\n\nindexes &gt;&gt; Syntax for using indexes\nselect\nfiltering &gt;&gt; .loc/.iloc\n\ndf.head(8) to see the first 8 rows\ndf.info() is like str in R\ndf.describe() is like summary in R\n\ninclude='all' to include string columns",
    "crumbs": [
      "Python",
      "Pandas"
    ]
  },
  {
    "objectID": "qmd/python-pandas.html#sec-py-pandas-series",
    "href": "qmd/python-pandas.html#sec-py-pandas-series",
    "title": "Pandas",
    "section": "Series",
    "text": "Series\n\na vector with an index (ordered data object)\ns = pd.Series([20, 21, 12],\n¬† ¬† ¬† ¬† ¬† ¬† ¬† index=['London', 'New York', 'Helsinki'])\n&gt;&gt; s\nLondon¬† ¬† ¬† 20\nNew York¬† ¬† 21\nHelsinki¬† ¬† 12\ndtype: int64\nMisc\n\nTwo Pandas Series with the same elements in a different order are not the same object\nList vs Series: list can only use numeric indexes, while the Pandas Series also allows textual indexes.\n\nArgs\n\ndata: Different data structures can be used, such as a list, a dictionary, or even a single value.\nindex: A labeled index for the elements in the series can be defined.\n\nIf not set, the elements will be numbered automatically, starting at zero.\n\ndtype: Sets the data types of the series\n\nUseful if all data in the series are of the same data type\n\nname: Series can be named.\n\nUseful if the Series is to be part of a DataFrame. Then the name is the corresponding column name in the DataFrame.\n\ncopy: True or False; specifies whether the passed data should be saved as a copy or not\n\nSubset: series_1[\"A\"] or series_1[0]\nFind the index of a value: list(series_1).index(&lt;value&gt;)\nChange value of an index: series_1[\"A\"] = 1 (1 is now the value for index, A)\nAdd a value to a series: series_1[\"D\"] = \"fourth_element\" (D is the next index in the sequence)\nFilter by condition(s): series_1[series_1 &gt; 4] or series_1[series_1 &gt; 4][series_1 != 8]\ndict to Series: pd.Series(dict_1)\nSeries to df: pd.DataFrame([series_1, series_2, series_3])\n\nSeries objects should either all have the same index or no index. Otherwise, a separate column will be created for each different index, for which the other rows have no value\n\nMutability\ndf = pd.DataFrame({\"name\": [\"bert\", \"albert\"]})\n\nseries = df[\"name\"]     # shallow copy\nseries[0] = \"roberta\"   # &lt;-- this changes the original DataFrame\n\nseries = df[\"name\"].copy(deep=True)\nseries[0] = \"roberta\"   # &lt;-- this does not change the original DataFrame\n\nseries = df[\"name\"].str.title()  # not a copy whatsoever\nseries[0] = \"roberta\"   # &lt;-- this does not change the original DataFrame\n\nCreating a deep copy will allocate new memory, so it‚Äôs good to reflect whether your script needs to be memory-efficient.",
    "crumbs": [
      "Python",
      "Pandas"
    ]
  },
  {
    "objectID": "qmd/python-pandas.html#sec-py-pandas-cats",
    "href": "qmd/python-pandas.html#sec-py-pandas-cats",
    "title": "Pandas",
    "section": "Categoricals",
    "text": "Categoricals\n\nMisc - Also see - Conversions for converting between types - Optimizations &gt;&gt; Memory Optimizations &gt;&gt; Variable Type\nCategorical (docs)\n\npython version of factors in R\n\nR‚Äôs levels are always of type string, while categories in pandas can be of any dtype.\nR allows for missing values to be included in its levels (pandas‚Äô categories). pandas does not allow NaN categories, but missing values can still be in the values.\n\nSee cat.codes below\n\n\nCreate a categorical series: s = pd.Series([\"a\", \"b\", \"c\", \"a\"], dtype=\"category\")\nCreate df of categoricals\ndf = pd.DataFrame({\"A\": list(\"abca\"), \"B\": list(\"bccd\"){style='color: #990000'}[}]{style='color: #990000'}, dtype=\"category\")\ndf[\"A\"]\n0¬† ¬† a\n1¬† ¬† b\n2¬† ¬† c\n3¬† ¬† a\n\nSpecify categories and add to dataframe (also see Set Categories below)\nraw_cat = pd.Categorical(\n¬† ¬† [\"a\", \"b\", \"c\", \"a\"], categories=[\"b\", \"c\", \"d\"], ordered=False   \n)\ndf[\"B\"] = raw_cat\ndf¬†\n¬† A¬† ¬† B\n0¬† a¬† NaN\n1¬† b¬† ¬† b\n2¬† c¬† ¬† c\n3¬† a¬† NaN\n\nNote that categories not in the specification get NaNs\n\nSee categories, check if ordered: cat.categories, cat.ordered\ns.cat.categories\nOut[61]: Index(['c', 'b', 'a'], dtype='object')\ns.cat.ordered\nOut[62]: False\nSet categories for a categorical: s = s.cat.set_categories([\"one\", \"two\", \"three\", \"four\"])\nRename categories w/cat.rename_categories\n# with a list of new categories\nnew_categories = [\"Group %s\" % g for g in s.cat.categories]\ns = s.cat.rename_categories(new_categories)\n\n# with a dict\ns = s.cat.rename_categories({1: \"x\", 2: \"y\", 3: \"z\"})\ns\n0¬† ¬† Group a\n1¬† ¬† Group b\n2¬† ¬† Group c\n3¬† ¬† Group a\nAdd a category (doesn‚Äôt have to be a string, e.g.¬†4): s = s.cat.add_categories([4])\nRemove categories\ns = s.cat.remove_categories([4])\ns.cat.remove_unused_categories()\nOrdered\n\nCreate ordered categoricals\nfrom pandas.api.types import CategoricalDtype\ns = pd.Series([\"a\", \"b\", \"c\", \"a\"])\ncat_type = CategoricalDtype(categories=[\"b\", \"c\", \"d\"], ordered=True)\ns_cat = s.astype(cat_type)\ns_cat¬†\n0¬† ¬† NaN\n1¬† ¬† ¬† b\n2¬† ¬† ¬† c\n3¬† ¬† NaN\ndtype: category\nCategories (3, object): ['b' &lt; 'c' &lt; 'd']\n\n# alt\ns = pd.Series([\"a\", \"b\", \"c\", \"a\"]).astype(CategoricalDtype(ordered=True))\nReorder: cat.reorder_categories\ns = pd.Series([1, 2, 3, 1], dtype=\"category\")\ns = s.cat.reorder_categories([2, 3, 1], ordered=True)\ns\n0¬† ¬† 1\n1¬† ¬† 2\n2¬† ¬† 3\n3¬† ¬† 1\ndtype: category\nCategories (3, int64): [2 &lt; 3 &lt; 1]\n\nCategory codes: .cat.codes\ns = pd.Series([\"a\", \"b\", np.nan, \"a\"], dtype=\"category\")\ns\n0¬† ¬† ¬† a\n1¬† ¬† ¬† b\n2¬† ¬† NaN\n3¬† ¬† ¬† a\ndtype: category\nCategories (2, object): ['a', 'b']\n\ns.cat.codes\n0¬† ¬† 0\n1¬† ¬† 1\n2¬† -1\n3¬† ¬† 0\ndtype: int8",
    "crumbs": [
      "Python",
      "Pandas"
    ]
  },
  {
    "objectID": "qmd/python-pandas.html#sec-py-pandas-ops",
    "href": "qmd/python-pandas.html#sec-py-pandas-ops",
    "title": "Pandas",
    "section": "Operations",
    "text": "Operations\n\nRead\n\nMisc\n\nFor large datasets, better to use data.table::fread (see Python, Misc &gt;&gt; Misc)\n\nCSV df = pd.read_csv('wb_data.csv', header=0)\n\n‚Äúusecols=[‚Äòcol1‚Äô, ‚Äòcol8‚Äô]‚Äù for only reading certain columns\n\nRead and process data in chunks\nfor chunk in pd.read_csv(\"dummy_dataset.csv\", chunksize=50000):¬†\n¬† ¬† print(type(chunk)) # process data\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\n&lt;class 'pandas.core.frame.DataFrame'&gt;\n\nIssue: Cannot perform operations that need the entire DataFrame. For instance, say you want to perform a groupby() operation on a column. Here, it is possible that rows corresponding to a group may lie in different chunks.\n\n\n\n\nWrite\n\nCSV: df.to_csv(\"file.csv\", sep = \"|\", index = False)\n\n‚Äúsep‚Äù - column delimiter (assume comma is default)\n‚Äúindex=False‚Äù -¬† instructs Pandas to NOT write the index of the DataFrame in the CSV file\n\n\n\n\nCreate/Copy\n\nDataframes\n\nSyntaxes\ndf = pd.DataFrame({\n¬† ¬† \"first_name\": [\"John\",\"jane\",\"emily\",\"Matt\",\"Alex\"],\n¬† ¬† \"last_name\": [\"Doe\",\"doe\",\"uth\",\"Dan\",\"mir\"],\n¬† ¬† \"group\": [\"A-12\",\"B-15\",\"A-18\",\"A-12\",\"C-15\"],\n¬† ¬† \"salary\": [\"$75000\",\"$72000\",\"¬£45000\",\"$77000\",\"¬£58,000\"]\n})\n\ndf = pd.DataFrame(\n¬† ¬† [\n¬† ¬† ¬† ¬† (1, 'A', 10.5, True),\n¬† ¬† ¬† ¬† (2, 'B', 10.0, False),\n¬† ¬† ¬† ¬† (3, 'A', 19.2, False)¬† ¬† ¬† ¬†\n¬† ¬† ],\n¬† ¬† columns=['colA', 'colB', 'colC', 'colD']\n)\n\ndata = pd.DataFrame([[\"A\", 1], [\"A\", 2], [\"B\", 1],¬†\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† [\"C\", 4], [\"A\", 10], [\"B\", 7]],¬†\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† columns = [\"col1\", \"col2\"])\n\ndf = pd.DataFrame([('bird', 389.0),\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ('bird', 24.0),\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ('mammal', 80.5),\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ('mammal', np.nan)],\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† index=['falcon', 'parrot', 'lion', 'monkey'],\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† columns=('class', 'max_speed'))\ndf\n¬† ¬† ¬† ¬† class¬† max_speed\nfalcon¬† ¬† bird¬† ¬† ¬† 389.0\nparrot¬† ¬† bird¬† ¬† ¬† 24.0\nlion¬† ¬† mammal¬† ¬† ¬† 80.5\nmonkey¬† mammal¬† ¬† ¬† ¬† NaN\nCreate a copy of a df: df2 = df1.copy()\n\ndeep=True (default) means it‚Äôs a deep rather than shallow copy. A deep copy is its own object and manipulations to it don‚Äôt affect the original.\n\n\n\n\n\nIndexes\n\nMisc\n\nRow labels\nDataFrame indexes do NOT have to be unique\nThe time it takes to search a dataframe is shorter with unique index\nBy default, the index is a numeric row index\n\nSet\ndf = df.set_index('col1')\ndf.set_index(column, inplace=True)\n\nprevious index is discarded\ninplace=True says modify orginal df\n\nChange indexes but keep previous index\ndf.reset_index() # converts index to a column, index now the default numeric\ndf = df.set_index('col2')\n\nreset_index\ninplace=True says modify orginal df (default = False)\ndrop=True discards old index\n\nChange to new indices or expand indices: df.reindex\nSee indexes: df.index.names\nSort: df.sort_index(ascending=False)\nSyntax for using indexes\n\nMisc\n\n‚Äú:‚Äù¬† is not mandatory as a placeholder for the column position\n\n.loc\n\ndf.loc[one_row_label] # Series\ndf.loc[list_of_row_labels] # DataFrame\ndf.loc[:, one_column_label] # Series\ndf.loc[:, list_of_column_labels] # DataFrame\ndf.loc[first_row:last_row]\n\n.iloc\n\ndf.iloc[one_row_position, :] # Series\ndf.iloc[list_of_row_positions, :] # DataFrame\n\n\nExample: df.iloc[0:3] or df.iloc[:3]\n\noutputs rows 0, 1, 2¬† (end point NOT included)\n\nExample: df.iloc[75:]\n\n(e.g.¬†78 rows) outputs rows 75, 76, 77 (end point, i.e., last row, NOT included)\n\nExample: df.loc['The Fool' : 'The High Priestess']\n\noutputs all rows from ‚ÄúThe Fool‚Äù to ‚ÄúThe High Priestess‚Äù (end point included)\n\nExample: Filtering cells\ndf.loc[\n¬† ¬† ['King of Wands', 'The Fool', 'The Devil'],\n¬† ¬† ['image', 'upright keywords']\n]\n\nindex values: ‚ÄòKing of Wands‚Äô, etc.\ncolumn names: ‚Äòimage‚Äô, etc.\n\nMulti-indexes\n\nUse list of variables to create multi-index: df.set_index(['family', 'vowel_inventories'], inplace=True)\n\n\n‚Äúfamily‚Äù is the 0th index; ‚Äúvowel_inventories‚Äù is the 1st index\n\nSee individual index values: df.index.get_level_values(0)\n\nGets the 0th index values (e.g.¬†‚Äúfamily‚Äù)\n\nSort by specific level: df.sort_index(level=1)\n\nNot specifying a level means df will be sorted first by level 0, then level 1, etc.\n\nReplace index values: df.rename(index={\"Arawakan\" : \"Maipurean\", \"old_value\" : \"new_value\"})\n\nReplaces instances in all levels of the multi-index\nFor only a specific level, use ‚Äúlevels=‚Äòlevel_name‚Äô‚Äù\n\nSubsetting via multi-index\n\nBasic\n\n\nrow_label = ('Indo-European', '2 Average (5-6)')\ndf.loc[row_label, :].tail(3)\nNote: Using ‚Äú:‚Äù or specifying columns may be necessary using a multi-index\nWith multiple multi-index values\nrow_labels = [\n¬† ¬† ('Arawakan', '2 Average (5-6)'),\n¬† ¬† ('Uralic', '2 Average (5-6)'),\n]\nrow_labels = (['Arawakan', 'Uralic'], '2 Average (5-6)') # alternate method\ndf.loc[row_labels, :]\nWith only 1 level of the multi-index\nrow_labels = pd.IndexSlice[:, '1 Small (2-4)']\ndf.loc[row_labels, :]\n\n# drops other level of the multi-index\ndf.loc['1 Small (2-4)']\n\n# more verbose\ndf.xs('1 Small (2-4)', level = 'vowel_inventories', drop_level=False)\n\nIgnores level 0 of the multi-index and filters only by level 1\nCould also use an alias, e.g.¬†idx = pd.IndexSlice if writing the whole thing gets annoying\n\n\n\n\n\nSelect\n\nAlso see Indexes\nSelect by name:\ndf[[\"name\",\"note\"]]\n\ndf.filter([\"Type\", \"Price\"])\n\ndat.loc[:, [\"f1\", \"f2\", \"f3\"]]\n\ntarget_columns = df.columns.str.contains('\\+') # boolean array: Trues, Falses\nX = df.iloc[:, ~target_columns]\nY = df.iloc[:, target_columns]\n\n‚Äú:‚Äù is a place holder in this case\n\nSubset\nts_df\n¬† ¬† t-3¬† t-2¬† t-1¬† t-0\n10¬† 10.0¬† 11.0¬† 12.0¬† 13.0\n11¬† 11.0¬† 12.0¬† 13.0¬† 14.0\n12¬† 12.0¬† 13.0¬† 14.0¬† 15.0\nAll but last column\nts_df.iloc[:, :-1]\n¬† ¬† t-3¬† t-2¬† t-1\n10¬† 10.0¬† 11.0¬† 12.0\n11¬† 11.0¬† 12.0¬† 13.0\n12¬† 12.0¬† 13.0¬† 14.0\n\nFor [:, :-2], the last 2 columns would not be included¬†\n\nAll but first column\nts_df.iloc[:, 1:]\n¬† ¬† t-2¬† t-1¬† t-0\n10¬† 11.0¬† 12.0¬† 13.0\n11¬† 12.0¬† 13.0¬† 14.0\n12¬† 13.0¬† 14.0¬† 15.0\n\ni.e.¬†the 0th indexed column is not included\nFor [:, 2:], the 0th and 1st indexed column would not be included\n\nAll columns between the 2nd col and 4th col\nts_df.iloc[:, 1:4]\n¬† ¬† t-2¬† t-1¬† t-0\n10¬† 11.0¬† 12.0¬† 13.0\n11¬† 12.0¬† 13.0¬† 14.0\n12¬† 13.0¬† 14.0¬† 15.0\n\nleft endpoint (1st index) is included but the right endpoint (4th index) is not\ni.e.¬†the 1st, 2nd, and 3rd indexed columns are included\n\nAll columns except the first 2 cols and the last col\nts_df.iloc[:, 2:-1]\n¬† ¬† t-1\n10¬† 12.0\n11¬† 13.0\n12¬† 14.0\n\ni.e.¬†0th and 1st indexed columns not included and ‚Äú-1‚Äù says don‚Äôt include the last column\n\n\n\n\nRename columns\ndf.rename({'variable': 'Year', 'value': 'GDP'}, axis=1, inplace=True)\n-   \"variable\" is renamed to \"Year\" and \"value\" is renamed to \"GDP\"\n\n\nDelete columns\n\ndf.drop(columns = [\"col1\"])\n\n\n\nFiltering\n\nMisc\n\nAlso see Indexes\nGrouped and regular pandas data frames have different APIs, so it‚Äôs not a guarantee that a method with the same name will do the same thing. üôÉ\n\nOn value: df_filtered = data[data.col1 == \"A\"]\nVia query: df_filtered = data.query(\"col1 == 'A'\")\n\nMore complicated\npurchases\n  .query(\"amount &lt;= amount.median() * 10\")\n# or\n.loc[lambda df: df[\"amount\"] &lt;= df[\"amount\"].median() * 10]\n\nBy group\npurchases\n  .groupby(\"country\")                                               \n  .apply(lambda df: df[df[\"amount\"] &lt;= df[\"amount\"].median() * 10]) \n  .reset_index(drop=True)\n\nFIlters each country by the median of its amount times 10.\nThe result of apply is a dataframe, but that dataframe has the index, country (i.e.¬†rownames). The problem is that the result also has a country column. Therefore using reset_index would move the index to a column, but since there‚Äôs already a country column, it will give you a ValueError. Including drop=True fixes this by dropping the index entirely.\n\nOn multiple values: df_filtered = data[(data.col1 == \"A\") | (data.col1 == \"B\")]\nBy pattern: df[df.profession.str.contains(\"engineer\")]\n\nOnly rows of profession variable with values containing ‚Äúengineer‚Äù\nAlso available\n\nstr.startswith: df_filtered = data[data.col1.str.startswith(\"Jo\")]\nstr.endswith: df_filtered = data[data.col1.str.endswith(\"n\")]\n\nIf column has NAs or NaNs, specify arg, ‚Äúnan=False‚Äù to ignore them, else you‚Äôll get a valueError\nMethods are case-sensitive unless you specify, ‚Äúcase=False:‚Äù\n\nBy negated pattern: df[~df.profession.str.contains(\"engineer\")]\nBy character type: df_filtered = data[data.col1.str.isnumeric()]\n\nAlso available\n\nupper-case: isupper()\nlower-case: islower()\nalphabetic: isalpha()\ndigits: isdigit()\ndecimal: isdecimal()\nwhitespace: isspace()\ntitlecase: istitle()\nalphanumeric: isalnum()\n\n\nBy month of a datetime variable: df[df.date_of_birth.dt.month==11]\n\ndatetime variable has yy-mm-dd format; dt.month accessor used to filter rows of ‚Äúdate_of_birth‚Äù variable wit\n\nConditional: df[df.note &gt; 90]\n\nBy string length: df_filtered = data[data.col1.str.len() &gt; 4]\n\nMulti-conditional\ndf[(df.date_of_birth.dt.year &gt; 2000) &¬†\n¬† (df.profession.str.contains(\"engineer\"))]\n\ndf.query(\"Distance &lt; 2 & Rooms &gt; 2\")\n%in%: df[df.group.isin([\"A\",\"C\"])]\nSmallest 2 values of note column: df.nsmallest(2, \"note\")\n\nnlargest also available\n\nOnly rows in a column with NA values: df[df.profession.isna()]\nOnly rows in a column that aren‚Äôt NA: df[df.profession.notna()]\nFind rows by index value: df.loc['Tony']\n\nloc is for the value, iloc is for position (numerical or logical)\n\nSee Select columns Example for iloc logical case\n\nWith groupby\ndata_grp = data.groupby(\"Company Name\")\ndata_grp.get_group(\"Amazon\")\n\nReturn 1st 3 rows (and 2 columns): df.loc[:3, [\"name\",\"note\"]]\nReturn 1st 3 rows and 3rd column: df.iloc[:3, 2]\n\nh 11th month\n\nUsing index to filter single cell or groups of cells\ndf.loc[\n¬† ¬† ['King of Wands', 'The Fool', 'The Devil'],\n¬† ¬† ['image', 'upright keywords']\n]\n\nindex values: ‚ÄòKing of Wands‚Äô, etc. (aka row names)\ncolumn names: ‚Äòimage‚Äô, etc.\n\n\n\n\nMutate\n\nLooks like all these functions do the same damn thing\nassign e.g.¬†Add columns\ndf[\"col3\"] = df[\"col1\"] + df[\"col2\"]\ndf = df.assign(col3 = df[\"col1\"] + df[\"col2\"])\napply¬† an operation to column(s)\n\nFunction\ndf[['colA', 'colB']] = df[['colA', 'colB']].apply(np.sqrt)\ndf[\"col3\"] = df.apply(my_func, axis=1)\n\ndocs: Series, DataFrame\naxis - arg for applying a function across rowwise or columnwise (default is 0, which is for columnwise)\nThe fact that mutated columns are assigned to the columns of the df are what keeps it from being like summarize\n\nIf np.sum were the function, then the output would be two numbers and this probably wouldn‚Äôt work.\n\n\nWithin a chain\npurchases\n  .groupby(\"country\")\n  .apply(lambda df: (df[\"amount\"] - df[\"discount\"]).sum())\n  .reset_index()\n  .rename(columns={0: \"total\"})\n\nWhen the result of the apply function is coerced to a dataframe by reset_index, the column will be named ‚Äú0‚Äù. rename is used to rename it total.\n\n\nmap\ndf['colE'] = df['colE'].map({'Hello': 'Good Bye', 'Hey': 'Bye'})\n\nEach case of ‚ÄúHello‚Äù is changed to ‚ÄúGood By‚Äù, etc.\nIf you give it a dict, it acts like a case_when or plyr::mapvalues or maybe recode\nValues in the column but aren‚Äôt included in the dict get NaNs\nThis also can take a lambda function\nDocs (only applies to Series, so I guess that means only 1 column at a time(?))\n\napplymap\ndf[['colA', 'colD']] = df[['colA', 'colD']].applymap(lambda x: x**2)\n\nDocs\nSays it applies a function elementwise, so its probably performing a loop\n\nTherefore better to avoid if a vectorized version of the function is available\n\n\n\n\n\nPivot\n\npivot_longer\n\nExample\nyear_list=list(df.iloc[:, 4:].columns)\ndf = pd.melt(df, id_vars=['Country Name','Series Name','Series Code','Country Code'], value_vars=year_list)\n\nyear_list has the variable names of the columns you want to merge into 1 column\n\n\npivot_wider\n\nExample\n# Step 1: add a count column to able to summarize when grouping\nlong_df['count'] = 1\n\n# Step 2: group by date and type and sum\ngrouped_long_df = long_df.groupby(['date', 'type']).sum().reset_index()\n\n# Step 3: build wide format from long format\nwide_df = grouped_long_df.pivot(index='date', columns='type', values='count').reset_index()\n\n‚Äúlong_df‚Äù has two columns: type and date\n\n\n\n\n\nBind_Rows\ndf1 = pd.DataFrame({'strata': 1, 'y': np.random.normal(loc=10, scale=1, size=size){style='color: #990000'}[}]{style='color: #990000'})\ndf2 = pd.DataFrame({'strata': 2, 'y': np.random.normal(loc=15, scale=2, size=size){style='color: #990000'}[}]{style='color: #990000'})\ndf3 = pd.DataFrame({'strata': 3, 'y': np.random.normal(loc=20, scale=3, size=size){style='color: #990000'}[}]{style='color: #990000'})\ndf4 = pd.DataFrame({'strata': 4, 'y': np.random.normal(loc=25, scale=4, size=size){style='color: #990000'}[}]{style='color: #990000'})\ndf = pd.concat([df1, df2, df3, df4])\n\ndf_ls = [df1, df2, df3, df4]\ndf_all = pd.concat([df_ls[i] for i in range(8)], axis=0)\n\n2 variables are created, ‚Äústrata‚Äù and ‚Äúy‚Äù, then merged into a df\nMake sure no rows are duplicates\n\ndf_loans = pd.concat([df, df_pdf], verify_integrity=True)\n\n\nBind_Cols\npd.concat([top_df, more_df], axis=1)\n\n\nCount\n\nvalue.counts: df[\"col_name\"].value_counts()\n\noutput arranged in descending order of frequencies\nfaster than groupby + size\nargs\n\nnormalize=False\ndropna=True\n\n\ngroupby + count\ndf.groupby(\"Product_Category\").size()\ndf.groupby(\"Product_Category\").count()\n\n‚Äúsize‚Äù includes null values\n‚Äúcount‚Äù doesn‚Äôt include null values\narranged by the index column\n\n\n\n\nArrange\n\ndf.sort_values(by = \"col_name\")\n\nargs\n\nascending=True\nignore_index=False (True will reset the index)\n\n\n\n\n\nGroup_By\n\nTo get a dataframe, instead of a series, after performing a grouped calculation, apply reset_index() to the result.\nFind number of groups\ndf_group = df.groupby(\"Product_Category\")\ndf_group.ngroups\ngroupby + count\ndf.groupby(\"Product_Category\").size()\ndf.groupby(\"Product_Category\").count()\n\n‚Äúsize‚Äù includes null values\n‚Äúcount‚Äù doesn‚Äôt include null values\n\nOnly groupby observed categories\ndf.groupby(\"col1\", observed=True)[\"col2\"].sum()\ncol1\nA¬† ¬† 49\nB¬† ¬† 43\nName: col2, dtype: int64\n\ncol1 is a category type and has 3 levels specified (A, B, C) but the column only has As and Bs\nobserved=False (default) would include C and a count of 0.\n\nCount NAs\ndf[\"col1\"] = df[\"col1\"].astype(\"string\")\ndf.groupby(\"col1\", dropna=False)[\"col2\"].sum()\n# output\ncol1\nA¬† ¬† ¬† 49.0\nB¬† ¬† ¬† 43.0\n&lt;NA&gt;¬† ¬† 30.0\nName: col2, dtype: float64\n\n** Will not work with categorical types **\n\nSo categorical variables, you must convert to strings to be able to group and count NAs\n\n\nCombo\ndf.groupby(\"col3\").agg({\"col1\":sum, \"col2\":max})\n\n¬† ¬† ¬† col1¬† col2\ncol3¬† ¬† ¬† ¬† ¬† ¬†\nA¬† ¬† ¬† ¬† 1¬† ¬† 2\nB¬† ¬† ¬† ¬† 8¬† ¬† 10\nAggregate with only 1 function\ndf.groupby(\"Product_Category\")[[\"UnitPrice(USD)\",\"Quantity\"]].mean()\n\nSee summarize for aggregate by more than 1 function\n\nExtract a group category\ndf_group = df.groupby(\"Product_Category\")\ndf_group.get_group('Healthcare')\n# or\ndf[df[\"Product_Category\"]=='Home']\nGroup objects are iterable\ndf_group = df.groupby(\"Product_Category\")\nfor name_of_group, contents_of_group in df_group:\n¬† ¬† print(name_of_group)\n¬† ¬† print(contents_of_group)\nGet summary stats on each category conditional on a column\ndf.groupby(\"Product_Category\")[[\"Quantity\"]].describe()\n\n\n\nSummarize\n\nagg can only operate on one column at time, so for transformations involving multiple columns (e.g.¬†col3 = col1 - col2), see Mutate section.\nWith groupby and agg\n\ndf.groupby(\"cat_col_name\").agg(new_col_name = (\"num_col_name\", \"func_name\"))\n\nTo reset the index¬† (i.e.¬†ungroup)\n\nuse .groupby arg, as_index=False\nuse .reset_index() at the end of the chain (outputs a dataframe instead of a series)\n\narg: drop=TRUE ‚Ä¶does something (maybe drops grouping columns)\n\n\n\n\nGroup aggregate with more than 1 function\ndf.groupby(\"Product_Category\")[[\"Quantity\"]].agg([min,max,sum,'mean'])\n\nWhen you mention ‚Äòmean‚Äô (with quotes), .aggregate() searches for a function mean belonging to pd.Series i.e.¬†pd.Series.mean().\nWhereas, if you mention mean (without quotes), .aggregate() will search for function named mean in default Python, which is unavailable and will throw an NameError exception.\n\nUse a different aggregate function on specific columns\nfunction_dictionary = {'OrderID':'count','Quantity':'mean'}\ndf.groupby(\"Product_Category\").aggregate(function_dictionary)\nFilter by Group, then Summarize by Group (link)\n\nOption 1\npurchases\n  .groupby(\"country\")                                               \n  .apply(lambda df: df[df[\"amount\"] &lt;= df[\"amount\"].median() * 10]) \n  .reset_index(drop=True)                                           \n  .groupby(\"country\")\n  .apply(lambda df: (df[\"amount\"] - df[\"discount\"]).sum())\n  .reset_index()\n  .rename(columns={0: \"total\"})\n\nFIlters each country by the median of its amount times 10.\nFor each country, subtracts a discount from amount, then sums and renames the column from ‚Äú0‚Äù to total.\n\nOption 2\npurchases\n  .assign(country_median=lambda df:                         \n      df.groupby(\"country\")[\"amount\"].transform(\"median\")   \n  )\n  .query(\"amount &lt;= country_median * 10\")                                  \n  .groupby(\"country\")\n  .apply(lambda df: (df[\"amount\"] - df[\"discount\"]).sum())\n  .reset_index()\n  .rename(columns={0: \"total\"})\n\nHere the median amount per country is calculated first and assigned to each row in purchases.\nquery is used to filter by each country‚Äôs median.\nThen, apply and groupby are used in the same manner as option 1.\n\n\n\n\n\nJoin\n\nUsing merge: pd.merge(df1,df2)\n\nMore versatile than join\nAutomatically detects a common column\nMethod: ‚Äúhow = ‚Äòinner‚Äô‚Äù (i.e.¬†default is inner join)\n\n‚Äòouter‚Äô , ‚Äòleft‚Äô , ‚Äòright‚Äô are available\n\nOn columns with different names\n\non = ‚Äúcol_a‚Äù\nleft_on = ‚Äòleft df col name‚Äô\nright_on = ‚Äòright df col name‚Äô\n\ncopy = True (default)\n\nUsing join\ndf1.set_index('Course', inplace=True)\ndf2.set_index('Course', inplace=True)\ndf3 = df1.join(df2, on = 'Course', how = 'left')\n\ninstance method that joins on the indexes of the dataframes\nThe column that we match on for the left dataframe doesn‚Äôt have to be its index. But for the right dataframe, the join key must be its index\nCan use multiple columns as the index by passing a list, e.g.¬†[‚ÄúCourse‚Äù, ‚ÄúStudent_ID‚Äù]\n* indexes do NOT have to be unique *\nFaster than merge\n\n\n\n\nDistinct\n\nFind number of unique values: data.Country.nunique()\nDisplay unique values: df[\"col3\"].unique()\nCreate a boolean column to indicated duplicated rows: df.duplicated(keep=False)\nCheck for duplicate ids: df_loans[df_loans.duplicated(keep=False)].sort_index()\nCount duplicated ids: df_check.index.duplicated().sum()\nDrop duplicated rows: df.drop_duplicates()\n\n\n\nReplace values\n\nIn the whole df\n\n1 value\ndf.replace(to_replace = '?', value = np.nan, inplace=True)\n\nreplaces all values == ? with NaN\nfaster than loc method\n\nMultiple values\ndf.replace([\"Male\", \"Female\"], [\"M\", \"F\"], inplace=True) # list\ndf.replace({\"United States\": \"USA\", \"US\": \"USA\"}, inplace=True) # dict\n\nOnly in specific columns\ndf.replace(\n¬† ¬† {\n¬† ¬† ¬† ¬† \"education\": {\"HS-grad\": \"High school\", \"Some-college\": \"College\"},\n¬† ¬† ¬† ¬† \"income\": {\"&lt;=50K\": 0, \"&gt;50K\": 1},\n¬† ¬† },\n¬† ¬† inplace=True,\n)\n\nreplacement only occurs in ‚Äúeducation‚Äù and ‚Äúincome‚Äù columns\n\nUsing Indexes\n\nExample (2 rows, 1 col): df.loc[['Four of Pentacles', 'Five of Pentacles'], 'suit'] = 'Pentacles'\n\nindex values: ‚ÄúFour of Pentacles‚Äù, ‚ÄúFive of Pentacles‚Äù\ncolumn name: ‚Äúsuit‚Äù\nReplaces the values in those cells with the value ‚ÄúPentacles‚Äù\n\nExample (1 row, 2 cols):\ndf.loc['King of Wands', ['suit', 'reversed_keywords']] = [\n¬† ¬† 'Wands', 'impulsiveness, haste, ruthlessness'\n]\n\nindex value: ‚ÄúKing of Wands‚Äù\ncolumn names: ‚Äúsuit‚Äù, :reversed_keywords‚Äù\nIn ‚Äúsuit,‚Äù replaces value with ‚ÄúWands‚Äù\nIn ‚Äúreversed words,‚Äù replaces value with ‚Äúimpulsiveness, haste, ruthlessness‚Äù\n\n\nReplace Na/NaNs in a column with a constant value\ndf['col_name'].fillna(value = 0.85, inplace = True)\nReplaces Na/NaNs in a column with the value of a function/method\ndf['price'].fillna(value = df.price.median(), inplace = True)\nReplaces Na/NaNs in a column with a group value (e.g.¬†group by fruit, then use price median)\n# median\ndf['price'].fillna(df.groupby('fruit')['price'].transform('median'), inplace = True)\nForward-Fill\ndf['price'].fillna(method = 'ffill', inplace = True)\ndf['price'] = df.groupby('fruit')['price'].ffill(limit = 1)\ndf['price'] = df.groupby('fruit')['price'].ffill()\n\nforward-fill: fills Na/NaN with previous non-Na/non-NaN value\nforward-fill, limited to 1: only fills with the previous value if there‚Äôs a non-Na/non-NaN 1 spot behind it.\n\nMay leave NAs/NaNs\n\nforward-fill by group\n\nBackward-Fill\ndf['price'].fillna(method = 'bfill', inplace = True)\n\nbackward-fill: fills Na/NaN with next non-Na/non-NaN value\n\nAlternating between backward-fill and forward-fill\ndf['price'] = df.groupby('fruit')['price'].ffill().bfill()\n\nalternating methods: for the 1st group a forward fill is performed; for the next group, a backward fill is performed; etc.\n\nInterpolation\n\ndf['price'].interpolate(method = 'linear', inplace = True)\ndf['price'] = df.groupby('fruit')['price'].apply(lambda x: x.interpolate(method='linear')) # by group\ndf['price'] = df.groupby('fruit')['price'].apply(lambda x: x.interpolate(method='linear')).bfill() # by group with backwards-fill\n\nInterpolation (e.g.¬†linear)\n\nMay leave NAs/NaNs\n\nInterpolation by group\nInterpolation + backwards-fill\n\nApply a conditional: says fill na with mean_price where ‚Äúweekday‚Äù column is TRUE; if FALSE, fill with mean_price*1.25\nmean_price = df.groupby('fruit')['price'].transform('mean')\ndf['price'].fillna((mean_price).where(cond = df.weekday, other = mean_price*1.25), inplace = True)\n\n\n\nStrings\n\nMisc\n\nregex is slow\nStored as ‚Äúobject‚Äù type but ‚ÄúStringDtype‚Äù is available. This new Dtype is optional for now but it may be required to do so in the future\nRegex with Examples in python and pandas\n\nFilter (see Filter section)\nReplace pattern using regex\ndf['colB'] = df['colB'].str.replace(r'\\D', '')\ndf['colB'] = df['colB'].replace(r'\\D', r'', regex=True)\n\n‚Äúr‚Äù indicates you‚Äôre using regex\nreplaces all non-numeric patterns (e.g.¬†letters, symbols) with an empty string\nReplace pattern with list comprehension (more efficient than loops)\n\nWith {{re}}\nimport re\ndf['colB'] = [re.sub('[^0-9]', '', x) for x in df['colB']]\n\nre is the regular expressions library\nReplaces everything not a number with empty string (carrot inside is a negation)\n\nSplit w/list output\n&gt;&gt; df[\"group\"].str.split(\"-\")\n0¬† ¬† [A, 1B]\n1¬† ¬† [B, 1B]\n2¬† ¬† [A, 1C]\n3¬† ¬† [A, 1B]\n4¬† ¬† [C, 1C]\n\n‚Äú-‚Äù is the delimiter. A-1B ‚Äì&gt; [A, 1B]\n\nSplit into separate columns\ndf[\"group1\"] = df[\"group\"].str.split(\"-\", expand=True)[0]\ndf[\"group2\"] = df[\"group\"].str.split(\"-\", expand=True)[1]\n\nexpand = True splits into separate columns\nBUT you have to manually create the new columns in the old df by assigning each column to a new column name in the old df.\n\nConcatenate strings from a list\nwords = ['word'] * 100000 # ['word', 'word', ...]\nsentence = \"\".join(words)\n\nMore efficient than ‚Äú+‚Äù operator\n\nConcatenate string columns\n\nList comprehension is fastest\ndf['all'] = [p1 + ' ' + p2 + ' ' + p3 + ' ' + p4 for p1, p2, p3, p4 in zip(df['a'], df['b'], df['c'], df['d'])]\n‚Äú+‚Äù operator with a space, ‚Äù ‚Äú, as the delimiter\ndf['all'] = df['a'] + ' ' + df['b'] + ' ' + df['c'] + ' ' + df['d']\n\nAlso fast and have read that this is most efficient for larger datasets\n\ndf['colE'] = df.colB.str.cat(df.colD) can be used for relatively small datasets (up to 100‚Äì150 rows)\narr1 = df['owner'].array\narr2 = df['gender'].array\narr3 = []\nfor i in range(len(arr1)):\n    if arr2[i] == 'M':\n        arr3.append('Mr. ' + arr1[i])\n    else:\n        arr3.append('Ms. ' + arr1[i])\ndf['name5'] = arr3\n\nVectorizes columns then concantenates with a +\nFor-loop w/vectorization was faster than apply + list comprehension or for-loop + itertuples or for-loop + iterrows\n\n\nConcatenate string and non-string columns\ndf['colE'] = df['colB'].astype(str) + '-' + df['colD']\nExtract pattern with list comprehension\nimport re\ndf['colB'] = [re.search('[0-9]', x)[0] for x in df['colB']]\n\nre is the regular expressions library\nextracts all numeric characters\n\nExtract all instances of pattern into a list\nresults_ls = re.findall(r'\\d+', s)\n\nFinds each number in string, ‚Äús,‚Äù and outputs into a list\nCan also use re.finditer\nfyi re.search only returns the first match\n\nExtract pattern using regex\ndf['colB'] = df['colB'].str.extract(r'(\\d+)', expand=False).astype(\"int\")\n\n‚Äúr‚Äù indicates you‚Äôre using regex\nextracts all numeric patterns and changes type to integer\n\nRemove pattern by map loop\nfrom string import ascii_letters\ndf['colB'] = df['colB'].map(lambda x: x.lstrip('+-').rstrip(ascii_letters))\n\npluses or minuses are removed from if they are the leading character\nlstrip removes leading characters that match characters provided\nrstrip removes trailing characters that match characters provided\n\nDoes a string contain one or more digits\nany(c.isdigit() for c in s)\n\n\n\nConversions\n\nConvert dict to pandas df (**slow for large lists**)\ndf = pd.DataFrame.from_dict(acct_dict, orient=\"index\", columns=[\"metrics\"])\n\nresult_df = DataFrame.from_dict(search.cv_results_, orient='columns')¬†\nprint(result_df.columns)\n\nkey of the dict is used as the index and value is column named ‚Äúmetrics‚Äù\n\nConvert pandas df to ndarray\nndarray = df.to_numpy()\n# using numpy\nndarray = np.asarray(df)\nConvert df to list or dict\ndf\n¬† ColA ColB¬† ColC\n0¬† ¬† 1¬† ¬† A¬† ¬† 4\n1¬† ¬† 2¬† ¬† B¬† ¬† 5\n2¬† ¬† 3¬† ¬† C¬† ¬† 6\n\nresult = df.values.tolist()\n[[1, 'A', 4], [2, 'B', 5], [3, 'C', 6]]\n\ndf.to_dict()\n{'ColA': {0: 1, 1: 2, 2: 3},\n'ColB': {0: 'A', 1: 'B', 2: 'C'},\n'ColC': {0: 4, 1: 5, 2: 6}}\n\ndf.to_dict(\"list\")\n{'ColA': [1, 2, 3], 'ColB': ['A', 'B', 'C'], 'ColC': [4, 5, 6]}\n\ndf.to_dict(\"split\")\n{'index': [0, 1, 2],\n'columns': ['ColA', 'ColB', 'ColC'],\n'data': [[1, 'A', 4], [2, 'B', 5], [3, 'C', 6]]}\nConvert string variable to datetime\ndf.date_of_birth = df.date_of_birth.astype(\"datetime64[ns]\")\ndf['Year'] = pd.to_datetime(df['Year'])\nConvert all ‚Äúobject‚Äù type columns to ‚Äúcategory‚Äù\nfor col in X.select_dtypes(include=['object']):\n¬† X[col] = X[col].astype('category')\nConvert column to numeric\ndf['GDP'] = df['GDP'].astype(float)\n\n\n\nSample and Simulate\n\nSimulate\n\nRandom normal variable (and group variable named strata)\ndf1 = pd.DataFrame({'strata': 1, 'y': np.random.normal(loc=10, scale=1, size=size))\n\n2 columns ‚Äústrata‚Äù (all 1s) and ‚Äúy‚Äù\nloc = mean, scale = sd, size = number of observations to create\n\n\nSample\n\nRows\ndf.sample(n = 15, replace = True, random_state=2) # with replacement\n\nsample_df = df.sample(int(len(tps_df) * 0.2)) # sample 20% of the data\n\nThis method is faster than sampling using random indices with NumPy\n\nRows and columns\ntps.sample(5, axis=1).sample(7, axis=0)\n\n5 columns and 7 rows\n\n\n\n\n\nChaining\n\nNeed to encapsulate code in parentheses\n# to assign to an object\nnew_df = (\n¬† ¬† melb\n¬† ¬† .query(\"Distance &lt; 2 & Rooms &gt; 2\") # query equals filter in Pandas\n¬† ¬† .filter([\"Type\", \"Price\"]) # filter equals select in Pandas\n¬† ¬† .groupby(\"Type\")\n¬† ¬† .agg([\"mean\", \"count\"]) # calcs average price and row count for each Type; creates subcolumns mean and count under Price\n¬† ¬† .reset_index() # converts matrix to df\n¬† ¬† .set_axis([\"Type\", \"averagePrice\", \"numberOfHouses\"], # renames Price to averagePrice and count to numberOfHouses\n¬† ¬† ¬† ¬† ¬† ¬† ¬† axis = 1,\n¬† ¬† ¬† ¬† ¬† ¬† ¬† inplace = False)\n¬† ¬† .assign(averagePriceRounded = lambda x: x[\"averagePrice\"] # assign equals mutate in Pandas (?)\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† .round(1))\n¬† ¬† .sort_values(by = [\"numberOfHouses\"],\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ascending = False)\n)\n\nif agg[\"mean\"] then there wouldn‚Äôt be a subcolumn mean, just the values of Price would be the mean\n\nUsing pipe\n\nargs\n\nfunc: Function to apply to the Series/DataFrame\nargs: Positional arguments passed to func\nkwargs: Keyword arguments passed to func\n\nReturns: object, the return type of func\nSyntax\ndef f1(df, arg1):\n# do something return # a dataframe\n\ndef f2(df, arg2):\n# do something return # a dataframe\n\ndef f3(df, arg3):\n# do something return # a dataframe\n\ndf = pd.DataFrame(..) # some dataframe\n\ndf.pipe(f3, arg3 = arg3).pipe(f2, arg2 = arg2).pipe(f1, arg1 = arg1)\n\nfunction 3 (f3) is executed then function 2 then function 1\n\n\n\n\n\nCrosstab\n\nExample\ndf = pd.DataFrame([[\"A\", \"X\"],¬†\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† [\"B\", \"Y\"],¬†\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† [\"C\", \"X\"],\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† [\"A\", \"X\"]],¬†\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† columns = [\"col1\", \"col2\"])\nprint(pd.crosstab(df.col1, df.col2))\ncol2¬† X¬† Y\ncol1¬† ¬† ¬†\nA¬† ¬† 2¬† 0\nB¬† ¬† 0¬† 1\nC¬† ¬† 1¬† 0\n\n\n\nPivot Table\n\nExample\nprint(df)\n¬† ¬† Name¬† Subject¬† Marks\n0¬† John¬† ¬† Maths¬† ¬† ¬† 6\n1¬† Mark¬† ¬† Maths¬† ¬† ¬† 5\n2¬† Peter¬† ¬† Maths¬† ¬† ¬† 3\n3¬† John¬† Science¬† ¬† ¬† 5\n4¬† Mark¬† Science¬† ¬† ¬† 8\n5¬† Peter¬† Science¬† ¬† 10\n6¬† John¬† English¬† ¬† 10\n7¬† Mark¬† English¬† ¬† ¬† 6\n8¬† Peter¬† English¬† ¬† ¬† 4\n\npd.pivot_table(df,¬†\n¬† ¬† ¬† ¬† ¬† ¬† ¬† index = [\"Name\"],\n¬† ¬† ¬† ¬† ¬† ¬† ¬† columns=[\"Subject\"],¬†\n¬† ¬† ¬† ¬† ¬† ¬† ¬† values='Marks',\n¬† ¬† ¬† ¬† ¬† ¬† ¬† fill_value=0)\nSubject¬† English¬† Maths¬† Science\nName¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬†\nJohn¬† ¬† ¬† ¬† ¬† 10¬† ¬† ¬† 6¬† ¬† ¬† ¬† 5\nMark¬† ¬† ¬† ¬† ¬† 6¬† ¬† ¬† 5¬† ¬† ¬† ¬† 8\nPeter¬† ¬† ¬† ¬† ¬† 4¬† ¬† ¬† 3¬† ¬† ¬† 10\nExample: drop lowest score for each letter grade, then calculate the average score for each letter grade\ngrades_df.pivot_table(index='name',\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† columns='letter grade',\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† values='score',\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† aggfunc = lambda series : (sorted(list(series))[-1] + sorted(list(series))[-2]) / 2)\n\nletter grade¬† ¬† A¬† ¬† B\nname\nArif¬† ¬† ¬† ¬† ¬† 96.5¬† 87.0\nKayla¬† ¬† ¬† ¬† 95.5¬† 84.0\n\ngrades_df\n\n2 names (‚Äúname‚Äù)\n6 scores (‚Äúscore‚Äù)\nOnly 2 letter grades associated with these scores (‚Äúletter grade‚Äù)\n\nindex: each row will be a ‚Äúname‚Äù\ncolumns: each column will be a ‚Äúletter grade‚Äù\nvalues: value in the cells will be from the ‚Äúscore‚Äù column according to each combination columns in the index and columns args\naggfunc: uses a lambda to compute the aggregated values\n\n‚Äúseries‚Äù is used a the variable¬† in the lambda function\nsorts series (ascending), takes the top two values (using negative list indexing), and averages them\n\nIterate over a df\n\nBetter to use a vectorized solution if possible\n\n\n\n\n\nIteration\n\niterrows\ndef salary_iterrows(df):\n¬† ¬† salary_sum = 0\n\n¬† ¬† for index, row in df.iterrows():\n¬† ¬† ¬† ¬† salary_sum += row['Employee Salary']\n\n¬† ¬† return salary_sum/df.shape[0]\n\nsalary_iterrows(data)\niteruples\ndef salary_itertuples(df):\n¬† ¬† salary_sum = 0\n\n¬† ¬† for row in df.itertuples():¬†\n¬† ¬† ¬† ¬† salary_sum += row._4\n\n¬† ¬† return salary_sum/df.shape[0]\n\nsalary_itertuples(data)\n\nFaster than iterrows",
    "crumbs": [
      "Python",
      "Pandas"
    ]
  },
  {
    "objectID": "qmd/python-pandas.html#sec-py-pandas-ts",
    "href": "qmd/python-pandas.html#sec-py-pandas-ts",
    "title": "Pandas",
    "section": "Time Series",
    "text": "Time Series\n\nMisc\n\nAlso see\n\nFeature Engineering, Time Series &gt;&gt; Misc has a list of python libraries for preprocessing\nA Collection of Must-Know Techniques for Working with Time Series Data in Python\n\nCollection of preprocessing recipes\n\n{{datetime}} in bkmks\n\n\n\n\nOperations\n\nLoad and set index frequency\n\nExample\n#Load the PCE and UMCSENT datasets\ndf = pd.read_csv(\n¬† ¬† filepath_or_buffer='UMCSENT_PCE.csv',\n¬† ¬† header=0,\n¬† ¬† index_col=0,\n¬† ¬† infer_datetime_format=True,\n¬† ¬† parse_dates=['DATE']\n)\n#Set the index frequency to 'Month-Start'\ndf = df.asfreq('MS')\n\n‚Äúheader=0‚Äù is default, says 1st row of file is the column names\n‚Äúindex_col=0‚Äù says use the first column as the df index\n‚Äúinfer_datetime_format=True‚Äù says infer the format of the datetime strings in the columns\n‚Äúparse_dates=[‚ÄòDATE‚Äô]‚Äù says convert ‚ÄúDATE‚Äù to datetime and format\n\n\nCreate date variable\n\nExample: w/date_range\n# DataFrame\ndate_range = pd.date_range('1/2/2022', periods=24, freq='H')\nsales = np.random.randint(100, 400, size=24)\nsales_data = pd.DataFrame(\n¬† ¬† sales,\n¬† ¬† index = date_range,\n¬† ¬† columns = ['Sales']\n)\n# Series\nrng = pd.date_range(\"1/1/2012\", periods=100, freq=\"S\")\nts = pd.Series(np.random.randint(0, 500, len(rng)), index=rng)\nMethods (article)\n\npandas.date_range ‚Äî Return a fixed frequency DatetimeIndex.\n\nstart: the start date of the date range generated\nend: the end date of the date range generated\nperiods: the number of dates generated\nfreq: default to ‚ÄúD‚Äù (daily), the interval between dates generated, it could be hourly, monthly or yearly\n\npandas.bdate_range ‚Äî Return a fixed frequency DatetimeIndex, with the business day as the default frequency.\npandas.period_range ‚Äî Return a fixed frequency PeriodIndex. The day (calendar) is the default frequency.\npandas.timedelta_range ‚Äî Return a fixed frequency TimedeltaIndex, with the day as the default frequency.\n\n\nCoerce to datetime\n\nSource has month first or day first\n# month first\n# e.g 9/16/2015 --&gt; 2015-09-16\ndf['joining_date'] = pd.to_datetime(df['joining_date'])\n\n# day first\n# e.g 16/9/2015 --&gt; 2015-09-16\ndf['joining_date'] = pd.to_datetime(df['joining_date'], dayfirst=True)\n\ndefault is month first\n* If the first digit is a number that can NOT be a month (e.g.¬†25), then it will parse it as a day instead. *\n\nFormat conditional on source‚Äôs delimiter\ndf['joining_date_clean'] = np.where(df['joining_date'].str.contains('/'),\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† pd.to_datetime(df['joining_date']),\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† pd.to_datetime(df['joining_date'], dayfirst=True)\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† )\n\nLike an ifelse. Source dates that have ‚Äú/‚Äù separating values are parsed as month-first and everything else as day-first\n\n\nTransform partial date columns\n\nExample: String 09/2007¬†will be transformed to date¬†2007-09-01\nimport datetime\n\ndef parse_first_brewed(text: str) -&gt; datetime.date:\n    parts = text.split('/')\n    if len(parts) == 2:\n        return datetime.date(int(parts[1]), int(parts[0]), 1)\n    elif len(parts) == 1:\n        return datetime.date(int(parts[0]), 1, 1)\n    else:\n        assert False, 'Unknown date format'\n\n&gt;&gt;&gt; parse_first_brewed('09/2007')\ndatetime.date(2007, 9, 1)\n\n&gt;&gt;&gt; parse_first_brewed('2006')\ndatetime.date(2006, 1, 1)\n\nExtract time components\n\nCreate ‚Äúyear-month‚Äù column: df[\"year_month\"] = df[\"created_at\"].dt.to_period(\"M\")\n\n‚ÄúM‚Äù is the ‚Äúoffset alias‚Äù string for month\n(Docs)\n\n\nFilter a range\ngallipoli_data.loc[\n¬† ¬† ¬† ¬† (gallipoli_data.DateTime &gt;= '2008-01-02')¬†\n¬† ¬† ¬† ¬† &¬†\n¬† ¬† ¬† ¬† (gallipoli_data.DateTime &lt;= '2008-01-03')\n¬† ¬† ]\nFill in gaps\n\nExample: hacky way to do it\npd.DataFrame(\n¬† ¬† sales_data.Sales.resample('h').mean()\n)\n\nfrequency is already hourly (‚Äòh‚Äô), so taking the mean doesn‚Äôt change the values. But NaNs will be added for datetime values that don‚Äôt exist.\n.fillna(0) can be added to .mean() if you want to fill the NaNs with something meaningful (e.g.¬†0)\n\nExplode interval between 2 date columns into a column with all the dates in that interval\n\nExample: 1 row\npd.date_range(calendar[\"checkin_date\"][0], calendar[\"checkout_date\"][0])\n# output\nDatetimeIndex(['2022-06-01', '2022-06-02', '2022-06-03',¬†\n¬† ¬† ¬† ¬† ¬† ¬† ¬† '2022-06-04', '2022-06-05', '2022-06-06',¬†\n¬† ¬† ¬† ¬† ¬† ¬† ¬† '2022-06-07'],\n¬† ¬† ¬† ¬† ¬† ¬† ¬† dtype='datetime64[ns]', freq='D')\nExample: all rows\n\ncalendar.loc[:, \"booked_days\"] = calendar.apply(\n\n¬† ¬† lambda x: list(\n¬† ¬† ¬† ¬† pd.date_range(\n¬† ¬† ¬† ¬† ¬† ¬† x.checkin_date,¬†\n¬† ¬† ¬† ¬† ¬† ¬† x.checkout_date + pd.DateOffset(days=1)\n¬† ¬† ¬† ¬† ).date\n¬† ¬† ),\n¬† ¬† axis = 1\n\n)\nExample: pivot_longer\n\n# explode¬†\ncalendar = calendar.explode(\n¬† ¬† column=\"booked_days\", ignore_index=True\n)[[\"property\",\"booked_days\"]]\n# display the first 5 rows\ncalendar.head()\n\n\n\n\n\nCalculations\n\nGet the min/max dates of a dataset: print(df.Date.agg(['min', 'max'])) (‚ÄúDate‚Äù is the date variable)\nFind difference between to date columns\ndf[\"days_to_checkin\"] = (df[\"checkin_date\"] - df[\"created_at\"]).dt.days\n\nnumber of days between the check-in date and the date booking was created (i.e.¬†number of days until the customer arrives)\n\nAdd 1 day to a subset of observations\ndf.loc[df[\"booking_id\"]==1001, \"checkout_date\"] = df.loc[df[\"booking_id\"]==1001, \"checkout_date\"] + pd.DateOffset(days=1)\n\nadds 1 day to the checkout date of the booking with id 1001\n\nAggregation\n\nMisc\n\nresample (docs) requires a datetime type column set as the index for the dataframe: df.index = df[‚ÄòDateTime‚Äô]\n\nbtw this function doesn‚Äôt resample in the bootstrapping sense of the word. Just a function that allows you to do window calculations on time series\n\nCommon time strings (docs)\n\ns for seconds\nt for minutes\nh for hours\nw for weeks\nm for months\nq for quarter\n\n\nRolling-Window\n\nExample: 30-day rolling average\nwindow_mavg_short=30\nstock_df['mav_short'] = stock_df['Close'] \\\n¬† ¬† .rolling(window=window_mavg_short) \\\n¬† ¬† .mean()\n\nStep-Window\n\nExample: Mean temperature every 3 hours\n\ngallipoli_data.Temperature.resample(rule = ‚Äò3h‚Äô).mean()\n\n‚Äú1.766667‚Äù is the average from 03:00:00 to 05:00:00\n‚Äú4.600000‚Äù is the average from 06:00:00 to 08:00:00\n‚Äúh‚Äù is the time string from hours\nBy default the calculation window starts on the left index (see next Example for right index) and doesn‚Äôt include the right index\n\ne.g.¬†index, ‚Äú09:00:00‚Äù, calculation window includes ‚Äú09:00:00‚Äù, ‚Äú10:00:00‚Äù, and ‚Äú11:00:00‚Äù\n\n\nExample: Max sunshine every 3 hrs\n\ngallipoli_data['Sunshine Duration'].resample(rule = '3h', closed= 'right').max()\n\n‚Äúclosed=‚Äòright‚Äô‚Äù says include the right index in the calculation but not the left\n\ne.g.¬†index, ‚Äú09:00:00‚Äù, calculation window includes ‚Äú10:00:00‚Äù, ‚Äú11:00:00‚Äù, and ‚Äú12:00:00‚Äù\n\n\n\n\n\n\n\nSimulation\n\nExample: simulation, gaussian\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nnp.random.seed(987)\ntime_series = [np.random.normal()*0.1, np.random.normal()*0.1]\nsigs = [0.1, 0.1]\nfor t in range(2000):\n¬† ¬† sig_t = np.sqrt(0.1 + 0.24*time_series[-1]**2 + 0.24*time_series[-2]**2 + 0.24*sigs[-1]**2 + 0.24*sigs[-2]**2)\n¬† ¬† y_t = np.random.normal() * sig_t\n¬† ¬† time_series.append(y_t)\n¬† ¬† sigs.append(sig_t)\n\ny = np.array(time_series[2:])\nplt.figure(figsize = (16,8))\nplt.plot(y, label = \"Simulated Time-Series\")\nplt.grid(alpha = 0.5)\nplt.legend(fontsize = 18)\nStandard GARCH time-series that‚Äôs frequently encountered in econometrics\n\nExample: simulation, beta\n\nfrom scipy.stats import beta\nnp.random.seed(321)\ntime_series = [beta(0.5,10).rvs()]\nfor t in range(2000):\n¬† ¬† alpha_t = 0.5 + time_series[-1] * 0.025 * t\n¬† ¬† beta_t = alpha_t * 20\n¬† ¬† y_t = beta(alpha_t, beta_t).rvs()\n¬† ¬† time_series.append(y_t)\n\ny = np.array(time_series[1:])\nplt.figure(figsize = (16,8))\nplt.plot(y, label = \"Simulated Time-Series\")\nplt.grid(alpha = 0.5)\nplt.legend(fontsize = 18)",
    "crumbs": [
      "Python",
      "Pandas"
    ]
  },
  {
    "objectID": "qmd/python-pandas.html#sec-py-pandas-opt",
    "href": "qmd/python-pandas.html#sec-py-pandas-opt",
    "title": "Pandas",
    "section": "Optimization",
    "text": "Optimization\n\nPerformance\n\nPandas will typically outperform numpy ndarrays in cases that involve significantly larger volume of data (say &gt;500K rows)\nBad performance by iteratively creating rows in a dataframe\n\nBetter to iteratively append lists then coerce to a dataframe at the end\nUse itertuples instead of iterrows in loops\n\nIterates through the data frame by converting each row of data as a list of tuples. Makes comparatively less number of function calls and hence carry less overhead.\ntqdm::tqdm is a progress bar for loops\n\n\nLibraries\n\n{{pandarallel}} - A simple and efficient tool to parallelize Pandas operations on all available CPUs.\n\narticle, article\n\n{{parallel_pandas}} -¬† A simple and efficient tool to parallelize Pandas operations on all available CPUs.\n\narticle\n\n{{modin}} - multi-processing package with identical APIs to Pandas, to speed up the Pandas workflow by changing 1 line of code. Modin offers accelerated performance for about 90+% of Pandas API. Modin uses Ray and Dask under the hood for distributed computing.\n\narticle\n\n{{numba}} - JIT compiler that translates a subset of Python and NumPy code into fast machine code.\n\nworks best with functions that involve many native Python loops, a lot of math, and even better, NumPy functions and arrays\nExample\n@numba.jit\ndef crazy_function(col1, col2, col3):\n¬† ¬† return (col1 ** 3 + col2 ** 2 + col3 * 10) ** 0.5\n\nmassive_df[\"f1001\"] = crazy_function(\n¬† ¬† massive_df[\"f1\"].values, massive_df[\"f56\"].values, massive_df[\"f44\"].values\n)\nWall time: 201 ms\n\n9GB dataset\nJIT stands for just in time, and it translates pure Python and NumPy code to native machine instructions\n\n\n\nUse numpy arrays\ndat['col1001'] = some_function(\n¬† ¬† ¬† ¬† ¬† dat['col1'].values, dat['col2'].values, dat['col3'].values\n¬† ¬† ¬† ¬† )\n\nAdding the .values to the column vectors coerces to ndarrays\nNumPy arrays are faster because they don‚Äôt perform additional calls for indexing, data type checking like Pandas Series\n\neval for non-mathematical operations (e.g.¬†boolean indexing, comparisons, etc.)\n\nDocs\nExample\nmassive_df.eval(\"col1001 = (col1 ** 3 + col2 ** 2 + col3 * 10) ** 0.5\", inplace=True)\n\nUsed a mathematical operation for his example for some reason\n\n\niloc vs loc\n\niloc faster for filtering rows: dat.iloc[range(10000)]\nloc faster for selecting columns: dat.loc[:, [\"f1\", \"f2\", \"f3\"]]\nnoticeable as data size increases\n\n\n\n\nMemory Optimization\n\nSee memory size of an object\ndata = pd.read_csv(\"dummy_dataset.csv\")\ndata.info(memory_usage = \"deep\")\ndata.memory_usage(deep=True)¬† / 1024 ** 2 # displays col sizes in MBs\n\n# or for just 1 variable\ndata.Country.memory_usage()\n\n# a few columns\nmemory_usage = df.memory_usage(deep=True) / 1024 ** 2 # displays col sizes in MBs\nmemory_usage.head(7)\nmemory_usage.sum() # total\nUse inplace transformation (i.e.¬†don‚Äôt create new copies of the df) to reduce memory load\ndf.fillna(0, inplace = True)\n# instead of\ndf_copy = df.fillna(0)\nLoad only the columns you need from a file\ncol_list = [\"Employee_ID\", \"First_Name\", \"Salary\", \"Rating\", \"Company\"]\ndata = pd.read_csv(\"dummy_dataset.csv\", usecols=col_list)\nVariable datatypes\n\n\nConvert variables to smaller types when possible\n\nVariables always receive largest memory types\n\nPandas will always assign int64 as the datatype of the integer-valued column, irrespective of the range of current values in the column.\n\n\nByte ranges (same bit options for floats)\n\n\nuint refers to unsigned, only positive integers\nint8: 8-bit-integer that covers integers from [-2‚Å∑, 2‚Å∑].\nint16: 16-bit-integer that covers integers from [-2¬π‚Åµ, 2¬π‚Åµ].\nint32: 32-bit-integer that covers integers from [-2¬≥¬π, 2¬≥¬π].\nint64: 64-bit-integer that covers integers from [-2‚Å∂¬≥, 2‚Å∂¬≥].\n\nConvert integer column to smaller type: data[\"Employee_ID\"] = data.Employee_ID.astype(np.int32)\nConvert all ‚Äúobject‚Äù type columns to ‚Äúcategory‚Äù\nfor col in X.select_dtypes(include=['object']):\n¬† X[col] = X[col].astype('category')\n\nobject datatype consumes the most memory. Either use str or category if there are few unique values in the feature\npd.Categorical data type can speed things up to 10 times while using LightGBM‚Äôs default categorical handler\n\nFor datetime or timedelta, use the native formats offered in pandas since they enable special manipulation functions\nFunction for checking and converting all numerical columns in dataframes to optimal types\ndef reduce_memory_usage(df, verbose=True):\n¬† ¬† numerics = [\"int8\", \"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\n¬† ¬† start_mem = df.memory_usage().sum() / 1024 ** 2\n¬† ¬† for col in df.columns:\n¬† ¬† ¬† ¬† col_type = df[col].dtypes\n¬† ¬† ¬† ¬† if col_type in numerics:\n¬† ¬† ¬† ¬† ¬† ¬† c_min = df[col].min()\n¬† ¬† ¬† ¬† ¬† ¬† c_max = df[col].max()\n¬† ¬† ¬† ¬† ¬† ¬† if str(col_type)[:3] == \"int\":\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† if c_min &gt; np.iinfo(np.int8).min and c_max &lt; np.iinfo(np.int8).max:\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† df[col] = df[col].astype(np.int8)\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† elif c_min &gt; np.iinfo(np.int16).min and c_max &lt; np.iinfo(np.int16).max:\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† df[col] = df[col].astype(np.int16)\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† elif c_min &gt; np.iinfo(np.int32).min and c_max &lt; np.iinfo(np.int32).max:\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† df[col] = df[col].astype(np.int32)\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† elif c_min &gt; np.iinfo(np.int64).min and c_max &lt; np.iinfo(np.int64).max:\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† df[col] = df[col].astype(np.int64)\n¬† ¬† ¬† ¬† ¬† ¬† else:\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† if (\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† c_min &gt; np.finfo(np.float16).min\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† and c_max &lt; np.finfo(np.float16).max\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ):\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† df[col] = df[col].astype(np.float16)\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† elif (\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† c_min &gt; np.finfo(np.float32).min\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† and c_max &lt; np.finfo(np.float32).max\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ):\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† df[col] = df[col].astype(np.float32)\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† else:\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† df[col] = df[col].astype(np.float64)\n¬† ¬† end_mem = df.memory_usage().sum() / 1024 ** 2\n¬† ¬† if verbose:\n¬† ¬† ¬† ¬† print(\n¬† ¬† ¬† ¬† ¬† ¬† \"Mem. usage decreased to {:.2f} Mb ({:.1f}% reduction)\".format(\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† end_mem, 100 * (start_mem - end_mem) / start_mem\n¬† ¬† ¬† ¬† ¬† ¬† )\n¬† ¬† ¬† ¬† )\n¬† ¬† return df\n\nBased on the minimum and maximum value of a numeric column and the above table, the function converts it to the smallest subtype possible\n\nCheck memory usage before and after conversion\nprint(\"Memory usage before changing the datatype:\", data.Country.memory_usage())\ndata[\"Country\"] = data.Country.astype(\"category\")\nprint(\"Memory usage after changing the datatype:\", data.Country.memory_usage())\nUse sparse types for variables with NaNs\n\n\nExample: data[\"Rating\"] = data.Rating.astype(\"Sparse[float32]\")\n\nSpecify datatype when loading data\ncol_list = [\"Employee_ID\", \"First_Name\", \"Salary\", \"Rating\", \"Country_Code\"]\ndata = pd.read_csv(\"dummy_dataset.csv\", usecols=col_list,¬†\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† dtype = {\"Employee_ID\":np.int32, \"Country_Code\":\"category\"})",
    "crumbs": [
      "Python",
      "Pandas"
    ]
  },
  {
    "objectID": "qmd/db-feature-stores.html",
    "href": "qmd/db-feature-stores.html",
    "title": "Feature Stores",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Databases",
      "Feature Stores"
    ]
  },
  {
    "objectID": "qmd/db-feature-stores.html#sec-db-featst-misc",
    "href": "qmd/db-feature-stores.html#sec-db-featst-misc",
    "title": "Feature Stores",
    "section": "",
    "text": "Benefits\n\nIncreases Reproducibility\n\nEasily track the version of the features used in each model and reproduce the model‚Äôs results if needed.\nUseful in a collaborative environment where multiple people are working on the same project.\n\nDiscovery and Testing Features Easier\n\nHaving features in a centralized location makes comparing the performance of various features and versions of those features easier.\n\nEasier to Scale\n\nIt‚Äôs easier to share features between ML models which means fewer resources (e.g.¬†development, deployment) will be required. Allowing more models to be added more efficiently and cheaply.\n\n\nFeatures of a Feature Store\n\nDesigned with ML modelling in mind\n\nCan handle large amounts of data and perform feature engineering at scale\n\nHandles versioning of features\n\nEasy to track which features were used for a particular model, making it simple to reproduce or deploy the model in the future\n\nAllows for different levels of access control\n\nA data scientist can work on a feature without worrying about affecting other users which can‚Äôt be said about warehouses\n\n\nBest used if you have a substantial number of features that are computationally expensive, frequently improved, and used in many ML models.\n\n\nHere ‚ÄúTransform‚Äù is referring to something like an AWS lambda function that‚Äôs triggered to transform the data\nCases where adding a feature store adds unnecessary complexity:\n\nFeature value needs to ‚Äúseen‚Äù by the client (e.g.¬†app)\n\nNot exactly sure why this matters or what ‚Äúseen‚Äù means\nMaybe this is a latency thing?\n\nFeature is in a data warehouse.\nFeature isn‚Äôt time dependent\n\nSo only streaming and not batch serving I think\n\nComputationally inexpensive\n\nExample\n\nEmbedding of a song, artist, and user features in a music streaming service.\n\nThere is a team updating user and song embeddings on a daily basis. Every time the model that consumes this feature, it is retrained ‚Äî high commercial value use cases will need to re-train periodically ‚Äî the training code will need to fetch the values of this feature that align with the training labels and the latest version of the embedding algorithm.\n\n\n\nPositioning within a pipeline\n\n\nLooks like something dbt (‚ÄúFeature Pipelines‚Äù) would write to.\nReminds me of the description of a data mart.\n\nConnectors\n\nTensorFlow‚Äôs TFXI (TensorFlow Extended Input/Output)\n\nModule allows you to easily read data from Feature store and feed it into your TensorFlow model.\nSupports data preprocessing, so you can do things like normalization and feature selection right within TensorFlow.\n\nPyTorch‚Äôs DataLoader\n\nClass that allows you to easily read data from Feature store, process, and feed it into your PyTorch model.",
    "crumbs": [
      "Databases",
      "Feature Stores"
    ]
  },
  {
    "objectID": "qmd/db-feature-stores.html#sec-db-featst-brands",
    "href": "qmd/db-feature-stores.html#sec-db-featst-brands",
    "title": "Feature Stores",
    "section": "Brands",
    "text": "Brands\n\nGoogle Vertex AI feature store\n\ndocs\n\nAmazon SageMaker Feature Store\n\ndocs\n\nDatabricks Feature Store\nHopsWorks Feature Store\ntecton.ai\n\nsite\ncloud agnostic\n\nbytehub\n\ngithub\n\nFeast\n\nIt is a standalone, open-source feature store that organizations use to store and serve features consistently for offline training and online inference.\n\nDataRobot\nAlgoworks\nHugging Face: A feature store for natural language processing (NLP) models that allows for easy sharing and management of pre-trained models and features.",
    "crumbs": [
      "Databases",
      "Feature Stores"
    ]
  },
  {
    "objectID": "qmd/geospatial-processing.html#sec-geo-proc-bd",
    "href": "qmd/geospatial-processing.html#sec-geo-proc-bd",
    "title": "Preprocessing",
    "section": "Big Data",
    "text": "Big Data\n\nReduce Size Via SQL Query\n\nUses OGR SQL (SQLite might also be accepted)\nChoose Layer\nlibrary(\"sf\")\nst_layers(\"data/Lower_layer_Super_Output_Areas_2021_EW_BGC_V3.gpkg\")\n## Driver: GPKG \n## Available layers:\n##            layer_name geometry_type features fields crs_name\n## 1 LSOA_2021_EW_BGC_V3 Multi Polygon    35672      7  OSGB36 / British National Grid         \n\nThis file only has one layer\n\nGet an idea of the columns in the layer by looking at the first row.\nst_read(\"data/Lower_layer_Super_Output_Areas_2021_EW_BGC_V3.gpkg\",\n        query = \"SELECT * FROM LSOA_2021_EW_BGC_V3 WHERE FID = 1\",\n        quiet = TRUE)\n\n## Simple feature collection with 1 feature and 7 fields\n## Geometry type: MULTIPOLYGON\n## Dimension:     XY\n## Bounding box:  xmin: 531948.3 ymin: 181263.5 xmax: 532308.9 ymax: 182011.9\n## Projected CRS: OSGB36 / British National Grid\n##    LSOA21CD            LSOA21NM  BNG_E  BNG_N     LONG      LAT\n## 1 E01000001 City of London 001A 532123 181632 -0.09714 51.51816\n##                                 GlobalID                          SHAPE\n## 1 {1A259A13-A525-4858-9CB0-E4952BA01AF6} MULTIPOLYGON (((532105.3 18...\n\nFID = 1 says look at the first row. FID is the feature ID attribute. I don‚Äôt think it‚Äôs actual column in the dataset.\n\nQuery the layer and filter\nst_read(\"data/Lower_layer_Super_Output_Areas_2021_EW_BGC_V3.gpkg\",\n        query = \"SELECT * FROM LSOA_2021_EW_BGC_V3 WHERE LSOA21CD LIKE 'W%'\",\n        quiet = TRUE)\n\nW% says looks for values that start with ‚ÄúW‚Äù (Wales) in the LSOA21CD column\n\nBased on the OCR SQL docs I think % is wildcard for multiple characters.\n\n\n\nUse a bounding box to filter overlapping geometries\n\nExample: Filter polygons overlapping the boundaries of Wales\n\nFilter Wales from a UK shapefile dataset\nuk &lt;- sf::st_read(\"data/Countries_December_2022_GB_BGC.gpkg\")\nwales &lt;- dplyr::filter(uk, CTRY22NM == \"Wales\")\nCreate Wales polygon\nwales_wkt &lt;-  \n  wales |&gt;\n  sf::st_geometry() |&gt;\n  sf::st_as_text()\nFilter overlapping geometries\nwales_lsoa &lt;- \n  sf::st_read(\"data/Lower_layer_Super_Output_Areas_2021_EW_BGC_V3.gpkg\",\n              wkt_filter = wales_wkt)\n\nSome English LSOAs along the Wales/England border in addition to the Welsh LSOAs are read in, because these technically overlap with the Wales polygon on the border itself. Not perfect but still reduces the data being read into memory.",
    "crumbs": [
      "Geospatial",
      "Preprocessing"
    ]
  },
  {
    "objectID": "qmd/db-duckdb.html#sec-db-duckdb-cli",
    "href": "qmd/db-duckdb.html#sec-db-duckdb-cli",
    "title": "DuckDB",
    "section": "CLI",
    "text": "CLI\n\nStart interactive shell: duckdb\nStart interactive shell on database file: duckdb path\\to\\file\nQuery csv, json, or parquet file directly\nduckdb -c \"SELECT * FROM 'data_source.[csv|csv.gz|json|json.gz|parqet]'\"\nRun SQL script: duckdb -c \".read path\\to\\script.sql\"",
    "crumbs": [
      "Databases",
      "DuckDB"
    ]
  },
  {
    "objectID": "qmd/json.html#sec-json-duck",
    "href": "qmd/json.html#sec-json-duck",
    "title": "JSON",
    "section": "DuckDB",
    "text": "DuckDB\n\nMisc\n\nNotes from\n\nDuckDB as the New jq\n\n-json flag says output as json instead of a sql query output\n\nThe output kind of looks like a scrunched up dictionary. So, you can pipe that into a CLI tool like jq (if you have it installed) (e.g.¬†&lt;query with -json&gt; | jq) to get pretty printing\n\nRead JSON from a URL in a query: from read_json('https://api.github.com/orgs/golang/repos')\n\nCLI\n\nExample\n\nData: Types of open source licenses used in golang repo\n[\n  {\n    \"id\": 1914329,\n    \"name\": \"gddo\",\n    \"license\": {\n      \"key\": \"bsd-3-clause\",\n      \"name\": \"BSD 3-Clause \\\"New\\\" or \\\"Revised\\\" License\",\n      ...\n    },\n    ...\n  },\n  {\n    \"id\": 11440704,\n    \"name\": \"glog\",\n    \"license\": {\n      \"key\": \"apache-2.0\",\n      \"name\": \"Apache License 2.0\",\n      ...\n    },\n    ...\n  },\n  ...\n]\nCount most common license types used\nduckdb -c \\\n\"select license-&gt;&gt;'key' as license, count(*) as count \\\nfrom 'repos.json' \\\ngroup by 1 \\\norder by count desc\"\n\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ   license    ‚îÇ count ‚îÇ\n‚îÇ   varchar    ‚îÇ int64 ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ bsd-3-clause ‚îÇ    23 ‚îÇ\n‚îÇ apache-2.0   ‚îÇ     5 ‚îÇ\n‚îÇ              ‚îÇ     2 ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\nThe bottom license with count = 2 is null (i.e.¬†no licence)\n-&gt;&gt; is used to drill down into a nested json field. (e.g.¬†license to key)",
    "crumbs": [
      "JSON"
    ]
  },
  {
    "objectID": "qmd/shiny-general.html",
    "href": "qmd/shiny-general.html",
    "title": "General",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Shiny",
      "General"
    ]
  },
  {
    "objectID": "qmd/shiny-general.html#resources",
    "href": "qmd/shiny-general.html#resources",
    "title": "38¬† General",
    "section": "",
    "text": "RStudio https://shiny.rstudio.com/tutorial/\n\nModules https://shiny.rstudio.com/articles/modules.html\nBeginner https://rstudio-education.github.io/shiny-course/\n\nHadley ebook https://mastering-shiny.org/\nEngineering Production-Grade Shiny Apps - {golem} book\nAdv. UI ebook https://unleash-shiny.rinterface.com/\nSimple app packaged, modules, etc.\n\nhttps://github.com/2DegreesInvesting/scenarioSelector#learning-shiny\n\nExamples and templates for various usecases (link)",
    "crumbs": [
      "Shiny",
      "<span class='chapter-number'>38</span>¬† <span class='chapter-title'>General</span>"
    ]
  },
  {
    "objectID": "qmd/shiny-general.html#misc",
    "href": "qmd/shiny-general.html#misc",
    "title": "38¬† General",
    "section": "38.2 Misc",
    "text": "38.2 Misc\n\nA common source of poor Shiny performance and maintainability is placing a large amount of logic within a single reactive statement such as an¬†observe()\n\nInstead of adding a bunch of renderPlots to an observe(), liberally use reactive variables and your code becomes much cleaner, faster, and more maintainable\n\nComparison with other products (article)\n\nGeneral Workflow (Dancho)\n\nSimple App\n\nSomething bare bones that has ui and server sections\nCould be a company or personal template or the ‚ÄúHello, World‚Äù app with slider and text output\n\nBetter App\n\nAdd images (e.g.¬†navbar logo, splash page background)\nAdjust layout (add sidebar, main panel, etc. to UI)\nAdd blank cards (pill, tab, etc.) which will later be filled with charts, tables, etc.\nTitles (navbar, side bar, main panel, etc.)\nUse Bootstrap 5 and style.css files (style navbar, titles, etc.)\n\nPlaceholder App\n\nIntegrate analysis into the app\nMake cards look professional\nUse widgets to make interactive visualizations (charts, tables)\n\nReactive App\n\nMake all calculations and events reactive\n\ni.e user input changes output (e.g.¬†data values/calculations that go into the charts, tables, etc.)\n\nThis is the basic working application\n\nFinal Product\n\nFeature examples\n\nAdd full data ingestion\nAdd search\nAdd additional inputs and calculations (i.e.¬†more analysis)\n\nCreate modular code (utils directory)\n\nCode gets long for more complicated apps. Modularizing it makes it more readable and easier to maintain\n\nMake output downloadable into a report\n\n\noptions(shiny.autoreload = TRUE) before running shiny::runApp(appDir = \"./tests\", port = 8080) allows you to make changes to app.R and view them almost instantaeously in the viewer. (link)",
    "crumbs": [
      "Shiny",
      "<span class='chapter-number'>38</span>¬† <span class='chapter-title'>General</span>"
    ]
  },
  {
    "objectID": "qmd/shiny-general.html#design",
    "href": "qmd/shiny-general.html#design",
    "title": "38¬† General",
    "section": "38.3 Design",
    "text": "38.3 Design\n\nMisc\n\npackages\n\n{designer} - makes the initial generation of a UI wireframe of a shiny application as quick and simple as possible. Good for PoCs. Allows you to drag and drop components.\n{shinyuieditor} - seems like it‚Äôs similar to {designer}\n\nAlso see BizSci video\n\n\nSemantic Layer - A¬† business representation of corporate data that helps end users access data autonomously using common business terms. By using common business terms, rather than data language, to access, manipulate, and organize information, a semantic layer simplifies the complexity of business data. Business terms are stored as objects in a semantic layer, which are accessed through business views\nask for feedback on each stage and implement changes accordingly\nInfinite Scrolling vs Pagination (article with code)\n\nInfinite scrolling is easier to use on mobile devices and requires fewer ‚Äúclicks‚Äù compared to pagination.\n\nMight increase user engagement on your page as viewers stay on your website longer; continuously searching for relevant content when they have no particular goal in mind.\n\n\n\nMap the user‚Äôs workflow\n\nWhy?\n\nSeeing how the users accomplish the task now will give you a better understanding of the whole process or even point to a competitive advantage.\nMaybe the existing tools have some inefficiencies that your app can address?\nOr maybe some parts of the current solution can be reused to speed up user adoption and onboarding?\n\nQuestions\n\nWhat is the main reason for building the app?\nWho are the users?\nWhat will the users accomplish with your app? What are their business goals?\nHow have they done it so far? Are they already used to any particular tools or workflows?\n\n\nFor Analysis apps\n\ninclude at least a few toy datasets so people know what the data is supposed to look like and be able to play around with the app\n\nadd a description card for each toy data set\n\nadd tool-tips for input ui text\nadd descriptions/instructions to the top of each page\n\nApps are for users (not you)\n\nWhat decision does the user make by visiting this app?\nWhat does the user learn?\nHow do they interact with your app to do this?\n\nData visualizations\n\nWhat do you want to tell with the data?\nWhat is the context?\nWho is your audience?",
    "crumbs": [
      "Shiny",
      "<span class='chapter-number'>38</span>¬† <span class='chapter-title'>General</span>"
    ]
  },
  {
    "objectID": "qmd/shiny-general.html#testing",
    "href": "qmd/shiny-general.html#testing",
    "title": "38¬† General",
    "section": "38.4 Testing",
    "text": "38.4 Testing\n\nCan test at any stage of the development process, even before it starts.\n\nYou can use the wireframes or mockups and manually change the ‚Äúscreens‚Äù as the user ‚Äúperforms an action‚Äù in the app.\n\n**Do not to leave testing for the last moment. When the app development is finished, rebuilding the UI will be costly.**\nTypes\n\nunit tests\nperformance tests\nin-depth user interviews\n\nSessions with the user where you ask the person to perform several tasks within the tool. This way you can see if there are any recurring problems with navigation or the general ease of use.\n\nhallway tests\n\nShort version of an in-depth user interview\nJust ask your colleagues to use the app for 5-10 minutes and note their impressions.\n\n\nFinal Checklist\n\nIs the app design responsive?\nDoes the user know what‚Äôs wrong when receiving an error message?\nIs the user well informed about the state of the app, e.g.¬†when waiting for the calculation, is it clear when it will be finished?",
    "crumbs": [
      "Shiny",
      "<span class='chapter-number'>38</span>¬† <span class='chapter-title'>General</span>"
    ]
  },
  {
    "objectID": "qmd/shiny-general.html#js",
    "href": "qmd/shiny-general.html#js",
    "title": "38¬† General",
    "section": "38.5 JS",
    "text": "38.5 JS\n\nwww - directiory for images, styles, or JS script. Same directory as app.R\nUse a javascript file + library\n\nExample stylish pop-up notification\n# Shiny App¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† // JS script; basic toastify notification\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† Shiny.addCustomMessageHandler('notify', function(msg){\nui &lt;- fluidPage(¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† Toastify({\n¬† tags$head(¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† text: msg\n¬† ¬† # toastify css dependency¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† }).showToast();\n¬† ¬† tags$link(¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† });\n¬† ¬† ¬† rel = \"stylesheet\",\n¬† ¬† ¬† type = \"text/css\",\n¬† ¬† ¬† href = \"https://cdn.jsdelivr.net/npm/toastify-js/src/toastify.min.css\"\n¬† ¬† ),\n¬† ¬† # our script\n¬† ¬† tags$script(\n¬† ¬† ¬† type = \"text/javascript\",\n¬† ¬† ¬† src = \"script.js\"\n¬† ¬† )\n¬† ),\n¬† h1(\"Notifications\"),\n¬† textInput(\"text\", \"Text\"),\n¬† actionButton(\"show\", \"Show\"),\n¬† # toastify js dependency\n¬† tags$script(\n¬† ¬† type = \"text/javascript\",\n¬† ¬† src = \"https://cdn.jsdelivr.net/npm/toastify-js\"\n¬† )\n)\nserver &lt;- function(input, output, session){\n¬† observeEvent(input$show, {\n¬† ¬† session$sendCustomMessage(\n¬† ¬† ¬† \"notify\",\n¬† ¬† ¬† input$text\n¬† })\n}\nshinyApp(ui, server)\n\ntoastify.js is a library for some snazzy notifications you can use that pop-up in your browser when a event (e.g.¬†button pushed) has happened.\nButton is pressed in ui, sends ‚Äúmessage‚Äù (text, data.frame, etc.) to server, server sends ‚Äúmessage‚Äù to js script in ui which triggers a notification in the browser.\nUI\n\ntags\\(head includes: * tags\\)link links to the toastify CSS stylesheet\n\ntags$script points to the path of the js script and provides the identifier (‚Äútype‚Äù), ‚Äútext/javascript‚Äù\n\ntags$script links to the toastify js dependency and also has the identifier, ‚Äútext/javascript‚Äù\n\nServer\n\nObserveEvent¬† listens for ui input ‚Äúshow‚Äù button to be pressed\nsendCustomMessage¬† is a js function to send a message from the server to the browser\n\nidentifier (‚Äútype‚Äù) = ‚Äúnotify‚Äù says which custom handler to use in the js script\n\n\njs script\n\naddCustomMessageHandler¬† is a js function that receives the message from the server\n\n‚Äúnotify‚Äù is the identifer that it listens for\nToastify()¬† takes a JSON formatted list of args\nshowToast¬† method sends snazzy notification to the user‚Äôs browser.",
    "crumbs": [
      "Shiny",
      "<span class='chapter-number'>38</span>¬† <span class='chapter-title'>General</span>"
    ]
  },
  {
    "objectID": "qmd/shiny-general.html#sec-shiny-gen-misc",
    "href": "qmd/shiny-general.html#sec-shiny-gen-misc",
    "title": "General",
    "section": "",
    "text": "Resources\n\nRStudio https://shiny.rstudio.com/tutorial/\n\nModules https://shiny.rstudio.com/articles/modules.html\nBeginner https://rstudio-education.github.io/shiny-course/\n\nHadley ebook https://mastering-shiny.org/\nEngineering Production-Grade Shiny Apps - {golem} book\nAdv. UI ebook https://unleash-shiny.rinterface.com/\nSimple app packaged, modules, etc.\n\nhttps://github.com/2DegreesInvesting/scenarioSelector#learning-shiny\n\nExamples and templates for various usecases (link)\n\nA common source of poor Shiny performance and maintainability is placing a large amount of logic within a single reactive statement such as an¬†observe()\n\nInstead of adding a bunch of renderPlots to an observe(), liberally use reactive variables and your code becomes much cleaner, faster, and more maintainable\n\noptions(shiny.autoreload = TRUE) before running shiny::runApp(appDir = \"./tests\", port = 8080) allows you to make changes to app.R and view them almost instantaeously in the viewer. (link)\nComparison with other products (article)",
    "crumbs": [
      "Shiny",
      "General"
    ]
  },
  {
    "objectID": "qmd/shiny-general.html#sec-shiny-gen-wkflw",
    "href": "qmd/shiny-general.html#sec-shiny-gen-wkflw",
    "title": "General",
    "section": "Workflow",
    "text": "Workflow\n\nDancho‚Äôs\n\nSimple App\n\nSomething bare bones that has ui and server sections\nCould be a company or personal template or the ‚ÄúHello, World‚Äù app with slider and text output\n\nBetter App\n\nAdd images (e.g.¬†navbar logo, splash page background)\nAdjust layout (add sidebar, main panel, etc. to UI)\nAdd blank cards (pill, tab, etc.) which will later be filled with charts, tables, etc.\nTitles (navbar, side bar, main panel, etc.)\nUse Bootstrap 5 and style.css files (style navbar, titles, etc.)\n\nPlaceholder App\n\nIntegrate analysis into the app\nMake cards look professional\nUse widgets to make interactive visualizations (charts, tables)\n\nReactive App\n\nMake all calculations and events reactive\n\ni.e user input changes output (e.g.¬†data values/calculations that go into the charts, tables, etc.)\n\nThis is the basic working application\n\nFinal Product\n\nFeature examples\n\nAdd full data ingestion\nAdd search\nAdd additional inputs and calculations (i.e.¬†more analysis)\n\nCreate modular code (utils directory)\n\nCode gets long for more complicated apps. Modularizing it makes it more readable and easier to maintain\n\nMake output downloadable into a report\n\n\nAetna Insurance\n\nNotes from Q&A Shatrunjai Singh | R in Insurance | RStudio\nRStudio Connect\n\nproduction & dev ‚Äúversion‚Äù (branches?)\n\nSteps\n\nBuild minimum viable product (MVP)\nUpload to dev branch\nPerform Test & Learn for about a month\n\nGive it to a small number of ppl\nHave them use it and get feedback\n\nCreate ‚Äúdev2‚Äù app\n\nApply fixes\nAdd most critical elements according to the feedback\n\nGive to a different, slightly larger group of people\n\nLooking for issues, bugs, etc.\n\nOnce satisfied that all major kinks are worked out, move app to production branch\n\nLaunched to entire company\n\nFor the 1st 6 months, ‚Äúrecalibrate analysis‚Äù every 1.5 months\nRecalibrate every 6 months afterwards\n\nDo packages need updated?\nDo the models need retrained\nIs the app still working as intended?\nIf it‚Äôs all good, do some code optimization, refactoring, etc.",
    "crumbs": [
      "Shiny",
      "General"
    ]
  },
  {
    "objectID": "qmd/shiny-general.html#sec-shiny-gen-design",
    "href": "qmd/shiny-general.html#sec-shiny-gen-design",
    "title": "General",
    "section": "Design",
    "text": "Design\n\nMisc\n\nPackages\n\n{designer} - Makes the initial generation of a UI wireframe of a shiny application as quick and simple as possible. Good for PoCs. Allows you to drag and drop components.\n{shinyuieditor} - Seems like it‚Äôs similar to {designer}\n\nAlso see BizSci video\n\n\nSemantic Layer - A business representation of corporate data that helps end users access data autonomously using common business terms. By using common business terms, rather than data language, to access, manipulate, and organize information, a semantic layer simplifies the complexity of business data. Business terms are stored as objects in a semantic layer, which are accessed through business views\nAsk for feedback on each stage and implement changes accordingly\nInfinite Scrolling vs Pagination (article with code)\n\nInfinite scrolling is easier to use on mobile devices and requires fewer ‚Äúclicks‚Äù compared to pagination.\n\nMight increase user engagement on your page as viewers stay on your website longer; continuously searching for relevant content when they have no particular goal in mind.\n\n\n\nMap the user‚Äôs workflow\n\nWhy?\n\nSeeing how the users accomplish the task now will give you a better understanding of the whole process or even point to a competitive advantage.\nMaybe the existing tools have some inefficiencies that your app can address?\nOr maybe some parts of the current solution can be reused to speed up user adoption and onboarding?\n\nQuestions\n\nWhat is the main reason for building the app?\nWho are the users?\nWhat will the users accomplish with your app? What are their business goals?\nHow have they done it so far? Are they already used to any particular tools or workflows?\n\n\nFor Analysis apps\n\ninclude at least a few toy datasets so people know what the data is supposed to look like and be able to play around with the app\n\nadd a description card for each toy data set\n\nadd tool-tips for input ui text\nadd descriptions/instructions to the top of each page\n\nApps are for users (not you)\n\nWhat decision does the user make by visiting this app?\nWhat does the user learn?\nHow do they interact with your app to do this?\n\nData visualizations\n\nWhat do you want to tell with the data?\nWhat is the context?\nWho is your audience?",
    "crumbs": [
      "Shiny",
      "General"
    ]
  },
  {
    "objectID": "qmd/shiny-general.html#sec-shiny-gen-test",
    "href": "qmd/shiny-general.html#sec-shiny-gen-test",
    "title": "General",
    "section": "Testing",
    "text": "Testing\n\nCan test at any stage of the development process, even before it starts.\n\nYou can use the wireframes or mockups and manually change the ‚Äúscreens‚Äù as the user ‚Äúperforms an action‚Äù in the app.\n\n**Do not to leave testing for the last moment. When the app development is finished, rebuilding the UI will be costly.**\nTypes\n\nUnit Tests\nPerformance Tests\nIn-Depth user interviews\n\nSessions with the user where you ask the person to perform several tasks within the tool. This way you can see if there are any recurring problems with navigation or the general ease of use.\n\nHallway Tests\n\nShort version of an in-depth user interview\nJust ask your colleagues to use the app for 5-10 minutes and note their impressions.\n\n\nFinal Checklist\n\nIs the app design responsive?\nDoes the user know what‚Äôs wrong when receiving an error message?\nIs the user well informed about the state of the app, e.g.¬†when waiting for the calculation, is it clear when it will be finished?",
    "crumbs": [
      "Shiny",
      "General"
    ]
  },
  {
    "objectID": "qmd/shiny-general.html#sec-shiny-gen-js",
    "href": "qmd/shiny-general.html#sec-shiny-gen-js",
    "title": "General",
    "section": "JS",
    "text": "JS\n\nwww - Directiory for images, styles, or JS script. Same directory as app.R\nUse a javascript file + library\nExample Stylish Pop-Up Notification\n\nJS Script\n// JS script; basic toastify notification\nShiny.addCustomMessageHandler('notify', \n                              function(msg){\n                                Toastify({\n                                  text: msg\n                                }).showToast();\n                              });\n\nKeep in www\ntoastify.js is a library for some snazzy notifications you can use that pop-up in your browser when a event (e.g.¬†button pushed) has happened.\naddCustomMessageHandler is a js function that receives the message from the server\n\n‚Äúnotify‚Äù is the identifer that it listens for\nToastify() takes a JSON formatted list of args\nshowToast method sends snazzy notification to the user‚Äôs browser.\n\n\nUI and Server\nui &lt;- fluidPage(\n¬† tags$head(\n¬† ¬† # toastify css dependency\n¬† ¬† tags$link(\n¬† ¬† ¬† rel = \"stylesheet\",\n¬† ¬† ¬† type = \"text/css\",\n¬† ¬† ¬† href = \"https://cdn.jsdelivr.net/npm/toastify-js/src/toastify.min.css\"\n¬† ¬† ),\n¬† ¬† # our script\n¬† ¬† tags$script(\n¬† ¬† ¬† type = \"text/javascript\",\n¬† ¬† ¬† src = \"script.js\"\n¬† ¬† )\n¬† ),\n¬† h1(\"Notifications\"),\n¬† textInput(\"text\", \"Text\"),\n¬† actionButton(\"show\", \"Show\"),\n¬† # toastify js dependency\n¬† tags$script(\n¬† ¬† type = \"text/javascript\",\n¬† ¬† src = \"https://cdn.jsdelivr.net/npm/toastify-js\"\n¬† )\n)\nserver &lt;- function(input, output, session){\n¬† observeEvent(input$show, {\n¬† ¬† session$sendCustomMessage(\n¬† ¬† ¬† \"notify\",\n¬† ¬† ¬† input$text\n¬† })\n}\nshinyApp(ui, server)\n\nButton is pressed in ui, sends ‚Äúmessage‚Äù (text, data.frame, etc.) to server, server sends ‚Äúmessage‚Äù to js script in ui which triggers a notification in the browser.\nUI\n\ntags$head includes:\n\ntags$link links to the toastify CSS stylesheet\ntags$script points to the path of the js script and provides the identifier (‚Äútype‚Äù), ‚Äútext/javascript‚Äù\n\ntags$script links to the toastify js dependency and also has the identifier, ‚Äútext/javascript‚Äù\n\nServer\n\nObserveEvent listens for ui input ‚Äúshow‚Äù button to be pressed\nsendCustomMessage is a js function to send a message from the server to the browser\n\nidentifier (‚Äútype‚Äù) = ‚Äúnotify‚Äù says which custom handler to use in the js script",
    "crumbs": [
      "Shiny",
      "General"
    ]
  },
  {
    "objectID": "qmd/shiny-modules.html",
    "href": "qmd/shiny-modules.html",
    "title": "Modules",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Shiny",
      "Modules"
    ]
  },
  {
    "objectID": "qmd/shiny-modules.html#sec-shiny-mod-misc",
    "href": "qmd/shiny-modules.html#sec-shiny-mod-misc",
    "title": "Modules",
    "section": "",
    "text": "Resources\n\nRStudio\nMastering Shiny, Ch. 19\nEngineering Production-Grade Shiny Apps - {golem} book\nSimple app packaged, modules, etc.\n\nhttps://github.com/2DegreesInvesting/scenarioSelector#learning-shiny\n\n\nPackages\n\n{golem} - also has many example apps and resources\n{rhino} - appsilon package",
    "crumbs": [
      "Shiny",
      "Modules"
    ]
  },
  {
    "objectID": "qmd/shiny-modules.html#sec-shiny-mod-wkflw",
    "href": "qmd/shiny-modules.html#sec-shiny-mod-wkflw",
    "title": "Modules",
    "section": "Workflow",
    "text": "Workflow\n\nDecompose Requirements to help determine modules\n\nPick a subset of the data\nThe metrics of interest are:\n\ne.g.¬†average departure delay, average arrival delay, proportion of flights with an arrival delay &gt; 5min\n\nFor each metric of interest, users should:\n\nExample:\n\nSee a time series plot of the average daily value of the metric\nBe able to click a download button to download a PNG of the plot\nRead a text summary that reports the number of days with breaches\n\n\n\nDemo function for testing modules\nmodule_demo &lt;- function () {\n¬† ¬† # define test data\n¬† ¬† df &lt;- data.frame(x = 1:30, y = 1:30)\n\n¬† ¬† # call module components\n¬† ¬† ui &lt;- fluidPage(module_ui(\"x\"))\n¬† ¬† server &lt;- function(input, output, session) {\n¬† ¬† ¬† ¬† module_server(\"x\", reactive([{df}]{style='color: #990000'}))\n¬† ¬† }\n¬† ¬† shinyApp(ui, server)\n}\nExample:\n\n\nHandle one project requirement at a time\nBuild the module for text summary of a metric\ntext_ui &lt;- function(id) {\n\n¬† fluidRow(textOutput(NS(id, \"text\")))\n\n}\n\ntext_server &lt;- function(id, df, vbl, threshhold) {\n\n¬† moduleServer(id, function(input, output, session) {\n\n¬† ¬† n &lt;- reactive({sum(df()[[vbl]] &gt; threshhold){style='color: #990000'}[}]{style='color: #990000'})\n\n¬† ¬† output$text &lt;- renderText({\n¬† ¬† ¬† paste(\"In this month\",\n¬† ¬† ¬† ¬† ¬† ¬† vbl,\n¬† ¬† ¬† ¬† ¬† ¬† \"exceeded the average daily threshhold of\",\n¬† ¬† ¬† ¬† ¬† ¬† threshhold,\n¬† ¬† ¬† ¬† ¬† ¬† \"a total of\", n(), \"days\")\n¬† ¬† })\n¬† })\n}\n\ntext_demo &lt;- function() {\n¬† df &lt;- data.frame(day = 1:30, arr_delay = 1:30)\n¬† ui &lt;- fluidPage(text_ui(\"x\"))\n¬† server &lt;- function(input, output, session) {\n¬† ¬† text_server(\"x\", reactive([{df}]{style='color: #990000'}), \"arr_delay\", 15)\n¬† }\n¬† shinyApp(ui, server)\n}\nBuild the module for plotting\nplot_ui &lt;- function(id) {\n\n¬† fluidRow(\n¬† ¬† column(11, plotOutput(NS(id, \"plot\"))),\n¬† ¬† column( 1, downloadButton(NS(id, \"dnld\"), label = \"\"))\n)\n\n}\n\nplot_server &lt;- function(id, df, vbl, threshhold = NULL) {\n\n¬† moduleServer(id, function(input, output, session) {\n¬† ¬† plot &lt;- reactive({viz_monthly(df(), vbl, threshhold){style='color: #990000'}[}]{style='color: #990000'})\n¬† ¬† output$plot &lt;- renderPlot({plot(){style='color: #990000'}[}]{style='color: #990000'})\n¬† ¬† output$dnld &lt;- downloadHandler(\n¬† ¬† ¬† filename = function() {paste0(vbl, '.png'){style='color: #990000'}[}]{style='color: #990000'},\n¬† ¬† ¬† content = function(file) {ggsave(file, plot()){style='color: #990000'}[}]{style='color: #990000'}\n¬† ¬† )\n¬† })\n\n}\n\nplot_demo &lt;- function() {\n¬† ¬† df &lt;- data.frame(day = 1:30, arr_delay = 1:30)\n¬† ¬† ui &lt;- fluidPage(plot_ui(\"x\"))\n¬† ¬† server &lt;- function(input, output, session) {\n¬† ¬† ¬† ¬† plot_server(\"x\", reactive([{df}]{style='color: #990000'}), \"arr_delay\")\n¬† ¬† }\n¬† ¬† shinyApp(ui, server)\n}\nCompose both modules\nmetric_ui &lt;- function(id) {\n\n¬† ¬† fluidRow(\n¬† ¬† ¬† text_ui(NS(id, \"metric\")),\n¬† ¬† ¬† plot_ui(NS(id, \"metric\"))\n¬† ¬† )\n\n}\n\nmetric_server &lt;- function(id, df, vbl, threshhold) {\n\n¬† ¬† moduleServer(id, function(input, output, session) {\n¬† ¬† ¬† text_server(\"metric\", df, vbl, threshhold)\n¬† ¬† ¬† plot_server(\"metric\", df, vbl, threshhold)\n¬† ¬† })\n\n}\n\nmetric_demo &lt;- function() {\n\n¬† ¬† df &lt;- data.frame(day = 1:30, arr_delay = 1:30)\n¬† ¬† ui &lt;- fluidPage(metric_ui(\"x\"))\n¬† ¬† server &lt;- function(input, output, session) {\n¬† ¬† metric_server(\"x\", reactive([{df}]{style='color: #990000'}), \"arr_delay\", 15)\n¬† ¬† }\n¬† ¬† shinyApp(ui, server)\n\n}\nBuild-out app\nui &lt;- fluidPage(\n\n¬† ¬† titlePanel(\"Flight Delay Report\"),\n\n¬† ¬† sidebarLayout(\n¬† ¬† ¬† sidebarPanel = sidebarPanel(\n¬† ¬† ¬† ¬† selectInput(\"month\", \"Month\",\n¬† ¬† ¬† ¬† choices = setNames(1:12, month.abb),\n¬† ¬† ¬† ¬† selected = 1\n¬† ¬† ¬† )\n¬† ¬† ),\n¬† ¬† mainPanel = mainPanel(\n¬† ¬† ¬† h2(textOutput(\"title\")),\n¬† ¬† ¬† h3(\"Average Departure Delay\"),\n¬† ¬† ¬† metric_ui(\"dep_delay\"),\n¬† ¬† ¬† h3(\"Average Arrival Delay\"),\n¬† ¬† ¬† metric_ui(\"arr_delay\"),\n¬† ¬† ¬† h3(\"Proportion Flights with &gt;5 Min Arrival Delay\"),\n¬† ¬† ¬† metric_ui(\"ind_arr_delay\")\n¬† ¬† )\n)\nserver &lt;- function(input, output, session) {\n\n¬† ¬† output$title &lt;- renderText({paste(month.abb[as.integer(input$month)], \"Report\"){style='color: #990000'}[}]{style='color: #990000'})\n¬† ¬† df_month &lt;- reactive({filter(ua_data, month == input$month){style='color: #990000'}[}]{style='color: #990000'})\n¬† ¬† metric_server(\"dep_delay\", df_month, vbl = \"dep_delay\", threshhold = 10)\n¬† ¬† metric_server(\"arr_delay\", df_month, vbl = \"arr_delay\", threshhold = 10)\n¬† ¬† metric_server(\"ind_arr_delay\", df_month, vbl = \"ind_arr_delay\", threshhold = 0.5)\n\n}",
    "crumbs": [
      "Shiny",
      "Modules"
    ]
  },
  {
    "objectID": "qmd/shiny-modules.html#sec-shiny-mod-bp",
    "href": "qmd/shiny-modules.html#sec-shiny-mod-bp",
    "title": "Modules",
    "section": "Best Practices",
    "text": "Best Practices\n\nThere shouldn‚Äôt be any deeply nested bracketing in your code\nPass reactive variable to modules. Don‚Äôt call modules inside of some other reactive statement like¬†observe()\n\nExample: Bad module calling\n¬† ¬† observe({\n¬† ¬† ¬† # Process data before sending it into the module\n¬† ¬† ¬† if (input$filterTo != \"special\") {\n¬† ¬† ¬† ¬† myModuleServer(\n¬† ¬† ¬† ¬† ¬† data %&gt;%¬†\n¬† ¬† ¬† ¬† ¬† ¬† filter(val == input$filterTo)\n¬† ¬† ¬† ¬† )\n¬† ¬† ¬† } else {\n¬† ¬† ¬† ¬† # Handle special case¬†\n¬† ¬† ¬† ¬† myModuleServer(\n¬† ¬† ¬† ¬† ¬† data %&gt;%¬†\n¬† ¬† ¬† ¬† ¬† ¬† ...\n¬† ¬† ¬† ¬† )\n¬† ¬† ¬† }\n¬† ¬† })\nExample: Good module calling\n¬† ¬† server &lt;- function(input, output, session) {\n\n¬† ¬† ¬† # initialise the app state...\n¬† ¬† ¬† app_state &lt;- reactiveValues(...)\n\n¬† ¬† ¬† ...¬†\n\n¬† ¬† ¬† # add server logic\n¬† ¬† ¬† mod_commute_mode(\"mode\", app_state)\n¬† ¬† ¬† mod_commute_map(\"map\", app_state)\n¬† ¬† ¬† mod_commute_table(\"table\", app_state)\n¬† ¬† ¬† mod_commute_filter(\"filter\", app_state)\n¬† ¬† }\n\nLiberally use reactive variables and your code becomes much cleaner, faster, and more maintainable\ndatasheet_df &lt;- reactive({\n¬† sample_data %&gt;%\n¬† filter(site %in% input$selectSiteDatasheets) %&gt;%¬†\n¬† ...\n})\n# Download button\noutput$download_datasheet &lt;- downloadHandler(\n¬† filename = function() {\n¬† ¬† paste(\"spreadsheet_\", input$selectSiteDatasheets, \".csv\", sep = \"\")\n¬† },\n¬† content = function(file) {\n¬† ¬† write.csv(datasheet_df(), file, row.names = FALSE)\n¬† }\n)\n\nHere datasheet_df is a reactive variable that Shiny will always keep up to date. Therefore the download button only needs to describe that it uses whatever the current value of that reactive is.\n\nThis separation keeps the code easy to reason about and allows easy use of datasheet_df in other contexts than just the download button.",
    "crumbs": [
      "Shiny",
      "Modules"
    ]
  },
  {
    "objectID": "qmd/mathematics-linear-algebra.html",
    "href": "qmd/mathematics-linear-algebra.html",
    "title": "Linear Algebra",
    "section": "",
    "text": "Resources",
    "crumbs": [
      "Mathematics",
      "Linear Algebra"
    ]
  },
  {
    "objectID": "qmd/mathematics-linear-algebra.html#sec-math-linalg-resc",
    "href": "qmd/mathematics-linear-algebra.html#sec-math-linalg-resc",
    "title": "Linear Algebra",
    "section": "",
    "text": "See Matrix Cookbook pdf in R &gt;&gt; Documents &gt;&gt; Mathematics\n\nderivatives, inverses, statistics, probability, etc.\n\nLink - A lot of matrix properties as related to regression, covariance, coefficients, etc.\nEBOOK statistical linear algebra: basics, transformations, decompositions, linear systems, regression - Matrix Algebra for Educational Scientists\nVideo Course: Linear Algebra for Data Science - Basics, Least Squares, Covariance, Regression, PCA, SVD",
    "crumbs": [
      "Mathematics",
      "Linear Algebra"
    ]
  },
  {
    "objectID": "qmd/mathematics-linear-algebra.html#sec-math-linalg-matmult",
    "href": "qmd/mathematics-linear-algebra.html#sec-math-linalg-matmult",
    "title": "Linear Algebra",
    "section": "Matrix Multiplication",
    "text": "Matrix Multiplication",
    "crumbs": [
      "Mathematics",
      "Linear Algebra"
    ]
  },
  {
    "objectID": "qmd/mathematics-linear-algebra.html#sec-math-linalg-matalg",
    "href": "qmd/mathematics-linear-algebra.html#sec-math-linalg-matalg",
    "title": "Linear Algebra",
    "section": "Matrix Algebra",
    "text": "Matrix Algebra\n\nAn expected value equation (VC stands for variance-covariance in example) multiplied by a matrix, C.\n\n\nC is factored out of an expected value as C\nC is factored out of a transpose as CT",
    "crumbs": [
      "Mathematics",
      "Linear Algebra"
    ]
  },
  {
    "objectID": "qmd/mathematics-linear-algebra.html#sec-math-linalg-fact",
    "href": "qmd/mathematics-linear-algebra.html#sec-math-linalg-fact",
    "title": "Linear Algebra",
    "section": "Factorization",
    "text": "Factorization",
    "crumbs": [
      "Mathematics",
      "Linear Algebra"
    ]
  },
  {
    "objectID": "qmd/networks-analysis.html",
    "href": "qmd/networks-analysis.html",
    "title": "Analysis",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Networks",
      "Analysis"
    ]
  },
  {
    "objectID": "qmd/networks-analysis.html#sec-net-anal-misc",
    "href": "qmd/networks-analysis.html#sec-net-anal-misc",
    "title": "Analysis",
    "section": "",
    "text": "Packages\n\n{statnet} - statnet is a collection of software packages for statistical network analysis that are designed to work together, with a common data structure and API, to provide seamless access to a broad range of network analytic and graphical methodology.\n\nList of individual package tutorials\nModels fit with MCMC, so can be slow.\n\n\nResources\n\nNetwork Analysis: Integrating Social Network Theory, Method, and Application with R\nR for Social Network Analysis\n\nAnalysis Questions\n\nAt a given moment in time:\n\nWho is connected to whom? Who is not connected?\nWhere, and who are the hubs?\nWhere and about what are the clusters? Are there silos?\n\nChanges over time:\n\nAre new connection forming?\nAre new patterns of connectivity forming?\nHow was our network before and after the introduction of an activity?\n\n\nIssues with Statistics\n\nThey are unable to leverage node features at all. All nodes with the same values for these summary statistics are indistinguishable from each other.\nThere is no learnable component in the production of these features. We cannot fit a custom objective or train them jointly with a downstream task.",
    "crumbs": [
      "Networks",
      "Analysis"
    ]
  },
  {
    "objectID": "qmd/networks-analysis.html#sec-net-anal-terms",
    "href": "qmd/networks-analysis.html#sec-net-anal-terms",
    "title": "Analysis",
    "section": "Terms",
    "text": "Terms\n\nCentrality\n\nMeasures, abstractly, how important a given graph is to the connectivity of the overall graph\nHigher for nodes which lie in paths that efficiently connect many nodes to each other.\nTypes:\n\nBetweenness - Nodes with high betweenness centrality tend to be the ‚Äúcrossroads‚Äù between nodes, i.e.¬†when seeking to connect with another node that isn‚Äôt immediately adjacent, it will typically involve a node with high betweeness centrality.\n\nThese nodes are important in keeping the network connected. Likely important intermediaries or bridges\nCalculated by counting the number of shortest paths that pass through a node and dividing by the total number of shortest paths in the network.\n\nCloseness - Nodes with high closeness centrality have quick access to many other nodes. These nodes have the shortest distance, in network terms, to all other nodes.\n\nThese nodes are important in spreading information to all other nodes as quickly and efficiently as possible.\nCloseness Centrality = 1 / (Sum of SPD from the node to all other nodes)\n\nWhere SPD is the shortest path distance. In practice, this would be done with a shortest path algorithm like Breadth-First Search¬†or¬†Dijkstra‚Äôs algorithm.\n\n\n\n\nClusters\n\nCluster Clique - a cluster that has at least one node that‚Äôs connected to another node outside of the cluster\nCluster Silo - a cluster that has no node connected to any other node outside of the cluster\n\nClustering coefficient\n\nMeasures the density of a node‚Äôs local portion of the graph.\nNodes who have neighbors that are all connected to each other will have a higher clustering coefficient\n\nDegeneracy - A network model is degenerate when the space that an MCMC sampler can explore is so constrained that the only way to get the observed g(y) is essentially to flicker between full and empty graphs in the right proportion.\n\nA good indication that you have a degenerate model is that you have NA values for standard errors on your model parameter estimates. You can‚Äôt calculate a variance ‚Äì and, therefore, a standard error ‚Äì if you simply flicker between full and empty graphs.\n\nDegree aka Degree Centrality - Total edges a given node has.\nDensity - the number of edges in the observed network divided by the number of possible edges\nEdges (aka Dyads)- connection between two nodes. Depending on the type of connection, the edge can have a direction.\nEdge Bundling - visually bundle together similar edges to reduce the visual clutter within a graph\nMultiplexity - The number of connections between two nodes\n\nCould be represented by the thickness, darkness, etc. of an edge between 2 nodes\nNodes that have high multiplicity with each other typically form clusters\n\nRandom Graph - A network with n nodes where the edges between nodes occur randomly with probability P (each potential edge is one Bernoulli trial). Network density is typically used for P.\nTransitivity of a relation means that when there is a tie from i to j, and also from j to h, then there is also a tie from i to h: friends of my friends are my friends\n\n\nTransitivity Index (aka Clustering Index) = # Transitive Triads / # Potentially Transitive Triads\n\nHas a range between 0 and 1 where 1 is a transitive graph.\nFor random graphs, the expected value of the transitivity index is close to the density of the graph.",
    "crumbs": [
      "Networks",
      "Analysis"
    ]
  },
  {
    "objectID": "qmd/networks-analysis.html#sec-net-anal-layalg",
    "href": "qmd/networks-analysis.html#sec-net-anal-layalg",
    "title": "Analysis",
    "section": "Layouts",
    "text": "Layouts\n\nSpring\n\nFruchterman-Reingold force-directed algorithm\n\narranges the nodes so the edges have similar length and minimum crossing edges\n\n\nRandom - nodes positioned uniformly at random in the unit square\nCircular - nodes on a circle\nBipartite - nodes in two straight lines\nSpectral - nodes positioned using the eigenvectors of the graph Laplacian",
    "crumbs": [
      "Networks",
      "Analysis"
    ]
  },
  {
    "objectID": "qmd/networks-analysis.html#sec-net-anal-ergm",
    "href": "qmd/networks-analysis.html#sec-net-anal-ergm",
    "title": "Analysis",
    "section": "Exponential Random Graph Models (ERGM)",
    "text": "Exponential Random Graph Models (ERGM)\n\nAnalogous to logistic regression: ERGMs predict the probability that a pair of nodes in a network will have a tie between them.\nMisc\n\nNotes from Introduction to ERGMs\nPackages\n\n{statnet} - See Misc &gt;&gt; Packages\n{ergmito} - Simulation and estimation of Exponential Random Graph Models (ERGMs) for small networks (up to 5 vertices for directed graphs and 7 for undirected networks) using exact statistics\n\nIn the case of small networks, the calculation of the likelihood of ERGMs becomes computationally feasible, which allows us avoiding approximations and do exact calculation, ultimately obtaining MLEs directly.\n\n\nCan be used for directed, undirected, valued, unvalued, and bipartite networks.\nFitting a model with just edges is kind of like an intercept-only regression model.\nLess informative for dense networks.\n\nExamples: From Introduction to ERGMs\n\nDense\n\n\n\nNot Dense\n\n\nTriads aka triangles (i.e.¬†transitive relationships) cause problems in ERGMs (more triads ‚Üí denser graph). They often lead to degeneracy.\n\nNAs for standard error estimates are a good indication of degeneracy.\nSince ERGMs don‚Äôt handle triads well, it is NOT recommended using ‚Äútriangle‚Äù as an adjustment variable in your model\n\n\n\nGoal: Describe the local selection forces that shape the global structure of a network\nExamples of networks examined using ERGM include knowledge networks, organizational networks, colleague networks, social media networks, and networks of scientific development.\nThe basic principle underlying the method is comparison of an observed network to Exponential Random Graphs.\n\nThe Null Hypothesis is a Erdos-Renyi graph\n\nA random graph where the degree of any node is binomially distributed (with n-1 Bernoulli trials per node, for a directed graph). (n is the number of nodes)\n\n\nEquation:\n\\[\n\\text{logit}(Y_{ij} = 1 \\; | \\; y_{ij}^c) = \\theta^T \\delta (y_{ij})\n\\]\n\n\\(\\theta\\) is a vector of coefficients\n\\(y_{ij}\\) denotes ijth dyad in graph \\(y\\)\n\nIf \\(y_{ij} = 1\\), then i and j are connected by an edge.\nIf \\(y_{ij} = 0\\), then i and j are NOT connected by an edge.\n\\(y_{ij}^c\\) is the complement (i.e.¬†all other pairs of vertices in \\(y\\) other than (i, j)).\n\n\\(\\delta(y_{ij})\\) is the change statistic. A measure of how the graph statistic \\(g(y)\\) changes if the ijth vertex is toggled on or off.\n\n\\(\\delta(y_{ij}) = g(y_{ij}^+) - g(y_{ij}^-)\\)\n\\(y_{ij}^+\\) is the same network as \\(y\\) except that \\(y_{ij} = 1\\).\n\\(y_{ij}^-\\) is the same network as \\(y\\) except that \\(y_{ij} = 0\\).\n\n\nExample:{statnet} Edges model\nbottmodel.01 &lt;- ergm(bott[[4]] ~ edges)\n\n## Evaluating log-likelihood at the estimate.\nsummary(bottmodel.01)\n## \n## ==========================\n## Summary of model fit\n## ==========================\n## \n## Formula:   bott[[4]] ~ edges\n## \n## Iterations:  4 out of 20 \n## \n## Monte Carlo MLE Results:\n##       Estimate Std. Error MCMC %  p-value    \n## edges  -0.7621     0.2047      0 0.000313 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n##      Null Deviance: 152.5  on 110  degrees of freedom\n##  Residual Deviance: 137.6  on 109  degrees of freedom\n##  \n## AIC: 139.6    BIC: 142.3    (Smaller is better.)\nExample:{statnet} Edges and Triads model\nsummary(bott[[4]]~edges+triangle)\n##    edges triangle \n##       35       40\n\nbottmodel.02 &lt;- ergm(bott[[4]]~edges+triangle)\nsummary(bottmodel.02)\n## \n## ==========================\n## Summary of model fit\n## ==========================\n## \n## Formula:   bott[[4]] ~ edges + triangle\n## \n## Iterations:  2 out of 20 \n## \n## Monte Carlo MLE Results:\n##          Estimate Std. Error MCMC % p-value\n## edges    -0.55772    0.58201      0   0.340\n## triangle -0.05674    0.15330      0   0.712\n## \n##      Null Deviance: 152.5  on 110  degrees of freedom\n##  Residual Deviance: 137.5  on 108  degrees of freedom\n##  \n## AIC: 141.5    BIC: 146.9    (Smaller is better.)\nattributes of the individuals who make up our graph vertices may affect their propensity to form (or receive) ties\ntest this hypothesis, we can employ nodal covariates using the nodecov() term.\nbottmodel.03 &lt;- ergm(bott[[4]]~edges+nodecov('age.month'))\nsummary(bottmodel.03)\n## \n## ==========================\n## Summary of model fit\n## ==========================\n## \n## Formula:   bott[[4]] ~ edges + nodecov(\"age.month\")\n## \n## Iterations:  4 out of 20 \n## \n## Monte Carlo MLE Results:\n##                    Estimate Std. Error MCMC % p-value\n## edges             -1.526483   1.335799      0   0.256\n## nodecov.age.month  0.009501   0.016352      0   0.562\n## \n##      Null Deviance: 152.5  on 110  degrees of freedom\n##  Residual Deviance: 137.3  on 108  degrees of freedom\n##  \n## AIC: 141.3    BIC: 146.7    (Smaller is better.)\n\nresult suggests that in this dataset, the combined age of the children involved in a dyad as no effect on the probability of a tie between them, in either direction.\n\nimitation is a directed relationship. We might expect that older children are more likely to be role models to others. To test this hypothesis, we can use the directed variants of nodecov(): nodeocov() (effect of an attribute on out-degree) and nodeicov() (effect of an attribute on in-degree). We note that the nodecov() group of terms are for numeric attributes; nodefactor() terms are available for categorical attributes.\nbottmodel.03b &lt;- ergm(bott[[4]]~edges+nodeicov('age.month'))\n\nsummary(bottmodel.03b)\n## \n## ==========================\n## Summary of model fit\n## ==========================\n## \n## Formula:   bott[[4]] ~ edges + nodeicov(\"age.month\")\n## \n## Iterations:  4 out of 20 \n## \n## Monte Carlo MLE Results:\n##                    Estimate Std. Error MCMC % p-value   \n## edges              -2.89853    0.96939      0 0.00345 **\n## nodeicov.age.month  0.05225    0.02272      0 0.02340 * \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n##      Null Deviance: 152.5  on 110  degrees of freedom\n##  Residual Deviance: 132.1  on 108  degrees of freedom\n##  \n## AIC: 136.1    BIC: 141.5    (Smaller is better.)\n\nThe number of other children who imitated a child increase with the child‚Äôs age",
    "crumbs": [
      "Networks",
      "Analysis"
    ]
  },
  {
    "objectID": "qmd/networks-analysis.html#sec-net-anal-ndembd",
    "href": "qmd/networks-analysis.html#sec-net-anal-ndembd",
    "title": "Analysis",
    "section": "Node Embeddings",
    "text": "Node Embeddings\n\nLearnable vectors of numbers that can be mapped to each node in the graph, allowing us to learn a unique representation for each node.\n\nUse as features in a downstream model.\n\nMethods\n\nDeepWalk and Node2vec papers\n\nUse the concept of a random walk, which involves beginning at a given node and randomly traversing edges, to produce pairs of nodes that are nearby each other.\nTrained by maximizing the cosine similarity between nodes that co-occurred in random walks.\n\nThis training objective leverages the homophily assumption, which states that nodes that are connected to each other tend to be similar to each other.\n\n\n\nIssues with Embeddings\n\nThey do not use node features at all. They assume that close-by nodes are similar without actually using the node features to confirm this assumption.\n\nThey rely on a fixed mapping from node to embedding (i.e.¬†this is a transductive method).\n\nFor dynamic graphs, where new nodes and edges may be added, the algorithm must be re-ran from scratch, and all node embeddings need to be recalculated. In real-world problems, this is quite a big issue, as most online platforms have new users signing up every day, and new edges being created constantly.",
    "crumbs": [
      "Networks",
      "Analysis"
    ]
  },
  {
    "objectID": "qmd/networks-analysis.html#sec-net-anal-gcn",
    "href": "qmd/networks-analysis.html#sec-net-anal-gcn",
    "title": "Analysis",
    "section": "Graph Convolutional Networks (GCN)",
    "text": "Graph Convolutional Networks (GCN)\n\nLearns representations of nodes by learning a function that aggregates a node‚Äôs neighborhood (the set of nodes connected to the original node), using both graph structure and node features.\n\nThese representations are a function of a node‚Äôs neighborhood and are not hardcoded per node (i.e.¬†this is an inductive method), so changes in graph structure do not require re-training the model.\n\nFor unsupervised learning tasks, the method is similar to Node2vec/DeepWalk\nLayers\n\nA layer takes a weighted average of the node features in the original node‚Äôs neighborhood, and the weights are learned by training the network\nAdding layers produces aggregations that use more of the graph.\n\nThe span of the subgraph used to produce a node‚Äôs embedding is expanded by 1 hop.",
    "crumbs": [
      "Networks",
      "Analysis"
    ]
  },
  {
    "objectID": "qmd/shiny-production.html",
    "href": "qmd/shiny-production.html",
    "title": "Production",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Shiny",
      "Production"
    ]
  },
  {
    "objectID": "qmd/shiny-production.html#misc",
    "href": "qmd/shiny-production.html#misc",
    "title": "38¬† Production",
    "section": "",
    "text": "{cicerone} - Provides guided tours to your shiny apps",
    "crumbs": [
      "Shiny",
      "<span class='chapter-number'>38</span>¬† <span class='chapter-title'>Production</span>"
    ]
  },
  {
    "objectID": "qmd/shiny-production.html#docker",
    "href": "qmd/shiny-production.html#docker",
    "title": "38¬† Production",
    "section": "38.2 Docker",
    "text": "38.2 Docker\n\n38.2.1 Misc\n\nPackages\n\n{deps} (article)\n\nLightweight method that installs packages via a DESCRIPTION json file\n\nBlend between package and renv approaches to dependency management\n\nProject scripts contain roxygen-like, decorator code about packages and those are used to build the DESCRIPTION json file\nImage size should be similar to the ‚Äúdeps/DESCRIPTION‚Äù method above\n\n\n\n\n\n38.2.2 Images\n\nImages have a 2GB limit\nBase Image Build Times\n\n\nSmaller images take longer to load because all the packages/libraries have to be compiled\nrstudio/base, rocker/shiny, rocker/r-ubuntu use binary packages\nrocker/r-bspm and eddelbuettel/r2u uses binary packages and apt-get\n\nOrder of Image Layers\n\n\nOrder is bottom to top when writing your dockerfile. (see example below)\nImportant for the bottom layers to be things that you might change most often. This way you don‚Äôt have to reinstall R everytime you change something in your app code.\n\n\n\n\n38.2.3 Dockerfiles\n\nExample\nFrom rocker/r-base:4.0.4\n\nRUN apt-get update && apt-get install -y \\\n    --no-install-recommneds \\\n    make libssl-dev libxml2-dev \\\n    && rm -rf /var/lib/apt/lists/*\n\nCOPY Rprofile.site /etc/R\nRUN install.r shiny ggplot2 htmltools plotly\n\nRUN addgroup --system app && adduser --system --ingroup app app\nWORKDIR /home/app\nCOPY app .\nRUN chown app:app -R /home/app\nUSER app\n\nEXPOSE 3838\nCMD [\"R\", \"-e\", \"shiny::runApp('/home/app', port = 3838, host = '0.0.0.0')\"]\n\n\n\n38.2.4 Dependencies\n\nMethods (article)\n# Explicit\nRUN install.r shiny ggplot2 htmltools plotly\n\n# DESCRIPTION file\nRUN install.r remotes\nCOPY DESCRIPTION .\nRUN Rscript -e \"remotes::install_deps()\"\n\n# renv package\nRUN install.r remotes renv\nCOPY ./renv.lock .\nRUN Rscript -e \"options(renv.consent=TRUE); renv::restore(lockfiile='/home/app/renv.lock')\"\n\nlittler::install.r ({littler} is installed on all Rocker images)\n\nImage Size Comparison\nREPOSITORY¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† TAG¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† SIZE\nanalythium/covidapp-shiny¬† renv¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† 1.7GB\nanalythium/covidapp-shiny¬† deps¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† 1.18GB\nanalythium/covidapp-shiny¬† basic¬† ¬† ¬† ¬† ¬† ¬† ¬†  1.24GB\n\nbasic Image (aka ‚Äúexplicit‚Äù method): 105 packages installed\ndeps Image (aka ‚ÄúDESCRIPTION‚Äù method): Has remotes added on top of these; remotes::install_deps() to install packages from the DESCRIPTION file\nrenv Image: Has remotes, renv and BH as extras.\n\nBH seems to be responsible for the size difference, this package provides Boost C++ header files.\nYour app will probably work perfectly fine without BH.\nYou can use renv::remove(\"BH\") to remove BH from the project or use the ‚Äúcustom‚Äù model and list all the packages to be added to the lockfile\n\n\n\n\n\n38.2.5 Security\n\nExample: Dockerfile\nRUN addgroup --system app && adduser --system --ingroup app app\nWORKDIR /home/app\nCOPY app .\nRUN chown app:app -R /home/app\nUSER app\n\nBest to create user groups and not run app as a root sudo\nchown, an abbreviation of change owner, is used on Unix and Unix-like operating systems to change the owner of file system files, directories. Unprivileged users who wish to change the group membership of a file that they own may use chgrp\n\n\n\n\n38.2.6 CI/CD\n\n\nUse github action for docker caching which builds the image and pushes your image to Docker Hub\nThen your compute instance (PaaS) pulls the image from that registry",
    "crumbs": [
      "Shiny",
      "<span class='chapter-number'>38</span>¬† <span class='chapter-title'>Production</span>"
    ]
  },
  {
    "objectID": "qmd/shiny-production.html#data-strategies",
    "href": "qmd/shiny-production.html#data-strategies",
    "title": "38¬† Production",
    "section": "38.3 Data Strategies",
    "text": "38.3 Data Strategies\n\nDo as little processing as possible in the app\nOptions\n\nBundle datasets alongside the source code, such that wherever the app is deployed, those datasets are available.\n\nDrawback: data would need to be kept in version control along with your source code, and a new deployment of the app would be required whenever the data is updated.\n\nFor frequently updated datasets, this is impractical, but may be valuable if those datasets are unlikely to change during the lifetime of a project.\n\n\nKeep data in cloud storage\n\nAllows collaborators to upload new data on an ad-hoc basis without touching the app itself. The app would then download data from the cloud for presentation during each user session\nBetter for frequently updated datasets\nOptimization (loading in secs instead of mins)\n\nUse parquet file format\nCaching the data for the app‚Äôs landing page or use asynchronous computing to initiate downloading the data while presenting a less data-intensive landing page\nPipeline\n\n\nPartition data:\n\nRaw data that is not computationally expensive or needs no processing\nProcessed data that is more computationally expensive to process.\n\nThe data processing pipeline is outside of the app. (e.g.¬†GitHub Actions workflow)\nData storage and app server should be in the same region to reduce latency\n\nCreate Pipeline Triggers\n\nWhen new raw data is uploaded, then data gets processed and into the app in a timely manner.\nWhen the source code for the app or the data processing pipeline change, the data processing pipeline should run afresh.\nIf changes to the structure of the raw dataset mean that the data processing pipeline produces malformed processed data, there should be a way to log that.",
    "crumbs": [
      "Shiny",
      "<span class='chapter-number'>38</span>¬† <span class='chapter-title'>Production</span>"
    ]
  },
  {
    "objectID": "qmd/shiny-production.html#optimization",
    "href": "qmd/shiny-production.html#optimization",
    "title": "38¬† Production",
    "section": "38.4 Optimization",
    "text": "38.4 Optimization\n\nMisc\n\nResources\n\nOffload Shiny‚Äôs Workload: COVID-19 processing for the WHO/Europe\n\nOverview of how they reduced loading times of a World Health Organization app from minutes to seconds\n\nShiny docs: caching and async programming\nChapter 15 of Engineering Production-Grade Shiny Apps covers, in detail, some common performance pitfalls and how to solve them.\n\n\nReduce the amount of computation occuring inside the app (by caching plots and tables, or by precomputing your data),\nAnalyze whether the app could be using too much reactivity or regenerating UI elements unnecessarily",
    "crumbs": [
      "Shiny",
      "<span class='chapter-number'>38</span>¬† <span class='chapter-title'>Production</span>"
    ]
  },
  {
    "objectID": "qmd/shiny-production.html#development",
    "href": "qmd/shiny-production.html#development",
    "title": "38¬† Production",
    "section": "38.5 Development",
    "text": "38.5 Development\n\nAWS deployment\n\n\nNotes from\n\nCreating a Dashboard Framework with AWS (Part 1)\n\nFeatures\n\nSecure, end-to-end encrypted (SSL, TLS) access to dashboards.\nSecure authentication through E-mail and Single-Sign-On (SSO).\nHorizontal scalability of dashboards according to usage, fail-safe.\nEasy adaptability by analysts through automation and continuous integration (CI/CD).\nEasy maintenance and extensibility for system operators.\n\nComponents\n\nApplication Load Balancer (ALB) to handle secure end-to-end (SSL) encrypted access to the dashboards based on different host names (host-based-routing).\nAWS Cognito for user authentication based on E-mail and SSO through Ping Federate.\nAWS Fargate for horizontal scalability, fail-safe operations and easy maintenance.\nAWS Codepipeline and Codebuild for automated build of dashboard Docker containers.\nExtensive usage of managed services requiring low maintenance (Fargate, Cognito, ALB) and Amazon Cloud Development Kit (CDK) to define and manage infrastructure-as-code managed in Git and deployed via Code Pipelines.\n\n\nAetna Insurance Notes from Q&A Shatrunjai Singh | R in Insurance | RStudio\n\nRStudio Connect\n\nproduction & dev ‚Äúversion‚Äù (branches?)\n\nSteps\n\nBuild minimum viable product (MVP)\nUpload to dev branch\nPerform Test & Learn for about a month\n\nGive it to a small number of ppl\nHave them use it and get feedback\n\nCreate ‚Äúdev2‚Äù app\n\nApply fixes\nAdd most critical elements according to the feedback\n\nGive to a different, slightly larger group of people\n\nLooking for issues, bugs, etc.\n\nOnce satisfied that all major kinks are worked out, move app to production branch\n\nLaunched to entire company\n\nFor the 1st 6 months, ‚Äúrecalibrate analysis‚Äù every 1.5 months\nRecalibrate every 6 months afterwards\n\nDo packages need updated?\nDo the models need retrained\nIs the app still working as intended?\nIf it‚Äôs all good, do some code optimization, refactoring, etc.",
    "crumbs": [
      "Shiny",
      "<span class='chapter-number'>38</span>¬† <span class='chapter-title'>Production</span>"
    ]
  },
  {
    "objectID": "qmd/shiny-production.html#security-1",
    "href": "qmd/shiny-production.html#security-1",
    "title": "38¬† Production",
    "section": "38.6 Security",
    "text": "38.6 Security\n\n{shinyauthr}\n\nfor user authentication. App doesn‚Äôt get rendered until user is authenticated\n\n{backendlessr}\n\nremotes::install_gitlab(\"rresults/backendlessr\")\nProvides user registration, login, logout, profiles\n\nSome small amounts of data can be sent to backendless (not log files)\n\nThere are some user counting functions in the package for keeping track of API calls (I think)\n\n\nAPI wrapper for backendless platform\nfree for up to 60 API calls per minute 1M API calls per month\n\nIf you need more, use invite code p6tvk3 when you create a new app to get 25% off for the first 6 months\n\nDemo login: backendlessr::shiny_demo_login (http://0.0.0.0:3838)\n\n\nClicking the ‚ÄúRegister‚Äù button calls the backendless API\nDisplays ‚ÄúSuccessful‚Äù if user registration worked\n\nSteps\n\nRegister at backendless and get a free account\nRegister you app\n\n\nGet the Application ID and API key\n\nAdd ID and key to .Renviron (for testing)\nBACKENDLESS_APPID = \"&lt;app id&gt;\"\nBACKENDLESS_KEY = \"&lt;api_key&gt;\"\n\n\nAdd ID and key to Docker Swarm secrets (for production)\nInstall package and run functions in your shiny app\n\nExample (basic)\n\nApp listens for actionLink open_login which is the user loggin into the app\nThen showElement unhides all the hidden things in the ui (e.g.¬†logout, loginmain module, plot) and actionLink log in becomes hidden (I think)\n\nBunch of user credential functions available but here‚Äôs a list that currently aren‚Äôt:\n\nSocial Network logins (e.g.¬†to allow a user to use their Facebook account to log in to our service);\nAsync calls;\nGeolocation;\nLogging (send log messages of your app to Backendless platform).\nEnhanced Security\nUser password reset",
    "crumbs": [
      "Shiny",
      "<span class='chapter-number'>38</span>¬† <span class='chapter-title'>Production</span>"
    ]
  },
  {
    "objectID": "qmd/shiny-production.html#sec-shiny-prod-misc",
    "href": "qmd/shiny-production.html#sec-shiny-prod-misc",
    "title": "Production",
    "section": "",
    "text": "{cicerone} - Provides guided tours to your shiny apps",
    "crumbs": [
      "Shiny",
      "Production"
    ]
  },
  {
    "objectID": "qmd/shiny-production.html#sec-shiny-prod-dock",
    "href": "qmd/shiny-production.html#sec-shiny-prod-dock",
    "title": "Production",
    "section": "Docker",
    "text": "Docker\n\nMisc\n\nPackages\n\n{deps} (article)\n\nLightweight method that installs packages via a DESCRIPTION json file\n\nBlend between package and renv approaches to dependency management\n\nProject scripts contain roxygen-like, decorator code about packages and those are used to build the DESCRIPTION json file\nImage size should be similar to the ‚Äúdeps/DESCRIPTION‚Äù method above\n\n\n\n\n\nImages\n\nImages have a 2GB limit\nBase Image Build Times\n\n\nSmaller images take longer to load because all the packages/libraries have to be compiled\nrstudio/base, rocker/shiny, rocker/r-ubuntu use binary packages\nrocker/r-bspm and eddelbuettel/r2u uses binary packages and apt-get\n\nOrder of Image Layers\n\n\nOrder is bottom to top when writing your dockerfile. (see example below)\nImportant for the bottom layers to be things that you might change most often. This way you don‚Äôt have to reinstall R everytime you change something in your app code.\n\n\n\n\nDockerfiles\n\nExample\nFrom rocker/r-base:4.0.4\n\nRUN apt-get update && apt-get install -y \\\n    --no-install-recommneds \\\n    make libssl-dev libxml2-dev \\\n    && rm -rf /var/lib/apt/lists/*\n\nCOPY Rprofile.site /etc/R\nRUN install.r shiny ggplot2 htmltools plotly\n\nRUN addgroup --system app && adduser --system --ingroup app app\nWORKDIR /home/app\nCOPY app .\nRUN chown app:app -R /home/app\nUSER app\n\nEXPOSE 3838\nCMD [\"R\", \"-e\", \"shiny::runApp('/home/app', port = 3838, host = '0.0.0.0')\"]\n\n\n\nDependencies\n\nMethods (article)\n# Explicit\nRUN install.r shiny ggplot2 htmltools plotly\n\n# DESCRIPTION file\nRUN install.r remotes\nCOPY DESCRIPTION .\nRUN Rscript -e \"remotes::install_deps()\"\n\n# renv package\nRUN install.r remotes renv\nCOPY ./renv.lock .\nRUN Rscript -e \"options(renv.consent=TRUE); renv::restore(lockfiile='/home/app/renv.lock')\"\n\nlittler::install.r ({littler} is installed on all Rocker images)\n\nImage Size Comparison\nREPOSITORY¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† TAG¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† SIZE\nanalythium/covidapp-shiny¬† renv¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† 1.7GB\nanalythium/covidapp-shiny¬† deps¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† 1.18GB\nanalythium/covidapp-shiny¬† basic¬† ¬† ¬† ¬† ¬† ¬† ¬†  1.24GB\n\nbasic Image (aka ‚Äúexplicit‚Äù method): 105 packages installed\ndeps Image (aka ‚ÄúDESCRIPTION‚Äù method): Has remotes added on top of these; remotes::install_deps() to install packages from the DESCRIPTION file\nrenv Image: Has remotes, renv and BH as extras.\n\nBH seems to be responsible for the size difference, this package provides Boost C++ header files.\nYour app will probably work perfectly fine without BH.\nYou can use renv::remove(\"BH\") to remove BH from the project or use the ‚Äúcustom‚Äù model and list all the packages to be added to the lockfile\n\n\n\n\n\nSecurity\n\nExample: Dockerfile\nRUN addgroup --system app && adduser --system --ingroup app app\nWORKDIR /home/app\nCOPY app .\nRUN chown app:app -R /home/app\nUSER app\n\nBest to create user groups and not run app as a root sudo\nchown, an abbreviation of change owner, is used on Unix and Unix-like operating systems to change the owner of file system files, directories. Unprivileged users who wish to change the group membership of a file that they own may use chgrp\n\n\n\n\nCI/CD\n\n\nUse github action for docker caching which builds the image and pushes your image to Docker Hub\nThen your compute instance (PaaS) pulls the image from that registry",
    "crumbs": [
      "Shiny",
      "Production"
    ]
  },
  {
    "objectID": "qmd/shiny-production.html#sec-shiny-prod-datstrat",
    "href": "qmd/shiny-production.html#sec-shiny-prod-datstrat",
    "title": "Production",
    "section": "Data Strategies",
    "text": "Data Strategies\n\nDo as little processing as possible in the app\nBundle datasets alongside the source code, such that wherever the app is deployed, those datasets are available.\n\nDrawback: data would need to be kept in version control along with your source code, and a new deployment of the app would be required whenever the data is updated.\n\nFor frequently updated datasets, this is impractical, but may be valuable if those datasets are unlikely to change during the lifetime of a project.\n\n\nKeep data in cloud storage\n\nAllows collaborators to upload new data on an ad-hoc basis without touching the app itself. The app would then download data from the cloud for presentation during each user session\nBetter for frequently updated datasets\nOptimization (loading in secs instead of mins)\n\nUse parquet file format\nCaching the data for the app‚Äôs landing page or use asynchronous computing to initiate downloading the data while presenting a less data-intensive landing page\nPipeline\n\n\nPartition data:\n\nRaw data that is not computationally expensive or needs no processing\nProcessed data that is more computationally expensive to process.\n\nThe data processing pipeline is outside of the app. (e.g.¬†GitHub Actions workflow)\nData storage and app server should be in the same region to reduce latency\n\nCreate Pipeline Triggers\n\nWhen new raw data is uploaded, then data gets processed and into the app in a timely manner.\nWhen the source code for the app or the data processing pipeline change, the data processing pipeline should run afresh.\nIf changes to the structure of the raw dataset mean that the data processing pipeline produces malformed processed data, there should be a way to log that.",
    "crumbs": [
      "Shiny",
      "Production"
    ]
  },
  {
    "objectID": "qmd/shiny-production.html#sec-shiny-prod-opt",
    "href": "qmd/shiny-production.html#sec-shiny-prod-opt",
    "title": "Production",
    "section": "Optimization",
    "text": "Optimization\n\nMisc\n\nResources\n\nOffload Shiny‚Äôs Workload: COVID-19 processing for the WHO/Europe\n\nOverview of how they reduced loading times of a World Health Organization app from minutes to seconds\n\nShiny docs: caching and async programming\nChapter 15 of Engineering Production-Grade Shiny Apps covers, in detail, some common performance pitfalls and how to solve them.\n\n\nReduce the amount of computation occuring inside the app (by caching plots and tables, or by precomputing your data),\nAnalyze whether the app could be using too much reactivity or regenerating UI elements unnecessarily",
    "crumbs": [
      "Shiny",
      "Production"
    ]
  },
  {
    "objectID": "qmd/shiny-production.html#sec-shiny-prod-deploy",
    "href": "qmd/shiny-production.html#sec-shiny-prod-deploy",
    "title": "Production",
    "section": "Deployment",
    "text": "Deployment\n\nAWS\n\n\nNotes from\n\nCreating a Dashboard Framework with AWS (Part 1)\n\nFeatures\n\nSecure, end-to-end encrypted (SSL, TLS) access to dashboards.\nSecure authentication through E-mail and Single-Sign-On (SSO).\nHorizontal scalability of dashboards according to usage, fail-safe.\nEasy adaptability by analysts through automation and continuous integration (CI/CD).\nEasy maintenance and extensibility for system operators.\n\nComponents\n\nApplication Load Balancer (ALB) to handle secure end-to-end (SSL) encrypted access to the dashboards based on different host names (host-based-routing).\nAWS Cognito for user authentication based on E-mail and SSO through Ping Federate.\nAWS Fargate for horizontal scalability, fail-safe operations and easy maintenance.\nAWS Codepipeline and Codebuild for automated build of dashboard Docker containers.\nExtensive usage of managed services requiring low maintenance (Fargate, Cognito, ALB) and Amazon Cloud Development Kit (CDK) to define and manage infrastructure-as-code managed in Git and deployed via Code Pipelines.",
    "crumbs": [
      "Shiny",
      "Production"
    ]
  },
  {
    "objectID": "qmd/shiny-production.html#sec-shiny-prod-sec",
    "href": "qmd/shiny-production.html#sec-shiny-prod-sec",
    "title": "Production",
    "section": "Security",
    "text": "Security\n\nMisc\n\nAlso see Docker &gt;&gt; Security\n{shinyauthr}\n\nfor user authentication. App doesn‚Äôt get rendered until user is authenticated\n\n\n\n\n{backendlessr}\n\nremotes::install_gitlab(\"rresults/backendlessr\")\nProvides user registration, login, logout, profiles\n\nSome small amounts of data can be sent to backendless (not log files)\n\nThere are some user counting functions in the package for keeping track of API calls (I think)\n\n\nAPI wrapper for backendless platform\nFree for up to 60 API calls per minute 1M API calls per month\n\nIf you need more, use invite code p6tvk3 when you create a new app to get 25% off for the first 6 months\n\nDemo login: backendlessr::shiny_demo_login (http://0.0.0.0:3838)\n\n\nClicking the ‚ÄúRegister‚Äù button calls the backendless API\nDisplays ‚ÄúSuccessful‚Äù if user registration worked\n\nSteps\n\nRegister at backendless and get a free account\nRegister you app\n\n\nGet the Application ID and API key\n\nAdd ID and key to .Renviron (for testing)\nBACKENDLESS_APPID = \"&lt;app id&gt;\"\nBACKENDLESS_KEY = \"&lt;api_key&gt;\"\nAdd ID and key to Docker Swarm secrets (for production)\nInstall package and run functions in your shiny app\n\nExampleBasic\nui &lt;- \n  fluidPage(\n    titlePanel(\"User access demo\"),\n    actionLink(\"open login\", \"Log in\"),\n    hidden(actionLink(\"logout\", \"Log out\")),\n    hidden(moduleLoginUserUI(\"loginmain\")),\n    hidden(plotOutput(\"demoPlot\", height = \"300px\"))\n    #...\n)\n\nserver &lt;- \n  function(input, output, session) {\n    # ...\n    ObserveEvent(\n      input$open_login,\n      {showElement(\"loginmain-login-form\")}\n      #...\n    )\n  }\n\nApp listens for actionLink open_login which is the user login into the app\nThen showElement unhides all the hidden things in the ui (e.g.¬†logout, loginmain module, demoPlot) and actionLink Log in becomes hidden (I think)\n\n\n\nBunch of user credential functions available but here‚Äôs a list that currently aren‚Äôt:\n\nSocial Network logins (e.g.¬†to allow a user to use their Facebook account to log in to our service);\nAsync calls;\nGeolocation;\nLogging (send log messages of your app to Backendless platform).\nEnhanced Security\nUser password reset",
    "crumbs": [
      "Shiny",
      "Production"
    ]
  },
  {
    "objectID": "qmd/shiny-ui.html",
    "href": "qmd/shiny-ui.html",
    "title": "38¬† UI",
    "section": "",
    "text": "Design Principles",
    "crumbs": [
      "Shiny",
      "<span class='chapter-number'>38</span>¬† <span class='chapter-title'>UI</span>"
    ]
  },
  {
    "objectID": "qmd/shiny-ui.html#design-principles",
    "href": "qmd/shiny-ui.html#design-principles",
    "title": "38¬† UI",
    "section": "",
    "text": "Misc\n\nNotes from\n\nErik Kennedy thread\n\n\nDe-Emphasize Dividing lines\n\nUse fonts that subtly convey brand\n\nContent cards should be lighter than their bg (in dark mode too)\n\nDon‚Äôt resize icons\n\n\nTheir level of detail and stroke weights are meant to work best at a certain size.\nInstead, try adding a border or container around them for some extra visual pop\n\nREMOVE-HIDE-LIGHTEN for cleaner designs\n\nBe consistent until it‚Äôs time not to be consistent\n\n\nBreak consistency example: when you‚Äôre trying to catch the user‚Äôs eye\n\nGood imagery\n\nRemove Congestion\n\nSee Designing Accessible Research with R/Shiny UI ‚Äì Part 2 for an example of an iterable workflow\nCongestion is when an app shows everything, all at once and that‚Äôs stressful for the user. It doesn‚Äôt elicit the behavior we want for an engaging, learning experience.\nThis stress triggers two subconscious actions ‚Äì run or freeze. Both lead to cognitive load and negative perception, and ultimately, failed adoption.\n\nRunning is expressed in bounce rate. A user will enter the app, become frustrated, and leave.\nFreezing means that a user pauses with a delayed time to understand. This can be expressed in a number of ways, but most likely a combination of longer session times with aimless user behavior.\n\nMinimize options, legends, etc. and move to the edges of the app (e.g.¬†header, sides)\n\nExample\n\nPrototype\n\n\nIssues\n\nThe tree selector consumes space on the dashboard\nSelecting via images doesn‚Äôt add a lot of value as not all tree species are easily identified by image\nThe UI is too dark\nThe visual accessibility feature was lost as some colors don‚Äôt work on a dark background\n\n\nFinal\n\nDashboard\n\n\nThe legend moved to the card (see mobile for more details\n\nTree card gives details about species and a summary\nClicking upper right icon flips the card to display the bar chart.\n\nSwitching languages ‚Äã‚Äãand information about the application moved to the header (right side of blue nav bar)\nClicking text in header opens pop-up with options. One for type of tree and one for type of scenario (beteen blue nav bar and map)\nAll other interactive elements that were scattered on the screen are now organized into the right vertical panel on the map. Here is also the color blindness option for people with visual disabilities, as the main focus of the application is on the color ratio on the map\n\nMobile\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n‚Äúlanguages‚Äù and ‚Äúabout‚Äù moved to the top",
    "crumbs": [
      "Shiny",
      "<span class='chapter-number'>38</span>¬† <span class='chapter-title'>UI</span>"
    ]
  },
  {
    "objectID": "qmd/shiny-ui.html#style",
    "href": "qmd/shiny-ui.html#style",
    "title": "38¬† UI",
    "section": "38.2 Style",
    "text": "38.2 Style\n\nMisc\n\nImages and style.css files should go into the ‚Äúwww‚Äù folder\n\nImplementing a css file\n\nExample: Basic\n\n\nmain.css file\n@import url('https://fonts.googleapis.com/css2?family=Poppins&display=swap');\n* {\n¬† margin: 0;\n¬† padding: 0;\n¬† box-sizing: border-box;\n¬† font-family: 'Poppins', sans-serif;\n}\nbody {\n¬† padding: 1rem;\n}\n#map {\n¬† height: 98vh !important;\n¬† border-radius: 0.5rem !important;\n}\napp.R\nui &lt;- fluidPage(\n¬† tags$head(tags$link(rel = \"stylesheet\", type = \"text/css\", href = \"main.css\")),\n¬† sidebarLayout(\n¬† ¬† ...\n¬† )\n)",
    "crumbs": [
      "Shiny",
      "<span class='chapter-number'>38</span>¬† <span class='chapter-title'>UI</span>"
    ]
  },
  {
    "objectID": "qmd/shiny-ui.html#responsiveness",
    "href": "qmd/shiny-ui.html#responsiveness",
    "title": "38¬† UI",
    "section": "38.3 Responsiveness",
    "text": "38.3 Responsiveness\n\nResponsiveness isn‚Äôt the same as performance. Performance is about completing an operation in the minimal amount of time, while responsiveness is about meeting human needs for feedback when executing an action\nMisc\n\nNotes from Improving the responsiveness of Shiny applications\n\nButton Click Registered\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(Left) Default arrow for your app\n(Middle) Hand indicates to the user that the app has registered their hovering over a button\n(Right) Arrow + Pie indicates to the user that the app has registered their clicking the button\n(generic) entry into CSS file\nhtml.shiny-busy .container-fluid {\n  cursor: wait;\n}\n\nLooks like this code is to produce something like the right-side image\n\n\nFor touch devices (i.e.¬†without cursor), you might want to take a look at {shinycssloaders} and/or {waiter}\nIf the user might have to wait longer than a few seconds for the process they‚Äôve just set in motion to complete, you should consider a progress indicator.\n\n{shiny} has a progress indicator, while {waiter} also offers a nice built-in-to-button option",
    "crumbs": [
      "Shiny",
      "<span class='chapter-number'>38</span>¬† <span class='chapter-title'>UI</span>"
    ]
  },
  {
    "objectID": "qmd/shiny-ui.html#sec-shiny-ui-desprin",
    "href": "qmd/shiny-ui.html#sec-shiny-ui-desprin",
    "title": "38¬† UI",
    "section": "",
    "text": "Misc\n\nNotes from\n\nErik Kennedy thread\n\n\nDe-Emphasize Dividing lines\n\nUse fonts that subtly convey brand\n\nContent cards should be lighter than their bg (in dark mode too)\n\nDon‚Äôt resize icons\n\n\nTheir level of detail and stroke weights are meant to work best at a certain size.\nInstead, try adding a border or container around them for some extra visual pop\n\nREMOVE-HIDE-LIGHTEN for cleaner designs\n\nBe consistent until it‚Äôs time not to be consistent\n\n\nBreak consistency example: when you‚Äôre trying to catch the user‚Äôs eye\n\nGood imagery\n\nRemove Congestion\n\nSee Designing Accessible Research with R/Shiny UI ‚Äì Part 2 for an example of an iterable workflow\nCongestion is when an app shows everything, all at once and that‚Äôs stressful for the user. It doesn‚Äôt elicit the behavior we want for an engaging, learning experience.\nThis stress triggers two subconscious actions ‚Äì run or freeze. Both lead to cognitive load and negative perception, and ultimately, failed adoption.\n\nRunning is expressed in bounce rate. A user will enter the app, become frustrated, and leave.\nFreezing means that a user pauses with a delayed time to understand. This can be expressed in a number of ways, but most likely a combination of longer session times with aimless user behavior.\n\nMinimize options, legends, etc. and move to the edges of the app (e.g.¬†header, sides)\n\nExample\n\nPrototype\n\n\nIssues\n\nThe tree selector consumes space on the dashboard\nSelecting via images doesn‚Äôt add a lot of value as not all tree species are easily identified by image\nThe UI is too dark\nThe visual accessibility feature was lost as some colors don‚Äôt work on a dark background\n\n\nFinal\n\nDashboard\n\n\nThe legend moved to the card (see mobile for more details\n\nTree card gives details about species and a summary\nClicking upper right icon flips the card to display the bar chart.\n\nSwitching languages and information about the application moved to the header (right side of blue nav bar)\nClicking text in header opens pop-up with options. One for type of tree and one for type of scenario (beteen blue nav bar and map)\nAll other interactive elements that were scattered on the screen are now organized into the right vertical panel on the map. Here is also the color blindness option for people with visual disabilities, as the main focus of the application is on the color ratio on the map\n\nMobile\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n‚Äúlanguages‚Äù and ‚Äúabout‚Äù moved to the top",
    "crumbs": [
      "Shiny",
      "<span class='chapter-number'>38</span>¬† <span class='chapter-title'>UI</span>"
    ]
  },
  {
    "objectID": "qmd/shiny-ui.html#sec-shiny-ui-style",
    "href": "qmd/shiny-ui.html#sec-shiny-ui-style",
    "title": "38¬† UI",
    "section": "Style",
    "text": "Style\n\nMisc\n\nImages and style.css files should go into the ‚Äúwww‚Äù folder\n\nImplementing a css file\n\nExample: Basic\n\n\nmain.css file\n@import url('https://fonts.googleapis.com/css2?family=Poppins&display=swap');\n* {\n¬† margin: 0;\n¬† padding: 0;\n¬† box-sizing: border-box;\n¬† font-family: 'Poppins', sans-serif;\n}\nbody {\n¬† padding: 1rem;\n}\n#map {\n¬† height: 98vh !important;\n¬† border-radius: 0.5rem !important;\n}\napp.R\nui &lt;- fluidPage(\n¬† tags$head(tags$link(rel = \"stylesheet\", type = \"text/css\", href = \"main.css\")),\n¬† sidebarLayout(\n¬† ¬† ...\n¬† )\n)",
    "crumbs": [
      "Shiny",
      "<span class='chapter-number'>38</span>¬† <span class='chapter-title'>UI</span>"
    ]
  },
  {
    "objectID": "qmd/shiny-ui.html#sec-shiny-ui-resp",
    "href": "qmd/shiny-ui.html#sec-shiny-ui-resp",
    "title": "38¬† UI",
    "section": "Responsiveness",
    "text": "Responsiveness\n\nResponsiveness isn‚Äôt the same as performance. Performance is about completing an operation in the minimal amount of time, while responsiveness is about meeting human needs for feedback when executing an action\nMisc\n\nNotes from Improving the responsiveness of Shiny applications\n\nButton Click Registered\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(Left) Default arrow for your app\n(Middle) Hand indicates to the user that the app has registered their hovering over a button\n(Right) Arrow + Pie indicates to the user that the app has registered their clicking the button\n(generic) entry into CSS file\nhtml.shiny-busy .container-fluid {\n  cursor: wait;\n}\n\nLooks like this code is to produce something like the right-side image\n\n\nFor touch devices (i.e.¬†without cursor), you might want to take a look at {shinycssloaders} and/or {waiter}\nIf the user might have to wait longer than a few seconds for the process they‚Äôve just set in motion to complete, you should consider a progress indicator.\n\n{shiny} has a progress indicator, while {waiter} also offers a nice built-in-to-button option",
    "crumbs": [
      "Shiny",
      "<span class='chapter-number'>38</span>¬† <span class='chapter-title'>UI</span>"
    ]
  },
  {
    "objectID": "qmd/shiny-ui-examples.html",
    "href": "qmd/shiny-ui-examples.html",
    "title": "UI Examples",
    "section": "",
    "text": "Map Apps",
    "crumbs": [
      "Shiny",
      "UI Examples"
    ]
  },
  {
    "objectID": "qmd/shiny-ui-examples.html#sec-shiny-uiex-map",
    "href": "qmd/shiny-ui-examples.html#sec-shiny-uiex-map",
    "title": "UI Examples",
    "section": "",
    "text": "Dancho full page map UI from app in learning lab 28\n\nCode only shown partially in learning lab 83\n\nServer part also shown\n\nControl panel had a transparent background\nClickable logos\n{shiny}, {fresh}, {shinyWidgets}\nUI\nmy_theme &lt;- create_theme(\n¬† ¬† theme = \"paper\",\n¬† ¬† bs_vars_global(\n¬† ¬† ¬† ¬† body_bg = \"black\",\n¬† ¬† ¬† ¬† text_color = \"#fff\"\n¬† ¬† ),\n¬† ¬† bs_vars_navbar(\n¬† ¬† ¬† ¬† default_bg = \"#75h8d1\",\n¬† ¬† ¬† ¬† default_color = \"#ffffff\",\n¬† ¬† ¬† ¬† default_link_color = \"#ffffff\",\n¬† ¬† ¬† ¬† default_link_active_color = \"#75b8d1\",\n¬† ¬† ¬† ¬† default_link_active_bg = \"#ffffff\",\n¬† ¬† ¬† ¬† default_link_hover_color = \"#2c3e50\"\n¬† ¬† ),\n¬† ¬† bs_vars_dropdown(\n¬† ¬† ¬† ¬† bg = \"#0006\",\n¬† ¬† ),\n¬† ¬† bs_vars_modal(\n¬† ¬† ¬† ¬† content_bg = \"#0006\"\n¬† ¬† )\n¬† ¬† bs_vars_wells(\n¬† ¬† ¬† ¬† bg = \"#75b8d1\n¬† ¬† )\n¬† ¬† bs_vars_input(\n¬† ¬† ¬† ¬† color = \"#FFF\",\n¬† ¬† ¬† ¬† color_placeholder = \"bdbdbd\"\n¬† ¬† ),\n¬† ¬† bas_vars_button(\n¬† ¬† ¬† ¬† default_color = \"black\",\n¬† ¬† ¬† ¬† primary_bg = \"black\",\n¬† ¬† ¬† ¬† success_bg = \"#188C9C\",\n¬† ¬† ¬† ¬† info_bq = \"#A6CEE3\",\n¬† ¬† ¬† ¬† info_color = \"#2c3e50\",\n¬† ¬† ¬† ¬† warning_bg = \"#CCBE93\",\n¬† ¬† ¬† ¬† danger_bg = \"#E31A1C\"\n¬† ¬† )\n¬† ¬† bs_vars_panel(\n¬† ¬† ¬† ¬† bg = \"#0006\",\n¬† ¬† ¬† ¬† default_heading_bg = \"#0006\",\n¬† ¬† ¬† ¬† default_text = \"white\"\n¬† ¬† )\n)\nui &lt;- bootstrapPage(\n\n¬† ¬† tags$style(type = \"text/css\", \"html, body {.lightbox width: ... not seen\n¬† ¬† tags$head(\n¬† ¬† ¬† ¬† HTML(\"&lt;style&gt;\n¬† ¬† ¬† ¬† ¬† ¬† h1,h2,h3,h4,h5,h6(color:#FFF !important;} .... possibly not seen\n¬† ¬† ¬† ¬† ¬† ¬† /form-control{color:#FFF;}\n¬† ¬† ¬† ¬† ¬† ¬† .dataTables_filter{color:white;}\n¬† ¬† ¬† ¬† ¬† ¬† thead{color:#FFF;}\n¬† ¬† ¬† ¬† ¬† ¬† &lt;/style&gt;\")\n¬† ¬† ),\n\n¬† ¬† use_theme(my_theme),\n¬† ¬† leafletOutput(\"map\", width = \"100%\", height = \"100%\" .... not seen\n\n¬† ¬† absolutePanel(\n¬† ¬† ¬† ¬† id = \"logos\",\n¬† ¬† ¬† ¬† style = \"z-index:300;bottom:50px;right:50px;\",\n¬† ¬† ¬† ¬† h2(\"Pharmacy Finder\")\n¬† ¬† ),\n\n¬† ¬† absolutePanel(\n¬† ¬† ¬† ¬† id = \"business-science\",\n¬† ¬† ¬† ¬† style = \"z-index:300;bottom:50px;left:50px;\", .... not seen\n¬† ¬† ¬† ¬† h4(\"Learn Shiny\", class = \"text_primary\"),\n¬† ¬† ¬† ¬† h5(\n¬† ¬† ¬† ¬† ¬† ¬† tags$img(src = \"https://www.business.scien .... not seen\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† style = \"width:48px;-webkit-filte .... not seen\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† \"Business Science\"\n¬† ¬† ¬† ¬† ) %&gt;%\n¬† ¬† ¬† ¬† ¬† ¬† tags$a(class = \"btn btn-primary btn-sm\", .... not seen\n¬† ¬† ),\n\n¬† ¬† absolutePanel(\n¬† ¬† ¬† ¬† id = \"controls\",\n¬† ¬† ¬† ¬† style = \"zindex:5000;top:10px;left:50px;\",¬† .... not seen\n¬† ¬† ¬† ¬† draggable = FALSE,\n¬† ¬† ¬† ¬† div(\n¬† ¬† ¬† ¬† ¬† ¬† class=\"panel panel-default\",\n¬† ¬† ¬† ¬† ¬† ¬† style = \"width:300px;\",\n¬† ¬† ¬† ¬† ¬† ¬† div(\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† class=\"panel-body\",\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† textInput(\"city\", \"City\", \"Pittsburgh\" .... not seen\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† selectInput(\"amenity\", \"Amenity Type\", .... not seen\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† shiny::actionButton(\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† inputId = \"submit\",\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† label = \"Search\",a\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† class = \"btn-default\"\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ),\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† downloadButton(\"download_csv\", \"Downlo .... not seen\n¬† ¬† ¬† ¬† ¬† ¬† )\n¬† ¬† ¬† ¬† )\n¬† ¬† )\n)",
    "crumbs": [
      "Shiny",
      "UI Examples"
    ]
  },
  {
    "objectID": "qmd/model-building-tidymodels.html",
    "href": "qmd/model-building-tidymodels.html",
    "title": "tidymodels",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Model Building",
      "tidymodels"
    ]
  },
  {
    "objectID": "qmd/model-building-tidymodels.html#sec-modlbld-tidymod-misc",
    "href": "qmd/model-building-tidymodels.html#sec-modlbld-tidymod-misc",
    "title": "tidymodels",
    "section": "",
    "text": "CV dataset terminology: training, validation, test ‚Äì&gt; analysis, assessment, test\n(I don‚Äôt think this is necessary anymore. Outcome may remain categorical) Transform categorical outcome variables to factors\n# outside recipe\nmutate(rain_tomorrow = factor(ifelse(rain_tomorrow, \"Rained\", \"Didn't Rain\")))\n\n# skip = TRUE is so this is tried to be applied the test set which won't have the outcome var when its being preprocessed\nstep_mutate(rain_tomorrow = factor(ifelse(rain_tomorrow, \"Rained\", \"Didn't Rain\"),\n¬† ¬† ¬† ¬† ¬† ¬† skip = TRUE)\n\n# this would have 0 and 1 values though\nstep_num2factor(rain_tomorrow)\nWith long running models, you can work with a sample of the data in order to iterate through initial ranges of tuning grids. Once you start to narrow down the range of hyperparameter values, then you can adjust the portion of the data and/or number of folds.\ntrain_fold_small &lt;- train %&gt;%\n¬† ¬† ¬† ¬† ¬† ¬† sample_n(4000) %&gt;%\n¬† ¬† ¬† ¬† ¬† ¬† vfold_cv(v = 2)\n# change data obj in resamples arg of tune_grid\nAccessing a fitted model object to see coefficient values, hyperparameter values, etc.\nlin_trained &lt;- lin_workflow %&gt;%\n¬† ¬† ¬† ¬† finalize_workflow(select_best(lin_tune)) %&gt;%\n¬† ¬† ¬† ¬† fit(split_obj) # or train_obj or test_ob, etc.\n\nlin_trained$fit$fit %&gt;%\n¬† ¬† ¬† ¬† broom::tidy()\n\n# returns a parsnip object\nextract_fit_parsnip(lin_trained) %&gt;%\n¬† ¬† tidy()\n# returns an engine specific object\nxgboost::xgb.importance(model = extract_fit_engine(xgb_fit)\nUtilizing a sparse matrix with blueprint arg\nwf_sparse &lt;-¬†\n¬† workflow() %&gt;%\n¬† add_recipe(text_rec, blueprint = sparse_bp) %&gt;%\n¬† add_model(lasso_spec)\n\nestimates, model performance metrics, etc. are unaffected\nAdvantages\n\nSpeed is gained from any specialized model algorithms built for sparse data.\nThe amount of memory this object requires decreases dramatically.\n\nEngines that can utilize a sparse matrix: glmnet, xgboost, and ranger.\n\nIf your CV results have close scores ‚Äì&gt; increase from 5-fold to 10 fold cv\n\nCan take much longer though\n\nBack-transform predictions if you made a non-recipe transformation of the target variable\n# log 10 transformed target variable\npreds_intervals &lt;- predict(\n¬† workflows::pull_workflow_fit(lm_wf),\n¬† workflows::pull_workflow_prepped_recipe(lm_wf) %&gt;% bake(ames_holdout),\n¬† type = \"pred_int\",\n¬† level = 0.90\n) %&gt;%¬†\n¬† mutate(across(contains(\".pred\"), ~10^.x))\nGet CV coefs (?)\nget_glmnet_coefs &lt;- function(x) {\n¬† x %&gt;%\n¬† ¬† extract_fit_engine() %&gt;%\n¬† ¬† tidy(return_zeros = TRUE) %&gt;%\n¬† ¬† rename(penalty = lambda)\n}\nparsnip_ctrl &lt;- control_grid(extract = get_glmnet_coefs)\n\nglmnet_res &lt;-\n¬† glmnet_wflow %&gt;%\n¬† tune_grid(\n¬† ¬† resamples = bt,\n¬† ¬† grid = grid,\n¬† ¬† control = parsnip_ctrl\n¬† )\nShowing table of results from multiple models\nlist(\n¬† \"Original\" = unbalanced_model,¬†\n¬† \"Undersampled\" = balanced_model,¬†\n¬† \"Smote\" = smote_model\n) %&gt;%\n¬† map(tidy) %&gt;%\n¬† imap(~ select(.x, \"Term\" = term, !!.y := \"estimate\")) %&gt;%\n¬† reduce(inner_join, by = \"Term\") %&gt;%\n¬† pretty_print()\nExtract training times\n\n\nextract_fit_time doesn‚Äôt include the overhead associated with {tidymodels} ‚Äî only training time performed by the engine\nAllows you to choose a much faster model that is almost as good as the best predictive model.\nCurrently only available in development versions of tidymodels\nAlso see Tune for xgboost example\nExample\ntaxi_fit &lt;- fit(boost_tree(mode = \"classification\"), tip ~ ., taxi)\nextract_fit_time(taxi_fit)\n\n#&gt; # A tibble: 1 √ó 2\n#&gt;   stage_id   elapsed\n#&gt;   &lt;chr&gt;        &lt;dbl&gt;\n#&gt; 1 boost_tree   0.120",
    "crumbs": [
      "Model Building",
      "tidymodels"
    ]
  },
  {
    "objectID": "qmd/model-building-tidymodels.html#sec-modbld-tidymod-steps",
    "href": "qmd/model-building-tidymodels.html#sec-modbld-tidymod-steps",
    "title": "tidymodels",
    "section": "Steps",
    "text": "Steps\n\nset-up\nrecipe\nSpecify model(s)\nworkflow, workflow_set\nfit, fit_resamples, or tune_grid\nautoplot\ncollect metrics\nRepeat as necessary\nfinalize_workflow\nlast_fit",
    "crumbs": [
      "Model Building",
      "tidymodels"
    ]
  },
  {
    "objectID": "qmd/model-building-tidymodels.html#sec-modbld-tidymod-debug",
    "href": "qmd/model-building-tidymodels.html#sec-modbld-tidymod-debug",
    "title": "tidymodels",
    "section": "Checks, Debugging",
    "text": "Checks, Debugging\n\ncheck number of predictors and preprocessing results\ncheck_recipe &lt;- function(recipe_obj) {\nrecipe_obj %&gt;%¬† # already created recipe instructions\n¬† ¬† prep() %&gt;%¬† # instantiates recipe (makes necessary calcs)\n¬† ¬† bake() %&gt;% # executes recipe on data\n¬† ¬† glimpse()\n}\ncheck tuning error\ntune_obj$.notes[[2]] # 2 is the fold number\ncheck a particular fold after preprocessing\nrecipe_obj %&gt;%\ntraining(cv_obj$splits[[2]]) %&gt;% # 2 is the fold number\nprep()\nshow_best: Display the top sub-models and their performance estimates\nshow_best(tune_obj, \"rmse\")\n\nexample orders models by rmse\n\nCheck learning curves after tuning a model\nautoplot(tune_obj)",
    "crumbs": [
      "Model Building",
      "tidymodels"
    ]
  },
  {
    "objectID": "qmd/model-building-tidymodels.html#sec-modbld-tidymod-helpers",
    "href": "qmd/model-building-tidymodels.html#sec-modbld-tidymod-helpers",
    "title": "tidymodels",
    "section": "Helpers",
    "text": "Helpers\n\npredictions + newdata tbl for workflow and stacks objects\naugment.workflow &lt;- function(x, newdata, ...) {\npredict(x, newdata, ...) %&gt;%\nbind_cols(newdata)\n}\n# model_stack is the class of a stack obj\naugment.model_stack &lt;- function(x, newdata, ...) {\npredict(x, newdata, ...) %&gt;%\nbind_cols(newdata)\n}",
    "crumbs": [
      "Model Building",
      "tidymodels"
    ]
  },
  {
    "objectID": "qmd/model-building-tidymodels.html#sec-modbld-tidymod-setup",
    "href": "qmd/model-building-tidymodels.html#sec-modbld-tidymod-setup",
    "title": "tidymodels",
    "section": "Set-up",
    "text": "Set-up\n\nSome packages\n\ntidyverse - cleaning\ntidymodels - model building\nscales - loss metrics\nlubridate - ts cleaning functions\ntextrecipes - tokenizers, etc.\nthemis - up/down-sampling for imbalanced/unbalanced outcomes\nstacks - ensembling\n\nSplit\nset.seed(2021)\nspl &lt;- initial_split(dataset, prop = .75)\ntrain &lt;- training(spl)\ntest &lt;- testing(spl)\ntrain_folds &lt;- train %&gt;%\n¬† ¬† ¬† ¬† vfold_cv(v = 5)\n\nMight be useful to create a row id column before splitting to keep track of observations after the split\n\ndf %&gt;% rowid_to_column(var = \"row_id\")¬† then use step_rm(row_id) to remove it in the recipe\n\n\nMetrics\nmset &lt;- metric_set(mn_log_loss)\nParallel\nlibrary(future)\nplan(multisession, workers = 10)\n\nAs of tune 1.2.0, the {future} framework is recommended and other frameworks will begin to be deprecated.\n\nGrid control options\ngrid_control &lt;- \n  control_grid(\n¬†   # save the out-of-sample predictions\n¬† ¬† save_pred = TRUE,\n¬† ¬† save_workflow = TRUE,\n¬† ¬† extract = extract_model\n¬† )\nggplot theme\ntheme_set(theme_light)\n\nLiked this theme for calibration curves",
    "crumbs": [
      "Model Building",
      "tidymodels"
    ]
  },
  {
    "objectID": "qmd/model-building-tidymodels.html#sec-modbld-tidymod-wkflw",
    "href": "qmd/model-building-tidymodels.html#sec-modbld-tidymod-wkflw",
    "title": "tidymodels",
    "section": "Workflow",
    "text": "Workflow\n\nformat: workflow function, add_recipe, add_model\nimb_wf &lt;-\n¬† workflow() %&gt;%\n¬† add_recipe(bird_rec) %&gt;%\n¬† add_model(bag_spec)\nCan also add fit(training(splits)) to the pipe to go ahead and fit the model if you‚Äôre not tuning, etc.\nFit multiple models and multiple recipes\npreproc &lt;- list(none = basic_recipe, pca = pca_recipe, sp_sign = ss_recipe)\nmodels &lt;- list(knn = knn_mod, logistic = lr_mod)\nwf_set &lt;- workflow_set(preproc, models, cross = TRUE)\n\ncross = TRUE says fit each model with each recipe",
    "crumbs": [
      "Model Building",
      "tidymodels"
    ]
  },
  {
    "objectID": "qmd/model-building-tidymodels.html#sec-modbld-tidymod-fit",
    "href": "qmd/model-building-tidymodels.html#sec-modbld-tidymod-fit",
    "title": "tidymodels",
    "section": "Fit",
    "text": "Fit\n\nfit:¬† fit a model on a dataset\nimb_fit &lt;- fit(workflow_obj, data = dat)\nfit_resample: cross-validation\nimb_rs &lt;-\n¬† fit_resamples(\n¬† ¬† workflow_obj,\n¬† ¬† resamples = cv_folds_obj,\n¬† ¬† metrics = metrics_obj\n¬† )\ncollect_metrics(imb_rs)\n\nUsing int_pctl(imb_rs, alpha = .1) will calculate percentile 90% CIs for the metrics.",
    "crumbs": [
      "Model Building",
      "tidymodels"
    ]
  },
  {
    "objectID": "qmd/model-building-tidymodels.html#sec-modbld-tidymod-tune",
    "href": "qmd/model-building-tidymodels.html#sec-modbld-tidymod-tune",
    "title": "tidymodels",
    "section": "Tune",
    "text": "Tune\n\nMisc\n\nEvery column in a tuning result that‚Äôs prefixed with a . has a collect_*() function associated with it that helps to summarize that column\nIf after training the model, you‚Äôd like to add additional metrics, you can used compute_metrics. Need save_pred = TRUE in the control_grid object. - e.g.¬†compute_metrics(xgb_res, metric_set(huber_loss, mape)) adds huber loss and mape to the tune_grid object\n\nCV and Tune Model with Multiple Recipes (article)\nlibrary(future)\nplan(multisession, workers = 10)\nset.seed(345)\n\ntune_res &lt;-\n¬† ¬† workflow_map(\n¬† ¬† ¬† ¬† wf_set,\n¬† ¬† ¬† ¬† \"tune_grid\",\n¬† ¬† ¬† ¬† resamples = cv_folds,\n¬† ¬† ¬† ¬† grid = 15,\n¬† ¬† ¬† ¬† metrics = metric_set(accuracy, roc_auc, mn_log_loss, sensitivity, specificity),\n¬† ¬† ¬† ¬† param_info = parameters(narrower_penalty)\n¬† ¬† )\n\n‚Äúwf_set is a workflow_set object\n‚Äúgrid = 15‚Äù: this workflow set is only tuning one model, glmnet, and the parameter being tuned is the penalty. So ‚Äúgrid = 15‚Äù says try 15 different values.\n‚Äúnarrow_penalty‚Äù is narrower_penalty &lt;- penalty(range = c(-3, 0)\nViz: autoplot(tune_res)\n\n\nInterpretation:\n\nWhen accuracy is high, specificity (accuracy at picking negative class) is very high but sensitivity (accuracy at picking positive class) is very low.\nAdd middling values of specificity and sensitivity, mean log loss is also middling\nmean log loss is high when sensitivity is high\nAUROC is high at middling values of specificity and sensitivity indicating a best model for discriminating both classes. (but which recipe is this; see next section)\n\nWorkflow Rank (X-Axis): In this example there were 15 values of penalty considered and 2 differerent recipes, so I‚Äôm guessing that‚Äôs where the 30 comes from.\n\nExtract a particular model/recipe combo and viz\n\ndownsample_rs &lt;-\n¬† tune_rs %&gt;%\n¬† extract_workflow_set_result(\"downsampling_glmnet\")\nautoplot(downsample_rs)\n\nY-Axis: metric value, X-Axis: parameter value (e.g.¬†glmnet penalty value)\nWhere ‚Äúdownsampling_glmnet‚Äù is the model/recipe combo you want to extract\nShows how downsampling affects the glmnet model\nInterpretation:\n\nLooks like the 1st half of workflow set in above viz is the downsampling recipe\nShows downsampling without recalibration of the predicted probabilities results in a model that can only pick 1 class well based on the penalty\nThe GOF metrics: high accuracy, low mean log loss, high auroc prefer the model that picks the negative class really well.\n\n\nShow results ordered by a metric\nrank_results(tune_rs, rank_metric = \"mn_log_loss\")\n\nresults ordered from best to worst according to the metric\n\n\ntune_grid\n\nFormat: workflow, data, grid, metrics, control\n\nlin_tune &lt;- lin_wf %&gt;%\n¬† tune_grid(train_fold,\n¬† ¬† ¬† ¬† ¬† ¬† grid = crossing(penalty = 10 ^ seq(-6, -.5, .05)),\n¬† ¬† ¬† ¬† ¬† ¬† metrics = mset,¬† # see set-up section\n¬† ¬† ¬† ¬† ¬† ¬† control = grid_control) # see set-up section\nTuning multiples of the same hyperparameter\n\nadd id to specification (e.g.¬†recipe step)\nuse id name instead of parameter name in grid\n\nstep_ns(var1, deg_free = tune(\"df_var1\"))\nstep_ns(var2, def_free = tune(\"df_var2\"))\n\ntune_obj &lt;- workflow_obj %&gt;%\n¬† ¬† tune_grid(train_fold,\n¬† ¬† ¬† ¬† ¬† ¬† ¬† grid = crossing(df_var1 = 1:4, df_var2 = 4:6),\n¬† ¬† ¬† ¬† ¬† ¬† ¬† metrics = mset,\n¬† ¬† ¬† ¬† ¬† ¬† ¬† control = grid_control)\nGrids\n\n{dials}\npurrr::crossing\ngrid = crossing(mtry = 2:7,\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† trees - seq(100, 1000, 100),\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† learn_rate = 0.1)\nLatin Hypercube\nxgb_grid &lt;- grid_latin_hypercube(\n¬† ¬† tree_depth(),\n¬† ¬† min_n = sample(2, 10, 3)\n¬† ¬† loss_reduction = sample(runif(min = 0.01, max = 0.1), 3),\n¬† ¬† sample_size = sample(0.5:0.8, 3),\n¬† ¬† mtry = sample(3:6, 3),\n¬† ¬† learn_rate = sample(runif(min = 0.03, max = 0.1), 3)\n)\n\nsize: total number of parameter value combinations (default: 3)\n\nRandom\nxgb_grid &lt;- grid_random(\n¬† ¬† tree_depth(),\n¬† ¬† min_n(),\n¬† ¬† loss_reduction(),\n¬† ¬† learn_rate(),\n¬† ¬† finalize(sample_prop(), train_data),\n¬† ¬† finalize(mtry(), train_baked),\n¬† ¬† size = 20\n)\n\nsize: default: 5\n\n\nGet coefficients for folds\nget_glmnet_coefs &lt;- function(x) {\n¬† x %&gt;%¬†\n¬† ¬† extract_fit_engine() %&gt;%¬†\n¬† ¬† tidy(return_zeros = TRUE) %&gt;%¬†\n¬† ¬† rename(penalty = lambda)\n}¬†\nglmnet_res &lt;-¬†\n¬† glmnet_wflow %&gt;%¬†\n¬† tune_grid(\n¬† ¬† resamples = bt,\n¬† ¬† grid = grid,\n¬† ¬† control = control_grid(extract = get_glmnet_coefs)\n¬† )\nglmnet_coefs &lt;-¬†\n¬† glmnet_res %&gt;%¬†\n¬† select(id, .extracts) %&gt;%¬†\n¬† unnest(.extracts) %&gt;%¬†\n¬† select(id, mixture, .extracts) %&gt;%¬†\n¬† group_by(id, mixture) %&gt;%¬† ¬† ¬† ¬† ¬† # ‚îê\n¬† slice(1) %&gt;%¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† # ‚îÇ Remove the redundant results\n¬† ungroup() %&gt;%¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† # ‚îò\n¬† unnest(.extracts)\nglmnet_coefs %&gt;%¬†\n¬† select(id, penalty, mixture, term, estimate) %&gt;%¬†\n¬† filter(term != \"(Intercept)\")\n#&gt; # A tibble: 300 √ó 5\n#&gt;¬† ¬† id¬† ¬† ¬† ¬† penalty mixture term¬† ¬† ¬† estimate\n#&gt;¬† ¬† &lt;chr&gt;¬† ¬† ¬† ¬† &lt;dbl&gt;¬† &lt;dbl&gt; &lt;chr&gt;¬† ¬† ¬† ¬† &lt;dbl&gt;\n#&gt;¬† 1 Bootstrap1 1¬† ¬† ¬† ¬† ¬† 0.1 Clark_Lake¬† ¬† 0.391\n#&gt;¬† 2 Bootstrap1 0.464¬† ¬† ¬† 0.1 Clark_Lake¬† ¬† 0.485\n#&gt;¬† 3 Bootstrap1 0.215¬† ¬† ¬† 0.1 Clark_Lake¬† ¬† 0.590\n#&gt;¬† 4 Bootstrap1 0.1¬† ¬† ¬† ¬† 0.1 Clark_Lake¬† ¬† 0.680\n#&gt;¬† 5 Bootstrap1 0.0464¬† ¬† ¬† 0.1 Clark_Lake¬† ¬† 0.746\n#&gt;¬† 6 Bootstrap1 0.0215¬† ¬† ¬† 0.1 Clark_Lake¬† ¬† 0.793\n#&gt;¬† 7 Bootstrap1 0.01¬† ¬† ¬† ¬† 0.1 Clark_Lake¬† ¬† 0.817\n#&gt;¬† 8 Bootstrap1 0.00464¬† ¬† 0.1 Clark_Lake¬† ¬† 0.828\n#&gt;¬† 9 Bootstrap1 0.00215¬† ¬† 0.1 Clark_Lake¬† ¬† 0.834\n#&gt; 10 Bootstrap1 0.001¬† ¬† ¬† 0.1 Clark_Lake¬† ¬† 0.837\n#&gt; # ‚Ä¶ with 290 more rows\n\nextract_fit_engineadds an additional column ‚Äú.extracts‚Äù to tuning output\n‚Äúreturn_zeros‚Äù keeps records of variables with penalized coefficients to zero during tuning\nViz\n\nglmnet_coefs %&gt;%¬†\n¬† filter(term != \"(Intercept)\") %&gt;%¬†\n¬† mutate(mixture = format(mixture)) %&gt;%¬†\n¬† ggplot(aes(x = penalty, y = estimate, col = mixture, groups = id)) +¬†\n¬† geom_hline(yintercept = 0, lty = 3) +\n¬† geom_line(alpha = 0.5, lwd = 1.2) +¬†\n¬† facet_wrap(~ term) +¬†\n¬† scale_x_log10() +\n¬† scale_color_brewer(palette = \"Accent\") +\n¬† labs(y = \"coefficient\") +\n¬† theme(legend.position = \"top\")\n\nInterpretation\n\nWith a pure lasso model (i.e., mixture = 1), the Austin station predictor is selected out in each resample. With a mixture of both penalties, its influence increases.\n\nAlso, as the penalty increases, the uncertainty in this coefficient decreases.\n\nThe Harlem predictor is either quickly selected out of the model or goes from negative to positive.\n\n\n\nSelect a faster model that‚Äôs almost as predictive as the best model (article)\n\nCurrently only available in development versions of tidymodels\nAlso see Misc for extracting training times from a single model\nSee article for visualizations\nFor large datasets, the trees hyperparameter can substantially effect the training time for a boosted tree model or bagged tree model.\nFit model\nbt_res &lt;- \n  tune_grid(\n    boosted_tree_mod, \n    tip ~ ., \n    taxi_folds, \n    control = control_grid(extract = extract_fit_time)\n  )\nExtract predictive metrics and training times\n# mean GOF metrics for tuning combos\nbt_metrics &lt;- collect_metrics(bt_res)\n# fit times across folds and tuning combos\nbt_extracts_unnested &lt;- bt_extracts %&gt;% unnest(cols = \".extracts\")\n\nUsing int_pctl(bt_rs, alpha = .1) will calculate percentile 90% CIs for the metrics.\n\nCalculate mean training times and join with GOF metrics\nbt_metrics_extracts &lt;-\n  left_join(\n    # summarized metrics across resamples\n    bt_metrics,\n    # summarize fit times across resamples\n    summarize(\n      bt_extracts_unnested, \n      elapsed = mean(elapsed),\n      .by = c(trees, learn_rate, .config)\n    ),\n    by = c(\"trees\", \"learn_rate\", \".config\")\n  ) %&gt;%\n  relocate(elapsed, .before = everything())\nSelect fastest model that‚Äôs within 1 standard deviation of most predictive model\nbest_fit &lt;-\n  bt_metrics_extracts %&gt;%\n  filter(.metric == \"roc_auc\") %&gt;%\n  arrange(desc(mean)) %&gt;%\n  select(mean, std_err, elapsed, .config)\n\nbest_speedy_fit &lt;- \n  bt_metrics_extracts %&gt;%\n  filter(.metric == \"roc_auc\") %&gt;%\n  filter(mean &gt;= best_fit$mean - best_fit$std_err) %&gt;%\n  arrange(elapsed) %&gt;%\n  slice(1)\n\nbest_speedy_fit",
    "crumbs": [
      "Model Building",
      "tidymodels"
    ]
  },
  {
    "objectID": "qmd/model-building-tidymodels.html#sec-modbld-tidymod-final",
    "href": "qmd/model-building-tidymodels.html#sec-modbld-tidymod-final",
    "title": "tidymodels",
    "section": "Finalize",
    "text": "Finalize\n\nMisc\n\nMake sure to set control option, ctrl &lt;- control_grid(save_workflow = TRUE)\ncollect_notes(tuning_results) - If you need more information about errors or warnings that occur\n\nTo turn off the interactive logging, set the verbose control option to TRUE\n\n\nfit_best(x, metric = NULL, parameters = NULL, verbose = FALSE, ...) is a shortcut for:\nbest_param &lt;- select_best(tune_results, metric) # or other `select_*()`\nwflow &lt;- finalize_workflow(wflow, best_param)¬† # or just `finalize_model()`\nwflow_fit &lt;- fit(wflow, data_set)\n\nx: results of class, ‚Äútune_results‚Äù, from functions like tune_*\n\nSelect a simpler model or a more complex model\ndownsample_rs &lt;-¬†\n¬† tune_rs %&gt;%¬†\n¬† extract_workflow_set_result(\"downsampling_glmnet\")\n\nbest_penalty &lt;-¬†\n¬† downsample_rs %&gt;%\n¬† select_by_one_std_err(-penalty, metric = \"mn_log_loss\")\n\nfinal_fit &lt;-¬†\n¬† wf_set %&gt;%¬†\n¬† extract_workflow(\"downsampling_glmnet\") %&gt;%\n¬† finalize_workflow(best_penalty) %&gt;%\n¬† last_fit(feeder_split)\n\nSelecting a simpler model can reduce chances of overfitting\nSelects a model that has a mean log loss score that‚Äôs 1 standard deviation from the top mean log loss value\n\nselect_by_pct_loss() also available\n\nThe first argument is passed to dplyr::arrange\n\nSo with ‚Äú-penalty,‚Äù penalty is a column in the results tibble that you want to sort by and the ‚Äú-‚Äù says you want start at the higher penalty parameter values\n\nA higher penalty (lasso model) means a simpler model\n‚Äútune_rs‚Äù is a workflow_map object\nThe last chunk finalizes the original tuneable workflow with this value for the penalty, and then last_fit ¬† fits the model one time to the (entire) training data and evaluates one time on the testing data.\nMetrics\ncollect_metrics(final_fit)\ncollect_predictions(final_fit) %&gt;%\n¬† conf_mat(squirrels, .pred_class)\nVariable Importance\nlibrary(vip)\nfeeder_vip &lt;-\n¬† extract_fit_engine(final_fit) %&gt;%\n¬† vi()\nfeeder_vip\n\nfeeder_vip %&gt;%\n¬† group_by(Sign) %&gt;%\n¬† slice_max(Importance, n = 15) %&gt;%\n¬† ungroup() %&gt;%\n¬† ggplot(aes(Importance, fct_reorder(Variable, Importance), fill = Sign)) +¬†\n¬† geom_col() +\n¬† facet_wrap(vars(Sign), scales = \"free_y\") +\n¬† labs(y = NULL) +\n¬† theme(legend.position = \"none\")\n\nfinalize_model, recipe, workflow\n\nafter finding the best parameter values, this updates the object\n\nlin_wf_best &lt;- lin_wf %&gt;%\n¬† finalize_workflow(select_best(lin_tune))\n\n# or manually list parameter/vale pairs\nxgb_wf_best &lt;- lin_wf %&gt;%¬†\n¬† finalize_workflow(list(mtry = 6, trees = 1200, learn_rate = 0.01)\nlast_fit: fit finalized model on entire learning dataset and collect metrics\nxgb_wf_best %&gt;%\n¬† ¬† last_fit(initial_split_obj, metrics = metric_set_obj) %&gt;%\n¬† ¬† collect_metrics()\n\nManually\n¬† ¬† ¬† ¬† xgb_wf_best %&gt;%\n¬† ¬† ¬† ¬† ¬† ¬† fit(train) %&gt;%\n¬† ¬† ¬† ¬† ¬† ¬† augment(test, type_predict = \"prob\") %&gt;%\n¬† ¬† ¬† ¬† ¬† ¬† mn_log_loss(churned, .pred_yes)\n\nPredict on new data\nxgb_wf_best %&gt;%\n¬† ¬† fit(new_data) %&gt;%\n¬† ¬† predict(newdata, type = \"prob\") %&gt;%\n¬† ¬† bind_cols(newdata)\nPredict on test set and score\nwflw_fit %&gt;%\n¬† ¬† predict(testing(splits) %&gt;% select(outcome_var)) %&gt;%\n¬† ¬† metric_set(rmse, mae, rsq)(outcome_var, .pred)",
    "crumbs": [
      "Model Building",
      "tidymodels"
    ]
  },
  {
    "objectID": "qmd/model-building-tidymodels.html#sec-modbld-tidymod-modspec",
    "href": "qmd/model-building-tidymodels.html#sec-modbld-tidymod-modspec",
    "title": "tidymodels",
    "section": "Model Specification",
    "text": "Model Specification\n\nFormat: model function, set_engine, set_mode (‚Äúclassification‚Äù or ‚Äúregression‚Äù)\nbag_spec &lt;-\n¬† bag_tree(min_n = 10) %&gt;%\n¬† set_engine(\"rpart\", times = 25) %&gt;%\n¬† set_mode(\"classification\")\nLogistic/Linear\n# lasso\nlogistic_reg(penalty = tune()) %&gt;%\n¬† ¬† set_engine(\"glmnet\")\n\nfunction: logistic_reg, linear_reg\n\nargs: penalty (Œª), mixture (Œ±) (number or tune())\n\npenalty grid examples: 10^seq(-7, -1, .1), 10 ^ seq(-6, -.5, .05)\npenalty default: penalty(range = c(-10, 0), trans = log10_trans())\n\nSilge uses range = c(-3, 0)\n\n\n\nengines\n\ntypes: glm, glmnet, LiblineaR (only logistic), stan, spark, keras\nLiblinear\n\nbuilt for large data\n\nfaster than using glmnet even w/sparse matrix\n\nrequires mixture = 0 or 1; default: 0 (ridge)\nalso regularizes intercept which affects coef values\n\nglmnet\n\nmixture default = 1 (lasso)\n\nspark\n\nmixture default = 0 (ridge)\n\nargs\n\npath_values: sequence of¬† values for penalty\n\nassigns a collection of penalty values used by each glmnet fit (regardless of the data or value of mixture)\n\nWithout this arg, glmnet uses multiple penalty values which depend on the data set; changing the data (or the mixture amount) often produces a different set of values\n\nSo, if you want to look at results for a specific penalty value, you need to use this arg to make sure its included.\n\n\nmust still give penalty arg a number\nif using ridge, include 0 in sequence of values for penalty\n\npath_values = c(0, 10^seq(-10, 1, length.out = 20))\n# example in docs\n10^seq(-3, 0, length.out = 10)\n# drob's range\n10 ^ seq(-6, -.5, .05)\n\n\n\nSupport Vector Machines (SVM)\nsvm_linear() %&gt;%¬†\n¬† set_engine(\"LiblineaR\") %&gt;%¬†\n¬† set_mode(\"regression\")\n\nmodes: regression or classification\nengines\n\nLiblineaR\n\nbuilt for bigdata (C++ library)\nNo probability predictions for classification models\n\nIn order to use tune_grid, metrics must be hard classification metrics (e.g.¬†accuracy, specificity, sensitivity, etc.)\nWon‚Äôt be able to adjust thresholds either\n\nL2-regularized\n\nkernlab\n\nkernel = ‚Äúvanilladot‚Äù\n\nargs\n\ncost: default = 1\nmargin: default = 0.1\n\n\n\nBoosted Trees\nxgb_mod &lt;- boost_tree(\"regression\",\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† mtry = tune(),\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† trees = tune(),\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† learn_rate = 0.01,\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† stop_iter = tune()) %&gt;%\n¬† ¬† ¬† ¬† set_engine(\"xgboost\")\n\nXGBoost\n\ndrob says\n\nlogging predictors has no effect on this algorithm\n\nargs\n\nmtry\ntrees\nlearn_rate\ntree_depth\nstop_iter: early stopping; stops if no improvement has been made after this many iterations\n\n\n\nRandom Forest\nrand_forest(\n¬† mode = \"regression\",\n¬† engine = \"ranger\",\n¬† mtry = tune(),\n¬† trees = tune(),\n¬† min_n = tune()\n) %&gt;%\n¬† set_engine(importance = \"permutation\",\n¬† ¬† ¬† ¬† ¬† ¬† seed = 63233,\n¬† ¬† ¬† ¬† ¬† ¬† quantreg = TRUE)\n\nranger doc\nargs\n\nmode: ‚Äúclassification‚Äù, ‚Äúregression‚Äù, ‚Äúunknown‚Äù\nengine: ranger (default), randomForest, spark\ndefault hyperparameters: mtry, trees, min_n\nimportance: ‚Äònone‚Äô, ‚Äòimpurity‚Äô, ‚Äòimpurity_corrected‚Äô, ‚Äòpermutation‚Äô\nnum.threads = 1 (default)\nquantreg = TRUE for quantile regression\npreds &lt;- regr_fit$fit %&gt;% predict(airquality, type = 'quantiles')\nhead(preds$predictions)\n\nCurrently have to extract the ranger model in order to obtain quantile predictions\nissue still open",
    "crumbs": [
      "Model Building",
      "tidymodels"
    ]
  },
  {
    "objectID": "qmd/model-building-tidymodels.html#sec-modbld-tidymod-recipe",
    "href": "qmd/model-building-tidymodels.html#sec-modbld-tidymod-recipe",
    "title": "tidymodels",
    "section": "Recipes",
    "text": "Recipes\n\nMisc\n\nDocs, FunctionReference\nStep args are tunable. Just use tune() for value in the arg and include it in tune_grid\n\ne.g spline degrees in step_ns or bs, number of bootstraps in step_impute_bag, or number of neighbors in step_impute_knn\n\nrecipe(formula, data)\nrecipe(rain_tomorrow ~ date + location + rain_today +\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† min_temp + max_temp + rainfall +\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† wind_gust_speed + wind_speed9am +\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† wind_speed3pm + humidity9am + humidity3pm + pressure9am +\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† pressure3pm + cloud9am + cloud3pm + temp9am + temp3pm +\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† wind_gust_dir + wind_dir9am + wind_dir3pm +\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† rain_today, data = train)\nVariable selectors\n\nbased on role\n\nhas_role, all_outcomes, all_predictors, all_numeric_predictors, all_nominal_predictors\n\nbased on type\n\nhas_type, all_numeric, all_nominal\n\n\n\n\n\nOther\n\nzv, nzv\n\nRemoves predictors with zero variance or near-zero variance\n\nstep_nsv(all_predictors(), freq_cut = 90/10)\n\nfreq_cut: ratio that determines when indicator variables with too few 1s or too few 0s are removed.\n\ndefault: 95/5\n\nunique_cut: percentage (not a decimal), number of unique values divided by the total number of samples\n\ndefault: 10\n\n\nstep_indicate_na\nexample_data &lt;- tibble(\n  x1 = c(1, 5, 8, NA, NA, 3),\n  x2 = c(1, NA, 3, 6, 2, 2),\n  x3 = c(NA, NA, NA, NA, NA, NA),\n  x4 = c(7, 8, 4, 2, 1, 1)\n)\n\nrecipe(~ ., data = example_data) |&gt;\n  step_indicate_na(all_predictors()) |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\n#&gt; # A tibble: 6 √ó 8\n#&gt;      x1    x2 x3       x4 na_ind_x1 na_ind_x2 na_ind_x3 na_ind_x4\n#&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;dbl&gt;     &lt;int&gt;     &lt;int&gt;     &lt;int&gt;     &lt;int&gt;\n#&gt; 1     1     1 NA        7         0         0         1         0\n#&gt; 2     5    NA NA        8         0         1         1         0\n#&gt; 3     8     3 NA        4         0         0         1         0\n#&gt; 4    NA     6 NA        2         1         0         1         0\n#&gt; 5    NA     2 NA        1         1         0         1         0\n#&gt; 6     3     2 NA        1         0         0         1         0\n\nCreates dummys for the NAs in a column(s)\nIf you‚Äôre doing this, I think you‚Äôre assuming the missingness is MNAR (See Missingness)\n\nstep_clean_names - From {textrecipes}, a step_* function for the {janitor} function from tidying column names\nstep_clean_levels - From {textrecipes}, some levels will produce bad column names if used for other things such as dummy variables, so this function cleans the bad category names.\n\n\n\nTransformations\n\nstep_log, BoxCox, YeoJohnson, sqrt\nstep_log(rainfall, offset = 1, base = 2)\n\nIf variable has zeros, offset adds 1 to the value.\nbase is the log base\n\nsplines\n\nstep_bs (B-Splines); step_ns (Natural Splines), step_poly (Orthogonal polynomial)\n\ndocs\n\ndeg_free: where larger values ‚Äì&gt; more complex curves\ndegree: bs-only, degree of polynomial spline (e.g.¬†2 quadratic, 3 cubic)\nOriginal variable replaced with ‚Äúvarname_ns_1‚Äù\n{splines2} versions\n\nA supplementary package to {splines}. The practical benefits\nI-splines as integrals of M-splines are used for monotone regressions; C-splines as integrals of I-splines are used for shape-restricted regressions\ntime-to-event analysis, the derivatives or integrals of the basis functions are needed in inferences\nallows basis functions of degree zero which is useful in some applications\nnatural cubic splines more computationally efficient\n\nstep_spline_nonnegative\ndata(ames, package = \"modeldata\")\n\nrecipe(Sale_Price ~ Lot_Frontage + Lot_Area, data = ames) |&gt;\n  step_spline_nonnegative(starts_with(\"Lot_\"), deg_free = 3) |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\n#&gt; # A tibble: 2,930 √ó 7\n#&gt;    Sale_Price Lot_Frontage_1 Lot_Frontage_2 Lot_Frontage_3 Lot_Area_1 Lot_Area_2\n#&gt;         &lt;int&gt;          &lt;dbl&gt;          &lt;dbl&gt;          &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n#&gt;  1     215000        0.00522       0.00428       0.00117      5.87e-6    9.76e-7\n#&gt;  2     105000        0.00543       0.00186       0.000213     2.45e-6    1.24e-7\n#&gt;  3     172000        0.00545       0.00190       0.000221     3.00e-6    1.94e-7\n#&gt;  4     244000        0.00563       0.00238       0.000335     2.35e-6    1.14e-7\n#&gt;  5     189900        0.00528       0.00164       0.000169     2.91e-6    1.81e-7\n#&gt;  6     195500        0.00539       0.00179       0.000198     2.09e-6    8.85e-8\n#&gt;  7     213500        0.00379       0.000572      0.0000287    9.17e-7    1.58e-8\n#&gt;  8     191500        0.00392       0.000624      0.0000331    9.38e-7    1.65e-8\n#&gt;  9     236500        0.00366       0.000521      0.0000247    1.03e-6    2.01e-8\n#&gt; 10     189000        0.00480       0.00114       0.0000900    1.53e-6    4.57e-8\n#&gt; # ‚Ñπ 2,920 more rows\n#&gt; # ‚Ñπ 1 more variable: Lot_Area_3 &lt;dbl&gt;\n\n\n\n\nCategorical\n\nAlso see\n\nDOCS: Handling Categorical Predictors\nFeature Engineering, General &gt;&gt; Categoricals\n\nstep_dummy: if there are C levels of the factor, there will be C - 1 dummy variables created and all but the first factor level are made into new columns\nstep_other\n\ncollapses categories that have a frequency below a specified threshold\nthreshold: proportion of total rows that a category will be collapsed into other\n\ndefault = 0.05\ngrid value examples: 0.001, 0.003, 0.01, 0.03\n\n\nstep_unknown\n\nIf NAs aren‚Äôt random, they can be pooled into a category called ‚ÄúUnknown‚Äù\n\n\n\n\nImputation\n\nAlso see Missingness\ntldr;\n\nTry bag first, knn second\n\nSeems to me that bag would be more flexible\n\nrolling would be useful for time series\n\ncommon args\n\nimpute_with: arg for including other predictors (besides outcome and variable to be imputed) in the imputation model.\nstep_impute_&lt;method&gt;(var, impute_with = imp_vars(pred1, pred2))\n‚Äúskip‚Äù arg: When set to false, this step will not be used when the recipe is baked (unless newdata=NULL)\n\nuseful when down/up-sampling an outcome variable. New data should be asis in the wild. So usecases where the step is good for training but not production.\ndefault = T for step_filter, slice, sample, naomit\n\n\nbag\n\nbagged tree model\ncategoricals and numerics can be imputed\nPredictors can have NAs but can‚Äôt be included in the list of variables to be imputed\n\nGuess you could do separate imputation steps if you want to include them as predictors\n\nspeed up by lowering bootstrap replications\n\n# keepX = FALSE says don't return a df of predictions - not sure why that's included (memory optimization?)\noptions = list(nbagg = 5, keepX = FALSE))\nknn\n\nK-Nearest Neighbors\ncategoricals and numerics can be imputed\n\ngower distance used (only distance available)\n\npossible that missing values will still occur after imputation if a large majority (or all) of the imputing variables (predictors?) are also missing\nCan be computationally intensive and is NOT robust to outliers\nneighbors = 5 by default\noptions: nthreads, eps (threshold for values to be set to 0, default = 1e-8)\n\nlinear, mean, median\n\nregression model, mean, median\nonly numerics can be imputed\nif missingness is missing not at random (MNAR) then mean shouldn‚Äôt be used (probably not median either)\n\nmode\n\nmost common value\ndiscrete variable (maybe only nominal variables)\nif more than one mode, one of them will be chosen at random\n\nroll\n\nrolling window\nwindow: window of data that you want to use to calculate the statistic\n\nFor time series, maybe look at a acf, pacf plot to get an idea for a good window size\n\nstatistic: stat to calculate over the window (e.g.¬†mean, median, etc)\n\nprobably takes a custom function with a single arg (data)\n\nvalues in the window that would be imputed in previous steps aren‚Äôt used to impute the current value.\n\n\n\n\nUp/Down-Sampling\n\n{themis} for unbalanced outcome variables\nCommon args\n\n‚Äúskip‚Äù: When set to false, this step will not be used when the recipe is baked (unless newdata=NULL)\n\nuseful when down/up-sampling an outcome variable. New data should be asis in the wild. So usecases where the step is good for training but not production.\ndefault = T for step_filter, slice, sample, naomit, and themis data balancing steps\n\n‚Äúover_ratio‚Äù: desired ratio of num_rows of¬† the minority level(s) to num_rows of the majority level. Default = 1 (i.e.¬†all levels have equal number of rows)\n\nExample: Bring the minority levels up to about 200 rows each where the majority level has 16562 rows in the data. Therefore, 200/16562 is approx 0.0121. Set over_ratio to 0.0121 and each minority level will have 200 rows in the upsampled dataset.\n\n‚Äúneighbors‚Äù:¬† number of clusters, default = 5\n\nUpsampling\n\nAll methods handle multi-level factor variables except rose.\nMisc\n\nApplying SMOTE without re-calibration is bad (paper)\n\ntldr;\n\nIf time isn‚Äôt an issue AND you have a binary variable, rose sounds like a good approach, otherwise use bsmote (probably should tune the all_neighbor option)\n\nupsample\n\nrepeats rows of minority level until dataset has desired ratio\n\nsmote\n\nSMOTE alg performs knn, generates points between the that level‚Äôs points and the centroid for each cluster\nPreprocessing: Every predictor in the formula must be numeric with no missing data.\n\nimpute missing data and encode all categoricals before using this step\n\nGenerated points can form ‚Äúbridges‚Äù between that level‚Äôs outlier points and the main distribution. This is like adding a feature that doesn‚Äôt exist in the sample.\n\nbsmote\n\nborderline-SMOTE alg is the same as SMOTE except that it generates points in the border region between classes. Ignores outliers in the resampling process.\nPreprocessing: Every predictor in the formula must be numeric with no missing data.\n\nimpute missing data and encode all categoricals before using this step\n\nall_neighbors; default = FALSE\n\nFALSE then points will be generated between nearest neighbors in its own class.\n\nThis can produce mini-clusters of points\n\nTRUE then points will be generated between neighbors of the upsampled class and the other classes.\n\nMay blur the line between clusters (and levels? Therefore potentially making classification tasks more difficult)\n\n\nOnly selecting from the borderline region can be an issue if there aren‚Äôt that many borderline points in the original dataset.\n\nadasyn\n\nsimilar to SMOTE, but generates the most points from points that have the smallest representation in the data, i.e.¬†generates a lot of outlier or extreme value points.\n\nMight produce much more stable results with tree models as compared to regression models\n\nPreprocessing: Every predictor in the formula must be numeric with no missing data.\n\nimpute missing data and encode all categoricals before using this step\n\n\nrose\n\nUses a ‚Äúsmoothed‚Äù bootstrap approach to generate new samples. (paper was incoherent as fuck)\nOnly for binary (factor) variables\nPreprocessing: Every predictor in the formula must be numeric with no missing data.\n\nimpute missing data and encode all categoricals before using this step\n\nno mention of missing data in the docs but probably still a good idea.\n\n\nminority_prop: same function as over_ratio. Default = 0.5\nminority_smoothness and majority_smoothness\n\ndefault = 1. Reducing this value will shrink the (resampling?) region in the feature space associated with that class (minority or majority).\nIf these regions are too large, the boundary between class regions blurs. If too small, maybe you aren‚Äôt getting the variance out of the resampling that you could be getting.\n\n\n\nDownsampling\n\nMisc\n\nManually\nminority_class &lt;- data %&gt;%\n¬† count(class) %&gt;%\n¬† filter(n == min(n))\n\nbalanced &lt;- data %&gt;%\n¬† group_split(class, .keep = TRUE) %&gt;%\n¬† map_dfr(\n¬† ¬† ~ {\n¬† ¬† ¬† if (.x$class[1] == minority_class$class) {\n¬† ¬† ¬† ¬† .x\n¬† ¬† ¬† } else {\n¬† ¬† ¬† ¬† slice_sample(\n¬† ¬† ¬† ¬† ¬† .x,\n¬† ¬† ¬† ¬† ¬† n = minority_class$n,\n¬† ¬† ¬† ¬† ¬† replace = FALSE\n¬† ¬† ¬† ¬† )\n¬† ¬† ¬† }\n¬† ¬† }\n¬† )\n\nbalanced %&gt;%\n¬† count(class) %&gt;%\n¬† pretty_print()\n\ntldr;\n\nnone of the methods below sound too bad.\n\ndownsample\n\nremoves majority level(s) rows randomly to match the under_ratio (see over_ratio defintion above)\n\nnearmiss\nremoves majority level(s) rows by undersampling points in the majority level(s) based on their distance to other points in the same level. - Default: neighbors = 5 - Preprocessing: Every predictor in the formula must be numeric with no missing data.\n\nimpute missing data and encode all categoricals before using this step\n\ntomek\n\nundersamples by removing pairs of points that each other‚Äôs nearest neighbor but are of different levels (tomek links)\n\nDoesn‚Äôt make sense to me this way. SInce it‚Äôs binary, maybe the point that‚Äôs the majority class gets removed the other point in the pair (minority class) gets saved.\n\nOnly for binary (factor) variables\nPreprocessing: Every predictor in the formula must be numeric with no missing data.\n\nimpute missing data and encode all categoricals before using this step\n\n\n\n\n\n\nFeature Reduction\n\nstep_discretize_xgb, step_pca_sparse, step_pca_sparse_bayes, step_pca_truncated\n\nRequires {embed}\n\nstep_discretize_cart\nlibrary(embed)\n\ndata(ames, package = \"modeldata\")\n\nrecipe(Sale_Price ~ Lot_Frontage + Lot_Area, data = ames) |&gt;\n  step_discretize_cart(all_numeric_predictors(), outcome = \"Sale_Price\") |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\n#&gt; # A tibble: 2,930 √ó 3\n#&gt;    Lot_Frontage Lot_Area              Sale_Price\n#&gt;    &lt;fct&gt;        &lt;fct&gt;                      &lt;int&gt;\n#&gt;  1 [118.5, Inf] [1.341e+04, Inf]          215000\n#&gt;  2 [60.5,81.5)  [1.093e+04,1.341e+04)     105000\n#&gt;  3 [60.5,81.5)  [1.341e+04, Inf]          172000\n#&gt;  4 [81.5,94.5)  [1.093e+04,1.341e+04)     244000\n#&gt;  5 [60.5,81.5)  [1.341e+04, Inf]          189900\n#&gt;  6 [60.5,81.5)  [8639,1.093e+04)          195500\n#&gt;  7 [24.5,49.5)  [-Inf,8639)               213500\n#&gt;  8 [24.5,49.5)  [-Inf,8639)               191500\n#&gt;  9 [24.5,49.5)  [-Inf,8639)               236500\n#&gt; 10 [49.5,60.5)  [-Inf,8639)               189000\n#&gt; # ‚Ñπ 2,920 more rows\n\nFits a decision tree using the numeric predictor against the outcome. Then replaces it with levels, according to the leafs of the tree.\n\nstep_umap\ndata(diamonds, package = \"ggplot2\")\n\nset.seed(1234)\n\nrecipe(price ~ ., data = diamonds) |&gt;\n  step_dummy(all_nominal_predictors()) |&gt;\n  step_normalize(all_numeric_predictors()) |&gt;\n  step_umap(all_numeric_predictors()) |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\n#&gt; # A tibble: 53,940 √ó 3\n#&gt;    price   UMAP1   UMAP2\n#&gt;    &lt;int&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n#&gt;  1   326  -0.605   5.56 \n#&gt;  2   326  -1.17  -16.8  \n#&gt;  3   327  -1.78   -5.43 \n#&gt;  4   334 -10.7   -12.3  \n#&gt;  5   335  14.2     2.66 \n#&gt;  6   336   2.50    1.79 \n#&gt;  7   336  -5.48    0.914\n#&gt;  8   337   7.39   -5.29 \n#&gt;  9   337  -1.43   -5.95 \n#&gt; 10   338  -6.38   -0.785\n#&gt; # ‚Ñπ 53,930 more rows\nstep_nnmf_sparse\nlibrary(Matrix) # needs to be loaded for step to work\n\ndata(ames, package = \"modeldata\")\n\nrecipe(Sale_Price ~ ., data = ames) |&gt;\n  step_dummy(all_nominal_predictors()) |&gt;\n  step_nzv(all_numeric_predictors()) |&gt;\n  step_normalize(all_numeric_predictors()) |&gt; \n  step_nnmf_sparse(all_numeric_predictors()) |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\n#&gt; # A tibble: 2,930 √ó 3\n#&gt;    Sale_Price   NNMF1  NNMF2\n#&gt;         &lt;int&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n#&gt;  1     215000  0.192  -0.473\n#&gt;  2     105000 -0.366   0.245\n#&gt;  3     172000 -0.165   0.141\n#&gt;  4     244000  0.255  -0.269\n#&gt;  5     189900  0.261  -0.160\n#&gt;  6     195500  0.311  -0.316\n#&gt;  7     213500  0.187  -0.307\n#&gt;  8     191500  0.0956 -0.475\n#&gt;  9     236500  0.315  -0.238\n#&gt; 10     189000  0.268  -0.112\n\nConverts numeric data into one or more components via non-negative matrix factorization with lasso penalization\n\nstep_kmeans\nlibrary(MachineShop)\n\nset.seed(1234)\n\nrecipe(~., data = mtcars) |&gt;\n  step_normalize(all_numeric_predictors()) |&gt;\n  step_kmeans(all_numeric_predictors(), k = 3) |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\n#&gt; # A tibble: 32 √ó 3\n#&gt;    KMeans1   KMeans2 KMeans3\n#&gt;      &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n#&gt;  1  0.583  -0.217     -0.823\n#&gt;  2  0.583  -0.165     -0.666\n#&gt;  3  0.634  -1.01       0.771\n#&gt;  4 -0.624  -0.309      1.00 \n#&gt;  5 -0.703   0.439     -0.666\n#&gt;  6 -0.910  -0.327      1.22 \n#&gt;  7 -0.857   0.918     -0.996\n#&gt;  8  0.125  -0.734      1.16 \n#&gt;  9  0.166  -0.655      1.97 \n#&gt; 10  0.0166  0.000617   0.684\n#&gt; # ‚Ñπ 22 more rows\nstep_depth\ndata(penguins, package = \"modeldata\")\n\nrecipe(species ~ bill_length_mm + bill_depth_mm, data = penguins) |&gt;\n  step_impute_mean(all_numeric_predictors()) |&gt;\n  step_depth(all_numeric_predictors(), class = \"species\") |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\n#&gt; # A tibble: 344 √ó 6\n#&gt;    bill_length_mm bill_depth_mm species depth_Adelie depth_Chinstrap\n#&gt;             &lt;dbl&gt;         &lt;dbl&gt; &lt;fct&gt;          &lt;dbl&gt;           &lt;dbl&gt;\n#&gt;  1           39.1          18.7 Adelie       0.349            0     \n#&gt;  2           39.5          17.4 Adelie       0.145            0     \n#&gt;  3           40.3          18   Adelie       0.217            0     \n#&gt;  4           43.9          17.2 Adelie       0.00658          0.0735\n#&gt;  5           36.7          19.3 Adelie       0.0789           0     \n#&gt;  6           39.3          20.6 Adelie       0.0395           0     \n#&gt;  7           38.9          17.8 Adelie       0.329            0     \n#&gt;  8           39.2          19.6 Adelie       0.132            0     \n#&gt;  9           34.1          18.1 Adelie       0.0263           0     \n#&gt; 10           42            20.2 Adelie       0.0461           0     \n#&gt; # ‚Ñπ 334 more rows\n#&gt; # ‚Ñπ 1 more variable: depth_Gentoo &lt;dbl&gt;\n\nRequires {ddalpha}\nCalculates a centroid from continuous variables and measures the distance the observation is from the centroid. There are various distance measures. The vignette has visual of a 2 dimension case for each distance measure. Contours are drawn around the centroid which give you an idea about the formulation of the distance measure.\nIt needs a categorical variable. I think there‚Äôs a centroid for each category. Probably works best if that categorical variable is the outcome, but I can also seeing it being useful with an informative categorical predictor.\nA higher depth value means the observation is closer to the centroid.",
    "crumbs": [
      "Model Building",
      "tidymodels"
    ]
  },
  {
    "objectID": "qmd/model-building-tidymodels.html#sec-modbld-tidymod-ex",
    "href": "qmd/model-building-tidymodels.html#sec-modbld-tidymod-ex",
    "title": "tidymodels",
    "section": "Examples",
    "text": "Examples\n\nBasic workflow (article)\npacman::p_load(tidymodels, embed, finetune, vip)\n\n# \"Accredation\" is the binary outcome\nset.seed(123)\nmuseum_split &lt;- initial_split(museum_parsed, strata = Accreditation)\nmuseum_train &lt;- training(museum_split)\nmuseum_test &lt;- testing(museum_split)\nset.seed(234)\nmuseum_folds &lt;- vfold_cv(museum_train, strata = Accreditation)\n\n# \"Subject_Matter\" is a high cardinality categorical predictor\nmuseum_rec &lt;-¬†\n¬† recipe(Accreditation ~ ., data = museum_train) %&gt;%\n¬† update_role(museum_id, new_role = \"id\") %&gt;%\n¬† step_lencode_glm(Subject_Matter, outcome = vars(Accreditation)) %&gt;%\n¬† step_dummy(all_nominal_predictors())\n\n# specify model and workflow\nxgb_spec &lt;-\n¬† boost_tree(\n¬† ¬† trees = tune(),\n¬† ¬† min_n = tune(),\n¬† ¬† mtry = tune(),\n¬† ¬† learn_rate = 0.01\n¬† ) %&gt;%\n¬† set_engine(\"xgboost\") %&gt;%\n¬† set_mode(\"classification\")\nxgb_wf &lt;- workflow(museum_rec, xgb_spec)\n\n# fit\ndoParallel::registerDoParallel()\nset.seed(345)\nxgb_rs &lt;- tune_race_anova(\n¬† xgb_wf,\n¬† resamples = museum_folds,\n¬† grid = 15,\n¬† control = control_race(verbose_elim = TRUE)\n)\n\n# evaluate\ncollect_metrics(xgb_rs)\nint_pctl(xgb_rs, alpha = .1) # Calculates percentile 90% CIs for the metrics.\n# viz for race_anova grid search strategy\nplot_race(xgb_rs)\n\n # fit on whole training set\nxgb_last &lt;- xgb_wf %&gt;%\n¬† finalize_workflow(select_best(xgb_rs, \"accuracy\")) %&gt;%\n¬† last_fit(museum_split)\n\n# evalute final model on test\ncollect_metrics(xgb_last)\ncollect_predictions(xgb_last) %&gt;%\n¬† ¬† conf_mat(Accreditation, .pred_class)\nxgb_last %&gt;%\n¬† extract_fit_engine() %&gt;%\n¬† vip()",
    "crumbs": [
      "Model Building",
      "tidymodels"
    ]
  },
  {
    "objectID": "qmd/regression-ordinal.html",
    "href": "qmd/regression-ordinal.html",
    "title": "Ordinal",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Regression",
      "Ordinal"
    ]
  },
  {
    "objectID": "qmd/regression-ordinal.html#sec-reg-ord-misc",
    "href": "qmd/regression-ordinal.html#sec-reg-ord-misc",
    "title": "Ordinal",
    "section": "",
    "text": "Notes from\n\nAnalysis of ordinal data with cumulative link models ‚Äî estimation with the R-package ordinal\n{ordinal} vignette, see below\nOrdinal regression in R: part 1\n\nPackages\n\nCumulative Link Models (CLMs)\n\n{ordinal} (Vignette)\n\nCLMs and CLMMs with location, scale and nominal effects (aka partial proportional odds), structured thresholds and flexible link functions\nMethods for marginal means, tests of functions of the coefficients, goodness-of-fit tests\n*Response variable should be an ordered factor class*\nCompatible with {marginaleffects}, {emmeans}, {ggeffects}\nPackage owner doesn‚Äôt seem to be developing the package further except for keeping on CRAN. Still has plenty of functionality, but it can take extra effort to get certain metrics and external packages to work with certain models (e.g.¬†clmm, clmm2 for mixed effects)\nCheck issues on its github for extra functionality since some users have added code there.\n\n{MASS::polr}\n\nStandard CLMs allowing for the 5 standard link functions but no further extensions\n\n{VGAM::vglm}\n\nCLMs using the cumulative link; allows for several link functions as well as partial effects.\nProportional odds model (cumulative logit model)\nProportional hazards model (cumulative cloglog model)\nContinuation ratio model (sequential logit model)\nStopping ratio model\nAdjacent categories model\n\n{rms::lrm, orm}\n\nCLMs with the 5 standard link functions but without scale effects, partial or structured thresholds.\n\n{mvord}:\n\nAn R Package for Fitting Multivariate Ordinal Regression Models\n\n{ordinalNet} (Vignette):\n\nFits ordinal regression models with elastic net penalty. Supported model families include cumulative probability, stopping ratio, continuation ratio, and adjacent category\n\n{brms}\n\nCLMs that include structured thresholds in addition to random-effects.\n\n{ordinalgmifs}\n\nOrdinal Regression for High-Dimensional Data\n\n{{statsmodels}}\n\nProbit, Logit, and Complementary Log-Log model\n\n\nML\n\n{partykit::ctree}\n\nTakes ordered factors as response vars and handles the rest (see vignette)\n\n{ordinalForest}\n\nPrediction and Variable Ranking with Ordinal Target Variables\n\n\n\nPaired data: Use robust cluster sandwich covariance adjustment to allow ordinal regression to work on paired data. (Harrell)\nOrdered probit regression: This is very, very similar to running an ordered logistic regression. The main difference is in the interpretation of the coefficients.\nSample size: Both ordered logistic and ordered probit, using maximum likelihood estimates, require sufficient sample size. How big is big is a topic of some debate, but they almost always require more cases than OLS regression.\nHarrell summary and comparison of a PO model vs Logistic in the case of an ordinal outcome\n\n\nFormula is an ordinal outcome, Y, with binary treatment variable, Tx, and adjustment variables, X.\nSometimes researchers tend to collapse an ordinal outcome into a binary outcome. Harrell is showing how using a logistic model is inefficient and lower power than a PO model\nOriginal is interactive with additional information (link)\nIncludes: efficiency, infomation used, assumptions, special cases, estimands, misc",
    "crumbs": [
      "Regression",
      "Ordinal"
    ]
  },
  {
    "objectID": "qmd/regression-ordinal.html#sec-reg-ord-eda",
    "href": "qmd/regression-ordinal.html#sec-reg-ord-eda",
    "title": "Ordinal",
    "section": "EDA",
    "text": "EDA\n\nMisc\n\nDescriptive stats for numeric explanatory variables\n\nCrosstabs\n\nExample: Aggregated Counts\nftable(xtabs(~ public + apply + pared, \n             data = dat))\n\n##¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† pared¬†  0¬†  1\n## public apply¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬†\n## 0¬† ¬† ¬† unlikely¬† ¬† ¬† ¬† ¬† ¬† ¬† 175¬† 14\n##¬† ¬† ¬† ¬† somewhat likely¬† ¬† ¬† ¬† 98¬† 26\n##¬† ¬† ¬† ¬† very likely¬† ¬† ¬† ¬† ¬† ¬† 20¬† 10\n## 1¬† ¬† ¬† unlikely¬† ¬† ¬† ¬† ¬† ¬† ¬†  25¬†  6\n##¬† ¬† ¬† ¬† somewhat likely¬† ¬† ¬† ¬† 12¬†  4\n##¬† ¬† ¬† ¬† very likely¬† ¬† ¬† ¬† ¬† ¬†  7¬†  3\n\napply is the response variable\nEmpty cells or small cells: If a cell has very few cases, the model may become unstable or it might not run at all.\n\nExample: Observation Level (article)\n\nwine %&gt;%\n  transmute(temp, contact, bottle, judge, rating = as.numeric(rating)) %&gt;%\n  pivot_wider(names_from = judge, values_from = rating) %&gt;%\n  gt() %&gt;%\n  tab_spanner(columns = `1`:`9`, label = \"judge\") %&gt;%\n  data_color(\n    columns = `1`:`9`,\n    colors = scales::col_numeric(\n      palette = c(\"white\", wine_red), domain = c(1, 5)\n    )\n  )\n\nData from Proportional Odds (PO) &gt;&gt; Example 1\nMost judges seem to rate warm bottles highest especially when contact = yes.\n\nExample: Look for variation in a potential random effect\n\nwine %&gt;%\n  count(judge, rating) %&gt;%\n  ggplot(aes(x = judge, y = rating)) +\n  geom_tile(aes(fill = n)) +\n  geom_text(aes(label = n), color = \"white\") +\n  scale_x_discrete(expand = c(0, 0)) +\n  scale_y_discrete(expand = c(0, 0)) +\n  theme(legend.position = \"none\") +\n  labs(title = \"Number of ratings by judge\")\n\nThere‚Äôs some judge-specific variability in the perception of bitterness of wine. judge 5, for instance, doesn‚Äôt stray far from rating = 3, while judge 7 didn‚Äôt consider any of the wines particularly bitter.\n\n\nHistograms\n\nThe shape can help you choose a link function (See Cumalative Link Models (CLM) &gt;&gt; Link Functions)\nIf distributions of a pre-treatment/baseline (outcome) variable looks substantially different than the (post-treatment) outcome variable, then the treatment likely had an effect\nExample: Simple Percentages\n\\(\\mbox{lvl}_1 = \\mbox{lvl}_{1/N} = 0.2219\\), \\(\\mbox{lvl}_2 = 0.2488\\), \\(\\mbox{lvl}_3 = 0.25\\), and \\(\\mbox{lvl}_4 = 0.2794\\)\n\nMonotonicly increasing, so skewing left\n\nExample: Histogram, Cumulative Proportion, Log Cumulative Proportion (article)\n\nwine_prop &lt;- wine %&gt;%\n  count(rating) %&gt;%\n  mutate(p = n / sum(n), cumsum_p = cumsum(p))\n\n(\n  ggplot(wine_prop, aes(x = rating, y = p)) +\n    geom_col(fill = wine_red) +\n    scale_y_continuous(labels = scales::percent, expand = c(0, 0)) +\n    labs(x = \"j\", y = \"proportion\")\n) +\n  (\n    ggplot(wine_prop, aes(x = as.integer(rating), y = cumsum_p)) +\n      geom_point(size = 2) +\n      geom_line(size = 1) +\n      labs(x = \"j\", y = \"cumulative proportion\")\n  ) +\n  (\n    ggplot(wine_prop,\n        aes(x = as.integer(rating), y = log(cumsum_p) - log(1 - cumsum_p))) +\n      geom_point(size = 2) +\n      geom_line(size = 1) +\n      labs(x = \"j\", y = \"logit(cumulative proportion)\")\n  )\n\nData from Proportional Odds (PO) &gt;&gt; Example 1\nProportions are used since the response is from a multinomial distribution\n\nSee Cumulative Link Models (CLM) &gt;&gt; Response variable follows the Multinomial distribution\n\nDistribution looks a little right-skewed.\n\n\nBi-variate boxplots\n\nLook for trends of the median\nExample: Ordinal vs Ordinal\n\n\nNo idea what the red/blue splits mean\nY-Axis: ordinal outcome: 4 levels (y-xis is messed up)\nX-Axis: pre-treatment variable: 7 levels\nBoxes have the counts\nSteady increase in the median\n\nMakes sense because someone starting w/a higher score (pre-treatment) should mostly end up with a higher score (post-treatment)\n\n\nExample: Numeric vs Ordinal\n\nggplot(dat, aes(x = apply, y = gpa)) +\n¬† geom_boxplot(size = .75) +\n¬† geom_jitter(alpha = .5) +\n¬† facet_grid(pared ~ public, margins = TRUE) +\n¬† theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1))\n\nIn the lower right hand corner, is the overall relationship between apply and gpa which appears slightly positive.\n\n\nPre-intervention means of the outcome by explanatory variables\n\nExample\nCC = 0, TV = 0 2.152\nCC = 0, TV = 1 2.087\nCC = 1, TV = 0 2.050\nCC = 1, TV = 1 1.979\n\nCC and TV are binary explanatory variables\nThe mean score of the pre-treatment outcome doesn‚Äôt change much given these two explanatory variables.\nI think this shows that for the most part that assignment of the two different treatments was balanced in terms of the scores of the baseline variable.\n\n\nDot Plots\n\nExample: 4 explanatory factor variables (article)\n\nwine %&gt;%\n  count(contact, rating, temp) %&gt;%\n  mutate(temp = fct_rev(temp)) %&gt;%\n  ggplot(aes(x = temp, y = rating, color = temp)) +\n  geom_point(aes(group = temp, size = n)) +\n  facet_wrap(~contact, scales = \"free_x\",\n             labeller = labeller(contact = label_both)) +\n  scale_size(breaks = c(1, 2, 4, 6, 8)) +\n  add_facet_borders()\n\nData from Proportional Odds (PO) &gt;&gt; Example 1\nMost judges seem to rate warm bottles highest especially when contact = yes.\nCompared to the crosstab, I think this more clearly shows that there is an effect present between temp and rating especially when moderated by contact.",
    "crumbs": [
      "Regression",
      "Ordinal"
    ]
  },
  {
    "objectID": "qmd/regression-ordinal.html#sec-reg-ord-diag",
    "href": "qmd/regression-ordinal.html#sec-reg-ord-diag",
    "title": "Ordinal",
    "section": "Diagnostics",
    "text": "Diagnostics\n\ncond.H - Located in top row of GOF metrics. The condition number of the Hessian is a measure of how identifiable the model is.\n\nValues larger than 1e4 indicate that the model may be ill defined\n\nDeviance: \\(-2 \\cdot (\\mbox{LogLik Reduced Model} - \\mbox{LogLik Saturated model})\\)\n\nLikelihood ratio statistic for the comparison of the full and reduced models\nReduced model is the model you just fit.\n\nCan usually be extracted from the model object (e.g.¬†{ordinal}: ll_reduced &lt;- mod$logLik)\n\nSaturated Model also called the Full Model\n\nAlso see Regression, Discrete &gt;&gt; Terms\nThe Full Model has a parameter for each observation and describes the data perfectly while the reduced model provides a more concise description of the data with fewer parameters.\nUsually calculated from the data themselves\ndata(wine, package = \"ordinal\")\ntab &lt;- with(wine, table(temp:contact, rating))\n## Get full log-likelihood (aka saturated model log-likelihood)\npi.hat &lt;- tab / rowSums(tab)\n(ll.full &lt;- sum(tab * ifelse(pi.hat &gt; 0, log(pi.hat), 0))) ## -84.01558\n\n\nGOF (rule of thumb): If the deviance about the same size as the difference in the number of parameters (i.e.¬†\\(p_{\\mbox{full}} - p_{\\mbox{reduced}}\\)), there is NOT evidence of lack of fit. (ordinal vignette, pg 14)\n\nExample (Have doubts this is correct)\n\nLooking at the number of parameters (no.par) for fm1 in Example 1 (below) and the model summary in Proportional Odds (PO) &gt;&gt; Example 1, the number of parameters for the reduced model is the \\(\\mbox{number of regression parameters} (2) + \\mbox{number of thresholds} (4)\\)\nFor the full model (aka saturated), the number of thresholds should be the same, and there should be one more regression parameter, an interaction between temp and contact. So, 7 should be the number of parameters for the full model\nTherefore, for a good-fitting model, the deviance should be close to \\(p_{\\text{full}} - p_{\\text{reduced}} = 7 - 6 = 1\\)\nThis example uses ‚Äúnumber of parameters‚Äù which is the phrase in the vignette, but I think it‚Äôs possible he might mean degrees of freedom (dof) which he immediatedly discusses afterwards. In the LR Test example below, under LR.Stat, which is essentially what deviance is, the number is around 11 which is quite aways from 1. Not exactly an apples to apples comparison, but the size after adding 1 parameter just makes me wonder if dof would match this scale of numbers for deviances better.\n\n\nModel Selection: A difference in deviance between two nested models is identical to the likelihood ratio statistic for the comparison of these models\n\nSee Example 1 (below)\n\nRequirement: Deviance tests are fine if the expected frequencies under the reduced model are not too small and as a general rule they should all be at least five.\n\nAlso see Discrete Analysis notebook\n\nResidual Deviance: \\(D_{\\text{resid}} = D_{\\text{total}} - D_{\\text{reduced}}\\)\n\nA concept similar to a residual sums of squares (RSS)\nTotal Deviance (\\(D_{\\text{total}}\\)) is the Null Deviance: \\(-2*(\\mbox{LogLik Null Model} - \\mbox{LogLik Saturated model})\\)\n\nAnalogous to the total sums of squares for linear models\n\n\\(D_{\\text{reduced}}\\) is the calculation of Deviance shown above\nSee example 7, pg 13 ({ordinal} vignette) for (manual) code\n\nExample 1: Model selection with LR tests\nfm2 &lt;- \n  ordinal::clm(rating ~ temp, \n               data = wine)\nanova(fm2, fm1)\n\n#&gt; Likelihood ratio tests of cumulative link models:\n#&gt; ¬† ¬† formula:¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† link:¬† threshold:\n#&gt; fm2 rating ~ temp¬† ¬† ¬† ¬† ¬†  logit¬† flexible\n#&gt; fm1 rating ~ temp + contact logit¬† flexible\n#&gt; ¬† ¬† no.par AIC¬† ¬†  logLik¬† LR.stat df  Pr(&gt;Chisq)\n#&gt; fm2 5¬† ¬† ¬† 194.03 -92.013\n#&gt; fm1 6¬† ¬† ¬† 184.98 -86.492¬† 11.043¬† 1¬†  0.0008902 ***\n\nFor fm1 model, see Proportional Odds (PO) &gt;&gt; Example 1\nSpecial method for clm objects from the package; produces an Analysis of Deviance (ANODE) table\nAdding contact produces a better model (pval &lt; 0.05)\n\nExample 2: LR tests for variables\ndrop1(fm1, \n      test = \"Chi\")\n\n#&gt; rating ~ contact + temp\n#&gt; ¬† ¬† ¬†   Df¬† AIC¬† ¬† LRT¬† ¬† Pr(&gt;Chi)\n#&gt; &lt;none&gt;¬† ¬†   184.98\n#&gt; contact 1¬†  194.03 11.043 0.0008902 ***\n#&gt; temp¬† ¬† 1¬†  209.91 26.928 2.112e-07 ***\n\n# fit null model\nfm0 &lt;- \n  ordinal::clm(rating ~ 1, \n               data = wine)\nadd1(fm0, \n     scope = ~ temp + contact, \n     test = \"Chi\")\n\n#&gt; ¬† ¬† ¬† ¬† Df¬† AIC¬† ¬† LRT¬† ¬† ¬† Pr(&gt;Chi)\n#&gt; &lt;none&gt;¬† ¬† ¬† 215.44\n#&gt; temp¬† ¬† 1¬† ¬†194.03¬†23.4113¬† 1.308e-06 ***\n#&gt; contact 1¬† ¬†209.91¬†7.5263¬†  0.00608   **\n\nFor fm1 model, see Proportional Odds (PO) &gt;&gt; Example 1\ndrop1\n\nTests the same thing as the Wald tests in the summary except with \\(\\chi^2\\) instead of t-tests\n\ni.e.¬†Whether the estimates, while controlling for the other variables, differ from 0, except with LR tests.\n\nIn this case, LR tests slightly more significant than the Wald tests\n\nadd1\n\nTests variables where they‚Äôre the only explanatory variable in the model (i.e.¬†ingores other variables)\nBoth variables still significant even without controlling for the other variable",
    "crumbs": [
      "Regression",
      "Ordinal"
    ]
  },
  {
    "objectID": "qmd/regression-ordinal.html#sec-reg-ord-log",
    "href": "qmd/regression-ordinal.html#sec-reg-ord-log",
    "title": "Ordinal",
    "section": "Logistic Regression",
    "text": "Logistic Regression\n\nSkipped this for now.",
    "crumbs": [
      "Regression",
      "Ordinal"
    ]
  },
  {
    "objectID": "qmd/regression-ordinal.html#sec-reg-ord-clm",
    "href": "qmd/regression-ordinal.html#sec-reg-ord-clm",
    "title": "Ordinal",
    "section": "Cumulative Link Models (CLM)",
    "text": "Cumulative Link Models (CLM)\n\nA general class of ordinal regression models that include many of the models in this note.\nTypes\n\nProportional Odds Model: CLM with a logit link\nProportional Hazards Model: CLM with a log-log link, for grouped survival times\n\\[\n-\\log(1-\\gamma_j(\\boldsymbol{x}_i)) = e^{\\theta_j - \\boldsymbol{x}_i^T \\boldsymbol{\\beta}}\n\\]\n\nLink Function: \\(\\log(‚àí \\log(1 ‚àí \\gamma))\\)\nInverse Link: \\(1 ‚àí e^{‚àí e^\\eta}\\)\nDistribution: \\(\\mbox{Gumbel} (\\min)^b\\) (?)\n\\(1 ‚àí \\gamma_j(x_i)\\) is the probability or survival beyond category \\(j\\) given \\(x_i\\).\n\nPartial Proportional Odds: Also referred to as Unequal Slopes, Partial Effects, and Nominal Effects\nCumulative Link Mixed Models (CLMM): CLMs with normally distributed random effects\n\nLink Functions\n\n\nNote the shape of the distributions of the response (see EDA) to help choose a link function\n\nSee Proportional Odds (PO) &gt;&gt; Example 2\nThe probit link is often used when the model is interpreted with reference to a latent variable\nKurtotic, see Mathematics, Statistics &gt;&gt; Descriptive Statistics &gt;&gt; Kurtosis (i.e.¬†Higher sharper peaks w/short tails, flatter peaks w/long tails)\n\nDefault parameter values fit a symmetric heavy tailed distribution (high, sharp peak)\n\n\n\nModel\n\n\n\\(j\\) is the jth ordinal category where \\(j_1 \\lt j_2 \\lt \\ldots\\)\n\\(i\\) is the ith observation\nThe regression part \\(x^T_i \\beta\\) is independent of \\(j\\), so \\(\\beta\\) has the same effect for each of the \\(J ‚àí 1\\) cumulative logits\nThe \\(\\{\\theta_j\\}\\) parameters provide each cumulative logit (for each \\(j,\\) see below) with its own intercept\n\n\\(\\theta\\) is called a ‚Äúthreshold.‚Äù See below, Latent Variable Concept\n\n\nResponse variable follows the Multinomial distribution\n\\[\n\\begin{aligned}\n&\\gamma_{i,j} = P(Y_i \\le j) = \\pi_{i,1} + \\cdots + \\pi_{i,j}\\\\\n&\\text{with} \\quad \\sum_{j=1}^J \\pi_{i,j} = 1\n\\end{aligned}\n\\]\n\nThe output is the probability that the response is the jth category or lower.\n\\(\\pi_{i,j}\\) denotes the probability that the ith observation falls in response category \\(j\\)\nThis distribution can be visualized by looking at the cumulative proportions in the observed data (See EDA &gt;&gt; Histograms)\n\nCumulative Logits (Logit Link)\n\n\n\\(j = 1 , \\ldots , J - 1\\) , so cumulative logits are defined for all but the last category\nIf \\(x\\) represent a treatment variable with two levels (e.g., placebo and treatment), then \\(x_2 ‚àí x_1 = 0 - 1 = -1\\) and the odds ratio is \\(e^{‚àí\\beta_{\\text{treatment}}}\\).\n\nSimilarly the odds ratio of the event \\(Y \\ge j\\) is \\(e^{\\beta_\\text{treatment}}\\) (i.e.¬†inverse of \\(Y \\le j\\)).\n\n\nLatent Variable Concept\n\nNotes from Models for Proportional and Nonproportional Odds\n‚ÄúTo motivate the ordinal regression model, it is often assumed that there is an unobservable latent variable ( \\(y^*\\) ) which is related to the actual response through the‚Äùthreshold concept.‚Äù An example of this is when respondents are asked to rate their agreement with a given statement using the categories ‚ÄúDisagree,‚Äù ‚ÄúNeutral,‚Äù ‚ÄúAgree.‚Äù These three options leave no room for any other response, though one can argue that these are three possibilities along a continuous scale of agreement that would also make provision for ‚ÄúStrongly Agree‚Äù and ‚ÄúDisagree somewhat.‚Äù The ordinal responses captured in \\(y\\) and the latent continuous variable \\(y^*\\) are linked through some fixed, but unknown, thresholds.‚Äù\nA response occurs in category \\(j\\) (\\(Y = j\\)) if the latent response process \\(y^*\\) exceeds the threshold value, \\(\\theta_{j-1}\\) , but does not exceed the threshold value, \\(\\theta_j\\) .\n\nI think \\(y^*\\) is continous on the scale of logits. \\(\\theta_j\\)s are also intercepts in the model equations and also on the scale of logits (see Proportional Odds (PO) &gt;&gt; Example 3)\n\nThe cumulative probabilities are given in terms of the cumulative logits with \\(J‚àí1\\) strictly increasing model thresholds \\(\\theta_1, \\theta_2, \\ldots , \\theta_{J-1}\\).\n\nWith \\(J = 4\\), we would have \\(J ‚àí1 = 3\\) cumulative probabilities, given in terms of 3 thresholds \\(\\theta_1\\), \\(\\theta_2\\), and \\(\\theta_3\\) . The thresholds represent the marginal response probabilities in the \\(J\\) categories.\nEach cumulative logit is a model equation with a threshold for an intercept\n\nTo set the location of the latent variable, it is common to set a threshold to zero. Usually, the first of the threshold parameters ( \\(\\theta_1\\)) is set to zero.\n\nAlternatively, the model intercept (\\(\\beta_0\\)) is set to zero and \\(J ‚àí1\\) thresholds are estimated. (Think this is the way {ordinal} does it.)\n\n\nStructured Thresholds\n\nIf ratings are the response variable, placing restrictions on thresholds and fitting a model allows us to test assumptions on how the judges are using the response scale\nAn advantage of applying additional restrictions on the thresholds is that the model has fewer parameters to estimate which increases model sensitivity.\nThreshold distances are affected by the shape of the latent variable distribution which is determined by the link function used\n\ni.e.¬†If it‚Äôs determined that threshold distances are equidistant under a normal distribution (logit link) assumption, then they will not be equidistant if a different link function is used.\n\nExample: Is the reponse scale being treated by judges as equidistant between values.\n\ne.g.¬†Is the distance between a rating of 2 and a rating of 1 the same as the distance between a rating of 2 and a rating of 3?\nMathematically: \\(\\theta_j ‚àí \\theta_{j‚àí1}\\) = constant for \\(j = 2, ..., J‚àí1\\) where \\(\\theta\\) are the thresholds and \\(J\\) is the number of levels in the response variable.\nFit equidistant model\nfm.equi &lt;- \n  clm(rating ~ temp + contact, \n      data = wine, \n      threshold = \"equidistant\")\nsummary(fm.equi)\n\n#&gt; link¬† threshold¬†  nobs¬† logLik¬† AIC¬† ¬† niter¬† max.grad¬† cond.H\n#&gt; logit equidistant 72¬†  -87.86¬†  183.73 5(0)¬†  4.80e-07¬† 3.2e+01\n#&gt; \n#&gt; ¬† ¬† ¬† ¬† ¬†  Estimate Std.Error z.value Pr(&gt;|z|)\n#&gt; tempwarm¬†  2.4632¬†  0.5164¬† ¬† 4.77¬† ¬† 1.84e-06 ***\n#&gt; contactyes 1.5080¬†  0.4712¬† ¬† 3.20¬† ¬† 0.00137 **\n#&gt; \n#&gt; Threshold coefficients:\n#&gt; ¬† ¬† ¬† ¬† ¬† ¬†  Estimate Std.Error z.value\n#&gt; threshold.1 -1.0010¬†  0.3978¬†  -2.517\n#&gt; spacing¬† ¬† ¬† 2.1229¬†  0.2455¬† ¬† 8.646\n\nspacing: Average distance between consecutive thresholds\n\nCompare spacing parameter with your model‚Äôs average spacing\nmean(diff(coef(fm1)[1:4]))\n#&gt; 2.116929\n\nfm1 is from Proportional Odds (PO) &gt;&gt; Example 1\nResult: fm1 spacing is very close to fm.equi spacing. Judges are likely applying the response scale as having equal distance between rating values.\n\nDoes applying threshold restrictions decrease the model‚Äôs GOF\nanova(fm1, fm.equi)\n\n#&gt; ¬† ¬† ¬† ¬† no.par AIC¬† ¬†  logLik LR.stat df Pr(&gt;Chisq)\n#&gt; fm.equi 4¬† ¬† ¬† 183.73 -87.865\n#&gt; fm1¬† ¬†  6¬† ¬† ¬† 184.98 -86.492 2.7454¬† 2¬† 0.2534\n\nNo statistical difference in log-likelihoods. Fewer parameters to estimate is better, so keep the equidistant thresholds",
    "crumbs": [
      "Regression",
      "Ordinal"
    ]
  },
  {
    "objectID": "qmd/regression-ordinal.html#sec-reg-ord-po",
    "href": "qmd/regression-ordinal.html#sec-reg-ord-po",
    "title": "Ordinal",
    "section": "Proportional Odds (PO)",
    "text": "Proportional Odds (PO)\n\nMisc\n\nThe treatment effect for proportional odds model will be the average of the treatment effects of \\(J-1\\) logistic regression models where each model is dichotmized at each, but not the last, ordinal level.\n\nThe intercepts in the proportional odds model will be similar to those intercepts from the \\(J-1\\) logistic regression models\n\nThe intercepts of these logistic models have to be different, but the slopes could (in principle) be the same or same-ish. If they are the same, then the proportional odds assumption holds.\n\nThe proportional odds model has a smaller std.error for its treatment effect than any of the treatment effects of the \\(J-1\\) logistic regression models (i.e.¬†more accurately estimated in p.o. model)\n\nBenefits\n\nIt enforces stochastic ordering (?)\nLots of choices for link functions.\nReasonable model for analysing ordinal data\n\n\\(\\beta\\) will be some sort of appropriately-weighted average of what you‚Äôd get for the separate logistic regressions\n\n\n\nAssumption: The independent variable effect is the same for all levels of the ordinal outcome variable\n\n\\(\\beta\\)s (i.e.¬†treatment effect) are not allowed to vary with \\(j\\) (i.e.¬†response variable levels) or equivalently that the threshold parameters \\({\\theta_j}\\) are not allowed to depend on regression variables\nExample:\n\nOutcome = health_status (1 for poor, 2 for average, 3 for good and 4 for excellent)\nIndependent Variable = family_income (1 for above avg, 0 for below average)\nIf the proportional odds assumption holds then:\n\nThe log odds of being at average health from poor health is \\(\\beta_1\\) if family income increases to above average status.\nThe log odds of being at good heath from average health is \\(\\beta_1\\) if family income increases to above average status.\nThe log odds of being at excellent heath from good health is \\(\\beta_1\\) if family income increases to above average status.\n\n\nTesting the assumption\n\nEven if the model fails the PO assumption, it can still be useful (See Misc &gt;&gt; Harrell summary and comparison of a PO model vs Logistic)\nIssues with testing\n\nSmall Sample Sizes: For some variables, you might not have enough power to detect important violations of the PO assumption.\nLarge Sample Sizes: For some variables, you will detect small, unimportant violations of the PO assumption and reject a good model.\nOmnidirectional goodness-of-fit tests don‚Äôt tell you which variables you should look at for improvements.\n\nYou can test this assumption by fitting a PO model and PPO model. Then, comparing both models via LR Test or using {ordinal::nominal_test} (See below, Partial Proportional Odds (PPO) &gt;&gt; Example 2 and Example 3)\nAssess graphically (article, Harrell RMS pg 316)\n\nCalculate estimated effects for each explanatory variable in a univariate logistic regression model (e.g.¬†glm(apply ~ pared, family = binomial))\nsf &lt;- function(y) {\n¬† c('Y&gt;=1' = qlogis(mean(y &gt;= 1)),\n¬† ¬† 'Y&gt;=2' = qlogis(mean(y &gt;= 2)),\n¬† ¬† 'Y&gt;=3' = qlogis(mean(y &gt;= 3)))\n}\n(s &lt;- \n    with(dat, \n         summary(as.numeric(apply) ~ pared + public + gpa, \n                 fun=sf)))\n#&gt; +-------+-----------+---+----+--------+------+\n#&gt; |¬† ¬† ¬†  |¬† ¬† ¬† ¬† ¬†  |N¬† |Y&gt;=1|Y&gt;=2¬† ¬† |Y&gt;=3¬† |\n#&gt; +-------+-----------+---+----+--------+------+\n#&gt; |pared¬† |No¬† ¬† ¬† ¬†  |337|Inf |-0.37834|-2.441|\n#&gt; |¬† ¬† ¬†  |Yes¬† ¬† ¬† ¬† | 63|Inf | 0.76547|-1.347|\n#&gt; +-------+-----------+---+----+--------+------+\n#&gt; |public |No¬† ¬† ¬† ¬†  |343|Inf |-0.20479|-2.345|\n#&gt; |¬† ¬† ¬†  |Yes¬† ¬† ¬† ¬† | 57|Inf |-0.17589|-1.548|\n#&gt; +-------+-----------+---+----+--------+------+\n#&gt; |gpa¬† ¬† |[1.90,2.73)|102|Inf |-0.39730|-2.773|\n#&gt; |¬† ¬† ¬†  |[2.73,3.00)| 99|Inf |-0.26415|-2.303|\n#&gt; |¬† ¬† ¬†  |[3.00,3.28)|100|Inf |-0.20067|-2.091|\n#&gt; |¬† ¬† ¬†  |[3.28,4.00]| 99|Inf | 0.06062|-1.804|\n#&gt; +-------+-----------+---+----+--------+------+\n#&gt; |Overall|¬† ¬† ¬† ¬† ¬†  |400|Inf |-0.20067|-2.197|\n#&gt; +-------+-----------+---+----+--------+------+\n\nFor \\(Y \\ge 2\\) and pared = no, -0.37834 is the same as the intercept of the univariate model\nFor \\(Y \\ge 2\\) and pared = yes, 0.76547 is the same as the intercept + coefficient of the univariate model\n\nFor each variable, calculate differences in estimates between levels of the ordinal response\ns[, 4] &lt;- s[, 4] - s[, 3]\ns[, 3] &lt;- s[, 3] - s[, 3]\ns\n## +-------+-----------+---+----+----+------+\n## |¬† ¬† ¬†  |¬† ¬† ¬† ¬† ¬†  |N¬† |Y&gt;=1|Y&gt;=2|Y&gt;=3¬† |\n## +-------+-----------+---+----+----+------+\n## |pared¬† |No¬† ¬† ¬† ¬†  |337|Inf |0¬† |-2.062|\n## |¬† ¬† ¬†  |Yes¬† ¬† ¬† ¬† | 63|Inf |0¬† |-2.113|\n## +-------+-----------+---+----+----+------+\n## |public |No¬† ¬† ¬† ¬†  |343|Inf |0¬† |-2.140|\n## |¬† ¬† ¬†  |Yes¬† ¬† ¬† ¬† | 57|Inf |0¬† |-1.372|\n## +-------+-----------+---+----+----+------+\n## |gpa¬† ¬† |[1.90,2.73)|102|Inf |0¬† |-2.375|\n## |¬† ¬† ¬†  |[2.73,3.00)| 99|Inf |0¬† |-2.038|\n## |¬† ¬† ¬†  |[3.00,3.28)|100|Inf |0¬† |-1.890|\n## |¬† ¬† ¬†  |[3.28,4.00]| 99|Inf |0¬† |-1.864|\n## +-------+-----------+---+----+----+------+\n## |Overall|¬† ¬† ¬† ¬† ¬†  |400|Inf |0¬† |-1.997|\n## +-------+-----------+---+----+----+------+\n\nFor pared, the difference in estimates between \\(Y \\ge 2\\) and \\(Y \\ge 3\\) is -2.062 and -2.113. Therefore, it would seem that the PO assumption holds up pretty well for pared.\nFor public, the difference in estimates between \\(Y \\ge 2\\) and \\(Y \\ge 3\\) is -2.140 and -1.372. Therefore, it would seem that the PO assumption does NOT hold up for public.\nIf there was a \\(Y \\ge 4\\), then for pared, the difference in estimates between \\(Y \\ge 3\\) and \\(Y \\ge 4\\) should be near 2 as well in order for the PO assumption to hold for that variable.\n\nPlot\n\nplot(s, which=1:3, pch=1:3, xlab='logit', main=' ', xlim=range(s[,3:4]))\n\nIn addition to the PO assumption not seemingly holding for public, there also seems to be some substantial differences between the quantiles of gpa.\n\nThe 3rd and 4th seem to be in agreement, but not the 1st and maybe not the 2nd.\n\n\n\n\n\nExample 1: Basic\n\nData\ndata(wine, package = \"ordinal\")\nglimpse(as_tibble(wine))\n\n#&gt; Rows: 72\n#&gt; Columns: 6\n#&gt; $ response &lt;dbl&gt; 36, 48, 47, 67, 77, 60, 83, 90, 17, 22, 14, 50, 30, 51, 90, 7‚Ä¶\n#&gt; $ rating   &lt;ord&gt; 2, 3, 3, 4, 4, 4, 5, 5, 1, 2, 1, 3, 2, 3, 5, 4, 2, 3, 3, 2, 5‚Ä¶\n#&gt; $ temp     &lt;fct&gt; cold, cold, cold, cold, warm, warm, warm, warm, cold, cold, c‚Ä¶\n#&gt; $ contact  &lt;fct&gt; no, no, yes, yes, no, no, yes, yes, no, no, yes, yes, no, no,‚Ä¶\n#&gt; $ bottle   &lt;fct&gt; 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5‚Ä¶\n#&gt; $ judge    &lt;fct&gt; 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3‚Ä¶\n\nOther variables\n\nresponse: alternative outcome; wine bitterness rating on a 0-100 scale\nRandom Effects\n\nbottle with 8 levels\njudge with 9 levels\n\n\n\nModel\nfm1 &lt;- \n  ordinal::clm(rating ~ contact + temp, \n               data = wine)\nsummary(fm1)\n\n#&gt; formula: rating ~ contact + temp\n#&gt; data: wine\n#&gt; \n#&gt; link threshold nobs logLik¬† AIC¬† ¬† niter max.grad¬† cond.H\n#&gt; logit flexible 72¬† -86.49¬† 184.98¬† 6(0)¬† 4.01e-12¬† 2.7e+01\n#&gt; \n#&gt; Coefficients:\n#&gt; ¬† ¬† ¬† ¬† ¬†  Estimate¬† Std.Error¬† z value¬† Pr(&gt;|z|)\n#&gt; contactyes 1.5278¬† ¬† 0.4766¬† ¬†  3.205¬† ¬† 0.00135 **\n#&gt; tempwarm¬†  2.5031¬† ¬† 0.5287¬† ¬†  4.735¬† ¬† 2.19e-06 ***\n#&gt; ---\n#&gt; Signif. codes: 0 ‚Äò***‚Äô 0.001 ‚Äò**‚Äô 0.01 ‚Äò*‚Äô 0.05 ‚Äò.‚Äô 0.1 ‚Äò ‚Äô 1\n#&gt; \n#&gt; Threshold coefficients:\n#&gt; ¬† ¬† Estimate Std.Error z value\n#&gt; 1|2 -1.3444¬† 0.5171¬†   -2.600\n#&gt; 2|3¬† 1.2508¬† 0.4379¬† ¬†  2.857\n#&gt; 3|4¬† 3.4669¬† 0.5978¬† ¬†  5.800\n#&gt; 4|5¬† 5.0064¬† 0.7309¬† ¬†  6.850\n#&gt; \n#&gt; confint(fm1)\n#&gt; ¬† ¬† ¬† ¬† ¬†  2.5 %¬† ¬†  97.5 %\n#&gt; tempwarm¬†  1.5097627 3.595225\n#&gt; contactyes 0.6157925 2.492404\n\nModel: \\(\\mbox{logit}(P(Y_i \\le j)) = \\theta_j ‚àí \\beta_1(\\mbox{temp}_i ) ‚àí \\beta_2(\\mbox{contact}_i)\\)\nResponse is wine rating (1 to 5 = most bitter)\n\nThe response variable should be an ordered factor class\n\nIn this example, both explanatory variables were factor variables.\n\n\ncond.H - Located in top row of GOF metrics. The condition number of the Hessian is a measure of how identifiable the model is.\n\nValues larger than 1e4 indicate that the model may be ill defined\n\nRegression Coefficients\n\ntemp: \\(\\beta_1(\\mbox{warm} ‚àí \\mbox{cold}) = 2.50\\)\n\nThe reference level is cold, and this is the effect of moving from cold to warm\n\ncontact: \\(\\beta_2(\\mbox{yes} ‚àí \\mbox{no}) = 1.53\\)\n\nThe odds ratio of bitterness being rated in category \\(j\\) or above (\\(\\mbox{OR}(Y ‚â• j)\\)) is \\(e^{\\beta_2(\\mbox{yes ‚àí no})} = e^{1.53} = 4.61\\).\n\nNote: this is \\(\\mbox{Pr}(Y \\ge j)\\) which is why we‚Äôre using the positive \\(\\beta\\)\n\nThe reference level is no, and this is the effect of moving from no to yes\n\nInterpretation:\n\ncontact and warm temperature both lead to higher probabilities of observations in the high categories\nMe: The odds of the rating being higher are \\(4.61\\) times greater when contact = yes than when contact = no.\n\n\nThresholds (aka intercepts)\n\n\\(\\{\\theta_j\\} = \\{‚àí1.34, 1.25, 3.47, 5.01\\}\\)\nOften the thresholds are not of primary interest, but they are an integral part of the model. .\nIt is not relevant to test whether the thresholds are equal to zero, so no p-values are provided for this test.\n\n\nVisualize Effects and Thresholds\n\ntidy(fm1, \n     conf.int = TRUE, \n     conf.type = \"Wald\") %&gt;%\n  ggplot(aes(y = term, x = estimate)) +\n  geom_point(size = 2) +\n  geom_linerange(size = 1, \n                 aes(xmin = conf.low, xmax = conf.high))\n\nEffect sizes are logits and thresholds are also on the log scale.\n\nExponentiate and Summarize\n\ngtsummary::tbl_regression(fm1, \n                          exponentiate = TRUE,\n                          intercept = TRUE)\n\nNot sure the PO model threshold CIs weren‚Äôt calculated. They were for the\n\nTest for interaction\nfm1_inter &lt;-\n  clm(rating ~ contact * temp, \n      data = wine, \n      link = \"logit\")\n\n#drop1(fm1_inter, test = \"Chisq\") accomplishes the same thing as anova\nanova(fm1, fm1_inter)\n#&gt; Likelihood ratio tests of cumulative link models:\n#&gt;  \n#&gt;           formula:                link: threshold:\n#&gt; fm1       rating ~ contact + temp logit flexible  \n#&gt; fm1_inter rating ~ contact * temp logit flexible  \n#&gt; \n#&gt;           no.par    AIC  logLik LR.stat df Pr(&gt;Chisq)\n#&gt; fm1            6 184.98 -86.492                      \n#&gt; fm1_inter      7 186.83 -86.416  0.1514  1     0.6972\n\nNo interaction which doesn‚Äôt totally surprise me given the EDA plot (See EDA &gt;&gt; Dot Plots), but I did think it‚Äôd be closer.\n\n\nExample 2: Weighted Complimentary Log-Log Link, Comparing Links\n\nData\n#&gt; ¬† year¬† pct income\n#&gt; 1¬† 1960¬† 6.5¬† ¬† 0-3\n#&gt; 2¬† 1960¬† 8.2¬† ¬† 3-5\n#&gt; 3¬† 1960 11.3¬† ¬† 5-7\n#&gt; 4¬† 1960 23.5¬† 7-10\n#&gt; 5¬† 1960 15.6¬† 10-12\n#&gt; 6¬† 1960 12.7¬† 12-15\n#&gt; 7¬† 1960 22.2¬† ¬† 15+\n#&gt; 8¬† 1970¬† 4.3¬† ¬† 0-3\n#&gt; 9¬† 1970¬† 6.0¬† ¬† 3-5\n#&gt; 10 1970¬† 7.7¬† ¬† 5-7\n#&gt; 11 1970 13.2¬† 7-10\n#&gt; 12 1970 10.5¬† 10-12\n#&gt; 13 1970 16.3¬† 12-15\n#&gt; 14 1970 42.1¬† ¬† 15+\n\nincome (ordered factor) are intervals in thousands of 1973 US dollars\npct (numeric) is percent of the population in that income bracket from that particular year\nyear (factor)\n\nComparing multiple links\n&gt; links &lt;- c(\"logit\", \"probit\", \"cloglog\", \"loglog\", \"cauchit\")\n&gt; sapply(links, \n         function(link) {\n            ordinal::clm(income ~ year, \n                         data = income, \n                         weights = pct, \n                         link = link)$logLik \n         })\n#&gt;  logit¬† ¬†  probit¬† ¬† cloglog¬†  loglog¬† ¬† cauchit\n#&gt; -353.3589 -353.8036 -352.8980 -355.6028 -352.8434\n\nThe cauchy distribution has the highest log-likelihood and is therefore the best fit to the data, but is closely followed by the complementary log-log link\n\nFit the cloglog for interpretation convenience\nmod &lt;- \n  clm(income ~ year, \n      data = income, \n      weights = pct, \n      link = \"cloglog\")\nsummary(mod)\n\n#&gt; link¬† ¬† threshold nobs¬† logLik¬† AIC¬† ¬† niter max.grad cond.H¬†\n#&gt; cloglog flexible¬† 200.1 -352.90 719.80 6(0)¬† 1.87e-11 7.8e+01\n#&gt; \n#&gt; ¬† ¬† ¬† ¬†   Estimate  Std. Error z value Pr(&gt;|z|)¬† ¬†\n#&gt; year1970¬† 0.5679¬† ¬† 0.1749¬†    3.247¬†  0.00116 **\n#&gt; \n#&gt; Threshold coefficients:\n#&gt; ¬† ¬† ¬† ¬† ¬† ¬† Estimate   Std. Error z value\n#&gt; 0-3|3-5¬† ¬†  -2.645724¬† 0.310948¬†  -8.509\n#&gt; 3-5|5-7¬† ¬†  -1.765970¬† 0.210267¬†  -8.399\n#&gt; 5-7|7-10¬† ¬† -1.141808¬† 0.164710¬†  -6.932\n#&gt; 7-10|10-12¬† -0.398434¬† 0.132125¬†  -3.016\n#&gt; 10-12|12-15¬† 0.004931¬† 0.123384¬†   0.040\n#&gt; 12-15|15+¬† ¬† 0.418985¬† 0.120193¬†   3.486\n\n** The uncertainty of parameter estimates depends on the sample size, which is unknown here, so hypothesis tests should not be considered **\nInterpretation\n\n‚ÄúIf \\(p_{1960}(x)\\) is proportion of the population with an income larger than \\(x\\) in 1960 and \\(p_{1970}(x)\\) is the equivalent in 1970, then approximately‚Äù\n\\[\n\\begin{aligned}\n\\log p_{1960} (x) &= e^{\\hat\\beta} \\log p_{1970}(x) \\\\\n                  &= e^{0.568} \\log p_{1970}(x)\n\\end{aligned}\n\\]\nSo for any income dollar amount, the proportion of the population in 1960 (or 1970 w/some algebra) with that income or greater can be estimated.\n\n\n\nExample 3: {MASS::polr} (article)\nm &lt;- \n  polr(apply ~ pared + public + gpa, \n       data = dat, \n       Hess = TRUE)\nsummary(m)\n\n## Coefficients:\n##¬† ¬† ¬† ¬† ¬† Value       Std. Error t value\n## pared¬†   1.0477¬† ¬† ¬† 0.266¬†     3.942\n## public  -0.0588¬† ¬† ¬† 0.298¬†    -0.197\n## gpa¬† ¬†   0.6159¬† ¬† ¬† 0.261¬†     2.363\n##¬†\n## Intercepts:\n##¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬†   Value¬† Std. Error t value\n## unlikely|somewhat likely¬† ¬†  2.204¬† 0.780¬† ¬† ¬† 2.827¬†\n## somewhat likely|very likely¬† 4.299¬† 0.804¬† ¬† ¬† 5.345¬†\n##¬†\n## Residual Deviance: 717.02¬†\n## AIC: 727.02\n\nResponse: apply to grad school with levels: unlikely, somewhat likely, and very likely (coded 1, 2, and 3)\n\nThe researchers believe there is a greater distance between somewhat likely and very likely than somewhat likely and unlikely.\n\nSee Cumulative Link Models (CLM) &gt;&gt; Structured Thresholds\n\n\nExplanatory:\n\npared - Parent‚Äôs Education (0/1), Graduate School or Not\npublic - Public School or Not (i.e.¬†Private) (0/1)\ngpa (numeric)\n\nModel\n\n\\[\n\\begin{aligned}\n\\mbox{logit}(\\hat P(Y\\le 1)) &= (2.20-1.05) \\times (\\mbox{pared}-(-0.06)) \\times (\\mbox{public}-0.616) \\times \\mbox{gpa} \\\\\n\\mbox{logit}(\\hat P(Y\\le 2)) &= (4.30-1.05) \\times (\\mbox{pared}-(-0.06)) \\times (\\mbox{public}-0.616) \\times \\mbox{gpa}\n\\end{aligned}\n\\]\n\nEquation shows the thresholds (aka cutpoints) as intercepts and the minus sign propugated through the rest of the regression equation\n\nAdd p-values to the summary table\n## store table\n(ctable &lt;- coef(summary(m)))\n## calculate and store p values\np &lt;- pnorm(abs(ctable[, \"t value\"]), lower.tail = FALSE) * 2\n## combined table\n(ctable &lt;- cbind(ctable, \"p value\" = p))\nCIs:\n\nProfile CIs: confint(m)\n\nAlso see Confidence & Prediction Intervals &gt;&gt; Misc\n\nAssuming normality: confint.default(m)\n\nInterpretation:\n\nIn Ordered Log Odds (aka Ordered Logits)\n\npared: For a one unit increase in pared (i.e., going from 0 to 1), we expect a \\(1.05\\) increase in the expected value of apply on the log odds scale, given all of the other variables in the model are held constant.\ngpa: For one unit increase in gpa, we would expect a \\(0.62\\) increase in the expected value of apply in the log odds scale, given that all of the other variables in the model are held constant\n\nIn Odds Ratios (aka Proportional Odds Ratios): (w/CIs exp(cbind(OR = coef(m), ci = confint(m))\n\nThis is a Proportional Odds Model so the effect size is the same at all levels of the response variable.\npared:\n\nFor students whose parents did have a graduate degree, the odds of being more likely (i.e., very or somewhat likely versus unlikely) to apply is \\(2.85\\) times that of students whose parents did NOT have a graduate degree, holding constant all other variables.\nFor students whose parents did have a graduate degrees, the odds of being less likely to apply (i.e., unlikely versus somewhat or very likely) is \\(2.85\\) times that of students whose parents did NOT have a graduate degree, holding constant all other variables.\n\npublic\n\nFor students in public school, the odds of being more likely (i.e., very or somewhat likely versus unlikely) to apply is \\(5.71\\%\\) lower [i.e., (\\(1 -0.943) \\times 100\\%\\)] than private school students, holding constant all other variables.\n\nFor students in private school, the odds of being more likely to apply is \\(1.06\\) times [i.e., \\(\\frac{1}{0.943}\\)] that of public school students, holding constant all other variables (positive odds ratio).\n\nFor students in private school, the odds of being less likely to apply (i.e., unlikely versus somewhat or very likely) is \\(5.71\\%\\) lower than public school students, holding constant all other variables.\n\nFor students in public school, the odds of being less likely to apply is \\(1.06\\) times that of private school students, holding constant all other variables (positive odds ratio).\n\n\ngpa\n\nFor every one unit increase in student‚Äôs GPA the odds of being more likely to apply (very or somewhat likely versus unlikely) is multiplied \\(1.85\\) times (i.e., increases \\(85\\%\\)), holding constant all other variables.\nFor every one unit decrease in student‚Äôs GPA the odds of being less likely to apply (unlikely versus somewhat or very likely) is multiplied \\(1.85\\) times, holding constant all other variables.\n\n\nPredictions: predict(m, data = newdat, type = \"probs\")\nPlot Predictions\n\nnewdat &lt;- cbind(newdat, \n                predict(m, \n                        newdat, \n                        type = \"probs\"))\nlnewdat &lt;- melt(newdat, \n                id.vars = c(\"pared\", \"public\", \"gpa\"),\n                variable.name = \"Level\", \n                value.name=\"Probability\")\nggplot(lnewdat, \n       aes(x = gpa, y = Probability, colour = Level)) +\n  geom_line() + \n  facet_grid(pared ~ public, \n             labeller=\"label_both\")",
    "crumbs": [
      "Regression",
      "Ordinal"
    ]
  },
  {
    "objectID": "qmd/regression-ordinal.html#sec-reg-ord-ppo",
    "href": "qmd/regression-ordinal.html#sec-reg-ord-ppo",
    "title": "Ordinal",
    "section": "Partial Proportional Odds (PPO)",
    "text": "Partial Proportional Odds (PPO)\n\n\\(\\beta\\)s are allowed to vary with \\(j\\) (ordinal outcome levels) or equivalently, the threshold parameters \\(\\{\\theta_j\\}\\) are allowed to depend on regression variables.\n\ni.e In PO, it was assumed that the independent variable effect is the same for all levels of the ordinal outcome variable. The PPO model allows that effect to vary according to the outcome variable level.\n\nTwo sets of thresholds are applied at conditions with and without an explanatory variable\nSee Cumulative Link Models (CLM) &gt;&gt; Model for how the PPO term fits into the model equation (e.g.¬†\\(\\tilde \\beta\\))\nExample 1: {ordinal}\nfm.nom &lt;- \n  clm(rating ~ temp, \n      nominal = ~ contact, \n      data = wine)\nsummary(fm.nom)\n\n#&gt; link threshold nobs logLik¬† AIC¬† ¬† niter¬† max.grad¬† cond.H\n#&gt; logit flexible 72¬† -86.21¬† 190.42¬† 6(0)¬† 1.64e-10¬† 4.8e+01\n#&gt; \n#&gt; ¬† ¬† ¬† ¬† Estimate¬† Std.Error¬† z.value¬† Pr(&gt;|z|)\n#&gt; tempwarm 2.519¬† ¬† 0.535¬† ¬† ¬† 4.708¬† ¬† 2.5e-06 ***\n#&gt; \n#&gt; Threshold coefficients:\n#&gt; ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† Estimate Std.Error¬† z.value\n#&gt; 1|2.(Intercept) -1.3230¬† 0.5623¬† ¬† -2.353\n#&gt; 2|3.(Intercept)¬† 1.2464¬† 0.4748¬† ¬† 2.625\n#&gt; 3|4.(Intercept)¬† 3.5500¬† 0.6560¬† ¬† 5.411\n#&gt; 4|5.(Intercept)¬† 4.6602¬† 0.8604¬† ¬† 5.416\n#&gt; 1|2.contactyes¬† -1.6151¬† 1.1618¬† ¬† -1.390\n#&gt; 2|3.contactyes¬† -1.5116¬† 0.5906¬† ¬† -2.559\n#&gt; 3|4.contactyes¬† -1.6748¬† 0.6488¬† ¬† -2.581\n#&gt; 4|5.contactyes¬† -1.0506¬† 0.8965¬† ¬† -1.172\n\nSee Proportional Odds &gt;&gt; Example 1 for variable descriptions\nPPO is for contact\nModel: \\(\\mbox{logit}(P(Y_i \\le j)) = \\theta_j + \\tilde{\\beta}_j (\\mbox{contact}_i) ‚àí \\beta(\\mbox{temp}_i)\\)\nTwo sets of thresholds are applied at conditions with (yes) and without (no) contact\nResponse variable should be an ordered factor class\n\nIn this example, both explanatory variables were factor variables.\n\nIt is not possible to estimate both \\(\\beta_2(\\mbox{contact}_i)\\) and \\(\\tilde{\\beta}_j(\\mbox{contact}_i)\\) in the same model. Consequently variables that appear in nominal cannot enter in the formula as well\nResults\n\n\\(\\hat \\beta(\\mbox{warm} ‚àí \\mbox{cold}) = 2.52\\)\n\nThe reference level is cold, and this is the effect of moving from cold to warm\n\n\\(\\{\\hat \\theta_j\\} = \\{‚àí1.32, 1.25, 3.55, 4.66\\}\\)\n\\({\\hat{\\tilde \\beta_j} (\\mbox{yes} ‚àí \\mbox{no})} = \\{‚àí1.62, ‚àí 1.51, ‚àí 1.67, ‚àí 1.05\\}\\)\n\nThe odds ratio of bitterness being rated in category \\(j\\) or above (\\(\\mbox{OR}(Y \\ge j)\\)) now vary with \\(j\\):\n\n\\(\\mbox{ORs} = e^{-\\hat{\\tilde \\beta}_j (\\mbox{yes}‚àí \\mbox{no})} = e^{\\{-(-1.6151), -(-1.5116), -(-1.6748), -(-1.0506)\\}} = \\{5.03, 4.53, 5.34, 2.86\\}\\)\nMe: The odds of the rating being greater or equal to \\(2\\) are \\(4.53\\) times greater when contact = yes than when contact = no.\n\n\nExample 2: Test the PO assumption for 1 variable w/LR Test\nanova(fm1, fm.nom)\n\n#&gt; ¬† ¬† ¬†  formula:¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬†  nominal: link: threshold:\n#&gt; fm1¬† ¬† rating ~ temp + contact¬† ~1¬† ¬† ¬†  logit flexible\n#&gt; fm.nom rating ~ temp¬† ¬† ¬† ¬† ¬† ¬† ~contact logit flexible\n#&gt; \n#&gt; ¬† ¬† ¬†  no.par AIC¬† ¬†  logLik¬† LR.stat df Pr(&gt;Chisq)\n#&gt; fm1¬† ¬† 6¬† ¬† ¬† 184.98 -86.492\n#&gt; fm.nom 9¬† ¬† ¬† 190.42 -86.209¬† 0.5667¬† 3¬† 0.904\n\nfm1 model from Proportional Odds &gt;&gt; Example 1, but the formula column in the summary tells you what variables are in it.\nThere is only little difference in the log-likelihoods of the two models and the test is insignificant (pval &lt; 0.05). Thus, there is no evidence that the proportional odds assumption is violated for contact.\n\nExample 3: Test the PO assumption for all variables w/LR Test\nnominal_test(fm1)\n\n#&gt; ¬† ¬† ¬† ¬† Df¬† logLik¬† AIC¬† ¬†  LRT¬† ¬†  Pr(&gt;Chi)\n#&gt; &lt;none&gt;¬† ¬†  -86.492¬† 184.98\n#&gt; temp¬† ¬† 3¬† -84.904¬† 187.81¬† 3.1750¬† 0.3654\n#&gt; contact 3¬† -86.209¬† 190.42¬† 0.5667¬† 0.9040\n\nBehavior of this function is similar to the anova example above except the action is taken with every variable in the formula\n\nIf any variables are in the scale argument, they are also tested.\n\nNote that contact has the same LRT pval as the previous example\nNo pvals &lt; 0.05, therefore no violations of the PO assumption for any variable.",
    "crumbs": [
      "Regression",
      "Ordinal"
    ]
  },
  {
    "objectID": "qmd/regression-ordinal.html#sec-reg-ord-se",
    "href": "qmd/regression-ordinal.html#sec-reg-ord-se",
    "title": "Ordinal",
    "section": "Scale Effects",
    "text": "Scale Effects\n\nAn alternative to nominal effects (partial proportional odds) when nonproportional odds structures are encountered in the data.\n\nUsing scale effects is often a better approach, and it uses fewer parameters which often lead to more sensitive tests than nominal effects\n\nSee Cumulative Link Models (CLM) &gt;&gt; Model for how scale effects fit into the model equation\nSomething about allowing the scale parameter of the latent variable distribution to depend on explanatory variables.\n\nLatent distribution is determined by the choice of link function\n\nExample 1: {ordinal}, Allow scale to vary according to a variable\nfm.sca &lt;- \n  clm(rating ~ temp + contact, \n      scale = ~ temp, \n      data = wine)\nsummary(fm.sca)\n\n#&gt; link threshold nobs logLik AIC niter max.grad cond.H\n#&gt; logit flexible 72 -86.44 186.88 8(0) 5.25e-09 1.0e+02\n#&gt; \n#&gt; ¬† ¬† ¬† ¬† ¬†  Estimate¬† Std.Error z.value¬† Pr(&gt;|z|)\n#&gt; tempwarm¬†  2.6294¬† ¬† 0.6860¬† ¬† 3.833¬† ¬† 0.000127 ***\n#&gt; contactyes 1.5878¬† ¬† 0.5301¬† ¬† 2.995¬† ¬† 0.002743 **\n#&gt; \n#&gt; log-scale coefficients:\n#&gt; ¬† ¬† ¬† ¬†  Estimate¬† Std.Error z.value¬† Pr(&gt;|z|)\n#&gt; tempwarm 0.09536¬†  0.29414¬†  0.324¬† ¬† 0.746\n#&gt; \n#&gt; Threshold coefficients:\n#&gt; ¬† ¬†  Estimate Std.Error¬† z value\n#&gt; 1|2 -1.3520¬†  0.5223¬† ¬† -2.588\n#&gt; 2|3¬† 1.2730¬†  0.4533¬† ¬†  2.808\n#&gt; 3|4¬† 3.6170¬†  0.7774¬† ¬†  4.653\n#&gt; 4|5¬† 5.2982¬†  1.2027¬† ¬†  4.405\n\nModel\n\\[\n\\begin{aligned}\n&\\mbox{logit}(P(Y_i \\le j)) = \\frac{\\theta_J - \\beta_1(\\mbox{temp}_i) - \\beta_2(\\mbox{contact}_i)}{e^{\\zeta(\\mbox{temp}_i)}} \\\\\n&\\text{where} \\quad i = 1, \\ldots, n \\; \\text{and} \\; j=1, \\ldots, J-1\n\\end{aligned}\n\\]\n\nScale parameter of the latent variable distribution varies by temp\n\nThe scale term in the denominator. See Cumulative Link Models (CLM) &gt;&gt; Model for more details on this term\n\nLocation parameter of the latent distribution is allowed to depend on both temp and contact\n\nThe location expression is the numerator\n\n\nResponse variable should be an ordered factor class\n\nIn this example, both explanatory variables were factor variables.\n\nResults\n\nLittle to no idea what any of this practically means\nThe location of the latent distribution is shifted \\(2.63\\sigma\\) (scale units) when temperature is warm as compared to cold conditions and \\(1.59\\sigma\\) when there‚Äôs presence (yes) of contact as compared to absence (no) of contact. (Reference levels: temp = cold, contact = no)\nThe scale of the latent distribution is \\(\\sigma\\) at cold conditions but \\(\\sigma \\times e^{\\zeta(warm‚àícold)} = \\sigma \\times e^{0.095} = 1.10\\sigma\\) , i.e., \\(10\\%\\) higher, at warm conditions.\nThe p value for the scale effect in the summary output shows that the ratio of scales is not significantly different from 1 (or equivalently that the difference on the log-scale is not different from 0).\n\n\nExample: Detect scale effects of variables\nscale_test(fm1)\n\n#&gt; ¬† ¬† ¬† ¬† Df  logLik¬† AIC¬† ¬† LRT¬† ¬†  Pr(&gt;Chi)\n#&gt; &lt;none&gt;¬† ¬†  -86.492¬† 184.98\n#&gt; temp¬† ¬† 1¬† -86.439¬† 186.88 0.10492 0.7460\n#&gt; contact 1¬† -86.355¬† 186.71 0.27330 0.6011\n\nfm1 comes from Proportional Odds (PO) &gt;&gt; Example 1\nFunction has similar behavior to nominal_test (see Partial Proportional Odds (PPO) &gt;&gt; Example 3)\nNo pvals &lt; 0.05, therefore no evidence of scale effects for any variable.",
    "crumbs": [
      "Regression",
      "Ordinal"
    ]
  },
  {
    "objectID": "qmd/regression-ordinal.html#sec-reg-ord-gor",
    "href": "qmd/regression-ordinal.html#sec-reg-ord-gor",
    "title": "Ordinal",
    "section": "Generalized Ordinal Regression",
    "text": "Generalized Ordinal Regression\n\nTodo",
    "crumbs": [
      "Regression",
      "Ordinal"
    ]
  },
  {
    "objectID": "qmd/latex.html",
    "href": "qmd/latex.html",
    "title": "LaTeX",
    "section": "",
    "text": "R = \n\\begin{pmatrix}\n1 & r_0r & r_0r^2 & \\cdots & r_0r^{T-1} \\\\\nr_0r & 1 & r_0r & \\cdots & r_0r^{T-2} \\\\\nr_0r^2 & r_0r & 1 & \\cdots & r_0r^{T-3} \\\\ \n\\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\ \nr_0r^{T-1} & r_0r^{T-2} & r_0r^{T-3} & \\cdots & 1 \n\\end{pmatrix}\n\\[\nR =\n\\begin{pmatrix}\n1 & r_0r & r_0r^2 & \\cdots & r_0r^{T-1} \\\\\nr_0r & 1 & r_0r & \\cdots & r_0r^{T-2} \\\\\nr_0r^2 & r_0r & 1 & \\cdots & r_0r^{T-3} \\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\nr_0r^{T-1} & r_0r^{T-2} & r_0r^{T-3} & \\cdots & 1\n\\end{pmatrix}\n\\]\n\n\n\n\ns(\\\\boldsymbol{\\\\hat{\\\\theta}}, y) = \n\\begin{pmatrix} \ns(\\hat{\\theta}, y_1)_1 & s(\\hat{\\theta}, y_1)_2 & \\cdots & s(\\hat{\\theta}, y_1)_k \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\ns(\\hat{\\theta}, y_n)_1 & s(\\hat{\\theta}, y_n)_2 & \\cdots & s(\\hat{\\theta}, y_n)_k \\\\\n\\end{pmatrix}\n\\[\ns(\\boldsymbol{\\hat{\\theta}}, y) =\n\\begin{pmatrix}\ns(\\hat{\\theta}, y_1)_1 & s(\\hat{\\theta}, y_1)_2 & \\cdots & s(\\hat{\\theta}, y_1)_k \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\ns(\\hat{\\theta}, y_n)_1 & s(\\hat{\\theta}, y_n)_2 & \\cdots & s(\\hat{\\theta}, y_n)_k \\\\\n\\end{pmatrix}\n\\]\n\n\n\n\n[1 \\; x_{1t} \\; x_{2t} \\; \\cdots \\; x_{mt}] \n\\cdot \\left( \\begin{array}{ccc} \n\\hat{\\beta}_{01} & \\cdots & \\hat{\\beta}_{0k} \\\\ \n\\vdots & \\ddots & \\vdots \\\\ \n\\hat{\\beta}_{m1} & \\cdots & \\hat{\\beta}_{mk} \\end{array} \\right) \n= \n[\\hat{y}_{t1} \\; \\hat{y}_{t2} \\; \\cdots \\; \\hat{y}_{tk}]\n\\[\n[1 \\; x_{1t} \\; x_{2t} \\; \\cdots \\; x_{mt}]\n\\cdot \\left( \\begin{array}{ccc}\n\\hat{\\beta}_{01} & \\cdots & \\hat{\\beta}_{0k} \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\hat{\\beta}_{m1} & \\cdots & \\hat{\\beta}_{mk} \\end{array} \\right)\n=\n[\\hat{y}_{t1} \\; \\hat{y}_{t2} \\; \\cdots \\; \\hat{y}_{tk}]\n\\]\n\n\n\n\nICC_{tt} = ICC_{tt'} = \\frac {\\sigma_b^2} {\\sigma_b^2 + \\sigma_e^2} r_{tt'}\n\\[\nICC_{tt} = ICC_{tt'} = \\frac {\\sigma_b^2} {\\sigma_b^2 + \\sigma_e^2} r_{tt'}\n\\]\n\n\n\n\ny_{ict} = \\mu + \\beta_0t + \\beta_1X_{ct} + b_{ct} + e_{ict}\n\\[\ny_{ict} = \\mu + \\beta_0t + \\beta_1X_{ct} + b_{ct} + e_{ict}\n\\]\n\n\n\n\n\\hat{\\theta} = \\arg\\max_{\\theta \\in \\Theta} \\sum\\_{i=1}^n w_i^{\\text{forest}} \\\\cdot l(\\theta; y_i)\n\\[\n\\hat{\\theta} = \\arg\\max_{\\theta \\in \\Theta} \\sum_{i=1}^n w_i^{\\text{forest}} \\cdot l(\\theta; y_i)\n\\]\n\n\n\n\n\\begin{align*}\nH(Y|X_1,\\cdots, X_k) = \\sum_{d = 0,1} \\sum_{{i_1}=1}^c \\cdots \\sum_{{i_k}=1}^c \\\\\nP(y_d | x_{i_1}, \\cdots, x_{i_k}) \\log P(y_d | x_{i_1}, \\cdots, x_{i_k})\n\\end{align*}\n\\[\n\\begin{align*}\nH(Y|X_1,\\cdots, X_k) = \\sum_{d = 0,1} \\sum_{{i_1}=1}^c \\cdots \\sum_{{i_k}=1}^c \\\\\nP(y_d | x_{i_1}, \\cdots, x_{i_k}) \\log P(y_d | x_{i_1}, \\cdots, x_{i_k})\n\\end{align*}\n\\]\n\n\n\n\n\\begin{align*}\nx^2 + y^2 &= 1 \\\\\ny &= \\sqrt{1 - x^2}\n\\end{align*}\n\nWith the ‚Äú&‚Äù symbols, the 2nd line stays lined up with the end of the first line\n\n\\[\n\\begin{align*}\nx^2 + y^2 &= 1 \\\\\ny &= \\sqrt{1 - x^2}\n\\end{align*}\n\\]\n\n\n\n\n\\begin{tabular} {lll}\nTransformation & Function & Elasticity \\\\\n\\hline\nLevel Level & Y\\;=\\;a+bX & \\epsilon = b \\cdot \\frac {X} {Y} \\\\  \nLog Level & \\log Y = a+bx & \\epsilon = b \\cdot X \\\\\nLevel-Log & Y = a + b \\cdot \\log X & \\epsilon = \\frac {b} {Y} \\\\\nLog-Log & \\log Y = a + b \\cdot \\log X & \\epsilon = b \\\\\n\\hline\n\\end{tabular}\n\nThis is ‚ÄúRaw LaTeX‚Äù and will be ignored when the output is HTML. (See Docs)\nIn the last column, for some unknown reason, using \\frac without another ‚Äú\\‚Äù symbol before it throws an error. Adding \\epsilon = to the expression made it work fine.\n\n\n\n\n\n\nc_{max}(t,\\mu, \\sigma) = \\max \\limits_{k = 1,..., dim(T)} \\left | \\frac {(t-\\mu)_k} {\\sqrt {\\Sigma_{kk}}} \\right |\n\nLarge absolute pipes require \\left, \\right\nThe text underneath ‚Äúmax‚Äù requires \\limits\n\n\\[\nc_{max}(t,\\mu, \\sigma) = \\max \\limits_{k = 1,..., dim(T)} \\left | \\frac {(t-\\mu)_k} {\\sqrt {\\Sigma_{kk}}} \\right |\n\\]\n\n\n\n\n\\begin{align*}\n\\mbox{net present value (npv)} = &\\sum \\limits_{i = 0}^m benefit_i*(1-discount)^i -\\\\\n&\\sum \\limits_{i=0}^m cost_i*(1-discount)^i\n\\end{align*}\n\\[\n\\begin{align*}\n\\mbox{net present value (npv)} = &\\sum \\limits_{i = 0}^m benefit_i*(1-discount)^i -\\\\\n&\\sum \\limits_{i=0}^m cost_i*(1-discount)^i\n\\end{align*}\n\\]\n\n\n\n\n\\epsilon_Z \\stackrel{\\text{iid}}{\\sim} \\mathcal{N}(0, 0.1)\n\\[\n\\epsilon_Z \\stackrel{\\text{iid}}{\\sim} \\mathcal{N}(0, 0.1)\n\\]\n\n\n\n\n\\mbox{posterior} = \\frac {\\mbox{Prob of observed variables} \\;\\times\\; \\mbox{Prior}} {\\mbox{Normalizing constant}}\n\\[\n\\mbox{posterior} = \\frac {\\mbox{Prob of observed variables} \\;\\times\\; \\mbox{Prior}} {\\mbox{Normalizing constant}}\n\\]\n\n\n\n\n\\lim\\limits_{x \\to +\\infty} \\sup \\frac{\\bar{F}(x)}{\\bar{F}_{\\exp}(x)} = \\frac{\\bar{F}(x)}{e^{-\\lambda x}}, \\;\\; \\forall \\lambda &gt; 0\n\\[\n\\lim\\limits_{x \\to +\\infty} \\sup \\frac{\\bar{F}(x)}{\\bar{F}_{\\exp}(x)} = \\frac{\\bar{F}(x)}{e^{-\\lambda x}}, \\;\\; \\forall \\lambda &gt; 0\n\\]\n\n\n\n\n\\text{vec} = \\left( \\begin{array}{cc} \\text{rand var1} \\\\\n \\text {rand var2} \\end{array} \\right)\n\\[\n\\text{vec} = \\left( \\begin{array}{cc} \\text{rand var1} \\\\\n\\text {rand var2} \\end{array} \\right)\n\\]\n\n\n\n\nSBD(x,y) = 1 - \\frac {\\max (\\text{NCCc}(x,y))} {\\left\\lVert x \\right\\rVert_2 \\; \\left\\lVert y \\right\\rVert_2}\n\\[\nSBD(x,y) = 1 - \\frac {\\max (\\text{NCCc}(x,y))} {\\left\\lVert x \\right\\rVert_2 \\; \\left\\lVert y \\right\\rVert_2}\n\\]\n\n\n\n\n\\begin{align}\n&D \\not\\!\\perp\\!\\!\\!\\perp A \\\\\n&D \\!\\perp\\!\\!\\!\\perp A \\\\\n&Y \\!\\perp\\!\\!\\!\\perp X|Z\n&Y \\perp\\mkern-10mu\\perp X\\;|\\;Z\n\\end{align}\n\\[\n\\begin{align}\n&D \\not\\!\\perp\\!\\!\\!\\perp A \\\\\n&D \\!\\perp\\!\\!\\!\\perp A \\\\\n&Y \\!\\perp\\!\\!\\!\\perp X|Z \\\\\n&Y \\perp\\mkern-10mu\\perp X\\;|\\;Z\n\\end{align}\n\\]\n\n\n\n\n||y-f||^2 + \\lambda \\int \\left(\\frac {\\partial^2 f(\\text{log[baseline profit]})}{\\partial \\; \\text{log[baseline profit]}^2}\\right)^2 \\partial x\n\\[\n||y-f||^2 + \\lambda \\int \\left(\\frac {\\partial^2 f(\\text{log[baseline profit]})}{\\partial \\; \\text{log[baseline profit]}^2}\\right)^2 \\partial x\n\\]\n\n\n\n\nE_{iz_{nm}} = \n\\left\\{ \\begin{array}{lcl}\n-\\beta_z z_{zm}P_{nm} \\left(1+ \\frac{1-\\lambda_k}{\\lambda_k} \\frac {1}{P_{nB_k}} \\right) & \\mbox{if} \\; m \\in \\beta_k \\\\\n-\\beta_z z_{zm}P_{nm} & \\mbox{if} \\; m \\in \\beta_k \n\\end{array}\\right.\n\nThere is no dash between {array} and {lcl}. It‚Äôs just two curly brackets touching each other\n\nIf you want the expressions on the left side to right-align use {rcl} ‚Äî for center-align, leave it blank I think\n\nDo not forget that period at end ‚Äî to the right of \\right\n\n\\[\nE_{iz_{nm}} =\n\\left\\{ \\begin{array}{lcl}\n-\\beta_z z_{zm}P_{nm} \\left(1+ \\frac{1-\\lambda_k}{\\lambda_k} \\frac {1}{P_{nB_k}} \\right) & \\mbox{if} \\; m \\in \\beta_k \\\\\n-\\beta_z z_{zm}P_{nm} & \\mbox{if} \\; m \\notin \\beta_k\n\\end{array}\\right.\n\\]\n\n\n\n\n\\begin{align}\n&P_{ni} = \\int L_{ni}(\\beta) \\; f(\\beta\\;|\\;\\boldsymbol{\\theta}) d\\beta\\\\\n&\\mbox{where} \\; L_{ni}(\\beta) = \\frac {e^{V_{ni}(\\beta)}}{\\sum_{j=1}^J e^{V_{ni}(\\beta)}} \n\\end{align}\n\nAlignment occurs where the & symbols are positioned\n\\boldsymbol is used bold the vector \\(\\theta\\)\n\n\\[\n\\begin{align}\n&P_{ni} = \\int L_{ni}(\\beta) \\; f(\\beta\\;|\\;\\boldsymbol{\\theta}) \\; d\\beta\\\\\n&\\mbox{where} \\; L_{ni}(\\beta) = \\frac {e^{V_{ni}(\\beta)}}{\\sum_{j=1}^J e^{V_{ni}(\\beta)}}\n\\end{align}\n\\]\n\n\n\n\n\\pi_i = \\mbox{Pr}(i \\in S) = \\sum \\limits_{i \\in s \\in S} \\mbox{Pr}(S = s) = \\frac {\\binom{N-1}{n-1}}{\\binom{N}{n}} = \\frac {n}{N}\n\n\\binom used for binomial coefficient/combination notation\n\n\\[\n\\pi_i = \\mbox{Pr}(i \\in S) = \\sum \\limits_{i \\in s \\in S} \\mbox{Pr}(S = s) = \\frac {\\binom{N-1}{n-1}}{\\binom{N}{n}} = \\frac {n}{N}\n\\]\n\n\n\n\n\\begin{aligned}\n&R_{i,j} = \\frac{S_i + S_j}{M_{i,j}}\\\\\n&\\begin{aligned}\n\\text{where} \\;\\; &S_i = \\left(\\frac{1}{T_i} \\sum_{j=1}^{T_i} \\lVert X_j - A_i \\rVert_{p}^q \\right)^{\\frac{1}{q}} \\;\\; \\text{and} \\\\\n&M_{i,j} = \\lVert A_i - A_j \\rVert_p = \\left(\\sum_{k=1}^n |a_{k,i} - a_{k,j}|^p\\right)^{\\frac{1}{p}}\n\\end{aligned}\n\\end{aligned}\n\nThe key for this nested align is the ‚Äú&‚Äù before the second \\begin{aligned}. Otherwise ‚Äúwhere‚Äù and \\(R_{i,j}\\) won‚Äôt be aligned flushly on the left side.\n\n\\[\n\\begin{aligned}\n&R_{i,j} = \\frac{S_i + S_j}{M_{i,j}}\\\\\n&\\begin{aligned}\n\\text{where} \\;\\; &S_i = \\left(\\frac{1}{T_i} \\sum_{j=1}^{T_i} \\lVert X_j - A_i \\rVert_{p}^q \\right)^{\\frac{1}{q}} \\;\\; \\text{and} \\\\\n&M_{i,j} = \\lVert A_i - A_j \\rVert_p = \\left(\\sum_{k=1}^n |a_{k,i} - a_{k,j}|^p\\right)^{\\frac{1}{p}}\n\\end{aligned}\n\\end{aligned}\n\\]\n\n\n\n\n\\begin{aligned}\n&Y_{ij} = [\\alpha_0 + \\beta_0 \\text{large}_{ij}] + [u_i + v_i \\text{large}_{ij} + \\epsilon_{ij}] \\\\\n&\\begin{aligned}\n\\text{where}\\quad \\epsilon &\\sim \\mathcal{N}(0, \\sigma^2) \\: \\text{and}\\\\\n\\left[ \\begin{array}{cc} u_i \\\\ v_i \\end{array} \\right] &\\sim \\mathcal{N} \\left(\\left[\\begin{array}{cc} 0\\\\0 \\end{array}\\right], \\left[\\begin{array}{cc} \\sigma^2_u\\\\\\rho\\sigma_u \\sigma_v \\quad \\sigma^2_v \\end{array}\\right]\\right)\n\\end{aligned}\n\\end{aligned}\n\\[\n\\begin{aligned}\n&Y_{ij} = [\\alpha_0 + \\beta_0 \\text{large}_{ij}] + [u_i + v_i \\text{large}_{ij} + \\epsilon_{ij}] \\\\\n&\\begin{aligned}\n\\text{where}\\quad \\epsilon &\\sim \\mathcal{N}(0, \\sigma^2) \\: \\text{and}\\\\\n\\left[ \\begin{array}{cc} u_i \\\\ v_i \\end{array} \\right] &\\sim \\mathcal{N} \\left(\\left[\\begin{array}{cc} 0\\\\0 \\end{array}\\right], \\left[\\begin{array}{cc} \\sigma^2_u\\\\\\rho\\sigma_u \\sigma_v \\quad \\sigma^2_v \\end{array}\\right]\\right)\n\\end{aligned}\n\\end{aligned}\n\\]\n\n\n\n\n\\left[\\begin{array}{cc} u_i\\\\v_i\\\\w_i\\\\x_i\\\\y_i\\\\z_i \\end{array} \\right] \n\\sim \\mathcal{N} \\left(\n\\left[\\begin{array}{cc} 0\\\\0\\\\0\\\\0\\\\0\\\\0\\end{array} \\right],\n\\begin{bmatrix}\n\\sigma_u^2 \\\\\n\\sigma_{uv} & \\sigma_v^2 \\\\\n\\sigma_{uw} & \\sigma_{vw} & \\sigma^2_w \\\\\n\\sigma_{ux} & \\sigma_{vx} & \\sigma_{wx} & \\sigma_x^2 \\\\\n\\sigma_{uy} & \\sigma_{vy} & \\sigma_{wy} & \\sigma_{xy} & \\sigma_{y}^2 \\\\\n\\sigma_{uz} & \\sigma_{vz} & \\sigma_{wz} & \\sigma_{xz} & \\sigma_{yz} & \\sigma_z^2\n\\end{bmatrix}\n\\right)  \n\nbmatrix stands for bracket matrix. Previous matrices (above) used pmatrix which stands for parentheses matrix.\nOthers:\n\nBmatrix for curly braces matrix\nvmatrix for a matrix with ‚Äúrectangular line boundary‚Äù\nVmatrix for a matrix with double vertical bars\nmatrix for a matrix without brackets\n\n\n\\[\n\\left[\\begin{array}{cc} u_i\\\\v_i\\\\w_i\\\\x_i\\\\y_i\\\\z_i \\end{array} \\right]\n\\sim \\mathcal{N} \\left(\n\\left[\\begin{array}{cc} 0\\\\0\\\\0\\\\0\\\\0\\\\0\\end{array} \\right],\n\\begin{bmatrix}\n\\sigma_u^2 \\\\\n\\sigma_{uv} & \\sigma_v^2 \\\\\n\\sigma_{uw} & \\sigma_{vw} & \\sigma^2_w \\\\\n\\sigma_{ux} & \\sigma_{vx} & \\sigma_{wx} & \\sigma_x^2 \\\\\n\\sigma_{uy} & \\sigma_{vy} & \\sigma_{wy} & \\sigma_{xy} & \\sigma_{y}^2 \\\\\n\\sigma_{uz} & \\sigma_{vz} & \\sigma_{wz} & \\sigma_{xz} & \\sigma_{yz} & \\sigma_z^2\n\\end{bmatrix}\n\\right)  \n\\]\n\n\n\n\n\\begin{align}\n&\\ \\textbf{Registered provinces for INGO } i \\\\\n\\text{Count of provinces}\\ \\sim&\\ \\operatorname{Ordered\\,Beta}(\\mu_{i_j}, \\phi_y, k_{0_y}, k_{1_y}) \\\\[8pt]\n&\\textbf{Model of Outcome Average} \\\\\n\\mu_i = &\\\n  \\mathrlap{\\begin{aligned}[t]\n  &\\beta_0 + \\beta_1\\ \\text{Issue[Arts and Culture]} + \\beta_2\\ \\text{Issue[Education]}\\ + \\\\\n  &\\beta_3\\ \\text{Issue[Industry and Humanitarian]} + \\beta_4\\ \\text{Issue[Economy and Trade]}\n  \\end{aligned}}\\\\[8pt]\n&\\ \\textbf{Priors} \\\\\n\\beta_0\\ \\sim&\\ \\operatorname{Student\\,t}(\\nu = 3, \\mu = 0, \\sigma = 2.5) && \\text{Intercept} \\\\\n\\beta_{1..12}\\ \\sim&\\ \\mathcal{N}(0, 5) && \\text{Coefficients}\n\\end{align}\n\ntextbf is bold font text\n\\operatorname treats text as functions like \\max or \\sin in terms of spacing\n\\mathrlap + \\begin{aligned}allows you to wrap extra long equations\n\n\\[\n\\begin{align}\n&\\ \\textbf{Registered Provinces for INGO } i \\\\\n\\text{Count of Provinces}\\ \\sim&\\ \\operatorname{Ordered\\,Beta}(\\mu_{i_j}, \\phi_y, k_{0_y}, k_{1_y}) \\\\[8pt]\n&\\textbf{Model of Outcome Average} \\\\\n\\mu_i = &\\\n  \\mathrlap{\\begin{aligned}[t]\n  &\\beta_0 + \\beta_1\\ \\text{Issue[Arts and Culture]} + \\beta_2\\ \\text{Issue[Education]}\\ + \\\\\n  &\\beta_3\\ \\text{Issue[Industry and Humanitarian]} + \\beta_4\\ \\text{Issue[Economy and Trade]}\n  \\end{aligned}}\\\\[4pt]\n&\\ \\textbf{Priors} \\\\\n\\beta_0\\ \\sim&\\ \\operatorname{Student\\,t}(\\nu = 3, \\mu = 0, \\sigma = 2.5) && \\text{Intercept} \\\\\n\\beta_{1..12}\\ \\sim&\\ \\mathcal{N}(0, 5) && \\text{Coefficients}\n\\end{align}\n\\]",
    "crumbs": [
      "LaTeX"
    ]
  },
  {
    "objectID": "qmd/geospatial-general.html",
    "href": "qmd/geospatial-general.html",
    "title": "General",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Geospatial",
      "General"
    ]
  },
  {
    "objectID": "qmd/geospatial-general.html#sec-geo-gen-misc",
    "href": "qmd/geospatial-general.html#sec-geo-gen-misc",
    "title": "General",
    "section": "",
    "text": "Resources\n\nSpatial sampling with R\n\nQGIS - free and open source\nArcGIS - expensive and industry-standard\nSpatiotemporal Data ‚Äî data cubes with spatial and regular temporal dimensions ‚Äî such as\n\ne.g.¬†gridded temperature values (raster time series) and vector data with temporal records at regular temporal instances (e.g.¬†election results in states).\n\n{stars} - regular intervals\n{sftime} - irregular intervals\n\n\nSpatial Resampling\n\nCreates cross-validation folds by k-means clustering coordinate variables\nlibrary(tidymodels)\nlibrary(spatialsample)\nset.seed(123)\nspatial_splits &lt;- spatial_clustering_cv(landslides, coords = c(\"x\", \"y\"), v = 5)\n\n# fit a logistic model\nglm_spec &lt;- logistic_reg()\nlsl_form &lt;- lslpts ~ slope + cplan + cprof + elev + log10_carea¬†\nlsl_wf &lt;- workflow(lsl_form, glm_spec)\ndoParallel::registerDoParallel()¬†\nregular_rs &lt;- fit_resamples(lsl_wf, bad_folds)",
    "crumbs": [
      "Geospatial",
      "General"
    ]
  },
  {
    "objectID": "qmd/geospatial-general.html#sec-geo-gen-terms",
    "href": "qmd/geospatial-general.html#sec-geo-gen-terms",
    "title": "General",
    "section": "Terms",
    "text": "Terms\n\nCensus Block Groups - ~600‚Äì3,000 population; the smallest geography reported; Wiki\nCensus Tract - ~4,000 average population; Docs\n\nAlso see Survey, Census Data &gt;&gt; Geographies\n\nGraticules - a network of lines on a map that delineate the geographic coordinates (degrees of latitude and longitude.)\n\nUse of graticules is not advised, unless the graphical output will be used for measurement or navigation, or the direction of North is important for the interpretation of the content, or the content is intended to display distortions and artifacts created by projection. Unnecessary use of graticules only adds visual clutter but little relevant information. Use of coastlines, administrative boundaries or place names permits most viewers of the output to orient themselves better than a graticule\n{sf::st_graticule}\n\nRaster Data - Grid data (instead of point/polygon data in Vector Data) where each square on this grid is a small cell, and each cell holds a single value representing some real-world phenomenon, e.g.¬†elevation, temperature, land cover type, rainfall amount, or color of a pixel in a satellite image. The entire collection of these cells and their values is what we call raster data. Raster data is better for continuous phenomena like elevation, soil moisture, or temperature. Most data from satellites and aerial photography comes in raster form.\nVector Data - Data that uses points, lines, and polygons (instead of grid cells like Raster Data) to represent features like roads, buildings, or country borders. Vector data is precise and good for discrete objects.\nVRT - File format that allows a virtual GDAL dataset to be composed from other GDAL datasets with repositioning, and algorithms potentially applied as well as various kinds of metadata altered or added. VRT descriptions of datasets can be saved in an XML format normally given the extension .vrt.\n\nBasically a metadata XML file describing various properties of the actual raster file, like pixel dimensions, geolocation, etc..",
    "crumbs": [
      "Geospatial",
      "General"
    ]
  },
  {
    "objectID": "qmd/geospatial-general.html#sec-geo-gen-opt",
    "href": "qmd/geospatial-general.html#sec-geo-gen-opt",
    "title": "General",
    "section": "Optimization",
    "text": "Optimization\n\nMisc\nVector Tiles\n\nMisc\n\nNotes from Push the limits of interactive mapping in R with vector tiles\n\nMcBain goes through a complete example with plenty of tips on simplification strategies and hosting mbtiles files\n\nIssues (solution: Vector Tiles)\n\nLimited number of features with DOM canvas\n\nThere‚Äôs a limit to how many features leaflet maps can handle, because at some point the DOM gets too full and your browser stops being able to parse it.\n\nLimited number of maps on same webpage\n\nOnce you start rendering spatial data on WebGL canvasses instead of the DOM you‚Äôll find there is a low number of WebGL contexts that can co-exist on any one web page, typically limiting you to only around 8 maps.\n\nFile sizes blow up to hundreds of MB\n\nTrying to reuse WebGL maps by toggling on and off different layers of data for the user at opportune times. This is an improvement, but data for all those layers piles up, and your toolchain wants to embed this in your page as reams of base64 encoded text. Page file sizes are completely blowing out.\n\n\n\nUse Cases\n\nSimplification of geometry is not desirable, e.g.¬†because of alignment issues\n\ne.g.¬†The zoomed-in road network has to align with the road network on the basemap, so that viewers can see features that lie along sections of road.\n\nSimplification of geometry doesn‚Äôt really help, you still have too many features\nCumulatively your datasets are too large to handle.\n\nVector Tiles¬† - contain arrays of annotated spatial coordinate data which is combined with a separately transmitted stylesheet to produce the tile image.\n\ni.e.¬†The edges of the roads, the boundaries of buildings etc. Not an image, but the building blocks for one\nDifferent stylesheets can use the same vector data to produce radically different looking maps that either highlight or omit data with certain attributes\nMapbox Vector Tiles (MVT) - specification; the de-facto standard for vector tile files\n\nstored as a Google protocol buffer - a tightly packed binary format.\n\n\nMBTiles - by Mapbox; describe a method of storing an entire MVT tileset inside a single file.\n\nInternally .mbtiles files are SQLlite databases containing two tables: metadata and tiles.\n\ntiles table\n\nindexed by z,x,y\ncontains a tile_data column for the vector tile protocol buffers, which are compressed using gzip\n\n\nSQLite format and gzip compression help with efficient retrieval and transmission\n\nUsing vector tiles we can have unlimited reference layers. Each one contributes nothing to the report file size since it is only streamed on demand when required.\nWorkflow to convert data to .tbtiles\n\nIn R, read source data as an sf, and wrangle\n\nTippecanoe expects by epsg 4326 by default\n\nWrite data out to geojson\nOn the command line, convert geojson to .mbtiles using the tippecanoe command line utility.\n\nTippecanoe sources\n\nMapbox version - repo\n\nMcBain says, he uses this version and hasn‚Äôt had any problems\nREADME has helpful cookbook section\n\nActively maintained community forked version - repo\nMay be a headache to get dependencies if using Windows\n\nAlternatively it can output a folder structure full of protocol buffer files.\n\nExample\ntippecanoe -zg \\\n¬† ¬† ¬† ¬† ¬† -o abs_mesh_blocks.mbtiles \\\n¬† ¬† ¬† ¬† ¬† --coalesce-densest-as-needed \\\n¬† ¬† ¬† ¬† ¬† --extend-zooms-if-still-dropping \\\n¬† ¬† ¬† ¬† ¬† mb_shapes.geojson\n\nMapping\n\nExample\nlibrary(mvtview)\nlibrary(rdeck)\n\n# Fire up the server\nserve_mvt(\"abs_mesh_blocks.mbtiles\", port = 8765)\n# Serving your tile data from http://0.0.0.0:8765/abs_mesh_blocks.json.\n# Run clean_mvt() to remove all server sessions.\n\nmesh_blocks &lt;- jsonlite::fromJSON(\"http://0.0.0.0:8765/abs_mesh_blocks.json\")\n\n# Map the data\nrdeck(\n¬† ¬† initial_bounds = structure(meshblocks$bounds, crs = 4326, class = \"bbox\") # set map limits using the tilejson\n) |&gt;\n¬† add_mvt_layer(\n¬† ¬† data = rdeck::tile_json(\"http://0.0.0.0:8765/abs_mesh_blocks.json\"),\n¬† ¬† get_fill_color = scale_color_linear(\n¬† ¬† ¬† random_attribute\n¬† ¬† ),\n¬† ¬† opacity = 0.6\n¬† )\n\nSee McBain article for options on hosting .mbtiles files\nRegarding ‚Äúabs_mesh_blocks‚Äù: {mvtview} provides a way to fetch the metadata table from .mbtiles as json by querying a json file with the same name as the .mbitles file.\nThe structure of ‚Äòtilejson‚Äô is yet another specification created by Mapbox, and is supported in deck.gl (and therefore {rdeck}) to describe tile endpoints.",
    "crumbs": [
      "Geospatial",
      "General"
    ]
  },
  {
    "objectID": "qmd/geospatial-general.html#sec-geo-gen-gridsys",
    "href": "qmd/geospatial-general.html#sec-geo-gen-gridsys",
    "title": "General",
    "section": "Grid Systems",
    "text": "Grid Systems\n\nMisc\n\nExplainer: Why using hexbins to visualize Australian electoral map is better than a typical provincial map.\n\ntl;dr: Geographical size distorts what the value is trying to measure. The value is the party that wins the parliamentary seat\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe bar graph shows the values the map is trying to visualize geographically. The hexabins better represent the close race by removing the distorting element which is the geographical size of the provinces.\nEach voting district (hexabin) is voting for 1 representative and has the same number of voters, but districts can have vastly different areas depending on population density.\n\nKeep unit at constant size (like hexabins) but alter hex shape to keep state shape.\n\n\nA better U.S. house election results map?\nResults\n\nstate size depends on number of districts which depends on population and therefore correctly conveys voting results visually across the whole country\nDistricts get distorted but the states retain their shape and so distortion of the overall visualization is minimized\n\n\n\n\nUber‚Äôs H3 grid system -\n\nMisc\n\nPackages: {h3r}, {h3-r}\ndocs\nAdd census data to H3 hexagons, calculate overlaps (article)\nFor large areas, you can reduce the number of hexagons by merging some hexagons into larger hexagons.\n\nReduces storage size\nIssue: leaves small gaps between hexagons\n\nmight not matter for your use case\n\nSolution: use Microsoft‚Äôs Quadkeys approach (see article)\n\n\nEach hexagon has a series of smaller hexagons that sit (mostly) inside of another, which creates a hierarchy that can be used for consistent referencing and analysis, all the way down to lengths of 2 feet for the edges.\n‚ÄúHexagons were an important choice because people in a city are often in motion, and hexagons minimize the quantization error introduced when users move through a city. Hexagons also allow us to approximate radiuses easily.‚Äù\nRe other shapes: ‚ÄúWe could use postal code areas, but such areas have unusual shapes and sizes which are not helpful for analysis, and are subject to change for reasons entirely unrelated to what we would use them for. Zones could also be drawn by Uber operations teams based on their knowledge of the city, but such zones require frequent updating as cities change and often define the edges of areas arbitrarily‚Äù\nGrid systems can have comparable shapes and sizes across the cities that Uber operates in and are not subject to arbitrary changes. While grid systems do not align to streets and neighborhoods in cities, they can be used to efficiently represent neighborhoods by clustering grid cells. Clustering can be done using objective functions, producing shapes much more useful for analysis. Determining membership of a cluster is as efficient as a set lookup operation.\n16 Resolutions\n\n0 - 15 (0 being coarsest and 15 being finest)\nEach finer resolution has cells with one seventh the area of the coarser resolution. Hexagons cannot be perfectly subdivided into seven hexagons, so the finer cells are only approximately contained within a parent cell.\nThe identifiers for these child cells can be easily truncated to find their ancestor cell at a coarser resolution, enabling efficient indexing. Because the children cells are only approximately contained, the truncation process produces a fixed amount of shape distortion. This distortion is only present when performing truncation of a cell identifier; when indexing locations at a specific resolution, the cell boundaries are exact.\nWant a resolution granular enough to introduce variability and wide enough to capture the effects of an area\nExample of resolution 6 in Iowa",
    "crumbs": [
      "Geospatial",
      "General"
    ]
  },
  {
    "objectID": "qmd/geospatial-general.html#sec-geo-gen-feats",
    "href": "qmd/geospatial-general.html#sec-geo-gen-feats",
    "title": "General",
    "section": "Features",
    "text": "Features\n\nCarto Spatial Features dataset ($) - https://carto.com/spatial-data-catalog/browser/?country=usa&category=derived&provider=carto\n\nResolution: Quadgrid level 15 (with cells of approximately 1x1km) and Quadgrid level 18 (with cells of approximately 100x100m).\n\nGuessing if the areas you‚Äôre interested in have high population density, then maybe 100 x 100 m cells would be more useful\n\nFeatures\n\nTotal population\nPopulation by gender\nPopulation by age and gender (e.g.¬†female_0_to_19)\nPOIs by category\n\nRetail Stores\nEducation\n\nNumber of education related POIs, incuding schools, universities, academies, etc.\n\nFinancial\n\nNumber of financial sector POIs, including ATMs and banks.\n\nFood, Drink\n\nNumber of sustenance related POIs, including restaurants, bars, cafes and pubs.\n\nHealthcare\n\nNumber of healthcare related POIs, including hospitals\n\nLeisure\n\nNumber of POIs related to leisure activities, such as theaters, stadiums and sport centers.\n\nTourism\n\nNumber of POIs related to tourism attractions\n\nTransportation\n\nNumber of transportation related POIs, including parking lots, car rentals, train stations and public transport stations.\n\n\n\n\nCarto Data Observatory ($) - https://carto.com/spatial-data-catalog/browser/dataset/mc_geographic\\_\\_4a11e98c/\n\nFeatures\n\nGeo id\nRegion id\nIndustry\nTotal Transactions Amount Index\nTransaction Count Index\nAccount Count Index\nAverage Ticket Size Index\nAverage Frequency of Transaction per Card Index\nAverage Spend Amount by Account Index",
    "crumbs": [
      "Geospatial",
      "General"
    ]
  },
  {
    "objectID": "qmd/geospatial-general.html#sec-geo-gen-interx",
    "href": "qmd/geospatial-general.html#sec-geo-gen-interx",
    "title": "General",
    "section": "Interactions",
    "text": "Interactions\n\nSimilar to interpolation but keeps the original spatial units as interpretive framework. Hence, the map reader can still rely on a known territorial division to develop its analyses\n\nThey produce understandable maps by smoothing complex spatial patterns\nThey enrich variables with contextual spatial information.\n\nMisc\n\nResources\n\nGetting Started with Potential - nice little mathematical summary, some background\n\nPackages\n\n{potential}: spatial interaction modeling via Stewart Potentials. Also capable of interpolation\n\n\nThere are two main ways of modeling spatial interactions: the first one focuses on links between places (flows), the second one focuses on places and their influence at a distance (potentials).\nComparisons ({potential}article)\n\nGDP per capita (cloropleth)\n\n\nTypical cloropleth at the municipality level\nValues have been binned\n\nPotential GDP per Capita (interaction)\n\n\nStewart Potentials have smoothed the values\nMunicipality boundaries still intact, so you could perform an analysis based on these GDP regions\n\nSmoothed GDP per Capita (interpolation)\n\n\nSimilar results as the interaction model except there are no boundaries",
    "crumbs": [
      "Geospatial",
      "General"
    ]
  },
  {
    "objectID": "qmd/geospatial-general.html#sec-geo-gen-interp",
    "href": "qmd/geospatial-general.html#sec-geo-gen-interp",
    "title": "General",
    "section": "Interpolation",
    "text": "Interpolation\n\nMisc\n\nThe process of using points with known values to estimate values at other points. In GIS applications, spatial interpolation is typically applied to a raster with estimates made for all cells. Spatial interpolation is therefore a means of creating surface data from sample points.\nPackages\n\n{gstat} - Has various interpolation methods.\n\nMeasurements can have strong regional variance, so the geographical distribution of measurements can have a strong influence on statistical estimates.\n\nExample: Temperature\n\n\nTwo different geographical distributions of sensors\n\n\nA concentration of sensors in North can lead to a cooler average regional temperature and vice versa for the South.\n\nDistribution of temperatures across the region for 1 day.\n\n\nWith this much variance in temperature, a density of sensors in one area can distort the overall average.\n\nInterpolation evens out the geographical distribution of measurments\n\n\n\n\n\n\nKriging\n\n\nConsiders not only the distances but also the other variables that have a linear relationship with the estimation variables\nMisc\n\nResources\n\nSpatial Sampling With R: Chapter 21 Introduction to kriging\n\nIn Ordinary Kriging (OK) it is assumed that the mean of the study variable is the same everywhere\n\nUses a correlated Gaussian process to guess at values between data points\n\n\nUncorrelated - white noise\nCorrelated - smooth\n\nThe closer in distance two points are to each other, the more likely they are to have a similar value (i.e.¬†geospatially correlated)\nExample: Temperature\n\n\nFewer known points means greater uncertainty\n\n\nInputs:\n\nThe measured values at the sampling points,\nThe geometric coordinates of the sampling points,\nThe geometric coordinates of the target points to interpolate,\nThe ‚Äúcalibrated‚Äù probabilistic model, with the spatial correlation obtained by data\n\nOutputs:\n\nThe estimated values at the target points,\nThe estimated uncertainty (variance) at the target points.\n\n\nComponents\n\nExperimental Variogram - A semivariogram that models the dissimilarity of the study variable at two locations, as a function of the vector.\n\\[\n\\gamma(h) = \\frac{1}{2n} \\sum (z(x_i) - z(x_i + h))\n\\]\n\n\\(h\\): Geographical distance between two observed points\n\\(z\\): Function calculating the variable you‚Äôre interested in. (e.g.¬†Temperature)\n\\(x_i\\): Location\n\nTheoretical Variogram - Used to model the experimental variogram\n\nOptions\n\nGaussian\n\\[\n\\gamma' = p(1 - e^{-(d^2/ (\\frac{4}{7}r)^2)}) + n\n\\]\nExponential\n\\[\n\\gamma' = p(1- e^{-3d / r}) + n\n\\]\nSpherical\n\\[\n\\gamma' =\n\\left\\{ \\begin{array}{lcl}\np\\left(\\frac{3d}{2r} - \\frac{d^3}{2r^3}\\right) +n & \\mbox{if} \\; d \\le r \\\\\np + n & \\mbox{if} \\; d \\gt r\n\\end{array}\\right.\n\\]\n\nWhere:\n\n\n\\(p\\): Partial Sill\n\\(d\\): Distance (same as \\(h\\))\n\\(n\\): Nugget\n\\(r\\): Range\n\n\nProcess: Optimize the parameters of the parameters of theoretical variogram to fit to the experimental one. Then predict values using the estimated theoretical variogram.",
    "crumbs": [
      "Geospatial",
      "General"
    ]
  },
  {
    "objectID": "qmd/geospatial-analysis.html",
    "href": "qmd/geospatial-analysis.html",
    "title": "Analysis",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Geospatial",
      "Analysis"
    ]
  },
  {
    "objectID": "qmd/geospatial-analysis.html#sec-geo-anal-misc",
    "href": "qmd/geospatial-analysis.html#sec-geo-anal-misc",
    "title": "Analysis",
    "section": "",
    "text": "Also see\n\nDomain Knowledge &gt;&gt; Epidemiology &gt;&gt; Disease Mapping",
    "crumbs": [
      "Geospatial",
      "Analysis"
    ]
  },
  {
    "objectID": "qmd/geospatial-analysis.html#sec-geo-anal-terms",
    "href": "qmd/geospatial-analysis.html#sec-geo-anal-terms",
    "title": "Analysis",
    "section": "Terms",
    "text": "Terms\n\nBuffer - a zone around a geographic feature containing locations that are within a specified distance of that feature, the buffer zone. A buffer is likely the most commonly used tool within the proximity analysis methods. Buffers are usually used to delineate protected zones around features or to show areas of influence.\nCatchment - The area inside any given polygon is closer to that polygon‚Äôs point than any other. Refers to the area of influence from which a retail location, such as a shopping center, or service, such as a hospital, is likely to draw its customers. (also see Retail &gt;&gt; Catchment)",
    "crumbs": [
      "Geospatial",
      "Analysis"
    ]
  },
  {
    "objectID": "qmd/geospatial-analysis.html#sec-geo-anal-proxanal",
    "href": "qmd/geospatial-analysis.html#sec-geo-anal-proxanal",
    "title": "Analysis",
    "section": "Proximity Analysis",
    "text": "Proximity Analysis\n\nExample: Basic Workflow\n\nData: Labels, Latitude, and Longitude\n\nCreate Simple Features (sf) Object\ncustomer_sf &lt;- \n  customer_table %&gt;%\n    sf::st_as_sf(coords = c(\"longitude\", \"latitude\"),\n                 crs = 4326)\n\nMerges the longitude and latitude columns into a geometry column and transforms the coordinates in that column according to projection (e.g.¬†crs = 4326)\n\nView points on a map\n\nmapview::mapview(customer_sf)\nCreate Buffer Zones\n\ncustomer_buffers &lt;- \n  customer_sf %&gt;%\n    sf::st_transform(26914) %&gt;%\n    sf::st_buffer(5000)\n\nmapview::mapview(customer_buffers)\n\nMost of projections use meters, and based on the size of the circles as related to the size of Denton, TX, I‚Äôm guessing the radius of each circle is 5000m. Although, that still looks a little small.\n\nCreate Isochrones\n\ncustomer_drivetimes &lt;- \n  customer_sf %&gt;%\n    mapboxapi::mb_isochrone(time = 10, \n                            profile = \"driving\", \n                            id_column = \"name\")\n\nmapview::mapview(customer_drivetimes)\n\n10 minutes drive-time from each location\ntime (minutes): The maximum time supported is 60 minutes. Reflects traffic conditions for the date and time at which the function is called.\n\nIf reproducibility of isochrones is required, supply an argument to the depart_at argument.\n\ndepart_at: Specifying a time makes it a time-aware isochrone. Useful for modeling peak business hours or rush hour traffic, etc.\n\ne.g.¬†Adding depart_at = ‚Äú2024-01-27T17:30‚Äù to the isochrone above gives you a 10-minute driving isochrone with predicted traffic at 5:30pm tomorrow\n\n\nAdd Demographic Data\n\ndenton_income &lt;- \n  tidycensus::get_acs(\n    geography = \"tract\",\n    variables = \"B19013_001\",\n    state = \"TX\",\n    county = \"Denton\",\n    geometry = TRUE\n  ) %&gt;%\n    select(tract_income = estimate) %&gt;%\n    sf::st_transform(st_crs(customer_sf))\n\ncustomers_with_income &lt;- customer_sf %&gt;%\n  sf::st_join(denton_income)\n\ncustomers_with_income\n\nAdds median income estimate according to the census tract each person lives in.\nJoins on the geometry variable\n\n\nCircular Buffer Approach\n\nNotes from GIS-based Approaches to Catchment Area Analyses of Mass Transit\nThe simplest and most common used approach to make catchment areas of a location is to consider the Euclidean distance from the location.\nDue to limitations (See below), it‚Äôs best suited for overall analyses of catchment areas.\nOften the level of detail in the method has been increased by dividing the catchment area into different rings depending on the distance to the station.\n\nExample: By applying weights for each ring it is possible to take into account that the expected share of potential travelers at a train station will drop when the distance to the stop is increased.\n\n\n\nLimitation: Does not take the geographical surroundings into account.\n\nExample: In most cases, the actual walking distance to/from a location is longer than the Euclidean distance since there are natural barriers like rivers, buildings, rail tracks etc.\n\nThis limitation is often coped with by applying a detour factor that reduces the buffer distance to compensate for the longer walking distance.\nHowever, in cases where the length of the detours varies considerably within the location‚Äôs surroundings, this solution is not very precise.\nFurthermore, areas that are separated completely from a location, e.g.¬†by rivers, might still be considered as part of the location‚Äôs catchment area\n\n\nUse Case: Ascertain Travel Potential to Determine Potential Station Locations\n\nEvery 50m along the proposed transit line, calculate the travel potential for that buffer area\n\nUsing the travel demand data for that buffer area, calculate travel potential\n\nTravel Potential Graph\n\n\nLeft side represents the transit line.\nRight Side\n\nY-Axis are locations where buffer areas were created.\nX-Axis: Travel Potential\n\nNot sure if that is just smoothed line with a point estimate of Travel Potential at each location or how exactly those values are calculated.\n\n50m isn‚Äôt a large distance so maybe all the locations aren‚Äôt shown on the Y-Axis and the number of calculations produces an already, mostly, smooth line on it‚Äôs own.\n\nPartitioning a buffer zone into rings or some kind of interpolation could provided more granular estimates around the central buffer location.",
    "crumbs": [
      "Geospatial",
      "Analysis"
    ]
  },
  {
    "objectID": "qmd/regression-discrete.html",
    "href": "qmd/regression-discrete.html",
    "title": "Discrete",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Regression",
      "Discrete"
    ]
  },
  {
    "objectID": "qmd/regression-discrete.html#sec-reg-disc-misc",
    "href": "qmd/regression-discrete.html#sec-reg-disc-misc",
    "title": "Discrete",
    "section": "",
    "text": "Also see Regression, Other\nPackages\n\n{glmnet}\n{contingencytables}\n\nCompanion package for the Statistical Analysis of Contingency Tables book\n\n\nResources\n\nDiscrete Analysis Notebook - Notes from Agresti books and PSU Course\nStatistical Analysis of Contingency Tables (R &gt;&gt; Documents &gt;&gt; Regression)\n\nCovers effect size estimation, confidence intervals, and hypothesis tests for the binomial and the multinomial distributions, unpaired and paired 2x2 tables, rxc tables, ordered rx2 and 2xc tables, paired cxc tables, and stratified tables.\n\n\nFor Diagnostics see:\n\n{DHARMa} - Built for Mixed Effects Models for count distributions but handles lm, glm (poisson) and MASS::glm.nb (neg.bin)\nDiagnostics, Probabilistic &gt;&gt; Visual Inspection &gt;&gt; Visual Inspection\nDiagnostics, GLM\n\nWith aggregated counts that are bound within a certain range, it can be better to turn the range of counts into percentages (see example) and model those as your outcome\n\nDistributions\n\nZero-One Inflated Beta\nmod_zoib &lt;-\n  brm(bf(outcome_pct ~ 1),\n      data = example_data,\n      family = zero_one_inflated_beta(),\n      cores = 4)\npp_check(mod_zoib)\n\nZero-One Inflated Binomial\n\nIn general, you can have a zero-N inflated binomial\n\n\nExample: Aggregated counts from 1 to 32 (Thread)\n\n\nIf something specific is generating 1 and 32 counts\n\nIdeally you‚Äôd do this, but these require creating bespoke distribution families which is possible in STAN\n\nIf you cannot get zero, then a 0-31-inflated binomial works fine.\nIf 0 is possible but it didn‚Äôt happen, then do a 1-32-inflated binomial.\n\nMore conveniently, you‚Äôd transform the range (1-32) to percentages where 100% = 32, and use zero-one inflated beta (currently available in {brms} or zero-one inflated binomial\n\nIf there is NOT something specific generating the 1 and 32 counts(?)\n\nYou can keep the counts and treat them as an ordered factor\n\nCollapse the counts from 2-31 into a category, so you have 3 categories: 1, 2-31, 32.\nModel as an ordered logit\nmod_ologit &lt;- \n  brm(bf(outcome_factor ~ 1),\n      data = example_data,\n      family = cumulative,\n      cores = 4)\npp_check(mod_ologit)",
    "crumbs": [
      "Regression",
      "Discrete"
    ]
  },
  {
    "objectID": "qmd/regression-discrete.html#sec-reg-disc-terms",
    "href": "qmd/regression-discrete.html#sec-reg-disc-terms",
    "title": "Discrete",
    "section": "Terms",
    "text": "Terms\n\nA saturated model is a regression model that includes a discrete (indicator) variable for each set of values the explanatory variables can take.\n\nAnother case is when there are as many estimated parameters as data points.\n\ne.g.¬†if you have 6 data points and fit a 5th-order polynomial to the data, you would have a saturated model (one parameter for each of the 5 powers of your independant variable plus one for the constant term).\n\nMulti-variable models require interactions to be able to cover each set of values that the explanatory variables can take (see 3rd example)\nSince saturated models, perfectly model the sample, they don‚Äôt generalize to the population well.\n\nNo data left to estimate variance.\n\nExamples of Saturated Models\n\nWages ~ College Graduation (binary)\n\\[\n\\operatorname{Wages}_i = \\alpha + \\beta \\:\\mathbb{I}\\{\\operatorname{College Graduate}\\}_i + \\epsilon_i\n\\]\nWages ~ Schooling (discrete, yrs).\n\\[\n\\begin{align}\n\\operatorname{Wages} &= \\alpha + \\beta_1 \\:\\mathbb{I}\\{s_i = 1\\} + \\beta_2 \\:\\mathbb{I}\\{s_i = 2\\} + ‚ãØ + \\beta_T \\:\\mathbb{I}\\{s_i = T\\}\n&\\text{where}\\quad s_i \\in \\{0, 1, 2,...T\\}\n\\end{align}\n\\]\n\n0 is the reference level; \\(\\beta\\) is the effect of j years of schooling.\n\nWages ~ College Graduation + Gender + Interaction.\n\\[\n\\operatorname{Wages} = \\alpha + \\beta_1 \\:\\mathbb{I}{\\operatorname{College Graduate}} + \\beta_2 \\:\\mathbb{I}\\{\\operatorname{Female}\\} + \\beta_3 \\:\\mathbb{I}\\{\\operatorname{College Graduate}\\} \\times \\:\\mathbb{I}\\{\\operatorname{Female}\\} + Œµ\n\\]\n\n\\(\\mathbb{E}[\\operatorname{Wages}_i | \\operatorname{College Graduate}_i = 0, \\operatorname{Female}_i = 0] = \\alpha\\)\n\nExpected value of Wages for individual i given they‚Äôre not a college graduate and are male\n\n\\(\\mathbb{E}[\\operatorname{Wages}_i | \\operatorname{College Graduate}_i = 1, \\operatorname{Female}_i = 0] = \\alpha + \\beta_1\\)\n\\(\\mathbb{E}[\\operatorname{Wages}_i | \\operatorname{College Graduate}_i = 0, \\operatorname{Female}_i = 1] = \\alpha + \\beta_2\\)\n\\(\\mathbb{E}[\\operatorname{Wages}_i | \\operatorname{College Graduate}_i = 1, \\operatorname{Female}_i = 1] = \\alpha + \\beta_1 + \\beta_2 + \\beta_3\\)\n\n\n\nNull Model has only one parameter, which is the intercept.\n\nThis is essentially the mean of all the data points.\nFor a bivariate model, this is a horizontal line with the same prediction for every point\n\nDeviance\n\\[\nD = 2(L_S - L_P) = 2(\\operatorname{loglik}(y\\;|\\;y) - \\operatorname{loglik}(\\mu\\;|\\;y))\n\\]\n\n\\(L_S\\) is the saturated model\n\\(L_P\\) is the ‚Äúproposed model‚Äù (i.e.¬†the model being fit)",
    "crumbs": [
      "Regression",
      "Discrete"
    ]
  },
  {
    "objectID": "qmd/regression-discrete.html#sec-reg-disc-binom",
    "href": "qmd/regression-discrete.html#sec-reg-disc-binom",
    "title": "Discrete",
    "section": "Binomial",
    "text": "Binomial\n\nExample: UCB Admissions\n# Array to tibble (see below for deaggregation this to 1/0)\nucb &lt;-¬†\n¬† ¬† as_tibble(UCBAdmissions) %&gt;%¬†\n¬† ¬† mutate(across(where(is.character), ~ as.factor(.))) %&gt;%¬†\n¬† ¬† pivot_wider(\n¬† ¬† ¬† ¬† id_cols = c(Gender, Dept),\n¬† ¬† ¬† ¬† names_from = Admit,\n¬† ¬† ¬† ¬† values_from = n,\n¬† ¬† ¬† ¬† values_fill = 0L\n¬† ¬† ¬† )\n\n## # A tibble: 12 √ó 4\n##¬† ¬† Gender Dept¬†    Admitted Rejected\n##¬† ¬† &lt;fct&gt;¬† &lt;fct&gt;¬† ¬†   &lt;dbl&gt;¬† ¬† &lt;dbl&gt;\n##¬† 1 Male¬†     A¬† ¬† ¬† ¬† ¬† 512¬† ¬† ¬† 313\n##¬† 2 Female    A¬† ¬† ¬† ¬† ¬†  89¬† ¬† ¬†  19\n##¬† 3 Male¬†     B¬† ¬† ¬† ¬† ¬† 353¬† ¬† ¬† 207\n##¬† 4 Female    B¬† ¬† ¬† ¬† ¬†  17¬† ¬† ¬† ¬† 8\n##¬† 5 Male¬†     C¬† ¬† ¬† ¬† ¬† 120¬† ¬† ¬† 205\n##¬† 6 Female    C¬† ¬† ¬† ¬† ¬† 202¬† ¬† ¬† 391\n##¬† 7 Male¬†     D¬† ¬† ¬† ¬† ¬† 138¬† ¬† ¬† 279\n##¬† 8 Female    D¬† ¬† ¬† ¬† ¬† 131¬† ¬† ¬† 244\n##¬† 9 Male¬†     E¬† ¬† ¬† ¬† ¬†  53¬† ¬† ¬† 138\n## 10 Female    E¬† ¬† ¬† ¬† ¬†  94¬† ¬† ¬† 299\n## 11 Male¬†     F¬† ¬† ¬† ¬† ¬†  22¬† ¬† ¬† 351\n## 12 Female    F¬† ¬† ¬† ¬† ¬†  24¬† ¬† ¬† 317\n\nglm(\n¬† cbind(Rejected, Admitted) ~ Gender + Dept,\n¬† data = ucb,\n¬† family = binomial\n)\n## Coefficients:\n## (Intercept)¬† GenderMale¬† ¬† ¬† ¬† DeptB¬† ¬† ¬† ¬† DeptC¬† ¬† ¬† ¬† DeptD¬† ¬† ¬† ¬† DeptE¬†\n##¬† ¬† -0.68192¬† ¬† ¬†0.09987¬† ¬† ¬† 0.04340¬† ¬† ¬† 1.26260¬† ¬† ¬† 1.29461¬† ¬† ¬† 1.73931¬†\n##¬† ¬† ¬† DeptF¬†\n##¬† ¬† 3.30648¬†\n##¬†\n## Degrees of Freedom: 11 Total (i.e. Null);¬† 5 Residual\n## Null Deviance: ¬† ¬† 877.1¬†\n## Residual Deviance: 20.2 AIC: 103.1\n\ncbind(Rejected, Admitted) says that ‚ÄúRejected‚Äù is the response variable since it is listed first in the cbind function\nCan also use a logistic model, but need case-level data (e.g.¬†0/1)\n\nDeaggregate count data into 0/1 case-level data\ndata(UCBadmit, package = \"rethinking\")\nucb &lt;- UCBadmit %&gt;%\n¬† mutate(applicant.gender = relevel(applicant.gender, ref = \"male\"))\n\n# deaggregate to 1/0\ndeagg_ucb &lt;- function(x, y) {\n¬† UCBadmit %&gt;%\n¬† ¬† select(-applications) %&gt;%\n¬† ¬† group_by(dept, applicant.gender) %&gt;%\n¬† ¬† tidyr::uncount(weights = !!sym(x)) %&gt;%\n¬† ¬† mutate(admitted = y) %&gt;%\n¬† ¬† select(dept, gender = applicant.gender, admitted)\n}\nucb_01 &lt;- purrr::map2_dfr(c(\"admit\", \"reject\"),\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† c(1, 0),\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ~ disagg_ucb(.x, .y)\n)\n\n\nExample: Treatment/Control\n¬† ¬† ¬† ¬† ¬† ¬† Disease¬† ¬† ¬† No Disease\nTreatment¬† ¬† ¬† ¬† 55¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† 67\nControl¬† ¬† ¬† ¬† ¬† 42¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† 34\n\ndf &lt;- tibble(treatment_status = c(\"treatment\", \"no_treatment\"),\n¬† ¬† ¬† disease = c(55, 42),\n¬† ¬† ¬† no_disease = c(67,34)) %&gt;%¬†\n¬† mutate(total = no_disease + disease,\n¬† ¬† ¬† ¬† proportion_disease = disease / total)¬†\n\nmodel_weighted &lt;- glm(proportion_disease ~ treatment_status,\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† data = df,\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† family = binomial,\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† weights = total)\nmodel_cbinded &lt;- glm(cbind(disease, no_disease) ~ treatment_status,\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† data = df,\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† family = binomial)\n\n# Aggregated counts expanded into case-level data\ndf_expanded &lt;- tibble(disease_status = c(1, 1, 0, 0),¬†\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† treatment_status = rep(c(\"treatment\", \"control\"), 2)) %&gt;%\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† .[c(rep(1, 55), rep(2, 42), rep(3, 67), rep(4, 34)), ]\n# logistic\nmodel_expanded &lt;- glm(disease_status ~ treatment_status, data = df_expanded, family = binomial(\"logit\"))\n\nAll methods are equivalent\n‚Äúdisease‚Äù is listed first in the cbind function, therefore it is the response variable.",
    "crumbs": [
      "Regression",
      "Discrete"
    ]
  },
  {
    "objectID": "qmd/regression-discrete.html#sec-reg-disc-pois",
    "href": "qmd/regression-discrete.html#sec-reg-disc-pois",
    "title": "Discrete",
    "section": "Poisson",
    "text": "Poisson\n\n\nMisc\nInterpretation\n\nEffect of a binary treatment\n\\[\ne^\\beta = \\mathbb{E}[Y(1)/\\mathbb{E}Y(0)] = \\theta_{\\text{ATE%}} + 1\n\\]\n\n\\(\\theta\\) is the effect interpreted as a percentage\n\\(\\mathbb{E}[Y(1)]\\) is the expected value of the outcome for a subject assigned to Treatment.\nTherefore, \\(e^\\beta - 1\\) is the average percent increase or decrease from baseline to treatment\nParameter may difficult to interpret in contexts where Y spans several order of magnitudes.\n\nExample: The econometrician may perceive a change in income from $5,000 to $6,000 very differently from a change in income from $100,000 to $101,000, yet both those changes are treatment effects in levels of $1,000 and thus contribute equally to Œ∏ATE%.",
    "crumbs": [
      "Regression",
      "Discrete"
    ]
  },
  {
    "objectID": "qmd/clustering-time-series.html",
    "href": "qmd/clustering-time-series.html",
    "title": "Time Series",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Clustering",
      "Time Series"
    ]
  },
  {
    "objectID": "qmd/clustering-time-series.html#sec-clust-misc",
    "href": "qmd/clustering-time-series.html#sec-clust-misc",
    "title": "Time Series",
    "section": "",
    "text": "AKA Time Series Classification\nMethods\n\nEuclidean Distance Approaches\nDynamic Time Warping\nShapelet-based\nKernel-based\nFeature-based\n\nResources\n\nPaper: Time-series clustering ‚Äì A decade review (2015)",
    "crumbs": [
      "Clustering",
      "Time Series"
    ]
  },
  {
    "objectID": "qmd/clustering-time-series.html#sec-clust-cvi",
    "href": "qmd/clustering-time-series.html#sec-clust-cvi",
    "title": "Time Series",
    "section": "Cluster Validity Indices (CVI)",
    "text": "Cluster Validity Indices (CVI)\n\nNotes from {dtwclust} vignette\nTypes\n\nFor either Hard (aka Crisp) or Soft (aka Fuzzy) Partitioning\nCharacteristics\n\nInternal - Try to define a measure of cluster purity\nExternal - Compare the obtained partition to the correct one. Thus, external CVIs can only be used if the ground truth is known\nRelative - ?\n\nScores\n\n\nAvailable through dtwclust::cvi\nGlobal centroid is one that‚Äôs computed using the whole dataset\n\ndtwclust implements whichever distance method that originally used in the clustering computation\n\nSome CVIs require symmetric distance functions (distance from a to b = distance b to a)\n\nA warning is printed if an asymmetric distance method was used\n\n{clue} - Compare repetitions of non-deterministic clustering methods (e.g.¬†partitional) where random element means you get a different result each time\n\nIt uses a measure called ‚Äúdissimilarities using minimal Euclidean membership distance‚Äù to compare different runs of a cluster method",
    "crumbs": [
      "Clustering",
      "Time Series"
    ]
  },
  {
    "objectID": "qmd/clustering-time-series.html#sec-clust-dtw",
    "href": "qmd/clustering-time-series.html#sec-clust-dtw",
    "title": "Time Series",
    "section": "{dtwclust}",
    "text": "{dtwclust}\n\nSee vignette appendices from examples\n\n\nWorkflow\n\nPick one of each:\n\nClustering Method\n\nPartitional\nHierarchical\nTADPole\nk-Shape\nFuzzy\n\nDistance Algorithm (See Association, Time Series &gt;&gt; Distances)\n\nDynamic Time Warping (DTW)\nSoft-DTW\nSlope Based Distance (SBD)\nTriangular Global Alignment Kernel¬† (TGAK)\n\nPrototype Method\n\nPartition Around Medoids (PAM)\nDTW Barycenter Averaging (DBA)\nShape Extraction\nFuzzy-Based\n\n\nNot all methods, distances, prototypes are interchangeable with each other\nNot all methods, distances, prototypes handle multivariate series or unequal length series\n\nMultivariate series are mentioned throughout\nI think this means comparing 2 different dataframes of time series but not sure (maybe examples in appendices)\n\nThe documentation for each function specifies if they use parallelization and how, but in general,\n\nAll distances use multi-threading (RcppParallel) (see TADPole method for more details on options)\nMulti-Processing (foreach) is leveraged during clustering but no further parallelization on each node\n\nGrid Search\n\ncompare_clusterings\nProvides a way to efficiently grid search different settings for a clustering method or multiple methods (I think)\n\n\n\n\nClustering Methods\n\nIn addition to what‚Äôs described below, validity measures in the next section can also be used to choose the best number of clusters\nHierarchical\n\nhierarchical_control through tsclust( type = \"hierarchical\")\nMemory size needs to be a consideration. Depending on resources, might be limited to smaller datasets\nCreates a hierarchy of groups in which, as the level in the hierarchy increases, clusters are created by merging the clusters from the next lower level, such that an ordered sequence of groupings is obtained.\nA (dis)similarity measure (linkage) between groups should be specified, in addition to the one that is used to calculate pairwise similarities.\nA specific number of clusters does not need to be specified for the hierarchy to be created, and the procedure is deterministic, so it will always give the same result for a chosen set of (dis)similarity measures\nIf data are easily grouped, the type of linkage used doesn‚Äôt make much of difference\nDendrogram - a binary tree where the height of each node is proportional to the value of the inter-group dissimilarity between its two daughter nodes\n\nVisually evaluate the dendrogram in order to assess the height at which the largest change in dissimilarity occurs, consequently cutting the dendrogram at said height and extracting the clusters that are created\n\n\n3rd level or 4th level from the top is where the disparities in segment length between the daughters start to even out\n\n\n\nPartitional\n\npartitional_control through tsclust(type = \"partitional\")\nNumber of clusters, k, needs to specified beforehand\n\nSometimes during iterations, clusters become empty, and instability created. Try lower value of k or different distance method\n\nStochastic due to their random start. Thus, it is common practice to test different random starts to evaluate several local optima and choose the best result out of all the repetitions.\nTends to produce spherical clusters, but has a lower complexity, so it can be applied to very large datasets\nSteps\n\nk centroids are randomly initialized, usually by choosing k objects from the dataset at random; these are assigned to individual clusters.\nThe distance between all objects in the data and all centroids is calculated, and each object is assigned to the cluster of its closest centroid.\nA prototyping function is applied to each cluster to update the corresponding centroid.\nDistances and centroids are updated iteratively until a certain number of iterations have elapsed, or no object changes clusters any more\n\n\nTADPole\n\nTADPole or tadpole_control through tsclust(type= \"tadpole\")\nRequires series of equal length (uses Sakoe-Chiba window)\nMemory size needs to be a consideration. Depending on resources, might be limited to smaller datasets\nUtilizes DTW lower bound method so lb_keogh or lb_improved needs to be specified\n\nIn order to use euclidean distance for the upper bound, symmetric1 step pattern needs to specified\n\ndc:¬† cutoff distance(s). Can be a vector with several values\n\nAnything below this distance is a neighbor\nNo details on how to determine the value(s) to use, may need to look at original paper\n\nDeterministic like hierarchical\nUses series from the data as centroids (like PAM)\nUses parallelization with certain specifications and will utilize all available threads\n\nUse RcppParallel::setThreadOptions( to adjust unless using {doParellel}\n\n\nk-Shape\n\npartitional_control through tsclust(type = \"partitional\")\nStochastic see the different component setting descriptions for more details\nSettings\n\nPartitional as the method\nSBD as the distance measure\nShape extraction as the centroid function\nz -normalization as the preprocessing step\n\n\nFuzzy\n\nfuzzy_control through tsclust(type = \"fuzzy\" )\nSoft clustering method (like Gaussian Mixture Models)\n\nA series can belong to more than one cluster\n\nA percentage of belongedness to each cluster\n\n\n\n\n\n\nPrototypes/Average Series/Centroids\n\nDefines a time-series that effectively summarizes the most important characteristics of all series in a given cluster\nPrototypes could be used for visualization aid or to perform time-series classification\nMethods\n\nPartition Around Medoids (PAM)\n\nA series in the data whose average distance to all other objects in the same cluster is minimal\nPossible to calculate the distance matrix once and use it for each iteration. Takes less compute time and is done by default\n\npartitional_control(pam.precompute = TRUE)\nMentions that if done this way then the matrix is allocated to memory. So can be an issue for large datasets and the option would need to switched off.\n\nWhen switched off, sparse matrices are used.\n\npartitional_control(pam.precompute = FALSE, pam.sparse=TRUE)\n\n\n\n\nDTW Barycenter Averaging (DBA)\n\nDBA\nWith series of different lengths, either symmetric1 or symmetric2 should be used\nExpensive unless used in conjunction with the DTW distance algorithm\nSteps\n\nFor each cluster, a series in the data is chosen at random to be the reference series (or centroid)\nDTW alignments are calculated wrt to the reference series\nFor each point on the reference series, the point values from the other series that map to that point are averaged.\nAll the calculated average points become the new reference series.\nProcess is iterated until convergence or number of iterations is reached\n\n\nShape Extraction\n\nshape_extraction\nRequires z-normalized in preprocessing argument\nIf lengths unequal, a reference series is chosen\nSteps\n\n2 series are aligned based maximizing a NCCc score\n\nIf unequal lengths zeros are append to the shorter series or the longer series is truncated (required for next step\n\nAligned series are entered row-wise into a matrix and the Rayleigh Quotient (::shrugs::) is maximized to obtain the final prototype\n\n\nFuzzy-based\n\nC-Means\n\nRequires series of equal lengths\nCentroid is like a weighted average using the cluster ‚Äúbelongedness‚Äù weight\n\\[\n\\mu_{c,i} = \\frac{\\sum_{p=1}^N u_{p,c}^m x_{p,i}}{\\sum_{p=1}^N u_{p,c}^m}\n\\]\n\n\\(\\mu\\): The ith element of the cth centroid\n\\(x\\): The ith element of the pth series in the data\n\\(u\\): ‚ÄúBelongedness‚Äù proportion of the pth series to the cth cluster\n\\(m\\): The fuzziness exponent and should be greater than 1, with a common default value of 2\n\n\nC-Medoids (FCMdd)\n\nCentroid selection similar to PAM\nHandles series of unequal lengths (as long as distance method permits)\n\\[\n\\mu_c = x_q \\\\\nq = \\arg \\min \\sum_{p=1}^N u_{p,c}^m d_{p,c}^2\n\\]\n\n\\(\\mu\\): The cth centroid\n\\(x_q\\):The qth series in the data\n\\(u\\): ‚ÄúBelongedness‚Äù proportion of the pth series to the cth cluster\n\\(d\\): Represents the distance between the pth series of the data and the cth centroid\nSee vignette for details on the optimization procedure",
    "crumbs": [
      "Clustering",
      "Time Series"
    ]
  },
  {
    "objectID": "qmd/db-vector.html",
    "href": "qmd/db-vector.html",
    "title": "Vector Databases",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Databases",
      "Vector Databases"
    ]
  },
  {
    "objectID": "qmd/db-vector.html#sec-db-vect-misc",
    "href": "qmd/db-vector.html#sec-db-vect-misc",
    "title": "Vector Databases",
    "section": "",
    "text": "Vector databases store embeddings and provide fast similarity searches\nSuperlinked Comparison (link): Prices, features, etc.\nComparison (link)\n\n\nOpen-Source and hosted cloud: If you lean towards open-source solutions, Weviate, Milvus, and Chroma emerge as top contenders. Pinecone, although not open-source, shines with its developer experience and a robust fully hosted solution.\nPerformance: When it comes to raw performance in queries per second, Milvus takes the lead, closely followed by Weviate and Qdrant. However, in terms of latency, Pinecone and Milvus both offer impressive sub-2ms results. If nmultiple pods are added for pinecone, then much higher QPS can be reached.\nCommunity Strength: Milvus boasts the largest community presence, followed by Weviate and Elasticsearch. A strong community often translates to better support, enhancements, and bug fixes.\nScalability, advanced features and security: Role-based access control, a feature crucial for many enterprise applications, is found in Pinecone, Milvus, and Elasticsearch. On the scaling front, dynamic segment placement is offered by Milvus and Chroma, making them suitable for ever-evolving datasets. If you‚Äôre in need of a database with a wide array of index types, Milvus‚Äô support for 11 different types is unmatched. While hybrid search is well-supported across the board, Elasticsearch does fall short in terms of disk index support.\nPricing: For startups or projects on a budget, Qdrant‚Äôs estimated $9 pricing for 50k vectors is hard to beat. On the other end of the spectrum, for larger projects requiring high performance, Pinecone and Milvus offer competitive pricing tiers.",
    "crumbs": [
      "Databases",
      "Vector Databases"
    ]
  },
  {
    "objectID": "qmd/db-vector.html#sec-db-vect-bran",
    "href": "qmd/db-vector.html#sec-db-vect-bran",
    "title": "Vector Databases",
    "section": "Brands",
    "text": "Brands\n\nQdrant - open source, free, and easy to use (example)\nChroma - can be used as a local in-memory (example)\nPinecone - Data Elixir is using this store for their chatbot; has a free tier\nPostgres with pgvector: Supports exact and approximate nearest neighbor search; L2 distance, inner product, and cosine distance; any language with a Postgres client\n\nAlso see Databases, PostgreSQL &gt;&gt; Extensions &gt;&gt; pgvector and pg_sparse for sparse embeddings (e.g.¬†SPLADE)",
    "crumbs": [
      "Databases",
      "Vector Databases"
    ]
  },
  {
    "objectID": "qmd/regression-other.html",
    "href": "qmd/regression-other.html",
    "title": "36¬† Other",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>36</span>¬† <span class='chapter-title'>Other</span>"
    ]
  },
  {
    "objectID": "qmd/regression-other.html#sec-reg-other-misc",
    "href": "qmd/regression-other.html#sec-reg-other-misc",
    "title": "36¬† Other",
    "section": "",
    "text": "Harrell: It is not appropriate to compute a mean or run parametric regression on ‚Äú% change‚Äù unless you first compute log((%_change/100) + 1)¬† to undo damage done by % change\nReaction time data can be modelled using several families of skewed distributions (Lindel√∏v, 2019)",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>36</span>¬† <span class='chapter-title'>Other</span>"
    ]
  },
  {
    "objectID": "qmd/regression-other.html#sec-reg-other-other",
    "href": "qmd/regression-other.html#sec-reg-other-other",
    "title": "36¬† Other",
    "section": "Other",
    "text": "Other\n\nRates between 0 and 1\n\nOutcome without 0s and 1s\n\nBeta Regression\n\nOutcome with 0s and 1s\n\nOrdered Beta Regression\n\npaper, {ordbetareg}, {glmmTMB}\nHeiss overview of the model\nHeiss example using a 0,1 Inf Ordered Beta Regression\n\nZero-inflated or Zero-1-inflated beta regression (ZOIB) (see {brms})\nIf the zeros and ones are censored, use tobit.\n\n\nOutcome variable is greater than 0\n\nGamma Regression - Can handle some dispersion with a log link\nCan model multiplicative dgp\nIf zero-inflated, use Tweedie Regression\n\nBounded Outcome Variable\n\nOrdered Beta Regression\n\npaper, {ordbetareg}, {glmmTMB}\nHeiss overview of the model\nHeiss example using a 0,1 Inf Ordered Beta Regression to model an outcome with values between 1 and 32\n\n\nTweedie Regression - Where the frequency of events follows a Poisson distrbution and the amount associated with each event follows an Exponential distribution\n\ne.g.¬†Insurance claims, Operational loss (banking)\nTweedie distribution is a Gamma distribution with a spike at zero.\n\nGeneralized Least Squares\n\nPackages\n\n{nlme::gls}\n\nMath - A Deep-Dive into Generalized Least Squares Estimation\nAlso See Weighted Least Squares and Weighted Least Squares &gt;&gt; Feasible Generalized Least Squares",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>36</span>¬† <span class='chapter-title'>Other</span>"
    ]
  },
  {
    "objectID": "qmd/regression-other.html#sec-reg-other-censtrunc",
    "href": "qmd/regression-other.html#sec-reg-other-censtrunc",
    "title": "36¬† Other",
    "section": "Censored and Truncated Data",
    "text": "Censored and Truncated Data\n\nCensored Data - Arise if exact values are only reported in a restricted range. Data may fall outside this range but those data are reported at the range limits (i.e.¬†at the minimum or maximum of the range)\n\ni.e.¬†Instances outside the range are recorded in the data but the true values of those instances are not.\nTobit Regression - gaussian response, assumes constant (homoskedastic variance)\n\nLog-Likelihood function\n\\[\n\\ln \\mathcal{L}(\\beta, \\sigma^2) =\n\\sum_{i=1}^N \\left[d_i\\left(\\frac{1}{2}\\ln 2\\pi - \\frac{1}{2}\\ln \\sigma^2 - \\frac{1}{2\\sigma^2} (y_i - \\boldsymbol{x}_i'\\beta)^2\\right) + (1-d_i) \\ln \\left(1-\\Phi(\\frac{\\boldsymbol{x}_i' \\beta}{\\sigma}\\right)\\right]\n\\]\n\n\\(d_i = 0\\) if \\(y_i = 0\\), and \\(1\\) otherwise\n\n1st term OLS likelihood\n2nd term accounts for the probability that observation i is censored.\n\n\nMarginal Effect\n\\[\n\\frac{\\partial \\mathbb{E}(y|x)}{\\partial x} = \\beta_1 \\left[\\Phi \\left(\\frac{\\beta_0 + \\beta_1 x}{\\sigma}\\right) \\right]\n\\]\n\n\\(\\beta_1\\) is multiplied by the CDF (\\(\\Phi\\)) of the Normal distribution\n\n\\(\\beta_1\\) is weighted by the probability of y occurring at \\(x\\)\nExample: work_completed ~ hourly rate\n\n\\(\\beta_1\\) is weighted by the probability that an individual is willing to work at the present hourly rate, as represented by the CDF.\n\n\n\\(\\sigma\\) is the standard deviation of the model‚Äôs residuals\n\nExample: Right-Censored at 800\nVGAM::vglm(resp ~ pred1 + pred2, \n           family = tobit(Upper = 800), \n           data = dat)\n\n2-Part Models (e.g.¬†Hurdle Models) - A binary (e.g.¬†Probit) regression model fits the exceedance probability of the lower limit and a truncated regression model fits the value given the lower limit is exceeded.\n\nTruncated Data - Arise if exact values are only reported in a restricted range. If data outside this range are omitted completely, we call it truncated\n\ni.e.¬†Instances outside the range are NOT recorded. No evidence of these instances are in the data.\nTruncated Regression - Also assumes constant (homoskedastic variance)\nA poisson model will try to predict 0s even if 0s are impossible. Therefore, you need a zero-truncated model.\nThe truncated normal model is different from a glm, because Œº and œÉ are not orthogonal and have to be estimated simultaneously. Misspecification of one parameter will lead to inconsistent estimation of the other. That‚Äôs why for these models, not only is Œº often specified as a function of regressors but also œÉ, often in the framework of GAMLSS (generalized additive models of location, scale, and shape).\n\nExpectation: \\(E[y|x] = \\mu + \\sigma + \\frac {\\phi(\\mu / \\sigma)}{\\Phi(\\mu / \\sigma)}\\)\n\nWhere \\(\\phi(\\cdot)\\) and \\(\\Phi(\\cdot)\\) are the probability density and cumulative distribution function of the standard normal distribution, respectively. This intrinsically depends on both Œº and œÉ.\n\n\n\nHeteroskadastic Variance - The variance of an underlying normal distribution does depend on covariates\n\n{crch}\n\nExamples\n\nInsurance: There is a claim on a policy that has a payout limit of u and a deductible of d,\n\nAny loss amount that is greater than u will be reported to the insurance company as a loss of u - d¬† because that is the amount the insurance company has to pay.\nInsurance loss data is left-truncated because the insurance company doesn‚Äôt know if there are values below the deductible d because policyholders won‚Äôt make a claim.\n\n‚Äútruncated‚Äù because the values (claims) are below d, so the instances aren‚Äôt recorded in the data.\n‚Äúleft‚Äù because the values are below d and not above\n\nInsurance loss data is also right-censored if the loss is greater than u because u is the most the insurance company will pay. Thus, it only knows that your claim is greater than u, not the exact claim amount.\n\n‚Äúcensored‚Äù because the values (claims) that are exactly u (policy limit) imply that claim is greater than u, so the instances are recorded but the true values are unknown.\n‚Äúright‚Äù because the values are above u and not below\n\n\nMeasuring Wind Speed: The instrument needs a minimum wind speed, m, to start working.\n\nIf wind speeds below this minimum are recorded as the minimum value, m, the data is censored.\n\ni.e.¬†Some other instrument detects a wind instance occurred and that instance is recorded as m even though the true speed of the wind of that instance is unknown.\n\nIf wind speeds below this minimum are NOT recorded at all, the data is truncated.\n\ni.e.¬†Any wind instances (detected or not) are not recorded. No evidence in the data that these instances will have ever occurred.",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>36</span>¬† <span class='chapter-title'>Other</span>"
    ]
  },
  {
    "objectID": "qmd/regression-other.html#sec-reg-other-fracreg",
    "href": "qmd/regression-other.html#sec-reg-other-fracreg",
    "title": "36¬† Other",
    "section": "Fractional Regression",
    "text": "Fractional Regression\n\nOutcome with values between 0 and 1\n** Use a fractional logit (aka quasi-binomial) only for big data situations **\n\nThe fractional logit model is not a statistical distribution, leading it to produce biased results.\nSee Kubinec article\n\nRecommends ordered beta regression, continuous bernoulli and provides examples\n\nIn a big data situation, it respects the bounds of proporitional/fractional outcomes, and is significantly easier to fit than the other alternatives.\nHaving a large dataset means that inefficiency or an incorrect form for the uncertainty of fractional logit estimates is unlikely to affect decision-making or inference.\n\nBeta: values lie between zero and one\n\nSee {betareg}, {DirichletReg}, {mgcv}, {brms}\n{ordbetareg}\n\nZero/One-Inflated Beta: larger percentage of the observations are at the boundaries (i.e.¬†high amounts of 0s and 1s\n\nSee {brms}, {VGAM}, {gamlss}\n\nLogistic, Quasi-Binomial, or GAM w/robust std.errors: outcome, y, is \\(0 \\le y \\le 1\\) (i.e.¬†0s and 1s included)\n\nExample\nlibrary(lmtest)\nlibrary(sandwich)\n# logistic w/robust std.errors\nmodel_glm = glm(\n¬† prate ~ mrate + ltotemp + age + sole,\n¬† data = d,\n¬† family = binomial\n)\nse_glm_robust &lt;- coeftest(model_glm, vcov = vcovHC(model_glm, type=\"HC\"))\n# quasi-binomial w/robust std.errors\nmodel_quasi = glm(\n¬† prate ~ mrate + ltotemp + age + sole,\n¬† data = d,\n¬† family = quasibinomial\n)\nse_glm_robust_quasi = coeftest(model_quasi, vcov = vcovHC(model_quasi, type=\"HC\"))\n\n# Can also use a GAM to get the same results\n# Useful for more complicated model specifications\nmodel_gam_re = gam(\n¬† prate ~ mrate + ltotemp + age + sole + s(id, bs = 're'),\n¬† data = d,\n¬† family = binomial,\n¬† method = 'REML'\n)",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>36</span>¬† <span class='chapter-title'>Other</span>"
    ]
  },
  {
    "objectID": "qmd/regression-other.html#sec-reg-other-zeroinftrunc",
    "href": "qmd/regression-other.html#sec-reg-other-zeroinftrunc",
    "title": "36¬† Other",
    "section": "Zero-Inflated and Zero-Truncated",
    "text": "Zero-Inflated and Zero-Truncated\n\nContinuous\n\nSome economists will use \\(\\ln(1 + Y)\\) or \\(\\mbox{arcsinh}(Y)\\) to model a skewed, continous \\(Y\\) with \\(0\\)s. In this case, the treatment effects (ATE) can‚Äôt be interpreted as percents. The effect sizes will depend on the scale of \\(Y\\). (see Thread)\nSolutions\n\nNormalize \\(Y\\) by a pretreatment baseline\n\\[\nY' = \\frac{Y}{Y_{\\text{pre}}}\n\\]\n\nWhere \\(Y_{\\text{pre}}\\) is the measured \\(Y\\) prior to treatment\nIn regression, average treatment effect (ATE) would then be\n\\[\n\\theta_{\\bar Y} = \\mathbb{E} \\left[\\frac{Y(1)}{Y_{\\text{pre}}} - \\frac{Y(0)}{Y_{\\text{pre}}} \\right]\n\\]\n\nWhere \\(Y(1)\\) is the value of \\(Y\\) for treated subjects\nInterpretation (e.g outcome = earnings): average treatment effect on earnings expressed as a percentage of pre-treatment earnings\n\n\nNormalizing \\(Y\\) by the expected outcome given observable covariates\n\\[\nY' = \\frac{Y}{\\mathbb{E}[Y(0) \\:|\\: X]}\n\\]\n\n\\(Y(0)\\) are the observed outcome values for the control group\nThe ‚Äú\\(|X\\)‚Äù is kind of confusing but I don‚Äôt think want the fitted values from a model where the outcome is the \\(Y(0)\\) values. I think they‚Äôd use a \\(\\hat Y\\) somewhere.\n\nSo I think \\(\\mathbb{E}[Y(0) | X]\\) just the mean of the \\(Y(0)\\) values\nInterpretation of this transformed variable (e.g.¬†outcome = earnings)\n\nan individual‚Äôs earnings as a percentage of the average control group‚Äôs earnings for people with the same observable characteristics X.\n\n\nAverage Treatment Effect (ATE) Interpretation (e.g.¬†outcome = earnings, \\(X\\) = pretreatment earnings, education)\n\nThe average change in earnings as a percentage of the control group‚Äôs earnings for people with the same education and previous earnings.\n\n\n\n\nML 2-step Hurdle\n\nSteps\n\nTransform Target variable to 0/1 where 1 is any count that isn‚Äôt a 0.\nUse a classifier to predict 0s (according to some probability threshold)\n\nRemember that models is predicting the probability of being a 1\n\nFilter rows that aren‚Äôt predicted to be 0, and predict counts using a regressor model\n\nround-up or round-down predictions based on which results in lower error?\n\n\n\nStatistical\n\nOptions: Poission, Neg.Binomial, Zero-Inf Poisson/Neg.Binomial, Poisson/Neg.Binomial Hurdle\n\nZero-Inf Poisson/Neg.Binomial\n\n\nUses a second underlying process that determines whether a count is zero or non-zero. Once a count is determined to be non-zero, the regular Poisson process takes over to determine its actual non-zero value based on the Poisson process‚Äôs PMF.\nœïi is the predicted probability from a logistic regression that yi is a 0. This vector of values is then plugged into both probability mass functions.",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>36</span>¬† <span class='chapter-title'>Other</span>"
    ]
  },
  {
    "objectID": "qmd/regression-other.html#sec-reg-other-multmod",
    "href": "qmd/regression-other.html#sec-reg-other-multmod",
    "title": "36¬† Other",
    "section": "Multi-modal",
    "text": "Multi-modal\n\nAlso see EDA &gt;&gt; Interactions &gt;&gt; Categorical Outcome\n{upsetr} - Might be useful to examine bimodal structure and determine cutpoints based on categorical predictor values and not just outcome values\n{gghdr} - Visualization of Highest Density Regions in ggplot2\nTest for more than one mode: multimode::modetest(dat)\n\nPerforms Ameijeiras-Alonso excess mass test/dip statistic\nHa: More than 1 mode\n\nFind location of modes: multimode::locmodes(dat, mod0 = 2, display = TRUE)\n\n\nAnti-Mode location might be a good place for a cutpoint\n\nIdeas\n\nQuantile Regression\nMixture Model\nEstablish cutpoints and model each modal distribution separately\nML",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>36</span>¬† <span class='chapter-title'>Other</span>"
    ]
  },
  {
    "objectID": "qmd/regression-other.html#sec-reg-other-wls",
    "href": "qmd/regression-other.html#sec-reg-other-wls",
    "title": "36¬† Other",
    "section": "Weighted Least Squares (WLS)",
    "text": "Weighted Least Squares (WLS)\n\nOLS with Weighted Pbservations\nCommonly used to overcome binomial or ‚Äúmegaphone-shaped‚Äù types of heteroskedacity of OLS residuals\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMisc\n\nAlso see\n\nOther &gt;&gt; Generalized Least Squares\nReal Estate &gt;&gt; Appraisal Methods &gt;&gt; CMA &gt;&gt; Market Price &gt;&gt; Case-Shiller Method for an example\n\nResources\n\nR &gt;&gt; Documents &gt;&gt; Econometrics &gt;&gt; applied-econometrics-in-r-zeileis-kleiber &gt;&gt; pg 76\n\nFeasible Generalized Least Squares (FGLS) seems to have advantages over WLS Allows you to find the ‚Äúform of the skedastic function to use and‚Ä¶ estimate it from the data‚Äù * See Zeileis applied econometrics book or another econometrics book for details\n\nThe residual error to be minimized becomes:\n\\[\n\\begin{align}\nJ(\\hat \\beta) &= \\mathbb{E} [(f(x) - y)^2]\\\\\n& = \\sum_i^n (w_i f(x_i) - y_i)^2\\\\\n& = \\sum_i^n (w_ix_i^T \\beta - y_i)^2\n\\end{align}\n\\]\n\nWhere \\(w_i\\) is the weight assigned to observation \\(i\\)\n\n\\(\\hat \\beta\\) becomes \\((X^T WX)^{-1}XY\\)\n\nWhere \\(W\\) is a diagonal matrix containing the weights for each observation\n\nFor ‚Äúmegaphone-shaped‚Äù and binomial types of heteroskedacity, it‚Äôs common to set the weights to equal to each observation‚Äôs squared residual error\n\\[\nw_i = (f(x_i)-y_i)^2\n\\]",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>36</span>¬† <span class='chapter-title'>Other</span>"
    ]
  },
  {
    "objectID": "qmd/regression-other.html#sec-reg-other-gee",
    "href": "qmd/regression-other.html#sec-reg-other-gee",
    "title": "36¬† Other",
    "section": "Generalized Estimating Equations (GEE)",
    "text": "Generalized Estimating Equations (GEE)\n\nModels that are used when individual observations are correlated within groups. Often used when repeated measures (panel data) for an individual are collected over time.\n\nYou make a good guess on the within-subject covariance structure. The model averages over all subjects, and instead of assuming that data were generated from a certain distribution, it uses moment assumptions to iteratively choose the best Œ≤ to describe the relationship between covariates and response.\nA semiparametric method: while we impose some structure on the data generating process (linearity), we do not fully specify its distribution. Estimating Œ≤ is purely an exercise in optimization.\nLimitations\n\nLikelihood-based methods are not available for usual statistical inference. GEE is a quasi-likelihood method.\nUnclear on how to perform model selection, as GEE is just an estimating procedure. There is no goodness-of-fit measure readily available.\nNo subject-specific estimates; if that is the goal of your study, use a different method. (see below)\n\nOther option is Generalized Linear Mixed Model (GLMM), but GLMMs require some parametric assumptions (See Econometrics, Mixed Effects)\n\n*Note that the interpretations of the resulting estimates are different for GLMM and GEE*\n\nScenarios\n\nYou are a doctor. You want to know how much a statin drug will lower your patient‚Äôs odds of getting a heart attack.\n\nGLMM answers this question\n\nYou are a state health official. You want to know how the number of people who die of heart attacks would change if everyone in the at-risk population took the stain drug.\n\nGEE answers this question. GEE estimates population-averaged model parameters and their standard errors\n\n\n\nMisc\n\nNotes from\n\nGeneralized Estimating Equations (GEE)\n\nPackages\n\n{gee}: traditional implementations (only has a manual)\n{geepack}: traditional implementations (1 vignette)\n\nCan also handle clustered categorical responses\n\n{multgee}: GEE solver for correlated nominal or ordinal multinomial responses using a local odds ratios parameterization\n\nThe traditional GEE implementation has severe computation challenges and may not be possible when the cluster sizes (large numbers of individuals per cluster) get too large (e.g.¬†&gt;1000)\n\nUse One-Step Generalized Estimating Equations method (article with code)\n\nOperates under the assumption of exchangeable correlation (see below)\nCharacteristics\n\nMatches the asymptotic efficiency of the fully iterated GEE;\nUses a simpler formula to estimate the [intra-cluster correlation] ICC that avoids summing over all pairs;\nCompletely avoids matrix multiplications and inversions for computational efficiency\n\n\n\n\nAssumptions (similar to the assumptions for GLMs)\n\nThe responses \\(Y_1, Y_2, \\ldots , Y_n\\) are correlated or clustered\nThere is a linear relationship between the covariates and a transformation of the response, described by the link function, g.\nWithin-cluster covariance has some structure (‚Äúworking covariance‚Äù)\nIndividuals in different clusters are uncorrelated\n\nCovariance Structure\n\nNeed to pick one of these working covariance structures in order to fit the GEE\nTypes\n\nIndependence: observations over time are independent)\nExchangeable (aka Compound Symmetry): all observations over time have the same correlation\n\nCorrelation across individuals is constant within a cluster\nIntra-Cluster Correlation (ICC) is the measure of this correlation.\n\nAR(1): correlation decreases as a power of how many timepoints apart two observations are\n\nReasable if measurements taken closer together (i.e.¬†probably more highly correlated)\n\nUnstructured: correlation between all timepoints may be different)\n\nIf the wrong covariance structure is chosen, Œ≤ will be estimated consistently, even if the working covariance structure is wrong. However, the standard errors computed from this will be wrong.\n\nTo fix this, use Huber-White ‚Äúsandwich estimator‚Äù (HC standard errors) for robustness. (See Econometrics, General &gt;&gt; Standard Errors)\n\nThe idea behind the sandwich variance estimator is to use the empirical residuals to approximate the underlying covariance.\nProblematic if:\n\nThe number of independent subjects is much smaller than the number of repeated measures\nThe design is unbalanced ‚Äì the number of repeated measures differs across individuals\n\n\n\n\nExample: geepack\n\nDescription: How does Vitamin E and copper level in the feeds affect the weights of pigs?\nData\nlibrary(\"geepack\")\ndata(dietox)\ndietox$Cu &lt;- as.factor(dietox$Cu)\ndietox$Evit &lt;- as.factor(dietox$Evit)\nhead(dietox)\n##¬† ¬†  Weight¬† ¬† ¬† Feed Time¬† Pig Evit Cu Litter\n## 1 26.50000¬† ¬† ¬† ¬† NA¬† ¬† 1 4601¬† ¬† 1¬† 1¬† ¬† ¬† 1\n## 2 27.59999¬† 5.200005¬† ¬† 2 4601¬† ¬† 1¬† 1¬† ¬† ¬† 1\n## 3 36.50000 17.600000¬† ¬† 3 4601¬† ¬† 1¬† 1¬† ¬† ¬† 1\n## 4 40.29999 28.500000¬† ¬† 4 4601¬† ¬† 1¬† 1¬† ¬† ¬† 1\n## 5 49.09998 45.200001¬† ¬† 5 4601¬† ¬† 1¬† 1¬† ¬† ¬† 1\n## 6 55.39999 56.900002¬† ¬† 6 4601¬† ¬† 1¬† 1¬† ¬† ¬† 1\n\nWeight of slaughter pigs measured weekly for 12 weeks\nStarting weight (i.e.¬†the weight at week (Time) 1 and Feed = NA)\nCumulated Feed Intake (Feed)\nEvit is an indicator of Vitamin E treatment\nCu is an indicator of Copper treatment\n\nModel: Independence Working Covariance Structure\nmf &lt;- formula(Weight ~ Time + Evit + Cu)\ngeeInd &lt;- geeglm(mf, id=Pig, data=dietox, family=gaussian, corstr=\"ind\")\nsummary(geeInd)\n\n##¬† Coefficients:\n##¬† ¬† ¬† ¬† ¬† ¬† Estimate¬† Std.err¬† ¬† Wald Pr(&gt;|W|)¬† ¬†\n## (Intercept) 15.07283¬† 1.42190¬† 112.371¬† &lt;2e-16 ***\n## Time¬† ¬† ¬† ¬† 6.94829¬† 0.07979 7582.549¬† &lt;2e-16 ***\n## Evit2¬† ¬† ¬† ¬† 2.08126¬† 1.84178¬† ¬† 1.277¬† ¬† 0.258¬† ¬†\n## Evit3¬† ¬† ¬† -1.11327¬† 1.84830¬† ¬† 0.363¬† ¬† 0.547¬† ¬†\n## Cu2¬† ¬† ¬† ¬† -0.78865¬† 1.53486¬† ¬† 0.264¬† ¬† 0.607¬† ¬†\n## Cu3¬† ¬† ¬† ¬† ¬† 1.77672¬† 1.82134¬† ¬† 0.952¬† ¬† 0.329¬† ¬†\n## ---\n## Signif. codes:¬† 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n##¬†\n## Estimated Scale Parameters:\n##¬† ¬† ¬† ¬† ¬† ¬† Estimate Std.err\n## (Intercept)¬† ¬† 48.28¬† 9.309\n##¬†\n## Correlation: Structure = independenceNumber of clusters:¬† 72¬† Maximum cluster size: 12\n\ncorstr=‚Äúind‚Äù is the argument for the covariance structure - See article for examples of the other structures and how they affect estimates\n\nANOVA\nanova(geeInd)\n\n## Analysis of 'Wald statistic' Table\n## Model: gaussian, link: identity\n## Response: Weight\n## Terms added sequentially (first to last)\n##¬†\n##¬† ¬† ¬† Df¬† X2 P(&gt;|Chi|)¬† ¬†\n## Time¬† 1 7507¬† ¬† &lt;2e-16 ***\n## Evit¬† 2¬† ¬† 4¬† ¬† ¬† 0.15¬† ¬†\n## Cu¬† ¬† 2¬† ¬† 2¬† ¬† ¬† 0.41¬† ¬†\n## ---\n## Signif. codes:¬† 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "Regression",
      "<span class='chapter-number'>36</span>¬† <span class='chapter-title'>Other</span>"
    ]
  },
  {
    "objectID": "qmd/regression-ordinal.html#sec-reg-ord-me",
    "href": "qmd/regression-ordinal.html#sec-reg-ord-me",
    "title": "Ordinal",
    "section": "Mixed Effects",
    "text": "Mixed Effects\n\nAlso see Econometrics, Mixed Effects\nExample 1: Varying Intercepts Model\n\nModel\nclmm_mod &lt;-\n  clmm(\n    rating ~ temp + contact + (1|judge),\n    data = wine, \n    link = \"logit\"\n  )\n# older function\nclmm2_mod &lt;-\n  clmm2(\n    rating ~ temp + contact, \n    random = judge,\n    data = wine, \n    link = \"logistic\",\n    Hess = TRUE\n  )\nsummary(clmm_mod)\n\n#&gt;  link  threshold nobs logLik AIC    niter    max.grad cond.H \n#&gt;  logit flexible  72   -81.57 177.13 332(999) 1.03e-05 2.8e+01\n#&gt; \n#&gt; Random effects:\n#&gt;  Groups Name        Variance Std.Dev.\n#&gt;  judge  (Intercept) 1.279    1.131   \n#&gt; Number of groups:  judge 9 \n#&gt; \n#&gt; Coefficients:\n#&gt;            Estimate Std. Error z value Pr(&gt;|z|)    \n#&gt; tempwarm     3.0630     0.5954   5.145 2.68e-07 ***\n#&gt; contactyes   1.8349     0.5125   3.580 0.000344 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Threshold coefficients:\n#&gt;     Estimate Std. Error z value\n#&gt; 1|2  -1.6237     0.6824  -2.379\n#&gt; 2|3   1.5134     0.6038   2.507\n#&gt; 3|4   4.2285     0.8090   5.227\n#&gt; 4|5   6.0888     0.9725   6.261\n\nSee Proportional Odds &gt;&gt; Example 1 for variable descriptions\nSome extension functions work with clmm and others work with clmm2\n\nConfusingly, clmm is the newer function, but the {ordinal} package owner stopped in the middle of developing it.\n\n\nCompare with PO Model\n\nfm1_tbl &lt;- \n  gtsummary::tbl_regression(fm1, \n                            exponentiate = TRUE,\n                            intercept = TRUE)\nclmm_tbl &lt;- \n  gtsummary::tbl_regression(clmm_mod, \n                            exponentiate = TRUE,\n                            intercept = TRUE)\ngtsummary::tbl_merge(\n  tbls = list(fm1_tbl, clmm_tbl),\n  tab_spanner = c(\"**PO Model**\", \"**Mixed Effects PO Model**\")\n)\n\nanova(fm1, clmm_mod)\n#&gt;          formula:                              link: threshold:\n#&gt; fm1      rating ~ contact + temp               logit flexible  \n#&gt; clmm_mod rating ~ temp + contact + (1 | judge) logit flexible  \n#&gt; \n#&gt;          no.par    AIC  logLik LR.stat df Pr(&gt;Chisq)   \n#&gt; fm1           6 184.98 -86.492                         \n#&gt; clmm_mod      7 177.13 -81.565   9.853  1   0.001696 **\n\nThe mixed model produces the better fit.\nNot sure why the PO model threshold CIs weren‚Äôt calculated by {gtsummary}\n\nGet Profile CIs\nprofile(clmm2_mod,\n        nSteps = 30) |&gt; \n  confint()\n\n#&gt;           2.5 %   97.5 %\n#&gt; stDev 0.5014978 2.266741\n\nAlso see Confidence & Prediction Intervals &gt;&gt; Misc\nPackage has only implemented the profile method for the variance estimate of the mixed model and can‚Äôt be used with the newer clmm function\nYou either need to set Hess = TRUE in the model function or specify a range for the estimate in the profile function.\n\nGet Conditional Modes\n\ntibble(\n  judge_effect = clmm_mod$ranef,\n  cond_var = clmm_mod$condVar\n) |&gt; \n  mutate(\n    judge = fct_reorder(factor(1:n()), \n                        judge_effect),\n    conf.low = judge_effect - qnorm(0.975) * sqrt(cond_var),\n    conf.high = judge_effect + qnorm(0.975) * sqrt(cond_var)\n  ) |&gt; \n  ggplot(aes(y = judge, x = judge_effect)) +\n  geom_point(size = 2) +\n  geom_linerange(size = 1, \n                 aes(xmin = conf.low, xmax = conf.high)) +\n  theme(panel.grid.major.x = element_line(color = \"grey\"))\n\nWith the fixed effect variables at their reference/baseline levels",
    "crumbs": [
      "Regression",
      "Ordinal"
    ]
  },
  {
    "objectID": "qmd/eda-multilevel-longitudinal.html",
    "href": "qmd/eda-multilevel-longitudinal.html",
    "title": "Multilevel, Longitudinal",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "EDA",
      "Multilevel, Longitudinal"
    ]
  },
  {
    "objectID": "qmd/eda-multilevel-longitudinal.html#sec-eda-multlong-misc",
    "href": "qmd/eda-multilevel-longitudinal.html#sec-eda-multlong-misc",
    "title": "Multilevel, Longitudinal",
    "section": "",
    "text": "Also see Econometrics, Mixed Effects &gt;&gt; Considerations &gt;&gt; Variable Assignment\nNeed to figure out if\n\nThere‚Äôs significant within-unit variation. If so, then FE model will likely be the best model\n\nArticle with simulated data showed that within variation around sd &lt; 0.5 didn‚Äôt detect the effect of explanatory variable but ymmv (depends on # of units, observations per unit, N)\n\nThere‚Äôs significant between-unit variation. If so, then RE model will likely be the best model",
    "crumbs": [
      "EDA",
      "Multilevel, Longitudinal"
    ]
  },
  {
    "objectID": "qmd/eda-multilevel-longitudinal.html#sec-eda-multlong-mult",
    "href": "qmd/eda-multilevel-longitudinal.html#sec-eda-multlong-mult",
    "title": "Multilevel, Longitudinal",
    "section": "Multilevel",
    "text": "Multilevel\n\nMisc\n\nAlso see Regression, Ordinal &gt;&gt; EDA &gt;&gt; Crosstabs\nIs my data clustered?\nSeparate variables into levels\n\nLevel One: Variables measured at the most frequently occurring observational unit\n\ni.e.¬†Variables that (for the most part) have different values for each row\ni.e.¬†Vary for each repeated measure of a subject and vary between subjects\n\nLevel Two: Variables measured on larger observational units\n\ni.e.¬†Constant for each repeated measure of a subject but vary between each subject\n\n\n\n\n\nUnivariate\n\nLevel 1 and Level 2\n\nGroup-level correlation or autocorrelation in variables can mislead or obscure patterns\n\nIf level 2 variable categories are pretty well balanced and there‚Äôs sufficient data, then plotting means can remove the correlation affect in the plot\n\nContinuous\n\nLooking at the skew, median/mean, bimodal or not\nExample:\n\n\n(Top) Each observation is plotted as if each observation is independent of the other\n\n* Ignores dependency (via repeated measures)\n\n(Bottom) Means for each subject or case or other level of a random variable\n\n* Removes dependency\n\nInterpretation: Right skew remains in both plots but plot 1‚Äôs decrease is smoother than plot 2‚Äôs\n\n\nCategorical\n\nCalculate proportions of each category and noting trends (ordinal variables) or severe imbalances\n\n\n\n\n\nBivariate\n\nQuestions\n\nIs there is a general trend suggesting that as the covariate increases the response either increases or decreases (trend)\nDo subjects at certain levels of the covariate tend to have similar mean values of the response (low variability)\nIs the variation in the response at different levels of the covariate (unequal variability)\n\nMe: Comparison between plots that take into account dependency and the same plot that doesn‚Äôt\n\nTrend in plot that ignores dependency but no trend in plot that removes dependency\n\nMay indicate within-subject variation\n\nNo trend in plot that ignores dependency but trend in plot that removes dependency\n\nMay indicate between-subject variation\n\n\nBoxplots (Categorical)\n\n\nLevel 1 categorical covariates (y-axis) vs continuous outcome (x-axis)\n* Ignores dependency (via repeated measures)\nInterpretation\n\nLeft: ordinal covariate, medians are close and boxes pretty much contained within each other but there might be a trend\nRight: Looks like some decent variation between categories\n\nMean outcome (per subject) vs covariate\n\n\n* Removes dependency\nInterpretation: looks like some decent variation between categories\n\n\nScatter (Continuous)\n\n\nLevel 1 continuous covariate (x-axis) vs continuous outcome (y-axis)\n* Ignores dependency (via repeated measures)\nActually a discrete¬† covariate being treated as continuous the fact that does seem to be a small trend is what‚Äôs important\nMean outcome (per subject) vs covariate\n\n\n* Removes dependency\nInterpretation: PEM not showing much of an correlation if any\n\n\nFacetting previous plots by subject\n\n\n\nLeft\n\nMostly downward trends but some upward trends\nUseful for prior formulation\nGives an idea about the uncertainty of the slope of this variable\n\nRight\n\nScarcity of points for some categories makes boxplots a bad idea\nDifficult to spot any trends\n\n* Removes dependency\n\n\n\n\nTrivariate\n\nScatter, color by random variable\n\n\nVariables\n\n‚Äúpoints per 60 min‚Äù (outcome)\n‚Äútime on ice‚Äù (fixed effect)\nfacetted by ‚Äúposition‚Äù (fixed effect)\ncolored by ‚Äúplayer‚Äù (potential random variable)\n\nInterpretation\n\nTheres does seem to be clustering by ‚Äúplayer‚Äù therefore a mixed effects model might be a good choice.\n\n\nNull Model (aka random intercept-only model)\nm0 &lt;- \n  lmer(pp60 ~ 1 + (1 | player),¬†\n       data = df)\n\njtools::summ(m0)\nGROUPING VARIABLES\nGROUP # GROUPS ICC\nplayer ¬† ¬† ¬† 20¬† ¬† ¬† 0.89\nICC &gt; 0.1 is generally accepted as the minimal threshold for justifying the use of Mixed Effects model (See ICC section)",
    "crumbs": [
      "EDA",
      "Multilevel, Longitudinal"
    ]
  },
  {
    "objectID": "qmd/eda-multilevel-longitudinal.html#sec-eda-multlong-long",
    "href": "qmd/eda-multilevel-longitudinal.html#sec-eda-multlong-long",
    "title": "Multilevel, Longitudinal",
    "section": "Longitudinal",
    "text": "Longitudinal\n\nMisc\n\nRepeated measures that have a sequential or time component\npackages: {brolgar}\n\n\n\nUnivariate\n\nContinuous Outcome vs.¬†Time\n\nFacetted by Observational Unit (e.g.¬†school)\n\n\n\nLinear Fit and Line Chart\n\nSpaghetti\n\n\n\n\n\nBivariate\n\nBold line is the overall fit with LOESS\nContinuous Outcome vs Time\n\nFacetted by Categorical\n\nFacetted by Binned Continuous\n\n\nTime Endpoints\n\n\n‚ÄúSchool Type‚Äù is a categorical, level 2 variable and ‚ÄúMath Score‚Äù is the numeric outcome\nLooking for change from the initial measurement to the final measurement\n\n\n\n\nLinear parameters\n\nFit a linear regression for each subject/unit with its repeated measurements\n\nSee univariate &gt;&gt; numerical outcome vs.¬†time &gt;&gt; Facetted by observational unit &gt;&gt; (left) linear fit\n\nAdvantages\n\nEach unit‚Äôs/subject‚Äôs data points can be summarized with two summary statistics‚Äîan intercept and a slope\n\nBigger advantage when there are more observations over time per unit/subject\n\nSeems like a good way for using empirical bayes (i.e.¬†use these distributions for prior specifications)\n\nDisadvantages\n\nSlopes cannot be estimated for those units/subjects with just a single observation\nR-squared values cannot be calculated for those units/subjects with no variability in test scores during the time period\nR-squared values must be 1 for those units/subjects with only two test scores.\n\nSummary Statistics\n\nMean and SD for intercepts and slopes\n\nUnivariate\n\n\\(y_t = \\beta_0 + \\beta_1 t + \\epsilon_t\\)\n\n\\(t\\) is the time variable (aka trend)\n\nParameter Distributions\n\nCorrelation\n\n\nLower intitial values (intercepts) show the greatest growth (slopes) over time\nCorrelation = -0.32\n\n\nBivariate\n\nProcess\n\nFilter data by Level 2 variable\nFor each category of the Level 2 variable, fit a regression, yt = Œ≤0 + Œ≤1t + Œµt, for each unit/subject.\nAggregate results\n\nParameter Distributions\n\n\n‚ÄúSchool Type‚Äù is a Level 2, binary variable",
    "crumbs": [
      "EDA",
      "Multilevel, Longitudinal"
    ]
  },
  {
    "objectID": "qmd/shiny-mobile.html",
    "href": "qmd/shiny-mobile.html",
    "title": "Mobile",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Shiny",
      "Mobile"
    ]
  },
  {
    "objectID": "qmd/shiny-mobile.html#sec-shiny-mobile-misc",
    "href": "qmd/shiny-mobile.html#sec-shiny-mobile-misc",
    "title": "Mobile",
    "section": "",
    "text": "Notes from\n\nHow to Make Mobile Apps With Shiny\n\nBasic financial app using {shinyMobile} (Code)\n\n\nPackages\n\n{shinyMobile} - Integrates the Framework7 mobile app framework that‚Äôs designed for Mobile-First applications\n{apexcharter} - Fast interactive charts\n\nThe Server part of the Shiny app remains the same.\nApple App Store submission process involves setting up an Apple Developer account, paying the annual developer fee, and submitting your app for review.\nTroubleshooting\n\nProvisioning Profiles: Make sure your developer account is correctly set up, and you have the right provisioning profiles for development and distribution.\nApp Performance: Check that your app performs well on actual devices, not just in simulators or as a web app.",
    "crumbs": [
      "Shiny",
      "Mobile"
    ]
  },
  {
    "objectID": "qmd/shiny-mobile.html#sec-shiny-mobile-ui",
    "href": "qmd/shiny-mobile.html#sec-shiny-mobile-ui",
    "title": "Mobile",
    "section": "UI",
    "text": "UI\n\nLayout\n\n\nf7Page this sets up your mobile page\nf7TabLayout This is the type of mobile layout. I prefer the Tab layout.\n\nf7Navbar sets up the navigation bar for your mobile app\n\nf7Tabs These expose tabs at the bottom of the mobile screen. They are swipeable so the app actually feels like a mobile app.\n\nf7Tab: Each tab can be added that includes shiny content like inputs and outputs\n\nf7Card: I use cards to house pieces of content like plots (in this case my portfolio growth plot)",
    "crumbs": [
      "Shiny",
      "Mobile"
    ]
  },
  {
    "objectID": "qmd/shiny-mobile.html#sec-shiny-mobile-nc",
    "href": "qmd/shiny-mobile.html#sec-shiny-mobile-nc",
    "title": "Mobile",
    "section": "Native Containers",
    "text": "Native Containers\n\nTo to install an app on an iPhone, Android, etc. you need to wrap it in a native container.\nCapacitor\n\nWebsite\nVS Code Extension\nDependencies (Docs)\n\nAndroid\n\nAndroid Studio\nAn Android SDK installation\n\niOS\n\nXcode\nXcode Command Line Tools\nHomebrew\nCocoapods\n\n\nExample: iOS containter\n1npm install @capacitor/core @capacitor/cli\n2npx cap init MyApp &lt;appId&gt;\n3# Integrate Capacitor with Your Framework7 Project\n4npx cap add ios\n5npx cap copy\n\n1\n\nSet Up Capacitor\n\n2\n\nCreate Capacitor Project\n\n3\n\nIf your Framework7 project was set up using a specific build tool (like Webpack or Vite), follow the integration steps specific to that setup. Essentially, you‚Äôll be configuring Capacitor to use the output directory of your web app as its webDir.\n\n4\n\nAdd iOS as a Platform\n\n5\n\nBuild Your Project: First, build your Framework7 project, then update Capacitor with the latest web assets\n\n\n\nDocs show a slightly different install procedure which uses a -D flag for the cli install\nappId: Specifies the app‚Äôs identifier. The id should be in a reverse-DNS format however, only alphanumeric and dot characters are allowed. e.g: com.example.myapp. Maps to id attribute of widget element in capacitor.config.ts\nDocs show an alternative command, npx cap init, which takes you through as series of questions about your app so it can fill out your config\nIn order to add iOS (or Android), you have to install their platforms\nnpm i @capacitor/android @capacitor/ios\nDocs show npx cap sync instead of copy, but actually sync runs copy and update. So, it‚Äôs pretty much the same thing when you‚Äôre intiating an app. (Command Reference)\n\n\nApache Cordova\n\nWebsite\nList of Cordova requirements for each platform: cordova requirements\nExample: iOS container\n1npm install -g cordova\n2cordova create myapp &lt;id&gt; &lt;app title&gt;\ncd myapp\n3cordova platform add ios\n4# Add app files to www\n5cordova build ios\n\n1\n\nInstall Cordova\n\n2\n\nCreate a Cordova Project\n\n3\n\nAdd iOS as a Platform\n\n4\n\nPlace your Framework7 app files into the www directory of the newly created Cordova project ‚Äî replacing the existing files.\n\n5\n\nBuild the iOS App\n\n\n\nid: Specifies the app‚Äôs identifier. The id should be in a reverse-DNS format however, only alphanumeric and dot characters are allowed. e.g: com.example.myapp. Maps to id attribute of widget element in config.xml\n\napp title: Application‚Äôs display title that maps name element in config.xml file\n\nProject Structure\nmyapp/\n|-- config.xml\n|-- merges/\n| | |-- android/\n| | |-- ios/\n|-- www/\n|-- platforms/\n| |-- android/\n| |-- ios/\n|-- plugins/\n  |-- cordova-plugin-camera/",
    "crumbs": [
      "Shiny",
      "Mobile"
    ]
  },
  {
    "objectID": "qmd/regression-linear.html",
    "href": "qmd/regression-linear.html",
    "title": "Linear",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Regression",
      "Linear"
    ]
  },
  {
    "objectID": "qmd/regression-linear.html#sec-reg-lin-misc",
    "href": "qmd/regression-linear.html#sec-reg-lin-misc",
    "title": "Linear",
    "section": "",
    "text": "Guide for suitable baseline models: link\n‚ÄúFor estimation of causal effects, it does typically not suffice to well control for confounders, we also need a sufficiently strong source of exogenous variation for our explanatory variable of interest.‚Äù\n\nFrom Empirical Economics with R: Confounders, Proxies, and Sources of Exogenuous Variations\nEven if you have good proxy variables for the true confounder(s) and use them as controls, estimated effects of explanatory variables with low s.d. (with mean = 0, e.g.¬†low 0.02, high 1) will still be inflated If the regression conditions aren‚Äôt met - for instance, if heteroskedasticity is present - then the OLS estimator is still unbiased but it is no longer best. Instead, a variation called generalized least squares (GLS) will be Best Linear Unbiased Estimator (BLUE)\n\nA model is said to be hierarchical if it contains all the lesser degree terms in the hierarchy\n\nExample\n\nhierarchical: y = x + x2 + x3 + x4\nnot hierarchical: y = x + x4\nhierarchical: y = x1 + x2 + x1x2\nnot hierarchical: y = x1 + x1x2\n\nIt is expected that all polynomial models should have this property because only hierarchical models are invariant under linear transformation.\n\nRcppArmadillo::fastLmPure Not sure what this does but it‚Äôs rcpp so maybe faster than lm for big data.\n.lm.fit is a base R lm function that is 30%-40% faster than lm.",
    "crumbs": [
      "Regression",
      "Linear"
    ]
  },
  {
    "objectID": "qmd/regression-linear.html#sec-reg-lin-ass",
    "href": "qmd/regression-linear.html#sec-reg-lin-ass",
    "title": "Linear",
    "section": "Assumptions",
    "text": "Assumptions\n\nAssumptions\n\nThe true relationship is linear\n\ni.e.¬†a linear relationship between the independent variable, x, and the dependent variable, y.\n\nErrors are normally¬†distributed\n\ni.e.¬†random fluctuations around the true line\n\nHomoscedasticity of errors (or, equal variance around the line)\n\ni.e.¬†variability in the response doesn‚Äôt increase as the value of the predictor increases\n\nIndependence of the observations\n\ni.e.¬†no autocorrelation\n\n\nCheck residuals\n\nhave a constant variance (Homoscedasticity of errors)\nbe approximately normally distributed (with a mean of zero) (Errors are normally distributed)\nbe independent of one another (independence, no autocorrelation)\n\nGelman‚Äôs order of importance (post)\n\nValidity. Most importantly, the data you are analyzing should map to the research question you are trying to answer. This sounds obvious but is often overlooked or ignored because it can be inconvenient\nAdditivity and linearity. The most important mathematical assumption of the regression model is that its deterministic component is a linear function of the separate predictors\nIndependence of errors\nEqual variance of errors\nNormality of errors - ‚ÄúNormality and equal variance are typically minor concerns, unless you‚Äôre using the model to make predictions for individual data points.‚Äù",
    "crumbs": [
      "Regression",
      "Linear"
    ]
  },
  {
    "objectID": "qmd/regression-linear.html#sec-reg-lin-eda",
    "href": "qmd/regression-linear.html#sec-reg-lin-eda",
    "title": "Linear",
    "section": "EDA",
    "text": "EDA\n\nPreliminary¬†\n\nSet seed\nCheck for missing values\nSee how many predictors you‚Äôre allowed to use given the sample size. Check bkmk: statistics ‚Äì&gt; sample size folder for Harrell articles\nremove predictors with zero variance recipes::step_zv(all_predictors()).\n\nCheck correlation matrix. greybox::assoc()\nCheck s.d. in predictors\n\nwithout enough variation in predictors (with mean = 0, e.g.¬†low sd =¬† 0.02, high¬† = 1), effects will be inflated even after controlling for potential confounders\n\nLook at series of pairwise plots: see which independent variables are linearly related to your dependent variable as they would be good predictors. Also look for¬† non-linearity¬† relations. See which of your independent variables are linearly related to each other as this indicates collinearity. Use greybox::spread w/log = false for linear relationships and log = true for nonlinear¬†\nCollinearity can cause sign flips of coefficients as variables are added or taken away when we create our model. Use vif() and greybox::determ pg 121¬†\nIn pairwise for prospective predictors, look for and make note of potential outlier points.\nLook at histograms of variables to check for skewness. We want normal distributions but distributions that are all similarly skewed isn‚Äôt too bad either.\nLook at summary statistics and examine the difference between the median and the mean as this indicates skewness.\nSkewness metric?\nLook at box plots to check for skewness and outliers. Investigate at outliers to see if they can be removed as they can have an outsized effect that causes inaccuracy in your model.\nIs the outcome variable is multi-modal?\n\nRegression, Other\nSolutions:\n\nQuantile Regression (See Regression, Quantile)\nMixture models",
    "crumbs": [
      "Regression",
      "Linear"
    ]
  },
  {
    "objectID": "qmd/regression-linear.html#sec-reg-lin-tran",
    "href": "qmd/regression-linear.html#sec-reg-lin-tran",
    "title": "Linear",
    "section": "Transformations",
    "text": "Transformations\n\nMisc\n\nSee Feature Engineering, General &gt;&gt; Continuous &gt;&gt; Transformations\n\nStandardization\n\nMethods\n\n{effectsize}\n\nCan also be used to standardize coefficients after the model has been fit\n\nCan also be done by multiplying the coefficient by the sd of the variable\n\nGood docs - applications, reliability, etc. of the many methods for mixed models, interaction terms are explained.\n\nCentering and Scaling\nmedian and MAD\n\nrobust\n\nMean and (2 ‚®Ø std dev)\n\nGelman (2008)\nAllows for factor variables to also be compared to numeric variables in terms of importance\n\nMedian and IQR\nMean and Gini Mean Difference\n\n\nScaling\n\nvar/max(var)\n\nMaking the largest value = 1 and the theoretical minimum = 0\n\n\nSquaring (quadratic)\n\nPredictors should be standardized.\n\nSquaring variables can produce some very large numbers and the numerical estimations of the effects can be get screwed up.\nCentering fixes collinearity issues when creating powers and interaction terms (CV post)\n\nCollinearity between the created terms and the main effects\n\n\n\nLog\n\nTaking the log of a measure translates the measure into magnitudes. So by using the logarithm of a predictor, we‚Äôre saying that we suspect that the magnitude of the predictor is related to outcome, in a linear fashion.",
    "crumbs": [
      "Regression",
      "Linear"
    ]
  },
  {
    "objectID": "qmd/regression-linear.html#sec-reg-lin-contrasts",
    "href": "qmd/regression-linear.html#sec-reg-lin-contrasts",
    "title": "Linear",
    "section": "Contrasts",
    "text": "Contrasts\n\nMisc\n\nNotes from Centering in Moderation Analysis: A Guide\n\nThree Ways to Change Contrasts\n# 1. assign to the contrasts() function\ncontrasts(data$G) &lt;- contr.sum\n\n# 2. The C() function (BIG C!)\ndata$G &lt;- C(data$G, contr = contr.sum)\n\n# 3. In the model\nlm(Y ~ G,\n   data = data,\n   contrasts = list(G = contr.sum))\nTreatment Contrasts\n\nDefault contrasts used by R, stats::contr.treatment()\nFor a factor variable, it constructs each dummy variable to be 0 for all but the first level of the factor, with the 1st level being left out.\nExample: Dummy variables for a factor variable with 3 levels\ncontr.treatment(levels(data$G))\n#&gt;    g2 g3\n#&gt; g1  0  0\n#&gt; g2  1  0\n#&gt; g3  0  1\n\nWhere g1 is the reference level.\n\nExample: Interpretation for a factor variable with 3 levels\nparameters::model_parameters(model)\n#&gt; Parameter   | Coefficient |   SE |          95% CI |  t(7) |      p\n#&gt; -------------------------------------------------------------------\n#&gt; (Intercept) |       23.33 | 0.96 | [ 21.05, 25.61] | 24.21 | &lt; .001\n#&gt; G [g2]      |       -3.19 | 1.22 | [ -6.08, -0.31] | -2.62 | 0.034 \n#&gt; G [g3]      |       -6.83 | 1.52 | [-10.44, -3.23] | -4.49 | 0.003\n\n(predicted_means &lt;- predict(model, \n                            newdata = data.frame(G = c(\"g1\", \"g2\", \"g3\"))))\n#&gt;        1        2        3 \n#&gt; 23.33333 20.14000 16.50000\npredicted_means[2] - predicted_means[1]\n#&gt;         2 \n#&gt; -3.193333\npredicted_means[3] - predicted_means[1]\n#&gt;         3 \n#&gt; -6.833333\n\nShows how the dummy variable coefficients are the difference in predicted means between a given factor level and the reference level.\n\n\nSum-to-Zero Contrasts\n\nThe intercept represents the grand mean which is the average of all group means of the factor variable.\nExample: Factor variable with 3 levels\ncontr.sum(levels(data$G))\n#&gt;    [,1] [,2]\n#&gt; g1    1    0\n#&gt; g2    0    1\n#&gt; g3   -1   -1\nExample: Interpretation for a factor variable with 3 levels\nmodel_parameters(model7)\n#&gt; Parameter   | Coefficient |   SE |         95% CI |  t(7) |      p\n#&gt; ------------------------------------------------------------------\n#&gt; (Intercept) |       19.99 | 0.57 | [18.65, 21.33] | 35.35 | &lt; .001\n#&gt; G [1]       |        3.34 | 0.79 | [ 1.47,  5.22] |  4.21 | 0.004 \n#&gt; G [2]       |        0.15 | 0.71 | [-1.53,  1.83] |  0.21 | 0.840\n\n(predicted_means &lt;- predict(model6, \n                            newdata = data.frame(G = c(\"g1\", \"g2\", \"g3\"))))\n#&gt;        1        2        3 \n#&gt; 23.33333 20.14000 16.50000\npredicted_means[1] - mean(predicted_means)\n#&gt;        1 \n#&gt; 3.342222\npredicted_means[2] - mean(predicted_means)\n#&gt;         2 \n#&gt; 0.1488889\n\nEach factor level coefficient is the difference between that level‚Äôs group mean and the grand mean.\n\nExample: Continuous and Categorical Predictors With Interaction\nmodel12 &lt;- lm(Y ~ Q * A,\n              data = data,\n              contrasts = list(A = contr.sum))\n\nmodel_parameters(model12)\n#&gt; Parameter   | Coefficient |   SE |          95% CI |  t(6) |     p\n#&gt; ------------------------------------------------------------------\n#&gt; (Intercept) |        4.98 | 6.79 | [-11.63, 21.59] |  0.73 | 0.491\n#&gt; Q           |        4.27 | 1.91 | [ -0.39,  8.94] |  2.24 | 0.066\n#&gt; A [1]       |       -7.38 | 6.79 | [-23.99,  9.23] | -1.09 | 0.319\n#&gt; Q √ó A [1]   |        1.70 | 1.91 | [ -2.96,  6.37] |  0.89 | 0.406\n\n# Predict for all combinations of Q = [0, 1] and A = [a1, a2]\npred_grid &lt;- expand.grid(Q = c(0, 1), \n                         A = c(\"a1\", \"a2\"))\n(predicted_means &lt;- predict(model11, newdata = pred_grid))\n#&gt;         1         2         3         4 \n#&gt; -2.400173  3.574452 12.358596 14.929210\n\n# Average slope of X\nmean(c(predicted_means[2] - predicted_means[1], \n       predicted_means[4] - predicted_means[3]))\n#&gt; [1] 4.272619\n\n\\(Q\\) is continuous while \\(A\\) is a two-level factor variable.\nThe main effect for \\(Q\\) is called the Grand Average Slope.\n\n\nDeviation Contrasts\n\nCombination of Treatment Contrasts and Sum-to-Zero Contrasts\n\nCoefficients are the same as in Treatment Contrasts\nIntercept is the grand mean as in Sum-to-Zero Contrasts\n\nAvailable via {datawizard::contr.deviation}\nExample:\nmodel8 &lt;- lm(Y ~ G,\n             data = data,\n             contrasts = list(G = contr.deviation))\n\nmodel_parameters(model8)\n#&gt; Parameter   | Coefficient |   SE |          95% CI |  t(7) |      p\n#&gt; -------------------------------------------------------------------\n#&gt; (Intercept) |       19.99 | 0.57 | [ 18.65, 21.33] | 35.35 | &lt; .001\n#&gt; G [g2]      |       -3.19 | 1.22 | [ -6.08, -0.31] | -2.62 | 0.034 \n#&gt; G [g3]      |       -6.83 | 1.52 | [-10.44, -3.23] | -4.49 | 0.003\n\n(predicted_means &lt;- predict(model6, \n                            newdata = data.frame(G = c(\"g1\", \"g2\", \"g3\"))))\n#&gt;        1        2        3 \n#&gt; 23.33333 20.14000 16.50000\nmean(predicted_means)\n#&gt; [1] 19.99111\npredicted_means[2] - predicted_means[1]\n#&gt;         2 \n#&gt; -3.193333\npredicted_means[3] - predicted_means[1]\n#&gt;         3 \n#&gt; -6.833333",
    "crumbs": [
      "Regression",
      "Linear"
    ]
  },
  {
    "objectID": "qmd/regression-linear.html#sec-reg-lin-modsel",
    "href": "qmd/regression-linear.html#sec-reg-lin-modsel",
    "title": "Linear",
    "section": "Model Selection",
    "text": "Model Selection\n\nHarrell says use all variables unless number of variables , p &gt; (nrow(df) / 15), then use Regularized Regression\nSee Harrell sample size biostat paper (above). Has overview of approaches (I think).\nUse best subsets with Mallow‚Äôs CP, AIC, and BIC as metrics for choosing best model. See R notebook, pg 121 and bkmk regression \\(\\rightarrow\\) multivariable folder for PSU stat 501\nCheck for partial correlations between predictor variables and outcome variable (see notebook and bookmarks) as a possible avenue of feature reduction and as a way of gauging true strength of the relationship between the predictor variables and outcome variable",
    "crumbs": [
      "Regression",
      "Linear"
    ]
  },
  {
    "objectID": "qmd/regression-linear.html#sec-reg-lin-interp",
    "href": "qmd/regression-linear.html#sec-reg-lin-interp",
    "title": "Linear",
    "section": "Interpretation",
    "text": "Interpretation\n\nMarginal Effect - The effect of the predictor after taking into account the effects of all the other predictors in the model\nsummary\n\nStandard Errors: An estimate of how much estimates would ‚Äòbounce around‚Äô from sample to sample, if we were to repeat this process over and over and over again.\n\nMore specifically, it is an estimate of the standard deviation of the sampling distribution of the estimate.\n\nt-stat: Ratio of parameter estimate and its SE\n\nused for hypothesis testing, specifically to test whether the parameter estimate is ‚Äòsignificantly‚Äô different from 0.\n\np-value: The probability of finding an estimated value that is as far or further from 0, if the null hypothesis were true.\n\nNote that if the null hypothesis is not true, it is not clear that this value is telling us anything meaningful at all.\n\nF-statistic: This ‚Äúsimultaneous‚Äù test checks to see if the model as a whole predicts the response variable better than chance alone.\n\ni.e.¬†whether or not all the estimates should be considered unable to be differentiated from 0\nIf this statistic‚Äôs p-value &lt; 0.05, then this suggests that at least some of the parameter estimates are not equal to 0\nUnless you only have an intercept model, you have multiple tests (e.g.¬†variables + intercept p-values). There is no protection from the problem of multiple comparisons without this.\n\nBear in mind that because p-values are random variables‚Äìwhether something is significant would vary from experiment to experiment, if the experiment were re-run‚Äìit is possible for these to be inconsistent with each other.\n\n\nResidual Standard Error\n\\[\n\\hat \\sigma = \\frac{\\sum (e^2)}{\\mbox{dof}}\n\\]\n\n\\(\\mbox{dof}\\) is degrees of freedom which is number of rows of the dataset - number of parameters estimated. e.g.¬†\\(\\hat Y_i = \\beta_0 + \\beta_1X_i + e_i\\) would have two parameters to estimate (\\(\\beta_0\\), \\(\\beta_1\\)).\n\n\nIntercept\n\nIf variables are centered so that the predictors have mean 0, this is the expected value of Yi when the predictor values are set to their means.\nIf variables are not centered so that the predictors have mean 0, this is the expected value of Yi when the predictors are set to 0.\n\nmay not be a realistic or interpretable situation (e.g.¬†what if the predictors were height and weight?)\n\n\nStandardized Predictors\n\nDimensions are in common units (standard deviations), and the\nCoefficients\n\nSimilar to Pearson Correlation Coefficients (r-scores)\nCan‚Äôt be compared to other standardized coefficients to determine importance\n\nUnless the ‚Äútrue‚Äù r-score value is zero, the estimate is driven in large part by the range of values of the predictor in the sample\nThe best that can ever be said is that variability in one explanatory variable within a specified range is more important to determining the level of the response than variability in another explanatory variable within another specified range\n\n‚Äúvariability‚Äù makes sense since the units are standard deviations\nTrying to interpret these coefficients in terms of ‚Äúimportance‚Äù is probably not practically useful then.\n\nUnless the data is large and/or the range of the predictors is representative of the population\n\n\n\n\nDoes one coefficient have a larger effect than the other?\n\nExample: does extra hours work have a statistically larger effect on salary than does number_compliments_to_the_boss\n\nFrom Testing The Equality of Regression Coefficients\n\nlibrary(parameters)\nlibrary(effectsize)\ndata(\"hardlyworking\", package = \"effectsize\")¬†\nhardlyworkingZ &lt;- standardize(hardlyworking)\nm &lt;- lm(salary ~ xtra_hours + n_comps, data = hardlyworkingZ)¬†\nmodel_parameters(m)\n#&gt; Parameter¬† | Coefficient |¬† SE |¬† ¬† ¬† ¬† 95% CI |¬† ¬† t(497) |¬† ¬† ¬† p\n#&gt; ---------------------------------------------------------------------\n#&gt; (Intercept) |¬† -7.19e-17 | 0.02 | [-0.03, 0.03] | -4.14e-15 | &gt; .999\n#&gt; xtra_hours¬† |¬† ¬† ¬† ¬† 0.81 | 0.02 | [ 0.78, 0.84] |¬† ¬† 46.60 | &lt; .001\n#&gt; n_comps¬† ¬† |¬† ¬† ¬† ¬† 0.41 | 0.02 | [ 0.37, 0.44] |¬† ¬† 23.51 | &lt; .001\n\n# method 1\n# assume both variables have same coeff and test if the models are different\nm0 &lt;- lm(salary ~ I(xtra_hours + n_comps), data = hardlyworkingZ)¬†\nmodel_parameters(m0)\nanova(m0, m) # Yes they are different and therefore xtra_hours does have a stronger effect\n#&gt; Analysis of Variance Table\n#&gt;¬†\n#&gt; Model 1: salary ~ I(xtra_hours + n_comps)\n#&gt; Model 2: salary ~ xtra_hours + n_comps\n#&gt;¬† Res.Df¬† ¬† RSS Df Sum of Sq¬† ¬† ¬† F¬† ¬† Pr(&gt;F)¬† ¬†\n#&gt; 1¬† ¬† 498 113.67¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬†\n#&gt; 2¬† ¬† 497¬† 74.95¬† 1¬† ¬† 38.716 256.73 &lt; 2.2e-16 ***\n\n# method 2\nlibrary(car)¬†\nlinearHypothesis(m, c(\"xtra_hours - n_comps\"))\n#&gt; Linear hypothesis test\n#&gt;¬†\n#&gt; Hypothesis:\n#&gt; xtra_hours - n_comps = 0\n#&gt;¬†\n#&gt; Model 1: restricted model\n#&gt; Model 2: salary ~ xtra_hours + n_comps\n#&gt;¬†\n#&gt;¬† Res.Df¬† ¬† RSS Df Sum of Sq¬† ¬† ¬† F¬† ¬† Pr(&gt;F)¬† ¬†\n#&gt; 1¬† ¬† 498 113.67¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬†\n#&gt; 2¬† ¬† 497¬† 74.95¬† 1¬† ¬† 38.716 256.73 &lt; 2.2e-16 ***\n\n# method 3\nlibrary(emmeans)¬†\ntrends &lt;- rbind(\n¬† emtrends(m, ~1, \"xtra_hours\"),\n¬† emtrends(m, ~1, \"n_comps\")\n)¬†\n# clean up so it does not error later\ntrends@grid# Regression, Gaussian\n\n1` &lt;- c(\"xtra_hours\", \"n_comps\")\ntrends@misc$estName &lt;- \"trend\"\n\ntrends\n#&gt;¬† 1¬† ¬† ¬† ¬† ¬† trend¬† ¬† SE¬† df lower.CL upper.CL\n#&gt;¬† xtra_hours 0.811 0.0174 497¬† ¬† 0.772¬† ¬† 0.850\n#&gt;¬† n_comps¬† ¬† 0.409 0.0174 497¬† ¬† 0.370¬† ¬† 0.448\n#&gt;¬†\n#&gt; Confidence level used: 0.95¬†\n#&gt; Conf-level adjustment: bonferroni method for 2 estimates\n\npairs(trends)\n#&gt;¬† contrast¬† ¬† ¬† ¬† ¬† ¬† estimate¬† ¬† SE¬† df t.ratio p.value\n#&gt;¬† xtra_hours - n_comps¬† ¬† 0.402 0.0251 497 16.023¬† &lt;.0001\n\n\nQuadratic\n\n¬µi = Œ± + Œ≤1xi + Œ≤2xi2\n\nŒ± is the intercept. Tells us the expected value of ¬µ when x is 0\n\nŒ≤1 and Œ≤2 are linear and square components of the regression curve, respectively",
    "crumbs": [
      "Regression",
      "Linear"
    ]
  },
  {
    "objectID": "qmd/regression-linear.html#sec-reg-lin-rep",
    "href": "qmd/regression-linear.html#sec-reg-lin-rep",
    "title": "Linear",
    "section": "Report",
    "text": "Report\n\nOutliers\n\nList any outliers that were removed and reasons why.\nReport results with and without outliers",
    "crumbs": [
      "Regression",
      "Linear"
    ]
  },
  {
    "objectID": "qmd/shiny-mobile.html#xcode",
    "href": "qmd/shiny-mobile.html#xcode",
    "title": "Mobile",
    "section": "Xcode",
    "text": "Xcode\n\nApple‚Äôs IDE for creating native macOS, iOS, and iPadOS applications. You can install Xcode by¬†using the Apple App Store¬†on your Mac\nSet-Up\n\nInstall CLI and Verify Installation\nxcode-select --install\nxcode-select -p\n# /Applications/Xcode.app/Contents/Developer\n\nSteps\n\nOpen the Project in Xcode:\n\nFor Cordova: Open the .xcworkspace in the platforms/ios/ folder.\nFor Capacitor: Open the .xcworkspace inside the ios folder.\n\nRun the App: Connect your iPhone to your Mac, select your device in Xcode, and hit the run button to build and run the app on your device. You might need to configure your developer settings and provisioning profiles via Xcode if it‚Äôs your first time running an app on a physical device.",
    "crumbs": [
      "Shiny",
      "Mobile"
    ]
  },
  {
    "objectID": "qmd/econometrics-mixed-effects-frequentist.html",
    "href": "qmd/econometrics-mixed-effects-frequentist.html",
    "title": "Mixed Effects",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Econometrics",
      "Mixed Effects"
    ]
  },
  {
    "objectID": "qmd/econometrics-mixed-effects-frequentist.html#sec-econ-me-freq-misc",
    "href": "qmd/econometrics-mixed-effects-frequentist.html#sec-econ-me-freq-misc",
    "title": "Mixed Effects",
    "section": "",
    "text": "Mixed Effects Model = Random Effects model = Multi-Level model = Hierarchical model\n\nI‚Äôve placed this model under econometrics, but it‚Äôs not considered an ‚Äúeconometrics‚Äù model given how widely it‚Äôs utilized across many disiplines and the history of its development.\n\nResources\n\nBayesian Generalized Linear Mixed Effects Models for Deception Detection Analyses\n\nPaper that‚Äôs an in-depth tutorial. Uses {brms}, {emmeans}, {parameters}\n\nMixed Models in R by Michael Clark\n\nPackages\n\n{lme4} - linear and generalized linear mixed-effects models; implemented using the ‚ÄòEigen‚Äô C++ library for numerical linear algebra and ‚ÄòRcppEigen‚Äô ‚Äúglue‚Äù.\n\nUsing {lmerTest} will produce a summary of lme4 models with pvals and dofs for coefficients\n\n{multilevelmod} - tidymodels wrapper for many mixed model packages.\n\n{tidymodels} workflows (optional outputs: lmer, glmer, stan_glmer objects)\n\n{plm} - Linear models for panel data in an econometrics context; including within/fixed effects, random effects, between, first-difference, nested random effects. Also includes robust covariance estimators (e.g Newey-West), and model tests.\n\nFormula syntax isn‚Äôt typical R format. The econometric estimators are unfamilar and will need extra research to figure out what they are, do, and how to use them appropriately.\n\n{glmmTMB} - for fitting generalized linear mixed models (GLMMs) and extensions\n\nWide range of statistical distributions (Gaussian, Poisson, binomial, negative binomial, Beta ‚Ä¶) and zero-inflation.\nFixed and random effects models can be specified for the conditional and zero-inflated components of the model, as well as fixed effects for the dispersion parameter.\n\n{spaMM} - Inference based on models with or without spatially-correlated random effects, multivariate responses, or non-Gaussian random effects (e.g., Beta).\n{merTools} - Allows construction of prediction intervals efficiently from large scale linear and generalized linear mixed-effects models\n\nAlso has tools for analyzing multiply imputed mixed efffects models\nComes with a shiny app to explore the model\n\n\nAdvantages of a mixed model (\\(y \\sim x + (x \\;|\\; g)\\)) vs a linear model with an interaction (\\(y \\sim x \\ast g\\))\n\nFrom T.J. Mahr tweet\nConceptual: Assumes participant means are drawn from the same latent population\nComputational: partial pooling / smoothing\nBoth will have very similar parameter estimates when the data is balanced with few to no outliers\n\nLinear least squares regression can overstate precision, producing t-statistics for each fixed effect that tend to be larger than they should be; the number of significant results in LLSR are then too great and not reflective of the true structure of the data\nStandard errors are underestimated in the interaction model.\n\nDoesn‚Äôt account for dependence in the repeated measure for each subject\n\nFor unbalanced data w/some group categories having few data points or with outliers, the mixed effects model regularizes/shrinks/smooths the estimates to the overall group (i.e.¬†population) mean\n\nExample of model formula interpretation\n\n\nMixed Effects and Longitudinal Data\n\nHarrell (article)\n\nSays that Mixed Models models can capture within-subject correlation of repeated measures over very short time intervals but not over extended time intervals where autocorrelation comes into play\n\nExample of a short interval is a series of tests on a subject over minutes when the subject does not fatigue\nExample of a long interval is a typical longitudinal clinical trial where patient responses are assessed weekly or monthly\n\nHe recommends a Markov model for longitudinal RCT data (see bkmks)\n\nBartlett\n\nMixed model repeated measures (MMRM) in Stata, SAS and R\n\nTutorial; uses {nlme::gls}",
    "crumbs": [
      "Econometrics",
      "Mixed Effects"
    ]
  },
  {
    "objectID": "qmd/econometrics-mixed-effects-frequentist.html#sec-econ-me-freq-consid",
    "href": "qmd/econometrics-mixed-effects-frequentist.html#sec-econ-me-freq-consid",
    "title": "Mixed Effects",
    "section": "Considerations",
    "text": "Considerations\n\nMotivation for using a Random Effects model\n\nYou think the little subgroups are part of some bigger group with a common mean effect\n\ne.g.¬†Multiple observations of a single person, or multiple people in a school, or multiple schools in a district, or multiple varieties of a single kind of fruit, or multiple kinds of vegetable from the same harvest, or multiple harvests of the same kind of vegetable, etc.\n\nThese subgroup means significantly deviate from the big group mean.\n\ni.e.¬†We suspect there to be between-subject variance and not just within-subject variance (linear model).\n\n\n\n\n\n\n\n\nWithin-Subject\n\n\n\n\n\n\n\nBetween-Subject\n\n\n\n\n\n\n\nThe Between-Subject Variance isn‚Äôt clearly depicted IMO. The Between-Subject Variance captures the differences between individual subjects or groups in their average responses or outcomes. It can be understood better by looking at the sample formula where the grand mean is subtracted from each subject‚Äôs mean.\n\\[\n\\text{Between-Subject Variance} = \\frac{\\sum_{i=1}^{k} n_i (\\bar{X}_i - \\bar{X})^2}{k-1}\n\\]\n\n\\(k\\) is the number of groups or levels of the independent variable.\n\nDenominator is a dof term.\n\n\\(n_i\\) is the number of observations in the ith group.\n\\(\\bar X_i\\) is the mean of the observations in the ith group.\n\\(\\bar X\\) is the grand mean.\n\n\n\nThese deviations follow a distribution, typically Gaussian.\n\nThat‚Äôs where the ‚Äúrandom‚Äù in random effects comes in: we‚Äôre assuming the deviations of subgroups from a parent follow the distribution of a random (Gaussian) variable\n\nThe variation between subgroups is assumed to have a normal distribution with a 0 mean and constant variance (variance estimated by the model).\n\nGEEs are a semi-parametric option for panel data (See Regression, Other &gt;&gt; Generalized Estimating Equations (GEE))\n\n\nFixed Effects or Random Effects?\n\nIf there‚Äôs likely correlation between unobserved group/cases variables (e.g.¬†individual talent) and treatment variable (i.e.¬†E(Œ±|x) != 0) AND there‚Äôs substantial variance between group units, then FE is a better choice (See Econometrics, Fixed Effects &gt;&gt; One-Way Fixed Effects &gt;&gt; Assumptions)\nIf cases units change little, or not at all, across time, a fixed effects model may not work very well or even at all (SEs for a FE model will be large)\n\nThe FE model is for analyzing within-units variance\n\nDo we wish to estimate the effects of variables whose values do not change across time, or do we merely wish to control for them?\n\nFE: these effects aren‚Äôt estimated but adjusted for by explicitly including a separate intercept term for each individual (Œ±i) in the regression equation\nRE: estimates these effects (might be biased if RE assumptions violated)\n\nThe RE model is for analyzing between-units variance\n\n\nThe amount of within-unit variation relative to between-unit variation has important implications for these two approaches\n\nArticle with simulated data showed that within variation around sd &lt; 0.5 didn‚Äôt detect the effect of explanatory variable but ymmv (depends on # of units, observations per unit, N)\n\nDurbin‚ÄìWu‚ÄìHausman test ({plm::phtest})\n\nIf H0 is not rejected, then both FE and RE are consistent but only RE is efficient. \\(\\rightarrow\\) use RE but if you have a lot of data, then FE is also fine.\nIf H0 is rejected, then only FE is consistent \\(\\rightarrow\\) use FE\n\nICC &gt; 0.1 is generally accepted as the minimal threshold for justifying the use of Mixed Effects Model (See Diagnostics &gt;&gt; ICC)\n\nPooling\n\nComplete pooling - Each unit is assumed to have the same effect\n\nExample: County is the grouping variable and radon level is the outcome\n\nAll counties are alike.\n\ni.e.¬†all characteristics of counties that affect radon levels in houses have the statistically same effect across counties. Therefore, the variable has no information.\n\nRun a single regression to estimate the average radon level in the whole state.\n\nlm(radon_level ~ predictors)\nNote that ‚Äúcounty‚Äù is NOT a predictor in this model\n\n\n\nNo pooling - All units are assumed to have independent effects\n\nExample: County is the grouping variable (although not a predictor in this case) and radon level is the outcome\n\nAll counties are different from each other.\n\ni.e.¬†there are no common characteristics of counties that affect radon levels in houses. Any characteristic a county has that affects radon levels is unique to that county.\n\nRun a regression for each county to estimate the average radon level for each county.\n\nlm(radon_level ~ 0 + county + predictors\nUsing the ‚Äú0 +‚Äù formula removes the common intercept which means each county will get it‚Äôs own intercept\n\n\n\nPartial pooling - Each unit is assumed to have a different effect, but the data for all of the observed units informs the estimates for each unit\n\nExample: County is the grouping variable (random effect) and radon level is the outcome\n\nAll counties are similar each other.\n\ni.e.¬†all charcteristics of counties that affect radon levels in house have statistically varying effects sizes depending on the particular county\n\nRun a multi-level regression to share information across counties.\n\nlmer(radon_level ~ predictors + (1 + predictor | county))\n\n\nThis can be a nice compromise between estimating an effect by completely pooling all groups, which masks group-level variation, and estimating an effect for all groups completely separately, which could give poor estimates for low-sample groups.\nIf you have few data points in a group, the group‚Äôs effect estimate will be based partially on the more abundant data from other groups. (2 min video)\n\nThis is called Regularization. Subjects with fewer data points will have their estimates pulled to the grand mean more than subjects with more data points.\n\nThe grand mean will be the (Intercept) value in the Fixed Effects section of the summary.\n\nYou can visualize the amount of regularization by comparing the conditional modes (see Model Equation &gt;&gt; Conditional Modes) to the group means in the observed data.\n\nExample (link)\n\n\n\nCode\n\nrt_lme_mod &lt;- \n  lmer(rt ~ 1 + (1|subid), \n       dat)\n\ncomparison_dat &lt;- \n  dat |&gt;\n  summarize(avg_rt = mean(rt), .by = subid) |&gt; \n  mutate(cond_modes = coef(rt_lme_mod)$subid[[1]]) |&gt; \n  tidyr::pivot_longer(\n    cols = c(avg_rt, cond_modes),\n    names_to = \"type\",\n    values_to = \"estimate\"\n  )\n\nggplot(comparison_dat,\n       aes(x = subid, y = estimate, color = type)) +\n  geom_point() +\n  scale_color_manual(values = c(\"#195198\", \"#BD9865\")) +\n  geom_hline(yintercept = lme4::fixef(rt_lme_mod)[[1]]) + # overall intercept/grand mean\n  scale_x_continuous(breaks = seq(0 , 10, 1), \n                     minor_breaks=seq(0, 10,1)) +\n  theme_notebook()\n\n\nThe conditional modes (gold) are all closer to the horizontal line which represents the grand mean.\nThere are few data points for each of the first five subjects, so in general, they are pulled more than the last five subjects which have many more data points.\n\ni.e.¬†The differences between the observed means (blue) and conditional modes (gold) are greater in the first five subjects than the last five subjects\nPoints already close to the grand mean can‚Äôt be pulled as much since they‚Äôre already very close to the grand mean (e.g.¬†subject 1)\n\n\n\n\nPartial pooling is typically accomplished through hierarchical models. Hierarchical models directly model the population of units. From a population model perspective, no pooling corresponds to infinite population variance, whereas complete pooling corresponds to zero population variance.\n\n\nVariable Assignment\n\nQuestions (article has examples)\n\nCan the groups we see be considered the full population of possible groups we could see, or are they more like a random sample of a larger set of groups?\n\nFull Population: Fixed\nRandom Sample: Random\n\nDo we want to estimate a coefficient for this grouping, or do we just want to account for the anticipated structure that the groups may impose on our observations?\n\nY: Fixed\nN: Random\n\n\nFixed Effects provide estimates of mean-differences or slopes.\n\n‚ÄúFixed‚Äù because they are effects that are constant for each subject/unit\nShould include within-subject variables (e.g.¬†random effect slope variables) and between-subject variables (e.g.¬†gender)\nLevel One: variables measured at the most frequently occurring observational unit\n\ni.e.¬†Vary for each repeated measure of a subject and vary between subjects\n\nIn the dataset, these variables that (for the most part) have different values for each row\nTime-dependent if you have longitudinal data\n\nFor a RE model, these are usually the adjustment variables\n\ne.g.¬†Conditioning on a confounder\n\n\nLevel Two: variables measured at the observational unit level\n\ni.e.¬†Constant for each repeated measure of a subject but vary between each subject\nFor a RE model, these are usually the treatment variables or variables of interest\n\nThey should contain the information about the between-subject variation\n\nIf a factor variable, it has levels which would not change in replications of the study\n\n\nRandom Effects estimate of variation between and within subgroups\n\nIntercepts variables (e.g.¬†v1 in Specification and Notation) should be grouping variables (i.e.¬†discrete or categorical) (e.g school, department)\nSlopes variables (e.g.¬†v2 in Specification and Notation) should be continuous within-subject variables (e.g time)\n\nThese variables should also be included in the fixed effects\n\nQuantifies how much of the overall variation can be attributed to that particular variable.\n\nExample: the variation in beetle DAMAGE was attributable to the FARM at which the damage took place, so you‚Äôd cluster by FARM (1|FARM)\n\nIf you want slopes to vary according to a variable, the variation of slopes between-units will be a random effect\n\nUsually a level 2 fixed effect variable\n\nIf you want intercepts to vary according to a variable, the variation of intercepts between-units will be a random effect\n\nThis will the unit/subject variable (e.g student id, store id) that has the repeated observations\n\nTypically categorical variables that we are not interested in measuring their effects on the outcome variable, but we do want to adjust for. This variation might capture effects of latent variables.\n\nThis factor variable has levels which can be thought of as a sample from a larger population of factor levels (e.g.¬†hockey players)\nExample: 2 hockey players both averaged around 20 minutes per game last year (fixed variable). Predictions of the amount of points scored by just accounting for this fixed variable would produce similar results. But using PLAYER as a random variable will capture the impact of persistent characteristics that might not be observable elsewhere in the explanatory data. PLAYER can be thought of as a proxy for ‚Äúoffensive talent‚Äù in a way.\n\nIf the values of the variable were chosen at random, then you should cluster by that variable (i.e.¬†choose as the random variable)\n\nExample: If you can rerun the study using different specific farms (i.e .different values of the FARM factor, see above) and still be able to draw the same conclusions, then FARM should be a random effect.\n\nHowever, if you had wanted to compare or control for these particular farms, then Farm would be ‚Äúfixed.‚Äù\nSay that there is nothing about comparing these specific fields that is of interest to the researcher. Rather, the researcher wants to generalize the results of this experiment to all fields. Then, FIELD would be ‚Äúrandom.‚Äù\n\n\nIf the random effects are correlated with variables of interest (fixed effects), leaving them out could lead to biased fixed effects. Including them can help more reliably isolate the influence of the fixed effects of interest and more accurately model a clustered system.\nTo see individual random effects: lme4::ranef(lme_mod) or lme4::ranef(tidy_mod$fit)",
    "crumbs": [
      "Econometrics",
      "Mixed Effects"
    ]
  },
  {
    "objectID": "qmd/econometrics-mixed-effects-frequentist.html#sec-econ-me-freq-assum",
    "href": "qmd/econometrics-mixed-effects-frequentist.html#sec-econ-me-freq-assum",
    "title": "Mixed Effects",
    "section": "Assumptions",
    "text": "Assumptions\n\nNo Time-Constant Unobserved Heterogeneity: \\(\\mathbb{E}(\\alpha_i\\;|\\;x_{it}) = 0\\)\n\ni.e.¬†No correlation between time-invariant valued, unobserved variables (aka random effect variable) and the explanatory variable of interest (e.g.¬†treatment)\n\nTime-Invariant Valued, Unobserved Variables: Variables that are constant across time for each unit and explain variation between-units\n\ne.g.¬†If random effect variable (aka clustering variable) is the individual, then the ‚Äútime-invariant, unobserved‚Äù variable could be something latent like talent or something measureable like intelligence or socio-economic status. Whatever is likely to explain the variance between-units in relation to the response.\n\n\nThe effect that these variables have must also be time-invariant\n\ne.g.¬†The effect of gender on the outcome at time 1 is the same as the effect of gender at time 5\nIf effects are time-varying, then an interaction of gender with time could be included\n\nIt‚Äôs not reasonable for this to be exactly zero in order to use a mixed effected model, but for situations where there‚Äôs high correlation, this model should be avoided\nExample: Violation\n\n\nEach group‚Äôs x values get larger from left to right as each group‚Äôs Œ± (aka y-intercepts) for each unit get larger\n\ni.e.¬†Mixed-Effects models fail in cases where there‚Äôs very high correlation between group intercepts and x, together with large between-group variability compared to the within-group variability\n\n**FE model would be better in this case\n\nGelman has a paper that describes how a mixed effect model can be fit in this situation though\n\n\n\nNo Time-Varying Unobserved Heterogeneity: \\(\\mathbb{E}(\\epsilon_{it}|x_{it}) = 0\\)\n\ni.e No endogeneity (no correlation between the residuals and explanatory variable of interest)\nIf violated, there needs to be explicit measurements of omitted time-invariant variables (see 1st assumption for definition) if they are thought to interact with other variables in the model.",
    "crumbs": [
      "Econometrics",
      "Mixed Effects"
    ]
  },
  {
    "objectID": "qmd/econometrics-mixed-effects-frequentist.html#sec-econ-me-freq-specnot",
    "href": "qmd/econometrics-mixed-effects-frequentist.html#sec-econ-me-freq-specnot",
    "title": "Mixed Effects",
    "section": "Specifications and Notation",
    "text": "Specifications and Notation\n\nEvery variable within the parentheses is for the random effects. Variables outside the parentheses are fixed effects.\nVarying Intercepts: (1 | v1)\n\nAKA Random Intercepts, where each level of the random variable (our random effect) had an adjustment to the overall intercept\nExample: Each department (random variable) has a different starting (intercepts) salary (outcome variable) for their faculty members (observations), while the annual rate (slope) at which salaries increase is consistent across the university (i.e.¬†effect is constant between-departments)\n\n\\[\n\\widehat{\\text{salary}}_i = \\beta_{0 j[i]} + \\beta_1 \\cdot \\text{experience}_i\n\\]\n\nWhere j[i] is the index for department\nThis strategy allows us to capture variation in the starting salary (intercepts) of our faculty\n\n\nVarying Slopes: (0 + v2| v1)\n\nAKA Random Slopes, which would allow the effect of the v2 to vary by v1\n\ni.e.¬†v2 is a predictor whose effect varies according to the level of the grouping variable, v1.\n\ni.e.¬†A slope for each level of the random effects variable\nExample: faculty salary (outcome) increase at different rates (slopes) depending on the department (random variable).\n\n\\[\n\\widehat{\\text{salary}}_i = \\beta_0 + \\beta_{1 j[i]} \\cdot \\text{experience}_i\n\\]\n\nWhere j[i] is the index for department\nThis strategy allows us to capture variation in the change (slopes) in salary\n\n\nVarying Slopes and Intercepts: (1 + v2 | v1) or just (v2 | v1) (See examples)\n\nSee above for descriptions of each\nExample: Each department (random variable) has a different starting (intercepts) salary (outcome variable) for their faculty members (observations), while the annual rate (slope) at which salaries increase varies depending on department\n\n\\[\n\\widehat{\\text{salary}}_i = \\beta_{0 j[i]} + \\beta_{1 j[i]} \\cdot \\text{experience}_i\n\\]\n\nWhere j[i] is the index for department\nSee above for descriptions of each type of variation this strategy captures\n\n\nNested Effects\n\ne.g.¬†Studying test scores (outcome variable) within schools (random variable) that are within districts (random variable)\n*For {lme4}, using Nested or Crossed notation is NOT important for parameter estimation. Both specifications will produce exactly the same results.*\n\nSee link for more details and references\nThe only situation where it does matter is if the nested variable (e.g.¬†v2) does not have unique values.\n\nFor example, if wards are nested within hospitals and there‚Äôs a ward named 11 that‚Äôs in more than one hospital except that they‚Äôre obviously different wards.\nIn that situation, you can either create a unique ward id or use the nested notation.\n\n\nNotation: (1 | v1 / v2) or (1 | v1) + (1 | v1:v2)\n\nSays intercepts varying among v1 and v2 within v1.\ne.g.¬†schools is v2 and districts is v1\n\nUnit IDs may be repeated within groups.\n\nExample: lme4::lmer(score ~ time  + (time | school/student_id))\n\nThe random effect at this level is the combination of school and student_id, which is unique.\nYou can verify this by calling ranef on the fitted model. It‚Äôll show you the unique combinations of student_ids within schools used in the model.\n\n\n\nCrossed Effects\n\nUse when you have more than one grouping variable, but those grouping variables aren‚Äôt nested within each other.\n*For {lme4}, using Nested or Crossed notation is NOT important for parameter estimation. Both specifications will produce exactly the same results.*\n\nSee link for more details and references\nThe only situation where it does matter is if the nested variable (e.g.¬†v2) does not have unique values.\n\nFor example, if wards are nested within hospitals and there‚Äôs a ward named 11 that‚Äôs in more than one hospital except that they‚Äôre obviously different wards.\nIn that situation, you can either create a unique ward id or use the nested notation.\n\n\nSuch models are common in item response theory, where subject and item factors are fully crossed\n\nTwo variables are said to be ‚Äúfully crossed‚Äù when every combination of their levels appears in the data.\nWhen variables are fully crossed, it means that their effects on the response variable can be estimated independently and without any confounding or aliasing. This allows you to properly disentangle and interpret the main effects and potential interactions between the variables.\n\nNotation: (1 | v1) + (1 | v2) says there‚Äôs an intercept for each level of each grouping variable.\n\nUncorrelated Random Effects\n\nIf you have model where slopes and intercepts aren‚Äôt very correlated and would like to reduce the complexity (i.e.¬†fewer dofs), then specifying an uncorrelated model means fewer parameters have to be estimated which lowers your degrees of freeedom.\n\nThis can also be a potential solution to nonconvergence issues.\n\nNotation: (x || g) or (1 | g) + (0 + x | g)\nIdeally these models should be restricted to cases where the predictor is measured on a ratio scale (i.e., the zero point on the scale is meaningful, not just a location defined by convenience or convention).\n\nThis is from a {lme4} vignette. They used a Day variable that was used as a varying slope variable as an example that meets the condition since Day = 0 is meaningful (even though it‚Äôs not a ratio I guess).",
    "crumbs": [
      "Econometrics",
      "Mixed Effects"
    ]
  },
  {
    "objectID": "qmd/econometrics-mixed-effects-frequentist.html#sec-econ-me-freq-strat",
    "href": "qmd/econometrics-mixed-effects-frequentist.html#sec-econ-me-freq-strat",
    "title": "Mixed Effects",
    "section": "Strategy",
    "text": "Strategy\n\nMisc\n\nAlso see\n\nBMLR Ch 8 &gt;&gt; Model Building Workflow: Simple to Complex\nModel Building, Concepts &gt;&gt; Misc\n\nBegins with a saturated fixed effects model, determines variance components based on that, and then simplifies the fixed part of the model after fixing the random part.\n\nOverall:\n\nEDA\nFit some simple, preliminary models, in part to establish a baseline for evaluating larger models.\nThen, build toward a final model for description and inference by attempting to add important covariates, centering certain variables, and checking model assumptions.\n\nProcess\n\nEDA at each level (See EDA, Multilevel, Longitudinal)\nExamine models with no predictors to assess variability at each level\nCreate Level One models: starting simple and adding terms as necessary (See Considerations &gt;&gt; Variable Assignment &gt;&gt; Fixed Effects )\nCreate Level Two models: starting simple and adding terms as necessary (See Considerations &gt;&gt; Variable Assignment &gt;&gt; Fixed Effects)\n\nBeginning with the equation for the intercept term.\n\nExamine the random effects and variance components (See Considerations &gt;&gt; Variable Assignment &gt;&gt; Random Effects)\n\nBeginning with a full set of error terms and then removing covariance terms and variance terms where advisable\n\ne.g.¬†When parameter estimates are failing to converge or producing impossible or unlikely values\n\nSee Specifications and Notation for different RE modeling strategies.",
    "crumbs": [
      "Econometrics",
      "Mixed Effects"
    ]
  },
  {
    "objectID": "qmd/econometrics-mixed-effects-frequentist.html#sec-econ-me-freq-diag",
    "href": "qmd/econometrics-mixed-effects-frequentist.html#sec-econ-me-freq-diag",
    "title": "Mixed Effects",
    "section": "Diagnostics",
    "text": "Diagnostics\n\nAlso see\n\nDiagnostics, Mixed Effects\nBMLR Ch.8 &gt;&gt; Model Building Workflow &gt;&gt; Unconditional Means for an ICC example\nExamples &gt;&gt; Random Intercept-Only model for an ICC example\n\nCheck Degrees of Freedom (DOF) in Fixed Effects Estimates\n\nThe degrees of freedom for the estimates should be close to the number of subjects (aka units)\nLarge degrees of freedom in comparison to the number of subjects indicates a misspecification of the model. (e.g.¬†60 subjects and dofs are close to the number of observations)\n\nDegrees of Freedom that are less than then number of subjects are fine and could be due to substantially imbalanced numbers of observations among the subjects.\n\nAlso papers should report the dofs.\nExample\nlibrary(lmerTest)\ndata(\"sleepstudy\")\nlmer_mod &lt;- \n  lmer(Reaction ~ 1 + Days + (1 + Days | Subject),\n       sleepstudy,\n       REML = 0)\nsummary(lmer_mod)\n\n#&gt; Fixed effects:\n#&gt;             Estimate Std. Error      df t value Pr(&gt;|t|)    \n#&gt; (Intercept)  251.405      6.632  18.001  37.907  &lt; 2e-16 ***\n#&gt; Days          10.467      1.502  18.000   6.968 1.65e-06 ***\n\nlmer_mod2 &lt;- \n  lmer(Reaction ~ 1 + Days + (1 | Subject),\n       sleepstudy,\n       REML = 0)\nsummary(lmer_mod2)\n#&gt; Fixed effects:\n#&gt;             Estimate Std. Error       df t value Pr(&gt;|t|)    \n#&gt; (Intercept) 251.4051     9.5062  24.4905   26.45   &lt;2e-16 ***\n#&gt; Days         10.4673     0.8017 162.0000   13.06   &lt;2e-16 ***\n\nThere are 18 subjects, so the first model is a well-specified model\nThe dof spikes to 162 for the Days fixed effect which is close to the number of total observations at 180. This indicates a misspecified model.\n\nThe 24.4905 is actually okay for the intercept.\n\nNote that you need to used {lmerTest} and not {lme4} to fit the model in order to get the dofs and pvals. Although there‚Äôs probably a helper function in {lmerTest} since it loads {lme4} that will give the same results if you fit the model using {lme4}\n\n\nInterClass Coefficient (ICC): The proportion of variation that is between-cases\n\n\\[\n\\rho = \\frac{\\sigma_0}{\\sigma_0 + \\sigma_\\epsilon}\n\\]\n\nWhere \\(\\sigma_0\\) is the between-case variance and \\(\\sigma_\\epsilon\\) is the within-case variance.\n1-ICC is the proportion of variation within cases\nStatistical power is a function of ICC (article)\n\n\nBoth higher ICCs and cluster size variability lead to reduced power\nThe dispersion parameter is a parameter used in the data simulation\n\nGuideline: ICC &gt; 0.1 is generally accepted as the minimal threshold for justifying the use of Mixed Effects Model\nExample: {sjPlot}\nlibrary(sjPlot)\ntab_model(lme_fit)\n\nMight need {lmerTest} loaded to get coefficient pvals\nAlso calculates two R2 values\n\nMarginal: proportion of variance explained , by the fixed effects only\nConditional: proportion of variance explained by the fixed effects and random effects",
    "crumbs": [
      "Econometrics",
      "Mixed Effects"
    ]
  },
  {
    "objectID": "qmd/econometrics-mixed-effects-frequentist.html#sec-econ-me-freq-examp",
    "href": "qmd/econometrics-mixed-effects-frequentist.html#sec-econ-me-freq-examp",
    "title": "Mixed Effects",
    "section": "Examples",
    "text": "Examples\n\nExample: {lme4}\n\nFrom Estimating multilevel models for change in R\nusl is UK sociological survey data\n\nlogincome is a logged income variable\npidp is the person‚Äôs id\nwave0 is the time variable that‚Äôs indexed at 0\n\n\n\nVarying InterceptsFixed Effect + Varying InterceptsVarying Slopes and Intercepts\n\n\nlibrary(lme4)\n# unconditional means model (a.k.a. random effects model)\nm0 &lt;- lmer(data = usl, logincome ~ 1 + (1 | pidp))\n\n# check results\nsummary(m0)\n## Linear mixed model fit by REML ['lmerMod']\n## Formula: logincome ~ 1 + (1 | pidp)\n##¬† ¬† Data: usl\n##¬†\n## REML criterion at convergence: 101669.8\n##¬†\n## Scaled residuals:¬†\n##¬† ¬† Min¬† ¬† ¬† 1Q¬† Median¬† ¬† ¬† 3Q¬† ¬† Max¬†\n## -7.2616 -0.2546¬† 0.0627¬† 0.3681¬† 5.8845¬†\n##¬†\n## Random effects:\n##¬† Groups¬† Name¬† ¬† ¬† ¬† Variance Std.Dev.\n##¬† pidp¬† ¬† (Intercept) 0.5203¬† 0.7213¬†\n##¬† Residual¬† ¬† ¬† ¬† ¬† ¬† 0.2655¬† 0.5152¬†\n## Number of obs: 52512, groups:¬† pidp, 8752\n##¬†\n## Fixed effects:\n##¬† ¬† ¬† ¬† ¬† ¬† Estimate Std. Error t value\n## (Intercept) 7.162798¬† 0.008031¬† 891.8\n\nInterpretation\n\nFixed effects:\n\n(Intercept) = 7.162798, which is the grand mean\n\nSays that the average of the individual averages of log income is 7.162798\n\n\nRandom effects:\n\nEverything that varies by pidp\nBetween-case (or person, group, cluster, etc.) variance: (Intercept) = 0.5203\nWithin-case (or person, group, cluster, etc.) variance: Residual = 0.2655\n\n\nICC\n\n\\(\\frac{0.520}{0.520 + 0.265} = 0.662\\)\nSays about 66% of the variation in log income comes between people while the remaining (~ 34%) is within people.\n\n\n\n\nlibrary(lme4)\n# unconditional change model (a.k.a. MLMC)\nm1 &lt;- lmer(data = usl, logincome ~ 1 + wave0 + (1 | pidp))\n\nsummary(m1)\n## Linear mixed model fit by REML ['lmerMod']\n## Formula: logincome ~ 1 + wave0 + (1 | pidp)\n##¬† ¬† Data: usl\n##¬†\n## REML criterion at convergence: 100654.8\n##¬†\n## Scaled residuals:¬†\n##¬† ¬† Min¬† ¬† ¬† 1Q¬† Median¬† ¬† ¬† 3Q¬† ¬† Max¬†\n## -7.1469 -0.2463¬† 0.0556¬† 0.3602¬† 5.7533¬†\n##¬†\n## Random effects:\n##¬† Groups¬† Name¬† ¬† ¬† ¬† Variance Std.Dev.\n##¬† pidp¬† ¬† (Intercept) 0.5213¬† 0.7220¬†\n##¬† Residual¬† ¬† ¬† ¬† ¬† ¬† 0.2593¬† 0.5092¬†\n## Number of obs: 52512, groups:¬† pidp, 8752\n##¬†\n## Fixed effects:\n##¬† ¬† ¬† ¬† ¬† ¬† Estimate Std. Error t value\n## (Intercept) 7.057963¬† 0.008665¬† 814.51\n## wave0¬† ¬† ¬†  0.041934¬† 0.001301¬† 32.23\n##¬†\n## Correlation of Fixed Effects:\n##¬† ¬† ¬† (Intr)\n## wave0 -0.375\n\nModel\n\n1 covariate\nRandom Effect: Cases; Fixed Effect: Time\n\nInterpretation\n\nFixed effects:\n\n(Intercept) = 7.057; the expected log income at the beginning of the study (wave 0).\nwave0 = 0.0419\n\nThe average rate of change with the passing of a wave.\n\nI don‚Äôt think this is a percentage. It‚Äôs the same as a standard OLS regression interpretation.\n\nSo after each wave, individual log income is expected to slowly increase by 0.041 on average.\n\n\nRandom Effects\n\nEverything varies by pidp\nBetween-Case (or person, group, cluster, etc.) variance in log income: (Intercept) = 0.5213\nWithin-Case (or person, group, cluster, etc.) variance in log income: Residual = 0.2593\nSo this stuff is still very similar numbers as the varying intercept-only model\n\n\nVisualize Effect\n\n\n\nCode\n\nusl$pred_m1 &lt;- predict(m1)\nusl %&gt;%¬†\n¬† filter(pidp %in% 1:5) %&gt;% # select just five individuals\n¬† ggplot(aes(wave, pred_m1, color = pidp)) +\n¬† geom_point(aes(wave, logincome)) + # points for observed log income\n¬† geom_smooth(method = lm, se = FALSE) + # linear line showing wave0 slope\n¬† theme_bw() +\n¬† labs(x = \"Wave\", y = \"Logincome\") +¬†\n¬† theme(legend.position = \"none\")\n\n\nwave was indexed to 0 for the model but now wave starts at 1. He might‚Äôve reverted wave to have a starting value of 1 for graphing purposes\nLines show the small, positive, fixed effect slope for wave0\nParallel lines means we assume the change in log income over time is the same for all the individuals\n\ni.e.¬†We assume there is no between-case variation in the rate of change.\n\n\n\n\n\nlibrary(lme4)\n# unconditional change model (a.k.a. MLMC) with re for change\nm2 &lt;- lmer(data = usl, logincome ~ 1 + wave0 + (1 + wave0 | pidp))\n\nsummary(m2)\n## Linear mixed model fit by REML ['lmerMod']\n## Formula: logincome ~ 1 + wave0 + (1 + wave0 | pidp)\n##¬† ¬† Data: usl\n##¬†\n## REML criterion at convergence: 98116.1\n##¬†\n## Scaled residuals:¬†\n##¬† ¬† Min¬† ¬† ¬† 1Q¬† Median¬† ¬† ¬† 3Q¬† ¬† Max¬†\n## -7.8825 -0.2306¬† 0.0464¬† 0.3206¬† 5.8611¬†\n##¬†\n## Random effects:\n##¬† Groups¬† Name¬† ¬† ¬† ¬† Variance Std.Dev. Corr¬†\n##¬† pidp¬† ¬† (Intercept) 0.69590¬† 0.8342¬† ¬† ¬† ¬†\n##¬† ¬† ¬† ¬† ¬† wave0¬† ¬† ¬†  0.01394¬† 0.1181¬† -0.51\n##¬† Residual¬† ¬† ¬† ¬† ¬† ¬† 0.21052¬† 0.4588¬† ¬† ¬† ¬†\n## Number of obs: 52512, groups:¬† pidp, 8752\n##¬†\n## Fixed effects:\n##¬† ¬† ¬† ¬† ¬† ¬† Estimate Std. Error t value\n## (Intercept) 7.057963¬† 0.009598¬† 735.39\n## wave0¬† ¬† ¬†  0.041934¬† 0.001723¬† 24.34\n##¬†\n## Correlation of Fixed Effects:\n##¬† ¬† ¬† (Intr)\n## wave0 -0.558\n\nModel\n\nRandom Effect: outcome (i.e.¬†intercept) by cases\nTime Effect by cases\nFixed Effect: time\n(1 + wave0 | pidp) - Says let both the intercept (‚Äú1‚Äù) and wave0 vary by pidp - Which means that average log income varies by person and the rate of change (slope) in log income over time (wave0 fixed effect) varies by person.\n\nInterpretation\n\nFixed effects:\n\n(Intercept) = 7.057963: The expected log income at the beginning of the study (wave 0).\nwave0 = 0.0419\n\nThe average rate of change with the passing of a wave.\n\nI don‚Äôt think this is a percentage. It‚Äôs the same as a standard OLS regression interpretation.\n\nSo after each wave, individual log income is expected to slowly increase by 0.041 on average.\n\n\nRandom effects:\n\nEverything that varies by pidp\nWithin-Case (or person, group, cluster, etc.) variance of log income:\n\nResidual = 0.21052\nHow much each person‚Äôs log income varies in relation to her average log income\n\nBetween-Case (or person, group, cluster, etc.) variance of log income:\n\n(Intercept) = 0.69590\nHow much each person‚Äôs average log income varies in relation to the group‚Äôs average log income (The fixed effect intercept which is also the grand mean)\n\nBetween-Case variance in the rates of change of log income over time\n\nwave0 = 0.01394\ni.e.¬†How much wave0‚Äôs fixed effect varies by person.\n2 standard deviations = 2 \\(\\cdot\\) 0.1181 = 0.2362. Therefore, for 95% of the population, the effect of wave0 can vary from 0.0419 - 0.2362 = -0.1943 to 0.0491 + 0.2362 = 0.2781 from person to person.\n\nThe correlation between subject-level intercepts and the subject-level slopes (wave0)\n\ncorr = -0.51 which is a pretty strong correlation.\nWe can expect persons starting out with high average log income (intercept) to be lightly affected by time (low wave0 effect). Those persons starting out with low average log incomes can be expected to be substantially affected by time (high wave0 effect).\n\n\n\nVisualize Effect\n\n\n\nCode\n\nusl$pred_m2 &lt;- predict(m2) \nusl %&gt;%  \n  filter(pidp %in% 1:5) %&gt;% # select just two individuals \n  ggplot(aes(wave, pred_m2, color = pidp)) + \n  geom_point(aes(wave, logincome)) + # points for observed logincome \n  geom_smooth(method = lm, se = FALSE) + # linear line based on prediction \n  theme_bw() + # nice theme \n  labs(x = \"Wave\", y = \"Logincome\") + # nice labels \n  theme(legend.position = \"none\")\n\n\nDifferent slopes for each person\n\n\n\n\n\nExample: Varying Slopes (Categorical) Varying Intercepts (link)\n\nIn the past, these types of {lme4} models had convergence issues, the article shows various atypical formulations for fitting these data. I didn‚Äôt have any problems fitting this model, but for higher dim data‚Ä¶ who knows. There‚Äôs also code for data simulation.\n\nTried a cross effects specification and it was worse ‚Äî i.e.¬†higher AIC and lower negative log likelihood.\n\n\n\nDataVarying Slopes and Intercepts\n\n\n\nVariables and Packages\n\n\nCode\n\nlibrary(tidyverse); library(lmerTest)\ndata(Machines, package = \"nlme\")\n\n# for some reason worker is an ordered factor.\nmachines &lt;- Machines |&gt; \n  mutate(Worker = factor(Worker, \n                         levels = 1:6, \n                         ordered = FALSE)) |&gt; \n  rename(worker = Worker, brand = Machine) |&gt; \n  group_by(brand, worker) |&gt; \n  mutate(avg_score = mean(score)) \n\nglimpse(machines)\n#&gt; Rows: 54\n#&gt; Columns: 4\n#&gt; Groups: brand, worker [18]\n#&gt; $ worker    &lt;fct&gt; 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 6, 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6‚Ä¶\n#&gt; $ brand     &lt;fct&gt; A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B‚Ä¶\n#&gt; $ score     &lt;dbl&gt; 52.0, 52.8, 53.1, 51.8, 52.8, 53.1, 60.0, 60.2, 58.4, 51.1, 52.3, 50.3, 50.9, 51.8, 51.4, 46.4, 44.8‚Ä¶\n#&gt; $ avg_score &lt;dbl&gt; 52.63333, 52.63333, 52.63333, 52.56667, 52.56667, 52.56667, 59.53333, 59.53333, 59.53333, 51.23333, ‚Ä¶\n\n\nscore (response) is a rating of quality of a product produced by a worker using a brand of machine.\n\nEDA\n\n\n\nCode\n\nggplot(machines,\n       aes(x = worker, y = score, group = brand, color = brand)) + \n  geom_point(size = 2, \n             alpha = 0.60) +\n  geom_path(aes(y = avg_score), \n            size = 1, \n            alpha = .75) +\n  scale_color_manual(values = notebook_colors[4:7]) +\n  theme_notebook()\n\n\nThere does seem to be variability between workers which makes a mixed model a sensible choice.\nThere‚Äôs a line cross between brands B and C for worker 6 which would indicate a potential interaction in a linear model\n\n\n\n\n\nModel\nmod_brand_worker_vsvi &lt;- \n  lmer(score ~ brand + (1 + brand | worker),\n       data = machines)\nsummary(mod_vs_vi)\n\n#&gt; Linear mixed model fit by REML. t-tests use Satterthwaite's method #&gt; ['lmerModLmerTest']\n#&gt; Formula: score ~ brand + (1 + brand | worker)\n#&gt;    Data: machines\n#&gt; \n#&gt; REML criterion at convergence: 208.3\n#&gt; \n#&gt; Scaled residuals: \n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -2.39357 -0.51377  0.02694  0.47252  2.53338 \n#&gt; \n#&gt; Random effects:\n#&gt;  Groups   Name        Variance Std.Dev. Corr       \n#&gt;  worker   (Intercept) 16.6392  4.0791              \n#&gt;           brandB      34.5540  5.8783    0.48      \n#&gt;           brandC      13.6170  3.6901   -0.37  0.30\n#&gt;  Residual              0.9246  0.9616              \n#&gt; Number of obs: 54, groups:  worker, 6\n#&gt; \n#&gt; Fixed effects:\n#&gt;             Estimate Std. Error     df t value Pr(&gt;|t|)    \n#&gt; (Intercept)   52.356      1.681  5.001  31.152 6.39e-07 ***\n#&gt; brandB         7.967      2.421  4.999   3.291 0.021708 *  \n#&gt; brandC        13.917      1.540  4.999   9.036 0.000278 ***\n#&gt; ---\n#&gt; Signif. codes:  0 ‚Äò***‚Äô 0.001 ‚Äò**‚Äô 0.01 ‚Äò*‚Äô 0.05 ‚Äò.‚Äô 0.1 ‚Äò ‚Äô 1\n#&gt; \n#&gt; Correlation of Fixed Effects:\n#&gt;        (Intr) brandB\n#&gt; brandB  0.463       \n#&gt; brandC -0.374  0.301\n\nFixed Effects\n\n(Intercept) = 52.36 is the grand mean score for brand A\nbrandB = 7.97 says switching from brand A to brand B increases the grand mean score by 7.97 points on average.\n\nSimilar for the brandC effect\n\n\nRandom Effects\n\n(Intercept) = 52.36 is the between-subject variance of each worker‚Äôs average score for brand A in relation to the grand mean score for brand A\nbrandB = 7.97 and brandC = 13.92 are how much the fixed effect of brand B vs.¬†A, and C vs.¬†A varies per person.\nResidual = 0.96 is the within-subject standard deviation of each worker‚Äôs rating in relation to their average rating.\n\n2 standard deviations = 2 \\(\\cdot\\) 0.96 = 1.82\nFor 95% of the population of workers, their individual ratings fluctuate around the average rating by \\(\\pm\\) 1.82\n\nThe correlation matrix in the summary is a bit difficult to decipher without the column names.\nvcov_mats &lt;- lme4::VarCorr(mod_vs_vi)\nattributes(vcov_mats$worker)$correlation\n#&gt;             (Intercept)    brandB     brandC\n#&gt; (Intercept)   1.0000000 0.4838743 -0.3652566\n#&gt; brandB        0.4838743 1.0000000  0.2965069\n#&gt; brandC       -0.3652566 0.2965069  1.0000000\n\n\n\n\n\n\nExample: Crossed and Nested Effects\n\nNested Model\n\n\nmod_stress_nested &lt;- \n  lmer(\n    stress ~ age + sex + experience + treatment + wardtype + hospsize \n             + (1 | hospital/ward), \n    data = nurses\n)\n\nsummary(mod_stress_nested)\n#&gt; REML criterion at convergence: 1640\n#&gt; \n#&gt; Scaled residuals: \n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -2.62082 -0.69562 -0.00134  0.66513  2.75800 \n#&gt; \n#&gt; Random effects:\n#&gt;  Groups        Name        Variance Std.Dev.\n#&gt;  ward:hospital (Intercept) 0.3366   0.5801  \n#&gt;  hospital      (Intercept) 0.1194   0.3455  \n#&gt;  Residual                  0.2174   0.4662  \n#&gt; Number of obs: 1000, groups:  ward:hospital, 100; hospital, 25\n#&gt; \n#&gt; Fixed effects:\n#&gt;                        Estimate Std. Error         df t value Pr(&gt;|t|)   \n#&gt; (Intercept)            5.379585   0.184686  46.055617  29.128  &lt; 2e-16 ***\n#&gt; age                    0.022114   0.002200 904.255524  10.053  &lt; 2e-16 ***\n#&gt; sexFemale             -0.453184   0.034991 906.006741 -12.952  &lt; 2e-16 ***\n#&gt; experience            -0.061653   0.004475 908.841521 -13.776  &lt; 2e-16 ***\n#&gt; treatmentTraining     -0.699851   0.119767  72.776723  -5.843  1.34e-07 ***\n#&gt; wardtypespecial care   0.050807   0.119767  72.775457   0.424  0.67266   \n#&gt; hospsizemedium         0.489400   0.201547  21.963644   2.428  0.02382 *  \n#&gt; hospsizelarge          0.901527   0.274824  22.015667   3.280  0.00342 **\n#&gt; ---\n#&gt; Signif. codes:  0 ‚Äò***‚Äô 0.001 ‚Äò**‚Äô 0.01 ‚Äò*‚Äô 0.05 ‚Äò.‚Äô 0.1 ‚Äò ‚Äô 1\n#&gt; \n#&gt; Correlation of Fixed Effects:\n#&gt;             (Intr) age    sexFml exprnc trtmnT wrdtyc hspszm\n#&gt; age         -0.172                                          \n#&gt; sexFemale   -0.142 -0.014                                   \n#&gt; experience  -0.002 -0.817  0.028                            \n#&gt; trtmntTrnng -0.326  0.005 -0.004  0.000                     \n#&gt; wrdtypspclc -0.324 -0.007 -0.002  0.008  0.000              \n#&gt; hospsizemdm -0.624  0.001 -0.002  0.000  0.000  0.000       \n#&gt; hospsizelrg -0.458 -0.001  0.004  0.002  0.000  0.000  0.419\n\nData\n\n\n\n\nExample: {ggeffects} Error Bar Plot\n\nlibrary(ggeffects); library(ggplot2)\n# create plot dataframe\n# Has 95% CIs for fixed effects and lists random effects\nplot_data &lt;- \n  ggpredict(\n    fit, \n    terms = c(\"Season\")\n  )\n\n#create plot\nplot_data |&gt; \n¬† #reorder factor levels for plotting\n¬† mutate(\n    x = ordered(\n          x, \n¬†         levels = c(\"Preseason\", \"Inseason\", \"Postseason\")\n        )\n  ) |&gt; \n¬† #use plot function with ggpredict objects\n¬† plot() +\n¬† #add ggplot2 as needed\n¬† theme_blank() + \n  ylim(c(3000,7000)) + \n  ggtitle(\"Session Distance by Season Phase\")\n\nDescription:\n\nOutcome: Distance\nFixed Effect: Season\nRandom Effect: Athlete\n\n\nExample: {tidymodels} Varying Intercepts\nlmer_spec &lt;-¬†\n¬† linear_reg() %&gt;%¬†\n¬† set_engine(\"lmer\")\n\ntidy_mod &lt;-¬†\n¬† lmer_spec %&gt;%¬†\n¬† fit(pp60 ~ position + toi + (1 | player),\n¬† ¬† ¬† data = df)\n\ntidy_mod\n## parsnip model object\n##¬†\n## Linear mixed model fit by REML ['lmerMod']\n## Formula: pp60 ~ position + toi + (1 | player)\n##¬† ¬† Data: data\n## REML criterion at convergence: 115.8825\n## Random effects:\n##¬† Groups¬† Name¬† ¬† ¬† ¬† Std.Dev.\n##¬† player¬† (Intercept) 0.6423¬†\n##¬† Residual¬† ¬† ¬† ¬† ¬† ¬† 0.3452¬†\n## Number of obs: 80, groups:¬† player, 20\n## Fixed Effects:\n## (Intercept)¬† ¬† positionF¬† ¬† ¬† ¬† ¬† toi¬†\n##¬† ¬† -0.16546¬† ¬† ¬† 1.48931¬† ¬† ¬† 0.06254",
    "crumbs": [
      "Econometrics",
      "Mixed Effects"
    ]
  },
  {
    "objectID": "qmd/econometrics-mixed-effects-frequentist.html#sec-econ-me-freq-bmlrch8",
    "href": "qmd/econometrics-mixed-effects-frequentist.html#sec-econ-me-freq-bmlrch8",
    "title": "Mixed Effects",
    "section": "BMLR Chapter 8: Multilevel Models",
    "text": "BMLR Chapter 8: Multilevel Models\n\nBeyond Multiple Linear Regression, Chapter 8: Introduction to Multilevel Models\nVariable Types\n\nLevel One: variables measured at the most frequently occurring observational unit\n\ni.e.¬†Variables that (for the most part) have different values for each row\ni.e.¬†Vary for each repeated measure of a subject and vary between subjects\n\nLevel Two: variables measured on larger observational units\n\ni.e.¬†Constant for each repeated measure of a subject but vary between each subject\n\n\nData Description in the Examples\n\nna (outcome) - Negative Affect (i.e.¬†Performance Anxiety)\nlarge - binary\n\nlarge = 1 means the performance type is a Large Ensemble\nlarge = 0 means the performance type is either a Solo or Small Ensemble\n\norch - binary\n\norch = 1 means the instrument is Orchestral\norch = 0 means the instrument is either a Keyboard or it‚Äôs Vocal\n\n\nTwo-Stage Model\n\nThe 2-stage model shows a clearer depiction of how a multi-level model is fit in principle\n\n2-stage model uses Ordinary Least Squares to fit a system of regression equations in stages\nThe multi-level model fits a composite of that system of equations using Maximum Likelihood Estimation\n\nProcess\n\nLevel 1 models are fitted for each subject/unit\nUsing the estimated level 1 intercepts and slopes as outcome variables, Level 2 intercept and slope models respectively are fit\nThe errors from the Level 2 models are the random effects\n\nIssues\n\nWeights every subject the same regardless of the number of repeated observations\nResponds to missing individual slopes (i.e.¬†subjects never exposed to treatment) by simply dropping those subjects\nDoes not share strength effectively across subjects\n\nSpecification\n\nLevel 1\n\\[\nY_{ij} = a_i + b_i \\text{large}_{ij} + \\epsilon_{ij}\n\\]\n\n\\(b_i\\) is the fixed effect for subject/unit \\(i\\) and observation \\(j\\) (i.e.¬†each subject has repeated observations)\n\nLevel 2\n\\[\n\\begin{align}\na_i &= \\alpha_0 + \\alpha_1 \\text{orch}_i + u_i \\\\\nb_i &= \\beta_0 + \\beta_1 \\text{orch}_i + v_i\n\\end{align}\n\\]\n\n\\(u_i\\) and \\(v_i\\) are the random effects for subject/unit \\(i\\)\n\\(a_i\\) is the true (i.e.¬†not estimated since no hat) mean of performance anxiety (outcome) when subject plays solos or small ensembles (large = 0)\n\\(b_i\\) is the true mean difference in performance anxiety (outcome) for subjecti between large ensembles and other performance types (large contrast)\n\n\n\n\n\nMulti-Level\n\nProcess\n\nLevel 1 and Level 2 equations have been combined through substitution then reduced into a composite model\nParameters estimated through Maximum Likelihood Estimation\n\nMisc\n\nError Distribution\n\nCorrelation between parameters is accounted for using a multivariate normal distribution estimated through a variance-covariance matrix of the error terms (aka random effects) of each Level (see link for details)\n\nAdding Level 1 variables to a model formula should change within-person variability (\\(\\hat\\sigma^2\\) and \\(\\hat\\sigma\\))\n\nEven without adding a Level 1 variable, small changes could occur due to numerical estimation procedures used in likelihood-based parameter estimates\n\nOptimization methods\n\nUsually very little difference between ML and REML parameter estimates\nRestricted Maximum Likelihood (REML) is preferable when the number of parameters is large or the primary interest is obtaining estimates of model parameters, either fixed effects or variance components associated with random effects\nMaximum Likelihood (MLE) should be used if nested fixed effects models are being compared using a likelihood ratio test, although REML is fine for nested models of random effects that have the same fixed effects.\n\nP-Values\n\nCan‚Äôt be calculated because the exact distribution of the test statistics under the null hypothesis (no fixed effect) is unknown, primarily because the exact degrees of freedom is not known\nRule of Thumb: t-values (ratios of parameter estimates to estimated standard errors) with absolute value above 2 indicate significant evidence that a particular model parameter is different than 0\nPackages that do report p-values of fixed effects typically using conservative assumptions, large-sample results, or approximate degrees of freedom for a t-distribution\n\nUsing {lmerTest} will produce a summary of {lme4} with pvals for coefficients\n\nParametric Bootstrap can be used to approximate the distribution of the likelihood test statistic and produce more accurate p-values by simulating data under the null hypothesis\n\nModel Comparison\n\nPseudo-R2\n\nSee below in Simple to Complex Model Building Workflow &gt;&gt;\n\nRandom Slopes and Intercepts Model (with 1 covariate)\nRandom Slopes and Intercepts Model (with 2 covariates)\n\n\nAIC, BIC\n\nSee Random Intercepts (with 2 covariates)\n\nLikelihood Ratio Test (LR-Test)\n\nSee Random Slopes and Intercepts Model (with 3 covariates)\n\n\n\nComposite Specification\n\\[\nY_{ij} = [\\alpha_0 + \\alpha_1 \\text{orch}_i + \\beta_0 \\text{large}_{ij} + \\beta_1 \\text{orch}_i \\text{large}_{ij}] + [u_i + v_i \\text{large}_{ij} + \\epsilon_{ij}]\n\\]\n\nComposite model of level 1 and level 2 equations\nRandom Slopes and Intercepts Model (with 2 covariates)\nFixed Effects: \\(\\alpha_0\\), \\(\\alpha_1\\), \\(\\beta_0\\) and \\(\\beta_1\\)\n\\(u_i + v_i \\cdot \\text{large}_{ij} + \\epsilon_{ij}\\) is the interesting part.\n\nThis part has all the error terms and any variables in the Level 2 equations\nThe first part is a typical regression with interaction terms.\n\n\nInterpretation\n\nSee Two-stage model for definitions for \\(a_i\\) and \\(b_i\\)\n\nKeyboardists and Vocalists (orchi = 0)\n\\[\n\\begin{align}\na_i &= \\alpha_0 + u_i \\\\\nb_i &= \\beta_0 + v_i\n\\end{align}\n\\]\nOrchestral Instrumentalists (orchi = 1)\n\\[\n\\begin{align}\na_i = (\\alpha_0 + \\alpha_1) + u_i \\\\\nb_i = (\\beta_0 + \\beta_1) + v_i\n\\end{align}\n\\]\n\nMean Performance Anxiety (outcome) when:\n\nKeyboardists or Vocalists (orch = 0) play solos or small ensembles (large = 0): \\(\\alpha_0\\)\nKeyboardists or vocalists (orch = 0) play large ensembles (large = 1): \\(\\alpha_0 + \\beta_0\\)\nOrchestral instrumentalists (orch = 1) play solos or small ensembles (large = 0): \\(\\alpha_0 + \\alpha_1\\alpha_0 + \\alpha_1\\)\nOrchestral instrumentalists (orch = 1) play large ensembles (large = 1): \\(\\alpha_0 + \\alpha_1 + \\beta_0 + \\beta_1\\)\n\n\nModel Summary (using lmer4::lmer())\n#&gt; ¬† ¬† Linear mixed model fit by REML ['lmerMod']¬†\n#&gt; A)¬† Formula: na ~ orch + large + orch:large + (large | id)¬†\n#&gt; ¬† ¬† ¬† ¬† Data: music¬†\n#&gt; B)¬† REML criterion at convergence: 2987¬†\n\n#&gt; B2)¬† ¬† ¬† AIC¬† ¬† ¬† BIC¬† logLik deviance df.resid¬†\n#&gt; ¬† ¬† ¬† ¬† 3007¬† ¬† 3041¬† ¬† -1496¬† ¬† 2991¬† ¬† ¬† 489¬†\n\n#&gt; ¬† ¬† Random effects:¬†\n#&gt; ¬† ¬† ¬† Groups¬† Name¬† ¬† ¬† ¬† Variance Std.Dev. Corr¬†\n#&gt; C)¬† id¬† ¬† ¬† (Intercept)¬†   5.655¬†  2.378¬† ¬† ¬† ¬† ¬†\n#&gt; D)¬† ¬† ¬† ¬† ¬† ¬† large¬† ¬† ¬† ¬† 0.452¬†  0.672¬† ¬†-0.63¬†\n#&gt; E)¬† Residual¬† ¬† ¬† ¬† ¬† ¬†   21.807¬†  4.670¬† ¬† ¬† ¬† ¬†\n#&gt; F)¬† Number of obs: 497, groups:¬† id, 37¬†\n\n#&gt; ¬† ¬† Fixed effects:¬†\n#&gt; ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† Estimate Std. Error t value¬†\n#&gt; G)¬† (Intercept)¬† 15.930¬† ¬† ¬† 0.641¬† 24.83¬†\n#&gt; H)¬† orch¬† ¬† ¬† ¬† ¬† 1.693¬† ¬† ¬† 0.945¬† ¬†1.79¬†\n#&gt; I)¬† large¬† ¬† ¬† ¬† -0.911¬† ¬† ¬† 0.845¬† -1.08¬†\n#&gt; J)¬† orch:large¬† ¬†-1.424¬† ¬† ¬† 1.099¬† -1.30\n\nDefinitions\n\nA: How our multilevel model is written in R, based on the composite model formulation.\nB: Measures of model performance. Since this model was fit using REML, this line only contains the REML criterion.\nB2: If the model is fit with ML instead of REML, the measures of performance will contain AIC, BIC, deviance, and the log-likelihood.\nC: Estimated variance components (\\(\\hat \\sigma^2_u\\) and \\(\\hat \\sigma_u\\)) associated with the intercept equation in Level Two. (between-unit variability)\nD: Estimated variance components (\\(\\hat \\sigma^2_v\\) and \\(\\hat \\sigma_v\\)) associated with the large ensemble (large = 1) effect equation in Level Two. (Also between-unit variability but just for the slope)\n\nAlso, in the ‚ÄúCorr‚Äù column, the estimated correlation (\\(\\hat \\rho_{uv}\\)) between the two Level Two error terms.\n\nE: Estimated variance components (\\(\\hat \\sigma^2\\) and \\(\\hat \\sigma\\) associated with the Level One equation. (within-unit variability)\nF: Total number of performances where data was collected (Level One observations = 497) and total number of subjects (Level Two observations = 37).\nG: Estimated fixed effect (\\(\\hat \\alpha_0\\)) for the intercept term, along with its standard error and t-value (which is the ratio of the estimated coefficient to its standard error).\nH: Estimated fixed effect (\\(\\hat \\alpha_1\\)) for the orchestral instrument (orch = 1) effect, along with its standard error and t-value.\nI: Estimated fixed effect (\\(\\hat \\beta_0\\)) for the large ensemble (large = 1) effect, along with its standard error and t-value.\nJ: Estimated fixed effect (\\(\\hat \\beta_1\\)) for the interaction between orchestral instruments (orch = 1) and large ensembles (large = 1), along with its standard error and t-value.\n\nInterpretations\n\nFixed effects:\n\n\\(\\hat \\alpha_0 = 15.9\\) ‚Äî The estimated mean performance anxiety (outcome) for solos and small ensembles (large = 0) for keyboard players and vocalists (orch = 0) is 15.9.\n\\(\\hat \\alpha_1 = 1.7\\) ‚Äî Orchestral instrumentalists (orch = 1) have an estimated mean performance anxiety (outcome) for solos and small ensembles (large = 0) which is 1.7 points higher than keyboard players and vocalists (orch = 0).\n\\(\\hat \\beta_0 = ‚àí0.9\\) ‚Äî Keyboard players and vocalists (orch = 0) have an estimated mean decrease in performance anxiety (outcome) of 0.9 points when playing in large ensembles (large = 1) instead of solos or small ensembles (large = 0).\n\\(\\hat \\beta_1 = ‚àí1.4\\) ‚Äî Orchestral instrumentalists (orch = 1) have an estimated mean decrease in performance anxiety (outcome) of 2.3 points when playing in large ensembles (large = 1) instead of solos or small ensembles (large = 0), 1.4 points greater than the mean decrease among keyboard players and vocalists (orch = 0).\n\nHe‚Äôs calculating simple slope/marginal effect (See Regression, Interactions) and choosing to use that as the interpretation for the interaction term\n\nFrom Ch. 1 (link): ‚ÄúWe interpret the coefficient for the interaction term by comparing slopes under fast and non-fast conditions; this produces a much more understandable interpretation for a reader than attempting to interpret the -0.011 (interaction coef) directly‚Äù\n\nFast and non-fast are levels of a main effect and a term in the interaction.\n\n\nCalculation: The interaction is the difference in slopes for one interaction variable (e.g.¬†large) at different values of the other interaction variable (e.g.¬†orch)\n\nRegression eq (ignoring the random effects part):\n\n\\(\\hat Y_i = 15.93 + 1.693\\text{orch}_i - 0.911\\text{large} ‚àí 1.424\\text{orch}_i \\times \\text{large}_i\\)\n\nExamining the large slope so only dealing with terms that contain ‚Äúlarge‚Äù\n\nlarge = 1 is reference category in the interaction that the coefficient is associated with so it remains constant\n\nWhen orch = 0 and large = 1, the slope for large is -0.911 = \\(\\hat \\beta_0\\)\n\n\\(- 0.911\\text{large} ‚àí 1.424\\text{orch}_i \\times \\text{large}_i = -0.911 \\times 1 - 1.424 \\times \\boldsymbol{0} \\times 1 = -0.911 = \\hat \\beta_0\\)\n\nWhen orch = 1 and large = 1, the slope for large is -2.335 (uses this one for his interpretation)\n\n\\(- 0.911\\text{large} ‚àí 1.424\\text{orch}_i \\times \\text{large}_i = -0.911 \\times 1 - 1.424 \\times \\boldsymbol{1} \\times 1 = -0.911 - 1.424 = -2.335\\) (i.e.¬†‚Äúdecrease‚Ä¶ of 2.3 points‚Äù)\n\nThe difference in these slopes is the interaction coefficient: \\(-2.335 - (-0.911) = -1.424\\)\n\n\n\nVariance components\n\n\\(\\hat \\sigma_u = 2.4\\) ‚Äî The estimated standard deviation of performance anxiety (outcome) for solos and small ensembles (large = 0) is 2.4 points, after controlling for instrument (orch) played.\n\\(\\hat \\sigma_v = 0.7\\) ‚Äî The estimated standard deviation of differences in performance anxiety (outcome) between large ensembles (large = 1) and other performance types (large = 0) is 0.7 points, after controlling for instrument (orch) played.\n\\(\\hat \\rho_{uv} = ‚àí0.64\\) ‚Äî The estimated correlation between performance anxiety scores (outcome) for solos and small ensembles (large = 0) and increases in performance anxiety (outcome) for large ensembles (large = 1) is -0.64, after controlling for instrument (orch) played.\n\nThose subjects with higher performance anxiety scores for solos and small ensembles tend to have greater decreases in performance anxiety for large ensemble performances.\n\n\\(\\hat \\sigma = 4.7\\) ‚Äî The estimated standard deviation in residuals for the individual regression models is 4.7 points.\n\n\n\n\n\n\nModel Building Workflow: Simple to Complex\n\nWorkflow should include:\n\nFixed effects that allow one to address primary research questions\nFixed effects that control for important covariates at all levels\nInvestigation of potential interactions\nVariables that are centered where interpretations can be enhanced\nImportant variance components\nRemoval of unnecessary terms\nA final model that tells a ‚Äúpersuasive story parsimoniously‚Äù\n\nModel Comparison (Also see multi-level &gt;&gt; misc &gt;&gt; p-values & model comparison)\n\nLR-Test for nested models\nAIC, BIC for unnested models\nConsider models that involve removing terms that have t-stats &lt; |2|\n\nModels\n\nUnconditional Means (aka Random Intercepts)\nRandom Slopes and Intercepts Model (with 1 covariate)\nRandom Slopes and Intercepts Model (with 2 covariates)\nRandom Intercepts (with 2 covariates)\nRandom Slopes and Intercepts Model (with 3 covariates)\nFinal Model\n\n\n\nUnconditional Means (aka Random Intercepts)\n\nSpecification\n\nLevel 1: \\(Y_{ij} = a_i + \\epsilon_{ij} \\quad \\text{where}\\: \\epsilon \\sim \\mathcal{N}(0, \\sigma^2)\\)\n\n\\(a_i\\) is the true mean response of all observations for subjecti\n\nLevel 2: \\(a_i = \\alpha_0 + u_i \\quad \\text{where} \\: u_i \\sim \\mathcal{N}(0, \\sigma^2_u)\\)\n\nEach subject‚Äôs intercept is assumed to be a random value from a normal distribution centered at \\(\\alpha_0\\) with variance \\(\\sigma^2_u\\).\n\nComposite: \\(Y_{ij} = \\alpha_0 + u_i + \\epsilon_{ij}\\)\n\nModel\n#Model A (Unconditional means model)\nmodel.a &lt;- lmer(na ~ 1 + (1 | id), REML = T, data = music)\n##¬† Groups¬† Name¬† ¬† ¬† ¬† Variance Std.Dev.\n##¬† id¬† ¬† ¬† (Intercept)¬† 4.95¬† ¬† 2.22¬† ¬†\n##¬† Residual¬† ¬† ¬† ¬† ¬† ¬† 22.46¬† ¬† 4.74\n##¬† Number of Level Two groups =¬† 37\n##¬† ¬† ¬† ¬† ¬† ¬† Estimate Std. Error t value\n## (Intercept)¬† ¬† 16.24¬† ¬† 0.4279¬† 37.94\nInterpretation\n\n\\(\\hat \\alpha_0 = 16.2\\) ‚Äî The estimated mean performance anxiety score across all performances and all subjects.\n\\(\\hat \\sigma^2 = 22.5\\) ‚Äî The estimated variance in within-person deviations.\n\\(\\hat \\sigma^2_u = 5.0\\) (rounded from 4.95) ‚Äî The estimated variance in between-person deviations.\nICC\n\\[\n\\hat \\rho = \\frac{\\text{between-person variability}}{\\text{total variability}} = \\frac{\\hat \\sigma^2_u}{\\hat \\sigma^2_u + \\hat \\sigma ^2} = \\frac{5.0}{5.0 + 22.5} = 0.182\n\\]\n\n18.2% of the total variability in performance anxiety scores are attributable to differences among subjects.\n\n*For plain random intercepts models only*, you can also say this same number says that the average correlation for any pair of responses from the same individual is a moderately low 0.182.\n\nAs \\(\\rho\\) approaches 0: responses from an individual are essentially independent and accounting for the multilevel structure of the data becomes less crucial.\nAs \\(\\rho\\) approaches 1: repeated observations from the same individual essentially provide no additional information and accounting for the multilevel structure becomes very important.\nEffective Sample Size (ESS): The number of independent pieces of information we have for modeling\n\nWith \\(\\rho\\) near 0: ESS approaches the total number of observations.\nWith \\(\\rho\\) near 1: ESS approaches the number of subjects in the study.\n\n\n\n\n\n\nRandom Slopes and Intercepts Model (with 1 covariate)\n\n1 - Level 1\nSpecification\n\nLevel 1: \\(Y_{ij} = a_i + b_i \\text{large}_{ij} + \\epsilon_{ij}\\)\nLevel 2\n\\[\na_i = \\alpha_0 + u_i \\\\\nb_i = \\beta_0 +v_i\n\\]\nComposite\n\\[\n\\begin{aligned}\n&Y_{ij} = [\\alpha_0 + \\beta_0 \\text{large}_{ij}] + [u_i + v_i \\text{large}_{ij} + \\epsilon_{ij}] \\\\\n&\\begin{aligned}\n\\text{where}\\quad \\epsilon &\\sim \\mathcal{N}(0, \\sigma^2) \\: \\text{and}\\\\\n\\left[ \\begin{array}{cc} u_i \\\\ v_i \\end{array} \\right] &\\sim \\mathcal{N} \\left(\\left[\\begin{array}{cc} 0\\\\0 \\end{array}\\right], \\left[\\begin{array}{cc} \\sigma^2_u\\\\\\rho\\sigma_u \\sigma_v \\quad \\sigma^2_v \\end{array}\\right]\\right)\n\\end{aligned}\n\\end{aligned}\n\\]\n\nModel\nmodel.b &lt;- lmer(na ~ large + (large | id), data = music)\n##¬† Groups¬† Name¬† ¬† ¬† ¬† Variance Std.Dev. Corr¬†\n##¬† id¬† ¬† ¬† (Intercept)¬† 6.333¬† 2.517¬† ¬† ¬† ¬† ¬†\n##¬† ¬† ¬† ¬† ¬† large¬† ¬† ¬† ¬† 0.743¬† 0.862¬† ¬† -0.76\n##¬† Residual¬† ¬† ¬† ¬† ¬† ¬† 21.771¬† 4.666\n##¬† Number of Level Two groups =¬† 37\n##¬† ¬† ¬† ¬† ¬† ¬† Estimate Std. Error t value\n## (Intercept)¬† 16.730¬† ¬† 0.4908¬† 34.09\n## large¬† ¬† ¬† ¬† -1.676¬† ¬† 0.5425¬† -3.09\nInterpretation\n\n\\(\\hat \\alpha_0 = 16.7\\) ‚Äî The mean performance anxiety level (outcome) before solos and small ensemble performances (large = 0).\n\\(\\hat \\beta_0 = ‚àí1.7\\) ‚Äî The mean decrease in performance anxiety (outcome) before large ensemble performances (large = 1).\n\nSubjects had a performance anxiety level of 16.7 before solos and small ensembles, and their anxiety levels were 1.7 points lower, on average, before large ensembles, producing an average performance anxiety level before large ensembles of 15.0\nStatistically significant since |t-value| &gt; 2\n\n\\(\\hat \\sigma^2 = 21.8\\) ‚Äî The variance in within-person deviations.\n\\(\\hat \\sigma^2_u = 6.3\\) ‚Äî The variance in between-person deviations in performance anxiety scores (outcome) before solos and small ensembles (large = 0).\n\\(\\hat \\sigma^2_v = 0.7\\) ‚Äî The variance in between-person deviations in increases (or decreases) (slope) in performance anxiety scores (outcome) before large ensembles (large = 1).\n\\(\\hat \\rho_{uv} = ‚àí0.76\\) (Corr column)\n\nSlopes and intercepts are negatively correlated\nA strong negative relationship between a subject‚Äôs performance anxiety (outcome) before solos and small ensembles (large = 0) (Intercept) and their (typical) decrease in performance anxiety (Slope) before large ensembles (large = 1) .\n\n\nPseudo-R2 (Not always a reliable performance measure)\n\nUnconditional Means vs Random Intercepts and Slopes\n\\[\n\\text{Psuedo R}^2_{L1} = \\frac{\\hat \\sigma^2 (\\text{Model A}) - \\hat\\sigma^2(\\text{Model B})}{\\hat \\sigma^2(\\text{Model A})} = \\frac{22.5 - 21.8}{22.5} = 0.031\n\\]\n\nWhere Model A: Unconditional Means; Model B: Random intercepts and slopes\nFYI\n\n\\(\\text{percent decrease} = \\frac{\\text{original value} - \\text{new value}}{\\text{orginal value}}\\)\n\npseudo-R2 uses this one\nIf negative, it‚Äôs a percent increase.\n\n\\(\\text{percent increase} = \\frac{\\text{new value} - \\text{original value}}{\\text{original value}}\\)\n\nIf negative, it‚Äôs a percent decrease.\n\n\n\nPositive value means Model B is an improvement over Model A\n\nMeans the variance in Model B‚Äôs error terms is smaller than Model A‚Äôs, and therefore better fits the data. (I think)\n\nThe estimated within-person variance \\(\\hat \\sigma^2\\) decreased by 3.1% (from 22.5 to 21.8) from the unconditional means model\nImplies that only 3.1% of within-person variability in performance anxiety scores can be explained by performance type\nValues of \\(\\hat \\sigma^2_u\\) and \\(\\hat \\sigma^2_v\\) from Model B cannot be compared to between-person variability from Model A using pseudo-R2, since the inclusion of performance type has changed the interpretation of these values\nIssues\n\n‚ÄúBecause of the complexity of estimating fixed effects and variance components at various levels of a multilevel model, it is not unusual to encounter situations in which covariates in a Level Two equation for the intercept (for example) remain constant (while other aspects of the model change), yet the associated pseudo R-squared values differ or are negative.‚Äù (?)\n\n\n\n\n\nRandom Slopes and Intercepts Model (with 2 covariates)\n\n1 - Level 1; 1 - Level 2\nSpecification, Model, Interpretation (see Multi-Level section above)\n\nSays the large effect (slope) varies across subjects/units\n\nPseudo-R2\n\\[\n\\begin{align}\n\\text{Psuedo R}^2_{L2_u} &= \\frac{\\hat \\sigma^2_u (\\text{Model B}) - \\hat\\sigma^2_u(\\text{Model C})}{\\hat \\sigma^2_u(\\text{Model B})} = \\frac{6.33 - 5.66}{6.33} = 0.106 \\\\\n\\text{Psuedo R}^2_{L2_v} &= \\frac{\\hat \\sigma^2_v (\\text{Model A}) - \\hat\\sigma^2_v(\\text{Model B})}{\\hat \\sigma^2_v(\\text{Model A})} = \\frac{0.74 - 0.45}{0.74} = 0.392\n\\end{align}\n\\]\n\nModel B is Random Slopes and Intercepts Model (with 1 covariate)\nModel C is Random Slopes and Intercepts Model (with 2 covariates)\nThe addition of Level 2 variable, orch, in Model C:\n\n(Top) Decreased the between-person variability in mean (intercept) performance anxiety (outcome) before solos and small ensembles (large = 0) by 10.6%\n(Bottom) Decreased the between-person variability in the effect (slope) of large ensembles (large = 1) on performance anxiety (outcome) by 39.2%\n\n\n\n\n\nRandom Intercepts (with 2 covariates)\n\n1 - Level 1; 1 - Level 2\nInstead of assuming that the large ensemble (large) effects, after accounting for instrument played (orch), vary by individual, we are assuming that large ensemble effect is fixed across subjects.\nSpecification\n\nLevel 1: \\(Y_{ij} = a_i + b_i \\text{large}_{ij} + \\epsilon_{ij}\\)\nLevel 2\n\\[\n\\begin{align}\n&a_i = \\alpha_0 + \\alpha_1 \\text{orch}_i + u_i\\\\\n&b_i = \\beta_0 + \\beta_1 \\text{orch}_i \\\\\n&\\text{where}\\: \\epsilon \\sim \\mathcal{N}(0, \\sigma^2)\\: \\text{and} \\: u_i \\sim \\mathcal{N}(0, \\sigma^2_u)\n\\end{align}\n\\]\n\nOnly difference is the random effect/error term, \\(v_i\\), isn‚Äôt specified\n\nComposite\n\nNot given, but probably very similar to the previous model just without \\(v_i\\)\n\n\nModel\nmodel.c2 &lt;- lmer(na ~ orch + large + orch:large + (1|id), data = music)\n##¬† Groups¬† Name¬† ¬† ¬† ¬† Variance Std.Dev.\n##¬† id¬† ¬† ¬† (Intercept)¬† 5.13¬† ¬† 2.27¬† ¬†\n##¬† Residual¬† ¬† ¬† ¬† ¬† ¬† 21.88¬† ¬† 4.68\n##¬† Number of Level Two groups =¬† 37\n##¬† ¬† ¬† ¬† ¬† ¬† Estimate Std. Error t value\n## (Intercept)¬† 15.9026¬† ¬† 0.6187¬† 25.703\n## orch¬† ¬† ¬† ¬† ¬† 1.7100¬† ¬† 0.9131¬†  1.873\n## large¬† ¬† ¬† ¬† -0.8918¬† ¬† 0.8415¬† -1.060\n## orch:large¬†  -1.4650¬† ¬† 1.0880¬† -1.347\nInterpretation\n\nSame as previous model except there‚Äôs no between-units variation estimate for the slope (see previous model and multi-level model section)\nEstimates are similar to the previous model‚Äôs. Largest difference seems to be the between-unit variation for the Intercept (5.66 vs 5.13)\n\nComparison\n¬† ¬† ¬† ¬† df¬† AIC\nmodel.c¬† 8 3003\nmodel.c2¬†6 2999\n¬† ¬† ¬† ¬† df¬† BIC\nmodel.c¬† 8 3037\nmodel.c2¬†6 3025\n\nModel C is the previous model and Model C2 is this model\nBoth criterion pick this model to predict best\nThe more complex model (e.g.¬†varying intercepts and varying slopes) doesn‚Äôt always perform best\n\n\n\n\nRandom Slopes and Intercepts Model (with 3 covariates)\n\n1 - Level 1; 2 - Level 2\nMPQnem has been centered (mean = 31.63)\n\nOtherwise some interpretations would involve MPQnem = 0 when the minimum score is 11 which would make the interpretation nonsensical\n\nSpecification\n\nLevel 1: \\(Y_{ij} = a_i + b_i \\text{large}_{ij} + \\epsilon_{ij}\\)\nLevel 2\n\\[\n\\begin{align}\na_i = \\alpha_0 + \\alpha_1 \\text{orch}_i + \\alpha_2 \\text{MPQnem}_i + u_i \\\\\nb_i = \\beta_0 + \\beta_1 \\text{orch}_i + \\beta_2 \\text{MPQnem}_i + v_i\n\\end{align}\n\\]\nComposite\n\\[\nY_{ij} = [\\alpha_0 + \\alpha_1 \\text{orch}_i + \\alpha_2 \\text{mpqnem}_i + \\beta_0 \\text{large}_{ij} + \\beta_1 \\text{orch}_i \\text{large}_{ij} + \\beta_2 \\text{mpqnem}_i \\text{large}_{ij}] + [u_i + v_i \\text{large}_{ij} + \\epsilon_{ij}]\n\\]\n\nModel\nmodel.d &lt;- lmer(na ~ orch + mpqnem + large +\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† orch:large + mpqnem:large +\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† (large | id),\n¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† data = music)\n##¬† Groups¬† Name¬† ¬† ¬† ¬† Variance Std.Dev. Corr¬†\n##¬† id¬† ¬† ¬† (Intercept)¬† 3.286¬† 1.813¬† ¬† ¬† ¬† ¬†\n##¬† ¬† ¬† ¬† ¬† large¬† ¬† ¬† ¬† 0.557¬† 0.746¬† ¬† -0.38\n##¬† Residual¬† ¬† ¬† ¬† ¬† ¬† 21.811¬† 4.670\n##¬† Number of Level Two groups =¬† 37\n##¬† ¬† ¬† ¬† ¬† ¬† ¬†  Estimate Std. Error t value\n## (Intercept)¬†  16.25679¬† ¬† 0.54756 29.6893\n## orch¬† ¬† ¬† ¬† ¬†  1.00069¬† ¬† 0.81713¬† 1.2246\n## cmpqnem¬† ¬† ¬† ¬† 0.14823¬† ¬† 0.03808¬† 3.8925\n## large¬† ¬† ¬† ¬†  -1.23484¬† ¬† 0.84320 -1.4645\n## orch:large¬† ¬† -0.94927¬† ¬† 1.10620 -0.8581\n## cmpqnem:large -0.03018¬† ¬† 0.05246 -0.5753\nInterpretation\n\nCompared to Random Slopes and Intercepts Model (with 2 covariates):\n\nDirections of the effects of instrument (orch) and performance type (large) are consistent\nEffect sizes and levels of significance are reduced because of the relative importance of the negative emotionality (mpqnem) term\n\n\\(\\alpha_0 = 16.26\\) ‚Äî The estimated mean performance anxiety (outcome) for solos and small ensembles (large = 0) is 16.26 for keyboard players and vocalists (orch=0) with an average level of negative emotionality at baseline (mpqnem = 31.63)\n\\(\\hat \\alpha_1 = 1.00\\) ‚Äî Orchestral instrument (orch = 1) players have an estimated mean performance anxiety (outcome) level before solos and small ensembles (large = 0) which is 1.00 point higher than keyboardists and vocalists (orch = 0), controlling for the effects of baseline negative emotionality.\n\\(\\hat \\sigma^2 = 0.15\\) ‚Äî A one point increase in baseline negative emotionality (mpqnem = 0) is associated with an estimated 0.15 mean increase in performance anxiety (outcome) levels before solos and small ensembles (large = 0), after controlling for instrument (orch).\n\\(\\hat \\beta_0 = ‚àí1.23\\) ‚Äî Keyboard players and vocalists (orch = 0) with an average level of baseline negative emotionality levels (mpqnem = 31.63) have an estimated mean decrease in performance anxiety level (outcome) of 1.23 points before large ensemble performances (large = 1) compared to other performance types (large = 0).\n\nSee multi-level&gt;&gt; interpretation for explainer on the interaction slope interpretations\n\n\\(\\hat \\beta_1 = ‚àí0.95\\) ‚Äî After accounting for baseline negative emotionality (mpqnem = 0), orchestral instrument players (orch = 1) have an estimated mean performance anxiety level (outcome) before solos and small ensembles (large = 0) which is 1.00 point higher than keyboardists and vocalists (orch = 0), while the mean performance anxiety (outcome) of orchestral players (orch = 1) is only .05 points higher before large ensembles (large = 1) (a difference of .95 points).\n\nSee multi-level &gt;&gt; interpretation for explainer on the interaction slope interpretations\n\n\\(\\hat \\beta_2 = ‚àí0.03\\) ‚Äî After accounting for instrument, a one-point increase in baseline negative emotionality is associated with an estimated 0.15 mean increase in performance anxiety levels before solos and small ensembles, but only an estimated 0.12 increase before large ensembles (a difference of .03 points).\n\nLR-Test for comparing nested models\ndrop_in_dev &lt;- anova(model.d, model.c, test = \"Chisq\")\n#&gt; ¬† ¬† ¬† ¬† npar¬† AIC¬† BIC logLik¬† dev Chisq Df¬† ¬† ¬† pval\n#&gt; model.c¬† ¬† 8 3007 3041¬† -1496 2991¬† ¬† NA NA¬† ¬† ¬† ¬† NA\n#&gt; model.d¬†  10 2996 3039¬† -1488 2976 14.73¬† 2 0.0006319\n\nMLE must be used instead of REML to estimate the parameters of each model\nmodel.d is the current and more complex model, Random Slopes and Intercepts Model (with 3 covariates)\nmodel.c is the less complex model, Random Slopes and Intercepts Model (with 2 covariates)\nProcess\n\nThe likelihood is larger (and the log-likelihood is less negative) under the larger model (Model D);\n\\(\\text{Chisq} = 14.734 = -2 \\cdot (-1488 - (-1496))\\)\nUsing dof = 2, signifying the number of additional terms in Model D, we obtain a p-value of .0006.\n\nA p-value &lt; 0.05 indicates the difference in log-likelihoods is significantly different from 0, and therefore the more complex model, model.d, fits the data better.\nNote: dropping the mpqnem:large term which has a t-stat &lt; |2| (-0.5753) produces a better model than model.d according the LR-test\n\n\n\n\nFinal Model\n\nDescription\n\nOne of other valid final models\nPerformance type categorical is no longer collapsed into the binary ‚Äúlarge‚Äù ensemble/not large and is has now been collapsed into the binary, solo, not solo\nAudience categorical has been transformed to dummies: students, juried, public with instructor as the reference category.\nVarying/random slopes for previous, students, juried, public, and solo\nmpqnem is in the solo level 2 equation, so the combined model has an interaction between the two in the fixed effects.\n\nSpecification\n\nLevel 1: \\(Y_{ij} = a_i + b_i \\text{previous}_{ij} + c_i \\text{students}_{ij} + d_i \\text{juried}_{ij} + e_i \\text{public}_{ij} + f_i \\text{solo}_{ij} + \\epsilon_{ij}\\)\nLevel 2\n\\[\n\\begin{align}\na_i &= \\alpha_0 + \\alpha_1 \\text{mpqpem}_i + \\alpha_2 \\text{mpqab}_i + \\alpha_3 \\text{orch}_i + \\alpha_4 \\text{mpqnem}_i + u_i \\\\\nb_i &= \\beta_0 + v_i \\\\\nc_i &= \\gamma_0 + w_i \\\\\nd_i &= \\delta_0 + x_i \\\\\ne_i &= \\epsilon_0 + y_i \\\\\nf_i &= \\zeta_0 + \\zeta_1 \\text{mpqnem}_i + z_i\n\\end{align}\n\\]\n\nVariance-Covariance matrix\n\\[\n\\left[\\begin{array}{cc} u_i\\\\v_i\\\\w_i\\\\x_i\\\\y_i\\\\z_i \\end{array} \\right]\n\\sim \\mathcal{N} \\left(\n\\left[\\begin{array}{cc} 0\\\\0\\\\0\\\\0\\\\0\\\\0\\end{array} \\right],\n\\begin{bmatrix}\n\\sigma_u^2 \\\\\n\\sigma_{uv} & \\sigma_v^2 \\\\\n\\sigma_{uw} & \\sigma_{vw} & \\sigma^2_w \\\\\n\\sigma_{ux} & \\sigma_{vx} & \\sigma_{wx} & \\sigma_x^2 \\\\\n\\sigma_{uy} & \\sigma_{vy} & \\sigma_{wy} & \\sigma_{xy} & \\sigma_{y}^2 \\\\\n\\sigma_{uz} & \\sigma_{vz} & \\sigma_{wz} & \\sigma_{xz} & \\sigma_{yz} & \\sigma_z^2\n\\end{bmatrix}\n\\right)  \n\\]\n6 variance terms and 15 correlation terms at Level Two, along with 1 variance term at Level One.\n\nThe number of correlation terms is equal to the number of unique pairs among Level Two random effects\n\n\n\nModel\n# Model F (One - of many - reasonable final models)\nmodel.f &lt;- lmer(na ~ previous + students + juried +¬†\n¬† ¬† public + solo + mpqpem + mpqab + orch + mpqnem +¬†\n¬† ¬† mpqnem:solo + (previous + students + juried +¬†\n¬† ¬† public + solo | id), REML = T, data = music)\n\n##¬† Groups¬† Name¬† ¬† ¬† ¬† Variance Std.Dev. Corr¬† ¬† ¬† ¬†\n##¬† id¬† ¬† ¬† (Intercept) 14.4802¬† 3.805¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬†\n##¬† ¬† ¬† ¬† ¬† previous¬† ¬†  0.0707¬† 0.266¬† ¬† -0.65¬† ¬† ¬†\n##¬† ¬† ¬† ¬† ¬† students¬† ¬†  8.2151¬† 2.866¬† ¬† -0.63¬† 0.00\n##¬† ¬† ¬† ¬† ¬† juried¬† ¬† ¬† 18.3177¬† 4.280¬† ¬† -0.64 -0.12\n##¬† ¬† ¬† ¬† ¬† public¬† ¬† ¬† 12.8094¬† 3.579¬† ¬† -0.83¬† 0.33\n##¬† ¬† ¬† ¬† ¬† solo¬† ¬† ¬† ¬†  0.7665¬† 0.876¬† ¬† -0.67¬† 0.47\n##¬† Residual¬† ¬† ¬† ¬† ¬† ¬† 15.2844¬† 3.910¬† ¬† ¬†\n##¬† ¬† ¬† ¬† ¬† ¬† ¬†\n##¬† 0.84¬† ¬† ¬† ¬† ¬† ¬†\n##¬† 0.66¬† 0.58¬† ¬† ¬†\n##¬† 0.49¬† 0.21¬† 0.90\n##¬†\n##¬† Number of Level Two groups =¬† 37\n##¬† ¬† ¬† ¬† ¬† ¬† Estimate Std. Error t value\n## (Intercept)¬† 8.36883¬† ¬† 1.91369¬† 4.3731\n## previous¬† ¬† -0.14303¬† ¬† 0.06247 -2.2895\n## students¬† ¬†  3.61115¬† ¬† 0.76796¬† 4.7022\n## juried¬† ¬† ¬†  4.07332¬† ¬† 1.03130¬† 3.9497\n## public¬† ¬† ¬†  3.06453¬† ¬† 0.89274¬† 3.4327\n## solo¬† ¬† ¬† ¬†  0.51647¬† ¬† 1.39635¬† 0.3699\n## mpqpem¬† ¬† ¬† -0.08312¬† ¬† 0.02408 -3.4524\n## mpqab¬† ¬† ¬† ¬† 0.20377¬† ¬† 0.04740¬† 4.2986\n## orch¬† ¬† ¬† ¬†  1.53138¬† ¬† 0.58384¬† 2.6230\n## mpqnem¬† ¬† ¬†  0.11465¬† ¬† 0.03591¬† 3.1930\n## solo:mpqnem¬† 0.08296¬† ¬† 0.04158¬† 1.9951\nInterpretation\n\nIn general\n\nPerformance anxiety (outcome) is higher when a musician is performing in front of students, a jury, or the general public rather than their instructor\n\nstudents, juried, and public are indicator variables created from the audience categorical variable (so that ‚ÄúInstructor‚Äù is the reference level in this model)\n\nPerformance anxiety (outcome) is lower for each additional diary the musician previously filled out\n\nprevious is the number of previous diary entries filled out by that individual\n\nMusicians with lower levels of positive emotionality (mpqpem) and higher levels of absorption (mpqab) tend to experience greater performance anxiety (outcome)\nThose who play orchestral instruments experience (orch = 1) more performance anxiety than those who play keyboards or sing (orch = 0)\n\nKey terms\n\n\\(\\hat \\alpha_4 = 0.11\\) (mpqnem, fixed) ‚Äî A one-point increase in baseline level of negative emotionality (mpqnem) is associated with an estimated 0.11 mean increase in performance anxiety (outcome) for musicians performing in an ensemble group (solo=0), after controlling for previous diary entries (previous), audience (dummy vars), positive emotionality (mpqpem), absorption (mpqab), and instrument (orch).\n\\(\\hat \\zeta_1 = 0.08\\) (solo:mpqnem, fixed) ‚Äî When musicians play solos (solo = 1), a one-point increase in baseline level of negative emotionality (mpqnem) is associated with an estimated 0.19 mean increase in performance anxiety (outcome), 0.08 points (73%) higher than musicians playing in ensemble groups (solo = 0), controlling for the effects of previous diary entries (previous), audience (dummy vars), positive emotionality (mpqpem), absorption (mpqab), and instrument (orch).\n\nSee multi-level &gt;&gt; interpretation for explainer on the interaction slope interpretations\n\n\nAddressing the researchers‚Äô primary hypothesis:\n\nAfter controlling for all these factors, we have significant evidence that musicians with higher levels of negative emotionality (mpqpem) experience higher levels of performance anxiety (outcome), and that this association is even more pronounced (interaction) when musicians are performing solos (solo = 1) rather than as part of an ensemble group (solo = 0).",
    "crumbs": [
      "Econometrics",
      "Mixed Effects"
    ]
  },
  {
    "objectID": "qmd/econometrics-mixed-effects-frequentist.html#sec-econ-me-freq-bmlrch9",
    "href": "qmd/econometrics-mixed-effects-frequentist.html#sec-econ-me-freq-bmlrch9",
    "title": "Mixed Effects",
    "section": "BMLR Chapter 9: Longitudinal Data",
    "text": "BMLR Chapter 9: Longitudinal Data\n\nBeyond Multiple Linear Regression, Chapter 9: Two-Level Longitudinal Data\nResearch Questions\n\nWhich factors most influence a school‚Äôs performance in Minnesota Comprehensive Assessment (MCA) testing?\nHow do the average math MCA-II scores for 6th graders enrolled in charter schools differ from scores for students who attend non-charter public schools? Do these differences persist after accounting for differences in student populations?\nAre there differences in yearly improvement between charter and non-charter public schools?\n\nData Answers\n\nWithin school‚Äîchanges over time\nBetween schools‚Äîeffects of school-specific covariates (charter or non-charter, urban or rural, percent free and reduced lunch, percent special education, and percent non-white) on 2008 math scores and rate of change between 2008 and 2010.\n\nMisc\n\nMissing data is a common phenomenon in longitudinal studies\n\n\n\n\n\nWithin-Subject\nBetween-Subject",
    "crumbs": [
      "Econometrics",
      "Mixed Effects"
    ]
  },
  {
    "objectID": "qmd/econometrics-mixed-effects-frequentist.html#sec-econ-me-freq-modeq",
    "href": "qmd/econometrics-mixed-effects-frequentist.html#sec-econ-me-freq-modeq",
    "title": "Mixed Effects",
    "section": "Model Equation",
    "text": "Model Equation\n\nMisc\n\nNotes from Video Playlist: Mixed Model Series\n\nCompanion resource to the videos. Contains more details, code, and references.\n\nAlso see the BMLR Chapter 8 section for a similar description\nIt‚Äôs useful to consider the model in two stages even though all the parameters are estimated at the same time (See Combined equation)\n\nIf all subjects have the same number of observations, the 2-Stage approach and Mixed Model should have very similar results.\nA use case for the 2-Stage approach is comparing the correlation between a dependent variable and two independent variables (i.e.¬†\\(Y \\sim X_1\\) and \\(Y \\sim X_2\\)) when the data is repeated measures.\n\nSubjects (aka units) should be balanced if you want valid standard errors (See Issues)\nExecute the 2-Stage approach for both model formulas and the 2nd Stage‚Äôs coefficient will be the Pearson Correlation coefficient. (Code from MMS Resource, also mentioned at the end of video 4 or 5)\n\n\nIssues with only using a 2-stage approach\n\nNot all random effects structures can be modelling using 2 stages\nThe variance estimate in the 2nd stage will be incorrect\n\nThe standard errors will be too small which will inflate the type I error rate.\nThis is most prevalent is designs with an imbalanced binary group variable fixed effect (e.g.¬†treatment/control) where one group has fewer subjects than the other group (See MMS resource/video 6)\nThere is a loss of power in designs that have an imbalance in the binary group variable and imbalance in the number of observations per subject. (See MMS resource/video 6)\n\nMixed Effects models have built-in regularization.\n\n\n2-Stage Model\n\n1st Stage\n\\[\nY_i = \\beta_iZ_i + \\epsilon_i, \\quad \\epsilon \\sim \\mathcal N (0, \\sigma^2I_{n_i})\n\\]\n\n\\(\\beta_i Z_i\\) is the slope and intercept times the design matrix for subject \\(i\\)\n\\(\\epsilon\\) is the within-subject error\n\\(\\sigma^2I_{n_i}\\) is the within-subject variance matrix which is just a diagonal matrix with \\(\\sigma^2\\)s along the diagonal which is the within-subject variation.\n\nIn this case \\(\\sigma^2\\) is constant for all subjects. (i.e.¬†the variance is pooled). Preferred method when you have small data which makes estimation of subject-level variance difficult. For\n\n\\(\\beta_i\\) says that there‚Äôs a slope for each subject which makes sense if there‚Äôs repeated measures\n\n2nd Stage\n\\[\n\\beta_i = \\beta A_i + b_i, \\quad b_i \\sim \\mathcal N (0, G)\n\\]\n\n\\(\\beta\\) is a group parameter estimate (i.e.¬†avarage intercept and average slope across all subjects). The intercept will be the grand mean and other coefficients are the fixed effects coefficients.\n\\(A_i\\) is a design matrix\n\nTypically an identity matrix unless it‚Äôs a more complex model\n\n\\(b_i\\) is the random effect between-subject error. Isn‚Äôt actually a subject-level vector of residuals. It‚Äôs an estimated distribution ‚Äî typically Gaussian.\n\n\\(G\\) is a 2 \\(\\times\\) 2 Covariance matrix (in this case). Same for all subjects.\n\nUpper-Left: Between-Subject Intercept Variance\nBottom-Right: Between-Subject Slope Variance\nUpper-Right: Covariance between slope and intercept (i.e.¬†a dependence between slope and intercept)\n\nSame for Bottom-Left.\n\n\n\n\nCombined: \\(Y_i = \\beta Z_i A_i + b_i Z_i + \\epsilon_i\\)\n\nTerm Estimates\nlibrary(lme4); library(Matrix)\ndata(\"sleepstudy\")\n# varying slopes and varying intercepts\nlmer_mod &lt;- \n  lmer(Reaction ~ 1 + Days + (1 + Days | Subject),\n       sleepstudy,\n       REML = 0)\nsummary(lmer_mod)\n\n#&gt; Random effects:\n#&gt;  Groups   Name        Variance Std.Dev. Corr\n#&gt;  Subject  (Intercept) 565.48   23.780       \n#&gt;           Days         32.68    5.717   0.08\n#&gt;  Residual             654.95   25.592       \n#&gt; Number of obs: 180, groups:  Subject, 18\n\n\\(\\sigma^2\\) is Residual = 654.95 (variance of the within-subject error)\n\nInterpreted as the within-subject variance. It‚Äôs the average variance of each subject‚Äôs response value in relation to their within-subject average response value.\n\n\\(G\\)\n\nSubject (Intercept) = 565.52 (Variance) (Upper-Left: Between-Subject Intercept Variance)\nDays = 32.48 (Variance) (Between-Subject Slope Variance)\nDays = 0.08 (Corr) (Upper-Right: Covariance between slope and intercept)\n\n\nConditional Modes\n\nCurrently I‚Äôm not sure what these are. I‚Äôve read/watched a few different things that state or imply different definitions of the term.\n\nFitted Values: These are essentially just fitted values for a mixed effects model. The difference is having a different intercept value for each subject id and/or a different slope for each subject id. (Mumford from Mixed Models Series)\n\nMumford is following a book on Longitudinal data. I‚Äôm leaning towards this definition.\n\nRandom Slopes/Intercepts: These take each subject‚Äôs random effect and adds it the fixed effect (model avereage) (Michael Clark implies this in Mixed Models in R)\nRandom Effects: {lme4}‚Äôs functional documentation and vignette imply that these are conditional modes through their function defintions (Bates et al)\n\nAKA BLUPs (Best Linear Unbiased Predictor). I‚Äôve seen this terminology discouraged a couple different places though.\n\nI think BLUPs comes from the original paper and Conditional Modes comes from the author of {lme4}. Both should be lost to history.\n\nConditional Modes (\\(\\hat \\beta\\)) as function of OLS and the Mixed Model Grand Mean (aka fixed effects intercept)\n\\[\n\\begin{align}\n&\\hat\\beta_i = W_i\\hat\\beta_i^{OLS}+(I_q-W_i)A_i\\hat\\beta \\\\\n&\\text{where} \\quad W_i= \\frac{G}{G+\\sigma^2(Z_i'Z_i)^{-1}}\n\\end{align}\n\\]\n\nNot essential to know this but it gives you insight to how the 2-Stage model and the Mixed Model estimates will differ. It also gives insight into what affects the amount of Regularization. (See Considerations &gt;&gt; Partial Pooling)\nThe top equation shows that if the weight \\(W\\) is large (e.g.¬†\\(1\\)), then the OLS estimate (subject-level means) will be close the mixed effects estimate\nThe bottom equation shows that if the Between-Subject Variance (\\(G\\)) is much larger than the Within-Subjects Variance (\\(\\sigma^2\\)), then the weight will be large (i.e.¬†close to \\(1\\)).\nFor a simple varying intercepts model \\(Z'Z\\) is just \\(\\frac{1}{N_i}\\). For a small, \\(N_i\\), \\(W\\) will be small which leads to the grand mean estimate, \\(\\hat\\beta\\), dominating the conditional mode equation which means more regularization for that subject.\n\nGet the conditional modes whatever they might be:\n\nFitted Values:\nfitted(lme_mod)\n# or\npredict(lme_mod,\n        new_data)\nRandom Slopes or Intercepts:\nrand_int &lt;- ranef(lme_mod)$group_var$`(Intercept)` + fixef(lme_mod$group_var$`(Intercept)`)\nrand_slope &lt;- ranef(lme_mod)$group_var$slope_var + fixef(lme_mod$group_var$slope_var)\n\nIf conditonal modes are just the random effects then ranef(lme_mod)\n\n\nExamples: Manual Method (Assumes Fitted Values are Conditional Modes)\n\nFixed Effect + Varying InterceptsVarying Slopes and Intercepts\n\n\nglimpse(dat.mod)\n#&gt; Rows: 1,500\n#&gt; Columns: 3\n#&gt; $ id      &lt;fct&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2‚Ä¶\n#&gt; $ y       &lt;dbl&gt; -18.5954522, -0.3249434, 12.8352423, -6.1749271, 7.1217294, 14.5693488, -2.4363220, -23.1468863, -4.016‚Ä¶\n#&gt; $ grp.all &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1‚Ä¶\n\nmod.lmer &lt;-  lmer(y ~ grp.all + (1 | id), \n                  dat.mod)\n\n# Using the model equation\ngroup.vec &lt;- rep(c(1, 0), each = 25) # distinct group.all values/num of subjects\ncond.modes &lt;- coef(mod.lmer)$id[, 1] + coef(mod.lmer)$id[, 2]*group.vec\n\n# Using predict()\nnew_dat &lt;- \n  dat.mod |&gt; \n  distinct(id, grp.all)\nme_preds &lt;- predict(mod.lmer2, \n                    new_dat)\n\nhead(cond.modes); head(as.numeric(me_preds))\n#&gt; [1] 10.425813 14.724778 12.815295  8.282038  4.234039  9.870040\n#&gt; [1] 10.425813 14.724778 12.815295  8.282038  4.234039  9.870040 \n\nIn a basic RCT design, grp.all would be your treatment/control variable and id would be your subject (aka unit) id.\n\n\n\nfm1 &lt;- \n  lmer(Reaction ~ Days + (Days|Subject), \n       sleepstudy)\n\n# random effects\nre_intercepts &lt;- ranef(fm1)$Subject$`(Intercept)`\nre_slopes &lt;- ranef(fm1)$Subject$`Days`\n# fixed effects\nfe_intercept &lt;- fixef(fm1)[1] # grand mean\nfe_slope &lt;- fixef(fm1)[2] # average slope for Days\n\nrandom_intercepts &lt;- fe_intercept + re_intercepts\nrandom_slopes &lt;- fe_slope + re_slopes\n\nrandom_df &lt;- tibble(\n  Subject = as.factor(levels(sleepstudy$Subject)),\n  random_slopes = random_slopes,\n  random_intercepts = random_intercepts\n)\n\n# Join each random_* to the repeated measures rows for Subject\nrandom_df_full &lt;- \n  sleepstudy |&gt; \n  select(Subject) |&gt; \n  left_join(random_df,\n            by = \"Subject\")\n\ncond_modes &lt;- random_df_full$random_intercepts + (random_df_full$random_slopes * sleepstudy$Days)\nfitted_values &lt;- fitted(fm1)\nhead(cond_modes); head(as.numeric(fitted_values))\n#&gt; [1] 253.6637 273.3299 292.9962 312.6624 332.3287 351.9950\n#&gt; [1] 253.6637 273.3299 292.9962 312.6624 332.3287 351.9950\n\nShows how random effects are used to get the conditional modes.\n\\[\n\\begin{aligned}\n\\text{Conditional Modes}_i &= (\\text{Random Effect}_{\\text{intercept}_i} + \\text{Fixed Effect}_{\\text{intercept}}) \\\\ &+ (\\text{Random Effect}_{\\text{Days}_i} + \\text{Fixed Effect}_{\\text{Days}}) \\times \\text{Days}_{i,j}\n\\end{aligned}\n\\]\n\nDays is the repeated measures id so to speak. In this study, each Subject (\\(i\\)) has 10 Days (\\(j\\)) of follow-up measurements",
    "crumbs": [
      "Econometrics",
      "Mixed Effects"
    ]
  },
  {
    "objectID": "qmd/geospatial-remote-sensing.html",
    "href": "qmd/geospatial-remote-sensing.html",
    "title": "Remote Sensing",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Geospatial",
      "Remote Sensing"
    ]
  },
  {
    "objectID": "qmd/geospatial-remote-sensing.html#sec-geo-rs-misc",
    "href": "qmd/geospatial-remote-sensing.html#sec-geo-rs-misc",
    "title": "Remote Sensing",
    "section": "",
    "text": "Resources\n\nGLCM Texture: A Tutorial\n\nTypes of Satellite Imagery\n\nOptical images : Images are taken by optical sensors (sensor is passive)\n\nGenerally provide a high level of detail\nLimited by nighttime conditions and adverse weather.\n\nRadar images : Radars send an electromagnetic wave and measure the component backscattered by the objects on the ground (the sensor is active).\n\nAcquires data at any time of the day and with any meteorological conditions, as the wavelength of the transmitted wave allows it to penetrate clouds.\nQuality is degraded by speckle noise.\n\n\nTypes of Measures\n\nTexture - Descriptive statistic that measures spatial relationships\n\nValues cannot be transferred from one situation to another\n\ne.g.¬†you can‚Äôt say, ‚Äúforests always have Contrast values between .5 and .7‚Äù\n\nPrimarily useful in comparing one part of an image to another part\n\nFor multi-image comparison (e.g.¬†mosaic):\n\nThe images analysed must be equivalent radiometrically, in regards to sun angle, and phenologically with regards to cyclically variable ground phenomena\n\n\n\nSpectral - Descriptive statistics that essentially measure chemical properties of the ground objects\n\nSpectral and spatial are very likely to be independent data and so complement one another\nGrey Level Co-occurrence Matrix (GCLM) - Used for texture measurements. A tabulation of how often different combinations of pixel brightness values (grey levels) occur in an image.\nPCA Issues\n\nEach new dataset requires recalculation of both, landscape metrics and principal components analysis (PCA)\nHighly correlated landscape metrics are used\nPCA results interpretation is not straightforward\n\nInformation Theory (IT) Based Metrics\n\nMarginal Entropy [H(x)] - Diversity (composition) of spatial categories - from monothematic patterns to multithematic patterns\nRelative Mutual Information [U] - Clumpiness (configuration) of spatial categories from fragmented patterns to consolidated patterns)\nH(x) and U are uncorrelated\nIssues\n\nRelative mutual information is a result of dividing mutual information by entropy. What to do when the entropy is zero?\nHow to incorporate the meaning of categories into the analysis?",
    "crumbs": [
      "Geospatial",
      "Remote Sensing"
    ]
  },
  {
    "objectID": "qmd/geospatial-remote-sensing.html#sec-geo-rs-terms",
    "href": "qmd/geospatial-remote-sensing.html#sec-geo-rs-terms",
    "title": "Remote Sensing",
    "section": "Terms",
    "text": "Terms\n\nNormalized Difference Vegetation Index (NDVI) - A widely-used metric for quantifying the health and density of vegetation using sensor data. It is calculated from spectrometric data at two specific bands: red and near-infrared. The spectrometric data is usually sourced from remote sensors, such as satellites.\n\nRange: -1 and 1\nInterpretation\n\n0: Area has nothing growing (e.g.¬†Deserts)\n1: Arean has dense, healthy vegetation\n&lt;0: Suggest lack of dry land (e.g.¬†oceans have NDVI = -1)\n\n\nSemantic Segmentation - The process of labelling pixels or regions of the image\n\nEssential in many applications including infrastructure planning, land cover, humanitarian crisis maps and environmental assessments.\n\nSpeckle Noise - Granular interference due to bouncing properties of emitted radio waves that degrades the quality of images and therefore their interpretability with a human eye.",
    "crumbs": [
      "Geospatial",
      "Remote Sensing"
    ]
  },
  {
    "objectID": "qmd/geospatial-remote-sensing.html#sec-geo-rs-patt",
    "href": "qmd/geospatial-remote-sensing.html#sec-geo-rs-patt",
    "title": "Remote Sensing",
    "section": "Pattern-based",
    "text": "Pattern-based\n\nEnables spatial analyses of raster data such as searching, change detection, clustering, or segmentation\nMisc\n\nNotes from Analysis of Spatial Patterns: Current State and Future Challenges (Slides)\n\nUse-Cases\n\nFinding similar spatial structures (one-to-many)\n\n\nTake the normalized cove of the mountain (or other structure) and compare other local areas with it. (i.e.¬†which coves are least dissimilar the mountain cove)\n\nQuantitative assessment of changes in spatial structures (one-to-one)\n\n\nThis pic represents the change in land coverage in the Amazon from two different time periods\nTake the normalized coves from the earlier time period and make a one-to-one comparison (i.e.¬†Calculate the difference in JSDs) with the coves of the currrent time period\nAreas with the greatest change have the highest JSD values.\n\nClustering similar spatial structure (many-to-many)\n\n\nCluster the normalized coves\nMetrics\n\nIntra-cluster heterogeneity - determines distances between all landscapes within a group\nInter-cluster isolation - determines distances between a given group and all others\n\n\n\nSteps\n\nDivide data into a large number of smaller areas (local landscapes)\nRepresent each area using a statistical description of the spatial pattern - a spatial signature.\n\nMost landscape metrics are single numbers representing specific features of a local landscape. Spatial signatures, on the other hand, are multi-element representations of landscape composition and configuration.\nThe basic signature is the co-occurrence matrix:\n\n\n\n\nagriculture\nforest\ngrassland\nwater\n\n\n\n\nagriculture\n272\n218\n4\n0\n\n\nforest\n218\n38778\n32\n12\n\n\ngrassland\n4\n32\n16\n0\n\n\nwater\n0\n12\n0\n2\n\n\n\n\nLand Coverage Categories: agriculture, forest, grassland, water, wetland, settlement, shrubland, sparse vegetation, bare area.\nLandform Categories: flat or nearly flat plains, smooth plains with some local relief, irregular plains with moderate relief, irregular plains with low hills, scattered moderate hills, moderate hills, scattered high hills, high hills, scattered low mountains, low mountains, scattered high mountains, high mountains, tablelands with moderate relief, tablelands with considerable relief, tablelands with high relief, tablelands with very high relief, surface water.\nI believe this is a comparison of two local landscapes where, for example, ag vs grass = 4 indicates there are 4 grid cells that coincide to a grassland in one local area and an agricultural area in the other local area.\n\nA spatial signature should allow simplification to the form of a normalized vector\nNormalized Co-Occurence Vector:\n\nCo-Occurence Vector (cove) - c(272, 218, 4, 0, 218, 38778, 32, 12, 4, 32, 16, 0, 0, 12, 0, 2)\n\nNumbers are taken from the co-occurrence matrix (See above) where the rows are combined end-to-end to create a vector.\n\nSimplified Co-Occurence Vector (cove) - c(136 , 218, 19389, 4, 32, 8, 0, 12, 0, 1)\n\nThe process name wasn‚Äôt mentioned in the slide, but here, the cove has been simplified by creating a vector with the halved diagonal values and unique values of off-diagonal cells in the original cove (Doubt order matters).\n\nNormalized Co-Occurence Vector\nsimple_cove &lt;- c(136, 218, 19389, 4, 32, 8, 0, 12, 0, 1)\nmoose &lt;- as.matrix(simple_cove)\nround(simple_cove/norm(moose), 4)\n#&gt; [1] 0.0069 0.0110 0.9792 0.0002 0.0016 0.0004 0.0000 0.0006 0.0000 0.0001\n\n\nSpatial signatures can be compared using a large number of existing distance or dissimilarity measures\n\nDissimilarity\n\nMeasuring the distance between two signatures in the form of normalized vectors allows determining dissimilarity between spatial structures.\nExample: Jensen-Shannon Divergence - Lower\n\n\n\n\n\n\n\n\nReference (cove_ref)\n\n\n\n\n\n\n\nArea of Interest (cove_x1)\n\n\n\n\n\n\ncove_ref &lt;- c(0.0069, 0.011, 0.9792, 0.0002, 0.0016, 0.0004, 0, 0.0006, 0, 0.0001)\ncove_x1 &lt;- c(0.1282, 0.0609, 0.8105, 0.0002, 0.0002, 0.0001, 0, 0, 0, 0)\ncove_mat1 &lt;- rbind(cove_ref, cove_x1)\n\nphilentropy::JSD(cove_mat)\n#&gt; jensen-shannon \n#&gt;     0.06826663\n\nA lower JSD means the two images are less dissimilar (i.e more similar)\n\nExample: Jensen-Shannon Divergence - Higher\n\n\n\n\n\n\n\n\nReference (cove_ref_ext)\n\n\n\n\n\n\n\nArea of Interest (cove_x1)\n\n\n\n\n\n\n# zeros added to match length of cov_x2\ncove_ref_ext &lt;- c(0.0069, 0.011, 0.9792, 0.0002, 0.0016, 0.0004, 0, 0.0006, 0, 0.0001, 0, 0, 0, 0, 0)\ncove_x2 &lt;- c(0.2033, 0.1335, 0.2944, 0.1747, 0.0562, 0.1307, 0.0035, 0.0002, 0.0004, 0.0015, 0.0007, 0.0005, 0, 0, 0.0005)\ncove_mat2 &lt;- rbind(cove_ref_ext, cove_x2)\n\nphilentropy::JSD(cove_mat2)\n#&gt; jensen-shannon \n#&gt;      0.4444198 \n\nA higher JSD means the two images are more dissimilar.\n\n\n\n\n\n\n\n\nReference (cove_ref)\nArea of Interest (cove_x1)\nReference (cove_ref_ext)\nArea of Interest (cove_x1)",
    "crumbs": [
      "Geospatial",
      "Remote Sensing"
    ]
  },
  {
    "objectID": "qmd/algorithms-ml.html",
    "href": "qmd/algorithms-ml.html",
    "title": "1¬† ML",
    "section": "",
    "text": "Discriminant Analysis",
    "crumbs": [
      "Algorithms",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>ML</span>"
    ]
  },
  {
    "objectID": "qmd/algorithms-ml.html#sec-alg-ml-discrim",
    "href": "qmd/algorithms-ml.html#sec-alg-ml-discrim",
    "title": "1¬† ML",
    "section": "",
    "text": "Misc\n\nThe features to train Quadratic Discriminant Analysis (QDA) should be strictly normally distributed, making it easy for QDA to calculate and fit an ellipsoid shape around the distribution\nVery fast even on a 1M row dataset\n\nLinear Discriminant Analysis (LDA)\n\nNotes from StatQuest: Linear Discriminant Analysis (LDA) clearly explained video\nGoal is find an axis (2 dim) for a binary outcome or plane (3 dim) for a 3-category outcome, etc. that separates the predictor data which is grouped by the outcome categories.\nHow well the groups are separated is determined by projecting the points on this lower dim object (e.g.¬†axis, plane, etc.) and looking at these criteria:\n\nDistance (d) between the means of covariates (by outcome group) should be maximized\nScatter (s2) ,i.e.¬†variation, of data points per covariate (by outcome group) should be minimized\n\nMaximizing the ratio of Distance to Scatter determines the GoF of the separator\n\n\nFigure shows a example of a 2 dim predictor dataset that been projected onto a 1 dim axis. Dots are colored according to a binary outcome (green/red)\nIn the binary case, the difference between the means is the distance, d.\n\nFor multinomial outcomes, there are a couple differences:\n\nA centroid between all the predictor data is chosen, and centroids within each category of the grouped predictor data are chosen. For each category, d is the distance between the group centroid and the overall centroid\n\n\nThe chosen group predictor centroids are determined by maximizing the distance-scatter ratio\n\nUsing the coordinates of the chosen group predictor centroids, a plane is determined.\n\n\nFor a 3 category outcome, 2 axes (i.e.¬†a plane) are determined which will optimally separate the outcome categories\n\n\nBy looking at which predictors are most correlated with the separator(s), you can determine which predictors are most important in the discrimination between the outcome categories.\nThe separation can be visualized by using charting the data according to the separators\n\n\nExample shows the data is less overlap between black and blue dots and therefore grouped better using LDA than PCA.",
    "crumbs": [
      "Algorithms",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>ML</span>"
    ]
  },
  {
    "objectID": "qmd/algorithms-ml.html#sec-alg-ml-svm",
    "href": "qmd/algorithms-ml.html#sec-alg-ml-svm",
    "title": "1¬† ML",
    "section": "Support Vector Machines",
    "text": "Support Vector Machines\n\n\nMisc\n\nPackages: {e1071}, {kernlab}, {LiblineaR}, {{sklearn}}\nAlso see\n\nModel Building, tidymodels &gt;&gt; Model Specification &gt;&gt; Support Vector Machines\nModel Building, sklearn &gt;&gt; Misc &gt;&gt; Tuning\n\n\nProcess\n\nWorks on the principle that you can linearly separate a set of points from another set of point simply by transforming the dataset from dimension n to dimension n + 1.\n\nThe transformation is made by a feature transformation function, œÜ(x). For two dimensions, a particular œÜ(x) might transform the vector, x = {x‚ÇÅ, x‚ÇÇ}, which is in 2 dimensions, to {x¬≤‚ÇÅ, ‚àö(2x‚ÇÅx‚ÇÇ), x¬≤‚ÇÇ}, which is 3 dimensions\n\nTransforming a set of vectors into a higher dimension, performing a mathematical operation (e.g.¬†dot product), and transforming the vectors back to lower dimension is involves many steps and therefore is computationally expensive.\n\nThe problem can become computationally intractable fairly quickly. Kernels are able perform these operations in much fewer steps.\n\nCreates a hyperplane at a threshold that is equidistant between classes of the target variable\nEdge observations are called Support Vectors and the distance between them and the threshold is called the Maximum Margin\n\nKernels\n\nGaussian Kernel\n\\[\nK(x,y) = e^{-\\gamma \\lVert x - y \\rVert^2}\n\\]\n\nK(x,y) performs the dot product in the higher dimensional space without having to first transform the vectors\n\n\nHyperparameters\n\ngamma ‚Äì All the kernels except the linear one require the gamma parameter. ({e1071} default: 1/(data dimension)\ncoef0 ‚Äì Parameter needed for kernels of type polynomial and sigmoid ({e1071} default: 0).\ncost ‚Äì The cost of constraints violation ({e1071} default: 1)‚Äîit is the ‚ÄòC‚Äô-constant of the regularization term in the Lagrange formulation.\n\nC = 1/Œª (R) or 1/Œ± (sklearn)\nWhen C is small, the regularization is strong, so the slope will be small\n\ndegree - Degree of the polynomial kernel function ({e1071} default: 3)\nepsilon - Needed for insensitive loss function (see Regression below) ({e1071} default: 0.1)\n\nWhen the value of epsilon is small, the model is robust to the outliers.\nWhen the value of epsilon is large, it will take outliers into account.\n\nnu - For {e1071}, needed for types: nu-classification, nu-regression, and one-classification\n\nRegression\n\n\nStochastic Gradient Descent is used in order minimize MAE loss\n\nAlso see\n\nModel building, sklearn &gt;&gt; Stochaistic Gradient Descent (SGD)\nLoss Functions &gt;&gt; Misc &gt;&gt; Mean Absolute Error (MAE)\n\n\nEpsilon Insensitive Loss - The idea is to use an ‚Äúinsensitive tube‚Äù where errors less than epsilon are ignored. For errors &gt; epsilon, the function is linear.\n\nEpsilon defines the ‚Äúwidth‚Äù of the tube.\nSee Loss Functions &gt;&gt; Huber loss for something similar\nSquared Epsilon Insensitive loss is the same but becomes squared loss past a tolerance of epsilon\n\nL2 typically used the penalty\nIn SVM for classification, ‚Äúmargin maximization‚Äù is the focus which is equivalent to the coefficient minimization with a L2 norm. For SVR, usually the focus is on ‚Äúepsilon insensitive.‚Äù\n\nVisualization\n\nDecision Boundary\n\nExample\n\n# Create a grid of points for prediction\nx1_grid &lt;- seq(min(data$x1), max(data$x1), length.out = 100)\nx2_grid &lt;- seq(min(data$x2), max(data$x2), length.out = 100)\ngrid &lt;- expand.grid(x1 = x1_grid, x2 = x2_grid)\n\npredicted_labels &lt;- predict(svm_model, newdata = grid)\n\nplot(data$x1, data$x2, col = factor(data$label), pch = 19, main = \"SVM Decision Boundary\")\npoints(grid$x1, grid$x2, col = factor(predicted_labels), pch = \".\", cex = 1.5)\nlegend(\"topright\", legend = levels(data$label), col = c(\"red\", \"blue\"), pch = 19)\n\n(Legend colors in the wrong order)\nx1_grid and x2_grid provide equally spaced points within the range of sample data.\ngrid is a df of all combinations of these points\nThe predicted labels from [grid] are colored and visualize the decision boundary.\n{e1071} provides a plot function that also does this: plot(svm_model, data = data, color.palette = heat.colors)",
    "crumbs": [
      "Algorithms",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>ML</span>"
    ]
  },
  {
    "objectID": "qmd/algorithms-ml.html#sec-alg-ml-trees",
    "href": "qmd/algorithms-ml.html#sec-alg-ml-trees",
    "title": "1¬† ML",
    "section": "1.1 Trees",
    "text": "1.1 Trees\n\nAlgorithmic models that recursively split the data into smaller and more homogeneous subgroups. Predictions are the same for every member of the subgroup (aka piece-wise constant). Forests smooth out the piecewise predictions by averaging over groups of trees.\nHow Tree models get probabilities\n\nMethod 1: Each tree predicts the class of x according to the leaf node x falls within. The leaf node output is the majority class of the training points it contains. The predictions of all trees are considered as votes, and the class with the most votes is taken as the output of the forest. This is the original formulation of random forests proposed by Breiman (2001).\nMethod 2: Each tree outputs a vector [p1,‚Ä¶,pk], where k is the number of classes, representing the predicted probability of each class given x. This may be estimated as the relative class frequencies of training points in the leaf node x falls within. The forest output is the average of these vectors across trees, representing a conditional distribution over classes given x.\nExample: Data point falls into a tree‚Äôs leaf where yellow is the predicted class (2nd child split) \n\nFor this tree in the ensemble, there are 3 yellow and 1 green in the terminal leaf (2nd child split). Therefore the probability of Yellow is 75%.\nFor each class, the trees that predict that class have their probabilites averaged to produced the predicted probability for that class.\n\n\n\n\n1.1.1 Decision Trees\n\nMisc\n\nPCA improves DT predictive performance in two important ways (py example):\n\nOrients key features together (that explain the most variance)\n\nDTs create orthogonal decision boundaries and PCs are orthogonal to each other.\n\nReduces the feature space\n\n\nClassification\n\nShowing entropy but misclassification error or the gini index can be used.\nCalculate Shannon Entropy of dependent variable (Y):\n where P(Y) is the marginal probability\n\nFor a binary variable, this would be\n\nIn general the Shannon Entropy equation is\n where p is a probability and c is the number of classes for the variable and S = subset of data or the node.\nProbabilities are between (0,1) and taking a log of numbers in this interval produces a negative value. Hence, the negative at the beginning of the expression.\nIf the natural log, ln, is used then it‚Äôs called deviance\n\nCalculate the entropy of the target, Y, with respect to¬†each independent variable, x. For variable,\n\n\n, with number of classes, c :\n\nI do NOT like the way the equation is written above. In videos, this type of entropy isn‚Äôt given a name, but I think it matches conditional entropy in its description and calculation.\n\nConditional Entropy (for a particular x),\n\n\nThis definition uses\n\n\nWhere H is used to as the symbol for entropy.\n\n\n\n\n\nThe marginal probability for that class of that variable, i.e.¬†ratio of instances of that class in the entire dataset.\n\nExample: \n\nNot explicitly shown above, but for the entropy calculations, it uses the sum of the rows as the denominator in probability calculations. This fits with a ‚Äúconditional‚Äù type of entropy.\n\n\nCalculate information gain for variable  \n\nRepeat for all independent variables\n\nSelect the independent variable with the largest gain for first split (\n\nFirst split, i.e.¬†root node, is the most influential variable\n\nIf categorical variable chosen, leaves are all levels of that variable\n\nSubset dataset by var == level (for each branch\nRepeat entropy and information gain calculations on the subsetted data set\n\nBranches with entropy &gt; 1 are split unless some other stopping criteria is reached\n\nChoose variable with largest information gain and split by that variable\nKeeping repeating until maxdepth reached or minimum node size (number of rows in subset) reached\n\nNumerical vars are binned and treated like categorical vars\nPredicted class is the mode of the classes in the appropriate terminal node\n\nRegression\n\nFor each predictor var, choose a separator value, s\n\ne.g var1 &gt; 5 and var1 &lt;= 5 where s = 5\n\nCalculate the mean y value for both regions then calculate the MSE ((obs - mean)^2) of both regions. Sum of both MSEs. The optimal separator produces the lowest sum MSE.\nWhichever predictor has lowest sum MSE is chosen as the split variable.\nRecursively repeat. For example, repeat on region where var1 &gt;5 and repeat on region where var1 &lt;= 5.\nContinue until max.depth, max splits reached or data points in created region is less than a minimum or MSEs being calculated are all greater than a chosen amount, or‚Ä¶ etc. (Hyperparameters)\nPrediction is the mean in the appropriate terminal node\n\n\n\n\n1.1.2 Random Forest\n\n\nSeveral independent decision trees are fitted. Each tree just gets a part of the variable and then splits the outcome space according to the features in X\nWhen we ‚Äò‚Äôdrop down‚Äô‚Äô a new point x, it will end up in a leaf for each tree. A leaf is a set with observations i and taking the average over all yi in that leaf gives the prediction for one tree. These predictions are then averaged to give the final result. Thus, for a given x if you want to predict the conditional mean of Y given that x, you:\n\n‚ÄúDrop down‚Äù the x each tree (this is indicated in red in the above figure). Since the splitting rules were made on X, your new point x will safely land somewhere in a leaf node.\nFor each tree you average the responses yi in that leaf to get an estimate of the conditional mean of each tree.\nYou average each conditional mean over the trees to get the final prediction.\n\nAveraging the prediction of all trees leads to a marked reduction in variance.\nMissing Predictor Data\n\nSee StatQuest: Random Forests Part 2: Missing data and clustering video for more details\nProcess: Classification model\n\nMissingness is in the training data\n\nChoose intial values for the missing data\n\nLooks at that predictor‚Äôs values that have the same outcome value as the observation with the missing data\n\nCategorical: For example, if the row has an observed outcome of 1 (i.e.¬†event), then it will look at that predictor‚Äôs values with outcome = 1 and choose the most popular category for the missing value\nNumeric: same as categorical, except the median value of predictor is chosen for the missing value\n\n\nCreate a ‚ÄúProximity Matrix‚Äù to determine which observation is most similar to observation with the missing data\n\nThe matrix values are counts of how many times each row ends up in the node as the missing data row across all the trees in the forest\nThe counts are then divided by the number of trees in the forest\n\nCategorical: Weights for each category are calculated (see video). These weights are multiplied times the observed frequency of the category in the training data. The category with the highest weighted frequency becomes the new value for the missing data.\nNumerical: The weights are used to calculate a weighted average. So, weight * median is the new value for the missing data\nProcess is repeated until the values don‚Äôt change within a tolerance\n\nMissingness in the out-of-sample data\n\nA copy of the observation with the missingness is made for each outcome category.\nThe proximity matrix procedure is done for each copy\nThen a prediction for each copy with it‚Äôs new value is made in each tree of the forest. (of course the label for each copy has now been stripped)\nWhichever copy had it‚Äôs (stripped) outcome label predicted correctly by the most trees wins and that label is prediction for that observation\n\n\n\n\n\n\n1.1.3 Isolation Forests\n\nUsed for anomaly detection. Algorithm related to binary search.\nNotes from paper: https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/icdm08b.pdf\nAlso see Anomaly Detection &gt;&gt; Isolation Forests\nThe tree algorithm chooses a predictor at random for the root node. Then randomly chooses either the minimum or the maximum of that variable as the splitting value. The algorithm recursively subsamples like normal trees (choosing variables and split points in the same manner) until each terminal node has one data point or replicates of the same data point or preset maximum tree height is reached. Across the trees of a forest, anomalies with have a shorter average path length from root to terminal node.\n\nThe algorithm is basically looking for observations with combinations of variables that have extreme values. The process of continually splitting subsamples of data will run out data points and be reduced to a single observation more quickly for an anomalous observation than a common observation.\nMakes sense. Picturing a tree structure, there shouldn‚Äôt be too many observations with more that a few minimums/maximums of variable values. The algorithm weeds out these observations as it moves down the tree structure.\n\nAny or all of these wouldn‚Äôt necessarily be global minimum/maximums since we‚Äôre dealing with subsamples of variable values as we move down the tree.\n\nPaper has some nice text boxes with pseudocode that goes through the steps of the algorithm.\n\nAnomaly scores range from 0 to 1. Observations with a shorter average path length will have a larger score.\n\nAnomaly score,\n\n\nWhere E(h(xi)) is the average path length across the isolation forest for that observation\n\n\n\nWhere H(i) is the Harmonic number,\n\n\nGuidelines\n\nThe closer an observation‚Äôs score is to 1 the more likely that it is an anomaly\nThe closer to zero, the more likely the observation isn‚Äôt an anomaly.\nObservations with scores around 0.5 means that the algorithm can‚Äôt find a distinction.\n\n\n\n\n\n1.1.4 Distributional Trees/Forests\n\n\nBlends the distributional modeling of gamlss (additive, nonlinear,¬† location, scale, shape) and the abrupt-change detection, additive + multiplicative effects capability, inclusion of interaction effects of decision trees / random forests. For regression trees, estimating all the distributional parameters instead of just the mean makes calculating the uncertainty easier.\n\nCART trees don‚Äôt have a¬†concept of statistical signiÔ¨Åcance, and so cannot distinguish between a signiÔ¨Åcant and an insigniÔ¨Åcant improvement in the information measure.\nCART tree predictions are¬†piecewise-constant (every observation in the node has the same prediction), it will not be accurate unless the tree is large. But a large tree is harder to interpret than a small one.\nLinear trends are difficult for trees with piecewise-constant predictions\n\nAlgorithm splits based on changes in the mean and higher moments. So able to capture things like changes in variance.\nConditional distributions allow for the computation of prediction intervals.\nThis framework embeds recursive partitioning into statistical model estimation and variable selection\nThe statistical formulation of the algorithm ensures the validity of interpretations drawn from the resulting model\n{partykit}\n\nNotes from: https://arxiv.org/pdf/1804.02921.pdf\ntl;dr procedure\n\nFor each distributional parameter (e.g.¬†mean, sd), a ‚Äúscore‚Äù matrix is computed using the target values and the distribution‚Äôs likelihood function\nThe score matrix is used to create a test statistic for each predictor\nThe predictor with the lowest p-value associated with its test statistic is the splitting variable\nThe split point is determined by the point that produces the lowest p-value in one of the split regions\nProcess continues for each leaf until no variables produce a p-value below a certain threshold (e.g.¬†Œ± = 0.05)\nThe distributional parameters associated with the leaf that a new observation falls into is used as the prediction for a tree.\n\nTree procedure\n\nFor each distributional parameter (e.g.¬†mean, std.dev), calculate the value of the maximum likelihood estimator (MLE)\nTake the derivative of the log-likelihood function. Plug in the value(s) of the MLE parameter(s) and a yi value to get a ‚Äúscore‚Äù for every value of the response variable. Repeat for each distributional parameter. (The score should fluctuate around zero.)\n\n\nand\n\n\nWhere k is the number of distributional parameters and n is the number of training observations\n\nGaussian example with\n :\n\nSolve for the MLE of the mean and calculate the value:\n\nSolve for the MLE of the variance and calculate the value:\n\nWe‚Äôre calculating a score for each value of the outcome variable so we can remove the summation symbol from the derivative of the log-likelihood function w.r.t. the mean. This leaves us with the mean score function:\n\nSame thing but with the derivative of the log-likelihood function w.r.t. the variance:\n\n\n\nNull Hypothesis test each predictor variable vs the parameter score matrix where H0 = independence ‚Äî Two methods: CTree and MOB\n\nCTree is permutation test based\n\nEach test statistic vector, T, for 1,‚Ä¶,l predictors and n observations is calculated by:\n\n\nwhere\n\n\nis the 1xk row of the score matrix and v is a transformation function that depends on whether the predictor variable, Z, is a numeric or character type.\n\n\n\nIf the predictor variable, Z, is a numeric:\n\nv is an identity function, so Z remains unchanged.\nCorresponding to the first observation, the first row of the score matrix is multiplied by the first value of the predictor variable resulting in a 1xk row vector.\nn 1xk row vectors are added together\nThe summed 1xk vector is transposed by the vec function into the k-vector, T.\n\nIf the predictor variable, Z, is a character variable with H categories:\n\nv creates an indicator variable where the hth value is 1 indicates that Zi‚Äôs value is the hth category.\nCorresponding to the first observation, we multiply this Hx1 vector times the 1xk, first row of the score matrix which results in a sparse Hxk matrix.\nn Hxk matrices are added together\nvec then stacks each column of the summed Hxk matrix to create a column vector, T, with H*k rows.\n\n\nT is standardized by maximum or quadratic method.\n\nt just represents a statistic that‚Äôs calculated from a permutation of the scores. T is handled in the same way.\npartykit::ctree.pdf shows the calculations for Œº and Œ£\n\n\nwhere Œ£+ is the pseudo-inverse of the covariance matrix\n\nCalculating the pseudo-inverse makes this method more computationally intensive\n\nUsing this quadratic method, c is Chi-Square test statistic.\n\n\n\nFor the maximum method, c is Normal test statistic (partykit::ctree.pdf)\nno idea why the numerator has one ‚Äúk‚Äù and the bottom has ‚Äúkk.‚Äù Maybe it‚Äôs a typo.\n\n\nFind the p-value associated with each predictor‚Äôs c statistic\n\n\n\nThis says the p-value, P, is the probability,\n\n\n, (associated with the null hypothesis for this particular variable)¬†that the standardized T stat is as or more extreme than the group of standardized t stats of the permuted scores.\n\n\n\nis the symmetric group of permutations and weights. Weights being either 0 or 1 depending on whether the observation is present in that node‚Äôs data subset.\n\n\n\n\nMOB stands for model based method\n\nUses a M-Fluctuation test to test for an ‚Äúinstability‚Äù by calculating a supLM test statistic. An instability is what‚Äôs interpreted from a p-value &lt; 0.05\n\n\n\n\nis a minimum amount of scores that you choose, then\n\nNo guidelines for\n\n\nIn addition to choosing a minimum, the paper does mention also trimming node data points at each end by 10%.\n\n\n\nis called ‚Äúits variance function.‚Äù\n\n\n\n\n\n\nmeans floor of nt, which means round down to the integer.\n\nThe subscript\n\n\nsays the scores are ordered from highest to lowest (aka anti-rank) according to the predictor variable, Zj, values\n\n\ncovariance matrix\n\n\nThe distribution from which the p-value is calculated has something to do with a Bessel process and stuff converging to a Brownian Bridge, so I decided to shut it down here.\n\nSee https://eeecon.uibk.ac.at/~zeileis/papers/Zeileis%2BHothorn%2BHornik-2008.pdf for details\n\n\n\nBonferonni adjust the p-value according the number of variables, m\n\n\n\nSelect predictor variables with adjusted p-values lower than the threshold\nFrom that selection, the predictor with the lowest p-value is chosen as the splitting variable.\n\nIf no p-values are lower than the threshold, splitting is halted and the terminal node is reached for that branch (aka pre-pruning).\nPre-pruning not usually done in distributional forests (mincriterion = 0).\n\nChoose the optimal split point for the chosen variable where the lowest p-value is produced in one of the two created sub-regions (i.e the maximum test statistic).\nProcedure is repeated (like traditional trees) in the created leaves and continues until stopping criteria reached (e.g.¬†no p-values lower than threshold, number of observations in node is below minimum, etc)\nIn practice, predicting, using just a tree, involves finding the node with the criterion that fits the new observation and using the estimated distributional parameters of the subsample belonging to that node as the model prediction.\n\nThe paper says this can be thought of as a weighted maximum likelihood estimation. The mathematical notation is similar to what I show below for forests.\n\n\nForest procedure\n\nThe idea of random forests is to train an ensemble of trees, each on diÔ¨Äerent training data obtained through resampling or subsampling. In each node only a random subset of the covariates is considered for splitting to reduce the correlation among the trees and to stabilize the variance of the model\nNotes from: https://arxiv.org/pdf/1701.02110.pdf\nPretty good explainer of the weight system used below to calculate predicted means across all leaves in the forest (from {drf} explainer)\n\nInstead of directly calculating the mean in a leaf node, one calculates the weights that are implicitly used when doing the mean calculation. The weight wi(x) is a function of (1) the test point x and (2) an observation i. That is, if we drop x down a tree, we observe in which leaf it ends up. All observations that are in that leaf get a 1, all others 0.\n\nSo if we end up in a leaf with observations (1,3,10), then the weight of observations 1,3,10 for that tree is 1, while all other observations get 0.\n\nWe then further divide that weight by the number of elements in the leaf node.\n\nIn the example before, we had 3 observations in the leaf node, so the weights for observations 1,3, 10 are 1/3 each, while all other observations still get a weight of 0 in this tree. Averaging these weights over all trees gives the final weight wi(x).\n\nCalculating the mean as we do in a traditional Random Forest, is the same as summing up wi(x)*yi\n\nFor predictions:\n\nFor a training observation and a tree, determine whether a new observation z¬†belongs in the same node as the training observation, zi.\nCalculate a weight according to whether the new observation and the training observation are in the same node.\n\nIf they are in the same node\n\n where i denotes the training observation and n is the number of observations in that node of that particular tree, t.\n\nIf zi, the training observation, and the new observation, z, aren‚Äôt in the same node\n\nwit = 0\n\n\nCalculate wit for each tree the training observation belongs to.\n\nResampling or subsampling may exclude a training observation from some of the trees\n\nSum all tree weights, wit, for that training observation and divide by the number of trees to get the forest-weight for that training observation.\n\n\n\nWhere Ti is the total trees that use that training observation in its learning sample.\nSo the forest weight is the average weight per tree for that training observation\n\nIn the paper, this process is described by a more compact notation:\n\n\nThe numerator in the end part of this equation indicates whether the bth terminal node, Bb, contains the training observation, zi, and the new observation, z, for tree, t.\n is the number of observations in that terminal node\n\n\nThe paper describes predictions being calculated by a weighted MLE as it did for a single tree, but for forests it didn‚Äôt explicitly give an ‚Äúin practice‚Äù description of process.\n\n\n\nEach parameter that has been calculated for each terminal node of each tree has a subset of the learning data associated with it. The forest weight-likelihood products of each observation are summed over this subset. The parameter with the largest sum is chosen as the prediction.\n\nRepeat for each distributional parameter.\n\n\n\n\n\n\n{drf}\n\n\nNotes from DRF: A Random Forest for (almost) everything\nUltimately when a RF finishes splitting data, each leaf should ideallly contain a homogeneous set of points in terms of approximating a the conditional distribution, P(Y|X=xi), but this only applies to the conditional mean of that leaf. As seen in the pic, the mean doesn‚Äôt fully describe that leaf‚Äôs conditional distribution.\n\nEvery distribution except x2 has a similar means, but sets (x1,x4,x6) and (x3, x5, x7) have different variances.\n\ndrf is able to fit a RF with these more homogeneous leaves by transforming the leaf‚Äôs yi subsamples into a Reproducing Kernel Hilbert Space with a kernel.\n\nIn this infinite-dimensional space, conditional means are able to fully represent conditional distributions and the Maximum Mean Discrepancy (MMD) is (efficiently) calculated.\nThe MMD measures the similarity between distributions\nThus if the conditional distribution of Y given xi and xj are similar, they will be grouped in the same leaf.\n\ndrf uses the same weighting system for its forest as {partykit} in order to produce predictions.",
    "crumbs": [
      "Algorithms",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>ML</span>"
    ]
  },
  {
    "objectID": "qmd/algorithms-ml.html#sec-alg-ml-boost",
    "href": "qmd/algorithms-ml.html#sec-alg-ml-boost",
    "title": "1¬† ML",
    "section": "1.2 Boosting",
    "text": "1.2 Boosting\n\nfrom https://www.economist.com/graphic-detail/2021/03/11/how-we-built-our-covid-19-risk-estimator\n\nIn order to capture such complexity, we needed to allow for the possibility that comorbidities do not have constant effects that can simply be added together, but instead interact with each other, producing overall risk levels that are either higher or lower than the sum of their parts.\n\nSays main effects weren‚Äôt good enough and needed to use interactions\n\nGradient-boosted trees make predictions by constructing a series of ‚Äúdecision trees‚Äù, one after the other. The first tree might begin by checking if a patient has hypertension, and then if they are older or younger than 65. It might find that people over 65 with hypertension often have a fatal outcome. If so, then whenever those conditions are met, it will move predictions in this direction. The next tree then seeks to improve on the prediction produced by the previous one. Relationships between variables are discovered or refined with each tree.\n\nDifference between XGBoost and LightGBM (Raschka)\n\nXGBoost‚Äôs trees are based on breadth-first search, comparing different features at each node.\nLightGBM performs depth-first search, focusing on a single feature & growing the tree from there.\n\n\n\n1.2.1 Gradient boosted machines (GBM)\n\nChoose a differentiable loss function, œÅ, such as \n\nIn gradient boosting, 1/n is exchanged for 1/2, to make it differentiable. The mean of the loss function, SSE in this case, calculated over all observations for a model is called the ‚Äúempirical risk‚Äù which is what boosting is trying to minimize.\n\nCalculate the negative gradient, aka first derivative. For regression trees, this turns out to be\n\n\nwhich is just the residuals. Pg 360 of The Elements of Statistical Learning has other loss functions and their negative gradients. Classification uses a multinomial deviance loss function.\n\nInitialize using the optimal constant model, which is just a single terminal node tree. Think this means the initial predictions are just mean of the target,\n\nCalculate the negative gradient vector (residuals), r, by plugging in the predicted values.\nFit a regression tree with the residuals, r, as the target variable. The mean of the residuals for that region (terminal node) is the prediction,\n .\nAdd the predicted residuals vector to the initial predictions vector,\n\n\nto get the next set of predictions to feed into the negative gradient equation.\n\nRepeat steps 4 - 6 until some stopping criteria is met.\n\n\n\n1.2.2 LightGBM\n\nFind optimal split points using a histogram based algorithm\n\nGOSS (Gradient Based One Side Sampling)\n\nretains instances with large gradients while performing random sampling on instances with small gradients.\n\nExample: Gaussian Regression - observations with small residuals are downsampled by random selection while those observations with large residuals remain\n\n\n\nEFB (Exclusive Feature Bundling)\n\nReduce feature space by bundling features together that are ‚Äúmutually exclusive‚Äù (i.e.¬†varA doesn‚Äôt take a value of 0 in the same observation as varB).\n\ni.e.¬†Bundles sparse features together\n\nCreate bundles and assign features\n\nConstruct a graph with weighted (measure of conflict between features) edges. Conflict is measure of the fraction of exclusive features which have overlapping non zero values.\nSort the features by count of non zero instances in descending order.\nLoop over the ordered list of features and assign the feature to an existing bundle (if conflict &lt; threshold) or create a new bundle (if conflict &gt; threshold).\n\nMerging\n\nArticle wasn‚Äôt coherent on this precedure\n\n\n\n\n\n1.2.3 XGBoost\n\nFYI has various gradient function families: binomial, poisson, tweedie, softmax (multi-category classification)\nUtilizes histogram-based algorithm for finding optimal split points.\n\nBuckets continuous features into discrete bins to construct feature histograms during training. It costs O(#data * #feature) for histogram building and O(#bin * #feature) for split point finding.\n\nRegularization: It penalizes more complex models through both LASSO (L1) and Ridge (L2) regularization to prevent overfitting.\n\nRegularization function\n\n\nThis function gets minimized during training\nT is the total number of trees\nw is a leaf weight\nTuning parameters\n\nŒ± controls how much we want to penalize the sum of the absolute value of leaf weights (L1 regularization)\nŒª controls how much we want to penalize the sum of squared leaf weights¬† (L2 regularization)\nŒ≥ is used to control how the number of trees in the model is penalized\n\n\n\nSparsity Awareness: XGBoost naturally admits sparse features for inputs by automatically ‚Äòlearning‚Äô best missing value depending on training loss and handles different types of sparsity patterns in the data more efficiently.\nWeighted Quantile Sketch: XGBoost employs the distributed weighted Quantile Sketch algorithm to effectively find the optimal split points among weighted datasets.\nMultinomial: all the trees are constructed at the same time, using a vector objective function instead of a scalar one, i.e.¬†there is an objective for each class. (i.e.¬†Classifying n classes generate trees n times more complex)\n\nThe objective name is multi:softprob when using the integrated objective in XGBoost. Although, the aim is not really the softprob , but the log loss of the softmax. But softmax is not the gradient of softmax , but the gradient of its log loss\n\\[\n\\begin {align}\n\\mbox{soft}_\\mbox{max}(x_i) &= \\frac{e^{x_i}}{\\sum_j e^{x_j}}\\\\\n\\mbox{log}_\\mbox{loss}(x_i) &= -\\ln(\\mbox{soft}_\\mbox{max}(x_i)) = -ln(e^{x_i}) + \\ln\\left(\\sum_j e^{x_j}\\right) = -x_i + \\ln\\left(\\sum_j e^{x_j}\\right) \\\\\n\\frac{\\partial\\mbox{log}_\\mbox{loss}(x_i)}{\\partial x_i} &= \\frac{\\partial (-x_i + \\ln\\left(\\sum_j e^{x_j}\\right)}{\\partial x_i} = -1 + \\frac{e^{x_i}}{\\sum_j e^{x_j}} = \\mbox{soft}_\\mbox{max}(x_i)\n\\end {align}\n\\]\n\ni.e.¬†the objective optimized is not softmax or softprob, but their log loss.\n\n\nCross-validation: The algorithm comes with built-in cross-validation method at each iteration, taking away the need to explicitly program this search and to specify the exact number of boosting iterations required in a single run.\nComponent-wise Boosting (From mboost_PKG tutorial docs)\n\ntl;dr - Same as GBM except instead of fitting a tree model to the residuals, you‚Äôre fitting many types of small models (few predictors). Best small model updates the predictions after each iteration.\nThe goal is to minimize empirical risk\n\n\nwhere\n\n\nis the loss function and\n\n\nthe predictor function\n\nThe loss function is usually the negative log-likelihood function for the distribution of the outcome variable. For a Gaussian distribution, this is equivalent to the least squares objective function\n\nSteps\n\nCompute the negative gradient of the loss function which is the negative first derivative with respect to the predictor function,\n .\n\n\ncan be thought of as the vector of predictions at the mth¬†iteration of the algorithm. So to begin, we create an initial vector,\n\n\nwith ‚Äúoffset values.‚Äù\n\nFor glmboost, the offset is the mean of the outcome variable. I‚Äôd guess it‚Äôs probably the same for gamboost.\n\nCompute the negative gradient vector:\n\n\nFor the first iteration, m = 1 and\n\n\nFit each baselearner to the negative gradient vector.\n\nA baselearner is like a subset statistical model of the overall statistical model. They can be linear regressors, penalized linear regressors, shallow trees, penalized splines, etc. Each one using one or more predictors with or without interaction terms.\n\nFor each baselearner, calculate the residual sum of squares,\n\n\nfrom it‚Äôs predictions.\n\nWhichever baselearner has the smallest RSS, scale it‚Äôs predictions with a learning rate factor and combine them with the previous iteration‚Äôs predictions\n\n\n\nwhere\n\n\nis the learning rate.\nOptimization of ŒΩ isn‚Äôt critical. Only required to be low enough as to not overshoot the minimum empirical risk, e.g.¬†ŒΩ = 0.1.\n\n\nAfter the predictions are updated, steps 3-6 are repeated until the number of iterations, set by the value of the mstop, is reached.\n\nmstop is a hyperparameter that you optimize to prevent overfitting. The value can be chosen by cv or AIC\n\n\nEach baselearner‚Äôs contribution to the final prediction vector is\n\n\nover all iterations where that baselearner was selected\nI think this is the value of the\n\n\non the y-axis of the partial dependence plots (pdp).\nIf the variables have been centered (maybe need to be completely standardized), then the magnitude of the y-axis (and the variable range within it) can be used as a signifier of variable importance and variables can be compared that way.\nIf a variable has multiple baselearners selected, you can combine all the contributions and plot the combined effect pdp by predict(which = ‚Äúvariable‚Äù), row summing the values, and plotting. See the end of mboost.pdf for details.\n\n\nComponent-wise boosting performs variable selection unlike some other boosting algorithms",
    "crumbs": [
      "Algorithms",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>ML</span>"
    ]
  },
  {
    "objectID": "qmd/shiny-python.html",
    "href": "qmd/shiny-python.html",
    "title": "Python",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Shiny",
      "Python"
    ]
  },
  {
    "objectID": "qmd/shiny-python.html#sec-shiny-py-misc",
    "href": "qmd/shiny-python.html#sec-shiny-py-misc",
    "title": "Python",
    "section": "",
    "text": "Resources\n\nBizSci Video: basic, plotly, finance app\nPosit Templates\n\nBuilt off of same technology as {{fastapi}} which is uvicorn",
    "crumbs": [
      "Shiny",
      "Python"
    ]
  }
]